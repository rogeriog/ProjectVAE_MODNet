start
Wed Feb 15 04:19:38 CET 2023
2023-02-15 04:19:39.778598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-15 04:19:39.932686: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
         OFM: s^1 - s^1  OFM: s^1 - s^2  ...  OFM: f^14 - f^13  OFM: f^14 - f^14
id                                       ...                                    
id40482        0.000000        0.000000  ...               0.0          0.000000
id40486        0.000000        0.000000  ...               0.0          0.000000
id40487        0.000000        0.000000  ...               0.0          0.000022
id40490        0.000000        0.080160  ...               0.0          0.000000
id40496        0.000000        0.000000  ...               0.0          0.000000
...                 ...             ...  ...               ...               ...
id83741        0.174544        0.746256  ...               0.0          0.000000
id83742        0.000000        0.000000  ...               0.0          1.815492
id83743        0.007311        0.241355  ...               0.0          0.000000
id83744        0.000000        0.000000  ...               0.0          0.439306
id83745        0.000000        0.203940  ...               0.0          0.000000

[93419 rows x 1024 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 90 0.001 64 1]
 [2.1 90 0.001 64 1]
 [1.5 70 0.0005 16 0]
 [2.5 50 0.0005 64 2]
 [2.0 110 0.0005 8 0]
 [2.0 170 0.0005 16 0]
 [2.0 25 0.001 16 1]
 [1.5 90 0.001 32 1]
 [0.5 25 0.001 64 0]
 [1.5 30 0.0005 8 1]]
[2.0 90 0.001 64 1] 0
Shape of dataset to encode: (93419, 943)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1886)        7544        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1886)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          711399      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          711399      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2643907     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,854,633
Trainable params: 5,846,335
Non-trainable params: 8,298
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-15 04:20:18.295752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-15 04:20:19.590972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:37:00.0, compute capability: 7.0
2023-02-15 04:20:19.592184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30971 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0
2023-02-15 04:20:19.619983: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-15 04:20:20.217480: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense_dec1/bias/v/Assign' id:1080 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_dec1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_dec1/bias/v, training/Adam/dense_dec1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:20:29.050649: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:450 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 11s - loss: 0.0076 - val_loss: 0.0018 - 11s/epoch - 134us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0096 - val_loss: 1222.4277 - 8s/epoch - 98us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0019 - val_loss: 9.9828e-04 - 8s/epoch - 99us/sample
Epoch 4/90
84077/84077 - 8s - loss: 9.9087e-04 - val_loss: 7.5440e-04 - 8s/epoch - 98us/sample
Epoch 5/90
84077/84077 - 8s - loss: 8.3542e-04 - val_loss: 6.7307e-04 - 8s/epoch - 96us/sample
Epoch 6/90
84077/84077 - 8s - loss: 6.9932e-04 - val_loss: 5.6382e-04 - 8s/epoch - 97us/sample
Epoch 7/90
84077/84077 - 8s - loss: 6.1299e-04 - val_loss: 5.0797e-04 - 8s/epoch - 96us/sample
Epoch 8/90
84077/84077 - 8s - loss: 0.0011 - val_loss: 4.8260e-04 - 8s/epoch - 96us/sample
Epoch 9/90
84077/84077 - 8s - loss: 5.4160e-04 - val_loss: 3.9265e-04 - 8s/epoch - 97us/sample
Epoch 10/90
84077/84077 - 8s - loss: 4.2390e-04 - val_loss: 3.7225e-04 - 8s/epoch - 96us/sample
Epoch 11/90
84077/84077 - 8s - loss: 3.7247e-04 - val_loss: 3.7066e-04 - 8s/epoch - 96us/sample
Epoch 12/90
84077/84077 - 8s - loss: 3.3854e-04 - val_loss: 2.8762e-04 - 8s/epoch - 96us/sample
Epoch 13/90
84077/84077 - 8s - loss: 3.0549e-04 - val_loss: 2.5782e-04 - 8s/epoch - 97us/sample
Epoch 14/90
84077/84077 - 8s - loss: 2.7937e-04 - val_loss: 2.4726e-04 - 8s/epoch - 97us/sample
Epoch 15/90
84077/84077 - 8s - loss: 2.6323e-04 - val_loss: 2.2953e-04 - 8s/epoch - 97us/sample
Epoch 16/90
84077/84077 - 8s - loss: 2.5056e-04 - val_loss: 2.1445e-04 - 8s/epoch - 96us/sample
Epoch 17/90
84077/84077 - 8s - loss: 2.3316e-04 - val_loss: 2.0598e-04 - 8s/epoch - 96us/sample
Epoch 18/90
84077/84077 - 8s - loss: 2.2203e-04 - val_loss: 2.0594e-04 - 8s/epoch - 99us/sample
Epoch 19/90
84077/84077 - 8s - loss: 2.1219e-04 - val_loss: 1.8998e-04 - 8s/epoch - 100us/sample
Epoch 20/90
84077/84077 - 8s - loss: 2.0441e-04 - val_loss: 1.8702e-04 - 8s/epoch - 96us/sample
Epoch 21/90
84077/84077 - 8s - loss: 1.9777e-04 - val_loss: 1.7947e-04 - 8s/epoch - 96us/sample
Epoch 22/90
84077/84077 - 8s - loss: 1.9058e-04 - val_loss: 1.7306e-04 - 8s/epoch - 96us/sample
Epoch 23/90
84077/84077 - 8s - loss: 1.8599e-04 - val_loss: 1.7564e-04 - 8s/epoch - 97us/sample
Epoch 24/90
84077/84077 - 8s - loss: 1.8098e-04 - val_loss: 1.6983e-04 - 8s/epoch - 96us/sample
Epoch 25/90
84077/84077 - 8s - loss: 1.7712e-04 - val_loss: 1.6293e-04 - 8s/epoch - 96us/sample
Epoch 26/90
84077/84077 - 8s - loss: 1.7512e-04 - val_loss: 1.6649e-04 - 8s/epoch - 96us/sample
Epoch 27/90
84077/84077 - 8s - loss: 1.7094e-04 - val_loss: 1.5920e-04 - 8s/epoch - 97us/sample
Epoch 28/90
84077/84077 - 8s - loss: 1.6797e-04 - val_loss: 1.5697e-04 - 8s/epoch - 96us/sample
Epoch 29/90
84077/84077 - 8s - loss: 1.6652e-04 - val_loss: 1.5339e-04 - 8s/epoch - 96us/sample
Epoch 30/90
84077/84077 - 8s - loss: 1.6224e-04 - val_loss: 1.5089e-04 - 8s/epoch - 96us/sample
Epoch 31/90
84077/84077 - 8s - loss: 1.6082e-04 - val_loss: 1.5153e-04 - 8s/epoch - 97us/sample
Epoch 32/90
84077/84077 - 8s - loss: 1.5892e-04 - val_loss: 1.5151e-04 - 8s/epoch - 97us/sample
Epoch 33/90
84077/84077 - 8s - loss: 1.5732e-04 - val_loss: 1.4782e-04 - 8s/epoch - 97us/sample
Epoch 34/90
84077/84077 - 8s - loss: 1.5522e-04 - val_loss: 1.4518e-04 - 8s/epoch - 96us/sample
Epoch 35/90
84077/84077 - 8s - loss: 1.5226e-04 - val_loss: 1.4423e-04 - 8s/epoch - 96us/sample
Epoch 36/90
84077/84077 - 8s - loss: 1.5187e-04 - val_loss: 1.4425e-04 - 8s/epoch - 96us/sample
Epoch 37/90
84077/84077 - 8s - loss: 1.5065e-04 - val_loss: 1.4394e-04 - 8s/epoch - 97us/sample
Epoch 38/90
84077/84077 - 8s - loss: 1.5187e-04 - val_loss: 1.4923e-04 - 8s/epoch - 96us/sample
Epoch 39/90
84077/84077 - 8s - loss: 1.4869e-04 - val_loss: 1.4239e-04 - 8s/epoch - 97us/sample
Epoch 40/90
84077/84077 - 8s - loss: 1.4710e-04 - val_loss: 1.3950e-04 - 8s/epoch - 96us/sample
Epoch 41/90
84077/84077 - 8s - loss: 1.4692e-04 - val_loss: 1.4186e-04 - 8s/epoch - 96us/sample
Epoch 42/90
84077/84077 - 8s - loss: 1.4466e-04 - val_loss: 1.4025e-04 - 8s/epoch - 98us/sample
Epoch 43/90
84077/84077 - 8s - loss: 1.4346e-04 - val_loss: 1.3729e-04 - 8s/epoch - 96us/sample
Epoch 44/90
84077/84077 - 8s - loss: 1.4352e-04 - val_loss: 1.3865e-04 - 8s/epoch - 96us/sample
Epoch 45/90
84077/84077 - 8s - loss: 1.4239e-04 - val_loss: 1.3591e-04 - 8s/epoch - 96us/sample
Epoch 46/90
84077/84077 - 8s - loss: 1.4157e-04 - val_loss: 1.3572e-04 - 8s/epoch - 96us/sample
Epoch 47/90
84077/84077 - 8s - loss: 1.4056e-04 - val_loss: 1.3522e-04 - 8s/epoch - 97us/sample
Epoch 48/90
84077/84077 - 8s - loss: 1.4063e-04 - val_loss: 1.3253e-04 - 8s/epoch - 97us/sample
Epoch 49/90
84077/84077 - 8s - loss: 1.3817e-04 - val_loss: 1.3351e-04 - 8s/epoch - 96us/sample
Epoch 50/90
84077/84077 - 8s - loss: 1.3828e-04 - val_loss: 1.3129e-04 - 8s/epoch - 96us/sample
Epoch 51/90
84077/84077 - 8s - loss: 1.3770e-04 - val_loss: 1.3218e-04 - 8s/epoch - 97us/sample
Epoch 52/90
84077/84077 - 8s - loss: 1.3667e-04 - val_loss: 1.3282e-04 - 8s/epoch - 96us/sample
Epoch 53/90
84077/84077 - 8s - loss: 1.3808e-04 - val_loss: 1.3240e-04 - 8s/epoch - 96us/sample
Epoch 54/90
84077/84077 - 8s - loss: 1.3610e-04 - val_loss: 1.3232e-04 - 8s/epoch - 96us/sample
Epoch 55/90
84077/84077 - 8s - loss: 1.3536e-04 - val_loss: 1.3036e-04 - 8s/epoch - 97us/sample
Epoch 56/90
84077/84077 - 8s - loss: 1.3529e-04 - val_loss: 1.2881e-04 - 8s/epoch - 97us/sample
Epoch 57/90
84077/84077 - 8s - loss: 1.3417e-04 - val_loss: 1.3092e-04 - 8s/epoch - 96us/sample
Epoch 58/90
84077/84077 - 8s - loss: 1.3353e-04 - val_loss: 1.2996e-04 - 8s/epoch - 96us/sample
Epoch 59/90
84077/84077 - 8s - loss: 1.3308e-04 - val_loss: 1.3163e-04 - 8s/epoch - 97us/sample
Epoch 60/90
84077/84077 - 8s - loss: 1.3374e-04 - val_loss: 1.2833e-04 - 8s/epoch - 97us/sample
Epoch 61/90
84077/84077 - 8s - loss: 1.3261e-04 - val_loss: 1.2702e-04 - 8s/epoch - 96us/sample
Epoch 62/90
84077/84077 - 8s - loss: 1.3143e-04 - val_loss: 1.3010e-04 - 8s/epoch - 96us/sample
Epoch 63/90
84077/84077 - 8s - loss: 1.3161e-04 - val_loss: 1.2830e-04 - 8s/epoch - 97us/sample
Epoch 64/90
84077/84077 - 8s - loss: 1.3665e-04 - val_loss: 1.2809e-04 - 8s/epoch - 97us/sample
Epoch 65/90
84077/84077 - 8s - loss: 1.3145e-04 - val_loss: 1.2647e-04 - 8s/epoch - 96us/sample
Epoch 66/90
84077/84077 - 8s - loss: 1.3039e-04 - val_loss: 1.2678e-04 - 8s/epoch - 96us/sample
Epoch 67/90
84077/84077 - 8s - loss: 1.2995e-04 - val_loss: 1.2639e-04 - 8s/epoch - 96us/sample
Epoch 68/90
84077/84077 - 8s - loss: 1.2984e-04 - val_loss: 1.2480e-04 - 8s/epoch - 96us/sample
Epoch 69/90
84077/84077 - 8s - loss: 1.2885e-04 - val_loss: 1.2489e-04 - 8s/epoch - 97us/sample
Epoch 70/90
84077/84077 - 8s - loss: 1.2872e-04 - val_loss: 1.2415e-04 - 8s/epoch - 96us/sample
Epoch 71/90
84077/84077 - 8s - loss: 1.2890e-04 - val_loss: 1.2658e-04 - 8s/epoch - 96us/sample
Epoch 72/90
84077/84077 - 8s - loss: 1.2822e-04 - val_loss: 1.2592e-04 - 8s/epoch - 96us/sample
Epoch 73/90
84077/84077 - 8s - loss: 1.2826e-04 - val_loss: 1.2392e-04 - 8s/epoch - 97us/sample
Epoch 74/90
84077/84077 - 8s - loss: 1.3045e-04 - val_loss: 1.2317e-04 - 8s/epoch - 96us/sample
Epoch 75/90
84077/84077 - 8s - loss: 1.2712e-04 - val_loss: 1.2404e-04 - 8s/epoch - 97us/sample
Epoch 76/90
84077/84077 - 8s - loss: 1.2731e-04 - val_loss: 1.2233e-04 - 8s/epoch - 96us/sample
Epoch 77/90
84077/84077 - 8s - loss: 1.2636e-04 - val_loss: 1.2415e-04 - 8s/epoch - 96us/sample
Epoch 78/90
84077/84077 - 8s - loss: 1.2619e-04 - val_loss: 1.2301e-04 - 8s/epoch - 97us/sample
Epoch 79/90
84077/84077 - 8s - loss: 1.2851e-04 - val_loss: 1.2205e-04 - 8s/epoch - 97us/sample
Epoch 80/90
84077/84077 - 8s - loss: 1.2648e-04 - val_loss: 1.2539e-04 - 8s/epoch - 96us/sample
Epoch 81/90
84077/84077 - 8s - loss: 1.2527e-04 - val_loss: 1.2304e-04 - 8s/epoch - 96us/sample
Epoch 82/90
84077/84077 - 8s - loss: 1.2611e-04 - val_loss: 1.2323e-04 - 8s/epoch - 96us/sample
Epoch 83/90
84077/84077 - 8s - loss: 1.2564e-04 - val_loss: 1.2350e-04 - 8s/epoch - 97us/sample
Epoch 84/90
84077/84077 - 8s - loss: 1.2553e-04 - val_loss: 1.2153e-04 - 8s/epoch - 96us/sample
Epoch 85/90
84077/84077 - 8s - loss: 1.2444e-04 - val_loss: 1.2010e-04 - 8s/epoch - 97us/sample
Epoch 86/90
84077/84077 - 8s - loss: 1.2439e-04 - val_loss: 1.2197e-04 - 8s/epoch - 96us/sample
Epoch 87/90
84077/84077 - 8s - loss: 1.2487e-04 - val_loss: 1.2065e-04 - 8s/epoch - 97us/sample
Epoch 88/90
84077/84077 - 8s - loss: 1.2346e-04 - val_loss: 1.1854e-04 - 8s/epoch - 97us/sample
Epoch 89/90
84077/84077 - 8s - loss: 1.2357e-04 - val_loss: 1.2375e-04 - 8s/epoch - 97us/sample
Epoch 90/90
84077/84077 - 8s - loss: 1.2379e-04 - val_loss: 1.2249e-04 - 8s/epoch - 96us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.00012249223703564966
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:32:32.638144: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:421 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014460620026007101
cosine 0.014285079758390747
MAE: 0.001607306777466406
RMSE: 0.007576481173921376
r2: 0.9554490081877969
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 90, 0.001, 0.4, 377, 0.0001237937672643294, 0.00012249223703564966, 0.014460620026007101, 0.014285079758390747, 0.001607306777466406, 0.007576481173921376, 0.9554490081877969, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1980)        7920        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1980)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          746837      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          746837      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2768457     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,139,171
Trainable params: 6,130,497
Non-trainable params: 8,674
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 04:32:39.474487: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/batch_normalization_5/gamma/v/Assign' id:2369 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/batch_normalization_5/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/batch_normalization_5/gamma/v, training_2/Adam/batch_normalization_5/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:32:47.906182: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1711 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 9s - loss: 0.0079 - val_loss: 0.0015 - 9s/epoch - 109us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 99us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0020 - val_loss: 0.0011 - 8s/epoch - 99us/sample
Epoch 4/90
84077/84077 - 8s - loss: 0.0024 - val_loss: 0.0012 - 8s/epoch - 99us/sample
Epoch 5/90
84077/84077 - 8s - loss: 9.5873e-04 - val_loss: 8.3755e-04 - 8s/epoch - 98us/sample
Epoch 6/90
84077/84077 - 8s - loss: 7.9678e-04 - val_loss: 6.7815e-04 - 8s/epoch - 99us/sample
Epoch 7/90
84077/84077 - 8s - loss: 8.7300e-04 - val_loss: 9.3966e-04 - 8s/epoch - 99us/sample
Epoch 8/90
84077/84077 - 8s - loss: 7.3188e-04 - val_loss: 5.3808e-04 - 8s/epoch - 99us/sample
Epoch 9/90
84077/84077 - 8s - loss: 5.5904e-04 - val_loss: 6.3664e-04 - 8s/epoch - 99us/sample
Epoch 10/90
84077/84077 - 8s - loss: 5.0291e-04 - val_loss: 4.2020e-04 - 8s/epoch - 99us/sample
Epoch 11/90
84077/84077 - 8s - loss: 6.1719e-04 - val_loss: 5.4854e-04 - 8s/epoch - 100us/sample
Epoch 12/90
84077/84077 - 8s - loss: 4.2823e-04 - val_loss: 3.3921e-04 - 8s/epoch - 99us/sample
Epoch 13/90
84077/84077 - 8s - loss: 3.7554e-04 - val_loss: 3.1965e-04 - 8s/epoch - 99us/sample
Epoch 14/90
84077/84077 - 8s - loss: 3.4297e-04 - val_loss: 2.9550e-04 - 8s/epoch - 99us/sample
Epoch 15/90
84077/84077 - 8s - loss: 3.2131e-04 - val_loss: 2.7497e-04 - 8s/epoch - 99us/sample
Epoch 16/90
84077/84077 - 8s - loss: 2.9709e-04 - val_loss: 2.6012e-04 - 8s/epoch - 100us/sample
Epoch 17/90
84077/84077 - 8s - loss: 2.8125e-04 - val_loss: 2.4681e-04 - 8s/epoch - 99us/sample
Epoch 18/90
84077/84077 - 8s - loss: 2.6604e-04 - val_loss: 2.3618e-04 - 8s/epoch - 99us/sample
Epoch 19/90
84077/84077 - 8s - loss: 2.5523e-04 - val_loss: 2.2603e-04 - 8s/epoch - 99us/sample
Epoch 20/90
84077/84077 - 8s - loss: 2.4262e-04 - val_loss: 2.1892e-04 - 8s/epoch - 100us/sample
Epoch 21/90
84077/84077 - 8s - loss: 2.3412e-04 - val_loss: 2.1270e-04 - 8s/epoch - 99us/sample
Epoch 22/90
84077/84077 - 8s - loss: 2.2634e-04 - val_loss: 2.0581e-04 - 8s/epoch - 99us/sample
Epoch 23/90
84077/84077 - 8s - loss: 2.2497e-04 - val_loss: 2.0295e-04 - 8s/epoch - 99us/sample
Epoch 24/90
84077/84077 - 8s - loss: 2.1563e-04 - val_loss: 1.9793e-04 - 8s/epoch - 99us/sample
Epoch 25/90
84077/84077 - 8s - loss: 2.0986e-04 - val_loss: 1.9735e-04 - 8s/epoch - 100us/sample
Epoch 26/90
84077/84077 - 8s - loss: 2.0681e-04 - val_loss: 1.8918e-04 - 8s/epoch - 99us/sample
Epoch 27/90
84077/84077 - 8s - loss: 2.0114e-04 - val_loss: 1.8689e-04 - 8s/epoch - 99us/sample
Epoch 28/90
84077/84077 - 8s - loss: 1.9723e-04 - val_loss: 1.8469e-04 - 8s/epoch - 99us/sample
Epoch 29/90
84077/84077 - 8s - loss: 1.9430e-04 - val_loss: 1.8355e-04 - 8s/epoch - 98us/sample
Epoch 30/90
84077/84077 - 8s - loss: 1.9141e-04 - val_loss: 1.7963e-04 - 8s/epoch - 100us/sample
Epoch 31/90
84077/84077 - 8s - loss: 1.8973e-04 - val_loss: 1.7802e-04 - 8s/epoch - 99us/sample
Epoch 32/90
84077/84077 - 8s - loss: 1.8629e-04 - val_loss: 1.8241e-04 - 8s/epoch - 99us/sample
Epoch 33/90
84077/84077 - 8s - loss: 1.8423e-04 - val_loss: 1.7466e-04 - 8s/epoch - 99us/sample
Epoch 34/90
84077/84077 - 8s - loss: 1.8099e-04 - val_loss: 1.7118e-04 - 8s/epoch - 99us/sample
Epoch 35/90
84077/84077 - 8s - loss: 1.7782e-04 - val_loss: 1.6929e-04 - 8s/epoch - 99us/sample
Epoch 36/90
84077/84077 - 8s - loss: 1.7509e-04 - val_loss: 1.6472e-04 - 8s/epoch - 99us/sample
Epoch 37/90
84077/84077 - 8s - loss: 1.7384e-04 - val_loss: 1.6284e-04 - 8s/epoch - 99us/sample
Epoch 38/90
84077/84077 - 8s - loss: 1.7257e-04 - val_loss: 1.6733e-04 - 8s/epoch - 99us/sample
Epoch 39/90
84077/84077 - 8s - loss: 1.7056e-04 - val_loss: 1.6280e-04 - 8s/epoch - 99us/sample
Epoch 40/90
84077/84077 - 8s - loss: 1.7023e-04 - val_loss: 1.6479e-04 - 8s/epoch - 99us/sample
Epoch 41/90
84077/84077 - 8s - loss: 1.6711e-04 - val_loss: 1.6007e-04 - 8s/epoch - 99us/sample
Epoch 42/90
84077/84077 - 8s - loss: 1.6747e-04 - val_loss: 1.5946e-04 - 8s/epoch - 99us/sample
Epoch 43/90
84077/84077 - 8s - loss: 1.6557e-04 - val_loss: 1.5674e-04 - 8s/epoch - 99us/sample
Epoch 44/90
84077/84077 - 8s - loss: 1.6498e-04 - val_loss: 1.5874e-04 - 8s/epoch - 99us/sample
Epoch 45/90
84077/84077 - 8s - loss: 1.6293e-04 - val_loss: 1.5462e-04 - 8s/epoch - 99us/sample
Epoch 46/90
84077/84077 - 8s - loss: 1.6066e-04 - val_loss: 1.5395e-04 - 8s/epoch - 100us/sample
Epoch 47/90
84077/84077 - 8s - loss: 1.5995e-04 - val_loss: 1.5130e-04 - 8s/epoch - 99us/sample
Epoch 48/90
84077/84077 - 8s - loss: 1.5891e-04 - val_loss: 1.5355e-04 - 8s/epoch - 99us/sample
Epoch 49/90
84077/84077 - 8s - loss: 1.5830e-04 - val_loss: 1.5475e-04 - 8s/epoch - 99us/sample
Epoch 50/90
84077/84077 - 8s - loss: 1.7274e-04 - val_loss: 1.5309e-04 - 8s/epoch - 99us/sample
Epoch 51/90
84077/84077 - 8s - loss: 1.5734e-04 - val_loss: 1.5057e-04 - 8s/epoch - 99us/sample
Epoch 52/90
84077/84077 - 8s - loss: 1.5878e-04 - val_loss: 1.5115e-04 - 8s/epoch - 99us/sample
Epoch 53/90
84077/84077 - 8s - loss: 1.5557e-04 - val_loss: 1.5237e-04 - 8s/epoch - 99us/sample
Epoch 54/90
84077/84077 - 8s - loss: 1.5577e-04 - val_loss: 1.4847e-04 - 8s/epoch - 99us/sample
Epoch 55/90
84077/84077 - 8s - loss: 1.5378e-04 - val_loss: 1.4850e-04 - 8s/epoch - 99us/sample
Epoch 56/90
84077/84077 - 8s - loss: 1.5930e-04 - val_loss: 1.4970e-04 - 8s/epoch - 99us/sample
Epoch 57/90
84077/84077 - 8s - loss: 1.5323e-04 - val_loss: 1.4746e-04 - 8s/epoch - 99us/sample
Epoch 58/90
84077/84077 - 8s - loss: 1.5209e-04 - val_loss: 1.4540e-04 - 8s/epoch - 99us/sample
Epoch 59/90
84077/84077 - 8s - loss: 1.5084e-04 - val_loss: 1.4728e-04 - 8s/epoch - 99us/sample
Epoch 60/90
84077/84077 - 8s - loss: 1.5089e-04 - val_loss: 1.4511e-04 - 8s/epoch - 99us/sample
Epoch 61/90
84077/84077 - 8s - loss: 1.5439e-04 - val_loss: 1.5639e-04 - 8s/epoch - 100us/sample
Epoch 62/90
84077/84077 - 8s - loss: 1.5199e-04 - val_loss: 1.4419e-04 - 8s/epoch - 99us/sample
Epoch 63/90
84077/84077 - 8s - loss: 1.4923e-04 - val_loss: 1.4469e-04 - 8s/epoch - 99us/sample
Epoch 64/90
84077/84077 - 8s - loss: 1.4810e-04 - val_loss: 1.4525e-04 - 8s/epoch - 99us/sample
Epoch 65/90
84077/84077 - 8s - loss: 1.4851e-04 - val_loss: 1.4496e-04 - 8s/epoch - 99us/sample
Epoch 66/90
84077/84077 - 8s - loss: 1.4751e-04 - val_loss: 1.3980e-04 - 8s/epoch - 100us/sample
Epoch 67/90
84077/84077 - 8s - loss: 1.4697e-04 - val_loss: 1.4249e-04 - 8s/epoch - 99us/sample
Epoch 68/90
84077/84077 - 8s - loss: 1.4656e-04 - val_loss: 1.4019e-04 - 8s/epoch - 99us/sample
Epoch 69/90
84077/84077 - 8s - loss: 1.4717e-04 - val_loss: 1.4348e-04 - 8s/epoch - 99us/sample
Epoch 70/90
84077/84077 - 8s - loss: 1.4580e-04 - val_loss: 1.4099e-04 - 8s/epoch - 99us/sample
Epoch 71/90
84077/84077 - 8s - loss: 1.4442e-04 - val_loss: 1.4125e-04 - 8s/epoch - 99us/sample
Epoch 72/90
84077/84077 - 8s - loss: 1.4412e-04 - val_loss: 1.4459e-04 - 8s/epoch - 99us/sample
Epoch 73/90
84077/84077 - 8s - loss: 1.4347e-04 - val_loss: 1.4284e-04 - 8s/epoch - 99us/sample
Epoch 74/90
84077/84077 - 8s - loss: 1.4411e-04 - val_loss: 1.3829e-04 - 8s/epoch - 99us/sample
Epoch 75/90
84077/84077 - 8s - loss: 1.4717e-04 - val_loss: 1.6413e-04 - 8s/epoch - 100us/sample
Epoch 76/90
84077/84077 - 8s - loss: 1.4601e-04 - val_loss: 1.3919e-04 - 8s/epoch - 99us/sample
Epoch 77/90
84077/84077 - 8s - loss: 1.4161e-04 - val_loss: 1.3988e-04 - 8s/epoch - 99us/sample
Epoch 78/90
84077/84077 - 8s - loss: 1.4190e-04 - val_loss: 1.3668e-04 - 8s/epoch - 99us/sample
Epoch 79/90
84077/84077 - 8s - loss: 1.4362e-04 - val_loss: 1.3804e-04 - 8s/epoch - 99us/sample
Epoch 80/90
84077/84077 - 8s - loss: 1.4104e-04 - val_loss: 1.3979e-04 - 8s/epoch - 100us/sample
Epoch 81/90
84077/84077 - 8s - loss: 1.4122e-04 - val_loss: 1.3907e-04 - 8s/epoch - 99us/sample
Epoch 82/90
84077/84077 - 8s - loss: 1.4025e-04 - val_loss: 1.3818e-04 - 8s/epoch - 99us/sample
Epoch 83/90
84077/84077 - 8s - loss: 1.4422e-04 - val_loss: 1.4168e-04 - 8s/epoch - 99us/sample
Epoch 84/90
84077/84077 - 8s - loss: 1.4080e-04 - val_loss: 1.3798e-04 - 8s/epoch - 99us/sample
Epoch 85/90
84077/84077 - 8s - loss: 1.3879e-04 - val_loss: 1.3685e-04 - 8s/epoch - 100us/sample
Epoch 86/90
84077/84077 - 8s - loss: 1.3911e-04 - val_loss: 1.3596e-04 - 8s/epoch - 99us/sample
Epoch 87/90
84077/84077 - 8s - loss: 1.3756e-04 - val_loss: 1.3378e-04 - 8s/epoch - 99us/sample
Epoch 88/90
84077/84077 - 8s - loss: 1.4139e-04 - val_loss: 1.3747e-04 - 8s/epoch - 99us/sample
Epoch 89/90
84077/84077 - 8s - loss: 1.3835e-04 - val_loss: 1.3567e-04 - 8s/epoch - 99us/sample
Epoch 90/90
84077/84077 - 8s - loss: 1.3741e-04 - val_loss: 1.3715e-04 - 8s/epoch - 99us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.00013715112475427897
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:45:09.148522: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1682 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.016858660953485514
cosine 0.016659061566538978
MAE: 0.0017287809316042534
RMSE: 0.00829206821109538
r2: 0.9465160881396715
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.001, 0.4, 377, 0.00013741018346631362, 0.00013715112475427897, 0.016858660953485514, 0.016659061566538978, 0.0017287809316042534, 0.00829206821109538, 0.9465160881396715, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 70 0.0005 16 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1414)        5656        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1414)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          533455      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          533455      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2018507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,425,889
Trainable params: 4,419,479
Non-trainable params: 6,410
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/70
2023-02-15 04:45:16.301668: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/bottleneck_zmean_2/kernel/v/Assign' id:3649 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/bottleneck_zmean_2/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/bottleneck_zmean_2/kernel/v, training_4/Adam/bottleneck_zmean_2/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:45:38.264975: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2988 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0685 - val_loss: 0.0675 - 23s/epoch - 278us/sample
Epoch 2/70
84077/84077 - 22s - loss: 0.0669 - val_loss: 0.0674 - 22s/epoch - 264us/sample
Epoch 3/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 4/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 5/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 6/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 7/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 8/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 9/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 10/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 11/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 12/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 13/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 14/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 15/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 16/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 17/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 18/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 19/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 20/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 21/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 22/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 23/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 24/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 25/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 26/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 27/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 264us/sample
Epoch 28/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 29/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 30/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 31/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 32/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 33/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 264us/sample
Epoch 34/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 35/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 36/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 37/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 264us/sample
Epoch 38/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 39/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 40/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 41/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 42/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 43/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 44/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 45/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 46/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 47/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 48/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 49/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 50/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 51/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 264us/sample
Epoch 52/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 53/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 54/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 55/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 56/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 57/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 58/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 59/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 264us/sample
Epoch 60/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 61/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 62/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 63/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 64/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 65/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 66/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 264us/sample
Epoch 67/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 267us/sample
Epoch 68/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 265us/sample
Epoch 69/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 266us/sample
Epoch 70/70
84077/84077 - 22s - loss: 0.0668 - val_loss: 0.0673 - 22s/epoch - 264us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.06734572755329564
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:11:18.609865: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2940 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1201774491141427
cosine 1.1436656249188026
MAE: 4.509169573953472
RMSE: 4.813567399102415
r2: -17069.747711264692
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'binary_crossentropy', 16, 70, 0.0005, 0.4, 377, 0.0668369111423403, 0.06734572755329564, 1.1201774491141427, 1.1436656249188026, 4.509169573953472, 4.813567399102415, -17069.747711264692, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 2357)        9428        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 2357)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          888966      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          888966      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          3267982     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,280,350
Trainable params: 7,270,168
Non-trainable params: 10,182
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 05:11:25.726684: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/decay/Assign' id:4783 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/decay, training_6/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:11:34.557555: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4316 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 10s - loss: 0.0040 - val_loss: 0.0012 - 10s/epoch - 117us/sample
Epoch 2/50
84077/84077 - 8s - loss: 0.1889 - val_loss: 8.1348e-04 - 8s/epoch - 100us/sample
Epoch 3/50
84077/84077 - 8s - loss: 7.8943e-04 - val_loss: 6.0044e-04 - 8s/epoch - 100us/sample
Epoch 4/50
84077/84077 - 8s - loss: 0.3684 - val_loss: 0.0013 - 8s/epoch - 100us/sample
Epoch 5/50
84077/84077 - 8s - loss: 7.9586e-04 - val_loss: 7.7859e-04 - 8s/epoch - 100us/sample
Epoch 6/50
84077/84077 - 8s - loss: 4.9357e-04 - val_loss: 3.7939e-04 - 8s/epoch - 100us/sample
Epoch 7/50
84077/84077 - 9s - loss: 3.7705e-04 - val_loss: 3.3659e-04 - 9s/epoch - 101us/sample
Epoch 8/50
84077/84077 - 8s - loss: 3.6751e-04 - val_loss: 3.5010e-04 - 8s/epoch - 100us/sample
Epoch 9/50
84077/84077 - 8s - loss: 3.7658e-04 - val_loss: 2.5637e-04 - 8s/epoch - 100us/sample
Epoch 10/50
84077/84077 - 8s - loss: 2.8678e-04 - val_loss: 2.4404e-04 - 8s/epoch - 101us/sample
Epoch 11/50
84077/84077 - 8s - loss: 2.5663e-04 - val_loss: 2.1850e-04 - 8s/epoch - 100us/sample
Epoch 12/50
84077/84077 - 8s - loss: 2.3636e-04 - val_loss: 1.9624e-04 - 8s/epoch - 100us/sample
Epoch 13/50
84077/84077 - 8s - loss: 2.2035e-04 - val_loss: 1.8510e-04 - 8s/epoch - 101us/sample
Epoch 14/50
84077/84077 - 8s - loss: 2.0696e-04 - val_loss: 2.0088e-04 - 8s/epoch - 101us/sample
Epoch 15/50
84077/84077 - 8s - loss: 1.9528e-04 - val_loss: 1.6601e-04 - 8s/epoch - 101us/sample
Epoch 16/50
84077/84077 - 8s - loss: 1.8259e-04 - val_loss: 1.6570e-04 - 8s/epoch - 100us/sample
Epoch 17/50
84077/84077 - 8s - loss: 1.7504e-04 - val_loss: 1.4916e-04 - 8s/epoch - 100us/sample
Epoch 18/50
84077/84077 - 8s - loss: 1.6311e-04 - val_loss: 1.4658e-04 - 8s/epoch - 100us/sample
Epoch 19/50
84077/84077 - 9s - loss: 1.5713e-04 - val_loss: 1.3503e-04 - 9s/epoch - 101us/sample
Epoch 20/50
84077/84077 - 8s - loss: 1.5107e-04 - val_loss: 1.3615e-04 - 8s/epoch - 101us/sample
Epoch 21/50
84077/84077 - 8s - loss: 1.5683e-04 - val_loss: 1.3214e-04 - 8s/epoch - 101us/sample
Epoch 22/50
84077/84077 - 8s - loss: 1.4141e-04 - val_loss: 1.2573e-04 - 8s/epoch - 100us/sample
Epoch 23/50
84077/84077 - 8s - loss: 1.3721e-04 - val_loss: 1.2076e-04 - 8s/epoch - 100us/sample
Epoch 24/50
84077/84077 - 8s - loss: 1.3331e-04 - val_loss: 1.1876e-04 - 8s/epoch - 100us/sample
Epoch 25/50
84077/84077 - 9s - loss: 1.3020e-04 - val_loss: 1.1613e-04 - 9s/epoch - 102us/sample
Epoch 26/50
84077/84077 - 8s - loss: 1.2782e-04 - val_loss: 1.1292e-04 - 8s/epoch - 101us/sample
Epoch 27/50
84077/84077 - 8s - loss: 1.2358e-04 - val_loss: 1.1146e-04 - 8s/epoch - 101us/sample
Epoch 28/50
84077/84077 - 8s - loss: 1.2147e-04 - val_loss: 1.0843e-04 - 8s/epoch - 100us/sample
Epoch 29/50
84077/84077 - 8s - loss: 1.1875e-04 - val_loss: 1.0747e-04 - 8s/epoch - 101us/sample
Epoch 30/50
84077/84077 - 8s - loss: 1.1755e-04 - val_loss: 1.0722e-04 - 8s/epoch - 100us/sample
Epoch 31/50
84077/84077 - 8s - loss: 1.1577e-04 - val_loss: 1.0509e-04 - 8s/epoch - 101us/sample
Epoch 32/50
84077/84077 - 9s - loss: 1.1370e-04 - val_loss: 1.0399e-04 - 9s/epoch - 101us/sample
Epoch 33/50
84077/84077 - 8s - loss: 1.1221e-04 - val_loss: 1.0297e-04 - 8s/epoch - 101us/sample
Epoch 34/50
84077/84077 - 8s - loss: 1.1151e-04 - val_loss: 1.0313e-04 - 8s/epoch - 100us/sample
Epoch 35/50
84077/84077 - 8s - loss: 1.0987e-04 - val_loss: 1.0178e-04 - 8s/epoch - 101us/sample
Epoch 36/50
84077/84077 - 8s - loss: 1.0895e-04 - val_loss: 9.9601e-05 - 8s/epoch - 100us/sample
Epoch 37/50
84077/84077 - 8s - loss: 1.0798e-04 - val_loss: 9.8267e-05 - 8s/epoch - 101us/sample
Epoch 38/50
84077/84077 - 8s - loss: 1.0758e-04 - val_loss: 9.8097e-05 - 8s/epoch - 101us/sample
Epoch 39/50
84077/84077 - 8s - loss: 1.0594e-04 - val_loss: 9.8070e-05 - 8s/epoch - 101us/sample
Epoch 40/50
84077/84077 - 8s - loss: 1.0591e-04 - val_loss: 9.7219e-05 - 8s/epoch - 101us/sample
Epoch 41/50
84077/84077 - 8s - loss: 1.0452e-04 - val_loss: 9.6167e-05 - 8s/epoch - 101us/sample
Epoch 42/50
84077/84077 - 8s - loss: 1.0433e-04 - val_loss: 9.6780e-05 - 8s/epoch - 100us/sample
Epoch 43/50
84077/84077 - 8s - loss: 1.0305e-04 - val_loss: 9.7744e-05 - 8s/epoch - 100us/sample
Epoch 44/50
84077/84077 - 8s - loss: 1.0326e-04 - val_loss: 9.5737e-05 - 8s/epoch - 101us/sample
Epoch 45/50
84077/84077 - 9s - loss: 1.0194e-04 - val_loss: 9.5361e-05 - 9s/epoch - 101us/sample
Epoch 46/50
84077/84077 - 8s - loss: 1.0522e-04 - val_loss: 9.6938e-05 - 8s/epoch - 101us/sample
Epoch 47/50
84077/84077 - 8s - loss: 1.0482e-04 - val_loss: 9.5227e-05 - 8s/epoch - 100us/sample
Epoch 48/50
84077/84077 - 8s - loss: 1.0144e-04 - val_loss: 9.4038e-05 - 8s/epoch - 100us/sample
Epoch 49/50
84077/84077 - 8s - loss: 1.0080e-04 - val_loss: 9.3444e-05 - 8s/epoch - 100us/sample
Epoch 50/50
84077/84077 - 8s - loss: 9.9962e-05 - val_loss: 9.2709e-05 - 8s/epoch - 100us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 9.27089524130223e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:18:29.558737: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4280 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.024425727702360414
cosine 0.024130702881187672
MAE: 0.002157155441554625
RMSE: 0.00942193862167466
r2: 0.9307865609209779
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 50, 0.0005, 0.4, 377, 9.996208548528252e-05, 9.27089524130223e-05, 0.024425727702360414, 0.024130702881187672, 0.002157155441554625, 0.00942193862167466, 0.9307865609209779, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 110 0.0005 8 0] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 1886)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          711399      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          711399      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2643907     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,854,633
Trainable params: 5,846,335
Non-trainable params: 8,298
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/110
2023-02-15 05:18:37.638107: W tensorflow/c/c_api.cc:291] Operation '{name:'training_8/Adam/bottleneck_zlog_4/kernel/v/Assign' id:6286 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/bottleneck_zlog_4/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/bottleneck_zlog_4/kernel/v, training_8/Adam/bottleneck_zlog_4/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:19:17.590008: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5616 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 43s - loss: 0.0679 - val_loss: 0.0674 - 43s/epoch - 506us/sample
Epoch 2/110
84077/84077 - 41s - loss: 0.0669 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 3/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 4/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 5/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 6/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 7/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 8/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 9/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 10/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 11/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 12/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 13/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 14/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 15/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 16/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 17/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 18/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 19/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 20/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 21/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 22/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 23/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 24/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 25/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 26/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 27/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 28/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 29/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 30/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 31/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 32/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 33/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 34/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 35/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 36/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 37/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 38/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 39/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 40/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 41/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 42/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 43/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 44/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 45/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 46/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 47/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 48/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 49/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 50/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 51/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 52/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 53/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 54/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 55/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 56/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 57/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 58/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 59/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 60/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 61/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 62/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 63/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 64/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 65/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 66/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 67/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 68/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 69/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 70/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 71/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 491us/sample
Epoch 72/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 73/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 74/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 75/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 487us/sample
Epoch 76/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 77/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 78/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 79/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 80/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 81/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 82/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 83/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 490us/sample
Epoch 84/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 85/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 86/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 487us/sample
Epoch 87/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 88/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 89/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 486us/sample
Epoch 90/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 91/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 92/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 93/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 94/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 487us/sample
Epoch 95/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 487us/sample
Epoch 96/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 97/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 98/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 99/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 100/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 101/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 102/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 103/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 486us/sample
Epoch 104/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 105/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 106/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 107/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 488us/sample
Epoch 108/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 487us/sample
Epoch 109/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
Epoch 110/110
84077/84077 - 41s - loss: 0.0668 - val_loss: 0.0673 - 41s/epoch - 489us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.06734573317432883
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:33:57.371008: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:5568 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1156380494282672
cosine 1.1474979151437525
MAE: 5.655979485785838
RMSE: 6.204591491917784
r2: -28213.87683469784
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 8, 110, 0.0005, 0.4, 377, 0.06683690984934196, 0.06734573317432883, 1.1156380494282672, 1.1474979151437525, 5.655979485785838, 6.204591491917784, -28213.87683469784, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 170 0.0005 16 0] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 1886)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          711399      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          711399      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2643907     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,854,633
Trainable params: 5,846,335
Non-trainable params: 8,298
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/170
2023-02-15 06:34:05.385144: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_17/beta/Assign' id:6750 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_17/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_17/beta, batch_normalization_17/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:34:27.854571: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:6953 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0677 - val_loss: 0.0674 - 24s/epoch - 290us/sample
Epoch 2/170
84077/84077 - 23s - loss: 0.0669 - val_loss: 0.0674 - 23s/epoch - 268us/sample
Epoch 3/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 4/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 5/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 6/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 7/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 8/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 9/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 10/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 11/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 268us/sample
Epoch 12/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 13/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 14/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 15/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 16/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 17/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 18/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 19/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 20/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 21/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 22/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 23/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 24/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 25/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 26/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 27/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 28/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 29/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 30/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 31/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 32/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 33/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 34/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 35/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 36/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 37/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 38/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 39/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 40/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 41/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 42/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 43/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 44/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 45/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 46/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 47/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 48/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 49/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 50/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 51/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 52/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 268us/sample
Epoch 53/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 54/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 55/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 56/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 57/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 58/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 59/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 60/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 61/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 62/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 63/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 64/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 65/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 66/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 67/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 68/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 69/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 70/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 71/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 72/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 73/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 74/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 75/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 76/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 77/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 78/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 79/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 268us/sample
Epoch 80/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 81/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 82/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 83/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 84/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 85/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 86/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 87/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 88/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 89/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 268us/sample
Epoch 90/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 91/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 92/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 93/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 94/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 95/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 96/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 97/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 98/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 99/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 100/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 101/170
84077/84077 - 23s - loss: 0.0669 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 102/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 103/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 104/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 105/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 106/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 107/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 108/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 109/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 110/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 111/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 112/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 113/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 114/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 115/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 116/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 117/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 118/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 119/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 120/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 121/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 122/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 123/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 124/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 125/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 126/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 127/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 128/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 129/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 130/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 131/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 132/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 133/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 134/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 135/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 136/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 137/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 138/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 139/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 140/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 141/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 142/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 143/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 277us/sample
Epoch 144/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 145/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 146/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 147/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 148/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 149/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 150/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 151/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 152/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 153/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 154/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 155/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 156/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 157/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 158/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 159/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 160/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 161/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 162/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 163/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 164/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 165/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 166/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 269us/sample
Epoch 167/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 168/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
Epoch 169/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 270us/sample
Epoch 170/170
84077/84077 - 23s - loss: 0.0668 - val_loss: 0.0673 - 23s/epoch - 271us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.06734587511020206
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:38:21.573651: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:6905 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1216529097565535
cosine 1.1434825049616837
MAE: 5.595190333287029
RMSE: 6.084656153557486
r2: -26874.293984146512
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 16, 170, 0.0005, 0.4, 377, 0.06683690972971015, 0.06734587511020206, 1.1216529097565535, 1.1434825049616837, 5.595190333287029, 6.084656153557486, -26874.293984146512, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 25 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 1886)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          711399      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          711399      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2643907     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,854,633
Trainable params: 5,846,335
Non-trainable params: 8,298
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-15 07:38:30.003880: W tensorflow/c/c_api.cc:291] Operation '{name:'training_12/Adam/bottleneck_zmean_6/kernel/v/Assign' id:8866 op device:{requested: '', assigned: ''} def:{{{node training_12/Adam/bottleneck_zmean_6/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_12/Adam/bottleneck_zmean_6/kernel/v, training_12/Adam/bottleneck_zmean_6/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:38:51.991154: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:8271 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0054 - val_loss: 0.0013 - 24s/epoch - 286us/sample
Epoch 2/25
84077/84077 - 22s - loss: 0.0014 - val_loss: 9.0984e-04 - 22s/epoch - 261us/sample
Epoch 3/25
84077/84077 - 22s - loss: 9.4506e-04 - val_loss: 7.5052e-04 - 22s/epoch - 262us/sample
Epoch 4/25
84077/84077 - 22s - loss: 7.0472e-04 - val_loss: 8.1714e-04 - 22s/epoch - 261us/sample
Epoch 5/25
84077/84077 - 22s - loss: 5.8823e-04 - val_loss: 9.8482e-04 - 22s/epoch - 262us/sample
Epoch 6/25
84077/84077 - 22s - loss: 5.2998e-04 - val_loss: 0.0011 - 22s/epoch - 261us/sample
Epoch 7/25
84077/84077 - 22s - loss: 4.9851e-04 - val_loss: 0.0013 - 22s/epoch - 262us/sample
Epoch 8/25
84077/84077 - 22s - loss: 4.7713e-04 - val_loss: 0.0013 - 22s/epoch - 261us/sample
Epoch 9/25
84077/84077 - 22s - loss: 4.5832e-04 - val_loss: 0.0012 - 22s/epoch - 261us/sample
Epoch 10/25
84077/84077 - 22s - loss: 4.4269e-04 - val_loss: 0.0013 - 22s/epoch - 262us/sample
Epoch 11/25
84077/84077 - 22s - loss: 4.3311e-04 - val_loss: 0.0012 - 22s/epoch - 261us/sample
Epoch 12/25
84077/84077 - 22s - loss: 4.2579e-04 - val_loss: 0.0013 - 22s/epoch - 262us/sample
Epoch 13/25
84077/84077 - 22s - loss: 4.1785e-04 - val_loss: 0.0012 - 22s/epoch - 261us/sample
Epoch 14/25
84077/84077 - 22s - loss: 4.1358e-04 - val_loss: 0.0013 - 22s/epoch - 263us/sample
Epoch 15/25
84077/84077 - 22s - loss: 4.0750e-04 - val_loss: 0.0011 - 22s/epoch - 261us/sample
Epoch 16/25
84077/84077 - 22s - loss: 4.0348e-04 - val_loss: 0.0011 - 22s/epoch - 263us/sample
Epoch 17/25
84077/84077 - 22s - loss: 4.0019e-04 - val_loss: 0.0012 - 22s/epoch - 262us/sample
Epoch 18/25
84077/84077 - 22s - loss: 3.9467e-04 - val_loss: 0.0011 - 22s/epoch - 262us/sample
Epoch 19/25
84077/84077 - 22s - loss: 3.9167e-04 - val_loss: 0.0010 - 22s/epoch - 262us/sample
Epoch 20/25
84077/84077 - 22s - loss: 3.8801e-04 - val_loss: 9.7312e-04 - 22s/epoch - 261us/sample
Epoch 21/25
84077/84077 - 22s - loss: 3.8636e-04 - val_loss: 9.9282e-04 - 22s/epoch - 262us/sample
Epoch 22/25
84077/84077 - 22s - loss: 3.8098e-04 - val_loss: 9.3691e-04 - 22s/epoch - 261us/sample
Epoch 23/25
84077/84077 - 22s - loss: 3.7747e-04 - val_loss: 9.8281e-04 - 22s/epoch - 263us/sample
Epoch 24/25
84077/84077 - 22s - loss: 3.7576e-04 - val_loss: 9.8867e-04 - 22s/epoch - 261us/sample
Epoch 25/25
84077/84077 - 22s - loss: 3.7447e-04 - val_loss: 8.7084e-04 - 22s/epoch - 261us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.0008708428164135152
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:47:41.516594: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:8242 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0783221012418274
cosine 0.07737021599216984
MAE: 0.004676401153987765
RMSE: 0.028666965964491545
r2: 0.3581206261348537
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 16, 25, 0.001, 0.4, 377, 0.00037446683473047284, 0.0008708428164135152, 0.0783221012418274, 0.07737021599216984, 0.004676401153987765, 0.028666965964491545, 0.3581206261348537, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 1414)         0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          533455      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          533455      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2018507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,425,889
Trainable params: 4,419,479
Non-trainable params: 6,410
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 07:47:50.131291: W tensorflow/c/c_api.cc:291] Operation '{name:'training_14/Adam/batch_normalization_21/gamma/v/Assign' id:10107 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/batch_normalization_21/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/batch_normalization_21/gamma/v, training_14/Adam/batch_normalization_21/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:48:03.826063: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:9526 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0055 - val_loss: 0.0018 - 15s/epoch - 184us/sample
Epoch 2/90
84077/84077 - 13s - loss: 0.0017 - val_loss: 0.0030 - 13s/epoch - 158us/sample
Epoch 3/90
84077/84077 - 13s - loss: 9.8034e-04 - val_loss: 6.8048e-04 - 13s/epoch - 158us/sample
Epoch 4/90
84077/84077 - 13s - loss: 7.0463e-04 - val_loss: 0.0022 - 13s/epoch - 159us/sample
Epoch 5/90
84077/84077 - 13s - loss: 5.5623e-04 - val_loss: 4.3014e-04 - 13s/epoch - 158us/sample
Epoch 6/90
84077/84077 - 13s - loss: 4.3865e-04 - val_loss: 3.5457e-04 - 13s/epoch - 158us/sample
Epoch 7/90
84077/84077 - 13s - loss: 3.6623e-04 - val_loss: 3.2173e-04 - 13s/epoch - 158us/sample
Epoch 8/90
84077/84077 - 13s - loss: 3.2277e-04 - val_loss: 2.9592e-04 - 13s/epoch - 159us/sample
Epoch 9/90
84077/84077 - 13s - loss: 2.9842e-04 - val_loss: 2.7515e-04 - 13s/epoch - 158us/sample
Epoch 10/90
84077/84077 - 13s - loss: 2.7539e-04 - val_loss: 2.6122e-04 - 13s/epoch - 158us/sample
Epoch 11/90
84077/84077 - 13s - loss: 2.6012e-04 - val_loss: 2.4563e-04 - 13s/epoch - 160us/sample
Epoch 12/90
84077/84077 - 13s - loss: 2.4619e-04 - val_loss: 2.4626e-04 - 13s/epoch - 159us/sample
Epoch 13/90
84077/84077 - 13s - loss: 2.3697e-04 - val_loss: 2.2750e-04 - 13s/epoch - 158us/sample
Epoch 14/90
84077/84077 - 13s - loss: 2.2887e-04 - val_loss: 2.2319e-04 - 13s/epoch - 159us/sample
Epoch 15/90
84077/84077 - 13s - loss: 2.2131e-04 - val_loss: 2.1916e-04 - 13s/epoch - 159us/sample
Epoch 16/90
84077/84077 - 13s - loss: 2.1650e-04 - val_loss: 2.1185e-04 - 13s/epoch - 158us/sample
Epoch 17/90
84077/84077 - 13s - loss: 2.1065e-04 - val_loss: 2.1384e-04 - 13s/epoch - 159us/sample
Epoch 18/90
84077/84077 - 13s - loss: 2.0594e-04 - val_loss: 2.1246e-04 - 13s/epoch - 160us/sample
Epoch 19/90
84077/84077 - 13s - loss: 2.0234e-04 - val_loss: 2.0390e-04 - 13s/epoch - 158us/sample
Epoch 20/90
84077/84077 - 13s - loss: 1.9903e-04 - val_loss: 2.0167e-04 - 13s/epoch - 158us/sample
Epoch 21/90
84077/84077 - 13s - loss: 1.9608e-04 - val_loss: 2.0154e-04 - 13s/epoch - 159us/sample
Epoch 22/90
84077/84077 - 13s - loss: 1.9302e-04 - val_loss: 1.9367e-04 - 13s/epoch - 158us/sample
Epoch 23/90
84077/84077 - 13s - loss: 1.9104e-04 - val_loss: 1.9036e-04 - 13s/epoch - 159us/sample
Epoch 24/90
84077/84077 - 13s - loss: 1.8754e-04 - val_loss: 1.8877e-04 - 13s/epoch - 159us/sample
Epoch 25/90
84077/84077 - 13s - loss: 1.8627e-04 - val_loss: 1.9069e-04 - 13s/epoch - 158us/sample
Epoch 26/90
84077/84077 - 13s - loss: 1.8375e-04 - val_loss: 1.8967e-04 - 13s/epoch - 158us/sample
Epoch 27/90
84077/84077 - 13s - loss: 1.8136e-04 - val_loss: 1.8277e-04 - 13s/epoch - 159us/sample
Epoch 28/90
84077/84077 - 13s - loss: 1.8299e-04 - val_loss: 1.8925e-04 - 13s/epoch - 158us/sample
Epoch 29/90
84077/84077 - 13s - loss: 1.7863e-04 - val_loss: 1.7930e-04 - 13s/epoch - 158us/sample
Epoch 30/90
84077/84077 - 13s - loss: 1.7860e-04 - val_loss: 1.7901e-04 - 13s/epoch - 159us/sample
Epoch 31/90
84077/84077 - 13s - loss: 1.7560e-04 - val_loss: 1.7982e-04 - 13s/epoch - 158us/sample
Epoch 32/90
84077/84077 - 13s - loss: 1.7440e-04 - val_loss: 1.7819e-04 - 13s/epoch - 158us/sample
Epoch 33/90
84077/84077 - 13s - loss: 1.7263e-04 - val_loss: 1.7933e-04 - 13s/epoch - 158us/sample
Epoch 34/90
84077/84077 - 13s - loss: 1.7266e-04 - val_loss: 1.7584e-04 - 13s/epoch - 159us/sample
Epoch 35/90
84077/84077 - 13s - loss: 1.7024e-04 - val_loss: 1.7602e-04 - 13s/epoch - 158us/sample
Epoch 36/90
84077/84077 - 13s - loss: 1.6955e-04 - val_loss: 1.7769e-04 - 13s/epoch - 158us/sample
Epoch 37/90
84077/84077 - 13s - loss: 1.6799e-04 - val_loss: 1.7353e-04 - 13s/epoch - 160us/sample
Epoch 38/90
84077/84077 - 13s - loss: 1.6704e-04 - val_loss: 1.7216e-04 - 13s/epoch - 158us/sample
Epoch 39/90
84077/84077 - 13s - loss: 1.6652e-04 - val_loss: 1.7123e-04 - 13s/epoch - 158us/sample
Epoch 40/90
84077/84077 - 13s - loss: 1.6617e-04 - val_loss: 1.6985e-04 - 13s/epoch - 160us/sample
Epoch 41/90
84077/84077 - 13s - loss: 1.6483e-04 - val_loss: 1.7297e-04 - 13s/epoch - 159us/sample
Epoch 42/90
84077/84077 - 13s - loss: 1.6346e-04 - val_loss: 1.7234e-04 - 13s/epoch - 158us/sample
Epoch 43/90
84077/84077 - 13s - loss: 1.6362e-04 - val_loss: 1.6996e-04 - 13s/epoch - 160us/sample
Epoch 44/90
84077/84077 - 13s - loss: 1.6336e-04 - val_loss: 1.7005e-04 - 13s/epoch - 159us/sample
Epoch 45/90
84077/84077 - 13s - loss: 1.6131e-04 - val_loss: 1.6292e-04 - 13s/epoch - 158us/sample
Epoch 46/90
84077/84077 - 13s - loss: 1.6103e-04 - val_loss: 1.6787e-04 - 13s/epoch - 160us/sample
Epoch 47/90
84077/84077 - 13s - loss: 1.6000e-04 - val_loss: 1.6367e-04 - 13s/epoch - 159us/sample
Epoch 48/90
84077/84077 - 13s - loss: 1.5988e-04 - val_loss: 1.6538e-04 - 13s/epoch - 158us/sample
Epoch 49/90
84077/84077 - 13s - loss: 1.5904e-04 - val_loss: 1.6722e-04 - 13s/epoch - 159us/sample
Epoch 50/90
84077/84077 - 13s - loss: 1.5826e-04 - val_loss: 1.6838e-04 - 13s/epoch - 159us/sample
Epoch 51/90
84077/84077 - 13s - loss: 1.5792e-04 - val_loss: 1.6222e-04 - 13s/epoch - 158us/sample
Epoch 52/90
84077/84077 - 13s - loss: 1.5743e-04 - val_loss: 1.7158e-04 - 13s/epoch - 160us/sample
Epoch 53/90
84077/84077 - 13s - loss: 1.5784e-04 - val_loss: 1.6624e-04 - 13s/epoch - 158us/sample
Epoch 54/90
84077/84077 - 13s - loss: 1.5655e-04 - val_loss: 1.6007e-04 - 13s/epoch - 158us/sample
Epoch 55/90
84077/84077 - 13s - loss: 1.5524e-04 - val_loss: 1.6190e-04 - 13s/epoch - 160us/sample
Epoch 56/90
84077/84077 - 13s - loss: 1.5529e-04 - val_loss: 1.6441e-04 - 13s/epoch - 158us/sample
Epoch 57/90
84077/84077 - 13s - loss: 1.5529e-04 - val_loss: 1.6180e-04 - 13s/epoch - 158us/sample
Epoch 58/90
84077/84077 - 13s - loss: 1.5364e-04 - val_loss: 1.5868e-04 - 13s/epoch - 159us/sample
Epoch 59/90
84077/84077 - 13s - loss: 1.5384e-04 - val_loss: 1.6189e-04 - 13s/epoch - 158us/sample
Epoch 60/90
84077/84077 - 13s - loss: 1.5398e-04 - val_loss: 1.6044e-04 - 13s/epoch - 158us/sample
Epoch 61/90
84077/84077 - 13s - loss: 1.5236e-04 - val_loss: 1.5790e-04 - 13s/epoch - 158us/sample
Epoch 62/90
84077/84077 - 13s - loss: 1.5235e-04 - val_loss: 1.6295e-04 - 13s/epoch - 160us/sample
Epoch 63/90
84077/84077 - 13s - loss: 1.5195e-04 - val_loss: 1.5700e-04 - 13s/epoch - 158us/sample
Epoch 64/90
84077/84077 - 13s - loss: 1.5237e-04 - val_loss: 1.5767e-04 - 13s/epoch - 158us/sample
Epoch 65/90
84077/84077 - 13s - loss: 1.5336e-04 - val_loss: 1.5610e-04 - 13s/epoch - 159us/sample
Epoch 66/90
84077/84077 - 13s - loss: 1.5143e-04 - val_loss: 1.5509e-04 - 13s/epoch - 158us/sample
Epoch 67/90
84077/84077 - 13s - loss: 1.5105e-04 - val_loss: 1.5902e-04 - 13s/epoch - 158us/sample
Epoch 68/90
84077/84077 - 13s - loss: 1.5010e-04 - val_loss: 1.5606e-04 - 13s/epoch - 158us/sample
Epoch 69/90
84077/84077 - 13s - loss: 1.4978e-04 - val_loss: 1.5306e-04 - 13s/epoch - 159us/sample
Epoch 70/90
84077/84077 - 13s - loss: 1.5007e-04 - val_loss: 1.5531e-04 - 13s/epoch - 159us/sample
Epoch 71/90
84077/84077 - 13s - loss: 1.4966e-04 - val_loss: 1.5921e-04 - 13s/epoch - 158us/sample
Epoch 72/90
84077/84077 - 13s - loss: 1.4902e-04 - val_loss: 1.5550e-04 - 13s/epoch - 159us/sample
Epoch 73/90
84077/84077 - 13s - loss: 1.4870e-04 - val_loss: 1.5460e-04 - 13s/epoch - 159us/sample
Epoch 74/90
84077/84077 - 13s - loss: 1.4852e-04 - val_loss: 1.5752e-04 - 13s/epoch - 158us/sample
Epoch 75/90
84077/84077 - 13s - loss: 1.4831e-04 - val_loss: 1.5715e-04 - 13s/epoch - 158us/sample
Epoch 76/90
84077/84077 - 13s - loss: 1.4793e-04 - val_loss: 1.5761e-04 - 13s/epoch - 159us/sample
Epoch 77/90
84077/84077 - 13s - loss: 1.4766e-04 - val_loss: 1.5394e-04 - 13s/epoch - 159us/sample
Epoch 78/90
84077/84077 - 13s - loss: 1.4799e-04 - val_loss: 1.5224e-04 - 13s/epoch - 158us/sample
Epoch 79/90
84077/84077 - 13s - loss: 1.4731e-04 - val_loss: 1.5469e-04 - 13s/epoch - 159us/sample
Epoch 80/90
84077/84077 - 13s - loss: 1.4647e-04 - val_loss: 1.5205e-04 - 13s/epoch - 159us/sample
Epoch 81/90
84077/84077 - 13s - loss: 1.4683e-04 - val_loss: 1.5074e-04 - 13s/epoch - 158us/sample
Epoch 82/90
84077/84077 - 13s - loss: 1.4633e-04 - val_loss: 1.5335e-04 - 13s/epoch - 158us/sample
Epoch 83/90
84077/84077 - 13s - loss: 1.4577e-04 - val_loss: 1.5284e-04 - 13s/epoch - 158us/sample
Epoch 84/90
84077/84077 - 13s - loss: 1.4538e-04 - val_loss: 1.5376e-04 - 13s/epoch - 160us/sample
Epoch 85/90
84077/84077 - 13s - loss: 1.4582e-04 - val_loss: 1.5194e-04 - 13s/epoch - 159us/sample
Epoch 86/90
84077/84077 - 13s - loss: 1.4550e-04 - val_loss: 1.5251e-04 - 13s/epoch - 158us/sample
Epoch 87/90
84077/84077 - 13s - loss: 1.4513e-04 - val_loss: 1.4839e-04 - 13s/epoch - 158us/sample
Epoch 88/90
84077/84077 - 13s - loss: 1.4476e-04 - val_loss: 1.5001e-04 - 13s/epoch - 159us/sample
Epoch 89/90
84077/84077 - 13s - loss: 1.4512e-04 - val_loss: 1.4929e-04 - 13s/epoch - 159us/sample
Epoch 90/90
84077/84077 - 13s - loss: 1.4468e-04 - val_loss: 1.5636e-04 - 13s/epoch - 158us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.00015636468522172743
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:07:51.743851: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:9497 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.016718588177580585
cosine 0.016516592955370755
MAE: 0.0018261327316026477
RMSE: 0.009160795299946118
r2: 0.9346552546252945
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 32, 90, 0.001, 0.4, 377, 0.00014468075168158658, 0.00015636468522172743, 0.016718588177580585, 0.016516592955370755, 0.0018261327316026477, 0.009160795299946118, 0.9346552546252945, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 25 0.001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 471)          444624      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 471)         1884        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 471)          0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          177944      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          177944      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          769032      ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,571,428
Trainable params: 1,568,790
Non-trainable params: 2,638
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-15 08:08:00.557826: W tensorflow/c/c_api.cc:291] Operation '{name:'training_16/Adam/dense_dec0_8/kernel/v/Assign' id:11469 op device:{requested: '', assigned: ''} def:{{{node training_16/Adam/dense_dec0_8/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_16/Adam/dense_dec0_8/kernel/v, training_16/Adam/dense_dec0_8/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:08:10.239785: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/mul' id:10780 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 11s - loss: 0.0601 - val_loss: 0.1084 - 11s/epoch - 136us/sample
Epoch 2/25
84077/84077 - 9s - loss: 0.0542 - val_loss: 0.0549 - 9s/epoch - 107us/sample
Epoch 3/25
84077/84077 - 9s - loss: 0.0520 - val_loss: 0.0519 - 9s/epoch - 107us/sample
Epoch 4/25
84077/84077 - 9s - loss: 0.0530 - val_loss: 0.0523 - 9s/epoch - 107us/sample
Epoch 5/25
84077/84077 - 9s - loss: 0.0541 - val_loss: 0.0571 - 9s/epoch - 107us/sample
Epoch 6/25
84077/84077 - 9s - loss: 0.0497 - val_loss: 0.0489 - 9s/epoch - 107us/sample
Epoch 7/25
84077/84077 - 9s - loss: 0.0483 - val_loss: 0.0482 - 9s/epoch - 108us/sample
Epoch 8/25
84077/84077 - 9s - loss: 0.0500 - val_loss: 0.0499 - 9s/epoch - 108us/sample
Epoch 9/25
84077/84077 - 9s - loss: 0.0519 - val_loss: 0.0525 - 9s/epoch - 107us/sample
Epoch 10/25
84077/84077 - 9s - loss: 0.0493 - val_loss: 0.0471 - 9s/epoch - 107us/sample
Epoch 11/25
84077/84077 - 9s - loss: 0.0550 - val_loss: 0.0517 - 9s/epoch - 108us/sample
Epoch 12/25
84077/84077 - 9s - loss: 0.0495 - val_loss: 0.0500 - 9s/epoch - 108us/sample
Epoch 13/25
84077/84077 - 9s - loss: 0.0495 - val_loss: 0.0506 - 9s/epoch - 107us/sample
Epoch 14/25
84077/84077 - 9s - loss: 0.0485 - val_loss: 0.0462 - 9s/epoch - 107us/sample
Epoch 15/25
84077/84077 - 9s - loss: 0.0462 - val_loss: 0.0462 - 9s/epoch - 108us/sample
Epoch 16/25
84077/84077 - 9s - loss: 0.0455 - val_loss: 0.0451 - 9s/epoch - 107us/sample
Epoch 17/25
84077/84077 - 9s - loss: 0.0462 - val_loss: 0.0464 - 9s/epoch - 107us/sample
Epoch 18/25
84077/84077 - 9s - loss: 0.0462 - val_loss: 0.0452 - 9s/epoch - 107us/sample
Epoch 19/25
84077/84077 - 9s - loss: 0.0448 - val_loss: 0.0441 - 9s/epoch - 107us/sample
Epoch 20/25
84077/84077 - 9s - loss: 0.0442 - val_loss: 0.0443 - 9s/epoch - 107us/sample
Epoch 21/25
84077/84077 - 9s - loss: 0.0442 - val_loss: 0.0440 - 9s/epoch - 108us/sample
Epoch 22/25
84077/84077 - 9s - loss: 0.0439 - val_loss: 0.0430 - 9s/epoch - 107us/sample
Epoch 23/25
84077/84077 - 9s - loss: 0.0439 - val_loss: 0.0454 - 9s/epoch - 107us/sample
Epoch 24/25
84077/84077 - 9s - loss: 0.0436 - val_loss: 0.0434 - 9s/epoch - 107us/sample
Epoch 25/25
84077/84077 - 9s - loss: 0.0433 - val_loss: 0.0430 - 9s/epoch - 108us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.04299700585716518
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:11:47.920854: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_8/outputlayer/BiasAdd' id:10732 op device:{requested: '', assigned: ''} def:{{{node decoder_model_8/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_8/outputlayer/MatMul, decoder_model_8/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.6387358721808226
cosine 0.9998271517429521
MAE: 0.6211002393331968
RMSE: 2.5102038724860356
r2: -4666.848842377515
RMSE zero-vector: 0.04004287452915337
['0.5custom_VAE', 'binary_crossentropy', 64, 25, 0.001, 0.4, 377, 0.04325846846838055, 0.04299700585716518, 0.6387358721808226, 0.9998271517429521, 0.6211002393331968, 2.5102038724860356, -4666.848842377515, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 1414)         0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          533455      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          533455      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2018507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,425,889
Trainable params: 4,419,479
Non-trainable params: 6,410
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/30
2023-02-15 08:11:56.618874: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/outputlayer_9/bias/m/Assign' id:12637 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/outputlayer_9/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/outputlayer_9/bias/m, training_18/Adam/outputlayer_9/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:12:37.671611: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_9/mul' id:12077 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 44s - loss: 0.0037 - val_loss: 0.0012 - 44s/epoch - 529us/sample
Epoch 2/30
84077/84077 - 42s - loss: 8.9986e-04 - val_loss: 0.0055 - 42s/epoch - 494us/sample
Epoch 3/30
84077/84077 - 42s - loss: 7.5965e-04 - val_loss: 0.0234 - 42s/epoch - 497us/sample
Epoch 4/30
84077/84077 - 42s - loss: 7.1019e-04 - val_loss: 0.0369 - 42s/epoch - 497us/sample
Epoch 5/30
84077/84077 - 42s - loss: 6.8226e-04 - val_loss: 0.0413 - 42s/epoch - 497us/sample
Epoch 6/30
84077/84077 - 42s - loss: 6.5879e-04 - val_loss: 0.0567 - 42s/epoch - 496us/sample
Epoch 7/30
84077/84077 - 42s - loss: 6.4282e-04 - val_loss: 0.0769 - 42s/epoch - 497us/sample
Epoch 8/30
84077/84077 - 42s - loss: 6.3120e-04 - val_loss: 0.0940 - 42s/epoch - 497us/sample
Epoch 9/30
84077/84077 - 42s - loss: 6.2019e-04 - val_loss: 0.1131 - 42s/epoch - 496us/sample
Epoch 10/30
84077/84077 - 42s - loss: 6.0785e-04 - val_loss: 0.1099 - 42s/epoch - 496us/sample
Epoch 11/30
84077/84077 - 42s - loss: 5.9788e-04 - val_loss: 0.1232 - 42s/epoch - 496us/sample
Epoch 12/30
84077/84077 - 42s - loss: 5.9111e-04 - val_loss: 0.0975 - 42s/epoch - 496us/sample
Epoch 13/30
84077/84077 - 42s - loss: 5.8343e-04 - val_loss: 0.1500 - 42s/epoch - 496us/sample
Epoch 14/30
84077/84077 - 42s - loss: 5.7981e-04 - val_loss: 0.1345 - 42s/epoch - 497us/sample
Epoch 15/30
84077/84077 - 42s - loss: 5.7232e-04 - val_loss: 0.1281 - 42s/epoch - 496us/sample
Epoch 16/30
84077/84077 - 42s - loss: 5.6797e-04 - val_loss: 0.1860 - 42s/epoch - 497us/sample
Epoch 17/30
84077/84077 - 42s - loss: 5.6357e-04 - val_loss: 0.1560 - 42s/epoch - 497us/sample
Epoch 18/30
84077/84077 - 42s - loss: 5.5939e-04 - val_loss: 0.1774 - 42s/epoch - 497us/sample
Epoch 19/30
84077/84077 - 42s - loss: 5.5550e-04 - val_loss: 0.1848 - 42s/epoch - 497us/sample
Epoch 20/30
84077/84077 - 42s - loss: 5.5177e-04 - val_loss: 0.2430 - 42s/epoch - 496us/sample
Epoch 21/30
84077/84077 - 42s - loss: 5.4780e-04 - val_loss: 0.2286 - 42s/epoch - 497us/sample
Epoch 22/30
84077/84077 - 42s - loss: 5.4566e-04 - val_loss: 0.2101 - 42s/epoch - 497us/sample
Epoch 23/30
84077/84077 - 42s - loss: 5.4502e-04 - val_loss: 0.2187 - 42s/epoch - 497us/sample
Epoch 24/30
84077/84077 - 42s - loss: 5.4196e-04 - val_loss: 0.2411 - 42s/epoch - 496us/sample
Epoch 25/30
84077/84077 - 42s - loss: 5.4062e-04 - val_loss: 0.3214 - 42s/epoch - 496us/sample
Epoch 26/30
84077/84077 - 42s - loss: 5.3755e-04 - val_loss: 0.2792 - 42s/epoch - 497us/sample
Epoch 27/30
84077/84077 - 42s - loss: 5.3489e-04 - val_loss: 0.3312 - 42s/epoch - 496us/sample
Epoch 28/30
84077/84077 - 42s - loss: 5.3389e-04 - val_loss: 0.2788 - 42s/epoch - 496us/sample
Epoch 29/30
84077/84077 - 42s - loss: 5.3165e-04 - val_loss: 0.3135 - 42s/epoch - 497us/sample
Epoch 30/30
84077/84077 - 42s - loss: 5.3139e-04 - val_loss: 0.2728 - 42s/epoch - 496us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.27278007103607843
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:32:50.905030: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:12048 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.16652027067652442
cosine 0.1650043233476037
MAE: 0.04499621648756696
RMSE: 0.5232319982041492
r2: -212.86811108697606
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 8, 30, 0.0005, 0.4, 377, 0.0005313923163152848, 0.27278007103607843, 0.16652027067652442, 0.1650043233476037, 0.04499621648756696, 0.5232319982041492, -212.86811108697606, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.0005 64 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 1414)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          533455      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          533455      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2018507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,425,889
Trainable params: 4,419,479
Non-trainable params: 6,410
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 08:33:00.464035: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/beta_2/Assign' id:13850 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/beta_2, training_20/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:33:10.745872: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:13351 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0708 - val_loss: 0.0725 - 12s/epoch - 147us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0681 - val_loss: 0.0684 - 9s/epoch - 109us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0678 - val_loss: 0.0680 - 9s/epoch - 108us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0671 - val_loss: 0.0675 - 9s/epoch - 108us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 13/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 18/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 19/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 21/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 22/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 23/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 24/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 25/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 26/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 27/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 29/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 30/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 31/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 32/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 33/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 35/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 36/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 37/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 40/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 41/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 42/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 43/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 45/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 46/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 47/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 53/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 54/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 58/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 59/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 60/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 64/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 65/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 69/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 70/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 71/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 72/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 77/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 78/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 83/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 84/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 85/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 86/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 87/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 89/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.06734573582533825
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:46:47.291477: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:13303 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2258162487852544
cosine 1.1718144899239373
MAE: 4.430151315837426
RMSE: 4.77575910130437
r2: -16792.459901552014
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'binary_crossentropy', 64, 90, 0.0005, 0.4, 377, 0.06683692327610755, 0.06734573582533825, 1.2258162487852544, 1.1718144899239373, 4.430151315837426, 4.77575910130437, -16792.459901552014, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 45 0.0005 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 2357)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          888966      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          888966      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          3267982     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,280,350
Trainable params: 7,270,168
Non-trainable params: 10,182
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/45
2023-02-15 08:46:57.105992: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/bottleneck_zlog_11/bias/v/Assign' id:15281 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/bottleneck_zlog_11/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/bottleneck_zlog_11/bias/v, training_22/Adam/bottleneck_zlog_11/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:47:07.381378: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:14669 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 13s - loss: 0.1203 - val_loss: 0.0015 - 13s/epoch - 149us/sample
Epoch 2/45
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 108us/sample
Epoch 3/45
84077/84077 - 9s - loss: 0.0422 - val_loss: 0.0030 - 9s/epoch - 108us/sample
Epoch 4/45
84077/84077 - 9s - loss: 0.0013 - val_loss: 8.6453e-04 - 9s/epoch - 108us/sample
Epoch 5/45
84077/84077 - 9s - loss: 31.3312 - val_loss: 0.0072 - 9s/epoch - 108us/sample
Epoch 6/45
84077/84077 - 9s - loss: 0.0033 - val_loss: 0.0022 - 9s/epoch - 108us/sample
Epoch 7/45
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0024 - 9s/epoch - 109us/sample
Epoch 8/45
84077/84077 - 9s - loss: 0.0269 - val_loss: 0.0098 - 9s/epoch - 108us/sample
Epoch 9/45
84077/84077 - 9s - loss: 0.0027 - val_loss: 0.0015 - 9s/epoch - 108us/sample
Epoch 10/45
84077/84077 - 9s - loss: 0.0033 - val_loss: 0.0029 - 9s/epoch - 108us/sample
Epoch 11/45
84077/84077 - 9s - loss: 0.0111 - val_loss: 0.0016 - 9s/epoch - 108us/sample
Epoch 12/45
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0012 - 9s/epoch - 108us/sample
Epoch 13/45
84077/84077 - 9s - loss: 0.0013 - val_loss: 0.0011 - 9s/epoch - 109us/sample
Epoch 14/45
84077/84077 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 108us/sample
Epoch 15/45
84077/84077 - 9s - loss: 0.0011 - val_loss: 9.7539e-04 - 9s/epoch - 108us/sample
Epoch 16/45
84077/84077 - 9s - loss: 0.0010 - val_loss: 0.0015 - 9s/epoch - 108us/sample
Epoch 17/45
84077/84077 - 9s - loss: 9.6521e-04 - val_loss: 9.0143e-04 - 9s/epoch - 109us/sample
Epoch 18/45
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0015 - 9s/epoch - 108us/sample
Epoch 19/45
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0039 - 9s/epoch - 108us/sample
Epoch 20/45
84077/84077 - 9s - loss: 1.3963 - val_loss: 0.0015 - 9s/epoch - 108us/sample
Epoch 21/45
84077/84077 - 9s - loss: 0.0011 - val_loss: 9.0394e-04 - 9s/epoch - 108us/sample
Epoch 22/45
84077/84077 - 9s - loss: 9.1793e-04 - val_loss: 8.9150e-04 - 9s/epoch - 108us/sample
Epoch 23/45
84077/84077 - 9s - loss: 8.4976e-04 - val_loss: 7.9241e-04 - 9s/epoch - 109us/sample
Epoch 24/45
84077/84077 - 9s - loss: 8.1709e-04 - val_loss: 8.0547e-04 - 9s/epoch - 108us/sample
Epoch 25/45
84077/84077 - 9s - loss: 8.1555e-04 - val_loss: 8.1662e-04 - 9s/epoch - 108us/sample
Epoch 26/45
84077/84077 - 9s - loss: 7.8049e-04 - val_loss: 7.4763e-04 - 9s/epoch - 108us/sample
Epoch 27/45
84077/84077 - 9s - loss: 7.6193e-04 - val_loss: 7.5616e-04 - 9s/epoch - 108us/sample
Epoch 28/45
84077/84077 - 9s - loss: 7.4925e-04 - val_loss: 7.2170e-04 - 9s/epoch - 110us/sample
Epoch 29/45
84077/84077 - 9s - loss: 7.6834e-04 - val_loss: 7.1610e-04 - 9s/epoch - 108us/sample
Epoch 30/45
84077/84077 - 9s - loss: 0.0013 - val_loss: 7.4304e-04 - 9s/epoch - 108us/sample
Epoch 31/45
84077/84077 - 9s - loss: 0.0021 - val_loss: 8.0156e-04 - 9s/epoch - 108us/sample
Epoch 32/45
84077/84077 - 9s - loss: 7.9443e-04 - val_loss: 7.2452e-04 - 9s/epoch - 108us/sample
Epoch 33/45
84077/84077 - 9s - loss: 7.4179e-04 - val_loss: 6.9341e-04 - 9s/epoch - 109us/sample
Epoch 34/45
84077/84077 - 9s - loss: 7.1799e-04 - val_loss: 6.8019e-04 - 9s/epoch - 108us/sample
Epoch 35/45
84077/84077 - 9s - loss: 7.1561e-04 - val_loss: 6.6874e-04 - 9s/epoch - 108us/sample
Epoch 36/45
84077/84077 - 9s - loss: 7.0205e-04 - val_loss: 6.5917e-04 - 9s/epoch - 108us/sample
Epoch 37/45
84077/84077 - 9s - loss: 6.7185e-04 - val_loss: 6.4967e-04 - 9s/epoch - 108us/sample
Epoch 38/45
84077/84077 - 9s - loss: 6.7976e-04 - val_loss: 6.2990e-04 - 9s/epoch - 109us/sample
Epoch 39/45
84077/84077 - 9s - loss: 6.4559e-04 - val_loss: 6.0691e-04 - 9s/epoch - 108us/sample
Epoch 40/45
84077/84077 - 9s - loss: 6.3093e-04 - val_loss: 6.0162e-04 - 9s/epoch - 108us/sample
Epoch 41/45
84077/84077 - 9s - loss: 6.2148e-04 - val_loss: 5.9731e-04 - 9s/epoch - 107us/sample
Epoch 42/45
84077/84077 - 9s - loss: 8.0227e-04 - val_loss: 6.3896e-04 - 9s/epoch - 108us/sample
Epoch 43/45
84077/84077 - 9s - loss: 6.2749e-04 - val_loss: 5.8238e-04 - 9s/epoch - 108us/sample
Epoch 44/45
84077/84077 - 9s - loss: 6.1063e-04 - val_loss: 5.7612e-04 - 9s/epoch - 109us/sample
Epoch 45/45
84077/84077 - 9s - loss: 5.9520e-04 - val_loss: 5.6133e-04 - 9s/epoch - 108us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.0005613348986593609
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:53:48.549640: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:14640 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.15122055628489905
cosine 0.14913740597527492
MAE: 0.004651143932115295
RMSE: 0.022604105824256256
r2: 0.6008577870555308
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'mse', 64, 45, 0.0005, 0.4, 377, 0.0005952039106564245, 0.0005613348986593609, 0.15122055628489905, 0.14913740597527492, 0.004651143932115295, 0.022604105824256256, 0.6008577870555308, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0008 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 1886)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          711399      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          711399      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2643907     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,854,633
Trainable params: 5,846,335
Non-trainable params: 8,298
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 08:53:58.996748: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_12/bias/Assign' id:15642 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_12/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_12/bias, dense_dec1_12/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:54:13.617792: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:15924 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0061 - val_loss: 0.0017 - 17s/epoch - 205us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0020 - val_loss: 9.9307e-04 - 14s/epoch - 163us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0012 - val_loss: 8.1193e-04 - 14s/epoch - 163us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0034 - val_loss: 9.5268e-04 - 14s/epoch - 162us/sample
Epoch 5/90
84077/84077 - 14s - loss: 7.7269e-04 - val_loss: 6.4974e-04 - 14s/epoch - 162us/sample
Epoch 6/90
84077/84077 - 14s - loss: 6.2738e-04 - val_loss: 4.5758e-04 - 14s/epoch - 163us/sample
Epoch 7/90
84077/84077 - 14s - loss: 4.8868e-04 - val_loss: 3.8883e-04 - 14s/epoch - 163us/sample
Epoch 8/90
84077/84077 - 14s - loss: 4.1066e-04 - val_loss: 3.3883e-04 - 14s/epoch - 162us/sample
Epoch 9/90
84077/84077 - 14s - loss: 3.5814e-04 - val_loss: 3.1676e-04 - 14s/epoch - 162us/sample
Epoch 10/90
84077/84077 - 14s - loss: 3.1966e-04 - val_loss: 2.7065e-04 - 14s/epoch - 164us/sample
Epoch 11/90
84077/84077 - 14s - loss: 2.9083e-04 - val_loss: 2.5459e-04 - 14s/epoch - 163us/sample
Epoch 12/90
84077/84077 - 14s - loss: 2.6992e-04 - val_loss: 2.3863e-04 - 14s/epoch - 162us/sample
Epoch 13/90
84077/84077 - 14s - loss: 2.5292e-04 - val_loss: 2.3061e-04 - 14s/epoch - 163us/sample
Epoch 14/90
84077/84077 - 14s - loss: 2.4080e-04 - val_loss: 2.1768e-04 - 14s/epoch - 164us/sample
Epoch 15/90
84077/84077 - 14s - loss: 2.3083e-04 - val_loss: 2.1497e-04 - 14s/epoch - 162us/sample
Epoch 16/90
84077/84077 - 14s - loss: 2.2224e-04 - val_loss: 2.1020e-04 - 14s/epoch - 162us/sample
Epoch 17/90
84077/84077 - 14s - loss: 2.1923e-04 - val_loss: 1.9987e-04 - 14s/epoch - 164us/sample
Epoch 18/90
84077/84077 - 14s - loss: 2.1189e-04 - val_loss: 2.0081e-04 - 14s/epoch - 163us/sample
Epoch 19/90
84077/84077 - 14s - loss: 2.0908e-04 - val_loss: 2.0160e-04 - 14s/epoch - 162us/sample
Epoch 20/90
84077/84077 - 14s - loss: 2.0324e-04 - val_loss: 1.9745e-04 - 14s/epoch - 163us/sample
Epoch 21/90
84077/84077 - 14s - loss: 1.9875e-04 - val_loss: 1.9185e-04 - 14s/epoch - 163us/sample
Epoch 22/90
84077/84077 - 14s - loss: 1.9493e-04 - val_loss: 1.8879e-04 - 14s/epoch - 162us/sample
Epoch 23/90
84077/84077 - 14s - loss: 1.9253e-04 - val_loss: 1.8771e-04 - 14s/epoch - 162us/sample
Epoch 24/90
84077/84077 - 14s - loss: 1.8983e-04 - val_loss: 1.8315e-04 - 14s/epoch - 162us/sample
Epoch 25/90
84077/84077 - 14s - loss: 1.8547e-04 - val_loss: 1.8413e-04 - 14s/epoch - 164us/sample
Epoch 26/90
84077/84077 - 14s - loss: 1.8272e-04 - val_loss: 1.7576e-04 - 14s/epoch - 163us/sample
Epoch 27/90
84077/84077 - 14s - loss: 1.9738e-04 - val_loss: 1.8122e-04 - 14s/epoch - 162us/sample
Epoch 28/90
84077/84077 - 14s - loss: 1.8255e-04 - val_loss: 1.7461e-04 - 14s/epoch - 164us/sample
Epoch 29/90
84077/84077 - 14s - loss: 1.7848e-04 - val_loss: 1.7458e-04 - 14s/epoch - 163us/sample
Epoch 30/90
84077/84077 - 14s - loss: 1.7587e-04 - val_loss: 1.7248e-04 - 14s/epoch - 162us/sample
Epoch 31/90
84077/84077 - 14s - loss: 1.7994e-04 - val_loss: 1.7351e-04 - 14s/epoch - 163us/sample
Epoch 32/90
84077/84077 - 14s - loss: 1.7369e-04 - val_loss: 1.7350e-04 - 14s/epoch - 164us/sample
Epoch 33/90
84077/84077 - 14s - loss: 1.7214e-04 - val_loss: 1.6765e-04 - 14s/epoch - 163us/sample
Epoch 34/90
84077/84077 - 14s - loss: 1.6974e-04 - val_loss: 1.6572e-04 - 14s/epoch - 162us/sample
Epoch 35/90
84077/84077 - 14s - loss: 1.7074e-04 - val_loss: 1.7532e-04 - 14s/epoch - 164us/sample
Epoch 36/90
84077/84077 - 14s - loss: 1.6826e-04 - val_loss: 1.6334e-04 - 14s/epoch - 163us/sample
Epoch 37/90
84077/84077 - 14s - loss: 1.6633e-04 - val_loss: 1.6654e-04 - 14s/epoch - 163us/sample
Epoch 38/90
84077/84077 - 14s - loss: 1.6550e-04 - val_loss: 1.6507e-04 - 14s/epoch - 164us/sample
Epoch 39/90
84077/84077 - 14s - loss: 1.6472e-04 - val_loss: 1.6505e-04 - 14s/epoch - 163us/sample
Epoch 40/90
84077/84077 - 14s - loss: 1.6493e-04 - val_loss: 1.6019e-04 - 14s/epoch - 162us/sample
Epoch 41/90
84077/84077 - 14s - loss: 1.6214e-04 - val_loss: 1.6085e-04 - 14s/epoch - 164us/sample
Epoch 42/90
84077/84077 - 14s - loss: 1.6322e-04 - val_loss: 1.6212e-04 - 14s/epoch - 163us/sample
Epoch 43/90
84077/84077 - 14s - loss: 1.6138e-04 - val_loss: 1.5900e-04 - 14s/epoch - 162us/sample
Epoch 44/90
84077/84077 - 14s - loss: 1.6006e-04 - val_loss: 1.6283e-04 - 14s/epoch - 162us/sample
Epoch 45/90
84077/84077 - 14s - loss: 1.5968e-04 - val_loss: 1.5699e-04 - 14s/epoch - 163us/sample
Epoch 46/90
84077/84077 - 14s - loss: 1.5923e-04 - val_loss: 1.5327e-04 - 14s/epoch - 163us/sample
Epoch 47/90
84077/84077 - 14s - loss: 1.5758e-04 - val_loss: 1.5498e-04 - 14s/epoch - 162us/sample
Epoch 48/90
84077/84077 - 14s - loss: 1.5671e-04 - val_loss: 1.5215e-04 - 14s/epoch - 164us/sample
Epoch 49/90
84077/84077 - 14s - loss: 1.5618e-04 - val_loss: 1.5406e-04 - 14s/epoch - 163us/sample
Epoch 50/90
84077/84077 - 14s - loss: 1.5519e-04 - val_loss: 1.5388e-04 - 14s/epoch - 163us/sample
Epoch 51/90
84077/84077 - 14s - loss: 1.5454e-04 - val_loss: 1.5265e-04 - 14s/epoch - 162us/sample
Epoch 52/90
84077/84077 - 14s - loss: 1.5458e-04 - val_loss: 1.5108e-04 - 14s/epoch - 164us/sample
Epoch 53/90
84077/84077 - 14s - loss: 1.5385e-04 - val_loss: 1.5166e-04 - 14s/epoch - 163us/sample
Epoch 54/90
84077/84077 - 14s - loss: 1.5279e-04 - val_loss: 1.4904e-04 - 14s/epoch - 163us/sample
Epoch 55/90
84077/84077 - 14s - loss: 1.5269e-04 - val_loss: 1.5184e-04 - 14s/epoch - 163us/sample
Epoch 56/90
84077/84077 - 14s - loss: 1.5200e-04 - val_loss: 1.4876e-04 - 14s/epoch - 164us/sample
Epoch 57/90
84077/84077 - 14s - loss: 1.5152e-04 - val_loss: 1.5112e-04 - 14s/epoch - 163us/sample
Epoch 58/90
84077/84077 - 14s - loss: 1.5183e-04 - val_loss: 1.5274e-04 - 14s/epoch - 162us/sample
Epoch 59/90
84077/84077 - 14s - loss: 1.5374e-04 - val_loss: 1.5671e-04 - 14s/epoch - 163us/sample
Epoch 60/90
84077/84077 - 14s - loss: 1.5144e-04 - val_loss: 1.4933e-04 - 14s/epoch - 163us/sample
Epoch 61/90
84077/84077 - 14s - loss: 1.5027e-04 - val_loss: 1.5689e-04 - 14s/epoch - 162us/sample
Epoch 62/90
84077/84077 - 14s - loss: 1.5033e-04 - val_loss: 1.4753e-04 - 14s/epoch - 162us/sample
Epoch 63/90
84077/84077 - 14s - loss: 1.5305e-04 - val_loss: 1.4833e-04 - 14s/epoch - 164us/sample
Epoch 64/90
84077/84077 - 14s - loss: 1.4851e-04 - val_loss: 1.4737e-04 - 14s/epoch - 163us/sample
Epoch 65/90
84077/84077 - 14s - loss: 1.4794e-04 - val_loss: 1.4873e-04 - 14s/epoch - 162us/sample
Epoch 66/90
84077/84077 - 14s - loss: 1.4799e-04 - val_loss: 1.4652e-04 - 14s/epoch - 162us/sample
Epoch 67/90
84077/84077 - 14s - loss: 1.4909e-04 - val_loss: 1.4599e-04 - 14s/epoch - 164us/sample
Epoch 68/90
84077/84077 - 14s - loss: 1.4690e-04 - val_loss: 1.4830e-04 - 14s/epoch - 163us/sample
Epoch 69/90
84077/84077 - 14s - loss: 1.4693e-04 - val_loss: 1.4518e-04 - 14s/epoch - 162us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.4683e-04 - val_loss: 1.4495e-04 - 14s/epoch - 163us/sample
Epoch 71/90
84077/84077 - 14s - loss: 1.4628e-04 - val_loss: 1.4420e-04 - 14s/epoch - 164us/sample
Epoch 72/90
84077/84077 - 14s - loss: 1.4546e-04 - val_loss: 1.4406e-04 - 14s/epoch - 162us/sample
Epoch 73/90
84077/84077 - 14s - loss: 1.4510e-04 - val_loss: 1.4780e-04 - 14s/epoch - 162us/sample
Epoch 74/90
84077/84077 - 14s - loss: 1.4629e-04 - val_loss: 1.4747e-04 - 14s/epoch - 164us/sample
Epoch 75/90
84077/84077 - 14s - loss: 1.4454e-04 - val_loss: 1.4442e-04 - 14s/epoch - 163us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.4451e-04 - val_loss: 1.4301e-04 - 14s/epoch - 162us/sample
Epoch 77/90
84077/84077 - 14s - loss: 1.4690e-04 - val_loss: 1.4320e-04 - 14s/epoch - 164us/sample
Epoch 78/90
84077/84077 - 14s - loss: 1.4665e-04 - val_loss: 1.4256e-04 - 14s/epoch - 163us/sample
Epoch 79/90
84077/84077 - 14s - loss: 1.4785e-04 - val_loss: 1.4428e-04 - 14s/epoch - 162us/sample
Epoch 80/90
84077/84077 - 14s - loss: 1.4364e-04 - val_loss: 1.4273e-04 - 14s/epoch - 164us/sample
Epoch 81/90
84077/84077 - 14s - loss: 1.4307e-04 - val_loss: 1.4354e-04 - 14s/epoch - 163us/sample
Epoch 82/90
84077/84077 - 14s - loss: 1.4348e-04 - val_loss: 1.4561e-04 - 14s/epoch - 162us/sample
Epoch 83/90
84077/84077 - 14s - loss: 1.4249e-04 - val_loss: 1.4173e-04 - 14s/epoch - 163us/sample
Epoch 84/90
84077/84077 - 14s - loss: 1.4260e-04 - val_loss: 1.3983e-04 - 14s/epoch - 164us/sample
Epoch 85/90
84077/84077 - 14s - loss: 1.4409e-04 - val_loss: 1.4428e-04 - 14s/epoch - 162us/sample
Epoch 86/90
84077/84077 - 14s - loss: 1.4227e-04 - val_loss: 1.4253e-04 - 14s/epoch - 163us/sample
Epoch 87/90
84077/84077 - 14s - loss: 1.4240e-04 - val_loss: 1.4023e-04 - 14s/epoch - 163us/sample
Epoch 88/90
84077/84077 - 14s - loss: 1.4291e-04 - val_loss: 1.4058e-04 - 14s/epoch - 162us/sample
Epoch 89/90
84077/84077 - 14s - loss: 1.4156e-04 - val_loss: 1.4129e-04 - 14s/epoch - 163us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.4380e-04 - val_loss: 1.4119e-04 - 14s/epoch - 164us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.00014118739673064804
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:14:34.266156: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:15895 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.016760704206635148
cosine 0.016560323362503838
MAE: 0.0017965001670253297
RMSE: 0.008503106286834388
r2: 0.9437051001698247
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 90, 0.0008, 0.4, 377, 0.00014380310166316613, 0.00014118739673064804, 0.016760704206635148, 0.016560323362503838, 0.0017965001670253297, 0.008503106286834388, 0.9437051001698247, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.0005 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 1414)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          533455      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          533455      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2018507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,425,889
Trainable params: 4,419,479
Non-trainable params: 6,410
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 09:14:44.954930: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_39/gamma/Assign' id:16764 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_39/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_39/gamma, batch_normalization_39/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:14:55.296676: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:17179 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 13s - loss: 0.0069 - val_loss: 0.0016 - 13s/epoch - 154us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 109us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0028 - val_loss: 0.0017 - 9s/epoch - 108us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 8.1352e-04 - 9s/epoch - 108us/sample
Epoch 5/90
84077/84077 - 9s - loss: 9.5580e-04 - val_loss: 7.3891e-04 - 9s/epoch - 108us/sample
Epoch 6/90
84077/84077 - 9s - loss: 7.1003e-04 - val_loss: 5.6584e-04 - 9s/epoch - 108us/sample
Epoch 7/90
84077/84077 - 9s - loss: 6.1150e-04 - val_loss: 0.0014 - 9s/epoch - 108us/sample
Epoch 8/90
84077/84077 - 9s - loss: 5.5511e-04 - val_loss: 4.0974e-04 - 9s/epoch - 108us/sample
Epoch 9/90
84077/84077 - 9s - loss: 4.5644e-04 - val_loss: 3.5184e-04 - 9s/epoch - 108us/sample
Epoch 10/90
84077/84077 - 9s - loss: 3.7472e-04 - val_loss: 3.0604e-04 - 9s/epoch - 109us/sample
Epoch 11/90
84077/84077 - 9s - loss: 3.3309e-04 - val_loss: 2.7459e-04 - 9s/epoch - 109us/sample
Epoch 12/90
84077/84077 - 9s - loss: 3.0558e-04 - val_loss: 2.5218e-04 - 9s/epoch - 108us/sample
Epoch 13/90
84077/84077 - 9s - loss: 2.8018e-04 - val_loss: 2.2866e-04 - 9s/epoch - 108us/sample
Epoch 14/90
84077/84077 - 9s - loss: 2.5592e-04 - val_loss: 2.1467e-04 - 9s/epoch - 108us/sample
Epoch 15/90
84077/84077 - 9s - loss: 2.4013e-04 - val_loss: 2.0498e-04 - 9s/epoch - 108us/sample
Epoch 16/90
84077/84077 - 9s - loss: 2.2641e-04 - val_loss: 1.9648e-04 - 9s/epoch - 109us/sample
Epoch 17/90
84077/84077 - 9s - loss: 2.1556e-04 - val_loss: 1.9252e-04 - 9s/epoch - 108us/sample
Epoch 18/90
84077/84077 - 9s - loss: 2.0802e-04 - val_loss: 1.8192e-04 - 9s/epoch - 108us/sample
Epoch 19/90
84077/84077 - 9s - loss: 2.0070e-04 - val_loss: 1.7728e-04 - 9s/epoch - 108us/sample
Epoch 20/90
84077/84077 - 9s - loss: 1.9570e-04 - val_loss: 1.8574e-04 - 9s/epoch - 108us/sample
Epoch 21/90
84077/84077 - 9s - loss: 1.8864e-04 - val_loss: 1.7081e-04 - 9s/epoch - 108us/sample
Epoch 22/90
84077/84077 - 9s - loss: 1.8408e-04 - val_loss: 1.6483e-04 - 9s/epoch - 109us/sample
Epoch 23/90
84077/84077 - 9s - loss: 1.7785e-04 - val_loss: 1.6299e-04 - 9s/epoch - 108us/sample
Epoch 24/90
84077/84077 - 9s - loss: 1.7638e-04 - val_loss: 1.6038e-04 - 9s/epoch - 109us/sample
Epoch 25/90
84077/84077 - 9s - loss: 1.7218e-04 - val_loss: 1.6006e-04 - 9s/epoch - 108us/sample
Epoch 26/90
84077/84077 - 9s - loss: 1.6824e-04 - val_loss: 1.5362e-04 - 9s/epoch - 108us/sample
Epoch 27/90
84077/84077 - 9s - loss: 1.6509e-04 - val_loss: 1.5126e-04 - 9s/epoch - 108us/sample
Epoch 28/90
84077/84077 - 9s - loss: 1.6296e-04 - val_loss: 1.5121e-04 - 9s/epoch - 109us/sample
Epoch 29/90
84077/84077 - 9s - loss: 1.6096e-04 - val_loss: 1.4921e-04 - 9s/epoch - 108us/sample
Epoch 30/90
84077/84077 - 9s - loss: 1.5796e-04 - val_loss: 1.4946e-04 - 9s/epoch - 108us/sample
Epoch 31/90
84077/84077 - 9s - loss: 1.5532e-04 - val_loss: 1.4452e-04 - 9s/epoch - 108us/sample
Epoch 32/90
84077/84077 - 9s - loss: 1.5522e-04 - val_loss: 1.4294e-04 - 9s/epoch - 108us/sample
Epoch 33/90
84077/84077 - 9s - loss: 1.5253e-04 - val_loss: 1.4304e-04 - 9s/epoch - 109us/sample
Epoch 34/90
84077/84077 - 9s - loss: 1.5095e-04 - val_loss: 1.4188e-04 - 9s/epoch - 109us/sample
Epoch 35/90
84077/84077 - 9s - loss: 1.5108e-04 - val_loss: 1.3858e-04 - 9s/epoch - 109us/sample
Epoch 36/90
84077/84077 - 9s - loss: 1.4830e-04 - val_loss: 1.4030e-04 - 9s/epoch - 108us/sample
Epoch 37/90
84077/84077 - 9s - loss: 1.4695e-04 - val_loss: 1.3742e-04 - 9s/epoch - 108us/sample
Epoch 38/90
84077/84077 - 9s - loss: 1.4613e-04 - val_loss: 1.3631e-04 - 9s/epoch - 108us/sample
Epoch 39/90
84077/84077 - 9s - loss: 1.4395e-04 - val_loss: 1.3579e-04 - 9s/epoch - 109us/sample
Epoch 40/90
84077/84077 - 9s - loss: 1.4267e-04 - val_loss: 1.3400e-04 - 9s/epoch - 109us/sample
Epoch 41/90
84077/84077 - 9s - loss: 1.4174e-04 - val_loss: 1.3099e-04 - 9s/epoch - 109us/sample
Epoch 42/90
84077/84077 - 9s - loss: 1.4073e-04 - val_loss: 1.3168e-04 - 9s/epoch - 108us/sample
Epoch 43/90
84077/84077 - 9s - loss: 1.4024e-04 - val_loss: 1.2960e-04 - 9s/epoch - 108us/sample
Epoch 44/90
84077/84077 - 9s - loss: 1.3920e-04 - val_loss: 1.2892e-04 - 9s/epoch - 108us/sample
Epoch 45/90
84077/84077 - 9s - loss: 1.3813e-04 - val_loss: 1.2952e-04 - 9s/epoch - 110us/sample
Epoch 46/90
84077/84077 - 9s - loss: 1.3907e-04 - val_loss: 1.2985e-04 - 9s/epoch - 109us/sample
Epoch 47/90
84077/84077 - 9s - loss: 1.3657e-04 - val_loss: 1.2494e-04 - 9s/epoch - 108us/sample
Epoch 48/90
84077/84077 - 9s - loss: 1.3528e-04 - val_loss: 1.2775e-04 - 9s/epoch - 108us/sample
Epoch 49/90
84077/84077 - 9s - loss: 1.3498e-04 - val_loss: 1.2651e-04 - 9s/epoch - 108us/sample
Epoch 50/90
84077/84077 - 9s - loss: 1.3625e-04 - val_loss: 1.2669e-04 - 9s/epoch - 109us/sample
Epoch 51/90
84077/84077 - 9s - loss: 1.3674e-04 - val_loss: 1.2544e-04 - 9s/epoch - 109us/sample
Epoch 52/90
84077/84077 - 9s - loss: 1.3306e-04 - val_loss: 1.2386e-04 - 9s/epoch - 108us/sample
Epoch 53/90
84077/84077 - 9s - loss: 1.3290e-04 - val_loss: 1.2351e-04 - 9s/epoch - 108us/sample
Epoch 54/90
84077/84077 - 9s - loss: 1.3174e-04 - val_loss: 1.2246e-04 - 9s/epoch - 108us/sample
Epoch 55/90
84077/84077 - 9s - loss: 1.3151e-04 - val_loss: 1.2206e-04 - 9s/epoch - 108us/sample
Epoch 56/90
84077/84077 - 9s - loss: 1.3071e-04 - val_loss: 1.2352e-04 - 9s/epoch - 109us/sample
Epoch 57/90
84077/84077 - 9s - loss: 1.3099e-04 - val_loss: 1.2381e-04 - 9s/epoch - 108us/sample
Epoch 58/90
84077/84077 - 9s - loss: 1.3083e-04 - val_loss: 1.2226e-04 - 9s/epoch - 108us/sample
Epoch 59/90
84077/84077 - 9s - loss: 1.2872e-04 - val_loss: 1.2264e-04 - 9s/epoch - 108us/sample
Epoch 60/90
84077/84077 - 9s - loss: 1.2895e-04 - val_loss: 1.2172e-04 - 9s/epoch - 108us/sample
Epoch 61/90
84077/84077 - 9s - loss: 1.2881e-04 - val_loss: 1.2062e-04 - 9s/epoch - 108us/sample
Epoch 62/90
84077/84077 - 9s - loss: 1.2790e-04 - val_loss: 1.1961e-04 - 9s/epoch - 109us/sample
Epoch 63/90
84077/84077 - 9s - loss: 1.2761e-04 - val_loss: 1.1965e-04 - 9s/epoch - 108us/sample
Epoch 64/90
84077/84077 - 9s - loss: 1.2750e-04 - val_loss: 1.2017e-04 - 9s/epoch - 108us/sample
Epoch 65/90
84077/84077 - 9s - loss: 1.2693e-04 - val_loss: 1.2013e-04 - 9s/epoch - 108us/sample
Epoch 66/90
84077/84077 - 9s - loss: 1.2569e-04 - val_loss: 1.1859e-04 - 9s/epoch - 108us/sample
Epoch 67/90
84077/84077 - 9s - loss: 1.2616e-04 - val_loss: 1.1991e-04 - 9s/epoch - 108us/sample
Epoch 68/90
84077/84077 - 9s - loss: 1.2502e-04 - val_loss: 1.1771e-04 - 9s/epoch - 109us/sample
Epoch 69/90
84077/84077 - 9s - loss: 1.2639e-04 - val_loss: 1.1758e-04 - 9s/epoch - 109us/sample
Epoch 70/90
84077/84077 - 9s - loss: 1.2497e-04 - val_loss: 1.1842e-04 - 9s/epoch - 108us/sample
Epoch 71/90
84077/84077 - 9s - loss: 1.2420e-04 - val_loss: 1.1823e-04 - 9s/epoch - 108us/sample
Epoch 72/90
84077/84077 - 9s - loss: 1.2487e-04 - val_loss: 1.1957e-04 - 9s/epoch - 108us/sample
Epoch 73/90
84077/84077 - 9s - loss: 1.2405e-04 - val_loss: 1.1619e-04 - 9s/epoch - 108us/sample
Epoch 74/90
84077/84077 - 9s - loss: 1.2341e-04 - val_loss: 1.1767e-04 - 9s/epoch - 110us/sample
Epoch 75/90
84077/84077 - 9s - loss: 1.2419e-04 - val_loss: 1.1539e-04 - 9s/epoch - 108us/sample
Epoch 76/90
84077/84077 - 9s - loss: 1.2318e-04 - val_loss: 1.1540e-04 - 9s/epoch - 108us/sample
Epoch 77/90
84077/84077 - 9s - loss: 1.2256e-04 - val_loss: 1.1518e-04 - 9s/epoch - 108us/sample
Epoch 78/90
84077/84077 - 9s - loss: 1.2226e-04 - val_loss: 1.1407e-04 - 9s/epoch - 108us/sample
Epoch 79/90
84077/84077 - 9s - loss: 1.2400e-04 - val_loss: 1.1524e-04 - 9s/epoch - 109us/sample
Epoch 80/90
84077/84077 - 9s - loss: 1.2160e-04 - val_loss: 1.1468e-04 - 9s/epoch - 109us/sample
Epoch 81/90
84077/84077 - 9s - loss: 1.2178e-04 - val_loss: 1.1481e-04 - 9s/epoch - 109us/sample
Epoch 82/90
84077/84077 - 9s - loss: 1.2215e-04 - val_loss: 1.1574e-04 - 9s/epoch - 108us/sample
Epoch 83/90
84077/84077 - 9s - loss: 1.2117e-04 - val_loss: 1.1665e-04 - 9s/epoch - 108us/sample
Epoch 84/90
84077/84077 - 9s - loss: 1.2084e-04 - val_loss: 1.1504e-04 - 9s/epoch - 109us/sample
Epoch 85/90
84077/84077 - 9s - loss: 1.2130e-04 - val_loss: 1.1530e-04 - 9s/epoch - 109us/sample
Epoch 86/90
84077/84077 - 9s - loss: 1.1991e-04 - val_loss: 1.1399e-04 - 9s/epoch - 108us/sample
Epoch 87/90
84077/84077 - 9s - loss: 1.1991e-04 - val_loss: 1.1478e-04 - 9s/epoch - 108us/sample
Epoch 88/90
84077/84077 - 9s - loss: 1.1980e-04 - val_loss: 1.1398e-04 - 9s/epoch - 108us/sample
Epoch 89/90
84077/84077 - 9s - loss: 1.1972e-04 - val_loss: 1.1412e-04 - 9s/epoch - 108us/sample
Epoch 90/90
84077/84077 - 9s - loss: 1.1956e-04 - val_loss: 1.1367e-04 - 9s/epoch - 109us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.0001136663506188143
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:28:27.980769: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:17150 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01270199613338462
cosine 0.012546509034396539
MAE: 0.0016234003023819043
RMSE: 0.006840653143900124
r2: 0.9637075885377299
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 64, 90, 0.0005, 0.4, 377, 0.00011956311658429434, 0.0001136663506188143, 0.01270199613338462, 0.012546509034396539, 0.0016234003023819043, 0.006840653143900124, 0.9637075885377299, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 50 0.0005 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2451)         2313744     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 2451)        9804        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 2451)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          924404      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          924404      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          3392532     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,564,888
Trainable params: 7,554,330
Non-trainable params: 10,558
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 09:28:38.703512: W tensorflow/c/c_api.cc:291] Operation '{name:'training_28/Adam/dense_dec0_14/bias/m/Assign' id:18971 op device:{requested: '', assigned: ''} def:{{{node training_28/Adam/dense_dec0_14/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_28/Adam/dense_dec0_14/bias/m, training_28/Adam/dense_dec0_14/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:28:53.775533: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:18434 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0069 - val_loss: 0.0016 - 18s/epoch - 214us/sample
Epoch 2/50
84077/84077 - 14s - loss: 5.3708 - val_loss: 0.0013 - 14s/epoch - 165us/sample
Epoch 3/50
84077/84077 - 14s - loss: 0.0083 - val_loss: 0.0059 - 14s/epoch - 165us/sample
Epoch 4/50
84077/84077 - 14s - loss: 0.0016 - val_loss: 0.0011 - 14s/epoch - 166us/sample
Epoch 5/50
84077/84077 - 14s - loss: 0.0012 - val_loss: 7.4193e-04 - 14s/epoch - 166us/sample
Epoch 6/50
84077/84077 - 14s - loss: 0.0018 - val_loss: 6.6260e-04 - 14s/epoch - 165us/sample
Epoch 7/50
84077/84077 - 14s - loss: 0.0086 - val_loss: 7.7021e-04 - 14s/epoch - 165us/sample
Epoch 8/50
84077/84077 - 14s - loss: 0.0230 - val_loss: 0.0015 - 14s/epoch - 166us/sample
Epoch 9/50
84077/84077 - 14s - loss: 9.1706e-04 - val_loss: 6.6822e-04 - 14s/epoch - 167us/sample
Epoch 10/50
84077/84077 - 14s - loss: 6.8350e-04 - val_loss: 6.0493e-04 - 14s/epoch - 166us/sample
Epoch 11/50
84077/84077 - 14s - loss: 6.7674e-04 - val_loss: 5.6176e-04 - 14s/epoch - 165us/sample
Epoch 12/50
84077/84077 - 14s - loss: 5.7707e-04 - val_loss: 5.5975e-04 - 14s/epoch - 166us/sample
Epoch 13/50
84077/84077 - 14s - loss: 5.9634e-04 - val_loss: 4.9470e-04 - 14s/epoch - 167us/sample
Epoch 14/50
84077/84077 - 14s - loss: 8.9118e-04 - val_loss: 0.0015 - 14s/epoch - 165us/sample
Epoch 15/50
84077/84077 - 14s - loss: 0.0011 - val_loss: 4.9348e-04 - 14s/epoch - 165us/sample
Epoch 16/50
84077/84077 - 14s - loss: 5.1156e-04 - val_loss: 4.8908e-04 - 14s/epoch - 167us/sample
Epoch 17/50
84077/84077 - 14s - loss: 4.7835e-04 - val_loss: 4.3203e-04 - 14s/epoch - 166us/sample
Epoch 18/50
84077/84077 - 14s - loss: 4.7734e-04 - val_loss: 4.1448e-04 - 14s/epoch - 166us/sample
Epoch 19/50
84077/84077 - 14s - loss: 4.4425e-04 - val_loss: 4.1102e-04 - 14s/epoch - 167us/sample
Epoch 20/50
84077/84077 - 14s - loss: 4.1910e-04 - val_loss: 3.7760e-04 - 14s/epoch - 166us/sample
Epoch 21/50
84077/84077 - 14s - loss: 4.1073e-04 - val_loss: 3.7180e-04 - 14s/epoch - 165us/sample
Epoch 22/50
84077/84077 - 14s - loss: 4.0195e-04 - val_loss: 3.6386e-04 - 14s/epoch - 166us/sample
Epoch 23/50
84077/84077 - 14s - loss: 3.9205e-04 - val_loss: 3.6370e-04 - 14s/epoch - 167us/sample
Epoch 24/50
84077/84077 - 14s - loss: 3.8357e-04 - val_loss: 3.5146e-04 - 14s/epoch - 165us/sample
Epoch 25/50
84077/84077 - 14s - loss: 3.7837e-04 - val_loss: 3.4934e-04 - 14s/epoch - 166us/sample
Epoch 26/50
84077/84077 - 14s - loss: 3.7316e-04 - val_loss: 3.4632e-04 - 14s/epoch - 167us/sample
Epoch 27/50
84077/84077 - 14s - loss: 3.6833e-04 - val_loss: 3.4161e-04 - 14s/epoch - 166us/sample
Epoch 28/50
84077/84077 - 14s - loss: 3.6350e-04 - val_loss: 3.3727e-04 - 14s/epoch - 165us/sample
Epoch 29/50
84077/84077 - 14s - loss: 3.5916e-04 - val_loss: 3.3421e-04 - 14s/epoch - 167us/sample
Epoch 30/50
84077/84077 - 14s - loss: 3.5404e-04 - val_loss: 3.3076e-04 - 14s/epoch - 166us/sample
Epoch 31/50
84077/84077 - 14s - loss: 3.4841e-04 - val_loss: 3.2628e-04 - 14s/epoch - 166us/sample
Epoch 32/50
84077/84077 - 14s - loss: 3.4556e-04 - val_loss: 3.2263e-04 - 14s/epoch - 166us/sample
Epoch 33/50
84077/84077 - 14s - loss: 3.4353e-04 - val_loss: 3.1793e-04 - 14s/epoch - 167us/sample
Epoch 34/50
84077/84077 - 14s - loss: 3.3961e-04 - val_loss: 3.1613e-04 - 14s/epoch - 166us/sample
Epoch 35/50
84077/84077 - 14s - loss: 3.3811e-04 - val_loss: 3.1598e-04 - 14s/epoch - 165us/sample
Epoch 36/50
84077/84077 - 14s - loss: 3.3577e-04 - val_loss: 3.1240e-04 - 14s/epoch - 166us/sample
Epoch 37/50
84077/84077 - 14s - loss: 3.3406e-04 - val_loss: 3.1232e-04 - 14s/epoch - 167us/sample
Epoch 38/50
84077/84077 - 14s - loss: 3.3247e-04 - val_loss: 3.1097e-04 - 14s/epoch - 166us/sample
Epoch 39/50
84077/84077 - 14s - loss: 3.3071e-04 - val_loss: 3.0721e-04 - 14s/epoch - 165us/sample
Epoch 40/50
84077/84077 - 14s - loss: 3.2976e-04 - val_loss: 3.0842e-04 - 14s/epoch - 167us/sample
Epoch 41/50
84077/84077 - 14s - loss: 3.2796e-04 - val_loss: 3.0705e-04 - 14s/epoch - 166us/sample
Epoch 42/50
84077/84077 - 14s - loss: 3.2627e-04 - val_loss: 3.0223e-04 - 14s/epoch - 165us/sample
Epoch 43/50
84077/84077 - 14s - loss: 3.2500e-04 - val_loss: 3.0678e-04 - 14s/epoch - 166us/sample
Epoch 44/50
84077/84077 - 14s - loss: 3.2314e-04 - val_loss: 3.0354e-04 - 14s/epoch - 167us/sample
Epoch 45/50
84077/84077 - 14s - loss: 3.2130e-04 - val_loss: 3.0148e-04 - 14s/epoch - 165us/sample
Epoch 46/50
84077/84077 - 14s - loss: 3.2012e-04 - val_loss: 3.0289e-04 - 14s/epoch - 166us/sample
Epoch 47/50
84077/84077 - 14s - loss: 3.1931e-04 - val_loss: 3.0032e-04 - 14s/epoch - 167us/sample
Epoch 48/50
84077/84077 - 14s - loss: 3.1804e-04 - val_loss: 2.9828e-04 - 14s/epoch - 166us/sample
Epoch 49/50
84077/84077 - 14s - loss: 3.1731e-04 - val_loss: 2.9906e-04 - 14s/epoch - 166us/sample
Epoch 50/50
84077/84077 - 14s - loss: 3.1613e-04 - val_loss: 2.9895e-04 - 14s/epoch - 166us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.00029895480474728507
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:40:19.518071: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:18405 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05899213017358157
cosine 0.05823377807496605
MAE: 0.0030640465200486343
RMSE: 0.015274002801569136
r2: 0.8177759843661916
RMSE zero-vector: 0.04004287452915337
['2.6custom_VAE', 'mse', 32, 50, 0.0005, 0.4, 377, 0.0003161331968880378, 0.00029895480474728507, 0.05899213017358157, 0.05823377807496605, 0.0030640465200486343, 0.015274002801569136, 0.8177759843661916, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 90 0.001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1508)         1423552     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 1508)        6032        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 1508)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          568893      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          568893      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2143057     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,710,427
Trainable params: 4,703,641
Non-trainable params: 6,786
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 09:40:31.219341: W tensorflow/c/c_api.cc:291] Operation '{name:'training_30/Adam/batch_normalization_47/gamma/v/Assign' id:20432 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/batch_normalization_47/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_30/Adam/batch_normalization_47/gamma/v, training_30/Adam/batch_normalization_47/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:40:42.178178: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:19711 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0782 - val_loss: 0.0744 - 14s/epoch - 165us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0713 - val_loss: 11.9986 - 9s/epoch - 112us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0679 - val_loss: 0.0678 - 9s/epoch - 112us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0671 - val_loss: 0.0675 - 9s/epoch - 112us/sample
Epoch 5/90
84077/84077 - 10s - loss: 0.0670 - val_loss: 0.0675 - 10s/epoch - 113us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 113us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 12/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0674 - 10s/epoch - 113us/sample
Epoch 13/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 18/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 19/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 21/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 22/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 23/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 24/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 25/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 26/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 27/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 29/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 30/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 31/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 32/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 33/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 35/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 36/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 37/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 40/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 41/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 42/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 43/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 45/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 46/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 47/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 53/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 54/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 58/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 59/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 60/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 64/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 65/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 69/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 70/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 71/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 72/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 77/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 78/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 83/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 84/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 85/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 86/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 87/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 89/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.06734575017779472
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:54:46.887210: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:19663 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2081352441572752
cosine 1.161881215850885
MAE: 4.58948362972588
RMSE: 4.908608627833743
r2: -17704.286247196495
RMSE zero-vector: 0.04004287452915337
['1.6custom_VAE', 'binary_crossentropy', 64, 90, 0.001, 0.4, 377, 0.0668369321031629, 0.06734575017779472, 1.2081352441572752, 1.161881215850885, 4.58948362972588, 4.908608627833743, -17704.286247196495, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0012000000000000001 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 1980)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          746837      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          746837      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2768457     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,139,171
Trainable params: 6,130,497
Non-trainable params: 8,674
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 09:54:58.404348: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/outputlayer_16/bias/m/Assign' id:21592 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/outputlayer_16/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/outputlayer_16/bias/m, training_32/Adam/outputlayer_16/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:55:09.219269: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:21032 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0143 - val_loss: 0.0015 - 14s/epoch - 165us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 110us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0017 - val_loss: 9.8456e-04 - 9s/epoch - 111us/sample
Epoch 4/90
84077/84077 - 9s - loss: 1.4631 - val_loss: 0.0132 - 9s/epoch - 111us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0060 - val_loss: 0.0032 - 9s/epoch - 110us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0048 - val_loss: 0.0026 - 9s/epoch - 110us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0021 - val_loss: 0.0016 - 9s/epoch - 110us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.3613 - val_loss: 0.0041 - 9s/epoch - 110us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0030 - val_loss: 0.0021 - 9s/epoch - 110us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 110us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0018 - 9s/epoch - 111us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0012 - val_loss: 0.0085 - 9s/epoch - 110us/sample
Epoch 13/90
84077/84077 - 9s - loss: 9.4301e-04 - val_loss: 7.3387e-04 - 9s/epoch - 110us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 6.9212e-04 - 9s/epoch - 110us/sample
Epoch 15/90
84077/84077 - 9s - loss: 8.7181e-04 - val_loss: 0.0023 - 9s/epoch - 110us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0025 - val_loss: 8.7575e-04 - 9s/epoch - 111us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0016 - 9s/epoch - 111us/sample
Epoch 18/90
84077/84077 - 9s - loss: 8.4572e-04 - val_loss: 6.1117e-04 - 9s/epoch - 110us/sample
Epoch 19/90
84077/84077 - 9s - loss: 9.5088e-04 - val_loss: 5.6974e-04 - 9s/epoch - 110us/sample
Epoch 20/90
84077/84077 - 9s - loss: 6.1392e-04 - val_loss: 5.5255e-04 - 9s/epoch - 110us/sample
Epoch 21/90
84077/84077 - 9s - loss: 5.2656e-04 - val_loss: 4.6659e-04 - 9s/epoch - 110us/sample
Epoch 22/90
84077/84077 - 9s - loss: 4.9851e-04 - val_loss: 4.3575e-04 - 9s/epoch - 111us/sample
Epoch 23/90
84077/84077 - 9s - loss: 7.2807e-04 - val_loss: 5.1960e-04 - 9s/epoch - 110us/sample
Epoch 24/90
84077/84077 - 9s - loss: 5.8488e-04 - val_loss: 4.1777e-04 - 9s/epoch - 110us/sample
Epoch 25/90
84077/84077 - 9s - loss: 4.2574e-04 - val_loss: 3.9180e-04 - 9s/epoch - 110us/sample
Epoch 26/90
84077/84077 - 9s - loss: 7.6840e-04 - val_loss: 4.6271e-04 - 9s/epoch - 110us/sample
Epoch 27/90
84077/84077 - 9s - loss: 4.6530e-04 - val_loss: 3.7401e-04 - 9s/epoch - 111us/sample
Epoch 28/90
84077/84077 - 9s - loss: 4.0836e-04 - val_loss: 3.7175e-04 - 9s/epoch - 110us/sample
Epoch 29/90
84077/84077 - 9s - loss: 5.1109e-04 - val_loss: 4.5834e-04 - 9s/epoch - 110us/sample
Epoch 30/90
84077/84077 - 9s - loss: 3.9622e-04 - val_loss: 3.4839e-04 - 9s/epoch - 110us/sample
Epoch 31/90
84077/84077 - 9s - loss: 3.7509e-04 - val_loss: 3.4101e-04 - 9s/epoch - 110us/sample
Epoch 32/90
84077/84077 - 9s - loss: 3.6921e-04 - val_loss: 3.3665e-04 - 9s/epoch - 112us/sample
Epoch 33/90
84077/84077 - 9s - loss: 3.5701e-04 - val_loss: 3.2524e-04 - 9s/epoch - 110us/sample
Epoch 34/90
84077/84077 - 9s - loss: 3.8449e-04 - val_loss: 3.2195e-04 - 9s/epoch - 111us/sample
Epoch 35/90
84077/84077 - 9s - loss: 3.4448e-04 - val_loss: 3.1581e-04 - 9s/epoch - 110us/sample
Epoch 36/90
84077/84077 - 9s - loss: 3.4156e-04 - val_loss: 3.4958e-04 - 9s/epoch - 110us/sample
Epoch 37/90
84077/84077 - 9s - loss: 3.3567e-04 - val_loss: 3.0819e-04 - 9s/epoch - 110us/sample
Epoch 38/90
84077/84077 - 9s - loss: 3.3609e-04 - val_loss: 3.1072e-04 - 9s/epoch - 111us/sample
Epoch 39/90
84077/84077 - 9s - loss: 3.2587e-04 - val_loss: 3.0162e-04 - 9s/epoch - 111us/sample
Epoch 40/90
84077/84077 - 9s - loss: 3.2217e-04 - val_loss: 2.9632e-04 - 9s/epoch - 110us/sample
Epoch 41/90
84077/84077 - 9s - loss: 3.1618e-04 - val_loss: 2.9275e-04 - 9s/epoch - 110us/sample
Epoch 42/90
84077/84077 - 9s - loss: 3.1216e-04 - val_loss: 2.8680e-04 - 9s/epoch - 110us/sample
Epoch 43/90
84077/84077 - 9s - loss: 3.0480e-04 - val_loss: 2.8031e-04 - 9s/epoch - 111us/sample
Epoch 44/90
84077/84077 - 9s - loss: 2.9991e-04 - val_loss: 2.8052e-04 - 9s/epoch - 110us/sample
Epoch 45/90
84077/84077 - 9s - loss: 2.9751e-04 - val_loss: 2.7644e-04 - 9s/epoch - 110us/sample
Epoch 46/90
84077/84077 - 9s - loss: 2.9480e-04 - val_loss: 2.7326e-04 - 9s/epoch - 110us/sample
Epoch 47/90
84077/84077 - 9s - loss: 2.9181e-04 - val_loss: 2.7147e-04 - 9s/epoch - 110us/sample
Epoch 48/90
84077/84077 - 9s - loss: 2.8843e-04 - val_loss: 2.6991e-04 - 9s/epoch - 110us/sample
Epoch 49/90
84077/84077 - 9s - loss: 2.8638e-04 - val_loss: 2.6697e-04 - 9s/epoch - 111us/sample
Epoch 50/90
84077/84077 - 9s - loss: 2.8585e-04 - val_loss: 2.6801e-04 - 9s/epoch - 111us/sample
Epoch 51/90
84077/84077 - 9s - loss: 2.8346e-04 - val_loss: 2.6467e-04 - 9s/epoch - 110us/sample
Epoch 52/90
84077/84077 - 9s - loss: 2.8045e-04 - val_loss: 2.6237e-04 - 9s/epoch - 110us/sample
Epoch 53/90
84077/84077 - 9s - loss: 2.7763e-04 - val_loss: 2.5991e-04 - 9s/epoch - 110us/sample
Epoch 54/90
84077/84077 - 9s - loss: 2.7691e-04 - val_loss: 2.5897e-04 - 9s/epoch - 111us/sample
Epoch 55/90
84077/84077 - 9s - loss: 2.7504e-04 - val_loss: 2.5732e-04 - 9s/epoch - 110us/sample
Epoch 56/90
84077/84077 - 9s - loss: 2.7294e-04 - val_loss: 2.5412e-04 - 9s/epoch - 110us/sample
Epoch 57/90
84077/84077 - 9s - loss: 2.7122e-04 - val_loss: 2.5372e-04 - 9s/epoch - 110us/sample
Epoch 58/90
84077/84077 - 9s - loss: 2.6771e-04 - val_loss: 2.5152e-04 - 9s/epoch - 110us/sample
Epoch 59/90
84077/84077 - 9s - loss: 2.6566e-04 - val_loss: 2.5004e-04 - 9s/epoch - 110us/sample
Epoch 60/90
84077/84077 - 9s - loss: 2.6341e-04 - val_loss: 2.4647e-04 - 9s/epoch - 112us/sample
Epoch 61/90
84077/84077 - 9s - loss: 2.6183e-04 - val_loss: 2.4685e-04 - 9s/epoch - 110us/sample
Epoch 62/90
84077/84077 - 9s - loss: 2.6116e-04 - val_loss: 2.4419e-04 - 9s/epoch - 110us/sample
Epoch 63/90
84077/84077 - 9s - loss: 2.5965e-04 - val_loss: 2.4461e-04 - 9s/epoch - 110us/sample
Epoch 64/90
84077/84077 - 9s - loss: 2.5887e-04 - val_loss: 2.4438e-04 - 9s/epoch - 110us/sample
Epoch 65/90
84077/84077 - 9s - loss: 2.5730e-04 - val_loss: 2.4322e-04 - 9s/epoch - 112us/sample
Epoch 66/90
84077/84077 - 9s - loss: 2.5648e-04 - val_loss: 2.4153e-04 - 9s/epoch - 110us/sample
Epoch 67/90
84077/84077 - 9s - loss: 2.5525e-04 - val_loss: 2.4162e-04 - 9s/epoch - 110us/sample
Epoch 68/90
84077/84077 - 9s - loss: 2.5442e-04 - val_loss: 2.4074e-04 - 9s/epoch - 110us/sample
Epoch 69/90
84077/84077 - 9s - loss: 2.5299e-04 - val_loss: 2.3999e-04 - 9s/epoch - 110us/sample
Epoch 70/90
84077/84077 - 9s - loss: 2.5337e-04 - val_loss: 2.3982e-04 - 9s/epoch - 111us/sample
Epoch 71/90
84077/84077 - 9s - loss: 2.5200e-04 - val_loss: 2.3769e-04 - 9s/epoch - 111us/sample
Epoch 72/90
84077/84077 - 9s - loss: 2.5093e-04 - val_loss: 2.3944e-04 - 9s/epoch - 110us/sample
Epoch 73/90
84077/84077 - 9s - loss: 2.5062e-04 - val_loss: 2.3868e-04 - 9s/epoch - 110us/sample
Epoch 74/90
84077/84077 - 9s - loss: 2.4915e-04 - val_loss: 2.3577e-04 - 9s/epoch - 110us/sample
Epoch 75/90
84077/84077 - 9s - loss: 2.4824e-04 - val_loss: 2.3499e-04 - 9s/epoch - 110us/sample
Epoch 76/90
84077/84077 - 9s - loss: 2.4778e-04 - val_loss: 2.3506e-04 - 9s/epoch - 111us/sample
Epoch 77/90
84077/84077 - 9s - loss: 2.4670e-04 - val_loss: 2.3420e-04 - 9s/epoch - 110us/sample
Epoch 78/90
84077/84077 - 9s - loss: 2.4600e-04 - val_loss: 2.3432e-04 - 9s/epoch - 110us/sample
Epoch 79/90
84077/84077 - 9s - loss: 2.4465e-04 - val_loss: 2.3247e-04 - 9s/epoch - 110us/sample
Epoch 80/90
84077/84077 - 9s - loss: 2.4418e-04 - val_loss: 2.3046e-04 - 9s/epoch - 110us/sample
Epoch 81/90
84077/84077 - 9s - loss: 2.4290e-04 - val_loss: 2.3167e-04 - 9s/epoch - 111us/sample
Epoch 82/90
84077/84077 - 9s - loss: 2.4226e-04 - val_loss: 2.3105e-04 - 9s/epoch - 110us/sample
Epoch 83/90
84077/84077 - 9s - loss: 2.4164e-04 - val_loss: 2.2979e-04 - 9s/epoch - 110us/sample
Epoch 84/90
84077/84077 - 9s - loss: 2.4209e-04 - val_loss: 2.2867e-04 - 9s/epoch - 110us/sample
Epoch 85/90
84077/84077 - 9s - loss: 2.4092e-04 - val_loss: 2.2905e-04 - 9s/epoch - 110us/sample
Epoch 86/90
84077/84077 - 9s - loss: 2.3999e-04 - val_loss: 2.2798e-04 - 9s/epoch - 111us/sample
Epoch 87/90
84077/84077 - 9s - loss: 2.3897e-04 - val_loss: 2.2793e-04 - 9s/epoch - 110us/sample
Epoch 88/90
84077/84077 - 9s - loss: 2.3865e-04 - val_loss: 2.2682e-04 - 9s/epoch - 110us/sample
Epoch 89/90
84077/84077 - 9s - loss: 2.3803e-04 - val_loss: 2.2721e-04 - 9s/epoch - 110us/sample
Epoch 90/90
84077/84077 - 9s - loss: 2.3832e-04 - val_loss: 2.2591e-04 - 9s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.00022590518907443573
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:08:56.828578: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:21003 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04119507947624215
cosine 0.04067808529201097
MAE: 0.002547308201046717
RMSE: 0.012751413886162048
r2: 0.8730133952819017
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.0012000000000000001, 0.4, 377, 0.00023832439950566953, 0.00022590518907443573, 0.04119507947624215, 0.04067808529201097, 0.002547308201046717, 0.012751413886162048, 0.8730133952819017, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 621.7719243683831
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 621.7719243683831.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [621.7719243683831]
[1.5 85 0.001 32 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 1414)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          533455      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          533455      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2018507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,425,889
Trainable params: 4,419,479
Non-trainable params: 6,410
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 10:09:08.785310: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_51/beta/Assign' id:21879 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_51/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_51/beta, batch_normalization_51/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:09:24.212759: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:22287 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0111 - val_loss: 0.0020 - 19s/epoch - 224us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0018 - val_loss: 0.0012 - 14s/epoch - 166us/sample
Epoch 3/85
84077/84077 - 14s - loss: 0.0012 - val_loss: 7.6692e-04 - 14s/epoch - 167us/sample
Epoch 4/85
84077/84077 - 14s - loss: 7.5974e-04 - val_loss: 7.4400e-04 - 14s/epoch - 169us/sample
Epoch 5/85
84077/84077 - 14s - loss: 6.6940e-04 - val_loss: 4.6509e-04 - 14s/epoch - 167us/sample
Epoch 6/85
84077/84077 - 14s - loss: 4.7342e-04 - val_loss: 3.9055e-04 - 14s/epoch - 166us/sample
Epoch 7/85
84077/84077 - 14s - loss: 3.9484e-04 - val_loss: 3.4169e-04 - 14s/epoch - 168us/sample
Epoch 8/85
84077/84077 - 14s - loss: 3.4501e-04 - val_loss: 2.9830e-04 - 14s/epoch - 167us/sample
Epoch 9/85
84077/84077 - 14s - loss: 3.1015e-04 - val_loss: 2.7062e-04 - 14s/epoch - 167us/sample
Epoch 10/85
84077/84077 - 14s - loss: 2.8588e-04 - val_loss: 2.4726e-04 - 14s/epoch - 166us/sample
Epoch 11/85
84077/84077 - 14s - loss: 2.6560e-04 - val_loss: 2.3875e-04 - 14s/epoch - 168us/sample
Epoch 12/85
84077/84077 - 14s - loss: 2.5188e-04 - val_loss: 2.2560e-04 - 14s/epoch - 167us/sample
Epoch 13/85
84077/84077 - 14s - loss: 2.4023e-04 - val_loss: 2.1779e-04 - 14s/epoch - 167us/sample
Epoch 14/85
84077/84077 - 14s - loss: 2.3220e-04 - val_loss: 2.1663e-04 - 14s/epoch - 168us/sample
Epoch 15/85
84077/84077 - 14s - loss: 2.2466e-04 - val_loss: 2.0626e-04 - 14s/epoch - 167us/sample
Epoch 16/85
84077/84077 - 14s - loss: 2.1934e-04 - val_loss: 1.9963e-04 - 14s/epoch - 166us/sample
Epoch 17/85
84077/84077 - 14s - loss: 2.1246e-04 - val_loss: 1.9423e-04 - 14s/epoch - 166us/sample
Epoch 18/85
84077/84077 - 14s - loss: 2.0793e-04 - val_loss: 1.9070e-04 - 14s/epoch - 168us/sample
Epoch 19/85
84077/84077 - 14s - loss: 2.0530e-04 - val_loss: 1.8694e-04 - 14s/epoch - 167us/sample
Epoch 20/85
84077/84077 - 14s - loss: 2.0108e-04 - val_loss: 1.8785e-04 - 14s/epoch - 167us/sample
Epoch 21/85
84077/84077 - 14s - loss: 1.9754e-04 - val_loss: 1.8593e-04 - 14s/epoch - 167us/sample
Epoch 22/85
84077/84077 - 14s - loss: 1.9574e-04 - val_loss: 1.8064e-04 - 14s/epoch - 168us/sample
Epoch 23/85
84077/84077 - 14s - loss: 1.9245e-04 - val_loss: 1.7923e-04 - 14s/epoch - 167us/sample
Epoch 24/85
84077/84077 - 14s - loss: 1.9190e-04 - val_loss: 1.7625e-04 - 14s/epoch - 167us/sample
Epoch 25/85
84077/84077 - 14s - loss: 1.8892e-04 - val_loss: 1.7505e-04 - 14s/epoch - 168us/sample
Epoch 26/85
84077/84077 - 14s - loss: 1.8694e-04 - val_loss: 1.7518e-04 - 14s/epoch - 167us/sample
Epoch 27/85
84077/84077 - 14s - loss: 1.8357e-04 - val_loss: 1.7362e-04 - 14s/epoch - 167us/sample
Epoch 28/85
84077/84077 - 14s - loss: 1.8182e-04 - val_loss: 1.7003e-04 - 14s/epoch - 167us/sample
Epoch 29/85
84077/84077 - 14s - loss: 1.8067e-04 - val_loss: 1.7296e-04 - 14s/epoch - 169us/sample
Epoch 30/85
84077/84077 - 14s - loss: 1.8213e-04 - val_loss: 1.7541e-04 - 14s/epoch - 167us/sample
Epoch 31/85
84077/84077 - 14s - loss: 1.7924e-04 - val_loss: 1.6813e-04 - 14s/epoch - 167us/sample
Epoch 32/85
84077/84077 - 14s - loss: 1.7748e-04 - val_loss: 1.6469e-04 - 14s/epoch - 167us/sample
Epoch 33/85
84077/84077 - 14s - loss: 1.7541e-04 - val_loss: 1.6662e-04 - 14s/epoch - 168us/sample
Epoch 34/85
84077/84077 - 14s - loss: 1.7454e-04 - val_loss: 1.6497e-04 - 14s/epoch - 167us/sample
Epoch 35/85
84077/84077 - 14s - loss: 1.7335e-04 - val_loss: 1.6261e-04 - 14s/epoch - 167us/sample
Epoch 36/85
84077/84077 - 14s - loss: 1.7399e-04 - val_loss: 1.6385e-04 - 14s/epoch - 167us/sample
Epoch 37/85
84077/84077 - 14s - loss: 1.7278e-04 - val_loss: 1.6034e-04 - 14s/epoch - 168us/sample
Epoch 38/85
84077/84077 - 14s - loss: 1.7039e-04 - val_loss: 1.5883e-04 - 14s/epoch - 167us/sample
Epoch 39/85
84077/84077 - 14s - loss: 1.6955e-04 - val_loss: 1.6037e-04 - 14s/epoch - 167us/sample
Epoch 40/85
84077/84077 - 14s - loss: 1.6849e-04 - val_loss: 1.5870e-04 - 14s/epoch - 169us/sample
Epoch 41/85
84077/84077 - 14s - loss: 1.6839e-04 - val_loss: 1.5619e-04 - 14s/epoch - 167us/sample
Epoch 42/85
84077/84077 - 14s - loss: 1.6903e-04 - val_loss: 1.5630e-04 - 14s/epoch - 167us/sample
Epoch 43/85
84077/84077 - 14s - loss: 1.6681e-04 - val_loss: 1.5685e-04 - 14s/epoch - 168us/sample
Epoch 44/85
84077/84077 - 14s - loss: 1.6611e-04 - val_loss: 1.5771e-04 - 14s/epoch - 167us/sample
Epoch 45/85
84077/84077 - 14s - loss: 1.6649e-04 - val_loss: 1.5637e-04 - 14s/epoch - 166us/sample
Epoch 46/85
84077/84077 - 14s - loss: 1.6562e-04 - val_loss: 1.5785e-04 - 14s/epoch - 168us/sample
Epoch 47/85
84077/84077 - 14s - loss: 1.6364e-04 - val_loss: 1.5537e-04 - 14s/epoch - 168us/sample
Epoch 48/85
84077/84077 - 14s - loss: 1.6381e-04 - val_loss: 1.5470e-04 - 14s/epoch - 167us/sample
Epoch 49/85
84077/84077 - 14s - loss: 1.6361e-04 - val_loss: 1.5137e-04 - 14s/epoch - 167us/sample
Epoch 50/85
84077/84077 - 14s - loss: 1.6246e-04 - val_loss: 1.5500e-04 - 14s/epoch - 168us/sample
Epoch 51/85
84077/84077 - 14s - loss: 1.6170e-04 - val_loss: 1.5318e-04 - 14s/epoch - 167us/sample
Epoch 52/85
84077/84077 - 14s - loss: 1.6327e-04 - val_loss: 1.5323e-04 - 14s/epoch - 166us/sample
Epoch 53/85
84077/84077 - 14s - loss: 1.6115e-04 - val_loss: 1.5105e-04 - 14s/epoch - 168us/sample
Epoch 54/85
84077/84077 - 14s - loss: 1.6121e-04 - val_loss: 1.5111e-04 - 14s/epoch - 167us/sample
Epoch 55/85
84077/84077 - 14s - loss: 1.5970e-04 - val_loss: 1.5329e-04 - 14s/epoch - 167us/sample
Epoch 56/85
84077/84077 - 14s - loss: 1.5968e-04 - val_loss: 1.4841e-04 - 14s/epoch - 168us/sample
Epoch 57/85
84077/84077 - 14s - loss: 1.5976e-04 - val_loss: 1.4873e-04 - 14s/epoch - 168us/sample
Epoch 58/85
84077/84077 - 14s - loss: 1.6100e-04 - val_loss: 1.4987e-04 - 14s/epoch - 167us/sample
Epoch 59/85
84077/84077 - 14s - loss: 1.5842e-04 - val_loss: 1.5124e-04 - 14s/epoch - 167us/sample
Epoch 60/85
84077/84077 - 14s - loss: 1.5821e-04 - val_loss: 1.4869e-04 - 14s/epoch - 168us/sample
Epoch 61/85
84077/84077 - 14s - loss: 1.5781e-04 - val_loss: 1.4779e-04 - 14s/epoch - 168us/sample
Epoch 62/85
84077/84077 - 14s - loss: 1.5866e-04 - val_loss: 1.5065e-04 - 14s/epoch - 166us/sample
Epoch 63/85
84077/84077 - 14s - loss: 1.5674e-04 - val_loss: 1.4884e-04 - 14s/epoch - 169us/sample
Epoch 64/85
84077/84077 - 14s - loss: 1.5688e-04 - val_loss: 1.4697e-04 - 14s/epoch - 167us/sample
Epoch 65/85
84077/84077 - 14s - loss: 1.5669e-04 - val_loss: 1.4806e-04 - 14s/epoch - 167us/sample
Epoch 66/85
84077/84077 - 14s - loss: 1.5670e-04 - val_loss: 1.4652e-04 - 14s/epoch - 167us/sample
Epoch 67/85
84077/84077 - 14s - loss: 1.5525e-04 - val_loss: 1.4896e-04 - 14s/epoch - 169us/sample
Epoch 68/85
84077/84077 - 14s - loss: 1.5542e-04 - val_loss: 1.4716e-04 - 14s/epoch - 167us/sample
Epoch 69/85
84077/84077 - 14s - loss: 1.5469e-04 - val_loss: 1.4554e-04 - 14s/epoch - 167us/sample
Epoch 70/85
84077/84077 - 14s - loss: 1.5539e-04 - val_loss: 1.4716e-04 - 14s/epoch - 168us/sample
Epoch 71/85
84077/84077 - 14s - loss: 1.5637e-04 - val_loss: 1.4690e-04 - 14s/epoch - 168us/sample
Epoch 72/85
84077/84077 - 14s - loss: 1.5579e-04 - val_loss: 1.4611e-04 - 14s/epoch - 167us/sample
Epoch 73/85
84077/84077 - 14s - loss: 1.5416e-04 - val_loss: 1.4507e-04 - 14s/epoch - 167us/sample
Epoch 74/85
84077/84077 - 14s - loss: 1.5415e-04 - val_loss: 1.4518e-04 - 14s/epoch - 168us/sample
Epoch 75/85
84077/84077 - 14s - loss: 1.5302e-04 - val_loss: 1.4595e-04 - 14s/epoch - 167us/sample
Epoch 76/85
84077/84077 - 14s - loss: 1.5344e-04 - val_loss: 1.4609e-04 - 14s/epoch - 167us/sample
Epoch 77/85
84077/84077 - 14s - loss: 1.5260e-04 - val_loss: 1.4577e-04 - 14s/epoch - 167us/sample
Epoch 78/85
84077/84077 - 14s - loss: 1.5153e-04 - val_loss: 1.4267e-04 - 14s/epoch - 168us/sample
Epoch 79/85
84077/84077 - 14s - loss: 1.5408e-04 - val_loss: 1.4308e-04 - 14s/epoch - 167us/sample
Epoch 80/85
84077/84077 - 14s - loss: 1.5380e-04 - val_loss: 1.4607e-04 - 14s/epoch - 167us/sample
Epoch 81/85
84077/84077 - 14s - loss: 1.5145e-04 - val_loss: 1.4539e-04 - 14s/epoch - 167us/sample
Epoch 82/85
84077/84077 - 14s - loss: 1.5274e-04 - val_loss: 1.4411e-04 - 14s/epoch - 168us/sample
Epoch 83/85
84077/84077 - 14s - loss: 1.5274e-04 - val_loss: 1.4464e-04 - 14s/epoch - 166us/sample
Epoch 84/85
84077/84077 - 14s - loss: 1.5114e-04 - val_loss: 1.4634e-04 - 14s/epoch - 167us/sample
Epoch 85/85
84077/84077 - 14s - loss: 1.5047e-04 - val_loss: 1.4436e-04 - 14s/epoch - 168us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.0001443609908531001
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:29:07.050324: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:22258 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01736774183555763
cosine 0.01715249059677078
MAE: 0.002103286196467305
RMSE: 0.00839267124589885
r2: 0.945190560751048
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 32, 85, 0.001, 0.4, 377, 0.0001504660205588585, 0.0001443609908531001, 0.01736774183555763, 0.01715249059677078, 0.002103286196467305, 0.00839267124589885, 0.945190560751048, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.0005 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 1414)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          533455      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          533455      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2018507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,425,889
Trainable params: 4,419,479
Non-trainable params: 6,410
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 10:29:19.143667: W tensorflow/c/c_api.cc:291] Operation '{name:'training_36/Adam/dense_dec0_18/bias/v/Assign' id:24190 op device:{requested: '', assigned: ''} def:{{{node training_36/Adam/dense_dec0_18/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_36/Adam/dense_dec0_18/bias/v, training_36/Adam/dense_dec0_18/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:29:34.716613: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_18/mul' id:23542 op device:{requested: '', assigned: ''} def:{{{node loss_18/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_18/mul/x, loss_18/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0052 - val_loss: 0.0016 - 19s/epoch - 228us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0017 - val_loss: 0.0014 - 14s/epoch - 168us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0010 - val_loss: 7.1449e-04 - 14s/epoch - 169us/sample
Epoch 4/90
84077/84077 - 14s - loss: 6.9571e-04 - val_loss: 5.4185e-04 - 14s/epoch - 168us/sample
Epoch 5/90
84077/84077 - 14s - loss: 5.3632e-04 - val_loss: 4.2672e-04 - 14s/epoch - 168us/sample
Epoch 6/90
84077/84077 - 14s - loss: 4.2676e-04 - val_loss: 3.6301e-04 - 14s/epoch - 168us/sample
Epoch 7/90
84077/84077 - 14s - loss: 3.5859e-04 - val_loss: 3.0159e-04 - 14s/epoch - 169us/sample
Epoch 8/90
84077/84077 - 14s - loss: 3.2356e-04 - val_loss: 2.8601e-04 - 14s/epoch - 169us/sample
Epoch 9/90
84077/84077 - 14s - loss: 2.9142e-04 - val_loss: 2.5604e-04 - 14s/epoch - 168us/sample
Epoch 10/90
84077/84077 - 14s - loss: 2.7151e-04 - val_loss: 2.4496e-04 - 14s/epoch - 168us/sample
Epoch 11/90
84077/84077 - 14s - loss: 2.5612e-04 - val_loss: 2.3572e-04 - 14s/epoch - 169us/sample
Epoch 12/90
84077/84077 - 14s - loss: 2.4475e-04 - val_loss: 2.2406e-04 - 14s/epoch - 169us/sample
Epoch 13/90
84077/84077 - 14s - loss: 2.3456e-04 - val_loss: 2.1014e-04 - 14s/epoch - 168us/sample
Epoch 14/90
84077/84077 - 14s - loss: 2.2764e-04 - val_loss: 2.0562e-04 - 14s/epoch - 168us/sample
Epoch 15/90
84077/84077 - 15s - loss: 2.2168e-04 - val_loss: 1.9690e-04 - 15s/epoch - 174us/sample
Epoch 16/90
84077/84077 - 14s - loss: 2.1404e-04 - val_loss: 1.9714e-04 - 14s/epoch - 168us/sample
Epoch 17/90
84077/84077 - 14s - loss: 2.1043e-04 - val_loss: 1.9080e-04 - 14s/epoch - 168us/sample
Epoch 18/90
84077/84077 - 14s - loss: 2.0475e-04 - val_loss: 1.8839e-04 - 14s/epoch - 169us/sample
Epoch 19/90
84077/84077 - 14s - loss: 2.0230e-04 - val_loss: 1.8754e-04 - 14s/epoch - 168us/sample
Epoch 20/90
84077/84077 - 14s - loss: 1.9753e-04 - val_loss: 1.8358e-04 - 14s/epoch - 168us/sample
Epoch 21/90
84077/84077 - 14s - loss: 1.9649e-04 - val_loss: 1.8097e-04 - 14s/epoch - 168us/sample
Epoch 22/90
84077/84077 - 14s - loss: 1.9260e-04 - val_loss: 1.7998e-04 - 14s/epoch - 169us/sample
Epoch 23/90
84077/84077 - 14s - loss: 1.9021e-04 - val_loss: 1.7905e-04 - 14s/epoch - 168us/sample
Epoch 24/90
84077/84077 - 14s - loss: 1.8945e-04 - val_loss: 1.7452e-04 - 14s/epoch - 169us/sample
Epoch 25/90
84077/84077 - 14s - loss: 1.8640e-04 - val_loss: 1.7356e-04 - 14s/epoch - 168us/sample
Epoch 26/90
84077/84077 - 14s - loss: 1.8438e-04 - val_loss: 1.7525e-04 - 14s/epoch - 170us/sample
Epoch 27/90
84077/84077 - 14s - loss: 1.8292e-04 - val_loss: 1.7562e-04 - 14s/epoch - 168us/sample
Epoch 28/90
84077/84077 - 14s - loss: 1.8115e-04 - val_loss: 1.6781e-04 - 14s/epoch - 168us/sample
Epoch 29/90
84077/84077 - 14s - loss: 1.7877e-04 - val_loss: 1.6980e-04 - 14s/epoch - 168us/sample
Epoch 30/90
84077/84077 - 14s - loss: 1.7911e-04 - val_loss: 1.6703e-04 - 14s/epoch - 169us/sample
Epoch 31/90
84077/84077 - 14s - loss: 1.7631e-04 - val_loss: 1.6565e-04 - 14s/epoch - 168us/sample
Epoch 32/90
84077/84077 - 14s - loss: 1.7522e-04 - val_loss: 1.7672e-04 - 14s/epoch - 168us/sample
Epoch 33/90
84077/84077 - 14s - loss: 1.7365e-04 - val_loss: 1.6279e-04 - 14s/epoch - 168us/sample
Epoch 34/90
84077/84077 - 14s - loss: 1.7301e-04 - val_loss: 1.6066e-04 - 14s/epoch - 169us/sample
Epoch 35/90
84077/84077 - 14s - loss: 1.7101e-04 - val_loss: 1.7092e-04 - 14s/epoch - 168us/sample
Epoch 36/90
84077/84077 - 14s - loss: 1.7196e-04 - val_loss: 1.6302e-04 - 14s/epoch - 168us/sample
Epoch 37/90
84077/84077 - 14s - loss: 1.6939e-04 - val_loss: 1.6151e-04 - 14s/epoch - 168us/sample
Epoch 38/90
84077/84077 - 14s - loss: 1.6772e-04 - val_loss: 1.5842e-04 - 14s/epoch - 169us/sample
Epoch 39/90
84077/84077 - 14s - loss: 1.6680e-04 - val_loss: 1.5653e-04 - 14s/epoch - 168us/sample
Epoch 40/90
84077/84077 - 14s - loss: 1.6571e-04 - val_loss: 1.5620e-04 - 14s/epoch - 168us/sample
Epoch 41/90
84077/84077 - 14s - loss: 1.6710e-04 - val_loss: 1.5731e-04 - 14s/epoch - 169us/sample
Epoch 42/90
84077/84077 - 14s - loss: 1.6528e-04 - val_loss: 1.5589e-04 - 14s/epoch - 169us/sample
Epoch 43/90
84077/84077 - 14s - loss: 1.6425e-04 - val_loss: 1.5760e-04 - 14s/epoch - 168us/sample
Epoch 44/90
84077/84077 - 14s - loss: 1.6312e-04 - val_loss: 1.5420e-04 - 14s/epoch - 168us/sample
Epoch 45/90
84077/84077 - 14s - loss: 1.6233e-04 - val_loss: 1.5958e-04 - 14s/epoch - 170us/sample
Epoch 46/90
84077/84077 - 14s - loss: 1.6160e-04 - val_loss: 1.5134e-04 - 14s/epoch - 168us/sample
Epoch 47/90
84077/84077 - 14s - loss: 1.6095e-04 - val_loss: 1.5281e-04 - 14s/epoch - 168us/sample
Epoch 48/90
84077/84077 - 14s - loss: 1.6073e-04 - val_loss: 1.5376e-04 - 14s/epoch - 169us/sample
Epoch 49/90
84077/84077 - 14s - loss: 1.6051e-04 - val_loss: 1.5322e-04 - 14s/epoch - 169us/sample
Epoch 50/90
84077/84077 - 14s - loss: 1.5911e-04 - val_loss: 1.5243e-04 - 14s/epoch - 168us/sample
Epoch 51/90
84077/84077 - 14s - loss: 1.6087e-04 - val_loss: 1.5364e-04 - 14s/epoch - 168us/sample
Epoch 52/90
84077/84077 - 14s - loss: 1.5846e-04 - val_loss: 1.6553e-04 - 14s/epoch - 170us/sample
Epoch 53/90
84077/84077 - 14s - loss: 1.5783e-04 - val_loss: 1.5049e-04 - 14s/epoch - 168us/sample
Epoch 54/90
84077/84077 - 14s - loss: 1.5811e-04 - val_loss: 1.5154e-04 - 14s/epoch - 168us/sample
Epoch 55/90
84077/84077 - 14s - loss: 1.5673e-04 - val_loss: 1.5488e-04 - 14s/epoch - 170us/sample
Epoch 56/90
84077/84077 - 14s - loss: 1.5727e-04 - val_loss: 1.4886e-04 - 14s/epoch - 168us/sample
Epoch 57/90
84077/84077 - 14s - loss: 1.5487e-04 - val_loss: 1.4934e-04 - 14s/epoch - 168us/sample
Epoch 58/90
84077/84077 - 14s - loss: 1.5410e-04 - val_loss: 1.4857e-04 - 14s/epoch - 169us/sample
Epoch 59/90
84077/84077 - 14s - loss: 1.5514e-04 - val_loss: 1.4969e-04 - 14s/epoch - 169us/sample
Epoch 60/90
84077/84077 - 14s - loss: 1.5752e-04 - val_loss: 1.5093e-04 - 14s/epoch - 168us/sample
Epoch 61/90
84077/84077 - 14s - loss: 1.5413e-04 - val_loss: 1.4667e-04 - 14s/epoch - 168us/sample
Epoch 62/90
84077/84077 - 14s - loss: 1.5619e-04 - val_loss: 1.4627e-04 - 14s/epoch - 169us/sample
Epoch 63/90
84077/84077 - 14s - loss: 1.5295e-04 - val_loss: 1.4800e-04 - 14s/epoch - 168us/sample
Epoch 64/90
84077/84077 - 14s - loss: 1.5289e-04 - val_loss: 1.4803e-04 - 14s/epoch - 168us/sample
Epoch 65/90
84077/84077 - 14s - loss: 1.5232e-04 - val_loss: 1.4595e-04 - 14s/epoch - 168us/sample
Epoch 66/90
84077/84077 - 14s - loss: 1.5135e-04 - val_loss: 1.4749e-04 - 14s/epoch - 170us/sample
Epoch 67/90
84077/84077 - 14s - loss: 1.5376e-04 - val_loss: 1.5043e-04 - 14s/epoch - 168us/sample
Epoch 68/90
84077/84077 - 14s - loss: 1.5054e-04 - val_loss: 1.4875e-04 - 14s/epoch - 168us/sample
Epoch 69/90
84077/84077 - 14s - loss: 1.5126e-04 - val_loss: 1.4670e-04 - 14s/epoch - 168us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.5169e-04 - val_loss: 1.4725e-04 - 14s/epoch - 170us/sample
Epoch 71/90
84077/84077 - 14s - loss: 1.4950e-04 - val_loss: 1.4680e-04 - 14s/epoch - 168us/sample
Epoch 72/90
84077/84077 - 14s - loss: 1.4892e-04 - val_loss: 1.4496e-04 - 14s/epoch - 168us/sample
Epoch 73/90
84077/84077 - 14s - loss: 1.4982e-04 - val_loss: 1.4526e-04 - 14s/epoch - 169us/sample
Epoch 74/90
84077/84077 - 14s - loss: 1.4937e-04 - val_loss: 1.4757e-04 - 14s/epoch - 169us/sample
Epoch 75/90
84077/84077 - 14s - loss: 1.4863e-04 - val_loss: 1.4442e-04 - 14s/epoch - 168us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.4866e-04 - val_loss: 1.4580e-04 - 14s/epoch - 168us/sample
Epoch 77/90
84077/84077 - 14s - loss: 1.4787e-04 - val_loss: 1.4401e-04 - 14s/epoch - 169us/sample
Epoch 78/90
84077/84077 - 14s - loss: 1.4855e-04 - val_loss: 1.4324e-04 - 14s/epoch - 168us/sample
Epoch 79/90
84077/84077 - 14s - loss: 1.4661e-04 - val_loss: 1.4294e-04 - 14s/epoch - 168us/sample
Epoch 80/90
84077/84077 - 14s - loss: 1.4712e-04 - val_loss: 1.4284e-04 - 14s/epoch - 169us/sample
Epoch 81/90
84077/84077 - 14s - loss: 1.4614e-04 - val_loss: 1.4180e-04 - 14s/epoch - 168us/sample
Epoch 82/90
84077/84077 - 14s - loss: 1.4594e-04 - val_loss: 1.4151e-04 - 14s/epoch - 168us/sample
Epoch 83/90
84077/84077 - 14s - loss: 1.4605e-04 - val_loss: 1.4170e-04 - 14s/epoch - 169us/sample
Epoch 84/90
84077/84077 - 14s - loss: 1.4607e-04 - val_loss: 1.4264e-04 - 14s/epoch - 169us/sample
Epoch 85/90
84077/84077 - 14s - loss: 1.4584e-04 - val_loss: 1.4267e-04 - 14s/epoch - 168us/sample
Epoch 86/90
84077/84077 - 14s - loss: 1.4454e-04 - val_loss: 1.4274e-04 - 14s/epoch - 168us/sample
Epoch 87/90
84077/84077 - 14s - loss: 1.4520e-04 - val_loss: 1.4101e-04 - 14s/epoch - 169us/sample
Epoch 88/90
84077/84077 - 14s - loss: 1.4515e-04 - val_loss: 1.4073e-04 - 14s/epoch - 170us/sample
Epoch 89/90
84077/84077 - 14s - loss: 1.4411e-04 - val_loss: 1.4274e-04 - 14s/epoch - 168us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.4380e-04 - val_loss: 1.3993e-04 - 14s/epoch - 168us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 0.000139928071665831
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:50:37.436754: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_18/outputlayer/BiasAdd' id:23513 op device:{requested: '', assigned: ''} def:{{{node decoder_model_18/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_18/outputlayer/MatMul, decoder_model_18/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.016441249885040112
cosine 0.016238463222703895
MAE: 0.001795825768016168
RMSE: 0.008240006203794606
r2: 0.947123053243222
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 32, 90, 0.0005, 0.4, 377, 0.00014379762576242517, 0.000139928071665831, 0.016441249885040112, 0.016238463222703895, 0.001795825768016168, 0.008240006203794606, 0.947123053243222, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 1980)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          746837      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          746837      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2768457     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,139,171
Trainable params: 6,130,497
Non-trainable params: 8,674
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 10:50:49.936791: W tensorflow/c/c_api.cc:291] Operation '{name:'training_38/Adam/dense_dec0_19/kernel/v/Assign' id:25468 op device:{requested: '', assigned: ''} def:{{{node training_38/Adam/dense_dec0_19/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_38/Adam/dense_dec0_19/kernel/v, training_38/Adam/dense_dec0_19/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:51:01.416246: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_19/mul' id:24804 op device:{requested: '', assigned: ''} def:{{{node loss_19/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_19/mul/x, loss_19/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0041 - val_loss: 9.3311e-04 - 15s/epoch - 179us/sample
Epoch 2/90
84077/84077 - 10s - loss: 9.6797e-04 - val_loss: 0.0040 - 10s/epoch - 114us/sample
Epoch 3/90
84077/84077 - 10s - loss: 0.0018 - val_loss: 9.3614e-04 - 10s/epoch - 114us/sample
Epoch 4/90
84077/84077 - 10s - loss: 8.0724e-04 - val_loss: 5.1652e-04 - 10s/epoch - 114us/sample
Epoch 5/90
84077/84077 - 10s - loss: 4.8946e-04 - val_loss: 3.9809e-04 - 10s/epoch - 114us/sample
Epoch 6/90
84077/84077 - 10s - loss: 5.4305e-04 - val_loss: 4.2983e-04 - 10s/epoch - 116us/sample
Epoch 7/90
84077/84077 - 10s - loss: 4.0106e-04 - val_loss: 3.2516e-04 - 10s/epoch - 115us/sample
Epoch 8/90
84077/84077 - 10s - loss: 3.5044e-04 - val_loss: 3.0821e-04 - 10s/epoch - 114us/sample
Epoch 9/90
84077/84077 - 10s - loss: 3.1428e-04 - val_loss: 2.6006e-04 - 10s/epoch - 114us/sample
Epoch 10/90
84077/84077 - 10s - loss: 2.7556e-04 - val_loss: 2.3794e-04 - 10s/epoch - 114us/sample
Epoch 11/90
84077/84077 - 10s - loss: 2.4653e-04 - val_loss: 2.0375e-04 - 10s/epoch - 116us/sample
Epoch 12/90
84077/84077 - 10s - loss: 2.3005e-04 - val_loss: 1.9843e-04 - 10s/epoch - 115us/sample
Epoch 13/90
84077/84077 - 10s - loss: 2.0389e-04 - val_loss: 1.6640e-04 - 10s/epoch - 114us/sample
Epoch 14/90
84077/84077 - 10s - loss: 1.9220e-04 - val_loss: 1.5921e-04 - 10s/epoch - 114us/sample
Epoch 15/90
84077/84077 - 10s - loss: 1.7729e-04 - val_loss: 1.4835e-04 - 10s/epoch - 114us/sample
Epoch 16/90
84077/84077 - 10s - loss: 1.6842e-04 - val_loss: 1.4138e-04 - 10s/epoch - 114us/sample
Epoch 17/90
84077/84077 - 10s - loss: 1.5909e-04 - val_loss: 1.3309e-04 - 10s/epoch - 115us/sample
Epoch 18/90
84077/84077 - 10s - loss: 1.5302e-04 - val_loss: 1.2800e-04 - 10s/epoch - 115us/sample
Epoch 19/90
84077/84077 - 10s - loss: 1.4732e-04 - val_loss: 1.2769e-04 - 10s/epoch - 114us/sample
Epoch 20/90
84077/84077 - 10s - loss: 1.4316e-04 - val_loss: 1.2155e-04 - 10s/epoch - 114us/sample
Epoch 21/90
84077/84077 - 10s - loss: 1.3859e-04 - val_loss: 1.1892e-04 - 10s/epoch - 114us/sample
Epoch 22/90
84077/84077 - 10s - loss: 1.3508e-04 - val_loss: 1.1648e-04 - 10s/epoch - 115us/sample
Epoch 23/90
84077/84077 - 10s - loss: 1.3295e-04 - val_loss: 1.1579e-04 - 10s/epoch - 115us/sample
Epoch 24/90
84077/84077 - 10s - loss: 1.3047e-04 - val_loss: 1.1416e-04 - 10s/epoch - 114us/sample
Epoch 25/90
84077/84077 - 10s - loss: 1.2716e-04 - val_loss: 1.1176e-04 - 10s/epoch - 114us/sample
Epoch 26/90
84077/84077 - 10s - loss: 1.2638e-04 - val_loss: 1.1045e-04 - 10s/epoch - 114us/sample
Epoch 27/90
84077/84077 - 10s - loss: 1.2378e-04 - val_loss: 1.0867e-04 - 10s/epoch - 115us/sample
Epoch 28/90
84077/84077 - 10s - loss: 1.2214e-04 - val_loss: 1.0638e-04 - 10s/epoch - 115us/sample
Epoch 29/90
84077/84077 - 10s - loss: 1.2014e-04 - val_loss: 1.0647e-04 - 10s/epoch - 114us/sample
Epoch 30/90
84077/84077 - 10s - loss: 1.1900e-04 - val_loss: 1.0532e-04 - 10s/epoch - 114us/sample
Epoch 31/90
84077/84077 - 10s - loss: 1.1797e-04 - val_loss: 1.0464e-04 - 10s/epoch - 115us/sample
Epoch 32/90
84077/84077 - 10s - loss: 1.1615e-04 - val_loss: 1.0504e-04 - 10s/epoch - 115us/sample
Epoch 33/90
84077/84077 - 10s - loss: 1.1484e-04 - val_loss: 1.0168e-04 - 10s/epoch - 115us/sample
Epoch 34/90
84077/84077 - 10s - loss: 1.1450e-04 - val_loss: 1.1008e-04 - 10s/epoch - 115us/sample
Epoch 35/90
84077/84077 - 10s - loss: 1.1393e-04 - val_loss: 1.0147e-04 - 10s/epoch - 114us/sample
Epoch 36/90
84077/84077 - 10s - loss: 1.1254e-04 - val_loss: 1.0185e-04 - 10s/epoch - 114us/sample
Epoch 37/90
84077/84077 - 10s - loss: 1.1304e-04 - val_loss: 1.0228e-04 - 10s/epoch - 114us/sample
Epoch 38/90
84077/84077 - 10s - loss: 1.1110e-04 - val_loss: 9.9974e-05 - 10s/epoch - 115us/sample
Epoch 39/90
84077/84077 - 10s - loss: 1.1001e-04 - val_loss: 9.8880e-05 - 10s/epoch - 115us/sample
Epoch 40/90
84077/84077 - 10s - loss: 1.0909e-04 - val_loss: 9.8770e-05 - 10s/epoch - 115us/sample
Epoch 41/90
84077/84077 - 10s - loss: 1.0831e-04 - val_loss: 9.7258e-05 - 10s/epoch - 114us/sample
Epoch 42/90
84077/84077 - 10s - loss: 1.0854e-04 - val_loss: 9.6998e-05 - 10s/epoch - 114us/sample
Epoch 43/90
84077/84077 - 10s - loss: 1.0734e-04 - val_loss: 9.7746e-05 - 10s/epoch - 114us/sample
Epoch 44/90
84077/84077 - 10s - loss: 1.0659e-04 - val_loss: 9.6730e-05 - 10s/epoch - 116us/sample
Epoch 45/90
84077/84077 - 10s - loss: 1.0703e-04 - val_loss: 9.5556e-05 - 10s/epoch - 115us/sample
Epoch 46/90
84077/84077 - 10s - loss: 1.0542e-04 - val_loss: 9.5955e-05 - 10s/epoch - 114us/sample
Epoch 47/90
84077/84077 - 10s - loss: 1.0485e-04 - val_loss: 9.5652e-05 - 10s/epoch - 114us/sample
Epoch 48/90
84077/84077 - 10s - loss: 1.0482e-04 - val_loss: 9.4579e-05 - 10s/epoch - 114us/sample
Epoch 49/90
84077/84077 - 10s - loss: 1.0551e-04 - val_loss: 9.4376e-05 - 10s/epoch - 115us/sample
Epoch 50/90
84077/84077 - 10s - loss: 1.0340e-04 - val_loss: 9.4876e-05 - 10s/epoch - 115us/sample
Epoch 51/90
84077/84077 - 10s - loss: 1.0282e-04 - val_loss: 9.3148e-05 - 10s/epoch - 114us/sample
Epoch 52/90
84077/84077 - 10s - loss: 1.0362e-04 - val_loss: 9.3818e-05 - 10s/epoch - 114us/sample
Epoch 53/90
84077/84077 - 10s - loss: 1.0217e-04 - val_loss: 9.2441e-05 - 10s/epoch - 114us/sample
Epoch 54/90
84077/84077 - 10s - loss: 1.0174e-04 - val_loss: 9.4491e-05 - 10s/epoch - 115us/sample
Epoch 55/90
84077/84077 - 10s - loss: 1.0175e-04 - val_loss: 9.1967e-05 - 10s/epoch - 116us/sample
Epoch 56/90
84077/84077 - 10s - loss: 1.0108e-04 - val_loss: 9.2686e-05 - 10s/epoch - 115us/sample
Epoch 57/90
84077/84077 - 10s - loss: 1.0061e-04 - val_loss: 9.1310e-05 - 10s/epoch - 115us/sample
Epoch 58/90
84077/84077 - 10s - loss: 1.0132e-04 - val_loss: 9.8106e-05 - 10s/epoch - 114us/sample
Epoch 59/90
84077/84077 - 10s - loss: 1.0165e-04 - val_loss: 9.2430e-05 - 10s/epoch - 115us/sample
Epoch 60/90
84077/84077 - 10s - loss: 1.0072e-04 - val_loss: 9.2111e-05 - 10s/epoch - 115us/sample
Epoch 61/90
84077/84077 - 10s - loss: 9.9172e-05 - val_loss: 9.1595e-05 - 10s/epoch - 114us/sample
Epoch 62/90
84077/84077 - 10s - loss: 9.9577e-05 - val_loss: 9.0653e-05 - 10s/epoch - 114us/sample
Epoch 63/90
84077/84077 - 10s - loss: 9.8764e-05 - val_loss: 9.0452e-05 - 10s/epoch - 114us/sample
Epoch 64/90
84077/84077 - 10s - loss: 9.8111e-05 - val_loss: 8.9504e-05 - 10s/epoch - 114us/sample
Epoch 65/90
84077/84077 - 10s - loss: 9.8094e-05 - val_loss: 8.9260e-05 - 10s/epoch - 116us/sample
Epoch 66/90
84077/84077 - 10s - loss: 9.7554e-05 - val_loss: 8.9365e-05 - 10s/epoch - 115us/sample
Epoch 67/90
84077/84077 - 10s - loss: 9.7353e-05 - val_loss: 8.9722e-05 - 10s/epoch - 114us/sample
Epoch 68/90
84077/84077 - 10s - loss: 9.6722e-05 - val_loss: 8.9274e-05 - 10s/epoch - 114us/sample
Epoch 69/90
84077/84077 - 10s - loss: 9.6788e-05 - val_loss: 8.8837e-05 - 10s/epoch - 115us/sample
Epoch 70/90
84077/84077 - 10s - loss: 9.6829e-05 - val_loss: 9.2121e-05 - 10s/epoch - 116us/sample
Epoch 71/90
84077/84077 - 10s - loss: 9.5899e-05 - val_loss: 8.7962e-05 - 10s/epoch - 115us/sample
Epoch 72/90
84077/84077 - 10s - loss: 9.5776e-05 - val_loss: 8.8015e-05 - 10s/epoch - 115us/sample
Epoch 73/90
84077/84077 - 10s - loss: 9.5567e-05 - val_loss: 8.8535e-05 - 10s/epoch - 114us/sample
Epoch 74/90
84077/84077 - 10s - loss: 9.5337e-05 - val_loss: 8.7345e-05 - 10s/epoch - 114us/sample
Epoch 75/90
84077/84077 - 10s - loss: 9.5211e-05 - val_loss: 8.7675e-05 - 10s/epoch - 116us/sample
Epoch 76/90
84077/84077 - 10s - loss: 9.5548e-05 - val_loss: 8.8310e-05 - 10s/epoch - 115us/sample
Epoch 77/90
84077/84077 - 10s - loss: 9.5110e-05 - val_loss: 8.7817e-05 - 10s/epoch - 115us/sample
Epoch 78/90
84077/84077 - 10s - loss: 9.4396e-05 - val_loss: 8.7719e-05 - 10s/epoch - 114us/sample
Epoch 79/90
84077/84077 - 10s - loss: 9.4535e-05 - val_loss: 8.7574e-05 - 10s/epoch - 114us/sample
Epoch 80/90
84077/84077 - 10s - loss: 9.4349e-05 - val_loss: 9.0449e-05 - 10s/epoch - 115us/sample
Epoch 81/90
84077/84077 - 10s - loss: 9.4333e-05 - val_loss: 8.6595e-05 - 10s/epoch - 115us/sample
Epoch 82/90
84077/84077 - 10s - loss: 9.4304e-05 - val_loss: 8.7089e-05 - 10s/epoch - 115us/sample
Epoch 83/90
84077/84077 - 10s - loss: 9.3844e-05 - val_loss: 8.7272e-05 - 10s/epoch - 114us/sample
Epoch 84/90
84077/84077 - 10s - loss: 9.3598e-05 - val_loss: 8.5745e-05 - 10s/epoch - 114us/sample
Epoch 85/90
84077/84077 - 10s - loss: 9.3073e-05 - val_loss: 8.5817e-05 - 10s/epoch - 114us/sample
Epoch 86/90
84077/84077 - 10s - loss: 9.3878e-05 - val_loss: 8.6554e-05 - 10s/epoch - 115us/sample
Epoch 87/90
84077/84077 - 10s - loss: 9.2966e-05 - val_loss: 8.6131e-05 - 10s/epoch - 116us/sample
Epoch 88/90
84077/84077 - 10s - loss: 9.3642e-05 - val_loss: 8.6616e-05 - 10s/epoch - 115us/sample
Epoch 89/90
84077/84077 - 10s - loss: 9.2831e-05 - val_loss: 8.5237e-05 - 10s/epoch - 114us/sample
Epoch 90/90
84077/84077 - 10s - loss: 9.2315e-05 - val_loss: 8.5436e-05 - 10s/epoch - 114us/sample
COMPRESSED VECTOR SIZE: 377
Loss in the autoencoder: 8.543617825538091e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 11:05:21.189295: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:24768 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.020477218888997846
cosine 0.020228966792285027
MAE: 0.00200883875252014
RMSE: 0.0082346090425191
r2: 0.9472184853233774
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 90, 0.001, 0.4, 377, 9.231548742454155e-05, 8.543617825538091e-05, 0.020477218888997846, 0.020228966792285027, 0.00200883875252014, 0.0082346090425191, 0.9472184853233774, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0005 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 2074)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 377)          782275      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 377)          782275      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 377)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2893007     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,423,709
Trainable params: 6,414,659
Non-trainable params: 9,050
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 11:05:34.356588: W tensorflow/c/c_api.cc:291] Operation '{name:'training_40/Adam/batch_normalization_62/gamma/v/Assign' id:26740 op device:{requested: '', assigned: ''} def:{{{node training_40/Adam/batch_normalization_62/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_40/Adam/batch_normalization_62/gamma/v, training_40/Adam/batch_normalization_62/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 11:05:45.846380: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:26082 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0079 - val_loss: 0.0021 - 15s/epoch - 181us/sample
Epoch 2/85
84077/84077 - 10s - loss: 0.0015 - val_loss: 0.0017 - 10s/epoch - 113us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0238 - val_loss: 0.0012 - 9s/epoch - 113us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 9.3777e-04 - 9s/epoch - 113us/sample
Epoch 5/85
84077/84077 - 9s - loss: 9.8434e-04 - val_loss: 6.9317e-04 - 9s/epoch - 113us/sample
Epoch 6/85
84077/84077 - 10s - loss: 7.4270e-04 - val_loss: 5.3660e-04 - 10s/epoch - 113us/sample
Epoch 7/85
84077/84077 - 10s - loss: 7.1585e-04 - val_loss: 4.8501e-04 - 10s/epoch - 114us/sample
Epoch 8/85
84077/84077 - 10s - loss: 5.0290e-04 - val_loss: 3.8425e-04 - 10s/epoch - 113us/sample
Epoch 9/85
84077/84077 - 9s - loss: 4.4600e-04 - val_loss: 3.7144e-04 - 9s/epoch - 113us/sample
Epoch 10/85
84077/84077 - 9s - loss: 3.9190e-04 - val_loss: 3.0468e-04 - 9s/epoch - 113us/sample
Epoch 11/85
84077/84077 - 9s - loss: 3.5624e-04 - val_loss: 6.3189e-04 - 9s/epoch - 113us/sample
Epoch 12/85
84077/84077 - 10s - loss: 3.3160e-04 - val_loss: 2.9371e-04 - 10s/epoch - 113us/sample
Epoch 13/85
84077/84077 - 10s - loss: 3.8414e-04 - val_loss: 2.5297e-04 - 10s/epoch - 114us/sample
Epoch 14/85
84077/84077 - 10s - loss: 2.8617e-04 - val_loss: 2.3436e-04 - 10s/epoch - 113us/sample
Epoch 15/85
84077/84077 - 10s - loss: 2.5677e-04 - val_loss: 2.2879e-04 - 10s/epoch - 113us/sample
Epoch 16/85
84077/84077 - 10s - loss: 2.8084e-04 - val_loss: 2.2067e-04 - 10s/epoch - 115us/sample
Epoch 17/85
84077/84077 - 10s - loss: 2.4020e-04 - val_loss: 2.0681e-04 - 10s/epoch - 113us/sample
Epoch 18/85
84077/84077 - 10s - loss: 2.2737e-04 - val_loss: 1.9479e-04 - 10s/epoch - 114us/sample
Epoch 19/85
84077/84077 - 10s - loss: 2.2570e-04 - val_loss: 1.9212e-04 - 10s/epoch - 114us/sample
Epoch 20/85
84077/84077 - 10s - loss: 2.1168e-04 - val_loss: 1.8124e-04 - 10s/epoch - 113us/sample
Epoch 21/85
84077/84077 - 9s - loss: 2.1191e-04 - val_loss: 2.2490e-04 - 9s/epoch - 113us/sample
Epoch 22/85
84077/84077 - 9s - loss: 2.0258e-04 - val_loss: 1.7253e-04 - 9s/epoch - 113us/sample
Epoch 23/85
84077/84077 - 10s - loss: 1.9184e-04 - val_loss: 1.7259e-04 - 10s/epoch - 113us/sample
Epoch 24/85
84077/84077 - 10s - loss: 1.8649e-04 - val_loss: 1.6708e-04 - 10s/epoch - 114us/sample
Epoch 25/85
slurmstepd: error: *** JOB 36005772 ON mb-cas101 CANCELLED AT 2023-02-15T11:09:30 ***
