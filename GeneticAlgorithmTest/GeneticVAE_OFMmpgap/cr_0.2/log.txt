start
Sun Feb 12 10:01:32 CET 2023
2023-02-12 10:01:35.175962: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-12 10:01:35.338444: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
         OFM: s^1 - s^1  OFM: s^1 - s^2  ...  OFM: f^14 - f^13  OFM: f^14 - f^14
id                                       ...                                    
id40482        0.000000        0.000000  ...               0.0          0.000000
id40486        0.000000        0.000000  ...               0.0          0.000000
id40487        0.000000        0.000000  ...               0.0          0.000022
id40490        0.000000        0.080160  ...               0.0          0.000000
id40496        0.000000        0.000000  ...               0.0          0.000000
...                 ...             ...  ...               ...               ...
id83741        0.174544        0.746256  ...               0.0          0.000000
id83742        0.000000        0.000000  ...               0.0          1.815492
id83743        0.007311        0.241355  ...               0.0          0.000000
id83744        0.000000        0.000000  ...               0.0          0.439306
id83745        0.000000        0.203940  ...               0.0          0.000000

[93419 rows x 1024 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 85 0.0012 64 1]
 [2.0 90 0.001 32 1]
 [2.0 90 0.001 64 1]
 [2.1 100 0.0008 64 0]
 [2.0 85 0.0012 64 2]
 [1.9 85 0.0012 64 1]
 [2.0 90 0.0008 32 1]
 [1.8 90 0.0008 16 2]
 [2.2 90 0.0006 64 0]
 [1.8 95 0.0012 64 1]]
[2.0 85 0.0012 64 1] 0
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1886)        7544        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1886)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-12 10:02:15.405507: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-12 10:02:16.069940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:37:00.0, compute capability: 7.0
2023-02-12 10:02:16.101432: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-12 10:02:16.715451: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/outputlayer/kernel/m/Assign' id:1011 op device:{requested: '', assigned: ''} def:{{{node training/Adam/outputlayer/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/outputlayer/kernel/m, training/Adam/outputlayer/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 10:02:29.224554: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:450 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0126 - val_loss: 0.0018 - 14s/epoch - 171us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0013 - 9s/epoch - 105us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0013 - val_loss: 0.0010 - 9s/epoch - 105us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0062 - val_loss: 0.0010 - 9s/epoch - 105us/sample
Epoch 5/85
84077/84077 - 9s - loss: 9.9915e-04 - val_loss: 8.6206e-04 - 9s/epoch - 105us/sample
Epoch 6/85
84077/84077 - 9s - loss: 8.0110e-04 - val_loss: 7.5908e-04 - 9s/epoch - 104us/sample
Epoch 7/85
84077/84077 - 9s - loss: 0.0020 - val_loss: 8.9245e-04 - 9s/epoch - 103us/sample
Epoch 8/85
84077/84077 - 9s - loss: 9.0224e-04 - val_loss: 6.6809e-04 - 9s/epoch - 105us/sample
Epoch 9/85
84077/84077 - 9s - loss: 6.9707e-04 - val_loss: 5.7058e-04 - 9s/epoch - 104us/sample
Epoch 10/85
84077/84077 - 9s - loss: 8.2187e-04 - val_loss: 5.3313e-04 - 9s/epoch - 103us/sample
Epoch 11/85
84077/84077 - 9s - loss: 5.5836e-04 - val_loss: 4.9778e-04 - 9s/epoch - 104us/sample
Epoch 12/85
84077/84077 - 9s - loss: 5.1722e-04 - val_loss: 4.5351e-04 - 9s/epoch - 105us/sample
Epoch 13/85
84077/84077 - 9s - loss: 4.8201e-04 - val_loss: 4.0996e-04 - 9s/epoch - 104us/sample
Epoch 14/85
84077/84077 - 9s - loss: 4.5008e-04 - val_loss: 4.5734e-04 - 9s/epoch - 103us/sample
Epoch 15/85
84077/84077 - 9s - loss: 4.4070e-04 - val_loss: 3.7246e-04 - 9s/epoch - 105us/sample
Epoch 16/85
84077/84077 - 9s - loss: 3.9013e-04 - val_loss: 3.3147e-04 - 9s/epoch - 103us/sample
Epoch 17/85
84077/84077 - 9s - loss: 3.6378e-04 - val_loss: 3.2183e-04 - 9s/epoch - 104us/sample
Epoch 18/85
84077/84077 - 9s - loss: 3.4459e-04 - val_loss: 3.0505e-04 - 9s/epoch - 105us/sample
Epoch 19/85
84077/84077 - 9s - loss: 3.2798e-04 - val_loss: 2.8555e-04 - 9s/epoch - 104us/sample
Epoch 20/85
84077/84077 - 9s - loss: 3.1530e-04 - val_loss: 2.7691e-04 - 9s/epoch - 103us/sample
Epoch 21/85
84077/84077 - 9s - loss: 3.0430e-04 - val_loss: 2.6804e-04 - 9s/epoch - 103us/sample
Epoch 22/85
84077/84077 - 9s - loss: 2.9391e-04 - val_loss: 2.5906e-04 - 9s/epoch - 103us/sample
Epoch 23/85
84077/84077 - 9s - loss: 2.8571e-04 - val_loss: 2.5197e-04 - 9s/epoch - 104us/sample
Epoch 24/85
84077/84077 - 9s - loss: 2.7771e-04 - val_loss: 2.4632e-04 - 9s/epoch - 104us/sample
Epoch 25/85
84077/84077 - 9s - loss: 2.7177e-04 - val_loss: 2.4446e-04 - 9s/epoch - 104us/sample
Epoch 26/85
84077/84077 - 9s - loss: 2.6558e-04 - val_loss: 2.4223e-04 - 9s/epoch - 103us/sample
Epoch 27/85
84077/84077 - 9s - loss: 2.6100e-04 - val_loss: 2.3653e-04 - 9s/epoch - 104us/sample
Epoch 28/85
84077/84077 - 9s - loss: 2.5646e-04 - val_loss: 2.3369e-04 - 9s/epoch - 104us/sample
Epoch 29/85
84077/84077 - 9s - loss: 2.5302e-04 - val_loss: 2.2697e-04 - 9s/epoch - 104us/sample
Epoch 30/85
84077/84077 - 9s - loss: 2.4917e-04 - val_loss: 2.2863e-04 - 9s/epoch - 103us/sample
Epoch 31/85
84077/84077 - 9s - loss: 2.4754e-04 - val_loss: 2.2380e-04 - 9s/epoch - 105us/sample
Epoch 32/85
84077/84077 - 9s - loss: 2.4434e-04 - val_loss: 2.2254e-04 - 9s/epoch - 104us/sample
Epoch 33/85
84077/84077 - 9s - loss: 2.4225e-04 - val_loss: 2.2112e-04 - 9s/epoch - 104us/sample
Epoch 34/85
84077/84077 - 9s - loss: 2.3970e-04 - val_loss: 2.1807e-04 - 9s/epoch - 103us/sample
Epoch 35/85
84077/84077 - 9s - loss: 2.3687e-04 - val_loss: 2.1784e-04 - 9s/epoch - 103us/sample
Epoch 36/85
84077/84077 - 9s - loss: 2.3406e-04 - val_loss: 2.1799e-04 - 9s/epoch - 104us/sample
Epoch 37/85
84077/84077 - 9s - loss: 2.3278e-04 - val_loss: 2.1591e-04 - 9s/epoch - 104us/sample
Epoch 38/85
84077/84077 - 9s - loss: 2.3129e-04 - val_loss: 2.1491e-04 - 9s/epoch - 103us/sample
Epoch 39/85
84077/84077 - 9s - loss: 2.2985e-04 - val_loss: 2.1294e-04 - 9s/epoch - 104us/sample
Epoch 40/85
84077/84077 - 9s - loss: 2.2794e-04 - val_loss: 2.1072e-04 - 9s/epoch - 103us/sample
Epoch 41/85
84077/84077 - 9s - loss: 2.2596e-04 - val_loss: 2.1074e-04 - 9s/epoch - 105us/sample
Epoch 42/85
84077/84077 - 9s - loss: 2.2536e-04 - val_loss: 2.1051e-04 - 9s/epoch - 104us/sample
Epoch 43/85
84077/84077 - 9s - loss: 2.2353e-04 - val_loss: 2.0712e-04 - 9s/epoch - 104us/sample
Epoch 44/85
84077/84077 - 9s - loss: 2.2254e-04 - val_loss: 2.0648e-04 - 9s/epoch - 103us/sample
Epoch 45/85
84077/84077 - 9s - loss: 2.2152e-04 - val_loss: 2.0748e-04 - 9s/epoch - 105us/sample
Epoch 46/85
84077/84077 - 9s - loss: 2.1965e-04 - val_loss: 2.0743e-04 - 9s/epoch - 104us/sample
Epoch 47/85
84077/84077 - 9s - loss: 2.1926e-04 - val_loss: 2.0555e-04 - 9s/epoch - 104us/sample
Epoch 48/85
84077/84077 - 9s - loss: 2.1784e-04 - val_loss: 2.0484e-04 - 9s/epoch - 103us/sample
Epoch 49/85
84077/84077 - 9s - loss: 2.2519e-04 - val_loss: 2.0529e-04 - 9s/epoch - 104us/sample
Epoch 50/85
84077/84077 - 9s - loss: 2.1738e-04 - val_loss: 2.0197e-04 - 9s/epoch - 104us/sample
Epoch 51/85
84077/84077 - 9s - loss: 2.1577e-04 - val_loss: 2.0218e-04 - 9s/epoch - 104us/sample
Epoch 52/85
84077/84077 - 9s - loss: 2.1629e-04 - val_loss: 2.0111e-04 - 9s/epoch - 104us/sample
Epoch 53/85
84077/84077 - 9s - loss: 2.1454e-04 - val_loss: 2.0158e-04 - 9s/epoch - 104us/sample
Epoch 54/85
84077/84077 - 9s - loss: 2.1303e-04 - val_loss: 1.9923e-04 - 9s/epoch - 103us/sample
Epoch 55/85
84077/84077 - 9s - loss: 2.1356e-04 - val_loss: 2.0538e-04 - 9s/epoch - 105us/sample
Epoch 56/85
84077/84077 - 9s - loss: 2.1264e-04 - val_loss: 1.9977e-04 - 9s/epoch - 104us/sample
Epoch 57/85
84077/84077 - 9s - loss: 2.1030e-04 - val_loss: 1.9720e-04 - 9s/epoch - 104us/sample
Epoch 58/85
84077/84077 - 9s - loss: 2.1078e-04 - val_loss: 1.9713e-04 - 9s/epoch - 103us/sample
Epoch 59/85
84077/84077 - 9s - loss: 2.0991e-04 - val_loss: 1.9777e-04 - 9s/epoch - 105us/sample
Epoch 60/85
84077/84077 - 9s - loss: 2.0863e-04 - val_loss: 1.9723e-04 - 9s/epoch - 104us/sample
Epoch 61/85
84077/84077 - 9s - loss: 2.0868e-04 - val_loss: 1.9729e-04 - 9s/epoch - 104us/sample
Epoch 62/85
84077/84077 - 9s - loss: 2.0787e-04 - val_loss: 1.9665e-04 - 9s/epoch - 103us/sample
Epoch 63/85
84077/84077 - 9s - loss: 2.0775e-04 - val_loss: 1.9574e-04 - 9s/epoch - 104us/sample
Epoch 64/85
84077/84077 - 9s - loss: 2.0670e-04 - val_loss: 1.9629e-04 - 9s/epoch - 104us/sample
Epoch 65/85
84077/84077 - 9s - loss: 2.0614e-04 - val_loss: 1.9505e-04 - 9s/epoch - 104us/sample
Epoch 66/85
84077/84077 - 9s - loss: 2.0550e-04 - val_loss: 1.9411e-04 - 9s/epoch - 104us/sample
Epoch 67/85
84077/84077 - 9s - loss: 2.0504e-04 - val_loss: 1.9380e-04 - 9s/epoch - 103us/sample
Epoch 68/85
84077/84077 - 9s - loss: 2.0398e-04 - val_loss: 1.9393e-04 - 9s/epoch - 104us/sample
Epoch 69/85
84077/84077 - 9s - loss: 2.0409e-04 - val_loss: 1.9433e-04 - 9s/epoch - 104us/sample
Epoch 70/85
84077/84077 - 9s - loss: 2.0415e-04 - val_loss: 1.9341e-04 - 9s/epoch - 103us/sample
Epoch 71/85
84077/84077 - 9s - loss: 2.0801e-04 - val_loss: 1.9348e-04 - 9s/epoch - 104us/sample
Epoch 72/85
84077/84077 - 9s - loss: 2.0221e-04 - val_loss: 1.9206e-04 - 9s/epoch - 103us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.0283e-04 - val_loss: 1.9206e-04 - 9s/epoch - 104us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.0923e-04 - val_loss: 2.2280e-04 - 9s/epoch - 104us/sample
Epoch 75/85
84077/84077 - 9s - loss: 2.0720e-04 - val_loss: 1.9218e-04 - 9s/epoch - 104us/sample
Epoch 76/85
84077/84077 - 9s - loss: 2.0146e-04 - val_loss: 1.9165e-04 - 9s/epoch - 103us/sample
Epoch 77/85
84077/84077 - 9s - loss: 2.0043e-04 - val_loss: 1.9007e-04 - 9s/epoch - 104us/sample
Epoch 78/85
84077/84077 - 9s - loss: 2.0034e-04 - val_loss: 1.9189e-04 - 9s/epoch - 104us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.9990e-04 - val_loss: 1.8894e-04 - 9s/epoch - 104us/sample
Epoch 80/85
84077/84077 - 9s - loss: 2.0025e-04 - val_loss: 1.9073e-04 - 9s/epoch - 104us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.9994e-04 - val_loss: 1.8982e-04 - 9s/epoch - 103us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.9893e-04 - val_loss: 1.8979e-04 - 9s/epoch - 105us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.9811e-04 - val_loss: 1.8728e-04 - 9s/epoch - 104us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.9870e-04 - val_loss: 1.8854e-04 - 9s/epoch - 104us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.9772e-04 - val_loss: 1.8706e-04 - 9s/epoch - 103us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018706266649755375
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 10:14:43.282223: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:421 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02409690991203899
cosine 0.02380460040202585
MAE: 0.0020189274436170613
RMSE: 0.009303901806540718
r2: 0.9325381759592972
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 85, 0.0012, 0.2, 188, 0.0001977157340755596, 0.00018706266649755375, 0.02409690991203899, 0.02380460040202585, 0.0020189274436170613, 0.009303901806540718, 0.9325381759592972, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1886)        7544        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1886)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.0_cr0.2_bs32_ep90_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 10:14:50.252113: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_4/beta/Assign' id:1443 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_4/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_4/beta, batch_normalization_4/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 10:14:51.097264: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_2/bias/m/Assign' id:2417 op device:{requested: '', assigned: ''} def:{{{node outputlayer_2/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_2/bias/m, outputlayer_2/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 10:14:51.944641: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2129 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030604858404899894
cosine 0.030233279991380053
MAE: 0.002209957634441004
RMSE: 0.010457837354445138
r2: 0.9146330260789663
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.0custom_VAE', 'mse', 32, 90, 0.001, 0.2, 188, '--', '--', 0.030604858404899894, 0.030233279991380053, 0.002209957634441004, 0.010457837354445138, 0.9146330260789663, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1886)        7544        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1886)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.0_cr0.2_bs64_ep90_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 10:14:58.978755: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_47/gamma/Assign' id:3244 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_47/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_47/gamma, batch_normalization_47/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 10:15:00.051843: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_4/bias/m/Assign' id:3777 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_4/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_4/bias/m, bottleneck_zlog_4/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 10:15:01.023394: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:3551 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.021056132518031987
cosine 0.020810217007102318
MAE: 0.001954063631485909
RMSE: 0.008579280844289145
r2: 0.9427532685224276
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.0custom_VAE', 'mse', 64, 90, 0.001, 0.2, 188, '--', '--', 0.021056132518031987, 0.020810217007102318, 0.001954063631485909, 0.008579280844289145, 0.9427532685224276, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 100 0.0008 64 0] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 1980)        7920        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 1980)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/100
2023-02-12 10:15:08.242498: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/batch_normalization_10_1/gamma/v/Assign' id:5269 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/batch_normalization_10_1/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/batch_normalization_10_1/gamma/v, training_2/Adam/batch_normalization_10_1/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 10:15:18.040184: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:4574 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 11s - loss: 2.7932 - val_loss: 0.0706 - 11s/epoch - 130us/sample
Epoch 2/100
84077/84077 - 9s - loss: 0.0699 - val_loss: 0.0678 - 9s/epoch - 112us/sample
Epoch 3/100
84077/84077 - 9s - loss: 0.0671 - val_loss: 0.0675 - 9s/epoch - 112us/sample
Epoch 4/100
84077/84077 - 10s - loss: 0.0687 - val_loss: 0.0676 - 10s/epoch - 114us/sample
Epoch 5/100
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 6/100
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 7/100
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 8/100
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 9/100
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 113us/sample
Epoch 10/100
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 113us/sample
Epoch 11/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 12/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 112us/sample
Epoch 13/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 14/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 15/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 16/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 17/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 18/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 19/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 20/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 21/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 22/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 23/100
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 24/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 25/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 26/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 27/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 28/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 113us/sample
Epoch 29/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 30/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 31/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 112us/sample
Epoch 32/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 33/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 34/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 35/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 36/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 37/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 38/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 39/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 40/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 41/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 42/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 43/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 44/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 45/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 46/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 47/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 48/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 49/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 50/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 51/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 52/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 53/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 54/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 55/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 56/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 57/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 58/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 59/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 60/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 61/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 62/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 63/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 64/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 65/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 66/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 67/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 68/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 69/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 70/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 71/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 72/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 73/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 74/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 75/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 76/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 77/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 78/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 79/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 80/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 81/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 82/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 83/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 84/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 85/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 86/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 87/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 88/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 89/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 90/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 91/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 92/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 93/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 94/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 95/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 96/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 97/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 98/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 99/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 100/100
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573188710585
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 10:30:11.733755: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:4526 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2646693839975112
cosine 1.1903126312048216
MAE: 5.667531774050809
RMSE: 6.321459870909595
r2: -29419.330359869356
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'binary_crossentropy', 64, 100, 0.0008, 0.2, 188, 0.06683691714466705, 0.06734573188710585, 1.2646693839975112, 1.1903126312048216, 5.667531774050809, 6.321459870909595, -29419.330359869356, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0012 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 1886)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 10:30:19.297878: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_6/kernel/Assign' id:5774 op device:{requested: '', assigned: ''} def:{{{node outputlayer_6/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_6/kernel, outputlayer_6/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 10:30:28.183875: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:5902 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 10s - loss: 0.0039 - val_loss: 8.9642e-04 - 10s/epoch - 121us/sample
Epoch 2/85
84077/84077 - 9s - loss: 8.4609e-04 - val_loss: 8.6408e-04 - 9s/epoch - 101us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0192 - val_loss: 6.9764e-04 - 9s/epoch - 101us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0031 - val_loss: 0.0012 - 9s/epoch - 102us/sample
Epoch 5/85
84077/84077 - 9s - loss: 7.1518e-04 - val_loss: 5.1025e-04 - 9s/epoch - 102us/sample
Epoch 6/85
84077/84077 - 9s - loss: 6.1584e-04 - val_loss: 4.9564e-04 - 9s/epoch - 102us/sample
Epoch 7/85
84077/84077 - 9s - loss: 5.0058e-04 - val_loss: 3.9876e-04 - 9s/epoch - 101us/sample
Epoch 8/85
84077/84077 - 9s - loss: 3.8696e-04 - val_loss: 3.3327e-04 - 9s/epoch - 101us/sample
Epoch 9/85
84077/84077 - 9s - loss: 3.5909e-04 - val_loss: 3.1290e-04 - 9s/epoch - 101us/sample
Epoch 10/85
84077/84077 - 9s - loss: 3.3344e-04 - val_loss: 2.8134e-04 - 9s/epoch - 102us/sample
Epoch 11/85
84077/84077 - 9s - loss: 3.0434e-04 - val_loss: 2.6809e-04 - 9s/epoch - 102us/sample
Epoch 12/85
84077/84077 - 9s - loss: 2.8073e-04 - val_loss: 2.4470e-04 - 9s/epoch - 102us/sample
Epoch 13/85
84077/84077 - 9s - loss: 2.6597e-04 - val_loss: 2.3215e-04 - 9s/epoch - 101us/sample
Epoch 14/85
84077/84077 - 9s - loss: 2.5373e-04 - val_loss: 2.2166e-04 - 9s/epoch - 101us/sample
Epoch 15/85
84077/84077 - 9s - loss: 2.3862e-04 - val_loss: 2.1365e-04 - 9s/epoch - 103us/sample
Epoch 16/85
84077/84077 - 9s - loss: 2.2629e-04 - val_loss: 2.0495e-04 - 9s/epoch - 102us/sample
Epoch 17/85
84077/84077 - 9s - loss: 2.2091e-04 - val_loss: 1.9578e-04 - 9s/epoch - 102us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.0755e-04 - val_loss: 1.8461e-04 - 9s/epoch - 102us/sample
Epoch 19/85
84077/84077 - 9s - loss: 2.0102e-04 - val_loss: 1.7794e-04 - 9s/epoch - 102us/sample
Epoch 20/85
84077/84077 - 9s - loss: 1.9499e-04 - val_loss: 1.7163e-04 - 9s/epoch - 102us/sample
Epoch 21/85
84077/84077 - 9s - loss: 1.8959e-04 - val_loss: 1.6754e-04 - 9s/epoch - 102us/sample
Epoch 22/85
84077/84077 - 9s - loss: 1.8524e-04 - val_loss: 1.6487e-04 - 9s/epoch - 101us/sample
Epoch 23/85
84077/84077 - 9s - loss: 1.8068e-04 - val_loss: 1.6029e-04 - 9s/epoch - 101us/sample
Epoch 24/85
84077/84077 - 9s - loss: 1.7815e-04 - val_loss: 1.5760e-04 - 9s/epoch - 102us/sample
Epoch 25/85
84077/84077 - 9s - loss: 1.7450e-04 - val_loss: 1.5651e-04 - 9s/epoch - 103us/sample
Epoch 26/85
84077/84077 - 9s - loss: 1.7248e-04 - val_loss: 1.5457e-04 - 9s/epoch - 102us/sample
Epoch 27/85
84077/84077 - 9s - loss: 1.7004e-04 - val_loss: 1.5175e-04 - 9s/epoch - 102us/sample
Epoch 28/85
84077/84077 - 9s - loss: 1.6788e-04 - val_loss: 1.5177e-04 - 9s/epoch - 101us/sample
Epoch 29/85
84077/84077 - 9s - loss: 1.6568e-04 - val_loss: 1.4931e-04 - 9s/epoch - 102us/sample
Epoch 30/85
84077/84077 - 9s - loss: 1.6430e-04 - val_loss: 1.4922e-04 - 9s/epoch - 102us/sample
Epoch 31/85
84077/84077 - 9s - loss: 1.6244e-04 - val_loss: 1.4714e-04 - 9s/epoch - 102us/sample
Epoch 32/85
84077/84077 - 9s - loss: 1.6055e-04 - val_loss: 1.4575e-04 - 9s/epoch - 102us/sample
Epoch 33/85
84077/84077 - 9s - loss: 1.5958e-04 - val_loss: 1.4410e-04 - 9s/epoch - 101us/sample
Epoch 34/85
84077/84077 - 9s - loss: 1.5844e-04 - val_loss: 1.4309e-04 - 9s/epoch - 102us/sample
Epoch 35/85
84077/84077 - 9s - loss: 1.5689e-04 - val_loss: 1.4276e-04 - 9s/epoch - 102us/sample
Epoch 36/85
84077/84077 - 9s - loss: 1.5652e-04 - val_loss: 1.4281e-04 - 9s/epoch - 102us/sample
Epoch 37/85
84077/84077 - 9s - loss: 1.5505e-04 - val_loss: 1.4195e-04 - 9s/epoch - 101us/sample
Epoch 38/85
84077/84077 - 9s - loss: 1.5483e-04 - val_loss: 1.4064e-04 - 9s/epoch - 101us/sample
Epoch 39/85
84077/84077 - 9s - loss: 1.5378e-04 - val_loss: 1.4067e-04 - 9s/epoch - 103us/sample
Epoch 40/85
84077/84077 - 9s - loss: 1.5294e-04 - val_loss: 1.4060e-04 - 9s/epoch - 102us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.5230e-04 - val_loss: 1.3933e-04 - 9s/epoch - 102us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.5108e-04 - val_loss: 1.3865e-04 - 9s/epoch - 101us/sample
Epoch 43/85
84077/84077 - 9s - loss: 1.5086e-04 - val_loss: 1.3773e-04 - 9s/epoch - 101us/sample
Epoch 44/85
84077/84077 - 9s - loss: 1.5050e-04 - val_loss: 1.3800e-04 - 9s/epoch - 103us/sample
Epoch 45/85
84077/84077 - 9s - loss: 1.4922e-04 - val_loss: 1.3799e-04 - 9s/epoch - 102us/sample
Epoch 46/85
84077/84077 - 9s - loss: 1.4909e-04 - val_loss: 1.3897e-04 - 9s/epoch - 102us/sample
Epoch 47/85
84077/84077 - 9s - loss: 1.4854e-04 - val_loss: 1.3693e-04 - 9s/epoch - 101us/sample
Epoch 48/85
84077/84077 - 9s - loss: 1.4791e-04 - val_loss: 1.3889e-04 - 9s/epoch - 102us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.4724e-04 - val_loss: 1.3602e-04 - 9s/epoch - 102us/sample
Epoch 50/85
84077/84077 - 9s - loss: 1.4683e-04 - val_loss: 1.3429e-04 - 9s/epoch - 102us/sample
Epoch 51/85
84077/84077 - 9s - loss: 1.4643e-04 - val_loss: 1.3553e-04 - 9s/epoch - 101us/sample
Epoch 52/85
84077/84077 - 9s - loss: 1.4629e-04 - val_loss: 1.3575e-04 - 9s/epoch - 101us/sample
Epoch 53/85
84077/84077 - 9s - loss: 1.4501e-04 - val_loss: 1.3330e-04 - 9s/epoch - 103us/sample
Epoch 54/85
84077/84077 - 9s - loss: 1.4517e-04 - val_loss: 1.3551e-04 - 9s/epoch - 102us/sample
Epoch 55/85
84077/84077 - 9s - loss: 1.4504e-04 - val_loss: 1.3434e-04 - 9s/epoch - 102us/sample
Epoch 56/85
84077/84077 - 9s - loss: 1.4460e-04 - val_loss: 1.3451e-04 - 9s/epoch - 101us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.4424e-04 - val_loss: 1.3423e-04 - 9s/epoch - 103us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.4357e-04 - val_loss: 1.3258e-04 - 9s/epoch - 102us/sample
Epoch 59/85
84077/84077 - 9s - loss: 1.4336e-04 - val_loss: 1.3417e-04 - 9s/epoch - 102us/sample
Epoch 60/85
84077/84077 - 9s - loss: 1.4317e-04 - val_loss: 1.3221e-04 - 9s/epoch - 102us/sample
Epoch 61/85
84077/84077 - 9s - loss: 1.4243e-04 - val_loss: 1.3346e-04 - 9s/epoch - 101us/sample
Epoch 62/85
84077/84077 - 9s - loss: 1.4275e-04 - val_loss: 1.3281e-04 - 9s/epoch - 101us/sample
Epoch 63/85
84077/84077 - 9s - loss: 1.4223e-04 - val_loss: 1.3236e-04 - 9s/epoch - 102us/sample
Epoch 64/85
84077/84077 - 9s - loss: 1.4192e-04 - val_loss: 1.3200e-04 - 9s/epoch - 102us/sample
Epoch 65/85
84077/84077 - 9s - loss: 1.4180e-04 - val_loss: 1.3242e-04 - 9s/epoch - 102us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.4079e-04 - val_loss: 1.3247e-04 - 9s/epoch - 101us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.4069e-04 - val_loss: 1.3141e-04 - 9s/epoch - 102us/sample
Epoch 68/85
84077/84077 - 9s - loss: 1.4067e-04 - val_loss: 1.3354e-04 - 9s/epoch - 103us/sample
Epoch 69/85
84077/84077 - 9s - loss: 1.4045e-04 - val_loss: 1.3190e-04 - 9s/epoch - 102us/sample
Epoch 70/85
84077/84077 - 9s - loss: 1.4067e-04 - val_loss: 1.3032e-04 - 9s/epoch - 102us/sample
Epoch 71/85
84077/84077 - 9s - loss: 1.4006e-04 - val_loss: 1.3024e-04 - 9s/epoch - 102us/sample
Epoch 72/85
84077/84077 - 9s - loss: 1.3983e-04 - val_loss: 1.3094e-04 - 9s/epoch - 101us/sample
Epoch 73/85
84077/84077 - 9s - loss: 1.3941e-04 - val_loss: 1.3076e-04 - 9s/epoch - 102us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.3947e-04 - val_loss: 1.3055e-04 - 9s/epoch - 102us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.3863e-04 - val_loss: 1.2977e-04 - 9s/epoch - 102us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.3875e-04 - val_loss: 1.3011e-04 - 9s/epoch - 101us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.3863e-04 - val_loss: 1.2899e-04 - 9s/epoch - 101us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.3876e-04 - val_loss: 1.2864e-04 - 9s/epoch - 102us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.3836e-04 - val_loss: 1.2976e-04 - 9s/epoch - 102us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.3762e-04 - val_loss: 1.2963e-04 - 9s/epoch - 101us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.3796e-04 - val_loss: 1.2872e-04 - 9s/epoch - 102us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.3758e-04 - val_loss: 1.2947e-04 - 9s/epoch - 101us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.3733e-04 - val_loss: 1.2876e-04 - 9s/epoch - 102us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.3747e-04 - val_loss: 1.2874e-04 - 9s/epoch - 102us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.3714e-04 - val_loss: 1.2815e-04 - 9s/epoch - 102us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012815055438682033
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 10:42:27.669971: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:5866 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03226744005778501
cosine 0.03187479013427257
MAE: 0.0022676688502254573
RMSE: 0.010373583928274666
r2: 0.9161325228275636
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 85, 0.0012, 0.2, 188, 0.0001371378077814383, 0.00012815055438682033, 0.03226744005778501, 0.03187479013427257, 0.0022676688502254573, 0.010373583928274666, 0.9161325228275636, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0012 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 1791)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 10:42:35.458111: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/bottleneck_zmean_7/kernel/m/Assign' id:7664 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/bottleneck_zmean_7/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/bottleneck_zmean_7/kernel/m, training_6/Adam/bottleneck_zmean_7/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 10:42:44.473711: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:7180 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 10s - loss: 0.0104 - val_loss: 0.0016 - 10s/epoch - 124us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 101us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 101us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0030 - val_loss: 0.0021 - 9s/epoch - 103us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0010 - 9s/epoch - 101us/sample
Epoch 6/85
84077/84077 - 8s - loss: 0.0055 - val_loss: 0.0033 - 8s/epoch - 101us/sample
Epoch 7/85
84077/84077 - 8s - loss: 0.0012 - val_loss: 9.0510e-04 - 8s/epoch - 101us/sample
Epoch 8/85
84077/84077 - 8s - loss: 8.2340e-04 - val_loss: 7.0535e-04 - 8s/epoch - 101us/sample
Epoch 9/85
84077/84077 - 8s - loss: 8.4455e-04 - val_loss: 6.1668e-04 - 8s/epoch - 101us/sample
Epoch 10/85
84077/84077 - 9s - loss: 6.7488e-04 - val_loss: 5.3672e-04 - 9s/epoch - 102us/sample
Epoch 11/85
84077/84077 - 9s - loss: 5.7509e-04 - val_loss: 5.1067e-04 - 9s/epoch - 102us/sample
Epoch 12/85
84077/84077 - 9s - loss: 5.1414e-04 - val_loss: 4.2497e-04 - 9s/epoch - 101us/sample
Epoch 13/85
84077/84077 - 8s - loss: 4.7146e-04 - val_loss: 3.9523e-04 - 8s/epoch - 101us/sample
Epoch 14/85
84077/84077 - 8s - loss: 4.3992e-04 - val_loss: 3.9700e-04 - 8s/epoch - 101us/sample
Epoch 15/85
84077/84077 - 9s - loss: 4.0821e-04 - val_loss: 3.5123e-04 - 9s/epoch - 102us/sample
Epoch 16/85
84077/84077 - 9s - loss: 3.8376e-04 - val_loss: 3.2383e-04 - 9s/epoch - 102us/sample
Epoch 17/85
84077/84077 - 9s - loss: 3.6815e-04 - val_loss: 3.1745e-04 - 9s/epoch - 101us/sample
Epoch 18/85
84077/84077 - 9s - loss: 3.5494e-04 - val_loss: 3.0359e-04 - 9s/epoch - 101us/sample
Epoch 19/85
84077/84077 - 8s - loss: 3.5182e-04 - val_loss: 2.9636e-04 - 8s/epoch - 101us/sample
Epoch 20/85
84077/84077 - 9s - loss: 3.3183e-04 - val_loss: 2.8682e-04 - 9s/epoch - 102us/sample
Epoch 21/85
84077/84077 - 9s - loss: 3.2335e-04 - val_loss: 2.8143e-04 - 9s/epoch - 102us/sample
Epoch 22/85
84077/84077 - 9s - loss: 3.1556e-04 - val_loss: 2.7885e-04 - 9s/epoch - 101us/sample
Epoch 23/85
84077/84077 - 9s - loss: 3.0954e-04 - val_loss: 2.7229e-04 - 9s/epoch - 101us/sample
Epoch 24/85
84077/84077 - 8s - loss: 3.0351e-04 - val_loss: 2.7025e-04 - 8s/epoch - 101us/sample
Epoch 25/85
84077/84077 - 9s - loss: 2.9845e-04 - val_loss: 2.6476e-04 - 9s/epoch - 101us/sample
Epoch 26/85
84077/84077 - 9s - loss: 2.9641e-04 - val_loss: 2.6316e-04 - 9s/epoch - 102us/sample
Epoch 27/85
84077/84077 - 8s - loss: 2.9008e-04 - val_loss: 2.6374e-04 - 8s/epoch - 101us/sample
Epoch 28/85
84077/84077 - 8s - loss: 2.8748e-04 - val_loss: 2.6157e-04 - 8s/epoch - 101us/sample
Epoch 29/85
84077/84077 - 8s - loss: 2.8465e-04 - val_loss: 2.5744e-04 - 8s/epoch - 101us/sample
Epoch 30/85
84077/84077 - 8s - loss: 2.8297e-04 - val_loss: 2.5316e-04 - 8s/epoch - 101us/sample
Epoch 31/85
84077/84077 - 9s - loss: 2.8106e-04 - val_loss: 2.5302e-04 - 9s/epoch - 102us/sample
Epoch 32/85
84077/84077 - 9s - loss: 2.8252e-04 - val_loss: 2.5287e-04 - 9s/epoch - 101us/sample
Epoch 33/85
84077/84077 - 9s - loss: 2.7578e-04 - val_loss: 2.5110e-04 - 9s/epoch - 101us/sample
Epoch 34/85
84077/84077 - 8s - loss: 2.7339e-04 - val_loss: 2.5032e-04 - 8s/epoch - 101us/sample
Epoch 35/85
84077/84077 - 8s - loss: 2.7288e-04 - val_loss: 2.4704e-04 - 8s/epoch - 101us/sample
Epoch 36/85
84077/84077 - 9s - loss: 2.7024e-04 - val_loss: 2.4588e-04 - 9s/epoch - 102us/sample
Epoch 37/85
84077/84077 - 9s - loss: 2.6858e-04 - val_loss: 2.4483e-04 - 9s/epoch - 101us/sample
Epoch 38/85
84077/84077 - 9s - loss: 2.6770e-04 - val_loss: 2.4234e-04 - 9s/epoch - 101us/sample
Epoch 39/85
84077/84077 - 8s - loss: 2.6514e-04 - val_loss: 2.4340e-04 - 8s/epoch - 101us/sample
Epoch 40/85
84077/84077 - 8s - loss: 2.6392e-04 - val_loss: 2.3990e-04 - 8s/epoch - 101us/sample
Epoch 41/85
84077/84077 - 9s - loss: 2.6939e-04 - val_loss: 2.3922e-04 - 9s/epoch - 101us/sample
Epoch 42/85
84077/84077 - 9s - loss: 2.6197e-04 - val_loss: 2.3678e-04 - 9s/epoch - 102us/sample
Epoch 43/85
84077/84077 - 8s - loss: 2.5914e-04 - val_loss: 2.3769e-04 - 8s/epoch - 101us/sample
Epoch 44/85
84077/84077 - 9s - loss: 2.5914e-04 - val_loss: 2.3669e-04 - 9s/epoch - 101us/sample
Epoch 45/85
84077/84077 - 8s - loss: 2.5744e-04 - val_loss: 2.3425e-04 - 8s/epoch - 101us/sample
Epoch 46/85
84077/84077 - 8s - loss: 2.5508e-04 - val_loss: 2.3440e-04 - 8s/epoch - 101us/sample
Epoch 47/85
84077/84077 - 9s - loss: 2.5416e-04 - val_loss: 2.3187e-04 - 9s/epoch - 102us/sample
Epoch 48/85
84077/84077 - 9s - loss: 2.7088e-04 - val_loss: 2.3357e-04 - 9s/epoch - 101us/sample
Epoch 49/85
84077/84077 - 9s - loss: 2.5271e-04 - val_loss: 2.3076e-04 - 9s/epoch - 101us/sample
Epoch 50/85
84077/84077 - 8s - loss: 2.5045e-04 - val_loss: 2.2960e-04 - 8s/epoch - 101us/sample
Epoch 51/85
84077/84077 - 9s - loss: 2.4976e-04 - val_loss: 2.2659e-04 - 9s/epoch - 101us/sample
Epoch 52/85
84077/84077 - 9s - loss: 2.4956e-04 - val_loss: 2.2756e-04 - 9s/epoch - 102us/sample
Epoch 53/85
84077/84077 - 9s - loss: 2.4838e-04 - val_loss: 2.2944e-04 - 9s/epoch - 102us/sample
Epoch 54/85
84077/84077 - 9s - loss: 2.5475e-04 - val_loss: 2.2875e-04 - 9s/epoch - 101us/sample
Epoch 55/85
84077/84077 - 8s - loss: 2.4731e-04 - val_loss: 2.2735e-04 - 8s/epoch - 101us/sample
Epoch 56/85
84077/84077 - 8s - loss: 2.4564e-04 - val_loss: 2.2662e-04 - 8s/epoch - 101us/sample
Epoch 57/85
84077/84077 - 8s - loss: 2.4600e-04 - val_loss: 2.2579e-04 - 8s/epoch - 101us/sample
Epoch 58/85
84077/84077 - 9s - loss: 2.4610e-04 - val_loss: 2.2423e-04 - 9s/epoch - 102us/sample
Epoch 59/85
84077/84077 - 9s - loss: 2.4325e-04 - val_loss: 2.2474e-04 - 9s/epoch - 101us/sample
Epoch 60/85
84077/84077 - 9s - loss: 2.4246e-04 - val_loss: 2.2332e-04 - 9s/epoch - 101us/sample
Epoch 61/85
84077/84077 - 8s - loss: 2.4128e-04 - val_loss: 2.2077e-04 - 8s/epoch - 101us/sample
Epoch 62/85
84077/84077 - 9s - loss: 2.4214e-04 - val_loss: 2.2176e-04 - 9s/epoch - 102us/sample
Epoch 63/85
84077/84077 - 9s - loss: 2.3973e-04 - val_loss: 2.2079e-04 - 9s/epoch - 101us/sample
Epoch 64/85
84077/84077 - 9s - loss: 2.3896e-04 - val_loss: 2.1948e-04 - 9s/epoch - 101us/sample
Epoch 65/85
84077/84077 - 9s - loss: 2.3856e-04 - val_loss: 2.2076e-04 - 9s/epoch - 101us/sample
Epoch 66/85
84077/84077 - 9s - loss: 2.3788e-04 - val_loss: 2.2179e-04 - 9s/epoch - 101us/sample
Epoch 67/85
84077/84077 - 9s - loss: 2.3880e-04 - val_loss: 2.2237e-04 - 9s/epoch - 101us/sample
Epoch 68/85
84077/84077 - 9s - loss: 2.3670e-04 - val_loss: 2.1912e-04 - 9s/epoch - 102us/sample
Epoch 69/85
84077/84077 - 9s - loss: 2.3478e-04 - val_loss: 2.2015e-04 - 9s/epoch - 101us/sample
Epoch 70/85
84077/84077 - 9s - loss: 2.3405e-04 - val_loss: 2.1703e-04 - 9s/epoch - 101us/sample
Epoch 71/85
84077/84077 - 8s - loss: 2.3680e-04 - val_loss: 2.1802e-04 - 8s/epoch - 101us/sample
Epoch 72/85
84077/84077 - 9s - loss: 2.3273e-04 - val_loss: 2.1668e-04 - 9s/epoch - 101us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.3183e-04 - val_loss: 2.1482e-04 - 9s/epoch - 102us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.3234e-04 - val_loss: 2.2520e-04 - 9s/epoch - 102us/sample
Epoch 75/85
84077/84077 - 9s - loss: 2.3146e-04 - val_loss: 2.1408e-04 - 9s/epoch - 101us/sample
Epoch 76/85
84077/84077 - 9s - loss: 2.3393e-04 - val_loss: 2.1453e-04 - 9s/epoch - 101us/sample
Epoch 77/85
84077/84077 - 9s - loss: 2.3248e-04 - val_loss: 2.1846e-04 - 9s/epoch - 101us/sample
Epoch 78/85
84077/84077 - 9s - loss: 2.3107e-04 - val_loss: 2.1357e-04 - 9s/epoch - 102us/sample
Epoch 79/85
84077/84077 - 9s - loss: 2.3403e-04 - val_loss: 2.2206e-04 - 9s/epoch - 101us/sample
Epoch 80/85
84077/84077 - 9s - loss: 2.3008e-04 - val_loss: 2.1330e-04 - 9s/epoch - 101us/sample
Epoch 81/85
84077/84077 - 8s - loss: 2.3240e-04 - val_loss: 2.1645e-04 - 8s/epoch - 101us/sample
Epoch 82/85
84077/84077 - 8s - loss: 2.2780e-04 - val_loss: 2.1260e-04 - 8s/epoch - 101us/sample
Epoch 83/85
84077/84077 - 9s - loss: 2.2726e-04 - val_loss: 2.1357e-04 - 9s/epoch - 103us/sample
Epoch 84/85
84077/84077 - 9s - loss: 2.2716e-04 - val_loss: 2.1206e-04 - 9s/epoch - 101us/sample
Epoch 85/85
84077/84077 - 9s - loss: 2.2647e-04 - val_loss: 2.1256e-04 - 9s/epoch - 101us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00021255611818316893
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 10:54:41.326354: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:7151 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031009049324683808
cosine 0.030630044646181354
MAE: 0.003109105259338565
RMSE: 0.010487518946026684
r2: 0.9142381192836994
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 64, 85, 0.0012, 0.2, 188, 0.0002264690948491715, 0.00021255611818316893, 0.031009049324683808, 0.030630044646181354, 0.003109105259338565, 0.010487518946026684, 0.9142381192836994, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0008 32 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 1886)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.0_cr0.2_bs32_ep90_loss_mse_lr0.0008_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 10:54:49.679856: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_19/beta/Assign' id:8170 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_19/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_19/beta, batch_normalization_19/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 10:54:51.392076: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_9/kernel/v/Assign' id:9151 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_9/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_9/kernel/v, dense_enc0_9/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 10:54:53.045497: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:8856 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.07488631812005543
cosine 0.07394203763437976
MAE: 0.0032573903399598786
RMSE: 0.01638197646617649
r2: 0.7903869158463871
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.0custom_VAE', 'mse', 32, 90, 0.0008, 0.2, 188, '--', '--', 0.07488631812005543, 0.07394203763437976, 0.0032573903399598786, 0.01638197646617649, 0.7903869158463871, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.8 90 0.0008 16 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 1697)         0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 10:55:01.293553: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_10/kernel/Assign' id:9427 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_10/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_10/kernel, dense_enc0_10/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 10:55:23.693487: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:9867 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 25s - loss: 0.0022 - val_loss: 6.0850e-04 - 25s/epoch - 294us/sample
Epoch 2/90
84077/84077 - 22s - loss: 6.3184e-04 - val_loss: 5.2186e-04 - 22s/epoch - 265us/sample
Epoch 3/90
84077/84077 - 22s - loss: 4.6000e-04 - val_loss: 4.0162e-04 - 22s/epoch - 267us/sample
Epoch 4/90
84077/84077 - 22s - loss: 4.0197e-04 - val_loss: 3.7959e-04 - 22s/epoch - 266us/sample
Epoch 5/90
84077/84077 - 22s - loss: 3.7564e-04 - val_loss: 3.6804e-04 - 22s/epoch - 264us/sample
Epoch 6/90
84077/84077 - 22s - loss: 3.5600e-04 - val_loss: 3.5965e-04 - 22s/epoch - 266us/sample
Epoch 7/90
84077/84077 - 22s - loss: 3.3928e-04 - val_loss: 3.6134e-04 - 22s/epoch - 264us/sample
Epoch 8/90
84077/84077 - 22s - loss: 3.2904e-04 - val_loss: 3.6007e-04 - 22s/epoch - 265us/sample
Epoch 9/90
84077/84077 - 22s - loss: 3.2184e-04 - val_loss: 3.6238e-04 - 22s/epoch - 266us/sample
Epoch 10/90
84077/84077 - 22s - loss: 3.1411e-04 - val_loss: 3.5883e-04 - 22s/epoch - 264us/sample
Epoch 11/90
84077/84077 - 22s - loss: 3.0850e-04 - val_loss: 3.3375e-04 - 22s/epoch - 265us/sample
Epoch 12/90
84077/84077 - 22s - loss: 3.0415e-04 - val_loss: 3.1934e-04 - 22s/epoch - 265us/sample
Epoch 13/90
84077/84077 - 22s - loss: 2.9922e-04 - val_loss: 3.2516e-04 - 22s/epoch - 264us/sample
Epoch 14/90
84077/84077 - 22s - loss: 2.9468e-04 - val_loss: 3.1218e-04 - 22s/epoch - 266us/sample
Epoch 15/90
84077/84077 - 22s - loss: 2.8967e-04 - val_loss: 3.0456e-04 - 22s/epoch - 265us/sample
Epoch 16/90
84077/84077 - 22s - loss: 2.8591e-04 - val_loss: 2.9084e-04 - 22s/epoch - 266us/sample
Epoch 17/90
84077/84077 - 22s - loss: 2.8211e-04 - val_loss: 2.9175e-04 - 22s/epoch - 265us/sample
Epoch 18/90
84077/84077 - 22s - loss: 2.7908e-04 - val_loss: 2.9161e-04 - 22s/epoch - 265us/sample
Epoch 19/90
84077/84077 - 22s - loss: 2.7587e-04 - val_loss: 2.8897e-04 - 22s/epoch - 265us/sample
Epoch 20/90
84077/84077 - 22s - loss: 2.7293e-04 - val_loss: 2.7441e-04 - 22s/epoch - 266us/sample
Epoch 21/90
84077/84077 - 22s - loss: 2.6986e-04 - val_loss: 2.6444e-04 - 22s/epoch - 264us/sample
Epoch 22/90
84077/84077 - 22s - loss: 2.6631e-04 - val_loss: 2.6517e-04 - 22s/epoch - 266us/sample
Epoch 23/90
84077/84077 - 22s - loss: 2.6563e-04 - val_loss: 2.6237e-04 - 22s/epoch - 265us/sample
Epoch 24/90
84077/84077 - 22s - loss: 2.6083e-04 - val_loss: 2.6263e-04 - 22s/epoch - 266us/sample
Epoch 25/90
84077/84077 - 22s - loss: 2.5813e-04 - val_loss: 2.5728e-04 - 22s/epoch - 266us/sample
Epoch 26/90
84077/84077 - 22s - loss: 2.5622e-04 - val_loss: 2.5007e-04 - 22s/epoch - 265us/sample
Epoch 27/90
84077/84077 - 22s - loss: 2.5441e-04 - val_loss: 2.4855e-04 - 22s/epoch - 265us/sample
Epoch 28/90
84077/84077 - 22s - loss: 2.5368e-04 - val_loss: 2.4691e-04 - 22s/epoch - 266us/sample
Epoch 29/90
84077/84077 - 22s - loss: 2.5200e-04 - val_loss: 2.5375e-04 - 22s/epoch - 264us/sample
Epoch 30/90
84077/84077 - 22s - loss: 2.4980e-04 - val_loss: 2.5182e-04 - 22s/epoch - 266us/sample
Epoch 31/90
84077/84077 - 22s - loss: 2.4936e-04 - val_loss: 2.4787e-04 - 22s/epoch - 266us/sample
Epoch 32/90
84077/84077 - 22s - loss: 2.4650e-04 - val_loss: 2.3957e-04 - 22s/epoch - 265us/sample
Epoch 33/90
84077/84077 - 22s - loss: 2.4516e-04 - val_loss: 2.4280e-04 - 22s/epoch - 265us/sample
Epoch 34/90
84077/84077 - 22s - loss: 2.4392e-04 - val_loss: 2.4708e-04 - 22s/epoch - 265us/sample
Epoch 35/90
84077/84077 - 22s - loss: 2.4301e-04 - val_loss: 2.3685e-04 - 22s/epoch - 265us/sample
Epoch 36/90
84077/84077 - 22s - loss: 2.4171e-04 - val_loss: 2.3667e-04 - 22s/epoch - 265us/sample
Epoch 37/90
84077/84077 - 22s - loss: 2.4003e-04 - val_loss: 2.3686e-04 - 22s/epoch - 264us/sample
Epoch 38/90
84077/84077 - 22s - loss: 2.3797e-04 - val_loss: 2.3214e-04 - 22s/epoch - 265us/sample
Epoch 39/90
84077/84077 - 22s - loss: 2.3868e-04 - val_loss: 2.3026e-04 - 22s/epoch - 264us/sample
Epoch 40/90
84077/84077 - 22s - loss: 2.3566e-04 - val_loss: 2.2951e-04 - 22s/epoch - 266us/sample
Epoch 41/90
84077/84077 - 22s - loss: 2.3567e-04 - val_loss: 2.3226e-04 - 22s/epoch - 265us/sample
Epoch 42/90
84077/84077 - 22s - loss: 2.3461e-04 - val_loss: 2.2705e-04 - 22s/epoch - 266us/sample
Epoch 43/90
84077/84077 - 22s - loss: 2.3299e-04 - val_loss: 2.2287e-04 - 22s/epoch - 264us/sample
Epoch 44/90
84077/84077 - 22s - loss: 2.3244e-04 - val_loss: 2.2488e-04 - 22s/epoch - 266us/sample
Epoch 45/90
84077/84077 - 22s - loss: 2.3232e-04 - val_loss: 2.2373e-04 - 22s/epoch - 264us/sample
Epoch 46/90
84077/84077 - 22s - loss: 2.3103e-04 - val_loss: 2.2278e-04 - 22s/epoch - 266us/sample
Epoch 47/90
84077/84077 - 22s - loss: 2.3041e-04 - val_loss: 2.1806e-04 - 22s/epoch - 265us/sample
Epoch 48/90
84077/84077 - 22s - loss: 2.2847e-04 - val_loss: 2.1973e-04 - 22s/epoch - 265us/sample
Epoch 49/90
84077/84077 - 22s - loss: 2.2826e-04 - val_loss: 2.2144e-04 - 22s/epoch - 265us/sample
Epoch 50/90
84077/84077 - 22s - loss: 2.2636e-04 - val_loss: 2.1479e-04 - 22s/epoch - 265us/sample
Epoch 51/90
84077/84077 - 22s - loss: 2.2739e-04 - val_loss: 2.0924e-04 - 22s/epoch - 265us/sample
Epoch 52/90
84077/84077 - 22s - loss: 2.2455e-04 - val_loss: 2.0966e-04 - 22s/epoch - 265us/sample
Epoch 53/90
84077/84077 - 22s - loss: 2.2435e-04 - val_loss: 2.1576e-04 - 22s/epoch - 265us/sample
Epoch 54/90
84077/84077 - 22s - loss: 2.2249e-04 - val_loss: 2.1341e-04 - 22s/epoch - 265us/sample
Epoch 55/90
84077/84077 - 22s - loss: 2.2372e-04 - val_loss: 2.1162e-04 - 22s/epoch - 265us/sample
Epoch 56/90
84077/84077 - 22s - loss: 2.2081e-04 - val_loss: 2.1030e-04 - 22s/epoch - 265us/sample
Epoch 57/90
84077/84077 - 22s - loss: 2.2184e-04 - val_loss: 2.0833e-04 - 22s/epoch - 265us/sample
Epoch 58/90
84077/84077 - 22s - loss: 2.2197e-04 - val_loss: 2.1229e-04 - 22s/epoch - 264us/sample
Epoch 59/90
84077/84077 - 22s - loss: 2.2050e-04 - val_loss: 2.0808e-04 - 22s/epoch - 266us/sample
Epoch 60/90
84077/84077 - 22s - loss: 2.1917e-04 - val_loss: 2.0883e-04 - 22s/epoch - 265us/sample
Epoch 61/90
84077/84077 - 22s - loss: 2.2060e-04 - val_loss: 2.0816e-04 - 22s/epoch - 265us/sample
Epoch 62/90
84077/84077 - 22s - loss: 2.1883e-04 - val_loss: 2.0662e-04 - 22s/epoch - 265us/sample
Epoch 63/90
84077/84077 - 22s - loss: 2.1907e-04 - val_loss: 2.0663e-04 - 22s/epoch - 265us/sample
Epoch 64/90
84077/84077 - 22s - loss: 2.1622e-04 - val_loss: 2.0545e-04 - 22s/epoch - 264us/sample
Epoch 65/90
84077/84077 - 22s - loss: 2.1738e-04 - val_loss: 2.0308e-04 - 22s/epoch - 266us/sample
Epoch 66/90
84077/84077 - 22s - loss: 2.1787e-04 - val_loss: 2.0776e-04 - 22s/epoch - 264us/sample
Epoch 67/90
84077/84077 - 22s - loss: 2.1510e-04 - val_loss: 2.0295e-04 - 22s/epoch - 266us/sample
Epoch 68/90
84077/84077 - 22s - loss: 2.1583e-04 - val_loss: 2.0353e-04 - 22s/epoch - 264us/sample
Epoch 69/90
84077/84077 - 22s - loss: 2.1509e-04 - val_loss: 2.0573e-04 - 22s/epoch - 266us/sample
Epoch 70/90
84077/84077 - 22s - loss: 2.1436e-04 - val_loss: 2.0011e-04 - 22s/epoch - 264us/sample
Epoch 71/90
84077/84077 - 22s - loss: 2.1347e-04 - val_loss: 2.0126e-04 - 22s/epoch - 266us/sample
Epoch 72/90
84077/84077 - 22s - loss: 2.1346e-04 - val_loss: 2.0124e-04 - 22s/epoch - 265us/sample
Epoch 73/90
84077/84077 - 22s - loss: 2.1392e-04 - val_loss: 2.0296e-04 - 22s/epoch - 266us/sample
Epoch 74/90
84077/84077 - 22s - loss: 2.1249e-04 - val_loss: 1.9817e-04 - 22s/epoch - 265us/sample
Epoch 75/90
84077/84077 - 22s - loss: 2.1271e-04 - val_loss: 2.0258e-04 - 22s/epoch - 264us/sample
Epoch 76/90
84077/84077 - 22s - loss: 2.1303e-04 - val_loss: 1.9819e-04 - 22s/epoch - 266us/sample
Epoch 77/90
84077/84077 - 22s - loss: 2.1146e-04 - val_loss: 2.0444e-04 - 22s/epoch - 264us/sample
Epoch 78/90
84077/84077 - 22s - loss: 2.1030e-04 - val_loss: 1.9791e-04 - 22s/epoch - 266us/sample
Epoch 79/90
84077/84077 - 22s - loss: 2.1284e-04 - val_loss: 1.9721e-04 - 22s/epoch - 264us/sample
Epoch 80/90
84077/84077 - 22s - loss: 2.1278e-04 - val_loss: 2.0211e-04 - 22s/epoch - 266us/sample
Epoch 81/90
84077/84077 - 22s - loss: 2.1044e-04 - val_loss: 2.0027e-04 - 22s/epoch - 265us/sample
Epoch 82/90
84077/84077 - 22s - loss: 2.0974e-04 - val_loss: 2.0349e-04 - 22s/epoch - 266us/sample
Epoch 83/90
84077/84077 - 22s - loss: 2.0881e-04 - val_loss: 2.0082e-04 - 22s/epoch - 264us/sample
Epoch 84/90
84077/84077 - 22s - loss: 2.0955e-04 - val_loss: 2.0065e-04 - 22s/epoch - 266us/sample
Epoch 85/90
84077/84077 - 22s - loss: 2.0939e-04 - val_loss: 1.9630e-04 - 22s/epoch - 265us/sample
Epoch 86/90
84077/84077 - 22s - loss: 2.0884e-04 - val_loss: 1.9873e-04 - 22s/epoch - 265us/sample
Epoch 87/90
84077/84077 - 22s - loss: 2.0784e-04 - val_loss: 1.9619e-04 - 22s/epoch - 265us/sample
Epoch 88/90
84077/84077 - 22s - loss: 2.0797e-04 - val_loss: 1.9895e-04 - 22s/epoch - 266us/sample
Epoch 89/90
84077/84077 - 22s - loss: 2.0799e-04 - val_loss: 2.0118e-04 - 22s/epoch - 265us/sample
Epoch 90/90
84077/84077 - 22s - loss: 2.0687e-04 - val_loss: 1.9766e-04 - 22s/epoch - 265us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00019765760348143755
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 11:28:29.468873: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:9831 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.06488708859530226
cosine 0.06410705174380359
MAE: 0.003114273216865522
RMSE: 0.015878361113036525
r2: 0.803123968969067
RMSE zero-vector: 0.04004287452915337
['1.8custom_VAE', 'logcosh', 16, 90, 0.0008, 0.2, 188, 0.00020686565997590225, 0.00019765760348143755, 0.06488708859530226, 0.06410705174380359, 0.003114273216865522, 0.015878361113036525, 0.803123968969067, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0006 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 2074)         0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 11:28:38.420861: W tensorflow/c/c_api.cc:291] Operation '{name:'training_10/Adam/iter/Assign' id:11659 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_10/Adam/iter, training_10/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 11:28:48.339374: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:11167 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0720 - val_loss: 0.0686 - 12s/epoch - 141us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0689 - val_loss: 19.3464 - 9s/epoch - 109us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.3138 - val_loss: 0.0700 - 9s/epoch - 109us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0720 - val_loss: 0.0716 - 9s/epoch - 109us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0723 - val_loss: 0.0700 - 9s/epoch - 109us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0709 - val_loss: 0.0700 - 9s/epoch - 110us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0678 - val_loss: 0.0678 - 9s/epoch - 109us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.0671 - val_loss: 0.0675 - 9s/epoch - 110us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0675 - 9s/epoch - 109us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 13/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 18/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 19/90
84077/84077 - 9s - loss: 0.0684 - val_loss: 0.0675 - 9s/epoch - 110us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0675 - 9s/epoch - 109us/sample
Epoch 21/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 22/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 23/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 111us/sample
Epoch 24/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 25/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 26/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 27/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 29/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 30/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 31/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 32/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 33/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 35/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 36/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 37/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 40/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 41/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 42/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 43/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 45/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 46/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 47/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 53/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 54/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 58/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 59/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 60/90
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 121us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 64/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 65/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 69/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 70/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 71/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 72/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 77/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 78/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 83/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 84/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 85/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 86/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 87/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 89/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0673457440144371
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 11:42:29.605110: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:11119 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2762837335157744
cosine 1.1929173499426404
MAE: 5.780259550233858
RMSE: 7.109492916513937
r2: -37127.08939561767
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'binary_crossentropy', 64, 90, 0.0006, 0.2, 188, 0.06683693824718688, 0.0673457440144371, 1.2762837335157744, 1.1929173499426404, 5.780259550233858, 7.109492916513937, -37127.08939561767, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.8 95 0.0012 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 1697)         0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/95
2023-02-12 11:42:38.912463: W tensorflow/c/c_api.cc:291] Operation '{name:'training_12/Adam/dense_dec1_12/bias/v/Assign' id:13112 op device:{requested: '', assigned: ''} def:{{{node training_12/Adam/dense_dec1_12/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_12/Adam/dense_dec1_12/bias/v, training_12/Adam/dense_dec1_12/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 11:42:48.608718: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:12488 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0085 - val_loss: 0.0016 - 12s/epoch - 140us/sample
Epoch 2/95
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0016 - 9s/epoch - 105us/sample
Epoch 3/95
84077/84077 - 9s - loss: 0.0018 - val_loss: 0.0011 - 9s/epoch - 104us/sample
Epoch 4/95
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0012 - 9s/epoch - 105us/sample
Epoch 5/95
84077/84077 - 9s - loss: 0.0060 - val_loss: 0.0042 - 9s/epoch - 106us/sample
Epoch 6/95
84077/84077 - 9s - loss: 0.0165 - val_loss: 0.0024 - 9s/epoch - 105us/sample
Epoch 7/95
84077/84077 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 104us/sample
Epoch 8/95
84077/84077 - 9s - loss: 0.0012 - val_loss: 0.0018 - 9s/epoch - 105us/sample
Epoch 9/95
84077/84077 - 9s - loss: 9.4675e-04 - val_loss: 7.9369e-04 - 9s/epoch - 104us/sample
Epoch 10/95
84077/84077 - 9s - loss: 8.1780e-04 - val_loss: 7.3489e-04 - 9s/epoch - 105us/sample
Epoch 11/95
84077/84077 - 9s - loss: 7.4807e-04 - val_loss: 6.6638e-04 - 9s/epoch - 106us/sample
Epoch 12/95
84077/84077 - 9s - loss: 7.7201e-04 - val_loss: 0.0015 - 9s/epoch - 105us/sample
Epoch 13/95
84077/84077 - 9s - loss: 6.8578e-04 - val_loss: 5.8173e-04 - 9s/epoch - 105us/sample
Epoch 14/95
84077/84077 - 9s - loss: 6.0393e-04 - val_loss: 5.3876e-04 - 9s/epoch - 104us/sample
Epoch 15/95
84077/84077 - 9s - loss: 5.7884e-04 - val_loss: 6.3649e-04 - 9s/epoch - 105us/sample
Epoch 16/95
84077/84077 - 9s - loss: 5.3572e-04 - val_loss: 4.8286e-04 - 9s/epoch - 105us/sample
Epoch 17/95
84077/84077 - 9s - loss: 5.2001e-04 - val_loss: 4.8324e-04 - 9s/epoch - 105us/sample
Epoch 18/95
84077/84077 - 9s - loss: 4.9356e-04 - val_loss: 4.4621e-04 - 9s/epoch - 105us/sample
Epoch 19/95
84077/84077 - 9s - loss: 4.7769e-04 - val_loss: 4.3200e-04 - 9s/epoch - 105us/sample
Epoch 20/95
84077/84077 - 9s - loss: 4.6190e-04 - val_loss: 4.2058e-04 - 9s/epoch - 106us/sample
Epoch 21/95
84077/84077 - 9s - loss: 4.4843e-04 - val_loss: 4.1006e-04 - 9s/epoch - 105us/sample
Epoch 22/95
84077/84077 - 9s - loss: 4.4136e-04 - val_loss: 4.0006e-04 - 9s/epoch - 105us/sample
Epoch 23/95
84077/84077 - 9s - loss: 4.3110e-04 - val_loss: 3.9587e-04 - 9s/epoch - 104us/sample
Epoch 24/95
84077/84077 - 9s - loss: 4.2537e-04 - val_loss: 3.8845e-04 - 9s/epoch - 105us/sample
Epoch 25/95
84077/84077 - 9s - loss: 4.1787e-04 - val_loss: 3.8305e-04 - 9s/epoch - 106us/sample
Epoch 26/95
84077/84077 - 9s - loss: 4.0939e-04 - val_loss: 3.8044e-04 - 9s/epoch - 105us/sample
Epoch 27/95
84077/84077 - 9s - loss: 4.0321e-04 - val_loss: 3.7246e-04 - 9s/epoch - 105us/sample
Epoch 28/95
84077/84077 - 9s - loss: 3.9929e-04 - val_loss: 3.6992e-04 - 9s/epoch - 104us/sample
Epoch 29/95
84077/84077 - 9s - loss: 3.9596e-04 - val_loss: 3.6614e-04 - 9s/epoch - 105us/sample
Epoch 30/95
84077/84077 - 9s - loss: 3.9221e-04 - val_loss: 3.6220e-04 - 9s/epoch - 106us/sample
Epoch 31/95
84077/84077 - 9s - loss: 3.8678e-04 - val_loss: 3.5957e-04 - 9s/epoch - 105us/sample
Epoch 32/95
84077/84077 - 9s - loss: 3.8239e-04 - val_loss: 3.5389e-04 - 9s/epoch - 105us/sample
Epoch 33/95
84077/84077 - 9s - loss: 3.7727e-04 - val_loss: 3.4979e-04 - 9s/epoch - 104us/sample
Epoch 34/95
84077/84077 - 9s - loss: 3.7382e-04 - val_loss: 3.4906e-04 - 9s/epoch - 104us/sample
Epoch 35/95
84077/84077 - 9s - loss: 3.7154e-04 - val_loss: 3.4708e-04 - 9s/epoch - 106us/sample
Epoch 36/95
84077/84077 - 9s - loss: 3.6883e-04 - val_loss: 3.4524e-04 - 9s/epoch - 105us/sample
Epoch 37/95
84077/84077 - 9s - loss: 3.6583e-04 - val_loss: 3.4338e-04 - 9s/epoch - 105us/sample
Epoch 38/95
84077/84077 - 9s - loss: 3.6444e-04 - val_loss: 3.4243e-04 - 9s/epoch - 104us/sample
Epoch 39/95
84077/84077 - 9s - loss: 3.6211e-04 - val_loss: 3.4143e-04 - 9s/epoch - 105us/sample
Epoch 40/95
84077/84077 - 9s - loss: 3.6039e-04 - val_loss: 3.3815e-04 - 9s/epoch - 105us/sample
Epoch 41/95
84077/84077 - 9s - loss: 3.5848e-04 - val_loss: 3.3764e-04 - 9s/epoch - 105us/sample
Epoch 42/95
84077/84077 - 9s - loss: 3.5680e-04 - val_loss: 3.3386e-04 - 9s/epoch - 106us/sample
Epoch 43/95
84077/84077 - 9s - loss: 3.5444e-04 - val_loss: 3.3304e-04 - 9s/epoch - 105us/sample
Epoch 44/95
84077/84077 - 9s - loss: 3.5285e-04 - val_loss: 3.3200e-04 - 9s/epoch - 105us/sample
Epoch 45/95
84077/84077 - 9s - loss: 3.5078e-04 - val_loss: 3.2993e-04 - 9s/epoch - 105us/sample
Epoch 46/95
84077/84077 - 9s - loss: 3.4732e-04 - val_loss: 3.2427e-04 - 9s/epoch - 107us/sample
Epoch 47/95
84077/84077 - 9s - loss: 3.4527e-04 - val_loss: 3.2599e-04 - 9s/epoch - 105us/sample
Epoch 48/95
84077/84077 - 9s - loss: 3.4401e-04 - val_loss: 3.2426e-04 - 9s/epoch - 104us/sample
Epoch 49/95
84077/84077 - 9s - loss: 3.4248e-04 - val_loss: 3.2320e-04 - 9s/epoch - 105us/sample
Epoch 50/95
84077/84077 - 9s - loss: 3.4119e-04 - val_loss: 3.2219e-04 - 9s/epoch - 105us/sample
Epoch 51/95
84077/84077 - 9s - loss: 3.3968e-04 - val_loss: 3.1891e-04 - 9s/epoch - 106us/sample
Epoch 52/95
84077/84077 - 9s - loss: 3.3853e-04 - val_loss: 3.1889e-04 - 9s/epoch - 105us/sample
Epoch 53/95
84077/84077 - 9s - loss: 3.3779e-04 - val_loss: 3.1721e-04 - 9s/epoch - 105us/sample
Epoch 54/95
84077/84077 - 9s - loss: 3.3709e-04 - val_loss: 3.1962e-04 - 9s/epoch - 105us/sample
Epoch 55/95
84077/84077 - 9s - loss: 3.3628e-04 - val_loss: 3.1735e-04 - 9s/epoch - 105us/sample
Epoch 56/95
84077/84077 - 9s - loss: 3.3507e-04 - val_loss: 3.1738e-04 - 9s/epoch - 106us/sample
Epoch 57/95
84077/84077 - 9s - loss: 3.3464e-04 - val_loss: 3.1470e-04 - 9s/epoch - 105us/sample
Epoch 58/95
84077/84077 - 9s - loss: 3.3345e-04 - val_loss: 3.1313e-04 - 9s/epoch - 105us/sample
Epoch 59/95
84077/84077 - 9s - loss: 3.3327e-04 - val_loss: 3.1483e-04 - 9s/epoch - 104us/sample
Epoch 60/95
84077/84077 - 9s - loss: 3.3266e-04 - val_loss: 3.1386e-04 - 9s/epoch - 105us/sample
Epoch 61/95
84077/84077 - 9s - loss: 3.3115e-04 - val_loss: 3.1477e-04 - 9s/epoch - 105us/sample
Epoch 62/95
84077/84077 - 9s - loss: 3.3080e-04 - val_loss: 3.1352e-04 - 9s/epoch - 105us/sample
Epoch 63/95
84077/84077 - 9s - loss: 3.2847e-04 - val_loss: 3.1159e-04 - 9s/epoch - 105us/sample
Epoch 64/95
84077/84077 - 9s - loss: 3.2799e-04 - val_loss: 3.1022e-04 - 9s/epoch - 105us/sample
Epoch 65/95
84077/84077 - 9s - loss: 3.2694e-04 - val_loss: 3.1020e-04 - 9s/epoch - 104us/sample
Epoch 66/95
84077/84077 - 9s - loss: 3.2540e-04 - val_loss: 3.0893e-04 - 9s/epoch - 105us/sample
Epoch 67/95
84077/84077 - 9s - loss: 3.2510e-04 - val_loss: 3.0840e-04 - 9s/epoch - 106us/sample
Epoch 68/95
84077/84077 - 9s - loss: 3.2513e-04 - val_loss: 3.0931e-04 - 9s/epoch - 105us/sample
Epoch 69/95
84077/84077 - 9s - loss: 3.2432e-04 - val_loss: 3.0896e-04 - 9s/epoch - 105us/sample
Epoch 70/95
84077/84077 - 9s - loss: 3.2387e-04 - val_loss: 3.0766e-04 - 9s/epoch - 105us/sample
Epoch 71/95
84077/84077 - 9s - loss: 3.2302e-04 - val_loss: 3.0570e-04 - 9s/epoch - 104us/sample
Epoch 72/95
84077/84077 - 9s - loss: 3.2178e-04 - val_loss: 3.0537e-04 - 9s/epoch - 105us/sample
Epoch 73/95
84077/84077 - 9s - loss: 3.2134e-04 - val_loss: 3.0527e-04 - 9s/epoch - 106us/sample
Epoch 74/95
84077/84077 - 9s - loss: 3.2018e-04 - val_loss: 3.0306e-04 - 9s/epoch - 105us/sample
Epoch 75/95
84077/84077 - 9s - loss: 3.1981e-04 - val_loss: 3.0433e-04 - 9s/epoch - 105us/sample
Epoch 76/95
84077/84077 - 9s - loss: 3.1894e-04 - val_loss: 3.0461e-04 - 9s/epoch - 105us/sample
Epoch 77/95
84077/84077 - 9s - loss: 3.1913e-04 - val_loss: 3.0319e-04 - 9s/epoch - 104us/sample
Epoch 78/95
84077/84077 - 9s - loss: 3.1836e-04 - val_loss: 3.0332e-04 - 9s/epoch - 105us/sample
Epoch 79/95
84077/84077 - 9s - loss: 3.1823e-04 - val_loss: 3.0194e-04 - 9s/epoch - 105us/sample
Epoch 80/95
84077/84077 - 9s - loss: 3.1801e-04 - val_loss: 3.0135e-04 - 9s/epoch - 105us/sample
Epoch 81/95
84077/84077 - 9s - loss: 3.1774e-04 - val_loss: 3.0207e-04 - 9s/epoch - 104us/sample
Epoch 82/95
84077/84077 - 9s - loss: 3.1750e-04 - val_loss: 3.0196e-04 - 9s/epoch - 105us/sample
Epoch 83/95
84077/84077 - 9s - loss: 3.1651e-04 - val_loss: 3.0220e-04 - 9s/epoch - 106us/sample
Epoch 84/95
84077/84077 - 9s - loss: 3.1600e-04 - val_loss: 3.0175e-04 - 9s/epoch - 105us/sample
Epoch 85/95
84077/84077 - 9s - loss: 3.1561e-04 - val_loss: 3.0021e-04 - 9s/epoch - 105us/sample
Epoch 86/95
84077/84077 - 9s - loss: 3.1544e-04 - val_loss: 2.9962e-04 - 9s/epoch - 105us/sample
Epoch 87/95
84077/84077 - 9s - loss: 3.1386e-04 - val_loss: 2.9858e-04 - 9s/epoch - 105us/sample
Epoch 88/95
84077/84077 - 9s - loss: 3.1252e-04 - val_loss: 2.9772e-04 - 9s/epoch - 105us/sample
Epoch 89/95
84077/84077 - 9s - loss: 3.1253e-04 - val_loss: 2.9915e-04 - 9s/epoch - 106us/sample
Epoch 90/95
84077/84077 - 9s - loss: 3.1209e-04 - val_loss: 2.9810e-04 - 9s/epoch - 105us/sample
Epoch 91/95
84077/84077 - 9s - loss: 3.1096e-04 - val_loss: 2.9852e-04 - 9s/epoch - 105us/sample
Epoch 92/95
84077/84077 - 9s - loss: 3.1176e-04 - val_loss: 2.9847e-04 - 9s/epoch - 104us/sample
Epoch 93/95
84077/84077 - 9s - loss: 3.1060e-04 - val_loss: 2.9633e-04 - 9s/epoch - 105us/sample
Epoch 94/95
84077/84077 - 9s - loss: 3.1043e-04 - val_loss: 2.9679e-04 - 9s/epoch - 106us/sample
Epoch 95/95
84077/84077 - 9s - loss: 3.0975e-04 - val_loss: 2.9636e-04 - 9s/epoch - 105us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00029636270552185735
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 11:56:39.401319: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:12459 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05590143664111676
cosine 0.05521275462743681
MAE: 0.002754631187300762
RMSE: 0.01427379284700277
r2: 0.8408826259801191
RMSE zero-vector: 0.04004287452915337
['1.8custom_VAE', 'mse', 64, 95, 0.0012, 0.2, 188, 0.00030975133138302685, 0.00029636270552185735, 0.05590143664111676, 0.05521275462743681, 0.002754631187300762, 0.01427379284700277, 0.8408826259801191, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0014 64 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 1980)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 11:56:48.908940: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_13/kernel/Assign' id:13421 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_13/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_13/kernel, bottleneck_zlog_13/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 11:56:58.753295: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:13743 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0551 - val_loss: 0.0016 - 12s/epoch - 143us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 105us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0079 - val_loss: 0.0018 - 9s/epoch - 106us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0013 - val_loss: 9.8772e-04 - 9s/epoch - 105us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0055 - val_loss: 0.0029 - 9s/epoch - 105us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0029 - val_loss: 0.0055 - 9s/epoch - 105us/sample
Epoch 7/85
84077/84077 - 9s - loss: 0.0027 - val_loss: 0.0012 - 9s/epoch - 106us/sample
Epoch 8/85
84077/84077 - 9s - loss: 0.0015 - val_loss: 9.1776e-04 - 9s/epoch - 105us/sample
Epoch 9/85
84077/84077 - 9s - loss: 0.0022 - val_loss: 0.0011 - 9s/epoch - 105us/sample
Epoch 10/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 8.3465e-04 - 9s/epoch - 105us/sample
Epoch 11/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 7.8802e-04 - 9s/epoch - 106us/sample
Epoch 12/85
84077/84077 - 9s - loss: 7.3843e-04 - val_loss: 6.2979e-04 - 9s/epoch - 106us/sample
Epoch 13/85
84077/84077 - 9s - loss: 6.9678e-04 - val_loss: 5.8417e-04 - 9s/epoch - 105us/sample
Epoch 14/85
84077/84077 - 9s - loss: 6.1402e-04 - val_loss: 0.0020 - 9s/epoch - 106us/sample
Epoch 15/85
84077/84077 - 9s - loss: 6.1648e-04 - val_loss: 4.9526e-04 - 9s/epoch - 105us/sample
Epoch 16/85
84077/84077 - 9s - loss: 5.3512e-04 - val_loss: 4.6713e-04 - 9s/epoch - 105us/sample
Epoch 17/85
84077/84077 - 9s - loss: 5.2413e-04 - val_loss: 4.4400e-04 - 9s/epoch - 105us/sample
Epoch 18/85
84077/84077 - 9s - loss: 6.1168e-04 - val_loss: 4.6058e-04 - 9s/epoch - 106us/sample
Epoch 19/85
84077/84077 - 9s - loss: 4.7645e-04 - val_loss: 4.4465e-04 - 9s/epoch - 105us/sample
Epoch 20/85
84077/84077 - 9s - loss: 4.3760e-04 - val_loss: 3.8649e-04 - 9s/epoch - 105us/sample
Epoch 21/85
84077/84077 - 9s - loss: 4.1495e-04 - val_loss: 3.7375e-04 - 9s/epoch - 105us/sample
Epoch 22/85
84077/84077 - 9s - loss: 4.1517e-04 - val_loss: 3.6667e-04 - 9s/epoch - 105us/sample
Epoch 23/85
84077/84077 - 9s - loss: 3.9134e-04 - val_loss: 3.5610e-04 - 9s/epoch - 106us/sample
Epoch 24/85
84077/84077 - 9s - loss: 3.8656e-04 - val_loss: 3.4842e-04 - 9s/epoch - 106us/sample
Epoch 25/85
84077/84077 - 9s - loss: 3.7205e-04 - val_loss: 3.4115e-04 - 9s/epoch - 105us/sample
Epoch 26/85
84077/84077 - 9s - loss: 3.6661e-04 - val_loss: 3.3043e-04 - 9s/epoch - 105us/sample
Epoch 27/85
84077/84077 - 9s - loss: 3.5713e-04 - val_loss: 3.3390e-04 - 9s/epoch - 105us/sample
Epoch 28/85
84077/84077 - 9s - loss: 3.5138e-04 - val_loss: 3.2334e-04 - 9s/epoch - 105us/sample
Epoch 29/85
84077/84077 - 9s - loss: 3.4674e-04 - val_loss: 3.1851e-04 - 9s/epoch - 106us/sample
Epoch 30/85
84077/84077 - 9s - loss: 3.4171e-04 - val_loss: 3.1675e-04 - 9s/epoch - 106us/sample
Epoch 31/85
84077/84077 - 9s - loss: 3.3763e-04 - val_loss: 3.1367e-04 - 9s/epoch - 105us/sample
Epoch 32/85
84077/84077 - 9s - loss: 3.3393e-04 - val_loss: 3.0892e-04 - 9s/epoch - 105us/sample
Epoch 33/85
84077/84077 - 9s - loss: 3.3010e-04 - val_loss: 3.0772e-04 - 9s/epoch - 105us/sample
Epoch 34/85
84077/84077 - 9s - loss: 3.2720e-04 - val_loss: 3.0444e-04 - 9s/epoch - 105us/sample
Epoch 35/85
84077/84077 - 9s - loss: 3.2403e-04 - val_loss: 3.0259e-04 - 9s/epoch - 106us/sample
Epoch 36/85
84077/84077 - 9s - loss: 3.2291e-04 - val_loss: 3.0045e-04 - 9s/epoch - 105us/sample
Epoch 37/85
84077/84077 - 9s - loss: 3.2011e-04 - val_loss: 2.9899e-04 - 9s/epoch - 105us/sample
Epoch 38/85
84077/84077 - 9s - loss: 3.1875e-04 - val_loss: 2.9910e-04 - 9s/epoch - 105us/sample
Epoch 39/85
84077/84077 - 9s - loss: 3.1670e-04 - val_loss: 2.9799e-04 - 9s/epoch - 105us/sample
Epoch 40/85
84077/84077 - 9s - loss: 3.1493e-04 - val_loss: 2.9620e-04 - 9s/epoch - 106us/sample
Epoch 41/85
84077/84077 - 9s - loss: 3.1349e-04 - val_loss: 2.9555e-04 - 9s/epoch - 106us/sample
Epoch 42/85
84077/84077 - 9s - loss: 3.1239e-04 - val_loss: 2.9309e-04 - 9s/epoch - 105us/sample
Epoch 43/85
84077/84077 - 9s - loss: 3.1114e-04 - val_loss: 2.9097e-04 - 9s/epoch - 105us/sample
Epoch 44/85
84077/84077 - 9s - loss: 3.0935e-04 - val_loss: 2.9779e-04 - 9s/epoch - 105us/sample
Epoch 45/85
84077/84077 - 9s - loss: 3.0892e-04 - val_loss: 2.9160e-04 - 9s/epoch - 105us/sample
Epoch 46/85
84077/84077 - 9s - loss: 3.0725e-04 - val_loss: 2.8801e-04 - 9s/epoch - 106us/sample
Epoch 47/85
84077/84077 - 9s - loss: 3.0628e-04 - val_loss: 2.8761e-04 - 9s/epoch - 105us/sample
Epoch 48/85
84077/84077 - 9s - loss: 3.0534e-04 - val_loss: 2.8916e-04 - 9s/epoch - 106us/sample
Epoch 49/85
84077/84077 - 9s - loss: 3.0403e-04 - val_loss: 2.8628e-04 - 9s/epoch - 105us/sample
Epoch 50/85
84077/84077 - 9s - loss: 3.0415e-04 - val_loss: 2.8824e-04 - 9s/epoch - 105us/sample
Epoch 51/85
84077/84077 - 9s - loss: 3.0242e-04 - val_loss: 2.8770e-04 - 9s/epoch - 106us/sample
Epoch 52/85
84077/84077 - 9s - loss: 3.0181e-04 - val_loss: 2.8645e-04 - 9s/epoch - 106us/sample
Epoch 53/85
84077/84077 - 9s - loss: 3.0106e-04 - val_loss: 2.8639e-04 - 9s/epoch - 105us/sample
Epoch 54/85
84077/84077 - 9s - loss: 2.9993e-04 - val_loss: 2.8459e-04 - 9s/epoch - 105us/sample
Epoch 55/85
84077/84077 - 9s - loss: 2.9946e-04 - val_loss: 2.8367e-04 - 9s/epoch - 105us/sample
Epoch 56/85
84077/84077 - 9s - loss: 2.9834e-04 - val_loss: 2.8217e-04 - 9s/epoch - 105us/sample
Epoch 57/85
84077/84077 - 9s - loss: 2.9694e-04 - val_loss: 2.8290e-04 - 9s/epoch - 106us/sample
Epoch 58/85
84077/84077 - 9s - loss: 2.9621e-04 - val_loss: 2.8188e-04 - 9s/epoch - 105us/sample
Epoch 59/85
84077/84077 - 9s - loss: 2.9580e-04 - val_loss: 2.8304e-04 - 9s/epoch - 105us/sample
Epoch 60/85
84077/84077 - 9s - loss: 2.9471e-04 - val_loss: 2.8206e-04 - 9s/epoch - 105us/sample
Epoch 61/85
84077/84077 - 9s - loss: 2.9363e-04 - val_loss: 2.8012e-04 - 9s/epoch - 105us/sample
Epoch 62/85
84077/84077 - 9s - loss: 2.9418e-04 - val_loss: 2.8106e-04 - 9s/epoch - 105us/sample
Epoch 63/85
84077/84077 - 9s - loss: 2.9271e-04 - val_loss: 2.7885e-04 - 9s/epoch - 106us/sample
Epoch 64/85
84077/84077 - 9s - loss: 2.9239e-04 - val_loss: 2.7806e-04 - 9s/epoch - 105us/sample
Epoch 65/85
84077/84077 - 9s - loss: 2.9200e-04 - val_loss: 2.7852e-04 - 9s/epoch - 106us/sample
Epoch 66/85
84077/84077 - 9s - loss: 2.9078e-04 - val_loss: 2.7859e-04 - 9s/epoch - 105us/sample
Epoch 67/85
84077/84077 - 9s - loss: 2.9074e-04 - val_loss: 2.7592e-04 - 9s/epoch - 105us/sample
Epoch 68/85
84077/84077 - 9s - loss: 2.8950e-04 - val_loss: 2.7626e-04 - 9s/epoch - 105us/sample
Epoch 69/85
84077/84077 - 9s - loss: 2.8927e-04 - val_loss: 2.7486e-04 - 9s/epoch - 106us/sample
Epoch 70/85
84077/84077 - 9s - loss: 2.8811e-04 - val_loss: 2.7412e-04 - 9s/epoch - 105us/sample
Epoch 71/85
84077/84077 - 9s - loss: 2.8723e-04 - val_loss: 2.7473e-04 - 9s/epoch - 105us/sample
Epoch 72/85
84077/84077 - 9s - loss: 2.8604e-04 - val_loss: 2.7469e-04 - 9s/epoch - 105us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.8645e-04 - val_loss: 2.7379e-04 - 9s/epoch - 105us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.8530e-04 - val_loss: 2.7275e-04 - 9s/epoch - 105us/sample
Epoch 75/85
84077/84077 - 9s - loss: 2.8516e-04 - val_loss: 2.7255e-04 - 9s/epoch - 106us/sample
Epoch 76/85
84077/84077 - 9s - loss: 2.8506e-04 - val_loss: 2.7228e-04 - 9s/epoch - 105us/sample
Epoch 77/85
84077/84077 - 9s - loss: 2.8384e-04 - val_loss: 2.7166e-04 - 9s/epoch - 105us/sample
Epoch 78/85
84077/84077 - 9s - loss: 2.8433e-04 - val_loss: 2.7115e-04 - 9s/epoch - 105us/sample
Epoch 79/85
84077/84077 - 9s - loss: 2.8317e-04 - val_loss: 2.7340e-04 - 9s/epoch - 105us/sample
Epoch 80/85
84077/84077 - 9s - loss: 2.8243e-04 - val_loss: 2.6982e-04 - 9s/epoch - 105us/sample
Epoch 81/85
84077/84077 - 9s - loss: 2.8301e-04 - val_loss: 2.7039e-04 - 9s/epoch - 106us/sample
Epoch 82/85
84077/84077 - 9s - loss: 2.8205e-04 - val_loss: 2.7104e-04 - 9s/epoch - 105us/sample
Epoch 83/85
84077/84077 - 9s - loss: 2.8181e-04 - val_loss: 2.7101e-04 - 9s/epoch - 105us/sample
Epoch 84/85
84077/84077 - 9s - loss: 2.8121e-04 - val_loss: 2.6714e-04 - 9s/epoch - 105us/sample
Epoch 85/85
84077/84077 - 9s - loss: 2.8079e-04 - val_loss: 2.7020e-04 - 9s/epoch - 105us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002702024534002331
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 12:09:24.399457: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:13714 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.042957739529453066
cosine 0.04240891620115414
MAE: 0.002655864875091967
RMSE: 0.013118314302528043
r2: 0.8656235500919969
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.0014, 0.2, 188, 0.0002807859963176981, 0.0002702024534002331, 0.042957739529453066, 0.04240891620115414, 0.002655864875091967, 0.013118314302528043, 0.8656235500919969, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.8 90 0.0008 32 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 1697)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 12:09:34.481801: W tensorflow/c/c_api.cc:291] Operation '{name:'training_16/Adam/outputlayer_14/kernel/v/Assign' id:15749 op device:{requested: '', assigned: ''} def:{{{node training_16/Adam/outputlayer_14/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_16/Adam/outputlayer_14/kernel/v, training_16/Adam/outputlayer_14/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 12:09:49.190892: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:15017 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0692 - val_loss: 0.0676 - 17s/epoch - 206us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0670 - val_loss: 0.0674 - 14s/epoch - 164us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 164us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 165us/sample
Epoch 5/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 6/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 7/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 8/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 9/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 10/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 11/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 12/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 13/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 14/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 15/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 163us/sample
Epoch 16/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 17/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 18/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 19/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 20/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 21/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 22/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 23/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 24/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 25/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 26/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 27/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 28/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 29/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 30/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 31/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 32/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 33/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 34/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 35/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 36/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 37/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 38/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 39/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 40/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 41/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 42/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 43/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 44/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 45/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 46/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 47/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 48/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 49/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 50/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 51/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 52/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 53/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 54/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 55/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 56/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 57/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 58/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 59/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 60/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 61/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 62/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 63/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 64/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 65/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 66/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 67/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 68/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 69/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 70/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 71/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 72/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 73/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 74/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 75/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 76/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 77/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 78/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 79/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 80/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 81/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 82/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 83/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 84/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 85/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 86/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 87/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 88/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
Epoch 89/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 165us/sample
Epoch 90/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 164us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573402769227
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 12:30:20.426296: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:14969 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2136540139623402
cosine 1.161435070174787
MAE: 5.019978170642153
RMSE: 5.240876676047687
r2: -20179.007380249976
RMSE zero-vector: 0.04004287452915337
['1.8custom_VAE', 'binary_crossentropy', 32, 90, 0.0008, 0.2, 188, 0.06683691386710988, 0.06734573402769227, 1.2136540139623402, 1.161435070174787, 5.019978170642153, 5.240876676047687, -20179.007380249976, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0008 32 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 1791)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 12:30:30.651478: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/bottleneck_zmean_15/kernel/m/Assign' id:16846 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/bottleneck_zmean_15/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/bottleneck_zmean_15/kernel/m, training_18/Adam/bottleneck_zmean_15/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 12:30:45.357995: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:16342 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0031 - val_loss: 9.7995e-04 - 17s/epoch - 208us/sample
Epoch 2/90
84077/84077 - 13s - loss: 9.3271e-04 - val_loss: 6.9533e-04 - 13s/epoch - 160us/sample
Epoch 3/90
84077/84077 - 13s - loss: 5.8971e-04 - val_loss: 4.7997e-04 - 13s/epoch - 160us/sample
Epoch 4/90
84077/84077 - 13s - loss: 4.7156e-04 - val_loss: 4.7550e-04 - 13s/epoch - 160us/sample
Epoch 5/90
84077/84077 - 14s - loss: 4.2456e-04 - val_loss: 3.6782e-04 - 14s/epoch - 161us/sample
Epoch 6/90
84077/84077 - 14s - loss: 3.7430e-04 - val_loss: 3.2230e-04 - 14s/epoch - 161us/sample
Epoch 7/90
84077/84077 - 13s - loss: 3.3740e-04 - val_loss: 2.9944e-04 - 13s/epoch - 160us/sample
Epoch 8/90
84077/84077 - 14s - loss: 3.1272e-04 - val_loss: 2.8005e-04 - 14s/epoch - 161us/sample
Epoch 9/90
84077/84077 - 14s - loss: 2.9776e-04 - val_loss: 2.7144e-04 - 14s/epoch - 161us/sample
Epoch 10/90
84077/84077 - 13s - loss: 2.8783e-04 - val_loss: 2.6065e-04 - 13s/epoch - 160us/sample
Epoch 11/90
84077/84077 - 13s - loss: 2.7920e-04 - val_loss: 2.5888e-04 - 13s/epoch - 160us/sample
Epoch 12/90
84077/84077 - 14s - loss: 2.7096e-04 - val_loss: 2.5078e-04 - 14s/epoch - 161us/sample
Epoch 13/90
84077/84077 - 13s - loss: 2.6515e-04 - val_loss: 2.4413e-04 - 13s/epoch - 160us/sample
Epoch 14/90
84077/84077 - 13s - loss: 2.5818e-04 - val_loss: 2.3868e-04 - 13s/epoch - 160us/sample
Epoch 15/90
84077/84077 - 14s - loss: 2.5270e-04 - val_loss: 2.3491e-04 - 14s/epoch - 161us/sample
Epoch 16/90
84077/84077 - 14s - loss: 2.4911e-04 - val_loss: 2.3225e-04 - 14s/epoch - 161us/sample
Epoch 17/90
84077/84077 - 13s - loss: 2.4643e-04 - val_loss: 2.3010e-04 - 13s/epoch - 160us/sample
Epoch 18/90
84077/84077 - 13s - loss: 2.4369e-04 - val_loss: 2.3026e-04 - 13s/epoch - 160us/sample
Epoch 19/90
84077/84077 - 14s - loss: 2.4167e-04 - val_loss: 2.3039e-04 - 14s/epoch - 161us/sample
Epoch 20/90
84077/84077 - 14s - loss: 2.3768e-04 - val_loss: 2.2832e-04 - 14s/epoch - 161us/sample
Epoch 21/90
84077/84077 - 13s - loss: 2.3534e-04 - val_loss: 2.2735e-04 - 13s/epoch - 160us/sample
Epoch 22/90
84077/84077 - 13s - loss: 2.3320e-04 - val_loss: 2.1572e-04 - 13s/epoch - 160us/sample
Epoch 23/90
84077/84077 - 14s - loss: 2.3031e-04 - val_loss: 2.1474e-04 - 14s/epoch - 161us/sample
Epoch 24/90
84077/84077 - 14s - loss: 2.2911e-04 - val_loss: 2.1359e-04 - 14s/epoch - 161us/sample
Epoch 25/90
84077/84077 - 13s - loss: 2.2719e-04 - val_loss: 2.1021e-04 - 13s/epoch - 160us/sample
Epoch 26/90
84077/84077 - 14s - loss: 2.2483e-04 - val_loss: 2.1056e-04 - 14s/epoch - 162us/sample
Epoch 27/90
84077/84077 - 13s - loss: 2.2283e-04 - val_loss: 2.0695e-04 - 13s/epoch - 160us/sample
Epoch 28/90
84077/84077 - 13s - loss: 2.2140e-04 - val_loss: 2.0806e-04 - 13s/epoch - 160us/sample
Epoch 29/90
84077/84077 - 13s - loss: 2.2010e-04 - val_loss: 2.0391e-04 - 13s/epoch - 160us/sample
Epoch 30/90
84077/84077 - 14s - loss: 2.1806e-04 - val_loss: 2.0149e-04 - 14s/epoch - 162us/sample
Epoch 31/90
84077/84077 - 13s - loss: 2.1701e-04 - val_loss: 2.0493e-04 - 13s/epoch - 160us/sample
Epoch 32/90
84077/84077 - 13s - loss: 2.1587e-04 - val_loss: 2.0229e-04 - 13s/epoch - 160us/sample
Epoch 33/90
84077/84077 - 14s - loss: 2.1500e-04 - val_loss: 2.0211e-04 - 14s/epoch - 161us/sample
Epoch 34/90
84077/84077 - 14s - loss: 2.1423e-04 - val_loss: 1.9781e-04 - 14s/epoch - 161us/sample
Epoch 35/90
84077/84077 - 13s - loss: 2.1218e-04 - val_loss: 1.9838e-04 - 13s/epoch - 160us/sample
Epoch 36/90
84077/84077 - 13s - loss: 2.1158e-04 - val_loss: 1.9744e-04 - 13s/epoch - 160us/sample
Epoch 37/90
84077/84077 - 13s - loss: 2.1062e-04 - val_loss: 1.9851e-04 - 13s/epoch - 160us/sample
Epoch 38/90
84077/84077 - 14s - loss: 2.0969e-04 - val_loss: 1.9651e-04 - 14s/epoch - 161us/sample
Epoch 39/90
84077/84077 - 13s - loss: 2.0943e-04 - val_loss: 1.9640e-04 - 13s/epoch - 160us/sample
Epoch 40/90
84077/84077 - 13s - loss: 2.0793e-04 - val_loss: 1.9515e-04 - 13s/epoch - 160us/sample
Epoch 41/90
84077/84077 - 14s - loss: 2.0750e-04 - val_loss: 1.9237e-04 - 14s/epoch - 161us/sample
Epoch 42/90
84077/84077 - 14s - loss: 2.0633e-04 - val_loss: 1.9155e-04 - 14s/epoch - 161us/sample
Epoch 43/90
84077/84077 - 13s - loss: 2.0549e-04 - val_loss: 1.9343e-04 - 13s/epoch - 160us/sample
Epoch 44/90
84077/84077 - 14s - loss: 2.0478e-04 - val_loss: 1.9183e-04 - 14s/epoch - 161us/sample
Epoch 45/90
84077/84077 - 14s - loss: 2.0406e-04 - val_loss: 1.9164e-04 - 14s/epoch - 161us/sample
Epoch 46/90
84077/84077 - 13s - loss: 2.0424e-04 - val_loss: 1.8838e-04 - 13s/epoch - 160us/sample
Epoch 47/90
84077/84077 - 13s - loss: 2.0233e-04 - val_loss: 1.9030e-04 - 13s/epoch - 160us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.0187e-04 - val_loss: 1.8890e-04 - 14s/epoch - 162us/sample
Epoch 49/90
84077/84077 - 13s - loss: 2.0105e-04 - val_loss: 1.8785e-04 - 13s/epoch - 160us/sample
Epoch 50/90
84077/84077 - 13s - loss: 2.0048e-04 - val_loss: 1.8718e-04 - 13s/epoch - 160us/sample
Epoch 51/90
84077/84077 - 13s - loss: 2.0027e-04 - val_loss: 1.8723e-04 - 13s/epoch - 160us/sample
Epoch 52/90
84077/84077 - 14s - loss: 1.9977e-04 - val_loss: 1.8702e-04 - 14s/epoch - 161us/sample
Epoch 53/90
84077/84077 - 13s - loss: 1.9903e-04 - val_loss: 1.8482e-04 - 13s/epoch - 160us/sample
Epoch 54/90
84077/84077 - 13s - loss: 1.9718e-04 - val_loss: 1.8633e-04 - 13s/epoch - 160us/sample
Epoch 55/90
84077/84077 - 14s - loss: 1.9698e-04 - val_loss: 1.8524e-04 - 14s/epoch - 161us/sample
Epoch 56/90
84077/84077 - 13s - loss: 1.9620e-04 - val_loss: 1.8245e-04 - 13s/epoch - 160us/sample
Epoch 57/90
84077/84077 - 13s - loss: 1.9423e-04 - val_loss: 1.8117e-04 - 13s/epoch - 160us/sample
Epoch 58/90
84077/84077 - 13s - loss: 1.9426e-04 - val_loss: 1.8034e-04 - 13s/epoch - 160us/sample
Epoch 59/90
84077/84077 - 13s - loss: 1.9449e-04 - val_loss: 1.8147e-04 - 13s/epoch - 160us/sample
Epoch 60/90
84077/84077 - 14s - loss: 1.9250e-04 - val_loss: 1.8343e-04 - 14s/epoch - 162us/sample
Epoch 61/90
84077/84077 - 13s - loss: 1.9302e-04 - val_loss: 1.8243e-04 - 13s/epoch - 160us/sample
Epoch 62/90
84077/84077 - 13s - loss: 1.9113e-04 - val_loss: 1.7933e-04 - 13s/epoch - 160us/sample
Epoch 63/90
84077/84077 - 14s - loss: 1.9192e-04 - val_loss: 1.7903e-04 - 14s/epoch - 162us/sample
Epoch 64/90
84077/84077 - 14s - loss: 1.9046e-04 - val_loss: 1.8068e-04 - 14s/epoch - 161us/sample
Epoch 65/90
84077/84077 - 13s - loss: 1.8969e-04 - val_loss: 1.7891e-04 - 13s/epoch - 160us/sample
Epoch 66/90
84077/84077 - 14s - loss: 1.8945e-04 - val_loss: 1.7732e-04 - 14s/epoch - 162us/sample
Epoch 67/90
84077/84077 - 13s - loss: 1.8845e-04 - val_loss: 1.7515e-04 - 13s/epoch - 160us/sample
Epoch 68/90
84077/84077 - 13s - loss: 1.8851e-04 - val_loss: 1.7397e-04 - 13s/epoch - 160us/sample
Epoch 69/90
84077/84077 - 14s - loss: 1.8721e-04 - val_loss: 1.7672e-04 - 14s/epoch - 161us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.8673e-04 - val_loss: 1.7483e-04 - 14s/epoch - 161us/sample
Epoch 71/90
84077/84077 - 13s - loss: 1.8656e-04 - val_loss: 1.7371e-04 - 13s/epoch - 160us/sample
Epoch 72/90
84077/84077 - 13s - loss: 1.8459e-04 - val_loss: 1.7406e-04 - 13s/epoch - 160us/sample
Epoch 73/90
84077/84077 - 14s - loss: 1.8535e-04 - val_loss: 1.7118e-04 - 14s/epoch - 162us/sample
Epoch 74/90
84077/84077 - 13s - loss: 1.8446e-04 - val_loss: 1.7321e-04 - 13s/epoch - 160us/sample
Epoch 75/90
84077/84077 - 13s - loss: 1.8396e-04 - val_loss: 1.7248e-04 - 13s/epoch - 160us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.8542e-04 - val_loss: 1.7258e-04 - 14s/epoch - 161us/sample
Epoch 77/90
84077/84077 - 13s - loss: 1.8306e-04 - val_loss: 1.7009e-04 - 13s/epoch - 160us/sample
Epoch 78/90
84077/84077 - 13s - loss: 1.8216e-04 - val_loss: 1.7263e-04 - 13s/epoch - 160us/sample
Epoch 79/90
84077/84077 - 14s - loss: 1.8188e-04 - val_loss: 1.6996e-04 - 14s/epoch - 161us/sample
Epoch 80/90
84077/84077 - 13s - loss: 1.8207e-04 - val_loss: 1.7054e-04 - 13s/epoch - 160us/sample
Epoch 81/90
84077/84077 - 13s - loss: 1.8122e-04 - val_loss: 1.7288e-04 - 13s/epoch - 160us/sample
Epoch 82/90
84077/84077 - 14s - loss: 1.8132e-04 - val_loss: 1.6997e-04 - 14s/epoch - 161us/sample
Epoch 83/90
84077/84077 - 14s - loss: 1.7994e-04 - val_loss: 1.6924e-04 - 14s/epoch - 161us/sample
Epoch 84/90
84077/84077 - 13s - loss: 1.8016e-04 - val_loss: 1.7056e-04 - 13s/epoch - 160us/sample
Epoch 85/90
84077/84077 - 13s - loss: 1.7991e-04 - val_loss: 1.7042e-04 - 13s/epoch - 160us/sample
Epoch 86/90
84077/84077 - 14s - loss: 1.7914e-04 - val_loss: 1.7285e-04 - 14s/epoch - 162us/sample
Epoch 87/90
84077/84077 - 13s - loss: 1.8033e-04 - val_loss: 1.6788e-04 - 13s/epoch - 160us/sample
Epoch 88/90
84077/84077 - 14s - loss: 1.7854e-04 - val_loss: 1.7416e-04 - 14s/epoch - 161us/sample
Epoch 89/90
84077/84077 - 14s - loss: 1.7888e-04 - val_loss: 1.6751e-04 - 14s/epoch - 161us/sample
Epoch 90/90
84077/84077 - 13s - loss: 1.7834e-04 - val_loss: 1.6971e-04 - 13s/epoch - 160us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00016971038509220294
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 12:50:47.956084: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:16306 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04908292143036536
cosine 0.04849901172090726
MAE: 0.0027077805845690094
RMSE: 0.01372491443602947
r2: 0.8528770696703538
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 32, 90, 0.0008, 0.2, 188, 0.0001783442238748597, 0.00016971038509220294, 0.04908292143036536, 0.04849901172090726, 0.0027077805845690094, 0.01372491443602947, 0.8528770696703538, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0012 16 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 1791)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 12:50:58.469563: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/bottleneck_zlog_16/bias/v/Assign' id:18262 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/bottleneck_zlog_16/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/bottleneck_zlog_16/bias/v, training_20/Adam/bottleneck_zlog_16/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 12:51:22.422004: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:17627 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0032 - val_loss: 0.0019 - 27s/epoch - 325us/sample
Epoch 2/85
84077/84077 - 23s - loss: 7.1976e-04 - val_loss: 5.4089e-04 - 23s/epoch - 275us/sample
Epoch 3/85
84077/84077 - 23s - loss: 5.3190e-04 - val_loss: 4.4381e-04 - 23s/epoch - 277us/sample
Epoch 4/85
84077/84077 - 23s - loss: 4.6217e-04 - val_loss: 4.0946e-04 - 23s/epoch - 275us/sample
Epoch 5/85
84077/84077 - 23s - loss: 4.3101e-04 - val_loss: 4.8852e-04 - 23s/epoch - 277us/sample
Epoch 6/85
84077/84077 - 23s - loss: 4.1829e-04 - val_loss: 5.9847e-04 - 23s/epoch - 275us/sample
Epoch 7/85
84077/84077 - 23s - loss: 4.1052e-04 - val_loss: 6.3831e-04 - 23s/epoch - 276us/sample
Epoch 8/85
84077/84077 - 23s - loss: 4.0062e-04 - val_loss: 9.1770e-04 - 23s/epoch - 276us/sample
Epoch 9/85
84077/84077 - 23s - loss: 3.9271e-04 - val_loss: 0.0012 - 23s/epoch - 276us/sample
Epoch 10/85
84077/84077 - 23s - loss: 3.8745e-04 - val_loss: 0.0011 - 23s/epoch - 276us/sample
Epoch 11/85
84077/84077 - 23s - loss: 3.8323e-04 - val_loss: 0.0015 - 23s/epoch - 276us/sample
Epoch 12/85
84077/84077 - 23s - loss: 3.7995e-04 - val_loss: 0.0013 - 23s/epoch - 277us/sample
Epoch 13/85
84077/84077 - 23s - loss: 3.7652e-04 - val_loss: 0.0014 - 23s/epoch - 276us/sample
Epoch 14/85
84077/84077 - 23s - loss: 3.7366e-04 - val_loss: 0.0016 - 23s/epoch - 277us/sample
Epoch 15/85
84077/84077 - 23s - loss: 3.7224e-04 - val_loss: 0.0013 - 23s/epoch - 275us/sample
Epoch 16/85
84077/84077 - 23s - loss: 3.6907e-04 - val_loss: 0.0012 - 23s/epoch - 277us/sample
Epoch 17/85
84077/84077 - 23s - loss: 3.6737e-04 - val_loss: 0.0014 - 23s/epoch - 277us/sample
Epoch 18/85
84077/84077 - 23s - loss: 3.6579e-04 - val_loss: 0.0014 - 23s/epoch - 276us/sample
Epoch 19/85
84077/84077 - 23s - loss: 3.6445e-04 - val_loss: 0.0012 - 23s/epoch - 278us/sample
Epoch 20/85
84077/84077 - 23s - loss: 3.6217e-04 - val_loss: 0.0012 - 23s/epoch - 274us/sample
Epoch 21/85
84077/84077 - 23s - loss: 3.5969e-04 - val_loss: 0.0013 - 23s/epoch - 277us/sample
Epoch 22/85
84077/84077 - 23s - loss: 3.5795e-04 - val_loss: 0.0013 - 23s/epoch - 275us/sample
Epoch 23/85
84077/84077 - 23s - loss: 3.5603e-04 - val_loss: 0.0012 - 23s/epoch - 277us/sample
Epoch 24/85
84077/84077 - 23s - loss: 3.5396e-04 - val_loss: 0.0013 - 23s/epoch - 276us/sample
Epoch 25/85
84077/84077 - 23s - loss: 3.5267e-04 - val_loss: 9.3825e-04 - 23s/epoch - 275us/sample
Epoch 26/85
84077/84077 - 23s - loss: 3.5138e-04 - val_loss: 0.0012 - 23s/epoch - 277us/sample
Epoch 27/85
84077/84077 - 23s - loss: 3.5007e-04 - val_loss: 0.0010 - 23s/epoch - 276us/sample
Epoch 28/85
84077/84077 - 23s - loss: 3.4873e-04 - val_loss: 8.7916e-04 - 23s/epoch - 277us/sample
Epoch 29/85
84077/84077 - 23s - loss: 3.4797e-04 - val_loss: 8.5780e-04 - 23s/epoch - 275us/sample
Epoch 30/85
84077/84077 - 23s - loss: 3.4621e-04 - val_loss: 8.9722e-04 - 23s/epoch - 275us/sample
Epoch 31/85
84077/84077 - 23s - loss: 3.4567e-04 - val_loss: 8.9485e-04 - 23s/epoch - 277us/sample
Epoch 32/85
84077/84077 - 23s - loss: 3.4548e-04 - val_loss: 9.2144e-04 - 23s/epoch - 276us/sample
Epoch 33/85
84077/84077 - 23s - loss: 3.4489e-04 - val_loss: 0.0010 - 23s/epoch - 276us/sample
Epoch 34/85
84077/84077 - 23s - loss: 3.4389e-04 - val_loss: 8.4097e-04 - 23s/epoch - 275us/sample
Epoch 35/85
84077/84077 - 23s - loss: 3.4347e-04 - val_loss: 8.4558e-04 - 23s/epoch - 276us/sample
Epoch 36/85
84077/84077 - 23s - loss: 3.4361e-04 - val_loss: 0.0010 - 23s/epoch - 276us/sample
Epoch 37/85
84077/84077 - 23s - loss: 3.4291e-04 - val_loss: 7.9975e-04 - 23s/epoch - 277us/sample
Epoch 38/85
84077/84077 - 23s - loss: 3.4229e-04 - val_loss: 8.1765e-04 - 23s/epoch - 276us/sample
Epoch 39/85
84077/84077 - 23s - loss: 3.4133e-04 - val_loss: 7.2888e-04 - 23s/epoch - 277us/sample
Epoch 40/85
84077/84077 - 23s - loss: 3.3932e-04 - val_loss: 7.3549e-04 - 23s/epoch - 275us/sample
Epoch 41/85
84077/84077 - 23s - loss: 3.3807e-04 - val_loss: 8.2516e-04 - 23s/epoch - 277us/sample
Epoch 42/85
84077/84077 - 23s - loss: 3.3744e-04 - val_loss: 7.9141e-04 - 23s/epoch - 276us/sample
Epoch 43/85
84077/84077 - 23s - loss: 3.3643e-04 - val_loss: 7.6123e-04 - 23s/epoch - 275us/sample
Epoch 44/85
84077/84077 - 23s - loss: 3.3594e-04 - val_loss: 7.4303e-04 - 23s/epoch - 277us/sample
Epoch 45/85
84077/84077 - 23s - loss: 3.3434e-04 - val_loss: 7.3363e-04 - 23s/epoch - 275us/sample
Epoch 46/85
84077/84077 - 23s - loss: 3.3289e-04 - val_loss: 7.5542e-04 - 23s/epoch - 276us/sample
Epoch 47/85
84077/84077 - 23s - loss: 3.3172e-04 - val_loss: 6.7708e-04 - 23s/epoch - 276us/sample
Epoch 48/85
84077/84077 - 23s - loss: 3.3079e-04 - val_loss: 6.5391e-04 - 23s/epoch - 277us/sample
Epoch 49/85
84077/84077 - 23s - loss: 3.2983e-04 - val_loss: 6.3530e-04 - 23s/epoch - 276us/sample
Epoch 50/85
84077/84077 - 23s - loss: 3.2919e-04 - val_loss: 6.8705e-04 - 23s/epoch - 275us/sample
Epoch 51/85
84077/84077 - 23s - loss: 3.2826e-04 - val_loss: 6.8916e-04 - 23s/epoch - 277us/sample
Epoch 52/85
84077/84077 - 23s - loss: 3.2801e-04 - val_loss: 6.5592e-04 - 23s/epoch - 276us/sample
Epoch 53/85
84077/84077 - 23s - loss: 3.2749e-04 - val_loss: 5.7503e-04 - 23s/epoch - 277us/sample
Epoch 54/85
84077/84077 - 23s - loss: 3.2765e-04 - val_loss: 5.9692e-04 - 23s/epoch - 275us/sample
Epoch 55/85
84077/84077 - 23s - loss: 3.2720e-04 - val_loss: 6.6582e-04 - 23s/epoch - 277us/sample
Epoch 56/85
84077/84077 - 23s - loss: 3.2644e-04 - val_loss: 6.7402e-04 - 23s/epoch - 275us/sample
Epoch 57/85
84077/84077 - 23s - loss: 3.2643e-04 - val_loss: 6.6120e-04 - 23s/epoch - 276us/sample
Epoch 58/85
84077/84077 - 23s - loss: 3.2672e-04 - val_loss: 5.6546e-04 - 23s/epoch - 276us/sample
Epoch 59/85
84077/84077 - 23s - loss: 3.2619e-04 - val_loss: 5.4630e-04 - 23s/epoch - 275us/sample
Epoch 60/85
84077/84077 - 23s - loss: 3.2556e-04 - val_loss: 5.6435e-04 - 23s/epoch - 277us/sample
Epoch 61/85
84077/84077 - 23s - loss: 3.2566e-04 - val_loss: 4.9814e-04 - 23s/epoch - 275us/sample
Epoch 62/85
84077/84077 - 23s - loss: 3.2538e-04 - val_loss: 5.8332e-04 - 23s/epoch - 276us/sample
Epoch 63/85
84077/84077 - 23s - loss: 3.2518e-04 - val_loss: 4.9231e-04 - 23s/epoch - 276us/sample
Epoch 64/85
84077/84077 - 23s - loss: 3.2522e-04 - val_loss: 5.0946e-04 - 23s/epoch - 275us/sample
Epoch 65/85
84077/84077 - 23s - loss: 3.2488e-04 - val_loss: 5.3793e-04 - 23s/epoch - 278us/sample
Epoch 66/85
84077/84077 - 23s - loss: 3.2461e-04 - val_loss: 5.0030e-04 - 23s/epoch - 276us/sample
Epoch 67/85
84077/84077 - 23s - loss: 3.2429e-04 - val_loss: 5.8335e-04 - 23s/epoch - 276us/sample
Epoch 68/85
84077/84077 - 23s - loss: 3.2420e-04 - val_loss: 4.9908e-04 - 23s/epoch - 276us/sample
Epoch 69/85
84077/84077 - 23s - loss: 3.2389e-04 - val_loss: 4.9113e-04 - 23s/epoch - 275us/sample
Epoch 70/85
84077/84077 - 23s - loss: 3.2321e-04 - val_loss: 5.0922e-04 - 23s/epoch - 277us/sample
Epoch 71/85
84077/84077 - 23s - loss: 3.2377e-04 - val_loss: 5.0427e-04 - 23s/epoch - 276us/sample
Epoch 72/85
84077/84077 - 23s - loss: 3.2351e-04 - val_loss: 5.2968e-04 - 23s/epoch - 277us/sample
Epoch 73/85
84077/84077 - 23s - loss: 3.2321e-04 - val_loss: 4.9401e-04 - 23s/epoch - 275us/sample
Epoch 74/85
84077/84077 - 23s - loss: 3.2296e-04 - val_loss: 5.6272e-04 - 23s/epoch - 277us/sample
Epoch 75/85
84077/84077 - 23s - loss: 3.2277e-04 - val_loss: 4.9411e-04 - 23s/epoch - 277us/sample
Epoch 76/85
84077/84077 - 23s - loss: 3.2235e-04 - val_loss: 4.8088e-04 - 23s/epoch - 276us/sample
Epoch 77/85
84077/84077 - 23s - loss: 3.2273e-04 - val_loss: 4.8089e-04 - 23s/epoch - 276us/sample
Epoch 78/85
84077/84077 - 23s - loss: 3.2264e-04 - val_loss: 5.0603e-04 - 23s/epoch - 275us/sample
Epoch 79/85
84077/84077 - 23s - loss: 3.2225e-04 - val_loss: 5.0849e-04 - 23s/epoch - 277us/sample
Epoch 80/85
84077/84077 - 23s - loss: 3.2164e-04 - val_loss: 5.5392e-04 - 23s/epoch - 276us/sample
Epoch 81/85
84077/84077 - 23s - loss: 3.2183e-04 - val_loss: 4.7222e-04 - 23s/epoch - 277us/sample
Epoch 82/85
84077/84077 - 23s - loss: 3.2182e-04 - val_loss: 5.8652e-04 - 23s/epoch - 276us/sample
Epoch 83/85
84077/84077 - 23s - loss: 3.2204e-04 - val_loss: 5.1615e-04 - 23s/epoch - 276us/sample
Epoch 84/85
84077/84077 - 23s - loss: 3.2127e-04 - val_loss: 5.2790e-04 - 23s/epoch - 276us/sample
Epoch 85/85
84077/84077 - 23s - loss: 3.2091e-04 - val_loss: 4.7276e-04 - 23s/epoch - 277us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00047275846221611254
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 13:23:54.479494: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:17591 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.15013971401805126
cosine 0.14811295398339344
MAE: 0.004766601565657334
RMSE: 0.03318305473150523
r2: 0.1398196804894014
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 16, 85, 0.0012, 0.2, 188, 0.0003209135054672103, 0.00047275846221611254, 0.15013971401805126, 0.14811295398339344, 0.004766601565657334, 0.03318305473150523, 0.1398196804894014, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9000000000000001 95 0.0012 32 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 1791)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/95
2023-02-12 13:24:05.392774: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/dense_enc0_17/bias/m/Assign' id:19395 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/dense_enc0_17/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/dense_enc0_17/bias/m, training_22/Adam/dense_enc0_17/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 13:24:20.459280: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:18912 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0031 - val_loss: 0.0013 - 18s/epoch - 216us/sample
Epoch 2/95
84077/84077 - 14s - loss: 0.0012 - val_loss: 6.3969e-04 - 14s/epoch - 166us/sample
Epoch 3/95
84077/84077 - 14s - loss: 6.0731e-04 - val_loss: 5.6878e-04 - 14s/epoch - 166us/sample
Epoch 4/95
84077/84077 - 14s - loss: 5.3500e-04 - val_loss: 4.3406e-04 - 14s/epoch - 166us/sample
Epoch 5/95
84077/84077 - 14s - loss: 4.4062e-04 - val_loss: 3.7824e-04 - 14s/epoch - 166us/sample
Epoch 6/95
84077/84077 - 14s - loss: 3.8568e-04 - val_loss: 3.3806e-04 - 14s/epoch - 166us/sample
Epoch 7/95
84077/84077 - 14s - loss: 3.5029e-04 - val_loss: 3.1857e-04 - 14s/epoch - 167us/sample
Epoch 8/95
84077/84077 - 14s - loss: 3.2824e-04 - val_loss: 2.9832e-04 - 14s/epoch - 166us/sample
Epoch 9/95
84077/84077 - 14s - loss: 3.1074e-04 - val_loss: 2.8339e-04 - 14s/epoch - 166us/sample
Epoch 10/95
84077/84077 - 14s - loss: 2.9540e-04 - val_loss: 2.6754e-04 - 14s/epoch - 167us/sample
Epoch 11/95
84077/84077 - 14s - loss: 2.8352e-04 - val_loss: 2.6057e-04 - 14s/epoch - 166us/sample
Epoch 12/95
84077/84077 - 14s - loss: 2.7346e-04 - val_loss: 2.5304e-04 - 14s/epoch - 165us/sample
Epoch 13/95
84077/84077 - 14s - loss: 2.6595e-04 - val_loss: 2.4783e-04 - 14s/epoch - 167us/sample
Epoch 14/95
84077/84077 - 14s - loss: 2.6021e-04 - val_loss: 2.3906e-04 - 14s/epoch - 167us/sample
Epoch 15/95
84077/84077 - 14s - loss: 2.5580e-04 - val_loss: 2.3692e-04 - 14s/epoch - 165us/sample
Epoch 16/95
84077/84077 - 14s - loss: 2.5149e-04 - val_loss: 2.3354e-04 - 14s/epoch - 166us/sample
Epoch 17/95
84077/84077 - 14s - loss: 2.4877e-04 - val_loss: 2.3612e-04 - 14s/epoch - 167us/sample
Epoch 18/95
84077/84077 - 14s - loss: 2.4429e-04 - val_loss: 2.3274e-04 - 14s/epoch - 165us/sample
Epoch 19/95
84077/84077 - 14s - loss: 2.4207e-04 - val_loss: 2.2961e-04 - 14s/epoch - 165us/sample
Epoch 20/95
84077/84077 - 14s - loss: 2.3944e-04 - val_loss: 2.2198e-04 - 14s/epoch - 167us/sample
Epoch 21/95
84077/84077 - 14s - loss: 2.3795e-04 - val_loss: 2.2666e-04 - 14s/epoch - 166us/sample
Epoch 22/95
84077/84077 - 14s - loss: 2.3583e-04 - val_loss: 2.2455e-04 - 14s/epoch - 166us/sample
Epoch 23/95
84077/84077 - 14s - loss: 2.3420e-04 - val_loss: 2.2296e-04 - 14s/epoch - 167us/sample
Epoch 24/95
84077/84077 - 14s - loss: 2.3319e-04 - val_loss: 2.2010e-04 - 14s/epoch - 166us/sample
Epoch 25/95
84077/84077 - 14s - loss: 2.3143e-04 - val_loss: 2.1929e-04 - 14s/epoch - 165us/sample
Epoch 26/95
84077/84077 - 14s - loss: 2.2982e-04 - val_loss: 2.1515e-04 - 14s/epoch - 166us/sample
Epoch 27/95
84077/84077 - 14s - loss: 2.2872e-04 - val_loss: 2.1646e-04 - 14s/epoch - 167us/sample
Epoch 28/95
84077/84077 - 14s - loss: 2.2743e-04 - val_loss: 2.1695e-04 - 14s/epoch - 166us/sample
Epoch 29/95
84077/84077 - 14s - loss: 2.2661e-04 - val_loss: 2.1913e-04 - 14s/epoch - 165us/sample
Epoch 30/95
84077/84077 - 14s - loss: 2.2563e-04 - val_loss: 2.1948e-04 - 14s/epoch - 165us/sample
Epoch 31/95
84077/84077 - 14s - loss: 2.2399e-04 - val_loss: 2.1186e-04 - 14s/epoch - 167us/sample
Epoch 32/95
84077/84077 - 14s - loss: 2.2353e-04 - val_loss: 2.0923e-04 - 14s/epoch - 166us/sample
Epoch 33/95
84077/84077 - 14s - loss: 2.2221e-04 - val_loss: 2.0923e-04 - 14s/epoch - 165us/sample
Epoch 34/95
84077/84077 - 14s - loss: 2.2124e-04 - val_loss: 2.1098e-04 - 14s/epoch - 167us/sample
Epoch 35/95
84077/84077 - 14s - loss: 2.2133e-04 - val_loss: 2.0741e-04 - 14s/epoch - 166us/sample
Epoch 36/95
84077/84077 - 14s - loss: 2.2005e-04 - val_loss: 2.0999e-04 - 14s/epoch - 165us/sample
Epoch 37/95
84077/84077 - 14s - loss: 2.1902e-04 - val_loss: 2.0803e-04 - 14s/epoch - 165us/sample
Epoch 38/95
84077/84077 - 14s - loss: 2.1894e-04 - val_loss: 2.0863e-04 - 14s/epoch - 167us/sample
Epoch 39/95
84077/84077 - 14s - loss: 2.1829e-04 - val_loss: 2.0644e-04 - 14s/epoch - 166us/sample
Epoch 40/95
84077/84077 - 14s - loss: 2.1768e-04 - val_loss: 2.0645e-04 - 14s/epoch - 165us/sample
Epoch 41/95
84077/84077 - 14s - loss: 2.1684e-04 - val_loss: 2.0551e-04 - 14s/epoch - 167us/sample
Epoch 42/95
84077/84077 - 14s - loss: 2.1618e-04 - val_loss: 2.0547e-04 - 14s/epoch - 166us/sample
Epoch 43/95
84077/84077 - 14s - loss: 2.1618e-04 - val_loss: 2.0452e-04 - 14s/epoch - 165us/sample
Epoch 44/95
84077/84077 - 14s - loss: 2.1518e-04 - val_loss: 2.0630e-04 - 14s/epoch - 167us/sample
Epoch 45/95
84077/84077 - 14s - loss: 2.1475e-04 - val_loss: 2.0632e-04 - 14s/epoch - 166us/sample
Epoch 46/95
84077/84077 - 14s - loss: 2.1465e-04 - val_loss: 2.0517e-04 - 14s/epoch - 165us/sample
Epoch 47/95
84077/84077 - 14s - loss: 2.1385e-04 - val_loss: 2.0229e-04 - 14s/epoch - 166us/sample
Epoch 48/95
84077/84077 - 14s - loss: 2.1266e-04 - val_loss: 2.0399e-04 - 14s/epoch - 167us/sample
Epoch 49/95
84077/84077 - 14s - loss: 2.1241e-04 - val_loss: 2.0377e-04 - 14s/epoch - 166us/sample
Epoch 50/95
84077/84077 - 14s - loss: 2.1251e-04 - val_loss: 2.0189e-04 - 14s/epoch - 165us/sample
Epoch 51/95
84077/84077 - 14s - loss: 2.1124e-04 - val_loss: 2.0142e-04 - 14s/epoch - 168us/sample
Epoch 52/95
84077/84077 - 14s - loss: 2.1022e-04 - val_loss: 2.0234e-04 - 14s/epoch - 167us/sample
Epoch 53/95
84077/84077 - 14s - loss: 2.1015e-04 - val_loss: 2.0051e-04 - 14s/epoch - 166us/sample
Epoch 54/95
84077/84077 - 14s - loss: 2.0971e-04 - val_loss: 2.0225e-04 - 14s/epoch - 166us/sample
Epoch 55/95
84077/84077 - 14s - loss: 2.0917e-04 - val_loss: 1.9770e-04 - 14s/epoch - 168us/sample
Epoch 56/95
84077/84077 - 14s - loss: 2.0921e-04 - val_loss: 1.9795e-04 - 14s/epoch - 166us/sample
Epoch 57/95
84077/84077 - 14s - loss: 2.0799e-04 - val_loss: 1.9899e-04 - 14s/epoch - 165us/sample
Epoch 58/95
84077/84077 - 14s - loss: 2.0724e-04 - val_loss: 1.9792e-04 - 14s/epoch - 167us/sample
Epoch 59/95
84077/84077 - 14s - loss: 2.0648e-04 - val_loss: 1.9530e-04 - 14s/epoch - 167us/sample
Epoch 60/95
84077/84077 - 14s - loss: 2.0604e-04 - val_loss: 1.9799e-04 - 14s/epoch - 167us/sample
Epoch 61/95
84077/84077 - 14s - loss: 2.0564e-04 - val_loss: 1.9699e-04 - 14s/epoch - 167us/sample
Epoch 62/95
84077/84077 - 14s - loss: 2.0497e-04 - val_loss: 1.9446e-04 - 14s/epoch - 166us/sample
Epoch 63/95
84077/84077 - 14s - loss: 2.0495e-04 - val_loss: 1.9537e-04 - 14s/epoch - 165us/sample
Epoch 64/95
84077/84077 - 14s - loss: 2.0441e-04 - val_loss: 1.9446e-04 - 14s/epoch - 166us/sample
Epoch 65/95
84077/84077 - 14s - loss: 2.0379e-04 - val_loss: 1.9278e-04 - 14s/epoch - 167us/sample
Epoch 66/95
84077/84077 - 14s - loss: 2.0349e-04 - val_loss: 1.9616e-04 - 14s/epoch - 166us/sample
Epoch 67/95
84077/84077 - 14s - loss: 2.0369e-04 - val_loss: 1.9355e-04 - 14s/epoch - 165us/sample
Epoch 68/95
84077/84077 - 14s - loss: 2.0256e-04 - val_loss: 1.9550e-04 - 14s/epoch - 167us/sample
Epoch 69/95
84077/84077 - 14s - loss: 2.0329e-04 - val_loss: 1.9347e-04 - 14s/epoch - 167us/sample
Epoch 70/95
84077/84077 - 14s - loss: 2.0154e-04 - val_loss: 1.9358e-04 - 14s/epoch - 165us/sample
Epoch 71/95
84077/84077 - 14s - loss: 2.0221e-04 - val_loss: 1.9356e-04 - 14s/epoch - 165us/sample
Epoch 72/95
84077/84077 - 14s - loss: 2.0077e-04 - val_loss: 1.9387e-04 - 14s/epoch - 168us/sample
Epoch 73/95
84077/84077 - 14s - loss: 2.0056e-04 - val_loss: 1.9238e-04 - 14s/epoch - 166us/sample
Epoch 74/95
84077/84077 - 14s - loss: 2.0016e-04 - val_loss: 1.9103e-04 - 14s/epoch - 166us/sample
Epoch 75/95
84077/84077 - 14s - loss: 1.9968e-04 - val_loss: 1.9009e-04 - 14s/epoch - 167us/sample
Epoch 76/95
84077/84077 - 14s - loss: 1.9976e-04 - val_loss: 1.9013e-04 - 14s/epoch - 166us/sample
Epoch 77/95
84077/84077 - 14s - loss: 2.0040e-04 - val_loss: 1.9114e-04 - 14s/epoch - 166us/sample
Epoch 78/95
84077/84077 - 14s - loss: 1.9943e-04 - val_loss: 1.9041e-04 - 14s/epoch - 165us/sample
Epoch 79/95
84077/84077 - 14s - loss: 1.9846e-04 - val_loss: 1.9089e-04 - 14s/epoch - 167us/sample
Epoch 80/95
84077/84077 - 14s - loss: 1.9801e-04 - val_loss: 1.9332e-04 - 14s/epoch - 166us/sample
Epoch 81/95
84077/84077 - 14s - loss: 1.9790e-04 - val_loss: 1.9113e-04 - 14s/epoch - 167us/sample
Epoch 82/95
84077/84077 - 14s - loss: 1.9769e-04 - val_loss: 1.8765e-04 - 14s/epoch - 167us/sample
Epoch 83/95
84077/84077 - 14s - loss: 1.9715e-04 - val_loss: 1.8656e-04 - 14s/epoch - 166us/sample
Epoch 84/95
84077/84077 - 14s - loss: 1.9731e-04 - val_loss: 1.8854e-04 - 14s/epoch - 165us/sample
Epoch 85/95
84077/84077 - 14s - loss: 1.9645e-04 - val_loss: 1.8679e-04 - 14s/epoch - 169us/sample
Epoch 86/95
84077/84077 - 14s - loss: 1.9680e-04 - val_loss: 1.8791e-04 - 14s/epoch - 166us/sample
Epoch 87/95
84077/84077 - 14s - loss: 1.9647e-04 - val_loss: 1.8709e-04 - 14s/epoch - 165us/sample
Epoch 88/95
84077/84077 - 14s - loss: 1.9639e-04 - val_loss: 1.8624e-04 - 14s/epoch - 166us/sample
Epoch 89/95
84077/84077 - 14s - loss: 1.9519e-04 - val_loss: 1.8567e-04 - 14s/epoch - 167us/sample
Epoch 90/95
84077/84077 - 14s - loss: 1.9511e-04 - val_loss: 1.8598e-04 - 14s/epoch - 166us/sample
Epoch 91/95
84077/84077 - 14s - loss: 1.9446e-04 - val_loss: 1.8625e-04 - 14s/epoch - 166us/sample
Epoch 92/95
84077/84077 - 14s - loss: 1.9438e-04 - val_loss: 1.8519e-04 - 14s/epoch - 167us/sample
Epoch 93/95
84077/84077 - 14s - loss: 1.9424e-04 - val_loss: 1.8356e-04 - 14s/epoch - 166us/sample
Epoch 94/95
84077/84077 - 14s - loss: 1.9394e-04 - val_loss: 1.8646e-04 - 14s/epoch - 168us/sample
Epoch 95/95
84077/84077 - 14s - loss: 1.9352e-04 - val_loss: 1.8531e-04 - 14s/epoch - 166us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018531423450835022
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 13:46:15.383766: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:18876 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05896536716696906
cosine 0.058244382667325914
MAE: 0.00300093571558365
RMSE: 0.01529277741443247
r2: 0.8173604962340346
RMSE zero-vector: 0.04004287452915337
['1.9000000000000001custom_VAE', 'logcosh', 32, 95, 0.0012, 0.2, 188, 0.00019351895160034117, 0.00018531423450835022, 0.05896536716696906, 0.058244382667325914, 0.00300093571558365, 0.01529277741443247, 0.8173604962340346, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 1980)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.1_cr0.2_bs64_ep90_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 13:46:27.030032: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_165/beta/Assign' id:20426 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_165/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_165/beta, batch_normalization_165/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 13:46:30.384682: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_19/kernel/v/Assign' id:20957 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_19/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_19/kernel/v, bottleneck_zlog_19/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 13:46:33.643863: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:20615 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03155072521026027
cosine 0.031172274978100677
MAE: 0.0026472517634604426
RMSE: 0.009973186671355546
r2: 0.922766556379676
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.1custom_VAE', 'logcosh', 64, 90, 0.001, 0.2, 188, '--', '--', 0.03155072521026027, 0.031172274978100677, 0.0026472517634604426, 0.009973186671355546, 0.922766556379676, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 80 0.0012 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 1886)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 13:46:44.914165: W tensorflow/c/c_api.cc:291] Operation '{name:'training_24/Adam/bottleneck_zlog_20/bias/m/Assign' id:22154 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/bottleneck_zlog_20/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_24/Adam/bottleneck_zlog_20/bias/m, training_24/Adam/bottleneck_zlog_20/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 13:46:55.702898: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:21633 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0585 - val_loss: 0.0013 - 14s/epoch - 167us/sample
Epoch 2/80
84077/84077 - 9s - loss: 9.5433e-04 - val_loss: 6.5587e-04 - 9s/epoch - 111us/sample
Epoch 3/80
84077/84077 - 9s - loss: 0.0017 - val_loss: 9.7416e-04 - 9s/epoch - 112us/sample
Epoch 4/80
84077/84077 - 9s - loss: 7.6575e-04 - val_loss: 6.4054e-04 - 9s/epoch - 111us/sample
Epoch 5/80
84077/84077 - 9s - loss: 6.0499e-04 - val_loss: 5.6672e-04 - 9s/epoch - 111us/sample
Epoch 6/80
84077/84077 - 9s - loss: 5.3978e-04 - val_loss: 4.9216e-04 - 9s/epoch - 111us/sample
Epoch 7/80
84077/84077 - 9s - loss: 4.7573e-04 - val_loss: 4.1783e-04 - 9s/epoch - 111us/sample
Epoch 8/80
84077/84077 - 9s - loss: 3.8544e-04 - val_loss: 3.5541e-04 - 9s/epoch - 111us/sample
Epoch 9/80
84077/84077 - 9s - loss: 5.0663e-04 - val_loss: 3.0155e-04 - 9s/epoch - 110us/sample
Epoch 10/80
84077/84077 - 9s - loss: 3.1245e-04 - val_loss: 2.7715e-04 - 9s/epoch - 111us/sample
Epoch 11/80
84077/84077 - 9s - loss: 2.8837e-04 - val_loss: 2.5706e-04 - 9s/epoch - 111us/sample
Epoch 12/80
84077/84077 - 9s - loss: 2.6478e-04 - val_loss: 2.4062e-04 - 9s/epoch - 112us/sample
Epoch 13/80
84077/84077 - 9s - loss: 2.4734e-04 - val_loss: 2.2623e-04 - 9s/epoch - 111us/sample
Epoch 14/80
84077/84077 - 9s - loss: 2.3250e-04 - val_loss: 2.0704e-04 - 9s/epoch - 111us/sample
Epoch 15/80
84077/84077 - 9s - loss: 2.1813e-04 - val_loss: 1.9715e-04 - 9s/epoch - 110us/sample
Epoch 16/80
84077/84077 - 9s - loss: 2.0726e-04 - val_loss: 1.8942e-04 - 9s/epoch - 111us/sample
Epoch 17/80
84077/84077 - 9s - loss: 1.9900e-04 - val_loss: 1.7957e-04 - 9s/epoch - 111us/sample
Epoch 18/80
84077/84077 - 9s - loss: 1.9170e-04 - val_loss: 1.7165e-04 - 9s/epoch - 112us/sample
Epoch 19/80
84077/84077 - 9s - loss: 1.8531e-04 - val_loss: 1.6820e-04 - 9s/epoch - 111us/sample
Epoch 20/80
84077/84077 - 9s - loss: 1.8046e-04 - val_loss: 1.6371e-04 - 9s/epoch - 112us/sample
Epoch 21/80
84077/84077 - 9s - loss: 1.7530e-04 - val_loss: 1.6037e-04 - 9s/epoch - 110us/sample
Epoch 22/80
84077/84077 - 9s - loss: 1.7209e-04 - val_loss: 1.5754e-04 - 9s/epoch - 111us/sample
Epoch 23/80
84077/84077 - 9s - loss: 1.6800e-04 - val_loss: 1.5309e-04 - 9s/epoch - 112us/sample
Epoch 24/80
84077/84077 - 9s - loss: 1.6594e-04 - val_loss: 1.5247e-04 - 9s/epoch - 111us/sample
Epoch 25/80
84077/84077 - 9s - loss: 1.6294e-04 - val_loss: 1.5070e-04 - 9s/epoch - 111us/sample
Epoch 26/80
84077/84077 - 9s - loss: 1.6164e-04 - val_loss: 1.4867e-04 - 9s/epoch - 111us/sample
Epoch 27/80
84077/84077 - 9s - loss: 1.5919e-04 - val_loss: 1.4748e-04 - 9s/epoch - 111us/sample
Epoch 28/80
84077/84077 - 9s - loss: 1.5802e-04 - val_loss: 1.4851e-04 - 9s/epoch - 111us/sample
Epoch 29/80
84077/84077 - 9s - loss: 1.5626e-04 - val_loss: 1.4728e-04 - 9s/epoch - 112us/sample
Epoch 30/80
84077/84077 - 9s - loss: 1.5508e-04 - val_loss: 1.4434e-04 - 9s/epoch - 111us/sample
Epoch 31/80
84077/84077 - 9s - loss: 1.5358e-04 - val_loss: 1.4435e-04 - 9s/epoch - 111us/sample
Epoch 32/80
84077/84077 - 9s - loss: 1.5252e-04 - val_loss: 1.4342e-04 - 9s/epoch - 110us/sample
Epoch 33/80
84077/84077 - 9s - loss: 1.5165e-04 - val_loss: 1.4162e-04 - 9s/epoch - 112us/sample
Epoch 34/80
84077/84077 - 9s - loss: 1.5057e-04 - val_loss: 1.4069e-04 - 9s/epoch - 112us/sample
Epoch 35/80
84077/84077 - 9s - loss: 1.4970e-04 - val_loss: 1.3878e-04 - 9s/epoch - 111us/sample
Epoch 36/80
84077/84077 - 9s - loss: 1.4913e-04 - val_loss: 1.3990e-04 - 9s/epoch - 111us/sample
Epoch 37/80
84077/84077 - 9s - loss: 1.4808e-04 - val_loss: 1.3974e-04 - 9s/epoch - 110us/sample
Epoch 38/80
84077/84077 - 9s - loss: 1.4757e-04 - val_loss: 1.3915e-04 - 9s/epoch - 111us/sample
Epoch 39/80
84077/84077 - 9s - loss: 1.4707e-04 - val_loss: 1.3814e-04 - 9s/epoch - 111us/sample
Epoch 40/80
84077/84077 - 9s - loss: 1.4608e-04 - val_loss: 1.3825e-04 - 9s/epoch - 111us/sample
Epoch 41/80
84077/84077 - 9s - loss: 1.4556e-04 - val_loss: 1.3751e-04 - 9s/epoch - 111us/sample
Epoch 42/80
84077/84077 - 9s - loss: 1.4500e-04 - val_loss: 1.3720e-04 - 9s/epoch - 111us/sample
Epoch 43/80
84077/84077 - 9s - loss: 1.4430e-04 - val_loss: 1.3765e-04 - 9s/epoch - 110us/sample
Epoch 44/80
84077/84077 - 9s - loss: 1.4384e-04 - val_loss: 1.3680e-04 - 9s/epoch - 111us/sample
Epoch 45/80
84077/84077 - 9s - loss: 1.4366e-04 - val_loss: 1.3704e-04 - 9s/epoch - 110us/sample
Epoch 46/80
84077/84077 - 10s - loss: 1.4251e-04 - val_loss: 1.3594e-04 - 10s/epoch - 113us/sample
Epoch 47/80
84077/84077 - 9s - loss: 1.4258e-04 - val_loss: 1.3653e-04 - 9s/epoch - 112us/sample
Epoch 48/80
84077/84077 - 9s - loss: 1.4213e-04 - val_loss: 1.3602e-04 - 9s/epoch - 111us/sample
Epoch 49/80
84077/84077 - 9s - loss: 1.4189e-04 - val_loss: 1.3439e-04 - 9s/epoch - 110us/sample
Epoch 50/80
84077/84077 - 9s - loss: 1.4136e-04 - val_loss: 1.3368e-04 - 9s/epoch - 110us/sample
Epoch 51/80
84077/84077 - 9s - loss: 1.4060e-04 - val_loss: 1.3310e-04 - 9s/epoch - 110us/sample
Epoch 52/80
84077/84077 - 9s - loss: 1.4038e-04 - val_loss: 1.3404e-04 - 9s/epoch - 111us/sample
Epoch 53/80
84077/84077 - 9s - loss: 1.4024e-04 - val_loss: 1.3304e-04 - 9s/epoch - 112us/sample
Epoch 54/80
84077/84077 - 9s - loss: 1.3986e-04 - val_loss: 1.3330e-04 - 9s/epoch - 111us/sample
Epoch 55/80
84077/84077 - 9s - loss: 1.3920e-04 - val_loss: 1.3329e-04 - 9s/epoch - 111us/sample
Epoch 56/80
84077/84077 - 9s - loss: 1.3902e-04 - val_loss: 1.3119e-04 - 9s/epoch - 111us/sample
Epoch 57/80
84077/84077 - 9s - loss: 1.3845e-04 - val_loss: 1.3179e-04 - 9s/epoch - 110us/sample
Epoch 58/80
84077/84077 - 9s - loss: 1.3807e-04 - val_loss: 1.3273e-04 - 9s/epoch - 111us/sample
Epoch 59/80
84077/84077 - 9s - loss: 1.3800e-04 - val_loss: 1.3529e-04 - 9s/epoch - 112us/sample
Epoch 60/80
84077/84077 - 9s - loss: 1.3759e-04 - val_loss: 1.3512e-04 - 9s/epoch - 111us/sample
Epoch 61/80
84077/84077 - 9s - loss: 1.3711e-04 - val_loss: 1.3087e-04 - 9s/epoch - 111us/sample
Epoch 62/80
84077/84077 - 9s - loss: 1.3681e-04 - val_loss: 1.3148e-04 - 9s/epoch - 111us/sample
Epoch 63/80
84077/84077 - 9s - loss: 1.3679e-04 - val_loss: 1.3122e-04 - 9s/epoch - 111us/sample
Epoch 64/80
84077/84077 - 9s - loss: 1.3671e-04 - val_loss: 1.3099e-04 - 9s/epoch - 111us/sample
Epoch 65/80
84077/84077 - 9s - loss: 1.3645e-04 - val_loss: 1.3055e-04 - 9s/epoch - 113us/sample
Epoch 66/80
84077/84077 - 9s - loss: 1.3599e-04 - val_loss: 1.3057e-04 - 9s/epoch - 111us/sample
Epoch 67/80
84077/84077 - 9s - loss: 1.3568e-04 - val_loss: 1.3034e-04 - 9s/epoch - 111us/sample
Epoch 68/80
84077/84077 - 9s - loss: 1.3536e-04 - val_loss: 1.2885e-04 - 9s/epoch - 111us/sample
Epoch 69/80
84077/84077 - 9s - loss: 1.3504e-04 - val_loss: 1.2903e-04 - 9s/epoch - 110us/sample
Epoch 70/80
84077/84077 - 9s - loss: 1.3520e-04 - val_loss: 1.3024e-04 - 9s/epoch - 111us/sample
Epoch 71/80
84077/84077 - 9s - loss: 1.3520e-04 - val_loss: 1.2856e-04 - 9s/epoch - 112us/sample
Epoch 72/80
84077/84077 - 9s - loss: 1.3452e-04 - val_loss: 1.3002e-04 - 9s/epoch - 112us/sample
Epoch 73/80
84077/84077 - 9s - loss: 1.3450e-04 - val_loss: 1.2833e-04 - 9s/epoch - 111us/sample
Epoch 74/80
84077/84077 - 9s - loss: 1.3422e-04 - val_loss: 1.2834e-04 - 9s/epoch - 111us/sample
Epoch 75/80
84077/84077 - 9s - loss: 1.3393e-04 - val_loss: 1.2827e-04 - 9s/epoch - 110us/sample
Epoch 76/80
84077/84077 - 9s - loss: 1.3382e-04 - val_loss: 1.2776e-04 - 9s/epoch - 111us/sample
Epoch 77/80
84077/84077 - 9s - loss: 1.3365e-04 - val_loss: 1.2917e-04 - 9s/epoch - 111us/sample
Epoch 78/80
84077/84077 - 9s - loss: 1.3328e-04 - val_loss: 1.2709e-04 - 9s/epoch - 113us/sample
Epoch 79/80
84077/84077 - 9s - loss: 1.3332e-04 - val_loss: 1.2864e-04 - 9s/epoch - 111us/sample
Epoch 80/80
84077/84077 - 9s - loss: 1.3320e-04 - val_loss: 1.2899e-04 - 9s/epoch - 111us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012899334609994614
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 13:59:15.065163: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_20/outputlayer/BiasAdd' id:21597 op device:{requested: '', assigned: ''} def:{{{node decoder_model_20/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_20/outputlayer/MatMul, decoder_model_20/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02994283520750632
cosine 0.029579528945255035
MAE: 0.002224316536396238
RMSE: 0.010199435041102987
r2: 0.9189684819007503
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 80, 0.0012, 0.2, 188, 0.00013319642290759702, 0.00012899334609994614, 0.02994283520750632, 0.029579528945255035, 0.002224316536396238, 0.010199435041102987, 0.9189684819007503, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 95 0.0012 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 1791)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/95
2023-02-12 13:59:27.185160: W tensorflow/c/c_api.cc:291] Operation '{name:'training_26/Adam/batch_normalization_53/gamma/v/Assign' id:23566 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/batch_normalization_53/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/batch_normalization_53/gamma/v, training_26/Adam/batch_normalization_53/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 13:59:38.329906: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_21/mul' id:22911 op device:{requested: '', assigned: ''} def:{{{node loss_21/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_21/mul/x, loss_21/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0161 - val_loss: 0.0016 - 15s/epoch - 173us/sample
Epoch 2/95
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0013 - 9s/epoch - 112us/sample
Epoch 3/95
84077/84077 - 10s - loss: 0.0096 - val_loss: 0.0308 - 10s/epoch - 113us/sample
Epoch 4/95
84077/84077 - 9s - loss: 0.0016 - val_loss: 9.7567e-04 - 9s/epoch - 112us/sample
Epoch 5/95
84077/84077 - 9s - loss: 0.0013 - val_loss: 9.6377e-04 - 9s/epoch - 111us/sample
Epoch 6/95
84077/84077 - 9s - loss: 8.2613e-04 - val_loss: 7.5808e-04 - 9s/epoch - 112us/sample
Epoch 7/95
84077/84077 - 9s - loss: 7.2587e-04 - val_loss: 5.7822e-04 - 9s/epoch - 111us/sample
Epoch 8/95
84077/84077 - 9s - loss: 5.9977e-04 - val_loss: 4.8737e-04 - 9s/epoch - 112us/sample
Epoch 9/95
84077/84077 - 10s - loss: 5.5211e-04 - val_loss: 4.4120e-04 - 10s/epoch - 113us/sample
Epoch 10/95
84077/84077 - 9s - loss: 5.0265e-04 - val_loss: 7.5576e-04 - 9s/epoch - 112us/sample
Epoch 11/95
84077/84077 - 9s - loss: 4.4570e-04 - val_loss: 3.6669e-04 - 9s/epoch - 111us/sample
Epoch 12/95
84077/84077 - 9s - loss: 4.0431e-04 - val_loss: 3.3308e-04 - 9s/epoch - 111us/sample
Epoch 13/95
84077/84077 - 9s - loss: 3.7653e-04 - val_loss: 3.2186e-04 - 9s/epoch - 112us/sample
Epoch 14/95
84077/84077 - 9s - loss: 4.0848e-04 - val_loss: 3.1603e-04 - 9s/epoch - 113us/sample
Epoch 15/95
84077/84077 - 9s - loss: 3.4068e-04 - val_loss: 2.8364e-04 - 9s/epoch - 112us/sample
Epoch 16/95
84077/84077 - 9s - loss: 3.2059e-04 - val_loss: 2.8607e-04 - 9s/epoch - 112us/sample
Epoch 17/95
84077/84077 - 9s - loss: 3.0523e-04 - val_loss: 2.7173e-04 - 9s/epoch - 111us/sample
Epoch 18/95
84077/84077 - 9s - loss: 3.0055e-04 - val_loss: 2.6589e-04 - 9s/epoch - 111us/sample
Epoch 19/95
84077/84077 - 9s - loss: 2.8443e-04 - val_loss: 2.4972e-04 - 9s/epoch - 113us/sample
Epoch 20/95
84077/84077 - 9s - loss: 3.1810e-04 - val_loss: 2.4934e-04 - 9s/epoch - 112us/sample
Epoch 21/95
84077/84077 - 9s - loss: 2.7509e-04 - val_loss: 2.4330e-04 - 9s/epoch - 111us/sample
Epoch 22/95
84077/84077 - 10s - loss: 2.6808e-04 - val_loss: 2.4795e-04 - 10s/epoch - 114us/sample
Epoch 23/95
84077/84077 - 9s - loss: 2.5924e-04 - val_loss: 2.2993e-04 - 9s/epoch - 112us/sample
Epoch 24/95
84077/84077 - 9s - loss: 2.5962e-04 - val_loss: 2.4012e-04 - 9s/epoch - 112us/sample
Epoch 25/95
84077/84077 - 9s - loss: 2.5709e-04 - val_loss: 2.2569e-04 - 9s/epoch - 113us/sample
Epoch 26/95
84077/84077 - 9s - loss: 2.4726e-04 - val_loss: 2.2436e-04 - 9s/epoch - 112us/sample
Epoch 27/95
84077/84077 - 9s - loss: 2.4454e-04 - val_loss: 2.1969e-04 - 9s/epoch - 111us/sample
Epoch 28/95
84077/84077 - 9s - loss: 2.4866e-04 - val_loss: 2.2067e-04 - 9s/epoch - 112us/sample
Epoch 29/95
84077/84077 - 9s - loss: 2.3718e-04 - val_loss: 2.1836e-04 - 9s/epoch - 111us/sample
Epoch 30/95
84077/84077 - 9s - loss: 2.3376e-04 - val_loss: 2.1383e-04 - 9s/epoch - 112us/sample
Epoch 31/95
84077/84077 - 9s - loss: 2.3368e-04 - val_loss: 2.1315e-04 - 9s/epoch - 113us/sample
Epoch 32/95
84077/84077 - 9s - loss: 2.2937e-04 - val_loss: 2.1014e-04 - 9s/epoch - 112us/sample
Epoch 33/95
84077/84077 - 9s - loss: 2.2977e-04 - val_loss: 2.0714e-04 - 9s/epoch - 111us/sample
Epoch 34/95
84077/84077 - 9s - loss: 2.2498e-04 - val_loss: 2.0634e-04 - 9s/epoch - 112us/sample
Epoch 35/95
84077/84077 - 9s - loss: 2.2755e-04 - val_loss: 2.0675e-04 - 9s/epoch - 113us/sample
Epoch 36/95
84077/84077 - 9s - loss: 2.2380e-04 - val_loss: 2.0386e-04 - 9s/epoch - 112us/sample
Epoch 37/95
84077/84077 - 9s - loss: 2.2102e-04 - val_loss: 2.0401e-04 - 9s/epoch - 113us/sample
Epoch 38/95
84077/84077 - 9s - loss: 2.1924e-04 - val_loss: 2.0099e-04 - 9s/epoch - 112us/sample
Epoch 39/95
84077/84077 - 9s - loss: 2.2035e-04 - val_loss: 2.0012e-04 - 9s/epoch - 111us/sample
Epoch 40/95
84077/84077 - 9s - loss: 2.1725e-04 - val_loss: 1.9656e-04 - 9s/epoch - 111us/sample
Epoch 41/95
84077/84077 - 9s - loss: 2.1822e-04 - val_loss: 1.9852e-04 - 9s/epoch - 113us/sample
Epoch 42/95
84077/84077 - 9s - loss: 2.1641e-04 - val_loss: 1.9576e-04 - 9s/epoch - 113us/sample
Epoch 43/95
84077/84077 - 9s - loss: 2.1712e-04 - val_loss: 1.9486e-04 - 9s/epoch - 112us/sample
Epoch 44/95
84077/84077 - 9s - loss: 2.1220e-04 - val_loss: 1.9299e-04 - 9s/epoch - 111us/sample
Epoch 45/95
84077/84077 - 9s - loss: 2.1129e-04 - val_loss: 1.9073e-04 - 9s/epoch - 111us/sample
Epoch 46/95
84077/84077 - 9s - loss: 2.1314e-04 - val_loss: 1.9097e-04 - 9s/epoch - 112us/sample
Epoch 47/95
84077/84077 - 10s - loss: 2.0903e-04 - val_loss: 1.9099e-04 - 10s/epoch - 113us/sample
Epoch 48/95
84077/84077 - 9s - loss: 2.0791e-04 - val_loss: 1.8956e-04 - 9s/epoch - 112us/sample
Epoch 49/95
84077/84077 - 9s - loss: 2.0730e-04 - val_loss: 1.8869e-04 - 9s/epoch - 112us/sample
Epoch 50/95
84077/84077 - 9s - loss: 2.1912e-04 - val_loss: 1.9856e-04 - 9s/epoch - 111us/sample
Epoch 51/95
84077/84077 - 9s - loss: 2.0945e-04 - val_loss: 1.9114e-04 - 9s/epoch - 111us/sample
Epoch 52/95
84077/84077 - 9s - loss: 2.0576e-04 - val_loss: 1.8879e-04 - 9s/epoch - 111us/sample
Epoch 53/95
84077/84077 - 9s - loss: 2.0440e-04 - val_loss: 1.8574e-04 - 9s/epoch - 113us/sample
Epoch 54/95
84077/84077 - 10s - loss: 2.0383e-04 - val_loss: 1.8765e-04 - 10s/epoch - 114us/sample
Epoch 55/95
84077/84077 - 9s - loss: 2.0314e-04 - val_loss: 1.8595e-04 - 9s/epoch - 112us/sample
Epoch 56/95
84077/84077 - 9s - loss: 2.0264e-04 - val_loss: 1.8445e-04 - 9s/epoch - 111us/sample
Epoch 57/95
84077/84077 - 9s - loss: 2.0237e-04 - val_loss: 1.8286e-04 - 9s/epoch - 112us/sample
Epoch 58/95
84077/84077 - 9s - loss: 2.0149e-04 - val_loss: 1.8382e-04 - 9s/epoch - 111us/sample
Epoch 59/95
84077/84077 - 9s - loss: 2.0879e-04 - val_loss: 1.9014e-04 - 9s/epoch - 112us/sample
Epoch 60/95
84077/84077 - 9s - loss: 2.0068e-04 - val_loss: 1.8240e-04 - 9s/epoch - 112us/sample
Epoch 61/95
84077/84077 - 9s - loss: 1.9979e-04 - val_loss: 1.8358e-04 - 9s/epoch - 112us/sample
Epoch 62/95
84077/84077 - 9s - loss: 1.9901e-04 - val_loss: 1.8132e-04 - 9s/epoch - 111us/sample
Epoch 63/95
84077/84077 - 9s - loss: 2.0064e-04 - val_loss: 1.8287e-04 - 9s/epoch - 111us/sample
Epoch 64/95
84077/84077 - 9s - loss: 1.9796e-04 - val_loss: 1.8335e-04 - 9s/epoch - 112us/sample
Epoch 65/95
84077/84077 - 9s - loss: 1.9710e-04 - val_loss: 1.7839e-04 - 9s/epoch - 113us/sample
Epoch 66/95
84077/84077 - 10s - loss: 1.9725e-04 - val_loss: 1.8034e-04 - 10s/epoch - 113us/sample
Epoch 67/95
84077/84077 - 9s - loss: 1.9660e-04 - val_loss: 1.7997e-04 - 9s/epoch - 112us/sample
Epoch 68/95
84077/84077 - 9s - loss: 1.9597e-04 - val_loss: 1.7973e-04 - 9s/epoch - 111us/sample
Epoch 69/95
84077/84077 - 9s - loss: 1.9583e-04 - val_loss: 1.7908e-04 - 9s/epoch - 111us/sample
Epoch 70/95
84077/84077 - 9s - loss: 1.9527e-04 - val_loss: 1.8094e-04 - 9s/epoch - 112us/sample
Epoch 71/95
84077/84077 - 9s - loss: 1.9502e-04 - val_loss: 1.7803e-04 - 9s/epoch - 112us/sample
Epoch 72/95
84077/84077 - 9s - loss: 1.9809e-04 - val_loss: 1.8215e-04 - 9s/epoch - 112us/sample
Epoch 73/95
84077/84077 - 10s - loss: 1.9405e-04 - val_loss: 1.7736e-04 - 10s/epoch - 113us/sample
Epoch 74/95
84077/84077 - 9s - loss: 1.9389e-04 - val_loss: 1.7696e-04 - 9s/epoch - 112us/sample
Epoch 75/95
84077/84077 - 9s - loss: 1.9465e-04 - val_loss: 1.7775e-04 - 9s/epoch - 111us/sample
Epoch 76/95
84077/84077 - 9s - loss: 1.9360e-04 - val_loss: 1.7672e-04 - 9s/epoch - 112us/sample
Epoch 77/95
84077/84077 - 9s - loss: 1.9422e-04 - val_loss: 1.7961e-04 - 9s/epoch - 112us/sample
Epoch 78/95
84077/84077 - 9s - loss: 1.9169e-04 - val_loss: 1.7662e-04 - 9s/epoch - 112us/sample
Epoch 79/95
84077/84077 - 9s - loss: 1.9174e-04 - val_loss: 1.7655e-04 - 9s/epoch - 112us/sample
Epoch 80/95
84077/84077 - 9s - loss: 1.9301e-04 - val_loss: 1.7827e-04 - 9s/epoch - 111us/sample
Epoch 81/95
84077/84077 - 9s - loss: 1.9204e-04 - val_loss: 1.7449e-04 - 9s/epoch - 111us/sample
Epoch 82/95
84077/84077 - 9s - loss: 1.9019e-04 - val_loss: 1.7430e-04 - 9s/epoch - 113us/sample
Epoch 83/95
84077/84077 - 9s - loss: 1.9116e-04 - val_loss: 1.7670e-04 - 9s/epoch - 112us/sample
Epoch 84/95
84077/84077 - 9s - loss: 1.8948e-04 - val_loss: 1.7466e-04 - 9s/epoch - 112us/sample
Epoch 85/95
84077/84077 - 9s - loss: 1.8986e-04 - val_loss: 1.7427e-04 - 9s/epoch - 112us/sample
Epoch 86/95
84077/84077 - 10s - loss: 1.9028e-04 - val_loss: 1.7568e-04 - 10s/epoch - 114us/sample
Epoch 87/95
84077/84077 - 9s - loss: 1.9058e-04 - val_loss: 1.8707e-04 - 9s/epoch - 111us/sample
Epoch 88/95
84077/84077 - 9s - loss: 1.8993e-04 - val_loss: 1.7635e-04 - 9s/epoch - 113us/sample
Epoch 89/95
84077/84077 - 9s - loss: 1.8887e-04 - val_loss: 1.7282e-04 - 9s/epoch - 112us/sample
Epoch 90/95
84077/84077 - 9s - loss: 1.8804e-04 - val_loss: 1.7292e-04 - 9s/epoch - 112us/sample
Epoch 91/95
84077/84077 - 9s - loss: 1.9201e-04 - val_loss: 1.7416e-04 - 9s/epoch - 111us/sample
Epoch 92/95
84077/84077 - 9s - loss: 1.9310e-04 - val_loss: 1.7192e-04 - 9s/epoch - 112us/sample
Epoch 93/95
84077/84077 - 9s - loss: 1.9105e-04 - val_loss: 1.7572e-04 - 9s/epoch - 113us/sample
Epoch 94/95
84077/84077 - 9s - loss: 1.8787e-04 - val_loss: 1.7152e-04 - 9s/epoch - 112us/sample
Epoch 95/95
84077/84077 - 9s - loss: 1.8719e-04 - val_loss: 1.7233e-04 - 9s/epoch - 111us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017232529105055036
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 14:14:25.099536: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_21/outputlayer/BiasAdd' id:22882 op device:{requested: '', assigned: ''} def:{{{node decoder_model_21/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_21/outputlayer/MatMul, decoder_model_21/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0200591372494921
cosine 0.019814411088023405
MAE: 0.002239627298300776
RMSE: 0.008116112383477246
r2: 0.9489392232943741
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 64, 95, 0.0012, 0.2, 188, 0.00018719040729763757, 0.00017232529105055036, 0.0200591372494921, 0.019814411088023405, 0.002239627298300776, 0.008116112383477246, 0.9489392232943741, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 511.49230331698664
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 511.49230331698664.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664]
[2.0 80 0.0012 64 0] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 1886)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 14:14:37.395532: W tensorflow/c/c_api.cc:291] Operation '{name:'training_28/Adam/dense_enc0_22/kernel/m/Assign' id:24701 op device:{requested: '', assigned: ''} def:{{{node training_28/Adam/dense_enc0_22/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_28/Adam/dense_enc0_22/kernel/m, training_28/Adam/dense_enc0_22/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 14:14:49.085219: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_22/mul' id:24185 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0734 - val_loss: 0.0681 - 15s/epoch - 182us/sample
Epoch 2/80
84077/84077 - 10s - loss: 0.0773 - val_loss: 0.0705 - 10s/epoch - 115us/sample
Epoch 3/80
84077/84077 - 10s - loss: 0.0723 - val_loss: 0.0693 - 10s/epoch - 113us/sample
Epoch 4/80
84077/84077 - 10s - loss: 0.0712 - val_loss: 0.0688 - 10s/epoch - 113us/sample
Epoch 5/80
84077/84077 - 10s - loss: 0.0688 - val_loss: 0.0683 - 10s/epoch - 113us/sample
Epoch 6/80
84077/84077 - 10s - loss: 0.0680 - val_loss: 0.0678 - 10s/epoch - 113us/sample
Epoch 7/80
84077/84077 - 10s - loss: 0.0671 - val_loss: 0.0675 - 10s/epoch - 113us/sample
Epoch 8/80
84077/84077 - 10s - loss: 0.0669 - val_loss: 0.0674 - 10s/epoch - 115us/sample
Epoch 9/80
84077/84077 - 10s - loss: 0.0669 - val_loss: 0.0674 - 10s/epoch - 114us/sample
Epoch 10/80
84077/84077 - 10s - loss: 0.0669 - val_loss: 0.0674 - 10s/epoch - 114us/sample
Epoch 11/80
84077/84077 - 10s - loss: 0.0669 - val_loss: 0.0674 - 10s/epoch - 114us/sample
Epoch 12/80
84077/84077 - 10s - loss: 0.0669 - val_loss: 0.0674 - 10s/epoch - 113us/sample
Epoch 13/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0674 - 10s/epoch - 113us/sample
Epoch 14/80
84077/84077 - 10s - loss: 0.0671 - val_loss: 0.0674 - 10s/epoch - 115us/sample
Epoch 15/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0674 - 10s/epoch - 114us/sample
Epoch 16/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0674 - 10s/epoch - 114us/sample
Epoch 17/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 18/80
84077/84077 - 10s - loss: 0.0670 - val_loss: 0.0674 - 10s/epoch - 113us/sample
Epoch 19/80
84077/84077 - 10s - loss: 0.0669 - val_loss: 0.0674 - 10s/epoch - 113us/sample
Epoch 20/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 21/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 22/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 23/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 24/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 25/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 26/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 27/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 115us/sample
Epoch 28/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 29/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 30/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 31/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 32/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 33/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 115us/sample
Epoch 34/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 35/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 36/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 37/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 38/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 39/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 116us/sample
Epoch 40/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 41/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 42/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 43/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 44/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 45/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 46/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 47/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 48/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 49/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 50/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 51/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 52/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 53/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 54/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 55/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 56/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 57/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 58/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 59/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 60/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 61/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 62/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 63/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 64/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 65/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 115us/sample
Epoch 66/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 67/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 115us/sample
Epoch 68/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 69/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 70/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 71/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 72/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 73/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 74/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 75/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 76/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
Epoch 77/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 115us/sample
Epoch 78/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 79/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 114us/sample
Epoch 80/80
84077/84077 - 10s - loss: 0.0668 - val_loss: 0.0673 - 10s/epoch - 113us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573965351068
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 14:27:26.933243: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_22/outputlayer/BiasAdd' id:24137 op device:{requested: '', assigned: ''} def:{{{node decoder_model_22/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_22/outputlayer/MatMul, decoder_model_22/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2773731619522586
cosine 1.1896622054020152
MAE: 5.130824163992036
RMSE: 6.3228067440410545
r2: -29376.975028604094
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 64, 80, 0.0012, 0.2, 188, 0.0668369283751698, 0.06734573965351068, 1.2773731619522586, 1.1896622054020152, 5.130824163992036, 6.3228067440410545, -29376.975028604094, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 80 0.0009999999999999998 32 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 1980)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 14:27:39.658724: W tensorflow/c/c_api.cc:291] Operation '{name:'training_30/Adam/dense_dec0_23/kernel/m/Assign' id:26060 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/dense_dec0_23/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_30/Adam/dense_dec0_23/kernel/m, training_30/Adam/dense_dec0_23/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 14:27:55.963950: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_23/mul' id:25510 op device:{requested: '', assigned: ''} def:{{{node loss_23/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_23/mul/x, loss_23/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0032 - val_loss: 0.0015 - 20s/epoch - 242us/sample
Epoch 2/80
84077/84077 - 14s - loss: 0.0028 - val_loss: 0.0013 - 14s/epoch - 171us/sample
Epoch 3/80
84077/84077 - 14s - loss: 6.8871e-04 - val_loss: 4.9435e-04 - 14s/epoch - 170us/sample
Epoch 4/80
84077/84077 - 14s - loss: 5.0847e-04 - val_loss: 5.3102e-04 - 14s/epoch - 170us/sample
Epoch 5/80
84077/84077 - 14s - loss: 4.9703e-04 - val_loss: 3.5126e-04 - 14s/epoch - 172us/sample
Epoch 6/80
84077/84077 - 15s - loss: 3.5801e-04 - val_loss: 3.0451e-04 - 15s/epoch - 173us/sample
Epoch 7/80
84077/84077 - 14s - loss: 3.2505e-04 - val_loss: 2.8412e-04 - 14s/epoch - 172us/sample
Epoch 8/80
84077/84077 - 14s - loss: 2.9184e-04 - val_loss: 2.4952e-04 - 14s/epoch - 170us/sample
Epoch 9/80
84077/84077 - 15s - loss: 2.7239e-04 - val_loss: 2.3764e-04 - 15s/epoch - 173us/sample
Epoch 10/80
84077/84077 - 14s - loss: 2.5576e-04 - val_loss: 2.2301e-04 - 14s/epoch - 172us/sample
Epoch 11/80
84077/84077 - 14s - loss: 2.4309e-04 - val_loss: 2.1479e-04 - 14s/epoch - 172us/sample
Epoch 12/80
84077/84077 - 14s - loss: 2.3409e-04 - val_loss: 2.0614e-04 - 14s/epoch - 170us/sample
Epoch 13/80
84077/84077 - 14s - loss: 2.2466e-04 - val_loss: 2.0310e-04 - 14s/epoch - 171us/sample
Epoch 14/80
84077/84077 - 14s - loss: 2.1795e-04 - val_loss: 1.9495e-04 - 14s/epoch - 172us/sample
Epoch 15/80
84077/84077 - 14s - loss: 2.1211e-04 - val_loss: 1.9069e-04 - 14s/epoch - 171us/sample
Epoch 16/80
84077/84077 - 14s - loss: 2.0644e-04 - val_loss: 1.8731e-04 - 14s/epoch - 170us/sample
Epoch 17/80
84077/84077 - 14s - loss: 2.0155e-04 - val_loss: 1.8497e-04 - 14s/epoch - 171us/sample
Epoch 18/80
84077/84077 - 15s - loss: 1.9773e-04 - val_loss: 1.7866e-04 - 15s/epoch - 173us/sample
Epoch 19/80
84077/84077 - 14s - loss: 1.9465e-04 - val_loss: 1.7889e-04 - 14s/epoch - 171us/sample
Epoch 20/80
84077/84077 - 14s - loss: 1.9166e-04 - val_loss: 1.7710e-04 - 14s/epoch - 170us/sample
Epoch 21/80
84077/84077 - 14s - loss: 1.8924e-04 - val_loss: 1.7568e-04 - 14s/epoch - 170us/sample
Epoch 22/80
84077/84077 - 15s - loss: 1.8661e-04 - val_loss: 1.7185e-04 - 15s/epoch - 173us/sample
Epoch 23/80
84077/84077 - 14s - loss: 1.8421e-04 - val_loss: 1.7254e-04 - 14s/epoch - 171us/sample
Epoch 24/80
84077/84077 - 14s - loss: 1.8196e-04 - val_loss: 1.6850e-04 - 14s/epoch - 170us/sample
Epoch 25/80
84077/84077 - 14s - loss: 1.8062e-04 - val_loss: 1.7143e-04 - 14s/epoch - 170us/sample
Epoch 26/80
84077/84077 - 14s - loss: 1.7860e-04 - val_loss: 1.6724e-04 - 14s/epoch - 172us/sample
Epoch 27/80
84077/84077 - 14s - loss: 1.7695e-04 - val_loss: 1.6658e-04 - 14s/epoch - 172us/sample
Epoch 28/80
84077/84077 - 14s - loss: 1.7496e-04 - val_loss: 1.6279e-04 - 14s/epoch - 170us/sample
Epoch 29/80
84077/84077 - 14s - loss: 1.7403e-04 - val_loss: 1.6320e-04 - 14s/epoch - 170us/sample
Epoch 30/80
84077/84077 - 15s - loss: 1.7252e-04 - val_loss: 1.6210e-04 - 15s/epoch - 174us/sample
Epoch 31/80
84077/84077 - 14s - loss: 1.7156e-04 - val_loss: 1.6205e-04 - 14s/epoch - 171us/sample
Epoch 32/80
84077/84077 - 14s - loss: 1.7047e-04 - val_loss: 1.5877e-04 - 14s/epoch - 171us/sample
Epoch 33/80
84077/84077 - 14s - loss: 1.6999e-04 - val_loss: 1.5977e-04 - 14s/epoch - 170us/sample
Epoch 34/80
84077/84077 - 14s - loss: 1.6824e-04 - val_loss: 1.5746e-04 - 14s/epoch - 172us/sample
Epoch 35/80
84077/84077 - 14s - loss: 1.6752e-04 - val_loss: 1.5612e-04 - 14s/epoch - 172us/sample
Epoch 36/80
84077/84077 - 14s - loss: 1.6711e-04 - val_loss: 1.5993e-04 - 14s/epoch - 171us/sample
Epoch 37/80
84077/84077 - 14s - loss: 1.6583e-04 - val_loss: 1.5437e-04 - 14s/epoch - 170us/sample
Epoch 38/80
84077/84077 - 15s - loss: 1.6486e-04 - val_loss: 1.5353e-04 - 15s/epoch - 173us/sample
Epoch 39/80
84077/84077 - 14s - loss: 1.6462e-04 - val_loss: 1.5319e-04 - 14s/epoch - 171us/sample
Epoch 40/80
84077/84077 - 14s - loss: 1.6364e-04 - val_loss: 1.5277e-04 - 14s/epoch - 171us/sample
Epoch 41/80
84077/84077 - 14s - loss: 1.6457e-04 - val_loss: 1.5350e-04 - 14s/epoch - 170us/sample
Epoch 42/80
84077/84077 - 14s - loss: 1.6271e-04 - val_loss: 1.5164e-04 - 14s/epoch - 171us/sample
Epoch 43/80
84077/84077 - 15s - loss: 1.6249e-04 - val_loss: 1.5405e-04 - 15s/epoch - 173us/sample
Epoch 44/80
84077/84077 - 14s - loss: 1.6176e-04 - val_loss: 1.5190e-04 - 14s/epoch - 171us/sample
Epoch 45/80
84077/84077 - 14s - loss: 1.6117e-04 - val_loss: 1.5273e-04 - 14s/epoch - 170us/sample
Epoch 46/80
84077/84077 - 14s - loss: 1.6084e-04 - val_loss: 1.5433e-04 - 14s/epoch - 171us/sample
Epoch 47/80
84077/84077 - 14s - loss: 1.6013e-04 - val_loss: 1.4893e-04 - 14s/epoch - 172us/sample
Epoch 48/80
84077/84077 - 14s - loss: 1.5993e-04 - val_loss: 1.5266e-04 - 14s/epoch - 170us/sample
Epoch 49/80
84077/84077 - 14s - loss: 1.5964e-04 - val_loss: 1.4897e-04 - 14s/epoch - 170us/sample
Epoch 50/80
84077/84077 - 14s - loss: 1.5869e-04 - val_loss: 1.4880e-04 - 14s/epoch - 171us/sample
Epoch 51/80
84077/84077 - 15s - loss: 1.5852e-04 - val_loss: 1.4944e-04 - 15s/epoch - 174us/sample
Epoch 52/80
84077/84077 - 14s - loss: 1.5862e-04 - val_loss: 1.4755e-04 - 14s/epoch - 170us/sample
Epoch 53/80
84077/84077 - 14s - loss: 1.5751e-04 - val_loss: 1.4852e-04 - 14s/epoch - 171us/sample
Epoch 54/80
84077/84077 - 14s - loss: 1.5710e-04 - val_loss: 1.4780e-04 - 14s/epoch - 172us/sample
Epoch 55/80
84077/84077 - 14s - loss: 1.5746e-04 - val_loss: 1.4694e-04 - 14s/epoch - 171us/sample
Epoch 56/80
84077/84077 - 14s - loss: 1.5687e-04 - val_loss: 1.4640e-04 - 14s/epoch - 170us/sample
Epoch 57/80
84077/84077 - 14s - loss: 1.5595e-04 - val_loss: 1.4630e-04 - 14s/epoch - 171us/sample
Epoch 58/80
84077/84077 - 14s - loss: 1.5595e-04 - val_loss: 1.4613e-04 - 14s/epoch - 172us/sample
Epoch 59/80
84077/84077 - 14s - loss: 1.5587e-04 - val_loss: 1.4783e-04 - 14s/epoch - 171us/sample
Epoch 60/80
84077/84077 - 14s - loss: 1.5561e-04 - val_loss: 1.4466e-04 - 14s/epoch - 171us/sample
Epoch 61/80
84077/84077 - 14s - loss: 1.5496e-04 - val_loss: 1.4542e-04 - 14s/epoch - 172us/sample
Epoch 62/80
84077/84077 - 14s - loss: 1.5505e-04 - val_loss: 1.4587e-04 - 14s/epoch - 171us/sample
Epoch 63/80
84077/84077 - 16s - loss: 1.5510e-04 - val_loss: 1.4497e-04 - 16s/epoch - 185us/sample
Epoch 64/80
84077/84077 - 14s - loss: 1.5435e-04 - val_loss: 1.4287e-04 - 14s/epoch - 171us/sample
Epoch 65/80
84077/84077 - 14s - loss: 1.5456e-04 - val_loss: 1.4749e-04 - 14s/epoch - 172us/sample
Epoch 66/80
84077/84077 - 14s - loss: 1.5413e-04 - val_loss: 1.4295e-04 - 14s/epoch - 170us/sample
Epoch 67/80
84077/84077 - 14s - loss: 1.5353e-04 - val_loss: 1.4539e-04 - 14s/epoch - 170us/sample
Epoch 68/80
84077/84077 - 14s - loss: 1.5344e-04 - val_loss: 1.4339e-04 - 14s/epoch - 172us/sample
Epoch 69/80
84077/84077 - 14s - loss: 1.5317e-04 - val_loss: 1.4349e-04 - 14s/epoch - 171us/sample
Epoch 70/80
84077/84077 - 14s - loss: 1.5298e-04 - val_loss: 1.4443e-04 - 14s/epoch - 171us/sample
Epoch 71/80
84077/84077 - 14s - loss: 1.5254e-04 - val_loss: 1.4338e-04 - 14s/epoch - 171us/sample
Epoch 72/80
84077/84077 - 14s - loss: 1.5251e-04 - val_loss: 1.4334e-04 - 14s/epoch - 172us/sample
Epoch 73/80
84077/84077 - 14s - loss: 1.5289e-04 - val_loss: 1.4273e-04 - 14s/epoch - 171us/sample
Epoch 74/80
84077/84077 - 14s - loss: 1.5208e-04 - val_loss: 1.4422e-04 - 14s/epoch - 171us/sample
Epoch 75/80
84077/84077 - 14s - loss: 1.5188e-04 - val_loss: 1.4348e-04 - 14s/epoch - 170us/sample
Epoch 76/80
84077/84077 - 14s - loss: 1.5207e-04 - val_loss: 1.4305e-04 - 14s/epoch - 172us/sample
Epoch 77/80
84077/84077 - 14s - loss: 1.5162e-04 - val_loss: 1.4415e-04 - 14s/epoch - 172us/sample
Epoch 78/80
84077/84077 - 14s - loss: 1.5163e-04 - val_loss: 1.4124e-04 - 14s/epoch - 171us/sample
Epoch 79/80
84077/84077 - 14s - loss: 1.5158e-04 - val_loss: 1.4194e-04 - 14s/epoch - 170us/sample
Epoch 80/80
84077/84077 - 14s - loss: 1.5118e-04 - val_loss: 1.4260e-04 - 14s/epoch - 172us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00014260316479106154
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 14:46:56.632757: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_23/outputlayer/BiasAdd' id:25474 op device:{requested: '', assigned: ''} def:{{{node decoder_model_23/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_23/outputlayer/MatMul, decoder_model_23/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03669761765774406
cosine 0.03625632499039703
MAE: 0.0023948798736837112
RMSE: 0.011662233300698757
r2: 0.8938120262895289
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 32, 80, 0.0009999999999999998, 0.2, 188, 0.00015118043488142948, 0.00014260316479106154, 0.03669761765774406, 0.03625632499039703, 0.0023948798736837112, 0.011662233300698757, 0.8938120262895289, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0006000000000000001 32 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 1791)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 14:47:09.775152: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/bottleneck_zmean_24/bias/v/Assign' id:27418 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/bottleneck_zmean_24/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/bottleneck_zmean_24/bias/v, training_32/Adam/bottleneck_zmean_24/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 14:47:26.344657: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_24/mul' id:26795 op device:{requested: '', assigned: ''} def:{{{node loss_24/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_24/mul/x, loss_24/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0114 - val_loss: 0.0014 - 21s/epoch - 246us/sample
Epoch 2/90
84077/84077 - 15s - loss: 0.0011 - val_loss: 7.0443e-04 - 15s/epoch - 175us/sample
Epoch 3/90
84077/84077 - 15s - loss: 6.3372e-04 - val_loss: 4.7192e-04 - 15s/epoch - 176us/sample
Epoch 4/90
84077/84077 - 15s - loss: 8.6301e-04 - val_loss: 4.9535e-04 - 15s/epoch - 176us/sample
Epoch 5/90
84077/84077 - 15s - loss: 5.2399e-04 - val_loss: 4.8248e-04 - 15s/epoch - 175us/sample
Epoch 6/90
84077/84077 - 15s - loss: 4.4739e-04 - val_loss: 3.7950e-04 - 15s/epoch - 175us/sample
Epoch 7/90
84077/84077 - 15s - loss: 0.0019 - val_loss: 3.8590e-04 - 15s/epoch - 175us/sample
Epoch 8/90
84077/84077 - 15s - loss: 3.9683e-04 - val_loss: 3.4909e-04 - 15s/epoch - 177us/sample
Epoch 9/90
84077/84077 - 15s - loss: 4.1602e-04 - val_loss: 3.3792e-04 - 15s/epoch - 176us/sample
Epoch 10/90
84077/84077 - 15s - loss: 3.6233e-04 - val_loss: 3.2663e-04 - 15s/epoch - 175us/sample
Epoch 11/90
84077/84077 - 15s - loss: 3.9427e-04 - val_loss: 3.1810e-04 - 15s/epoch - 175us/sample
Epoch 12/90
84077/84077 - 15s - loss: 3.4396e-04 - val_loss: 3.0981e-04 - 15s/epoch - 177us/sample
Epoch 13/90
84077/84077 - 15s - loss: 3.3973e-04 - val_loss: 3.1212e-04 - 15s/epoch - 175us/sample
Epoch 14/90
84077/84077 - 15s - loss: 3.3181e-04 - val_loss: 3.0296e-04 - 15s/epoch - 175us/sample
Epoch 15/90
84077/84077 - 15s - loss: 3.2308e-04 - val_loss: 2.9688e-04 - 15s/epoch - 176us/sample
Epoch 16/90
84077/84077 - 15s - loss: 3.1781e-04 - val_loss: 2.9029e-04 - 15s/epoch - 175us/sample
Epoch 17/90
84077/84077 - 15s - loss: 3.1319e-04 - val_loss: 2.8712e-04 - 15s/epoch - 175us/sample
Epoch 18/90
84077/84077 - 15s - loss: 3.0853e-04 - val_loss: 2.8327e-04 - 15s/epoch - 176us/sample
Epoch 19/90
84077/84077 - 15s - loss: 3.0581e-04 - val_loss: 2.8357e-04 - 15s/epoch - 176us/sample
Epoch 20/90
84077/84077 - 15s - loss: 3.0357e-04 - val_loss: 2.8065e-04 - 15s/epoch - 175us/sample
Epoch 21/90
84077/84077 - 15s - loss: 3.0173e-04 - val_loss: 2.7873e-04 - 15s/epoch - 175us/sample
Epoch 22/90
84077/84077 - 15s - loss: 3.0016e-04 - val_loss: 2.7764e-04 - 15s/epoch - 177us/sample
Epoch 23/90
84077/84077 - 15s - loss: 2.9716e-04 - val_loss: 2.7159e-04 - 15s/epoch - 175us/sample
Epoch 24/90
84077/84077 - 15s - loss: 2.9498e-04 - val_loss: 2.7045e-04 - 15s/epoch - 175us/sample
Epoch 25/90
84077/84077 - 15s - loss: 2.9328e-04 - val_loss: 2.7038e-04 - 15s/epoch - 176us/sample
Epoch 26/90
84077/84077 - 15s - loss: 2.9168e-04 - val_loss: 2.6682e-04 - 15s/epoch - 176us/sample
Epoch 27/90
84077/84077 - 15s - loss: 2.9001e-04 - val_loss: 2.6850e-04 - 15s/epoch - 175us/sample
Epoch 28/90
84077/84077 - 15s - loss: 2.8878e-04 - val_loss: 2.6598e-04 - 15s/epoch - 175us/sample
Epoch 29/90
84077/84077 - 15s - loss: 2.8804e-04 - val_loss: 2.6798e-04 - 15s/epoch - 176us/sample
Epoch 30/90
84077/84077 - 15s - loss: 2.8696e-04 - val_loss: 2.6552e-04 - 15s/epoch - 175us/sample
Epoch 31/90
84077/84077 - 15s - loss: 2.8556e-04 - val_loss: 2.6403e-04 - 15s/epoch - 175us/sample
Epoch 32/90
84077/84077 - 15s - loss: 2.8502e-04 - val_loss: 2.6244e-04 - 15s/epoch - 177us/sample
Epoch 33/90
84077/84077 - 15s - loss: 2.8431e-04 - val_loss: 2.6185e-04 - 15s/epoch - 176us/sample
Epoch 34/90
84077/84077 - 15s - loss: 2.8339e-04 - val_loss: 2.6017e-04 - 15s/epoch - 175us/sample
Epoch 35/90
84077/84077 - 15s - loss: 2.8322e-04 - val_loss: 2.6351e-04 - 15s/epoch - 176us/sample
Epoch 36/90
84077/84077 - 15s - loss: 2.8305e-04 - val_loss: 2.6069e-04 - 15s/epoch - 177us/sample
Epoch 37/90
84077/84077 - 15s - loss: 2.8246e-04 - val_loss: 2.6011e-04 - 15s/epoch - 175us/sample
Epoch 38/90
84077/84077 - 15s - loss: 2.8175e-04 - val_loss: 2.6088e-04 - 15s/epoch - 176us/sample
Epoch 39/90
84077/84077 - 15s - loss: 2.8147e-04 - val_loss: 2.6095e-04 - 15s/epoch - 176us/sample
Epoch 40/90
84077/84077 - 15s - loss: 2.8107e-04 - val_loss: 2.5853e-04 - 15s/epoch - 175us/sample
Epoch 41/90
84077/84077 - 15s - loss: 2.8054e-04 - val_loss: 2.5973e-04 - 15s/epoch - 175us/sample
Epoch 42/90
84077/84077 - 15s - loss: 2.8005e-04 - val_loss: 2.5899e-04 - 15s/epoch - 177us/sample
Epoch 43/90
84077/84077 - 15s - loss: 2.7971e-04 - val_loss: 2.5995e-04 - 15s/epoch - 175us/sample
Epoch 44/90
84077/84077 - 15s - loss: 2.7947e-04 - val_loss: 2.5823e-04 - 15s/epoch - 175us/sample
Epoch 45/90
84077/84077 - 15s - loss: 2.7921e-04 - val_loss: 2.5917e-04 - 15s/epoch - 177us/sample
Epoch 46/90
84077/84077 - 15s - loss: 2.7858e-04 - val_loss: 2.5733e-04 - 15s/epoch - 176us/sample
Epoch 47/90
84077/84077 - 15s - loss: 2.7874e-04 - val_loss: 2.5659e-04 - 15s/epoch - 175us/sample
Epoch 48/90
84077/84077 - 15s - loss: 2.7786e-04 - val_loss: 2.5587e-04 - 15s/epoch - 175us/sample
Epoch 49/90
84077/84077 - 15s - loss: 2.7686e-04 - val_loss: 2.5548e-04 - 15s/epoch - 177us/sample
Epoch 50/90
84077/84077 - 15s - loss: 2.7593e-04 - val_loss: 2.5461e-04 - 15s/epoch - 175us/sample
Epoch 51/90
84077/84077 - 15s - loss: 2.7546e-04 - val_loss: 2.5412e-04 - 15s/epoch - 175us/sample
Epoch 52/90
84077/84077 - 15s - loss: 2.7555e-04 - val_loss: 2.5485e-04 - 15s/epoch - 177us/sample
Epoch 53/90
84077/84077 - 15s - loss: 2.7489e-04 - val_loss: 2.5460e-04 - 15s/epoch - 176us/sample
Epoch 54/90
84077/84077 - 15s - loss: 2.7461e-04 - val_loss: 2.5461e-04 - 15s/epoch - 175us/sample
Epoch 55/90
84077/84077 - 15s - loss: 2.7352e-04 - val_loss: 2.5291e-04 - 15s/epoch - 175us/sample
Epoch 56/90
84077/84077 - 15s - loss: 2.7391e-04 - val_loss: 2.5370e-04 - 15s/epoch - 176us/sample
Epoch 57/90
84077/84077 - 15s - loss: 2.7347e-04 - val_loss: 2.5291e-04 - 15s/epoch - 176us/sample
Epoch 58/90
84077/84077 - 15s - loss: 2.7366e-04 - val_loss: 2.5219e-04 - 15s/epoch - 175us/sample
Epoch 59/90
84077/84077 - 15s - loss: 2.7291e-04 - val_loss: 2.5268e-04 - 15s/epoch - 175us/sample
Epoch 60/90
84077/84077 - 15s - loss: 2.7258e-04 - val_loss: 2.5322e-04 - 15s/epoch - 177us/sample
Epoch 61/90
84077/84077 - 15s - loss: 2.7234e-04 - val_loss: 2.5259e-04 - 15s/epoch - 175us/sample
Epoch 62/90
84077/84077 - 15s - loss: 2.7209e-04 - val_loss: 2.5118e-04 - 15s/epoch - 176us/sample
Epoch 63/90
84077/84077 - 15s - loss: 2.7137e-04 - val_loss: 2.5143e-04 - 15s/epoch - 175us/sample
Epoch 64/90
84077/84077 - 15s - loss: 2.7151e-04 - val_loss: 2.5108e-04 - 15s/epoch - 177us/sample
Epoch 65/90
84077/84077 - 15s - loss: 2.7112e-04 - val_loss: 2.5043e-04 - 15s/epoch - 175us/sample
Epoch 66/90
84077/84077 - 15s - loss: 2.7113e-04 - val_loss: 2.5002e-04 - 15s/epoch - 175us/sample
Epoch 67/90
84077/84077 - 15s - loss: 2.7096e-04 - val_loss: 2.5094e-04 - 15s/epoch - 177us/sample
Epoch 68/90
84077/84077 - 15s - loss: 2.7084e-04 - val_loss: 2.5011e-04 - 15s/epoch - 175us/sample
Epoch 69/90
84077/84077 - 15s - loss: 2.7043e-04 - val_loss: 2.5034e-04 - 15s/epoch - 175us/sample
Epoch 70/90
84077/84077 - 15s - loss: 2.7045e-04 - val_loss: 2.5060e-04 - 15s/epoch - 176us/sample
Epoch 71/90
84077/84077 - 15s - loss: 2.6992e-04 - val_loss: 2.5118e-04 - 15s/epoch - 175us/sample
Epoch 72/90
84077/84077 - 15s - loss: 2.7041e-04 - val_loss: 2.5024e-04 - 15s/epoch - 175us/sample
Epoch 73/90
84077/84077 - 15s - loss: 2.6961e-04 - val_loss: 2.5025e-04 - 15s/epoch - 176us/sample
Epoch 74/90
84077/84077 - 15s - loss: 2.6987e-04 - val_loss: 2.4928e-04 - 15s/epoch - 176us/sample
Epoch 75/90
84077/84077 - 15s - loss: 2.6954e-04 - val_loss: 2.4985e-04 - 15s/epoch - 175us/sample
Epoch 76/90
84077/84077 - 15s - loss: 2.6954e-04 - val_loss: 2.5064e-04 - 15s/epoch - 175us/sample
Epoch 77/90
84077/84077 - 15s - loss: 2.6939e-04 - val_loss: 2.4902e-04 - 15s/epoch - 176us/sample
Epoch 78/90
84077/84077 - 15s - loss: 2.6918e-04 - val_loss: 2.4991e-04 - 15s/epoch - 175us/sample
Epoch 79/90
84077/84077 - 15s - loss: 2.6951e-04 - val_loss: 2.4910e-04 - 15s/epoch - 175us/sample
Epoch 80/90
84077/84077 - 15s - loss: 2.6896e-04 - val_loss: 2.4894e-04 - 15s/epoch - 177us/sample
Epoch 81/90
84077/84077 - 15s - loss: 2.6909e-04 - val_loss: 2.4963e-04 - 15s/epoch - 175us/sample
Epoch 82/90
84077/84077 - 15s - loss: 2.6917e-04 - val_loss: 2.4921e-04 - 15s/epoch - 175us/sample
Epoch 83/90
84077/84077 - 15s - loss: 2.6890e-04 - val_loss: 2.4940e-04 - 15s/epoch - 176us/sample
Epoch 84/90
84077/84077 - 15s - loss: 2.6890e-04 - val_loss: 2.4904e-04 - 15s/epoch - 177us/sample
Epoch 85/90
84077/84077 - 15s - loss: 2.6892e-04 - val_loss: 2.4901e-04 - 15s/epoch - 175us/sample
Epoch 86/90
84077/84077 - 15s - loss: 2.6847e-04 - val_loss: 2.4836e-04 - 15s/epoch - 175us/sample
Epoch 87/90
84077/84077 - 15s - loss: 2.6841e-04 - val_loss: 2.4847e-04 - 15s/epoch - 176us/sample
Epoch 88/90
84077/84077 - 15s - loss: 2.6813e-04 - val_loss: 2.4721e-04 - 15s/epoch - 177us/sample
Epoch 89/90
84077/84077 - 15s - loss: 2.6807e-04 - val_loss: 2.4861e-04 - 15s/epoch - 175us/sample
Epoch 90/90
84077/84077 - 15s - loss: 2.6795e-04 - val_loss: 2.4856e-04 - 15s/epoch - 175us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00024855678372683846
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 15:09:23.072243: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_24/outputlayer/BiasAdd' id:26759 op device:{requested: '', assigned: ''} def:{{{node decoder_model_24/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_24/outputlayer/MatMul, decoder_model_24/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.11149696045013355
cosine 0.11003260642860259
MAE: 0.0036389624411424983
RMSE: 0.019903025735619646
r2: 0.6905448635626883
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 32, 90, 0.0006000000000000001, 0.2, 188, 0.00026795349046120824, 0.00024855678372683846, 0.11149696045013355, 0.11003260642860259, 0.0036389624411424983, 0.019903025735619646, 0.6905448635626883, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 80 0.0009999999999999998 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_63 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_63 (ReLU)                (None, 1886)         0           ['batch_normalization_63[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 15:09:36.528685: W tensorflow/c/c_api.cc:291] Operation '{name:'training_34/Adam/outputlayer_25/bias/v/Assign' id:28777 op device:{requested: '', assigned: ''} def:{{{node training_34/Adam/outputlayer_25/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_34/Adam/outputlayer_25/bias/v, training_34/Adam/outputlayer_25/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 15:09:48.273519: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_25/mul' id:28080 op device:{requested: '', assigned: ''} def:{{{node loss_25/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul/x, loss_25/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.0250 - val_loss: 0.0011 - 16s/epoch - 188us/sample
Epoch 2/80
84077/84077 - 10s - loss: 0.0019 - val_loss: 7.0806e-04 - 10s/epoch - 116us/sample
Epoch 3/80
84077/84077 - 10s - loss: 7.5682e-04 - val_loss: 6.2620e-04 - 10s/epoch - 116us/sample
Epoch 4/80
84077/84077 - 10s - loss: 7.8873e-04 - val_loss: 5.5709e-04 - 10s/epoch - 116us/sample
Epoch 5/80
84077/84077 - 10s - loss: 6.1338e-04 - val_loss: 0.0017 - 10s/epoch - 116us/sample
Epoch 6/80
84077/84077 - 10s - loss: 5.3772e-04 - val_loss: 4.7799e-04 - 10s/epoch - 116us/sample
Epoch 7/80
84077/84077 - 10s - loss: 4.6507e-04 - val_loss: 0.0024 - 10s/epoch - 116us/sample
Epoch 8/80
84077/84077 - 10s - loss: 3.9333e-04 - val_loss: 3.4053e-04 - 10s/epoch - 117us/sample
Epoch 9/80
84077/84077 - 10s - loss: 3.3766e-04 - val_loss: 2.9843e-04 - 10s/epoch - 116us/sample
Epoch 10/80
84077/84077 - 10s - loss: 3.0772e-04 - val_loss: 2.6208e-04 - 10s/epoch - 116us/sample
Epoch 11/80
84077/84077 - 10s - loss: 3.0080e-04 - val_loss: 4.0467e-04 - 10s/epoch - 116us/sample
Epoch 12/80
84077/84077 - 10s - loss: 2.7976e-04 - val_loss: 2.3316e-04 - 10s/epoch - 116us/sample
Epoch 13/80
84077/84077 - 10s - loss: 2.5151e-04 - val_loss: 2.2212e-04 - 10s/epoch - 117us/sample
Epoch 14/80
84077/84077 - 10s - loss: 2.4109e-04 - val_loss: 2.1833e-04 - 10s/epoch - 116us/sample
Epoch 15/80
84077/84077 - 10s - loss: 2.2882e-04 - val_loss: 2.0267e-04 - 10s/epoch - 116us/sample
Epoch 16/80
84077/84077 - 10s - loss: 2.1701e-04 - val_loss: 1.9110e-04 - 10s/epoch - 116us/sample
Epoch 17/80
84077/84077 - 10s - loss: 2.0826e-04 - val_loss: 1.8292e-04 - 10s/epoch - 116us/sample
Epoch 18/80
84077/84077 - 10s - loss: 2.0009e-04 - val_loss: 1.7480e-04 - 10s/epoch - 116us/sample
Epoch 19/80
84077/84077 - 10s - loss: 1.9381e-04 - val_loss: 1.7119e-04 - 10s/epoch - 117us/sample
Epoch 20/80
84077/84077 - 10s - loss: 1.8840e-04 - val_loss: 1.6495e-04 - 10s/epoch - 116us/sample
Epoch 21/80
84077/84077 - 10s - loss: 1.8307e-04 - val_loss: 1.6032e-04 - 10s/epoch - 116us/sample
Epoch 22/80
84077/84077 - 10s - loss: 1.7968e-04 - val_loss: 1.5874e-04 - 10s/epoch - 116us/sample
Epoch 23/80
84077/84077 - 10s - loss: 1.7589e-04 - val_loss: 1.5725e-04 - 10s/epoch - 116us/sample
Epoch 24/80
84077/84077 - 10s - loss: 1.7289e-04 - val_loss: 1.5676e-04 - 10s/epoch - 117us/sample
Epoch 25/80
84077/84077 - 10s - loss: 1.7044e-04 - val_loss: 1.5411e-04 - 10s/epoch - 116us/sample
Epoch 26/80
84077/84077 - 10s - loss: 1.6824e-04 - val_loss: 1.5175e-04 - 10s/epoch - 116us/sample
Epoch 27/80
84077/84077 - 10s - loss: 1.6631e-04 - val_loss: 1.4838e-04 - 10s/epoch - 116us/sample
Epoch 28/80
84077/84077 - 10s - loss: 1.6493e-04 - val_loss: 1.4861e-04 - 10s/epoch - 116us/sample
Epoch 29/80
84077/84077 - 10s - loss: 1.6352e-04 - val_loss: 1.4944e-04 - 10s/epoch - 116us/sample
Epoch 30/80
84077/84077 - 10s - loss: 1.6156e-04 - val_loss: 1.4657e-04 - 10s/epoch - 117us/sample
Epoch 31/80
84077/84077 - 10s - loss: 1.6041e-04 - val_loss: 1.4659e-04 - 10s/epoch - 116us/sample
Epoch 32/80
84077/84077 - 10s - loss: 1.5936e-04 - val_loss: 1.4407e-04 - 10s/epoch - 116us/sample
Epoch 33/80
84077/84077 - 10s - loss: 1.5828e-04 - val_loss: 1.4423e-04 - 10s/epoch - 116us/sample
Epoch 34/80
84077/84077 - 10s - loss: 1.5704e-04 - val_loss: 1.4200e-04 - 10s/epoch - 116us/sample
Epoch 35/80
84077/84077 - 10s - loss: 1.5552e-04 - val_loss: 1.4162e-04 - 10s/epoch - 116us/sample
Epoch 36/80
84077/84077 - 10s - loss: 1.5463e-04 - val_loss: 1.4385e-04 - 10s/epoch - 117us/sample
Epoch 37/80
84077/84077 - 10s - loss: 1.5422e-04 - val_loss: 1.4153e-04 - 10s/epoch - 116us/sample
Epoch 38/80
84077/84077 - 10s - loss: 1.5261e-04 - val_loss: 1.4003e-04 - 10s/epoch - 116us/sample
Epoch 39/80
84077/84077 - 10s - loss: 1.5223e-04 - val_loss: 1.4134e-04 - 10s/epoch - 116us/sample
Epoch 40/80
84077/84077 - 10s - loss: 1.5168e-04 - val_loss: 1.3863e-04 - 10s/epoch - 116us/sample
Epoch 41/80
84077/84077 - 10s - loss: 1.5126e-04 - val_loss: 1.4118e-04 - 10s/epoch - 116us/sample
Epoch 42/80
84077/84077 - 10s - loss: 1.4984e-04 - val_loss: 1.3764e-04 - 10s/epoch - 117us/sample
Epoch 43/80
84077/84077 - 10s - loss: 1.4954e-04 - val_loss: 1.3659e-04 - 10s/epoch - 116us/sample
Epoch 44/80
84077/84077 - 10s - loss: 1.4907e-04 - val_loss: 1.3726e-04 - 10s/epoch - 116us/sample
Epoch 45/80
84077/84077 - 10s - loss: 1.4770e-04 - val_loss: 1.3640e-04 - 10s/epoch - 116us/sample
Epoch 46/80
84077/84077 - 10s - loss: 1.4756e-04 - val_loss: 1.3856e-04 - 10s/epoch - 116us/sample
Epoch 47/80
84077/84077 - 10s - loss: 1.4661e-04 - val_loss: 1.3584e-04 - 10s/epoch - 117us/sample
Epoch 48/80
84077/84077 - 10s - loss: 1.4650e-04 - val_loss: 1.3611e-04 - 10s/epoch - 116us/sample
Epoch 49/80
84077/84077 - 10s - loss: 1.4586e-04 - val_loss: 1.3391e-04 - 10s/epoch - 116us/sample
Epoch 50/80
84077/84077 - 10s - loss: 1.4602e-04 - val_loss: 1.3468e-04 - 10s/epoch - 116us/sample
Epoch 51/80
84077/84077 - 10s - loss: 1.4491e-04 - val_loss: 1.3409e-04 - 10s/epoch - 116us/sample
Epoch 52/80
84077/84077 - 10s - loss: 1.4453e-04 - val_loss: 1.3384e-04 - 10s/epoch - 116us/sample
Epoch 53/80
84077/84077 - 10s - loss: 1.4465e-04 - val_loss: 1.3471e-04 - 10s/epoch - 117us/sample
Epoch 54/80
84077/84077 - 10s - loss: 1.4340e-04 - val_loss: 1.3449e-04 - 10s/epoch - 116us/sample
Epoch 55/80
84077/84077 - 10s - loss: 1.4370e-04 - val_loss: 1.3322e-04 - 10s/epoch - 116us/sample
Epoch 56/80
84077/84077 - 10s - loss: 1.4310e-04 - val_loss: 1.3359e-04 - 10s/epoch - 115us/sample
Epoch 57/80
84077/84077 - 10s - loss: 1.4259e-04 - val_loss: 1.3196e-04 - 10s/epoch - 116us/sample
Epoch 58/80
84077/84077 - 10s - loss: 1.4261e-04 - val_loss: 1.3180e-04 - 10s/epoch - 117us/sample
Epoch 59/80
84077/84077 - 10s - loss: 1.4173e-04 - val_loss: 1.3160e-04 - 10s/epoch - 116us/sample
Epoch 60/80
84077/84077 - 10s - loss: 1.4162e-04 - val_loss: 1.3127e-04 - 10s/epoch - 116us/sample
Epoch 61/80
84077/84077 - 10s - loss: 1.4122e-04 - val_loss: 1.3159e-04 - 10s/epoch - 116us/sample
Epoch 62/80
84077/84077 - 10s - loss: 1.4083e-04 - val_loss: 1.3152e-04 - 10s/epoch - 116us/sample
Epoch 63/80
84077/84077 - 10s - loss: 1.4044e-04 - val_loss: 1.3066e-04 - 10s/epoch - 116us/sample
Epoch 64/80
84077/84077 - 10s - loss: 1.4060e-04 - val_loss: 1.3040e-04 - 10s/epoch - 117us/sample
Epoch 65/80
84077/84077 - 10s - loss: 1.4023e-04 - val_loss: 1.3133e-04 - 10s/epoch - 116us/sample
Epoch 66/80
84077/84077 - 10s - loss: 1.3974e-04 - val_loss: 1.3072e-04 - 10s/epoch - 116us/sample
Epoch 67/80
84077/84077 - 10s - loss: 1.3955e-04 - val_loss: 1.3123e-04 - 10s/epoch - 116us/sample
Epoch 68/80
84077/84077 - 10s - loss: 1.3945e-04 - val_loss: 1.3031e-04 - 10s/epoch - 116us/sample
Epoch 69/80
84077/84077 - 10s - loss: 1.3919e-04 - val_loss: 1.3220e-04 - 10s/epoch - 117us/sample
Epoch 70/80
84077/84077 - 10s - loss: 1.3961e-04 - val_loss: 1.2925e-04 - 10s/epoch - 116us/sample
Epoch 71/80
84077/84077 - 10s - loss: 1.3880e-04 - val_loss: 1.2933e-04 - 10s/epoch - 116us/sample
Epoch 72/80
84077/84077 - 10s - loss: 1.3813e-04 - val_loss: 1.2849e-04 - 10s/epoch - 116us/sample
Epoch 73/80
84077/84077 - 10s - loss: 1.3816e-04 - val_loss: 1.3002e-04 - 10s/epoch - 116us/sample
Epoch 74/80
84077/84077 - 10s - loss: 1.3816e-04 - val_loss: 1.2978e-04 - 10s/epoch - 116us/sample
Epoch 75/80
84077/84077 - 10s - loss: 1.3757e-04 - val_loss: 1.2959e-04 - 10s/epoch - 117us/sample
Epoch 76/80
84077/84077 - 10s - loss: 1.3781e-04 - val_loss: 1.2992e-04 - 10s/epoch - 116us/sample
Epoch 77/80
84077/84077 - 10s - loss: 1.3775e-04 - val_loss: 1.2897e-04 - 10s/epoch - 116us/sample
Epoch 78/80
84077/84077 - 10s - loss: 1.3753e-04 - val_loss: 1.2986e-04 - 10s/epoch - 116us/sample
Epoch 79/80
84077/84077 - 10s - loss: 1.3687e-04 - val_loss: 1.2844e-04 - 10s/epoch - 116us/sample
Epoch 80/80
84077/84077 - 10s - loss: 1.3664e-04 - val_loss: 1.2878e-04 - 10s/epoch - 116us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012877874342020657
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 15:22:41.727266: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_25/outputlayer/BiasAdd' id:28044 op device:{requested: '', assigned: ''} def:{{{node decoder_model_25/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_25/outputlayer/MatMul, decoder_model_25/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03128523950047892
cosine 0.030903710288303438
MAE: 0.002377433031431879
RMSE: 0.010259402550223172
r2: 0.9179800143638029
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 80, 0.0009999999999999998, 0.2, 188, 0.00013664123290661992, 0.00012877874342020657, 0.03128523950047892, 0.030903710288303438, 0.002377433031431879, 0.010259402550223172, 0.9179800143638029, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0008 32 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_66 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_66 (ReLU)                (None, 1980)         0           ['batch_normalization_66[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.1_cr0.2_bs32_ep90_loss_logcosh_lr0.0008_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 15:22:55.773245: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_27/kernel/Assign' id:29657 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_27/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_27/kernel, bottleneck_zmean_27/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 15:23:00.412945: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_27/kernel/v/Assign' id:30159 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_27/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_27/kernel/v, dense_dec0_27/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 15:23:04.998536: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_27/outputlayer/BiasAdd' id:29783 op device:{requested: '', assigned: ''} def:{{{node decoder_model_27/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_27/outputlayer/MatMul, decoder_model_27/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.06167852768588633
cosine 0.06092414304827725
MAE: 0.0029038017496187396
RMSE: 0.015135664801121412
r2: 0.8210606710649985
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.1custom_VAE', 'logcosh', 32, 90, 0.0008, 0.2, 188, '--', '--', 0.06167852768588633, 0.06092414304827725, 0.0029038017496187396, 0.015135664801121412, 0.8210606710649985, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0008 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_69 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_69 (ReLU)                (None, 1791)         0           ['batch_normalization_69[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 15:23:18.628407: W tensorflow/c/c_api.cc:291] Operation '{name:'training_36/Adam/dense_dec0_28/kernel/v/Assign' id:31465 op device:{requested: '', assigned: ''} def:{{{node training_36/Adam/dense_dec0_28/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_36/Adam/dense_dec0_28/kernel/v, training_36/Adam/dense_dec0_28/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 15:23:30.757356: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_28/mul' id:30801 op device:{requested: '', assigned: ''} def:{{{node loss_28/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_28/mul/x, loss_28/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0037 - val_loss: 0.0010 - 17s/epoch - 197us/sample
Epoch 2/85
84077/84077 - 10s - loss: 8.4495e-04 - val_loss: 9.0544e-04 - 10s/epoch - 118us/sample
Epoch 3/85
84077/84077 - 10s - loss: 8.7406e-04 - val_loss: 0.9447 - 10s/epoch - 118us/sample
Epoch 4/85
84077/84077 - 10s - loss: 8.6868e-04 - val_loss: 5.7739e-04 - 10s/epoch - 117us/sample
Epoch 5/85
84077/84077 - 10s - loss: 6.0081e-04 - val_loss: 4.3725e-04 - 10s/epoch - 117us/sample
Epoch 6/85
84077/84077 - 10s - loss: 4.8318e-04 - val_loss: 4.1072e-04 - 10s/epoch - 117us/sample
Epoch 7/85
84077/84077 - 10s - loss: 4.1603e-04 - val_loss: 3.3535e-04 - 10s/epoch - 117us/sample
Epoch 8/85
84077/84077 - 10s - loss: 3.4633e-04 - val_loss: 2.9428e-04 - 10s/epoch - 117us/sample
Epoch 9/85
84077/84077 - 10s - loss: 3.1661e-04 - val_loss: 2.7633e-04 - 10s/epoch - 117us/sample
Epoch 10/85
84077/84077 - 10s - loss: 2.9406e-04 - val_loss: 2.4968e-04 - 10s/epoch - 118us/sample
Epoch 11/85
84077/84077 - 10s - loss: 2.7514e-04 - val_loss: 2.3345e-04 - 10s/epoch - 117us/sample
Epoch 12/85
84077/84077 - 10s - loss: 2.5592e-04 - val_loss: 2.2788e-04 - 10s/epoch - 117us/sample
Epoch 13/85
84077/84077 - 10s - loss: 2.4306e-04 - val_loss: 2.0568e-04 - 10s/epoch - 117us/sample
Epoch 14/85
84077/84077 - 10s - loss: 2.3052e-04 - val_loss: 1.9788e-04 - 10s/epoch - 117us/sample
Epoch 15/85
84077/84077 - 10s - loss: 2.1664e-04 - val_loss: 1.8688e-04 - 10s/epoch - 118us/sample
Epoch 16/85
84077/84077 - 10s - loss: 2.0726e-04 - val_loss: 1.8020e-04 - 10s/epoch - 117us/sample
Epoch 17/85
84077/84077 - 10s - loss: 2.0022e-04 - val_loss: 1.7283e-04 - 10s/epoch - 117us/sample
Epoch 18/85
84077/84077 - 10s - loss: 1.9146e-04 - val_loss: 1.6722e-04 - 10s/epoch - 117us/sample
Epoch 19/85
84077/84077 - 10s - loss: 1.8661e-04 - val_loss: 1.6252e-04 - 10s/epoch - 117us/sample
Epoch 20/85
84077/84077 - 10s - loss: 1.8237e-04 - val_loss: 1.5967e-04 - 10s/epoch - 118us/sample
Epoch 21/85
84077/84077 - 10s - loss: 1.7832e-04 - val_loss: 1.5738e-04 - 10s/epoch - 117us/sample
Epoch 22/85
84077/84077 - 10s - loss: 1.7423e-04 - val_loss: 1.5493e-04 - 10s/epoch - 117us/sample
Epoch 23/85
84077/84077 - 10s - loss: 1.7145e-04 - val_loss: 1.5400e-04 - 10s/epoch - 117us/sample
Epoch 24/85
84077/84077 - 10s - loss: 1.6842e-04 - val_loss: 1.4933e-04 - 10s/epoch - 117us/sample
Epoch 25/85
84077/84077 - 10s - loss: 1.6760e-04 - val_loss: 1.4926e-04 - 10s/epoch - 117us/sample
Epoch 26/85
84077/84077 - 10s - loss: 1.6442e-04 - val_loss: 1.4911e-04 - 10s/epoch - 118us/sample
Epoch 27/85
84077/84077 - 10s - loss: 1.6275e-04 - val_loss: 1.4711e-04 - 10s/epoch - 117us/sample
Epoch 28/85
84077/84077 - 10s - loss: 1.6107e-04 - val_loss: 1.4579e-04 - 10s/epoch - 117us/sample
Epoch 29/85
84077/84077 - 10s - loss: 1.5979e-04 - val_loss: 1.4593e-04 - 10s/epoch - 117us/sample
Epoch 30/85
84077/84077 - 10s - loss: 1.5874e-04 - val_loss: 1.4404e-04 - 10s/epoch - 117us/sample
Epoch 31/85
84077/84077 - 10s - loss: 1.5685e-04 - val_loss: 1.4380e-04 - 10s/epoch - 117us/sample
Epoch 32/85
84077/84077 - 10s - loss: 1.5503e-04 - val_loss: 1.4143e-04 - 10s/epoch - 118us/sample
Epoch 33/85
84077/84077 - 10s - loss: 1.5493e-04 - val_loss: 1.4109e-04 - 10s/epoch - 117us/sample
Epoch 34/85
84077/84077 - 10s - loss: 1.5337e-04 - val_loss: 1.4055e-04 - 10s/epoch - 117us/sample
Epoch 35/85
84077/84077 - 10s - loss: 1.5260e-04 - val_loss: 1.4102e-04 - 10s/epoch - 117us/sample
Epoch 36/85
84077/84077 - 10s - loss: 1.5174e-04 - val_loss: 1.3940e-04 - 10s/epoch - 117us/sample
Epoch 37/85
84077/84077 - 10s - loss: 1.5252e-04 - val_loss: 1.3849e-04 - 10s/epoch - 118us/sample
Epoch 38/85
84077/84077 - 10s - loss: 1.5012e-04 - val_loss: 1.3731e-04 - 10s/epoch - 117us/sample
Epoch 39/85
84077/84077 - 10s - loss: 1.4907e-04 - val_loss: 1.3696e-04 - 10s/epoch - 117us/sample
Epoch 40/85
84077/84077 - 10s - loss: 1.4831e-04 - val_loss: 1.3701e-04 - 10s/epoch - 117us/sample
Epoch 41/85
84077/84077 - 10s - loss: 1.4786e-04 - val_loss: 1.3580e-04 - 10s/epoch - 117us/sample
Epoch 42/85
84077/84077 - 10s - loss: 1.4734e-04 - val_loss: 1.3540e-04 - 10s/epoch - 118us/sample
Epoch 43/85
84077/84077 - 10s - loss: 1.4829e-04 - val_loss: 1.3451e-04 - 10s/epoch - 117us/sample
Epoch 44/85
84077/84077 - 10s - loss: 1.4604e-04 - val_loss: 1.3481e-04 - 10s/epoch - 117us/sample
Epoch 45/85
84077/84077 - 10s - loss: 1.4533e-04 - val_loss: 1.3462e-04 - 10s/epoch - 117us/sample
Epoch 46/85
84077/84077 - 10s - loss: 1.4494e-04 - val_loss: 1.3344e-04 - 10s/epoch - 117us/sample
Epoch 47/85
84077/84077 - 10s - loss: 1.4455e-04 - val_loss: 1.3421e-04 - 10s/epoch - 117us/sample
Epoch 48/85
84077/84077 - 10s - loss: 1.4440e-04 - val_loss: 1.3421e-04 - 10s/epoch - 118us/sample
Epoch 49/85
84077/84077 - 10s - loss: 1.4319e-04 - val_loss: 1.3420e-04 - 10s/epoch - 117us/sample
Epoch 50/85
84077/84077 - 10s - loss: 1.4282e-04 - val_loss: 1.3358e-04 - 10s/epoch - 117us/sample
Epoch 51/85
84077/84077 - 10s - loss: 1.4260e-04 - val_loss: 1.3274e-04 - 10s/epoch - 117us/sample
Epoch 52/85
84077/84077 - 10s - loss: 1.4233e-04 - val_loss: 1.3280e-04 - 10s/epoch - 117us/sample
Epoch 53/85
84077/84077 - 10s - loss: 1.4175e-04 - val_loss: 1.3110e-04 - 10s/epoch - 117us/sample
Epoch 54/85
84077/84077 - 10s - loss: 1.4152e-04 - val_loss: 1.3175e-04 - 10s/epoch - 118us/sample
Epoch 55/85
84077/84077 - 10s - loss: 1.4088e-04 - val_loss: 1.3123e-04 - 10s/epoch - 117us/sample
Epoch 56/85
84077/84077 - 10s - loss: 1.4056e-04 - val_loss: 1.3062e-04 - 10s/epoch - 117us/sample
Epoch 57/85
84077/84077 - 10s - loss: 1.4026e-04 - val_loss: 1.3086e-04 - 10s/epoch - 116us/sample
Epoch 58/85
84077/84077 - 10s - loss: 1.4029e-04 - val_loss: 1.3070e-04 - 10s/epoch - 117us/sample
Epoch 59/85
84077/84077 - 10s - loss: 1.3947e-04 - val_loss: 1.3055e-04 - 10s/epoch - 118us/sample
Epoch 60/85
84077/84077 - 10s - loss: 1.3948e-04 - val_loss: 1.2978e-04 - 10s/epoch - 118us/sample
Epoch 61/85
84077/84077 - 10s - loss: 1.4082e-04 - val_loss: 1.3086e-04 - 10s/epoch - 117us/sample
Epoch 62/85
84077/84077 - 10s - loss: 1.3916e-04 - val_loss: 1.2924e-04 - 10s/epoch - 117us/sample
Epoch 63/85
84077/84077 - 10s - loss: 1.3832e-04 - val_loss: 1.2989e-04 - 10s/epoch - 117us/sample
Epoch 64/85
84077/84077 - 10s - loss: 1.3835e-04 - val_loss: 1.3030e-04 - 10s/epoch - 118us/sample
Epoch 65/85
84077/84077 - 10s - loss: 1.3736e-04 - val_loss: 1.2816e-04 - 10s/epoch - 118us/sample
Epoch 66/85
84077/84077 - 10s - loss: 1.3766e-04 - val_loss: 1.2955e-04 - 10s/epoch - 117us/sample
Epoch 67/85
84077/84077 - 10s - loss: 1.3688e-04 - val_loss: 1.2842e-04 - 10s/epoch - 117us/sample
Epoch 68/85
84077/84077 - 10s - loss: 1.3707e-04 - val_loss: 1.2935e-04 - 10s/epoch - 117us/sample
Epoch 69/85
84077/84077 - 10s - loss: 1.3666e-04 - val_loss: 1.2821e-04 - 10s/epoch - 117us/sample
Epoch 70/85
84077/84077 - 10s - loss: 1.3648e-04 - val_loss: 1.2751e-04 - 10s/epoch - 118us/sample
Epoch 71/85
84077/84077 - 10s - loss: 1.3606e-04 - val_loss: 1.2706e-04 - 10s/epoch - 118us/sample
Epoch 72/85
84077/84077 - 10s - loss: 1.3571e-04 - val_loss: 1.2625e-04 - 10s/epoch - 117us/sample
Epoch 73/85
84077/84077 - 10s - loss: 1.3601e-04 - val_loss: 1.2762e-04 - 10s/epoch - 117us/sample
Epoch 74/85
84077/84077 - 10s - loss: 1.3576e-04 - val_loss: 1.2701e-04 - 10s/epoch - 117us/sample
Epoch 75/85
84077/84077 - 10s - loss: 1.3560e-04 - val_loss: 1.2756e-04 - 10s/epoch - 117us/sample
Epoch 76/85
84077/84077 - 10s - loss: 1.3579e-04 - val_loss: 1.2706e-04 - 10s/epoch - 118us/sample
Epoch 77/85
84077/84077 - 10s - loss: 1.3471e-04 - val_loss: 1.2652e-04 - 10s/epoch - 117us/sample
Epoch 78/85
84077/84077 - 10s - loss: 1.3442e-04 - val_loss: 1.2528e-04 - 10s/epoch - 117us/sample
Epoch 79/85
84077/84077 - 10s - loss: 1.3478e-04 - val_loss: 1.2520e-04 - 10s/epoch - 117us/sample
Epoch 80/85
84077/84077 - 10s - loss: 1.3495e-04 - val_loss: 1.2688e-04 - 10s/epoch - 117us/sample
Epoch 81/85
84077/84077 - 10s - loss: 1.3403e-04 - val_loss: 1.2644e-04 - 10s/epoch - 118us/sample
Epoch 82/85
84077/84077 - 10s - loss: 1.3404e-04 - val_loss: 1.2558e-04 - 10s/epoch - 117us/sample
Epoch 83/85
84077/84077 - 10s - loss: 1.3343e-04 - val_loss: 1.2518e-04 - 10s/epoch - 117us/sample
Epoch 84/85
84077/84077 - 10s - loss: 1.3374e-04 - val_loss: 1.2578e-04 - 10s/epoch - 117us/sample
Epoch 85/85
84077/84077 - 10s - loss: 1.3385e-04 - val_loss: 1.2556e-04 - 10s/epoch - 117us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012556480584337447
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 15:37:20.239123: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_28/outputlayer/BiasAdd' id:30765 op device:{requested: '', assigned: ''} def:{{{node decoder_model_28/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_28/outputlayer/MatMul, decoder_model_28/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03095793236105747
cosine 0.03059779141836854
MAE: 0.0022997946370088276
RMSE: 0.00993650569392425
r2: 0.92303124516328
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 85, 0.0008, 0.2, 188, 0.00013384540968835963, 0.00012556480584337447, 0.03095793236105747, 0.03059779141836854, 0.0022997946370088276, 0.00993650569392425, 0.92303124516328, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0008 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_72 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_72 (ReLU)                (None, 1791)         0           ['batch_normalization_72[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 15:37:34.902010: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_29/kernel/Assign' id:31646 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_29/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_29/kernel, dense_enc0_29/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 15:37:51.910968: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_29/mul' id:32079 op device:{requested: '', assigned: ''} def:{{{node loss_29/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_29/mul/x, loss_29/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0066 - val_loss: 0.0236 - 22s/epoch - 260us/sample
Epoch 2/90
84077/84077 - 15s - loss: 0.0059 - val_loss: 0.0011 - 15s/epoch - 175us/sample
Epoch 3/90
84077/84077 - 15s - loss: 0.0012 - val_loss: 9.6087e-04 - 15s/epoch - 175us/sample
Epoch 4/90
84077/84077 - 15s - loss: 0.0050 - val_loss: 7.7783e-04 - 15s/epoch - 176us/sample
Epoch 5/90
84077/84077 - 15s - loss: 7.9221e-04 - val_loss: 6.2622e-04 - 15s/epoch - 175us/sample
Epoch 6/90
84077/84077 - 15s - loss: 7.5185e-04 - val_loss: 5.7399e-04 - 15s/epoch - 175us/sample
Epoch 7/90
84077/84077 - 15s - loss: 5.8153e-04 - val_loss: 4.8710e-04 - 15s/epoch - 175us/sample
Epoch 8/90
84077/84077 - 15s - loss: 5.9497e-04 - val_loss: 4.8549e-04 - 15s/epoch - 177us/sample
Epoch 9/90
84077/84077 - 15s - loss: 5.1256e-04 - val_loss: 4.1719e-04 - 15s/epoch - 175us/sample
Epoch 10/90
84077/84077 - 15s - loss: 4.7128e-04 - val_loss: 3.9926e-04 - 15s/epoch - 175us/sample
Epoch 11/90
84077/84077 - 15s - loss: 4.2454e-04 - val_loss: 3.6803e-04 - 15s/epoch - 177us/sample
Epoch 12/90
84077/84077 - 15s - loss: 3.9532e-04 - val_loss: 3.5576e-04 - 15s/epoch - 175us/sample
Epoch 13/90
84077/84077 - 15s - loss: 3.7274e-04 - val_loss: 3.3164e-04 - 15s/epoch - 175us/sample
Epoch 14/90
84077/84077 - 15s - loss: 3.5638e-04 - val_loss: 3.2610e-04 - 15s/epoch - 175us/sample
Epoch 15/90
84077/84077 - 15s - loss: 3.4286e-04 - val_loss: 3.1683e-04 - 15s/epoch - 176us/sample
Epoch 16/90
84077/84077 - 15s - loss: 3.3219e-04 - val_loss: 3.0714e-04 - 15s/epoch - 176us/sample
Epoch 17/90
84077/84077 - 15s - loss: 3.2257e-04 - val_loss: 3.0087e-04 - 15s/epoch - 175us/sample
Epoch 18/90
84077/84077 - 15s - loss: 3.1638e-04 - val_loss: 2.9932e-04 - 15s/epoch - 175us/sample
Epoch 19/90
84077/84077 - 15s - loss: 3.0890e-04 - val_loss: 2.8438e-04 - 15s/epoch - 175us/sample
Epoch 20/90
84077/84077 - 15s - loss: 3.0281e-04 - val_loss: 2.9083e-04 - 15s/epoch - 176us/sample
Epoch 21/90
84077/84077 - 15s - loss: 2.9696e-04 - val_loss: 2.8221e-04 - 15s/epoch - 175us/sample
Epoch 22/90
84077/84077 - 15s - loss: 2.9234e-04 - val_loss: 2.7727e-04 - 15s/epoch - 175us/sample
Epoch 23/90
84077/84077 - 15s - loss: 2.8809e-04 - val_loss: 2.7589e-04 - 15s/epoch - 177us/sample
Epoch 24/90
84077/84077 - 15s - loss: 2.8528e-04 - val_loss: 2.7538e-04 - 15s/epoch - 175us/sample
Epoch 25/90
84077/84077 - 15s - loss: 2.8070e-04 - val_loss: 2.6775e-04 - 15s/epoch - 175us/sample
Epoch 26/90
84077/84077 - 15s - loss: 2.7844e-04 - val_loss: 2.6672e-04 - 15s/epoch - 176us/sample
Epoch 27/90
84077/84077 - 15s - loss: 2.7651e-04 - val_loss: 2.6468e-04 - 15s/epoch - 176us/sample
Epoch 28/90
84077/84077 - 15s - loss: 2.7456e-04 - val_loss: 2.6248e-04 - 15s/epoch - 175us/sample
Epoch 29/90
84077/84077 - 15s - loss: 2.7052e-04 - val_loss: 2.5790e-04 - 15s/epoch - 175us/sample
Epoch 30/90
84077/84077 - 15s - loss: 2.6974e-04 - val_loss: 2.5491e-04 - 15s/epoch - 175us/sample
Epoch 31/90
84077/84077 - 15s - loss: 2.6712e-04 - val_loss: 2.5242e-04 - 15s/epoch - 177us/sample
Epoch 32/90
84077/84077 - 15s - loss: 2.6417e-04 - val_loss: 2.5670e-04 - 15s/epoch - 175us/sample
Epoch 33/90
84077/84077 - 15s - loss: 2.6340e-04 - val_loss: 2.5046e-04 - 15s/epoch - 175us/sample
Epoch 34/90
84077/84077 - 15s - loss: 2.6158e-04 - val_loss: 2.5192e-04 - 15s/epoch - 175us/sample
Epoch 35/90
84077/84077 - 15s - loss: 2.5938e-04 - val_loss: 2.5370e-04 - 15s/epoch - 176us/sample
Epoch 36/90
84077/84077 - 15s - loss: 2.5868e-04 - val_loss: 2.4715e-04 - 15s/epoch - 175us/sample
Epoch 37/90
84077/84077 - 15s - loss: 2.5730e-04 - val_loss: 2.4214e-04 - 15s/epoch - 175us/sample
Epoch 38/90
84077/84077 - 15s - loss: 2.5526e-04 - val_loss: 2.4885e-04 - 15s/epoch - 175us/sample
Epoch 39/90
84077/84077 - 15s - loss: 2.5328e-04 - val_loss: 2.4379e-04 - 15s/epoch - 175us/sample
Epoch 40/90
84077/84077 - 15s - loss: 2.5326e-04 - val_loss: 2.4892e-04 - 15s/epoch - 177us/sample
Epoch 41/90
84077/84077 - 15s - loss: 2.5092e-04 - val_loss: 2.4092e-04 - 15s/epoch - 175us/sample
Epoch 42/90
84077/84077 - 15s - loss: 2.5036e-04 - val_loss: 2.4409e-04 - 15s/epoch - 175us/sample
Epoch 43/90
84077/84077 - 15s - loss: 2.4887e-04 - val_loss: 2.4519e-04 - 15s/epoch - 175us/sample
Epoch 44/90
84077/84077 - 15s - loss: 2.4837e-04 - val_loss: 2.3713e-04 - 15s/epoch - 177us/sample
Epoch 45/90
84077/84077 - 15s - loss: 2.4642e-04 - val_loss: 2.4009e-04 - 15s/epoch - 175us/sample
Epoch 46/90
84077/84077 - 15s - loss: 2.4586e-04 - val_loss: 2.3836e-04 - 15s/epoch - 175us/sample
Epoch 47/90
84077/84077 - 15s - loss: 2.4478e-04 - val_loss: 2.3838e-04 - 15s/epoch - 176us/sample
Epoch 48/90
84077/84077 - 15s - loss: 2.4391e-04 - val_loss: 2.3656e-04 - 15s/epoch - 176us/sample
Epoch 49/90
84077/84077 - 15s - loss: 2.4324e-04 - val_loss: 2.4158e-04 - 15s/epoch - 175us/sample
Epoch 50/90
84077/84077 - 15s - loss: 2.4285e-04 - val_loss: 2.3562e-04 - 15s/epoch - 175us/sample
Epoch 51/90
84077/84077 - 15s - loss: 2.4227e-04 - val_loss: 2.3405e-04 - 15s/epoch - 177us/sample
Epoch 52/90
84077/84077 - 15s - loss: 2.4117e-04 - val_loss: 2.3897e-04 - 15s/epoch - 176us/sample
Epoch 53/90
84077/84077 - 15s - loss: 2.3978e-04 - val_loss: 2.3342e-04 - 15s/epoch - 175us/sample
Epoch 54/90
84077/84077 - 15s - loss: 2.3965e-04 - val_loss: 2.3486e-04 - 15s/epoch - 176us/sample
Epoch 55/90
84077/84077 - 15s - loss: 2.4056e-04 - val_loss: 2.3153e-04 - 15s/epoch - 176us/sample
Epoch 56/90
84077/84077 - 15s - loss: 2.3853e-04 - val_loss: 2.3608e-04 - 15s/epoch - 176us/sample
Epoch 57/90
84077/84077 - 16s - loss: 2.3865e-04 - val_loss: 2.3433e-04 - 16s/epoch - 192us/sample
Epoch 58/90
84077/84077 - 18s - loss: 2.3903e-04 - val_loss: 2.3054e-04 - 18s/epoch - 214us/sample
Epoch 59/90
84077/84077 - 18s - loss: 2.3735e-04 - val_loss: 2.3020e-04 - 18s/epoch - 216us/sample
Epoch 60/90
84077/84077 - 19s - loss: 2.3722e-04 - val_loss: 2.2942e-04 - 19s/epoch - 232us/sample
Epoch 61/90
84077/84077 - 22s - loss: 2.3567e-04 - val_loss: 2.2911e-04 - 22s/epoch - 256us/sample
Epoch 62/90
84077/84077 - 21s - loss: 2.3587e-04 - val_loss: 2.3167e-04 - 21s/epoch - 255us/sample
Epoch 63/90
84077/84077 - 22s - loss: 2.3541e-04 - val_loss: 2.2860e-04 - 22s/epoch - 257us/sample
Epoch 64/90
84077/84077 - 22s - loss: 2.3532e-04 - val_loss: 2.3106e-04 - 22s/epoch - 256us/sample
Epoch 65/90
84077/84077 - 21s - loss: 2.3416e-04 - val_loss: 2.3052e-04 - 21s/epoch - 249us/sample
Epoch 66/90
84077/84077 - 22s - loss: 2.3418e-04 - val_loss: 2.2952e-04 - 22s/epoch - 261us/sample
Epoch 67/90
84077/84077 - 21s - loss: 2.3433e-04 - val_loss: 2.2776e-04 - 21s/epoch - 252us/sample
Epoch 68/90
84077/84077 - 22s - loss: 2.3341e-04 - val_loss: 2.2651e-04 - 22s/epoch - 257us/sample
Epoch 69/90
84077/84077 - 21s - loss: 2.3226e-04 - val_loss: 2.3054e-04 - 21s/epoch - 255us/sample
Epoch 70/90
84077/84077 - 21s - loss: 2.3155e-04 - val_loss: 2.2842e-04 - 21s/epoch - 253us/sample
Epoch 71/90
84077/84077 - 22s - loss: 2.3153e-04 - val_loss: 2.2602e-04 - 22s/epoch - 259us/sample
Epoch 72/90
84077/84077 - 21s - loss: 2.3304e-04 - val_loss: 2.2889e-04 - 21s/epoch - 252us/sample
Epoch 73/90
84077/84077 - 22s - loss: 2.3139e-04 - val_loss: 2.2769e-04 - 22s/epoch - 261us/sample
Epoch 74/90
84077/84077 - 21s - loss: 2.3042e-04 - val_loss: 2.2971e-04 - 21s/epoch - 251us/sample
Epoch 75/90
84077/84077 - 22s - loss: 2.3021e-04 - val_loss: 2.2616e-04 - 22s/epoch - 259us/sample
Epoch 76/90
84077/84077 - 21s - loss: 2.3007e-04 - val_loss: 2.2691e-04 - 21s/epoch - 254us/sample
Epoch 77/90
84077/84077 - 21s - loss: 2.2958e-04 - val_loss: 2.2683e-04 - 21s/epoch - 251us/sample
Epoch 78/90
84077/84077 - 22s - loss: 2.3052e-04 - val_loss: 2.2649e-04 - 22s/epoch - 263us/sample
Epoch 79/90
84077/84077 - 21s - loss: 2.2893e-04 - val_loss: 2.2658e-04 - 21s/epoch - 249us/sample
Epoch 80/90
84077/84077 - 22s - loss: 2.2896e-04 - val_loss: 2.2939e-04 - 22s/epoch - 258us/sample
Epoch 81/90
84077/84077 - 21s - loss: 2.2796e-04 - val_loss: 2.2551e-04 - 21s/epoch - 255us/sample
Epoch 82/90
84077/84077 - 21s - loss: 2.2838e-04 - val_loss: 2.2429e-04 - 21s/epoch - 255us/sample
Epoch 83/90
84077/84077 - 22s - loss: 2.2807e-04 - val_loss: 2.2406e-04 - 22s/epoch - 258us/sample
Epoch 84/90
84077/84077 - 21s - loss: 2.2766e-04 - val_loss: 2.2207e-04 - 21s/epoch - 251us/sample
Epoch 85/90
84077/84077 - 22s - loss: 2.2649e-04 - val_loss: 2.2587e-04 - 22s/epoch - 261us/sample
Epoch 86/90
84077/84077 - 21s - loss: 2.2751e-04 - val_loss: 2.2127e-04 - 21s/epoch - 252us/sample
Epoch 87/90
84077/84077 - 22s - loss: 2.2677e-04 - val_loss: 2.2316e-04 - 22s/epoch - 257us/sample
Epoch 88/90
84077/84077 - 21s - loss: 2.2600e-04 - val_loss: 2.2226e-04 - 21s/epoch - 253us/sample
Epoch 89/90
84077/84077 - 21s - loss: 2.2563e-04 - val_loss: 2.2578e-04 - 21s/epoch - 253us/sample
Epoch 90/90
84077/84077 - 22s - loss: 2.2581e-04 - val_loss: 2.1925e-04 - 22s/epoch - 261us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00021925418002729719
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 16:03:22.172025: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_29/outputlayer/BiasAdd' id:32050 op device:{requested: '', assigned: ''} def:{{{node decoder_model_29/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_29/outputlayer/MatMul, decoder_model_29/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.027162415494209797
cosine 0.02683508409958026
MAE: 0.002172531050230972
RMSE: 0.010688882446658686
r2: 0.9108211751391592
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 32, 90, 0.0008, 0.2, 188, 0.00022580563983383257, 0.00021925418002729719, 0.027162415494209797, 0.02683508409958026, 0.002172531050230972, 0.010688882446658686, 0.9108211751391592, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 80 0.0012 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_75 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_75 (ReLU)                (None, 2074)         0           ['batch_normalization_75[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 16:03:38.773965: W tensorflow/c/c_api.cc:291] Operation '{name:'training_40/Adam/dense_enc0_30/kernel/m/Assign' id:33817 op device:{requested: '', assigned: ''} def:{{{node training_40/Adam/dense_enc0_30/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_40/Adam/dense_enc0_30/kernel/m, training_40/Adam/dense_enc0_30/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 16:03:54.964131: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_30/mul' id:33341 op device:{requested: '', assigned: ''} def:{{{node loss_30/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_30/mul/x, loss_30/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0037 - val_loss: 0.0014 - 22s/epoch - 258us/sample
Epoch 2/80
84077/84077 - 13s - loss: 0.0065 - val_loss: 7.4876e-04 - 13s/epoch - 154us/sample
Epoch 3/80
84077/84077 - 13s - loss: 7.6444e-04 - val_loss: 0.0015 - 13s/epoch - 157us/sample
Epoch 4/80
84077/84077 - 13s - loss: 7.3779e-04 - val_loss: 6.4387e-04 - 13s/epoch - 158us/sample
Epoch 5/80
84077/84077 - 13s - loss: 6.3932e-04 - val_loss: 4.8607e-04 - 13s/epoch - 153us/sample
Epoch 6/80
84077/84077 - 13s - loss: 4.8250e-04 - val_loss: 3.8529e-04 - 13s/epoch - 160us/sample
Epoch 7/80
84077/84077 - 13s - loss: 3.9083e-04 - val_loss: 3.4571e-04 - 13s/epoch - 158us/sample
Epoch 8/80
84077/84077 - 13s - loss: 3.6294e-04 - val_loss: 3.1642e-04 - 13s/epoch - 154us/sample
Epoch 9/80
84077/84077 - 13s - loss: 3.3924e-04 - val_loss: 2.8842e-04 - 13s/epoch - 159us/sample
Epoch 10/80
84077/84077 - 13s - loss: 3.1577e-04 - val_loss: 2.6615e-04 - 13s/epoch - 158us/sample
Epoch 11/80
84077/84077 - 13s - loss: 2.8160e-04 - val_loss: 2.4862e-04 - 13s/epoch - 156us/sample
Epoch 12/80
84077/84077 - 13s - loss: 2.6158e-04 - val_loss: 2.2623e-04 - 13s/epoch - 158us/sample
Epoch 13/80
84077/84077 - 13s - loss: 2.4164e-04 - val_loss: 2.1229e-04 - 13s/epoch - 156us/sample
Epoch 14/80
84077/84077 - 13s - loss: 2.2731e-04 - val_loss: 1.9796e-04 - 13s/epoch - 156us/sample
Epoch 15/80
84077/84077 - 13s - loss: 2.1539e-04 - val_loss: 1.9207e-04 - 13s/epoch - 160us/sample
Epoch 16/80
84077/84077 - 13s - loss: 2.0676e-04 - val_loss: 1.8188e-04 - 13s/epoch - 153us/sample
Epoch 17/80
84077/84077 - 13s - loss: 1.9799e-04 - val_loss: 1.7333e-04 - 13s/epoch - 158us/sample
Epoch 18/80
84077/84077 - 13s - loss: 1.9226e-04 - val_loss: 1.6904e-04 - 13s/epoch - 160us/sample
Epoch 19/80
84077/84077 - 13s - loss: 1.8701e-04 - val_loss: 1.6563e-04 - 13s/epoch - 154us/sample
Epoch 20/80
84077/84077 - 13s - loss: 1.8251e-04 - val_loss: 1.6411e-04 - 13s/epoch - 159us/sample
Epoch 21/80
84077/84077 - 13s - loss: 1.7833e-04 - val_loss: 1.5937e-04 - 13s/epoch - 156us/sample
Epoch 22/80
84077/84077 - 13s - loss: 1.7506e-04 - val_loss: 1.5702e-04 - 13s/epoch - 155us/sample
Epoch 23/80
84077/84077 - 13s - loss: 1.7241e-04 - val_loss: 1.5395e-04 - 13s/epoch - 160us/sample
Epoch 24/80
84077/84077 - 13s - loss: 1.6960e-04 - val_loss: 1.5460e-04 - 13s/epoch - 156us/sample
Epoch 25/80
84077/84077 - 13s - loss: 1.6747e-04 - val_loss: 1.5002e-04 - 13s/epoch - 155us/sample
Epoch 26/80
84077/84077 - 13s - loss: 1.6528e-04 - val_loss: 1.4897e-04 - 13s/epoch - 160us/sample
Epoch 27/80
84077/84077 - 13s - loss: 1.6349e-04 - val_loss: 1.4773e-04 - 13s/epoch - 157us/sample
Epoch 28/80
84077/84077 - 13s - loss: 1.6162e-04 - val_loss: 1.4562e-04 - 13s/epoch - 155us/sample
Epoch 29/80
84077/84077 - 13s - loss: 1.6088e-04 - val_loss: 1.4593e-04 - 13s/epoch - 159us/sample
Epoch 30/80
84077/84077 - 13s - loss: 1.5911e-04 - val_loss: 1.4411e-04 - 13s/epoch - 154us/sample
Epoch 31/80
84077/84077 - 13s - loss: 1.5757e-04 - val_loss: 1.4328e-04 - 13s/epoch - 159us/sample
Epoch 32/80
84077/84077 - 13s - loss: 1.5697e-04 - val_loss: 1.4305e-04 - 13s/epoch - 159us/sample
Epoch 33/80
84077/84077 - 13s - loss: 1.5557e-04 - val_loss: 1.4174e-04 - 13s/epoch - 153us/sample
Epoch 34/80
84077/84077 - 13s - loss: 1.5449e-04 - val_loss: 1.4011e-04 - 13s/epoch - 160us/sample
Epoch 35/80
84077/84077 - 13s - loss: 1.5366e-04 - val_loss: 1.3945e-04 - 13s/epoch - 157us/sample
Epoch 36/80
84077/84077 - 13s - loss: 1.5223e-04 - val_loss: 1.3865e-04 - 13s/epoch - 155us/sample
Epoch 37/80
84077/84077 - 13s - loss: 1.5210e-04 - val_loss: 1.3904e-04 - 13s/epoch - 159us/sample
Epoch 38/80
84077/84077 - 13s - loss: 1.5118e-04 - val_loss: 1.3727e-04 - 13s/epoch - 157us/sample
Epoch 39/80
84077/84077 - 13s - loss: 1.4993e-04 - val_loss: 1.3756e-04 - 13s/epoch - 156us/sample
Epoch 40/80
84077/84077 - 13s - loss: 1.4968e-04 - val_loss: 1.3775e-04 - 13s/epoch - 159us/sample
Epoch 41/80
84077/84077 - 13s - loss: 1.4921e-04 - val_loss: 1.3652e-04 - 13s/epoch - 156us/sample
Epoch 42/80
84077/84077 - 13s - loss: 1.4825e-04 - val_loss: 1.3534e-04 - 13s/epoch - 156us/sample
Epoch 43/80
84077/84077 - 13s - loss: 1.4720e-04 - val_loss: 1.3732e-04 - 13s/epoch - 160us/sample
Epoch 44/80
84077/84077 - 13s - loss: 1.4692e-04 - val_loss: 1.3507e-04 - 13s/epoch - 153us/sample
Epoch 45/80
84077/84077 - 13s - loss: 1.4637e-04 - val_loss: 1.3398e-04 - 13s/epoch - 159us/sample
Epoch 46/80
84077/84077 - 13s - loss: 1.4627e-04 - val_loss: 1.3481e-04 - 13s/epoch - 158us/sample
Epoch 47/80
84077/84077 - 13s - loss: 1.4543e-04 - val_loss: 1.3456e-04 - 13s/epoch - 155us/sample
Epoch 48/80
84077/84077 - 13s - loss: 1.4507e-04 - val_loss: 1.3293e-04 - 13s/epoch - 159us/sample
Epoch 49/80
84077/84077 - 13s - loss: 1.4478e-04 - val_loss: 1.3294e-04 - 13s/epoch - 156us/sample
Epoch 50/80
84077/84077 - 13s - loss: 1.4397e-04 - val_loss: 1.3464e-04 - 13s/epoch - 155us/sample
Epoch 51/80
84077/84077 - 13s - loss: 1.4378e-04 - val_loss: 1.3326e-04 - 13s/epoch - 160us/sample
Epoch 52/80
84077/84077 - 13s - loss: 1.4318e-04 - val_loss: 1.3166e-04 - 13s/epoch - 157us/sample
Epoch 53/80
84077/84077 - 13s - loss: 1.4304e-04 - val_loss: 1.3128e-04 - 13s/epoch - 155us/sample
Epoch 54/80
84077/84077 - 13s - loss: 1.4340e-04 - val_loss: 1.3125e-04 - 13s/epoch - 159us/sample
Epoch 55/80
84077/84077 - 13s - loss: 1.4174e-04 - val_loss: 1.3052e-04 - 13s/epoch - 155us/sample
Epoch 56/80
84077/84077 - 13s - loss: 1.4194e-04 - val_loss: 1.3121e-04 - 13s/epoch - 159us/sample
Epoch 57/80
84077/84077 - 13s - loss: 1.4192e-04 - val_loss: 1.3322e-04 - 13s/epoch - 159us/sample
Epoch 58/80
84077/84077 - 13s - loss: 1.4127e-04 - val_loss: 1.3075e-04 - 13s/epoch - 152us/sample
Epoch 59/80
84077/84077 - 13s - loss: 1.4061e-04 - val_loss: 1.2906e-04 - 13s/epoch - 159us/sample
Epoch 60/80
84077/84077 - 13s - loss: 1.4024e-04 - val_loss: 1.3113e-04 - 13s/epoch - 159us/sample
Epoch 61/80
84077/84077 - 13s - loss: 1.4072e-04 - val_loss: 1.2983e-04 - 13s/epoch - 154us/sample
Epoch 62/80
84077/84077 - 13s - loss: 1.3980e-04 - val_loss: 1.2956e-04 - 13s/epoch - 159us/sample
Epoch 63/80
84077/84077 - 13s - loss: 1.3967e-04 - val_loss: 1.2887e-04 - 13s/epoch - 157us/sample
Epoch 64/80
84077/84077 - 13s - loss: 1.3935e-04 - val_loss: 1.2918e-04 - 13s/epoch - 157us/sample
Epoch 65/80
84077/84077 - 13s - loss: 1.3881e-04 - val_loss: 1.2938e-04 - 13s/epoch - 159us/sample
Epoch 66/80
84077/84077 - 13s - loss: 1.3872e-04 - val_loss: 1.2975e-04 - 13s/epoch - 156us/sample
Epoch 67/80
84077/84077 - 13s - loss: 1.3841e-04 - val_loss: 1.2786e-04 - 13s/epoch - 155us/sample
Epoch 68/80
84077/84077 - 14s - loss: 1.3816e-04 - val_loss: 1.2901e-04 - 14s/epoch - 161us/sample
Epoch 69/80
84077/84077 - 13s - loss: 1.3832e-04 - val_loss: 1.2955e-04 - 13s/epoch - 154us/sample
Epoch 70/80
84077/84077 - 13s - loss: 1.3826e-04 - val_loss: 1.2784e-04 - 13s/epoch - 158us/sample
Epoch 71/80
84077/84077 - 13s - loss: 1.3759e-04 - val_loss: 1.2805e-04 - 13s/epoch - 159us/sample
Epoch 72/80
84077/84077 - 13s - loss: 1.3740e-04 - val_loss: 1.2896e-04 - 13s/epoch - 155us/sample
Epoch 73/80
84077/84077 - 13s - loss: 1.3713e-04 - val_loss: 1.2910e-04 - 13s/epoch - 159us/sample
Epoch 74/80
84077/84077 - 13s - loss: 1.3680e-04 - val_loss: 1.2697e-04 - 13s/epoch - 157us/sample
Epoch 75/80
84077/84077 - 13s - loss: 1.3674e-04 - val_loss: 1.2809e-04 - 13s/epoch - 155us/sample
Epoch 76/80
84077/84077 - 13s - loss: 1.3659e-04 - val_loss: 1.2751e-04 - 13s/epoch - 160us/sample
Epoch 77/80
84077/84077 - 13s - loss: 1.3622e-04 - val_loss: 1.2670e-04 - 13s/epoch - 157us/sample
Epoch 78/80
84077/84077 - 13s - loss: 1.3669e-04 - val_loss: 1.2717e-04 - 13s/epoch - 155us/sample
Epoch 79/80
84077/84077 - 13s - loss: 1.3558e-04 - val_loss: 1.2627e-04 - 13s/epoch - 159us/sample
Epoch 80/80
84077/84077 - 13s - loss: 1.3595e-04 - val_loss: 1.2727e-04 - 13s/epoch - 158us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012727407483417212
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 16:21:21.126500: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_30/outputlayer/BiasAdd' id:33305 op device:{requested: '', assigned: ''} def:{{{node decoder_model_30/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_30/outputlayer/MatMul, decoder_model_30/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0313014535513769
cosine 0.03092356459759034
MAE: 0.002464786395598293
RMSE: 0.010133359898986383
r2: 0.9199758095590627
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 64, 80, 0.0012, 0.2, 188, 0.00013594811160109755, 0.00012727407483417212, 0.0313014535513769, 0.03092356459759034, 0.002464786395598293, 0.010133359898986383, 0.9199758095590627, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 2
Fitness    = 511.49230331698664
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 511.49230331698664.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664]
[1.9 90 0.0008 64 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_78 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_78 (ReLU)                (None, 1791)         0           ['batch_normalization_78[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 16:21:38.048682: W tensorflow/c/c_api.cc:291] Operation '{name:'training_42/Adam/dense_dec0_31/bias/v/Assign' id:35297 op device:{requested: '', assigned: ''} def:{{{node training_42/Adam/dense_dec0_31/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_42/Adam/dense_dec0_31/bias/v, training_42/Adam/dense_dec0_31/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 16:21:54.154677: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_31/mul' id:34626 op device:{requested: '', assigned: ''} def:{{{node loss_31/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_31/mul/x, loss_31/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0044 - val_loss: 0.0014 - 22s/epoch - 260us/sample
Epoch 2/90
84077/84077 - 13s - loss: 8.4027e-04 - val_loss: 7.1946e-04 - 13s/epoch - 158us/sample
Epoch 3/90
84077/84077 - 13s - loss: 0.0031 - val_loss: 7.8755e-04 - 13s/epoch - 160us/sample
Epoch 4/90
84077/84077 - 13s - loss: 7.0848e-04 - val_loss: 5.7665e-04 - 13s/epoch - 154us/sample
Epoch 5/90
84077/84077 - 13s - loss: 6.0106e-04 - val_loss: 4.9472e-04 - 13s/epoch - 159us/sample
Epoch 6/90
84077/84077 - 13s - loss: 5.2875e-04 - val_loss: 4.8854e-04 - 13s/epoch - 160us/sample
Epoch 7/90
84077/84077 - 13s - loss: 4.6533e-04 - val_loss: 4.0189e-04 - 13s/epoch - 155us/sample
Epoch 8/90
84077/84077 - 13s - loss: 3.9850e-04 - val_loss: 3.8135e-04 - 13s/epoch - 160us/sample
Epoch 9/90
84077/84077 - 13s - loss: 3.5748e-04 - val_loss: 3.2044e-04 - 13s/epoch - 157us/sample
Epoch 10/90
84077/84077 - 13s - loss: 3.3348e-04 - val_loss: 2.7676e-04 - 13s/epoch - 156us/sample
Epoch 11/90
84077/84077 - 13s - loss: 2.9672e-04 - val_loss: 2.6766e-04 - 13s/epoch - 160us/sample
Epoch 12/90
84077/84077 - 13s - loss: 2.8145e-04 - val_loss: 2.4011e-04 - 13s/epoch - 159us/sample
Epoch 13/90
84077/84077 - 13s - loss: 2.6302e-04 - val_loss: 2.2803e-04 - 13s/epoch - 156us/sample
Epoch 14/90
84077/84077 - 13s - loss: 2.4395e-04 - val_loss: 2.1048e-04 - 13s/epoch - 160us/sample
Epoch 15/90
84077/84077 - 13s - loss: 2.3266e-04 - val_loss: 1.9894e-04 - 13s/epoch - 157us/sample
Epoch 16/90
84077/84077 - 13s - loss: 2.2081e-04 - val_loss: 1.9335e-04 - 13s/epoch - 158us/sample
Epoch 17/90
84077/84077 - 13s - loss: 2.1161e-04 - val_loss: 1.8459e-04 - 13s/epoch - 160us/sample
Epoch 18/90
84077/84077 - 13s - loss: 2.0510e-04 - val_loss: 1.8432e-04 - 13s/epoch - 154us/sample
Epoch 19/90
84077/84077 - 13s - loss: 1.9750e-04 - val_loss: 1.7248e-04 - 13s/epoch - 160us/sample
Epoch 20/90
84077/84077 - 14s - loss: 1.9152e-04 - val_loss: 1.6985e-04 - 14s/epoch - 161us/sample
Epoch 21/90
84077/84077 - 13s - loss: 1.8526e-04 - val_loss: 1.6392e-04 - 13s/epoch - 154us/sample
Epoch 22/90
84077/84077 - 13s - loss: 1.8204e-04 - val_loss: 1.6081e-04 - 13s/epoch - 160us/sample
Epoch 23/90
84077/84077 - 13s - loss: 1.7917e-04 - val_loss: 1.5885e-04 - 13s/epoch - 158us/sample
Epoch 24/90
84077/84077 - 13s - loss: 1.7469e-04 - val_loss: 1.5466e-04 - 13s/epoch - 158us/sample
Epoch 25/90
84077/84077 - 13s - loss: 1.7237e-04 - val_loss: 1.5207e-04 - 13s/epoch - 160us/sample
Epoch 26/90
84077/84077 - 13s - loss: 1.6855e-04 - val_loss: 1.5230e-04 - 13s/epoch - 158us/sample
Epoch 27/90
84077/84077 - 13s - loss: 1.6803e-04 - val_loss: 1.5094e-04 - 13s/epoch - 156us/sample
Epoch 28/90
84077/84077 - 14s - loss: 1.6511e-04 - val_loss: 1.4990e-04 - 14s/epoch - 161us/sample
Epoch 29/90
84077/84077 - 13s - loss: 1.6379e-04 - val_loss: 1.4903e-04 - 13s/epoch - 155us/sample
Epoch 30/90
84077/84077 - 13s - loss: 1.6259e-04 - val_loss: 1.4591e-04 - 13s/epoch - 159us/sample
Epoch 31/90
84077/84077 - 13s - loss: 1.6070e-04 - val_loss: 1.4849e-04 - 13s/epoch - 160us/sample
Epoch 32/90
84077/84077 - 13s - loss: 1.6002e-04 - val_loss: 1.4338e-04 - 13s/epoch - 156us/sample
Epoch 33/90
84077/84077 - 14s - loss: 1.5874e-04 - val_loss: 1.4303e-04 - 14s/epoch - 161us/sample
Epoch 34/90
84077/84077 - 13s - loss: 1.5725e-04 - val_loss: 1.4184e-04 - 13s/epoch - 157us/sample
Epoch 35/90
84077/84077 - 13s - loss: 1.5611e-04 - val_loss: 1.4234e-04 - 13s/epoch - 156us/sample
Epoch 36/90
84077/84077 - 13s - loss: 1.5488e-04 - val_loss: 1.4161e-04 - 13s/epoch - 160us/sample
Epoch 37/90
84077/84077 - 13s - loss: 1.5442e-04 - val_loss: 1.3992e-04 - 13s/epoch - 160us/sample
Epoch 38/90
84077/84077 - 13s - loss: 1.5387e-04 - val_loss: 1.3929e-04 - 13s/epoch - 156us/sample
Epoch 39/90
84077/84077 - 13s - loss: 1.5245e-04 - val_loss: 1.3991e-04 - 13s/epoch - 160us/sample
Epoch 40/90
84077/84077 - 13s - loss: 1.5153e-04 - val_loss: 1.3827e-04 - 13s/epoch - 157us/sample
Epoch 41/90
84077/84077 - 13s - loss: 1.5077e-04 - val_loss: 1.3749e-04 - 13s/epoch - 158us/sample
Epoch 42/90
84077/84077 - 13s - loss: 1.5071e-04 - val_loss: 1.3782e-04 - 13s/epoch - 160us/sample
Epoch 43/90
84077/84077 - 13s - loss: 1.4990e-04 - val_loss: 1.3698e-04 - 13s/epoch - 154us/sample
Epoch 44/90
84077/84077 - 13s - loss: 1.4919e-04 - val_loss: 1.3644e-04 - 13s/epoch - 160us/sample
Epoch 45/90
84077/84077 - 13s - loss: 1.4826e-04 - val_loss: 1.3615e-04 - 13s/epoch - 160us/sample
Epoch 46/90
84077/84077 - 13s - loss: 1.4780e-04 - val_loss: 1.3654e-04 - 13s/epoch - 156us/sample
Epoch 47/90
84077/84077 - 13s - loss: 1.4717e-04 - val_loss: 1.3416e-04 - 13s/epoch - 160us/sample
Epoch 48/90
84077/84077 - 13s - loss: 1.4688e-04 - val_loss: 1.3468e-04 - 13s/epoch - 158us/sample
Epoch 49/90
84077/84077 - 13s - loss: 1.4595e-04 - val_loss: 1.3553e-04 - 13s/epoch - 156us/sample
Epoch 50/90
84077/84077 - 14s - loss: 1.4553e-04 - val_loss: 1.3397e-04 - 14s/epoch - 161us/sample
Epoch 51/90
84077/84077 - 13s - loss: 1.4526e-04 - val_loss: 1.3353e-04 - 13s/epoch - 158us/sample
Epoch 52/90
84077/84077 - 13s - loss: 1.4534e-04 - val_loss: 1.3335e-04 - 13s/epoch - 157us/sample
Epoch 53/90
84077/84077 - 13s - loss: 1.4466e-04 - val_loss: 1.3265e-04 - 13s/epoch - 160us/sample
Epoch 54/90
84077/84077 - 13s - loss: 1.4366e-04 - val_loss: 1.3396e-04 - 13s/epoch - 156us/sample
Epoch 55/90
84077/84077 - 13s - loss: 1.4335e-04 - val_loss: 1.3281e-04 - 13s/epoch - 159us/sample
Epoch 56/90
84077/84077 - 13s - loss: 1.4299e-04 - val_loss: 1.3242e-04 - 13s/epoch - 160us/sample
Epoch 57/90
84077/84077 - 13s - loss: 1.4258e-04 - val_loss: 1.3114e-04 - 13s/epoch - 153us/sample
Epoch 58/90
84077/84077 - 14s - loss: 1.4259e-04 - val_loss: 1.3345e-04 - 14s/epoch - 161us/sample
Epoch 59/90
84077/84077 - 13s - loss: 1.4210e-04 - val_loss: 1.3130e-04 - 13s/epoch - 159us/sample
Epoch 60/90
84077/84077 - 13s - loss: 1.4154e-04 - val_loss: 1.3112e-04 - 13s/epoch - 156us/sample
Epoch 61/90
84077/84077 - 13s - loss: 1.4142e-04 - val_loss: 1.3036e-04 - 13s/epoch - 160us/sample
Epoch 62/90
84077/84077 - 13s - loss: 1.4123e-04 - val_loss: 1.3308e-04 - 13s/epoch - 159us/sample
Epoch 63/90
84077/84077 - 13s - loss: 1.4105e-04 - val_loss: 1.3075e-04 - 13s/epoch - 157us/sample
Epoch 64/90
84077/84077 - 13s - loss: 1.4054e-04 - val_loss: 1.2965e-04 - 13s/epoch - 160us/sample
Epoch 65/90
84077/84077 - 13s - loss: 1.4022e-04 - val_loss: 1.2977e-04 - 13s/epoch - 157us/sample
Epoch 66/90
84077/84077 - 13s - loss: 1.3999e-04 - val_loss: 1.2895e-04 - 13s/epoch - 158us/sample
Epoch 67/90
84077/84077 - 13s - loss: 1.3960e-04 - val_loss: 1.3012e-04 - 13s/epoch - 160us/sample
Epoch 68/90
84077/84077 - 13s - loss: 1.3937e-04 - val_loss: 1.3021e-04 - 13s/epoch - 154us/sample
Epoch 69/90
84077/84077 - 13s - loss: 1.3873e-04 - val_loss: 1.2865e-04 - 13s/epoch - 160us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.3835e-04 - val_loss: 1.2877e-04 - 14s/epoch - 161us/sample
Epoch 71/90
84077/84077 - 13s - loss: 1.3844e-04 - val_loss: 1.3006e-04 - 13s/epoch - 155us/sample
Epoch 72/90
84077/84077 - 13s - loss: 1.3844e-04 - val_loss: 1.2816e-04 - 13s/epoch - 160us/sample
Epoch 73/90
84077/84077 - 13s - loss: 1.3811e-04 - val_loss: 1.2868e-04 - 13s/epoch - 158us/sample
Epoch 74/90
84077/84077 - 13s - loss: 1.3758e-04 - val_loss: 1.2796e-04 - 13s/epoch - 158us/sample
Epoch 75/90
84077/84077 - 13s - loss: 1.3751e-04 - val_loss: 1.2770e-04 - 13s/epoch - 160us/sample
Epoch 76/90
84077/84077 - 13s - loss: 1.3721e-04 - val_loss: 1.2705e-04 - 13s/epoch - 158us/sample
Epoch 77/90
84077/84077 - 13s - loss: 1.3658e-04 - val_loss: 1.2752e-04 - 13s/epoch - 156us/sample
Epoch 78/90
84077/84077 - 14s - loss: 1.3684e-04 - val_loss: 1.2708e-04 - 14s/epoch - 161us/sample
Epoch 79/90
84077/84077 - 13s - loss: 1.3636e-04 - val_loss: 1.2733e-04 - 13s/epoch - 156us/sample
Epoch 80/90
84077/84077 - 13s - loss: 1.3620e-04 - val_loss: 1.2789e-04 - 13s/epoch - 158us/sample
Epoch 81/90
84077/84077 - 13s - loss: 1.3575e-04 - val_loss: 1.2705e-04 - 13s/epoch - 160us/sample
Epoch 82/90
84077/84077 - 13s - loss: 1.3573e-04 - val_loss: 1.2639e-04 - 13s/epoch - 155us/sample
Epoch 83/90
84077/84077 - 14s - loss: 1.3567e-04 - val_loss: 1.2637e-04 - 14s/epoch - 161us/sample
Epoch 84/90
84077/84077 - 13s - loss: 1.3541e-04 - val_loss: 1.2716e-04 - 13s/epoch - 158us/sample
Epoch 85/90
84077/84077 - 13s - loss: 1.3531e-04 - val_loss: 1.2690e-04 - 13s/epoch - 155us/sample
Epoch 86/90
84077/84077 - 13s - loss: 1.3527e-04 - val_loss: 1.2690e-04 - 13s/epoch - 160us/sample
Epoch 87/90
84077/84077 - 13s - loss: 1.3512e-04 - val_loss: 1.2570e-04 - 13s/epoch - 159us/sample
Epoch 88/90
84077/84077 - 13s - loss: 1.3429e-04 - val_loss: 1.2571e-04 - 13s/epoch - 157us/sample
Epoch 89/90
84077/84077 - 13s - loss: 1.3483e-04 - val_loss: 1.2661e-04 - 13s/epoch - 160us/sample
Epoch 90/90
84077/84077 - 13s - loss: 1.3443e-04 - val_loss: 1.2747e-04 - 13s/epoch - 157us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012746943978228777
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 16:41:40.905916: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_31/outputlayer/BiasAdd' id:34590 op device:{requested: '', assigned: ''} def:{{{node decoder_model_31/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_31/outputlayer/MatMul, decoder_model_31/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03207241660315698
cosine 0.03168679753825727
MAE: 0.0023491421285275036
RMSE: 0.010179900145950683
r2: 0.9192337687584791
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 90, 0.0008, 0.2, 188, 0.00013442716317624677, 0.00012746943978228777, 0.03207241660315698, 0.03168679753825727, 0.0023491421285275036, 0.010179900145950683, 0.9192337687584791, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0006000000000000001 32 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_81 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_81 (ReLU)                (None, 1697)         0           ['batch_normalization_81[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 16:41:58.083890: W tensorflow/c/c_api.cc:291] Operation '{name:'training_44/Adam/batch_normalization_83/beta/m/Assign' id:36482 op device:{requested: '', assigned: ''} def:{{{node training_44/Adam/batch_normalization_83/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_44/Adam/batch_normalization_83/beta/m, training_44/Adam/batch_normalization_83/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 16:42:22.962793: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_32/mul' id:35911 op device:{requested: '', assigned: ''} def:{{{node loss_32/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_32/mul/x, loss_32/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 31s - loss: 0.0750 - val_loss: 0.0020 - 31s/epoch - 370us/sample
Epoch 2/90
84077/84077 - 23s - loss: 0.0011 - val_loss: 6.4381e-04 - 23s/epoch - 272us/sample
Epoch 3/90
84077/84077 - 23s - loss: 5.8811e-04 - val_loss: 5.0849e-04 - 23s/epoch - 273us/sample
Epoch 4/90
84077/84077 - 23s - loss: 4.7369e-04 - val_loss: 4.0632e-04 - 23s/epoch - 268us/sample
Epoch 5/90
84077/84077 - 23s - loss: 4.1904e-04 - val_loss: 4.1306e-04 - 23s/epoch - 277us/sample
Epoch 6/90
84077/84077 - 23s - loss: 3.8686e-04 - val_loss: 3.2628e-04 - 23s/epoch - 270us/sample
Epoch 7/90
84077/84077 - 23s - loss: 3.4567e-04 - val_loss: 3.0342e-04 - 23s/epoch - 273us/sample
Epoch 8/90
84077/84077 - 23s - loss: 3.3110e-04 - val_loss: 2.8832e-04 - 23s/epoch - 272us/sample
Epoch 9/90
84077/84077 - 22s - loss: 3.1437e-04 - val_loss: 2.7656e-04 - 22s/epoch - 267us/sample
Epoch 10/90
84077/84077 - 23s - loss: 2.9737e-04 - val_loss: 2.6897e-04 - 23s/epoch - 272us/sample
Epoch 11/90
84077/84077 - 23s - loss: 2.8697e-04 - val_loss: 2.5753e-04 - 23s/epoch - 272us/sample
Epoch 12/90
84077/84077 - 22s - loss: 2.8015e-04 - val_loss: 2.5566e-04 - 22s/epoch - 267us/sample
Epoch 13/90
84077/84077 - 23s - loss: 2.7383e-04 - val_loss: 2.5296e-04 - 23s/epoch - 277us/sample
Epoch 14/90
84077/84077 - 23s - loss: 2.6930e-04 - val_loss: 2.4831e-04 - 23s/epoch - 269us/sample
Epoch 15/90
84077/84077 - 23s - loss: 2.6523e-04 - val_loss: 2.4821e-04 - 23s/epoch - 273us/sample
Epoch 16/90
84077/84077 - 23s - loss: 2.6107e-04 - val_loss: 2.4405e-04 - 23s/epoch - 272us/sample
Epoch 17/90
84077/84077 - 23s - loss: 2.5852e-04 - val_loss: 2.4334e-04 - 23s/epoch - 268us/sample
Epoch 18/90
84077/84077 - 23s - loss: 2.5584e-04 - val_loss: 2.4024e-04 - 23s/epoch - 276us/sample
Epoch 19/90
84077/84077 - 23s - loss: 2.5383e-04 - val_loss: 2.3707e-04 - 23s/epoch - 270us/sample
Epoch 20/90
84077/84077 - 23s - loss: 2.5088e-04 - val_loss: 2.3624e-04 - 23s/epoch - 269us/sample
Epoch 21/90
84077/84077 - 23s - loss: 2.4915e-04 - val_loss: 2.3955e-04 - 23s/epoch - 274us/sample
Epoch 22/90
84077/84077 - 23s - loss: 2.4715e-04 - val_loss: 2.3287e-04 - 23s/epoch - 268us/sample
Epoch 23/90
84077/84077 - 23s - loss: 2.4603e-04 - val_loss: 2.3060e-04 - 23s/epoch - 272us/sample
Epoch 24/90
84077/84077 - 23s - loss: 2.4485e-04 - val_loss: 2.3113e-04 - 23s/epoch - 272us/sample
Epoch 25/90
84077/84077 - 23s - loss: 2.4335e-04 - val_loss: 2.3128e-04 - 23s/epoch - 268us/sample
Epoch 26/90
84077/84077 - 23s - loss: 2.4190e-04 - val_loss: 2.2757e-04 - 23s/epoch - 277us/sample
Epoch 27/90
84077/84077 - 23s - loss: 2.4026e-04 - val_loss: 2.2730e-04 - 23s/epoch - 269us/sample
Epoch 28/90
84077/84077 - 23s - loss: 2.3925e-04 - val_loss: 2.2916e-04 - 23s/epoch - 273us/sample
Epoch 29/90
84077/84077 - 23s - loss: 2.3826e-04 - val_loss: 2.2631e-04 - 23s/epoch - 271us/sample
Epoch 30/90
84077/84077 - 22s - loss: 2.3695e-04 - val_loss: 2.2618e-04 - 22s/epoch - 267us/sample
Epoch 31/90
84077/84077 - 23s - loss: 2.3608e-04 - val_loss: 2.2469e-04 - 23s/epoch - 272us/sample
Epoch 32/90
84077/84077 - 23s - loss: 2.3512e-04 - val_loss: 2.2410e-04 - 23s/epoch - 273us/sample
Epoch 33/90
84077/84077 - 22s - loss: 2.3462e-04 - val_loss: 2.2346e-04 - 22s/epoch - 267us/sample
Epoch 34/90
84077/84077 - 23s - loss: 2.3268e-04 - val_loss: 2.2029e-04 - 23s/epoch - 278us/sample
Epoch 35/90
84077/84077 - 22s - loss: 2.3212e-04 - val_loss: 2.2029e-04 - 22s/epoch - 267us/sample
Epoch 36/90
84077/84077 - 23s - loss: 2.3168e-04 - val_loss: 2.2029e-04 - 23s/epoch - 273us/sample
Epoch 37/90
84077/84077 - 23s - loss: 2.3028e-04 - val_loss: 2.2017e-04 - 23s/epoch - 272us/sample
Epoch 38/90
84077/84077 - 22s - loss: 2.2969e-04 - val_loss: 2.1937e-04 - 22s/epoch - 268us/sample
Epoch 39/90
84077/84077 - 23s - loss: 2.2858e-04 - val_loss: 2.2065e-04 - 23s/epoch - 275us/sample
Epoch 40/90
84077/84077 - 23s - loss: 2.2750e-04 - val_loss: 2.1577e-04 - 23s/epoch - 270us/sample
Epoch 41/90
84077/84077 - 23s - loss: 2.2692e-04 - val_loss: 2.2111e-04 - 23s/epoch - 268us/sample
Epoch 42/90
84077/84077 - 23s - loss: 2.2640e-04 - val_loss: 2.1949e-04 - 23s/epoch - 276us/sample
Epoch 43/90
84077/84077 - 22s - loss: 2.2595e-04 - val_loss: 2.1733e-04 - 22s/epoch - 267us/sample
Epoch 44/90
84077/84077 - 23s - loss: 2.2542e-04 - val_loss: 2.2034e-04 - 23s/epoch - 273us/sample
Epoch 45/90
84077/84077 - 23s - loss: 2.2460e-04 - val_loss: 2.1384e-04 - 23s/epoch - 273us/sample
Epoch 46/90
84077/84077 - 22s - loss: 2.2397e-04 - val_loss: 2.1309e-04 - 22s/epoch - 267us/sample
Epoch 47/90
84077/84077 - 23s - loss: 2.2357e-04 - val_loss: 2.1501e-04 - 23s/epoch - 278us/sample
Epoch 48/90
84077/84077 - 22s - loss: 2.2283e-04 - val_loss: 2.1218e-04 - 22s/epoch - 267us/sample
Epoch 49/90
84077/84077 - 23s - loss: 2.2234e-04 - val_loss: 2.1168e-04 - 23s/epoch - 273us/sample
Epoch 50/90
84077/84077 - 23s - loss: 2.2157e-04 - val_loss: 2.1249e-04 - 23s/epoch - 272us/sample
Epoch 51/90
84077/84077 - 22s - loss: 2.2123e-04 - val_loss: 2.1048e-04 - 22s/epoch - 266us/sample
Epoch 52/90
84077/84077 - 23s - loss: 2.2030e-04 - val_loss: 2.1100e-04 - 23s/epoch - 273us/sample
Epoch 53/90
84077/84077 - 23s - loss: 2.2001e-04 - val_loss: 2.1126e-04 - 23s/epoch - 272us/sample
Epoch 54/90
84077/84077 - 22s - loss: 2.1922e-04 - val_loss: 2.1091e-04 - 22s/epoch - 267us/sample
Epoch 55/90
84077/84077 - 23s - loss: 2.1878e-04 - val_loss: 2.1081e-04 - 23s/epoch - 279us/sample
Epoch 56/90
84077/84077 - 22s - loss: 2.1838e-04 - val_loss: 2.0860e-04 - 22s/epoch - 267us/sample
Epoch 57/90
84077/84077 - 23s - loss: 2.1813e-04 - val_loss: 2.0589e-04 - 23s/epoch - 274us/sample
Epoch 58/90
84077/84077 - 23s - loss: 2.1806e-04 - val_loss: 2.0867e-04 - 23s/epoch - 272us/sample
Epoch 59/90
84077/84077 - 22s - loss: 2.1728e-04 - val_loss: 2.0629e-04 - 22s/epoch - 266us/sample
Epoch 60/90
84077/84077 - 23s - loss: 2.1701e-04 - val_loss: 2.0710e-04 - 23s/epoch - 276us/sample
Epoch 61/90
84077/84077 - 23s - loss: 2.1677e-04 - val_loss: 2.0600e-04 - 23s/epoch - 269us/sample
Epoch 62/90
84077/84077 - 22s - loss: 2.1619e-04 - val_loss: 2.0608e-04 - 22s/epoch - 267us/sample
Epoch 63/90
84077/84077 - 23s - loss: 2.1572e-04 - val_loss: 2.0807e-04 - 23s/epoch - 277us/sample
Epoch 64/90
84077/84077 - 22s - loss: 2.1516e-04 - val_loss: 2.0752e-04 - 22s/epoch - 267us/sample
Epoch 65/90
84077/84077 - 23s - loss: 2.1453e-04 - val_loss: 2.0211e-04 - 23s/epoch - 274us/sample
Epoch 66/90
84077/84077 - 23s - loss: 2.1350e-04 - val_loss: 2.0397e-04 - 23s/epoch - 271us/sample
Epoch 67/90
84077/84077 - 22s - loss: 2.1294e-04 - val_loss: 2.0438e-04 - 22s/epoch - 267us/sample
Epoch 68/90
84077/84077 - 23s - loss: 2.1271e-04 - val_loss: 2.0116e-04 - 23s/epoch - 278us/sample
Epoch 69/90
84077/84077 - 22s - loss: 2.1266e-04 - val_loss: 2.0278e-04 - 22s/epoch - 267us/sample
Epoch 70/90
84077/84077 - 23s - loss: 2.1249e-04 - val_loss: 2.0466e-04 - 23s/epoch - 272us/sample
Epoch 71/90
84077/84077 - 23s - loss: 2.1188e-04 - val_loss: 2.0358e-04 - 23s/epoch - 274us/sample
Epoch 72/90
84077/84077 - 22s - loss: 2.1158e-04 - val_loss: 2.0046e-04 - 22s/epoch - 267us/sample
Epoch 73/90
84077/84077 - 23s - loss: 2.1154e-04 - val_loss: 2.0212e-04 - 23s/epoch - 274us/sample
Epoch 74/90
84077/84077 - 23s - loss: 2.1157e-04 - val_loss: 2.0092e-04 - 23s/epoch - 270us/sample
Epoch 75/90
84077/84077 - 22s - loss: 2.1127e-04 - val_loss: 2.0066e-04 - 22s/epoch - 267us/sample
Epoch 76/90
84077/84077 - 23s - loss: 2.1099e-04 - val_loss: 2.0021e-04 - 23s/epoch - 278us/sample
Epoch 77/90
84077/84077 - 22s - loss: 2.1072e-04 - val_loss: 2.0139e-04 - 22s/epoch - 267us/sample
Epoch 78/90
84077/84077 - 23s - loss: 2.1063e-04 - val_loss: 1.9962e-04 - 23s/epoch - 274us/sample
Epoch 79/90
84077/84077 - 23s - loss: 2.1037e-04 - val_loss: 2.0155e-04 - 23s/epoch - 272us/sample
Epoch 80/90
84077/84077 - 22s - loss: 2.0999e-04 - val_loss: 1.9599e-04 - 22s/epoch - 267us/sample
Epoch 81/90
84077/84077 - 23s - loss: 2.0964e-04 - val_loss: 1.9909e-04 - 23s/epoch - 276us/sample
Epoch 82/90
84077/84077 - 23s - loss: 2.0939e-04 - val_loss: 1.9890e-04 - 23s/epoch - 270us/sample
Epoch 83/90
84077/84077 - 22s - loss: 2.0929e-04 - val_loss: 1.9907e-04 - 22s/epoch - 267us/sample
Epoch 84/90
84077/84077 - 23s - loss: 2.0888e-04 - val_loss: 1.9789e-04 - 23s/epoch - 278us/sample
Epoch 85/90
84077/84077 - 23s - loss: 2.0907e-04 - val_loss: 2.0064e-04 - 23s/epoch - 268us/sample
Epoch 86/90
84077/84077 - 23s - loss: 2.0878e-04 - val_loss: 1.9764e-04 - 23s/epoch - 274us/sample
Epoch 87/90
84077/84077 - 23s - loss: 2.0805e-04 - val_loss: 1.9596e-04 - 23s/epoch - 270us/sample
Epoch 88/90
84077/84077 - 22s - loss: 2.0828e-04 - val_loss: 1.9762e-04 - 22s/epoch - 266us/sample
Epoch 89/90
84077/84077 - 23s - loss: 2.0777e-04 - val_loss: 1.9656e-04 - 23s/epoch - 278us/sample
Epoch 90/90
84077/84077 - 22s - loss: 2.0763e-04 - val_loss: 1.9618e-04 - 22s/epoch - 267us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001961813309688045
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 17:16:16.381610: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_32/outputlayer/BiasAdd' id:35875 op device:{requested: '', assigned: ''} def:{{{node decoder_model_32/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_32/outputlayer/MatMul, decoder_model_32/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.07142782659865828
cosine 0.07054403997432027
MAE: 0.0030460761102685855
RMSE: 0.016370184473221045
r2: 0.790663084141468
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 32, 90, 0.0006000000000000001, 0.2, 188, 0.00020763114022859567, 0.0001961813309688045, 0.07142782659865828, 0.07054403997432027, 0.0030460761102685855, 0.016370184473221045, 0.790663084141468, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0008 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_84 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_84 (ReLU)                (None, 1791)         0           ['batch_normalization_84[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 17:16:34.077663: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_33/kernel/Assign' id:36902 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_33/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_33/kernel, dense_dec1_33/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 17:16:50.309259: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_33/mul' id:37189 op device:{requested: '', assigned: ''} def:{{{node loss_33/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_33/mul/x, loss_33/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0313 - val_loss: 0.0015 - 22s/epoch - 264us/sample
Epoch 2/90
84077/84077 - 13s - loss: 0.0014 - val_loss: 0.0011 - 13s/epoch - 161us/sample
Epoch 3/90
84077/84077 - 13s - loss: 0.0019 - val_loss: 0.0011 - 13s/epoch - 155us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0011 - val_loss: 9.8386e-04 - 14s/epoch - 161us/sample
Epoch 5/90
84077/84077 - 13s - loss: 0.0606 - val_loss: 0.0013 - 13s/epoch - 160us/sample
Epoch 6/90
84077/84077 - 13s - loss: 0.0017 - val_loss: 0.0011 - 13s/epoch - 156us/sample
Epoch 7/90
84077/84077 - 14s - loss: 9.1914e-04 - val_loss: 8.2890e-04 - 14s/epoch - 161us/sample
Epoch 8/90
84077/84077 - 13s - loss: 7.7008e-04 - val_loss: 6.4538e-04 - 13s/epoch - 158us/sample
Epoch 9/90
84077/84077 - 13s - loss: 0.0012 - val_loss: 0.0019 - 13s/epoch - 158us/sample
Epoch 10/90
84077/84077 - 14s - loss: 0.0011 - val_loss: 9.5111e-04 - 14s/epoch - 161us/sample
Epoch 11/90
84077/84077 - 13s - loss: 6.5452e-04 - val_loss: 5.3216e-04 - 13s/epoch - 156us/sample
Epoch 12/90
84077/84077 - 13s - loss: 5.7478e-04 - val_loss: 4.9870e-04 - 13s/epoch - 158us/sample
Epoch 13/90
84077/84077 - 14s - loss: 6.1305e-04 - val_loss: 4.5837e-04 - 14s/epoch - 162us/sample
Epoch 14/90
84077/84077 - 13s - loss: 4.8715e-04 - val_loss: 4.2623e-04 - 13s/epoch - 155us/sample
Epoch 15/90
84077/84077 - 14s - loss: 4.6800e-04 - val_loss: 3.8703e-04 - 14s/epoch - 161us/sample
Epoch 16/90
84077/84077 - 13s - loss: 4.3686e-04 - val_loss: 3.7541e-04 - 13s/epoch - 158us/sample
Epoch 17/90
84077/84077 - 13s - loss: 4.0083e-04 - val_loss: 3.5246e-04 - 13s/epoch - 157us/sample
Epoch 18/90
84077/84077 - 14s - loss: 3.8095e-04 - val_loss: 3.5043e-04 - 14s/epoch - 162us/sample
Epoch 19/90
84077/84077 - 13s - loss: 3.6650e-04 - val_loss: 3.2960e-04 - 13s/epoch - 158us/sample
Epoch 20/90
84077/84077 - 13s - loss: 3.4774e-04 - val_loss: 3.0215e-04 - 13s/epoch - 157us/sample
Epoch 21/90
84077/84077 - 13s - loss: 3.3491e-04 - val_loss: 3.4270e-04 - 13s/epoch - 161us/sample
Epoch 22/90
84077/84077 - 13s - loss: 3.3217e-04 - val_loss: 2.8457e-04 - 13s/epoch - 159us/sample
Epoch 23/90
84077/84077 - 13s - loss: 3.1007e-04 - val_loss: 2.7998e-04 - 13s/epoch - 158us/sample
Epoch 24/90
84077/84077 - 14s - loss: 3.0127e-04 - val_loss: 2.7063e-04 - 14s/epoch - 161us/sample
Epoch 25/90
84077/84077 - 13s - loss: 2.9178e-04 - val_loss: 2.6240e-04 - 13s/epoch - 154us/sample
Epoch 26/90
84077/84077 - 14s - loss: 2.8433e-04 - val_loss: 2.7478e-04 - 14s/epoch - 162us/sample
Epoch 27/90
84077/84077 - 13s - loss: 2.7875e-04 - val_loss: 2.5492e-04 - 13s/epoch - 159us/sample
Epoch 28/90
84077/84077 - 13s - loss: 2.7179e-04 - val_loss: 2.5157e-04 - 13s/epoch - 156us/sample
Epoch 29/90
84077/84077 - 14s - loss: 2.6869e-04 - val_loss: 2.4743e-04 - 14s/epoch - 161us/sample
Epoch 30/90
84077/84077 - 13s - loss: 2.6317e-04 - val_loss: 2.4563e-04 - 13s/epoch - 159us/sample
Epoch 31/90
84077/84077 - 13s - loss: 2.6085e-04 - val_loss: 2.4236e-04 - 13s/epoch - 159us/sample
Epoch 32/90
84077/84077 - 14s - loss: 2.5659e-04 - val_loss: 2.4009e-04 - 14s/epoch - 161us/sample
Epoch 33/90
84077/84077 - 13s - loss: 2.5421e-04 - val_loss: 2.3537e-04 - 13s/epoch - 158us/sample
Epoch 34/90
84077/84077 - 13s - loss: 2.4955e-04 - val_loss: 2.3283e-04 - 13s/epoch - 157us/sample
Epoch 35/90
84077/84077 - 14s - loss: 2.7424e-04 - val_loss: 2.5373e-04 - 14s/epoch - 162us/sample
Epoch 36/90
84077/84077 - 13s - loss: 2.6963e-04 - val_loss: 2.3239e-04 - 13s/epoch - 154us/sample
Epoch 37/90
84077/84077 - 14s - loss: 2.4819e-04 - val_loss: 2.2906e-04 - 14s/epoch - 161us/sample
Epoch 38/90
84077/84077 - 13s - loss: 2.4318e-04 - val_loss: 2.2870e-04 - 13s/epoch - 160us/sample
Epoch 39/90
84077/84077 - 13s - loss: 2.4269e-04 - val_loss: 2.2511e-04 - 13s/epoch - 157us/sample
Epoch 40/90
84077/84077 - 14s - loss: 2.3889e-04 - val_loss: 2.2295e-04 - 14s/epoch - 161us/sample
Epoch 41/90
84077/84077 - 13s - loss: 2.3697e-04 - val_loss: 2.2361e-04 - 13s/epoch - 158us/sample
Epoch 42/90
84077/84077 - 13s - loss: 2.3541e-04 - val_loss: 2.2082e-04 - 13s/epoch - 157us/sample
Epoch 43/90
84077/84077 - 14s - loss: 2.3472e-04 - val_loss: 2.2190e-04 - 14s/epoch - 162us/sample
Epoch 44/90
84077/84077 - 13s - loss: 2.3068e-04 - val_loss: 2.1768e-04 - 13s/epoch - 159us/sample
Epoch 45/90
84077/84077 - 13s - loss: 2.3013e-04 - val_loss: 2.2297e-04 - 13s/epoch - 157us/sample
Epoch 46/90
84077/84077 - 14s - loss: 2.2957e-04 - val_loss: 2.2088e-04 - 14s/epoch - 161us/sample
Epoch 47/90
84077/84077 - 13s - loss: 2.3123e-04 - val_loss: 2.1551e-04 - 13s/epoch - 155us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.2628e-04 - val_loss: 2.1349e-04 - 14s/epoch - 162us/sample
Epoch 49/90
84077/84077 - 14s - loss: 2.2455e-04 - val_loss: 2.1351e-04 - 14s/epoch - 161us/sample
Epoch 50/90
84077/84077 - 13s - loss: 2.2316e-04 - val_loss: 2.1132e-04 - 13s/epoch - 154us/sample
Epoch 51/90
84077/84077 - 14s - loss: 2.2389e-04 - val_loss: 2.1436e-04 - 14s/epoch - 161us/sample
Epoch 52/90
84077/84077 - 13s - loss: 2.2155e-04 - val_loss: 2.0874e-04 - 13s/epoch - 160us/sample
Epoch 53/90
84077/84077 - 13s - loss: 2.2050e-04 - val_loss: 2.5032e-04 - 13s/epoch - 158us/sample
Epoch 54/90
84077/84077 - 13s - loss: 2.2157e-04 - val_loss: 2.0753e-04 - 13s/epoch - 161us/sample
Epoch 55/90
84077/84077 - 13s - loss: 2.1843e-04 - val_loss: 2.0804e-04 - 13s/epoch - 158us/sample
Epoch 56/90
84077/84077 - 13s - loss: 2.1742e-04 - val_loss: 2.0693e-04 - 13s/epoch - 158us/sample
Epoch 57/90
84077/84077 - 14s - loss: 2.1867e-04 - val_loss: 2.0536e-04 - 14s/epoch - 162us/sample
Epoch 58/90
84077/84077 - 13s - loss: 2.1544e-04 - val_loss: 2.1195e-04 - 13s/epoch - 155us/sample
Epoch 59/90
84077/84077 - 13s - loss: 2.1494e-04 - val_loss: 2.0514e-04 - 13s/epoch - 160us/sample
Epoch 60/90
84077/84077 - 13s - loss: 2.1407e-04 - val_loss: 2.0480e-04 - 13s/epoch - 161us/sample
Epoch 61/90
84077/84077 - 13s - loss: 2.1342e-04 - val_loss: 2.0458e-04 - 13s/epoch - 156us/sample
Epoch 62/90
84077/84077 - 14s - loss: 2.1353e-04 - val_loss: 2.0578e-04 - 14s/epoch - 161us/sample
Epoch 63/90
84077/84077 - 13s - loss: 2.1234e-04 - val_loss: 2.0226e-04 - 13s/epoch - 158us/sample
Epoch 64/90
84077/84077 - 13s - loss: 2.1135e-04 - val_loss: 2.0436e-04 - 13s/epoch - 157us/sample
Epoch 65/90
84077/84077 - 14s - loss: 2.1060e-04 - val_loss: 2.0203e-04 - 14s/epoch - 162us/sample
Epoch 66/90
84077/84077 - 13s - loss: 2.0992e-04 - val_loss: 2.0097e-04 - 13s/epoch - 158us/sample
Epoch 67/90
84077/84077 - 13s - loss: 2.1106e-04 - val_loss: 2.0169e-04 - 13s/epoch - 157us/sample
Epoch 68/90
84077/84077 - 14s - loss: 2.0886e-04 - val_loss: 2.0056e-04 - 14s/epoch - 161us/sample
Epoch 69/90
84077/84077 - 13s - loss: 2.0789e-04 - val_loss: 2.0120e-04 - 13s/epoch - 157us/sample
Epoch 70/90
84077/84077 - 13s - loss: 2.0813e-04 - val_loss: 1.9990e-04 - 13s/epoch - 160us/sample
Epoch 71/90
84077/84077 - 13s - loss: 2.0805e-04 - val_loss: 2.0266e-04 - 13s/epoch - 161us/sample
Epoch 72/90
84077/84077 - 13s - loss: 2.0686e-04 - val_loss: 1.9949e-04 - 13s/epoch - 155us/sample
Epoch 73/90
84077/84077 - 14s - loss: 2.0716e-04 - val_loss: 2.0130e-04 - 14s/epoch - 161us/sample
Epoch 74/90
84077/84077 - 13s - loss: 2.0740e-04 - val_loss: 1.9956e-04 - 13s/epoch - 160us/sample
Epoch 75/90
84077/84077 - 13s - loss: 2.0549e-04 - val_loss: 1.9703e-04 - 13s/epoch - 158us/sample
Epoch 76/90
84077/84077 - 14s - loss: 2.0534e-04 - val_loss: 2.0005e-04 - 14s/epoch - 161us/sample
Epoch 77/90
84077/84077 - 13s - loss: 2.0446e-04 - val_loss: 1.9807e-04 - 13s/epoch - 158us/sample
Epoch 78/90
84077/84077 - 13s - loss: 2.0456e-04 - val_loss: 1.9974e-04 - 13s/epoch - 157us/sample
Epoch 79/90
84077/84077 - 14s - loss: 2.0360e-04 - val_loss: 2.0003e-04 - 14s/epoch - 162us/sample
Epoch 80/90
84077/84077 - 13s - loss: 2.0556e-04 - val_loss: 1.9852e-04 - 13s/epoch - 158us/sample
Epoch 81/90
84077/84077 - 13s - loss: 2.0325e-04 - val_loss: 1.9627e-04 - 13s/epoch - 157us/sample
Epoch 82/90
84077/84077 - 14s - loss: 2.0242e-04 - val_loss: 1.9559e-04 - 14s/epoch - 161us/sample
Epoch 83/90
84077/84077 - 13s - loss: 2.0242e-04 - val_loss: 1.9664e-04 - 13s/epoch - 157us/sample
Epoch 84/90
84077/84077 - 14s - loss: 2.0186e-04 - val_loss: 1.9579e-04 - 14s/epoch - 161us/sample
Epoch 85/90
84077/84077 - 13s - loss: 2.0181e-04 - val_loss: 1.9635e-04 - 13s/epoch - 159us/sample
Epoch 86/90
84077/84077 - 13s - loss: 2.0143e-04 - val_loss: 1.9610e-04 - 13s/epoch - 157us/sample
Epoch 87/90
84077/84077 - 14s - loss: 2.0150e-04 - val_loss: 1.9352e-04 - 14s/epoch - 162us/sample
Epoch 88/90
84077/84077 - 13s - loss: 2.0022e-04 - val_loss: 1.9542e-04 - 13s/epoch - 158us/sample
Epoch 89/90
84077/84077 - 13s - loss: 2.0191e-04 - val_loss: 1.9483e-04 - 13s/epoch - 157us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.9983e-04 - val_loss: 1.9444e-04 - 14s/epoch - 161us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00019443542984770056
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 17:36:42.775258: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_33/outputlayer/BiasAdd' id:37160 op device:{requested: '', assigned: ''} def:{{{node decoder_model_33/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_33/outputlayer/MatMul, decoder_model_33/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023014896667007297
cosine 0.022736899738709848
MAE: 0.002007349974706767
RMSE: 0.00937597678792011
r2: 0.9315207086788749
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 64, 90, 0.0008, 0.2, 188, 0.00019982973629479616, 0.00019443542984770056, 0.023014896667007297, 0.022736899738709848, 0.002007349974706767, 0.00937597678792011, 0.9315207086788749, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0006000000000000001 32 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_87 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_87 (ReLU)                (None, 1697)         0           ['batch_normalization_87[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 17:37:00.750378: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_89/moving_mean/Assign' id:38267 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_89/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_89/moving_mean, batch_normalization_89/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 17:37:26.340854: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_34/mul' id:38444 op device:{requested: '', assigned: ''} def:{{{node loss_34/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_34/mul/x, loss_34/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 32s - loss: 0.0088 - val_loss: 0.0151 - 32s/epoch - 383us/sample
Epoch 2/90
84077/84077 - 23s - loss: 0.0020 - val_loss: 0.0010 - 23s/epoch - 269us/sample
Epoch 3/90
84077/84077 - 22s - loss: 0.0014 - val_loss: 8.7169e-04 - 22s/epoch - 267us/sample
Epoch 4/90
84077/84077 - 23s - loss: 0.0010 - val_loss: 7.3812e-04 - 23s/epoch - 275us/sample
Epoch 5/90
84077/84077 - 22s - loss: 7.5429e-04 - val_loss: 5.7851e-04 - 22s/epoch - 265us/sample
Epoch 6/90
84077/84077 - 23s - loss: 6.0899e-04 - val_loss: 5.2412e-04 - 23s/epoch - 272us/sample
Epoch 7/90
84077/84077 - 23s - loss: 5.3488e-04 - val_loss: 4.5535e-04 - 23s/epoch - 270us/sample
Epoch 8/90
84077/84077 - 22s - loss: 4.8304e-04 - val_loss: 4.1645e-04 - 22s/epoch - 266us/sample
Epoch 9/90
84077/84077 - 23s - loss: 4.4575e-04 - val_loss: 3.8284e-04 - 23s/epoch - 272us/sample
Epoch 10/90
84077/84077 - 23s - loss: 4.2219e-04 - val_loss: 3.7460e-04 - 23s/epoch - 269us/sample
Epoch 11/90
84077/84077 - 22s - loss: 4.0794e-04 - val_loss: 3.5892e-04 - 22s/epoch - 265us/sample
Epoch 12/90
84077/84077 - 23s - loss: 3.9497e-04 - val_loss: 5.5819e-04 - 23s/epoch - 278us/sample
Epoch 13/90
84077/84077 - 22s - loss: 3.7998e-04 - val_loss: 3.3163e-04 - 22s/epoch - 265us/sample
Epoch 14/90
84077/84077 - 23s - loss: 3.6907e-04 - val_loss: 3.2663e-04 - 23s/epoch - 272us/sample
Epoch 15/90
84077/84077 - 23s - loss: 3.5722e-04 - val_loss: 3.2032e-04 - 23s/epoch - 270us/sample
Epoch 16/90
84077/84077 - 22s - loss: 3.5407e-04 - val_loss: 3.0860e-04 - 22s/epoch - 265us/sample
Epoch 17/90
84077/84077 - 23s - loss: 3.4244e-04 - val_loss: 3.0715e-04 - 23s/epoch - 277us/sample
Epoch 18/90
84077/84077 - 22s - loss: 3.3578e-04 - val_loss: 2.9872e-04 - 22s/epoch - 266us/sample
Epoch 19/90
84077/84077 - 23s - loss: 3.2676e-04 - val_loss: 2.8945e-04 - 23s/epoch - 270us/sample
Epoch 20/90
84077/84077 - 23s - loss: 3.2315e-04 - val_loss: 2.9309e-04 - 23s/epoch - 272us/sample
Epoch 21/90
84077/84077 - 22s - loss: 3.1486e-04 - val_loss: 2.7709e-04 - 22s/epoch - 265us/sample
Epoch 22/90
84077/84077 - 23s - loss: 3.1797e-04 - val_loss: 2.7656e-04 - 23s/epoch - 270us/sample
Epoch 23/90
84077/84077 - 23s - loss: 3.0748e-04 - val_loss: 3.4875e-04 - 23s/epoch - 271us/sample
Epoch 24/90
84077/84077 - 22s - loss: 3.0831e-04 - val_loss: 2.8266e-04 - 22s/epoch - 265us/sample
Epoch 25/90
84077/84077 - 23s - loss: 2.9879e-04 - val_loss: 2.6853e-04 - 23s/epoch - 277us/sample
Epoch 26/90
84077/84077 - 22s - loss: 2.9881e-04 - val_loss: 2.7245e-04 - 22s/epoch - 266us/sample
Epoch 27/90
84077/84077 - 23s - loss: 2.9256e-04 - val_loss: 2.6187e-04 - 23s/epoch - 271us/sample
Epoch 28/90
84077/84077 - 23s - loss: 2.8663e-04 - val_loss: 2.5885e-04 - 23s/epoch - 271us/sample
Epoch 29/90
84077/84077 - 22s - loss: 2.8679e-04 - val_loss: 2.5408e-04 - 22s/epoch - 265us/sample
Epoch 30/90
84077/84077 - 23s - loss: 2.8188e-04 - val_loss: 2.5545e-04 - 23s/epoch - 275us/sample
Epoch 31/90
84077/84077 - 22s - loss: 2.7868e-04 - val_loss: 2.4783e-04 - 22s/epoch - 267us/sample
Epoch 32/90
84077/84077 - 23s - loss: 2.7660e-04 - val_loss: 2.4746e-04 - 23s/epoch - 271us/sample
Epoch 33/90
84077/84077 - 23s - loss: 2.7343e-04 - val_loss: 2.4483e-04 - 23s/epoch - 270us/sample
Epoch 34/90
84077/84077 - 22s - loss: 2.7841e-04 - val_loss: 2.5103e-04 - 22s/epoch - 261us/sample
Epoch 35/90
84077/84077 - 16s - loss: 2.7062e-04 - val_loss: 2.4440e-04 - 16s/epoch - 187us/sample
Epoch 36/90
84077/84077 - 15s - loss: 2.6837e-04 - val_loss: 2.4058e-04 - 15s/epoch - 184us/sample
Epoch 37/90
84077/84077 - 15s - loss: 2.6609e-04 - val_loss: 2.3791e-04 - 15s/epoch - 184us/sample
Epoch 38/90
84077/84077 - 15s - loss: 2.6455e-04 - val_loss: 2.3601e-04 - 15s/epoch - 183us/sample
Epoch 39/90
84077/84077 - 15s - loss: 2.6368e-04 - val_loss: 2.3999e-04 - 15s/epoch - 182us/sample
Epoch 40/90
84077/84077 - 15s - loss: 2.6105e-04 - val_loss: 2.3820e-04 - 15s/epoch - 183us/sample
Epoch 41/90
84077/84077 - 15s - loss: 2.5973e-04 - val_loss: 2.3454e-04 - 15s/epoch - 183us/sample
Epoch 42/90
84077/84077 - 15s - loss: 2.6373e-04 - val_loss: 2.3821e-04 - 15s/epoch - 183us/sample
Epoch 43/90
84077/84077 - 15s - loss: 2.5657e-04 - val_loss: 2.3175e-04 - 15s/epoch - 182us/sample
Epoch 44/90
84077/84077 - 15s - loss: 2.5288e-04 - val_loss: 2.2902e-04 - 15s/epoch - 183us/sample
Epoch 45/90
84077/84077 - 16s - loss: 2.5890e-04 - val_loss: 2.3504e-04 - 16s/epoch - 184us/sample
Epoch 46/90
84077/84077 - 15s - loss: 2.5304e-04 - val_loss: 2.2517e-04 - 15s/epoch - 183us/sample
Epoch 47/90
84077/84077 - 15s - loss: 2.5990e-04 - val_loss: 2.2945e-04 - 15s/epoch - 182us/sample
Epoch 48/90
84077/84077 - 15s - loss: 2.5176e-04 - val_loss: 2.2556e-04 - 15s/epoch - 182us/sample
Epoch 49/90
84077/84077 - 15s - loss: 2.4949e-04 - val_loss: 2.2551e-04 - 15s/epoch - 184us/sample
Epoch 50/90
84077/84077 - 15s - loss: 2.5035e-04 - val_loss: 2.2539e-04 - 15s/epoch - 183us/sample
Epoch 51/90
84077/84077 - 15s - loss: 2.4808e-04 - val_loss: 2.2229e-04 - 15s/epoch - 182us/sample
Epoch 52/90
84077/84077 - 15s - loss: 2.4654e-04 - val_loss: 2.4047e-04 - 15s/epoch - 183us/sample
Epoch 53/90
84077/84077 - 15s - loss: 2.4654e-04 - val_loss: 2.2262e-04 - 15s/epoch - 184us/sample
Epoch 54/90
84077/84077 - 15s - loss: 2.4582e-04 - val_loss: 2.2129e-04 - 15s/epoch - 183us/sample
Epoch 55/90
84077/84077 - 15s - loss: 2.4544e-04 - val_loss: 2.2842e-04 - 15s/epoch - 182us/sample
Epoch 56/90
84077/84077 - 15s - loss: 2.4664e-04 - val_loss: 2.4068e-04 - 15s/epoch - 184us/sample
Epoch 57/90
84077/84077 - 15s - loss: 2.4454e-04 - val_loss: 2.6714e-04 - 15s/epoch - 184us/sample
Epoch 58/90
84077/84077 - 15s - loss: 2.4318e-04 - val_loss: 2.2016e-04 - 15s/epoch - 182us/sample
Epoch 59/90
84077/84077 - 15s - loss: 2.4125e-04 - val_loss: 2.2166e-04 - 15s/epoch - 183us/sample
Epoch 60/90
84077/84077 - 16s - loss: 2.3789e-04 - val_loss: 2.1602e-04 - 16s/epoch - 184us/sample
Epoch 61/90
84077/84077 - 15s - loss: 2.4168e-04 - val_loss: 2.1596e-04 - 15s/epoch - 183us/sample
Epoch 62/90
84077/84077 - 15s - loss: 2.3723e-04 - val_loss: 2.1754e-04 - 15s/epoch - 182us/sample
Epoch 63/90
84077/84077 - 15s - loss: 2.5312e-04 - val_loss: 2.1581e-04 - 15s/epoch - 182us/sample
Epoch 64/90
84077/84077 - 15s - loss: 2.3853e-04 - val_loss: 2.5603e-04 - 15s/epoch - 183us/sample
Epoch 65/90
84077/84077 - 15s - loss: 2.3986e-04 - val_loss: 2.1467e-04 - 15s/epoch - 184us/sample
Epoch 66/90
84077/84077 - 15s - loss: 2.3539e-04 - val_loss: 2.1380e-04 - 15s/epoch - 182us/sample
Epoch 67/90
84077/84077 - 15s - loss: 2.3606e-04 - val_loss: 2.1379e-04 - 15s/epoch - 182us/sample
Epoch 68/90
84077/84077 - 15s - loss: 2.3440e-04 - val_loss: 2.1596e-04 - 15s/epoch - 183us/sample
Epoch 69/90
84077/84077 - 15s - loss: 2.3280e-04 - val_loss: 2.4022e-04 - 15s/epoch - 184us/sample
Epoch 70/90
84077/84077 - 15s - loss: 2.4162e-04 - val_loss: 2.1238e-04 - 15s/epoch - 182us/sample
Epoch 71/90
84077/84077 - 15s - loss: 2.3613e-04 - val_loss: 2.1394e-04 - 15s/epoch - 182us/sample
Epoch 72/90
84077/84077 - 15s - loss: 2.3253e-04 - val_loss: 2.0958e-04 - 15s/epoch - 182us/sample
Epoch 73/90
84077/84077 - 16s - loss: 2.3402e-04 - val_loss: 2.1470e-04 - 16s/epoch - 184us/sample
Epoch 74/90
84077/84077 - 15s - loss: 2.3262e-04 - val_loss: 2.1204e-04 - 15s/epoch - 183us/sample
Epoch 75/90
84077/84077 - 15s - loss: 2.3522e-04 - val_loss: 2.1058e-04 - 15s/epoch - 183us/sample
Epoch 76/90
84077/84077 - 15s - loss: 2.3066e-04 - val_loss: 2.1193e-04 - 15s/epoch - 183us/sample
Epoch 77/90
84077/84077 - 15s - loss: 2.2913e-04 - val_loss: 2.1056e-04 - 15s/epoch - 184us/sample
Epoch 78/90
84077/84077 - 15s - loss: 2.3189e-04 - val_loss: 2.1812e-04 - 15s/epoch - 183us/sample
Epoch 79/90
84077/84077 - 15s - loss: 2.3200e-04 - val_loss: 2.1054e-04 - 15s/epoch - 182us/sample
Epoch 80/90
84077/84077 - 15s - loss: 2.2672e-04 - val_loss: 2.0961e-04 - 15s/epoch - 183us/sample
Epoch 81/90
84077/84077 - 15s - loss: 2.3243e-04 - val_loss: 2.1008e-04 - 15s/epoch - 184us/sample
Epoch 82/90
84077/84077 - 15s - loss: 2.2828e-04 - val_loss: 2.0885e-04 - 15s/epoch - 182us/sample
Epoch 83/90
84077/84077 - 15s - loss: 2.3592e-04 - val_loss: 2.0788e-04 - 15s/epoch - 182us/sample
Epoch 84/90
84077/84077 - 15s - loss: 2.2562e-04 - val_loss: 2.0655e-04 - 15s/epoch - 184us/sample
Epoch 85/90
84077/84077 - 15s - loss: 2.2554e-04 - val_loss: 2.1024e-04 - 15s/epoch - 183us/sample
Epoch 86/90
84077/84077 - 15s - loss: 2.3690e-04 - val_loss: 2.0750e-04 - 15s/epoch - 182us/sample
Epoch 87/90
84077/84077 - 15s - loss: 2.2987e-04 - val_loss: 2.0553e-04 - 15s/epoch - 182us/sample
Epoch 88/90
84077/84077 - 15s - loss: 2.2575e-04 - val_loss: 2.0703e-04 - 15s/epoch - 183us/sample
Epoch 89/90
84077/84077 - 15s - loss: 2.2780e-04 - val_loss: 2.1080e-04 - 15s/epoch - 184us/sample
Epoch 90/90
84077/84077 - 15s - loss: 2.2445e-04 - val_loss: 2.0503e-04 - 15s/epoch - 183us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00020503496521498887
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 18:04:19.433222: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_34/outputlayer/BiasAdd' id:38415 op device:{requested: '', assigned: ''} def:{{{node decoder_model_34/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_34/outputlayer/MatMul, decoder_model_34/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.026762824732279596
cosine 0.026435753864064875
MAE: 0.0025198173061388588
RMSE: 0.009782879641036626
r2: 0.9253694053237312
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 90, 0.0006000000000000001, 0.2, 188, 0.00022445499989043514, 0.00020503496521498887, 0.026762824732279596, 0.026435753864064875, 0.0025198173061388588, 0.009782879641036626, 0.9253694053237312, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0008 32 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_90 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_90 (ReLU)                (None, 1886)         0           ['batch_normalization_90[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 18:04:36.157056: W tensorflow/c/c_api.cc:291] Operation '{name:'training_50/Adam/batch_normalization_92/beta/v/Assign' id:40391 op device:{requested: '', assigned: ''} def:{{{node training_50/Adam/batch_normalization_92/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_50/Adam/batch_normalization_92/beta/v, training_50/Adam/batch_normalization_92/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 18:04:54.695932: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_35/mul' id:39706 op device:{requested: '', assigned: ''} def:{{{node loss_35/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_35/mul/x, loss_35/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0029 - val_loss: 9.5434e-04 - 24s/epoch - 291us/sample
Epoch 2/85
84077/84077 - 16s - loss: 0.0011 - val_loss: 5.5847e-04 - 16s/epoch - 188us/sample
Epoch 3/85
84077/84077 - 16s - loss: 6.4840e-04 - val_loss: 5.0200e-04 - 16s/epoch - 188us/sample
Epoch 4/85
84077/84077 - 16s - loss: 5.5106e-04 - val_loss: 4.5677e-04 - 16s/epoch - 188us/sample
Epoch 5/85
84077/84077 - 16s - loss: 4.5119e-04 - val_loss: 3.8786e-04 - 16s/epoch - 188us/sample
Epoch 6/85
84077/84077 - 16s - loss: 4.0184e-04 - val_loss: 3.4663e-04 - 16s/epoch - 189us/sample
Epoch 7/85
84077/84077 - 16s - loss: 3.5949e-04 - val_loss: 3.0249e-04 - 16s/epoch - 188us/sample
Epoch 8/85
84077/84077 - 16s - loss: 3.2420e-04 - val_loss: 2.8045e-04 - 16s/epoch - 187us/sample
Epoch 9/85
84077/84077 - 16s - loss: 3.0433e-04 - val_loss: 2.7244e-04 - 16s/epoch - 188us/sample
Epoch 10/85
84077/84077 - 16s - loss: 2.9093e-04 - val_loss: 2.5269e-04 - 16s/epoch - 189us/sample
Epoch 11/85
84077/84077 - 16s - loss: 2.8116e-04 - val_loss: 2.4864e-04 - 16s/epoch - 188us/sample
Epoch 12/85
84077/84077 - 16s - loss: 2.7225e-04 - val_loss: 2.4091e-04 - 16s/epoch - 188us/sample
Epoch 13/85
84077/84077 - 16s - loss: 2.6487e-04 - val_loss: 2.3304e-04 - 16s/epoch - 188us/sample
Epoch 14/85
84077/84077 - 16s - loss: 2.5772e-04 - val_loss: 2.2669e-04 - 16s/epoch - 188us/sample
Epoch 15/85
84077/84077 - 16s - loss: 2.5254e-04 - val_loss: 2.2425e-04 - 16s/epoch - 189us/sample
Epoch 16/85
84077/84077 - 16s - loss: 2.4810e-04 - val_loss: 2.1986e-04 - 16s/epoch - 188us/sample
Epoch 17/85
84077/84077 - 16s - loss: 2.4478e-04 - val_loss: 2.1877e-04 - 16s/epoch - 187us/sample
Epoch 18/85
84077/84077 - 16s - loss: 2.4151e-04 - val_loss: 2.1428e-04 - 16s/epoch - 189us/sample
Epoch 19/85
84077/84077 - 16s - loss: 2.3772e-04 - val_loss: 2.1221e-04 - 16s/epoch - 188us/sample
Epoch 20/85
84077/84077 - 16s - loss: 2.3582e-04 - val_loss: 2.0999e-04 - 16s/epoch - 188us/sample
Epoch 21/85
84077/84077 - 16s - loss: 2.3245e-04 - val_loss: 2.0283e-04 - 16s/epoch - 188us/sample
Epoch 22/85
84077/84077 - 16s - loss: 2.2991e-04 - val_loss: 2.0374e-04 - 16s/epoch - 188us/sample
Epoch 23/85
84077/84077 - 16s - loss: 2.2676e-04 - val_loss: 2.0435e-04 - 16s/epoch - 189us/sample
Epoch 24/85
84077/84077 - 16s - loss: 2.2560e-04 - val_loss: 2.0074e-04 - 16s/epoch - 188us/sample
Epoch 25/85
84077/84077 - 16s - loss: 2.2347e-04 - val_loss: 1.9818e-04 - 16s/epoch - 188us/sample
Epoch 26/85
84077/84077 - 16s - loss: 2.2104e-04 - val_loss: 1.9736e-04 - 16s/epoch - 188us/sample
Epoch 27/85
84077/84077 - 16s - loss: 2.1995e-04 - val_loss: 1.9350e-04 - 16s/epoch - 189us/sample
Epoch 28/85
84077/84077 - 16s - loss: 2.1842e-04 - val_loss: 1.9484e-04 - 16s/epoch - 188us/sample
Epoch 29/85
84077/84077 - 16s - loss: 2.1637e-04 - val_loss: 1.9280e-04 - 16s/epoch - 187us/sample
Epoch 30/85
84077/84077 - 16s - loss: 2.1618e-04 - val_loss: 1.9098e-04 - 16s/epoch - 188us/sample
Epoch 31/85
84077/84077 - 16s - loss: 2.1432e-04 - val_loss: 1.8910e-04 - 16s/epoch - 190us/sample
Epoch 32/85
84077/84077 - 16s - loss: 2.1225e-04 - val_loss: 1.9130e-04 - 16s/epoch - 188us/sample
Epoch 33/85
84077/84077 - 16s - loss: 2.1035e-04 - val_loss: 1.8808e-04 - 16s/epoch - 188us/sample
Epoch 34/85
84077/84077 - 16s - loss: 2.1036e-04 - val_loss: 1.8672e-04 - 16s/epoch - 189us/sample
Epoch 35/85
84077/84077 - 16s - loss: 2.0819e-04 - val_loss: 1.8644e-04 - 16s/epoch - 188us/sample
Epoch 36/85
84077/84077 - 16s - loss: 2.0665e-04 - val_loss: 1.8371e-04 - 16s/epoch - 187us/sample
Epoch 37/85
84077/84077 - 16s - loss: 2.0481e-04 - val_loss: 1.8168e-04 - 16s/epoch - 188us/sample
Epoch 38/85
84077/84077 - 16s - loss: 2.0287e-04 - val_loss: 1.8015e-04 - 16s/epoch - 189us/sample
Epoch 39/85
84077/84077 - 16s - loss: 2.0185e-04 - val_loss: 1.8183e-04 - 16s/epoch - 188us/sample
Epoch 40/85
84077/84077 - 16s - loss: 2.0049e-04 - val_loss: 1.7969e-04 - 16s/epoch - 187us/sample
Epoch 41/85
84077/84077 - 16s - loss: 1.9971e-04 - val_loss: 1.7782e-04 - 16s/epoch - 189us/sample
Epoch 42/85
84077/84077 - 16s - loss: 1.9864e-04 - val_loss: 1.7706e-04 - 16s/epoch - 189us/sample
Epoch 43/85
84077/84077 - 16s - loss: 1.9735e-04 - val_loss: 1.7512e-04 - 16s/epoch - 188us/sample
Epoch 44/85
84077/84077 - 16s - loss: 1.9542e-04 - val_loss: 1.7472e-04 - 16s/epoch - 188us/sample
Epoch 45/85
84077/84077 - 16s - loss: 1.9430e-04 - val_loss: 1.7386e-04 - 16s/epoch - 190us/sample
Epoch 46/85
84077/84077 - 16s - loss: 1.9335e-04 - val_loss: 1.7156e-04 - 16s/epoch - 188us/sample
Epoch 47/85
84077/84077 - 16s - loss: 1.9217e-04 - val_loss: 1.7189e-04 - 16s/epoch - 188us/sample
Epoch 48/85
84077/84077 - 16s - loss: 1.9132e-04 - val_loss: 1.6998e-04 - 16s/epoch - 188us/sample
Epoch 49/85
84077/84077 - 16s - loss: 1.9116e-04 - val_loss: 1.7065e-04 - 16s/epoch - 189us/sample
Epoch 50/85
84077/84077 - 16s - loss: 1.8943e-04 - val_loss: 1.6912e-04 - 16s/epoch - 188us/sample
Epoch 51/85
84077/84077 - 16s - loss: 1.8900e-04 - val_loss: 1.6734e-04 - 16s/epoch - 188us/sample
Epoch 52/85
84077/84077 - 16s - loss: 1.8755e-04 - val_loss: 1.6806e-04 - 16s/epoch - 188us/sample
Epoch 53/85
84077/84077 - 16s - loss: 1.8742e-04 - val_loss: 1.6784e-04 - 16s/epoch - 189us/sample
Epoch 54/85
84077/84077 - 16s - loss: 1.8657e-04 - val_loss: 1.6723e-04 - 16s/epoch - 188us/sample
Epoch 55/85
84077/84077 - 16s - loss: 1.8491e-04 - val_loss: 1.6596e-04 - 16s/epoch - 188us/sample
Epoch 56/85
84077/84077 - 16s - loss: 1.8457e-04 - val_loss: 1.6400e-04 - 16s/epoch - 188us/sample
Epoch 57/85
84077/84077 - 16s - loss: 1.8395e-04 - val_loss: 1.6573e-04 - 16s/epoch - 189us/sample
Epoch 58/85
84077/84077 - 16s - loss: 1.8319e-04 - val_loss: 1.6346e-04 - 16s/epoch - 188us/sample
Epoch 59/85
84077/84077 - 16s - loss: 1.8238e-04 - val_loss: 1.6244e-04 - 16s/epoch - 188us/sample
Epoch 60/85
84077/84077 - 16s - loss: 1.8181e-04 - val_loss: 1.6262e-04 - 16s/epoch - 188us/sample
Epoch 61/85
84077/84077 - 16s - loss: 1.8116e-04 - val_loss: 1.6172e-04 - 16s/epoch - 188us/sample
Epoch 62/85
84077/84077 - 16s - loss: 1.7988e-04 - val_loss: 1.6159e-04 - 16s/epoch - 189us/sample
Epoch 63/85
84077/84077 - 16s - loss: 1.7949e-04 - val_loss: 1.6131e-04 - 16s/epoch - 188us/sample
Epoch 64/85
84077/84077 - 16s - loss: 1.7885e-04 - val_loss: 1.6319e-04 - 16s/epoch - 188us/sample
Epoch 65/85
84077/84077 - 16s - loss: 1.7847e-04 - val_loss: 1.5940e-04 - 16s/epoch - 188us/sample
Epoch 66/85
84077/84077 - 16s - loss: 1.7787e-04 - val_loss: 1.6032e-04 - 16s/epoch - 189us/sample
Epoch 67/85
84077/84077 - 16s - loss: 1.7804e-04 - val_loss: 1.5699e-04 - 16s/epoch - 188us/sample
Epoch 68/85
84077/84077 - 16s - loss: 1.7671e-04 - val_loss: 1.5789e-04 - 16s/epoch - 187us/sample
Epoch 69/85
84077/84077 - 16s - loss: 1.7664e-04 - val_loss: 1.5690e-04 - 16s/epoch - 188us/sample
Epoch 70/85
84077/84077 - 16s - loss: 1.7632e-04 - val_loss: 1.5636e-04 - 16s/epoch - 189us/sample
Epoch 71/85
84077/84077 - 16s - loss: 1.7528e-04 - val_loss: 1.5708e-04 - 16s/epoch - 188us/sample
Epoch 72/85
84077/84077 - 16s - loss: 1.7505e-04 - val_loss: 1.5615e-04 - 16s/epoch - 187us/sample
Epoch 73/85
84077/84077 - 16s - loss: 1.7522e-04 - val_loss: 1.5484e-04 - 16s/epoch - 188us/sample
Epoch 74/85
84077/84077 - 16s - loss: 1.7473e-04 - val_loss: 1.5632e-04 - 16s/epoch - 188us/sample
Epoch 75/85
84077/84077 - 16s - loss: 1.7363e-04 - val_loss: 1.5885e-04 - 16s/epoch - 189us/sample
Epoch 76/85
84077/84077 - 16s - loss: 1.7373e-04 - val_loss: 1.5623e-04 - 16s/epoch - 187us/sample
Epoch 77/85
84077/84077 - 16s - loss: 1.7348e-04 - val_loss: 1.5356e-04 - 16s/epoch - 188us/sample
Epoch 78/85
84077/84077 - 16s - loss: 1.7252e-04 - val_loss: 1.5465e-04 - 16s/epoch - 188us/sample
Epoch 79/85
84077/84077 - 16s - loss: 1.7230e-04 - val_loss: 1.5374e-04 - 16s/epoch - 190us/sample
Epoch 80/85
84077/84077 - 16s - loss: 1.7229e-04 - val_loss: 1.5414e-04 - 16s/epoch - 188us/sample
Epoch 81/85
84077/84077 - 16s - loss: 1.7127e-04 - val_loss: 1.5310e-04 - 16s/epoch - 188us/sample
Epoch 82/85
84077/84077 - 16s - loss: 1.7104e-04 - val_loss: 1.5250e-04 - 16s/epoch - 188us/sample
Epoch 83/85
84077/84077 - 16s - loss: 1.7132e-04 - val_loss: 1.5267e-04 - 16s/epoch - 189us/sample
Epoch 84/85
84077/84077 - 16s - loss: 1.6995e-04 - val_loss: 1.5370e-04 - 16s/epoch - 189us/sample
Epoch 85/85
84077/84077 - 16s - loss: 1.7035e-04 - val_loss: 1.5240e-04 - 16s/epoch - 188us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001523991669610183
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 18:27:06.850711: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_35/outputlayer/BiasAdd' id:39670 op device:{requested: '', assigned: ''} def:{{{node decoder_model_35/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_35/outputlayer/MatMul, decoder_model_35/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04575120419493167
cosine 0.04518629673083219
MAE: 0.0026002024246430974
RMSE: 0.012578395585984514
r2: 0.8764449847903859
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 32, 85, 0.0008, 0.2, 188, 0.00017035466852093035, 0.0001523991669610183, 0.04575120419493167, 0.04518629673083219, 0.0026002024246430974, 0.012578395585984514, 0.8764449847903859, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0006000000000000001 32 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_93 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_93 (ReLU)                (None, 1886)         0           ['batch_normalization_93[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 18:27:23.885401: W tensorflow/c/c_api.cc:291] Operation '{name:'training_52/Adam/dense_enc0_36/kernel/m/Assign' id:41437 op device:{requested: '', assigned: ''} def:{{{node training_52/Adam/dense_enc0_36/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_52/Adam/dense_enc0_36/kernel/m, training_52/Adam/dense_enc0_36/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 18:27:42.525057: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_36/mul' id:40984 op device:{requested: '', assigned: ''} def:{{{node loss_36/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_36/mul/x, loss_36/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 25s - loss: 0.7700 - val_loss: 0.0026 - 25s/epoch - 294us/sample
Epoch 2/90
84077/84077 - 16s - loss: 0.0047 - val_loss: 0.0011 - 16s/epoch - 187us/sample
Epoch 3/90
84077/84077 - 16s - loss: 0.0014 - val_loss: 0.0010 - 16s/epoch - 188us/sample
Epoch 4/90
84077/84077 - 16s - loss: 0.0012 - val_loss: 8.4901e-04 - 16s/epoch - 189us/sample
Epoch 5/90
84077/84077 - 16s - loss: 9.0086e-04 - val_loss: 6.6062e-04 - 16s/epoch - 187us/sample
Epoch 6/90
84077/84077 - 16s - loss: 7.4678e-04 - val_loss: 5.8569e-04 - 16s/epoch - 187us/sample
Epoch 7/90
84077/84077 - 16s - loss: 6.1782e-04 - val_loss: 5.0908e-04 - 16s/epoch - 188us/sample
Epoch 8/90
84077/84077 - 16s - loss: 5.4422e-04 - val_loss: 4.7836e-04 - 16s/epoch - 189us/sample
Epoch 9/90
84077/84077 - 16s - loss: 5.0045e-04 - val_loss: 4.4973e-04 - 16s/epoch - 188us/sample
Epoch 10/90
84077/84077 - 16s - loss: 4.6513e-04 - val_loss: 4.2921e-04 - 16s/epoch - 188us/sample
Epoch 11/90
84077/84077 - 16s - loss: 4.3901e-04 - val_loss: 4.0242e-04 - 16s/epoch - 188us/sample
Epoch 12/90
84077/84077 - 16s - loss: 4.1598e-04 - val_loss: 3.9046e-04 - 16s/epoch - 189us/sample
Epoch 13/90
84077/84077 - 16s - loss: 3.9814e-04 - val_loss: 3.7585e-04 - 16s/epoch - 187us/sample
Epoch 14/90
84077/84077 - 16s - loss: 3.8277e-04 - val_loss: 3.6840e-04 - 16s/epoch - 187us/sample
Epoch 15/90
84077/84077 - 16s - loss: 3.6857e-04 - val_loss: 3.5741e-04 - 16s/epoch - 189us/sample
Epoch 16/90
84077/84077 - 16s - loss: 3.5702e-04 - val_loss: 3.5099e-04 - 16s/epoch - 188us/sample
Epoch 17/90
84077/84077 - 16s - loss: 3.4736e-04 - val_loss: 3.4056e-04 - 16s/epoch - 187us/sample
Epoch 18/90
84077/84077 - 16s - loss: 3.3839e-04 - val_loss: 3.4043e-04 - 16s/epoch - 188us/sample
Epoch 19/90
84077/84077 - 16s - loss: 3.3112e-04 - val_loss: 3.3612e-04 - 16s/epoch - 189us/sample
Epoch 20/90
84077/84077 - 16s - loss: 3.2349e-04 - val_loss: 3.2959e-04 - 16s/epoch - 188us/sample
Epoch 21/90
84077/84077 - 16s - loss: 3.1658e-04 - val_loss: 3.2178e-04 - 16s/epoch - 187us/sample
Epoch 22/90
84077/84077 - 16s - loss: 3.1116e-04 - val_loss: 3.1644e-04 - 16s/epoch - 188us/sample
Epoch 23/90
84077/84077 - 16s - loss: 3.0934e-04 - val_loss: 3.2153e-04 - 16s/epoch - 189us/sample
Epoch 24/90
84077/84077 - 16s - loss: 3.0432e-04 - val_loss: 3.1314e-04 - 16s/epoch - 188us/sample
Epoch 25/90
84077/84077 - 16s - loss: 3.0109e-04 - val_loss: 3.1086e-04 - 16s/epoch - 187us/sample
Epoch 26/90
84077/84077 - 16s - loss: 2.9861e-04 - val_loss: 3.1517e-04 - 16s/epoch - 188us/sample
Epoch 27/90
84077/84077 - 16s - loss: 2.9664e-04 - val_loss: 3.0871e-04 - 16s/epoch - 190us/sample
Epoch 28/90
84077/84077 - 16s - loss: 2.9416e-04 - val_loss: 3.0505e-04 - 16s/epoch - 188us/sample
Epoch 29/90
84077/84077 - 16s - loss: 2.9088e-04 - val_loss: 3.0307e-04 - 16s/epoch - 188us/sample
Epoch 30/90
84077/84077 - 16s - loss: 2.9028e-04 - val_loss: 3.0389e-04 - 16s/epoch - 188us/sample
Epoch 31/90
84077/84077 - 16s - loss: 2.8831e-04 - val_loss: 3.0575e-04 - 16s/epoch - 189us/sample
Epoch 32/90
84077/84077 - 16s - loss: 2.8810e-04 - val_loss: 3.0112e-04 - 16s/epoch - 188us/sample
Epoch 33/90
84077/84077 - 16s - loss: 2.8343e-04 - val_loss: 2.9880e-04 - 16s/epoch - 187us/sample
Epoch 34/90
84077/84077 - 16s - loss: 2.8282e-04 - val_loss: 2.9652e-04 - 16s/epoch - 189us/sample
Epoch 35/90
84077/84077 - 16s - loss: 2.8059e-04 - val_loss: 2.9635e-04 - 16s/epoch - 189us/sample
Epoch 36/90
84077/84077 - 16s - loss: 2.7955e-04 - val_loss: 2.9639e-04 - 16s/epoch - 187us/sample
Epoch 37/90
84077/84077 - 16s - loss: 2.7824e-04 - val_loss: 2.9902e-04 - 16s/epoch - 188us/sample
Epoch 38/90
84077/84077 - 16s - loss: 2.7676e-04 - val_loss: 2.8239e-04 - 16s/epoch - 190us/sample
Epoch 39/90
84077/84077 - 16s - loss: 2.7517e-04 - val_loss: 2.8698e-04 - 16s/epoch - 188us/sample
Epoch 40/90
84077/84077 - 16s - loss: 2.7427e-04 - val_loss: 2.9198e-04 - 16s/epoch - 188us/sample
Epoch 41/90
84077/84077 - 16s - loss: 2.7297e-04 - val_loss: 2.7909e-04 - 16s/epoch - 189us/sample
Epoch 42/90
84077/84077 - 16s - loss: 2.7142e-04 - val_loss: 2.8469e-04 - 16s/epoch - 189us/sample
Epoch 43/90
84077/84077 - 16s - loss: 2.7151e-04 - val_loss: 2.8348e-04 - 16s/epoch - 187us/sample
Epoch 44/90
84077/84077 - 16s - loss: 2.7073e-04 - val_loss: 2.7723e-04 - 16s/epoch - 188us/sample
Epoch 45/90
84077/84077 - 16s - loss: 2.6923e-04 - val_loss: 2.8015e-04 - 16s/epoch - 190us/sample
Epoch 46/90
84077/84077 - 16s - loss: 2.6862e-04 - val_loss: 2.7636e-04 - 16s/epoch - 188us/sample
Epoch 47/90
84077/84077 - 16s - loss: 2.6778e-04 - val_loss: 2.7380e-04 - 16s/epoch - 188us/sample
Epoch 48/90
84077/84077 - 16s - loss: 2.6754e-04 - val_loss: 2.7626e-04 - 16s/epoch - 188us/sample
Epoch 49/90
84077/84077 - 16s - loss: 2.6742e-04 - val_loss: 2.7907e-04 - 16s/epoch - 189us/sample
Epoch 50/90
84077/84077 - 16s - loss: 2.6579e-04 - val_loss: 2.7891e-04 - 16s/epoch - 188us/sample
Epoch 51/90
84077/84077 - 16s - loss: 2.6428e-04 - val_loss: 2.7701e-04 - 16s/epoch - 188us/sample
Epoch 52/90
84077/84077 - 16s - loss: 2.6435e-04 - val_loss: 2.7004e-04 - 16s/epoch - 188us/sample
Epoch 53/90
84077/84077 - 16s - loss: 2.6287e-04 - val_loss: 2.6851e-04 - 16s/epoch - 190us/sample
Epoch 54/90
84077/84077 - 16s - loss: 2.6285e-04 - val_loss: 2.6930e-04 - 16s/epoch - 187us/sample
Epoch 55/90
84077/84077 - 16s - loss: 2.6164e-04 - val_loss: 2.6683e-04 - 16s/epoch - 187us/sample
Epoch 56/90
84077/84077 - 16s - loss: 2.6224e-04 - val_loss: 2.7203e-04 - 16s/epoch - 189us/sample
Epoch 57/90
84077/84077 - 16s - loss: 2.6137e-04 - val_loss: 2.6764e-04 - 16s/epoch - 189us/sample
Epoch 58/90
84077/84077 - 16s - loss: 2.6136e-04 - val_loss: 2.6841e-04 - 16s/epoch - 187us/sample
Epoch 59/90
84077/84077 - 16s - loss: 2.6051e-04 - val_loss: 2.6822e-04 - 16s/epoch - 188us/sample
Epoch 60/90
84077/84077 - 16s - loss: 2.5957e-04 - val_loss: 2.6658e-04 - 16s/epoch - 189us/sample
Epoch 61/90
84077/84077 - 16s - loss: 2.5903e-04 - val_loss: 2.6998e-04 - 16s/epoch - 188us/sample
Epoch 62/90
84077/84077 - 16s - loss: 2.6092e-04 - val_loss: 2.6717e-04 - 16s/epoch - 187us/sample
Epoch 63/90
84077/84077 - 16s - loss: 2.5857e-04 - val_loss: 2.6533e-04 - 16s/epoch - 189us/sample
Epoch 64/90
84077/84077 - 16s - loss: 2.5742e-04 - val_loss: 2.6408e-04 - 16s/epoch - 189us/sample
Epoch 65/90
84077/84077 - 16s - loss: 2.5751e-04 - val_loss: 2.5949e-04 - 16s/epoch - 187us/sample
Epoch 66/90
84077/84077 - 16s - loss: 2.5677e-04 - val_loss: 2.6625e-04 - 16s/epoch - 188us/sample
Epoch 67/90
84077/84077 - 16s - loss: 2.5607e-04 - val_loss: 2.6170e-04 - 16s/epoch - 189us/sample
Epoch 68/90
84077/84077 - 16s - loss: 2.5531e-04 - val_loss: 2.6188e-04 - 16s/epoch - 188us/sample
Epoch 69/90
84077/84077 - 16s - loss: 2.5506e-04 - val_loss: 2.6681e-04 - 16s/epoch - 187us/sample
Epoch 70/90
84077/84077 - 16s - loss: 2.5752e-04 - val_loss: 2.6526e-04 - 16s/epoch - 188us/sample
Epoch 71/90
84077/84077 - 16s - loss: 2.5418e-04 - val_loss: 2.6217e-04 - 16s/epoch - 188us/sample
Epoch 72/90
84077/84077 - 16s - loss: 2.5437e-04 - val_loss: 2.6427e-04 - 16s/epoch - 190us/sample
Epoch 73/90
84077/84077 - 16s - loss: 2.5492e-04 - val_loss: 2.6681e-04 - 16s/epoch - 187us/sample
Epoch 74/90
84077/84077 - 16s - loss: 2.5375e-04 - val_loss: 2.6270e-04 - 16s/epoch - 188us/sample
Epoch 75/90
84077/84077 - 16s - loss: 2.5330e-04 - val_loss: 2.5944e-04 - 16s/epoch - 188us/sample
Epoch 76/90
84077/84077 - 16s - loss: 2.5321e-04 - val_loss: 2.5950e-04 - 16s/epoch - 189us/sample
Epoch 77/90
84077/84077 - 16s - loss: 2.5358e-04 - val_loss: 2.5752e-04 - 16s/epoch - 188us/sample
Epoch 78/90
84077/84077 - 16s - loss: 2.5212e-04 - val_loss: 2.6532e-04 - 16s/epoch - 188us/sample
Epoch 79/90
84077/84077 - 16s - loss: 2.5143e-04 - val_loss: 2.5852e-04 - 16s/epoch - 188us/sample
Epoch 80/90
84077/84077 - 16s - loss: 2.5108e-04 - val_loss: 2.5921e-04 - 16s/epoch - 189us/sample
Epoch 81/90
84077/84077 - 16s - loss: 2.5042e-04 - val_loss: 2.6048e-04 - 16s/epoch - 188us/sample
Epoch 82/90
84077/84077 - 16s - loss: 2.5071e-04 - val_loss: 2.5801e-04 - 16s/epoch - 187us/sample
Epoch 83/90
84077/84077 - 16s - loss: 2.5080e-04 - val_loss: 2.6133e-04 - 16s/epoch - 188us/sample
Epoch 84/90
84077/84077 - 16s - loss: 2.4913e-04 - val_loss: 2.5358e-04 - 16s/epoch - 189us/sample
Epoch 85/90
84077/84077 - 16s - loss: 2.4890e-04 - val_loss: 2.6068e-04 - 16s/epoch - 189us/sample
Epoch 86/90
84077/84077 - 16s - loss: 2.4893e-04 - val_loss: 2.5314e-04 - 16s/epoch - 188us/sample
Epoch 87/90
84077/84077 - 16s - loss: 2.4807e-04 - val_loss: 2.5291e-04 - 16s/epoch - 188us/sample
Epoch 88/90
84077/84077 - 16s - loss: 2.4838e-04 - val_loss: 2.5470e-04 - 16s/epoch - 189us/sample
Epoch 89/90
84077/84077 - 16s - loss: 2.4765e-04 - val_loss: 2.5127e-04 - 16s/epoch - 188us/sample
Epoch 90/90
84077/84077 - 16s - loss: 2.4721e-04 - val_loss: 2.5387e-04 - 16s/epoch - 187us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00025386527932586066
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 18:51:13.842347: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_36/outputlayer/BiasAdd' id:40955 op device:{requested: '', assigned: ''} def:{{{node decoder_model_36/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_36/outputlayer/MatMul, decoder_model_36/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03211402271642552
cosine 0.0317227276085938
MAE: 0.002365288668620203
RMSE: 0.012219130621527416
r2: 0.8834237700252514
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 90, 0.0006000000000000001, 0.2, 188, 0.0002472133492258073, 0.00025386527932586066, 0.03211402271642552, 0.0317227276085938, 0.002365288668620203, 0.012219130621527416, 0.8834237700252514, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 85 0.0008 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_96 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_96 (ReLU)                (None, 1697)         0           ['batch_normalization_96[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE1.7999999999999998_cr0.2_bs32_ep85_loss_mse_lr0.0008_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 18:51:31.612423: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_37/kernel/Assign' id:41952 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_37/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_37/kernel, dense_dec1_37/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 18:51:38.121130: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_38/bias/v/Assign' id:43009 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_38/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_38/bias/v, dense_dec1_38/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 18:51:44.569917: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_38/outputlayer/BiasAdd' id:42657 op device:{requested: '', assigned: ''} def:{{{node decoder_model_38/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_38/outputlayer/MatMul, decoder_model_38/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023569856568572804
cosine 0.023288054397606475
MAE: 0.0020727265292934637
RMSE: 0.009467292635522595
r2: 0.9301518921114655
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['1.7999999999999998custom_VAE', 'mse', 32, 85, 0.0008, 0.2, 188, '--', '--', 0.023569856568572804, 0.023288054397606475, 0.0020727265292934637, 0.009467292635522595, 0.9301518921114655, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0006000000000000001 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_99 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_99 (ReLU)                (None, 1791)         0           ['batch_normalization_99[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 18:52:01.511200: W tensorflow/c/c_api.cc:291] Operation '{name:'training_54/Adam/dense_dec0_39/bias/m/Assign' id:44225 op device:{requested: '', assigned: ''} def:{{{node training_54/Adam/dense_dec0_39/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_54/Adam/dense_dec0_39/bias/m, training_54/Adam/dense_dec0_39/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 18:52:15.542256: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_39/mul' id:43668 op device:{requested: '', assigned: ''} def:{{{node loss_39/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_39/mul/x, loss_39/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0037 - val_loss: 0.0010 - 20s/epoch - 240us/sample
Epoch 2/90
84077/84077 - 11s - loss: 8.2454e-04 - val_loss: 8.2606e-04 - 11s/epoch - 128us/sample
Epoch 3/90
84077/84077 - 11s - loss: 0.0072 - val_loss: 6.9843e-04 - 11s/epoch - 129us/sample
Epoch 4/90
84077/84077 - 11s - loss: 6.7543e-04 - val_loss: 6.5065e-04 - 11s/epoch - 129us/sample
Epoch 5/90
84077/84077 - 11s - loss: 5.9646e-04 - val_loss: 4.7639e-04 - 11s/epoch - 128us/sample
Epoch 6/90
84077/84077 - 11s - loss: 5.2507e-04 - val_loss: 3.9005e-04 - 11s/epoch - 128us/sample
Epoch 7/90
84077/84077 - 11s - loss: 4.0416e-04 - val_loss: 3.4663e-04 - 11s/epoch - 128us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.8442e-04 - val_loss: 3.3755e-04 - 11s/epoch - 128us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.3076e-04 - val_loss: 2.9200e-04 - 11s/epoch - 128us/sample
Epoch 10/90
84077/84077 - 11s - loss: 3.0928e-04 - val_loss: 2.6030e-04 - 11s/epoch - 128us/sample
Epoch 11/90
84077/84077 - 11s - loss: 2.8523e-04 - val_loss: 2.5894e-04 - 11s/epoch - 130us/sample
Epoch 12/90
84077/84077 - 11s - loss: 2.6542e-04 - val_loss: 2.2364e-04 - 11s/epoch - 128us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.5182e-04 - val_loss: 2.1106e-04 - 11s/epoch - 128us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.3789e-04 - val_loss: 2.0446e-04 - 11s/epoch - 128us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.2394e-04 - val_loss: 1.9372e-04 - 11s/epoch - 128us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.1426e-04 - val_loss: 1.8971e-04 - 11s/epoch - 129us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.0574e-04 - val_loss: 1.8368e-04 - 11s/epoch - 130us/sample
Epoch 18/90
84077/84077 - 11s - loss: 1.9749e-04 - val_loss: 1.7464e-04 - 11s/epoch - 128us/sample
Epoch 19/90
84077/84077 - 11s - loss: 1.9149e-04 - val_loss: 1.6808e-04 - 11s/epoch - 128us/sample
Epoch 20/90
84077/84077 - 11s - loss: 1.8826e-04 - val_loss: 1.6701e-04 - 11s/epoch - 128us/sample
Epoch 21/90
84077/84077 - 11s - loss: 1.8251e-04 - val_loss: 1.6279e-04 - 11s/epoch - 128us/sample
Epoch 22/90
84077/84077 - 11s - loss: 1.7912e-04 - val_loss: 1.6357e-04 - 11s/epoch - 129us/sample
Epoch 23/90
84077/84077 - 11s - loss: 1.7712e-04 - val_loss: 1.5918e-04 - 11s/epoch - 129us/sample
Epoch 24/90
84077/84077 - 11s - loss: 1.7277e-04 - val_loss: 1.5537e-04 - 11s/epoch - 128us/sample
Epoch 25/90
84077/84077 - 11s - loss: 1.7065e-04 - val_loss: 1.5222e-04 - 11s/epoch - 128us/sample
Epoch 26/90
84077/84077 - 11s - loss: 1.6837e-04 - val_loss: 1.5117e-04 - 11s/epoch - 128us/sample
Epoch 27/90
84077/84077 - 11s - loss: 1.6653e-04 - val_loss: 1.5055e-04 - 11s/epoch - 128us/sample
Epoch 28/90
84077/84077 - 11s - loss: 1.6471e-04 - val_loss: 1.4967e-04 - 11s/epoch - 130us/sample
Epoch 29/90
84077/84077 - 11s - loss: 1.6259e-04 - val_loss: 1.4684e-04 - 11s/epoch - 128us/sample
Epoch 30/90
84077/84077 - 11s - loss: 1.6137e-04 - val_loss: 1.4427e-04 - 11s/epoch - 128us/sample
Epoch 31/90
84077/84077 - 11s - loss: 1.6016e-04 - val_loss: 1.4520e-04 - 11s/epoch - 128us/sample
Epoch 32/90
84077/84077 - 11s - loss: 1.5891e-04 - val_loss: 1.4348e-04 - 11s/epoch - 128us/sample
Epoch 33/90
84077/84077 - 11s - loss: 1.5736e-04 - val_loss: 1.4466e-04 - 11s/epoch - 130us/sample
Epoch 34/90
84077/84077 - 11s - loss: 1.5603e-04 - val_loss: 1.4197e-04 - 11s/epoch - 128us/sample
Epoch 35/90
84077/84077 - 11s - loss: 1.5555e-04 - val_loss: 1.4079e-04 - 11s/epoch - 128us/sample
Epoch 36/90
84077/84077 - 11s - loss: 1.5435e-04 - val_loss: 1.3999e-04 - 11s/epoch - 128us/sample
Epoch 37/90
84077/84077 - 11s - loss: 1.5280e-04 - val_loss: 1.3979e-04 - 11s/epoch - 128us/sample
Epoch 38/90
84077/84077 - 11s - loss: 1.5215e-04 - val_loss: 1.3841e-04 - 11s/epoch - 129us/sample
Epoch 39/90
84077/84077 - 11s - loss: 1.5142e-04 - val_loss: 1.3812e-04 - 11s/epoch - 129us/sample
Epoch 40/90
84077/84077 - 11s - loss: 1.5084e-04 - val_loss: 1.3801e-04 - 11s/epoch - 128us/sample
Epoch 41/90
84077/84077 - 11s - loss: 1.5042e-04 - val_loss: 1.3780e-04 - 11s/epoch - 128us/sample
Epoch 42/90
84077/84077 - 11s - loss: 1.4987e-04 - val_loss: 1.3751e-04 - 11s/epoch - 128us/sample
Epoch 43/90
84077/84077 - 11s - loss: 1.4858e-04 - val_loss: 1.3647e-04 - 11s/epoch - 129us/sample
Epoch 44/90
84077/84077 - 11s - loss: 1.4770e-04 - val_loss: 1.3608e-04 - 11s/epoch - 129us/sample
Epoch 45/90
84077/84077 - 11s - loss: 1.4751e-04 - val_loss: 1.3578e-04 - 11s/epoch - 128us/sample
Epoch 46/90
84077/84077 - 11s - loss: 1.4682e-04 - val_loss: 1.3470e-04 - 11s/epoch - 128us/sample
Epoch 47/90
84077/84077 - 11s - loss: 1.4634e-04 - val_loss: 1.3417e-04 - 11s/epoch - 128us/sample
Epoch 48/90
84077/84077 - 11s - loss: 1.4582e-04 - val_loss: 1.3455e-04 - 11s/epoch - 128us/sample
Epoch 49/90
84077/84077 - 11s - loss: 1.4565e-04 - val_loss: 1.3381e-04 - 11s/epoch - 130us/sample
Epoch 50/90
84077/84077 - 11s - loss: 1.4463e-04 - val_loss: 1.3313e-04 - 11s/epoch - 128us/sample
Epoch 51/90
84077/84077 - 11s - loss: 1.4427e-04 - val_loss: 1.3354e-04 - 11s/epoch - 128us/sample
Epoch 52/90
84077/84077 - 11s - loss: 1.4362e-04 - val_loss: 1.3381e-04 - 11s/epoch - 128us/sample
Epoch 53/90
84077/84077 - 11s - loss: 1.4360e-04 - val_loss: 1.3264e-04 - 11s/epoch - 128us/sample
Epoch 54/90
84077/84077 - 11s - loss: 1.4241e-04 - val_loss: 1.3210e-04 - 11s/epoch - 128us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.4230e-04 - val_loss: 1.3106e-04 - 11s/epoch - 129us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.4207e-04 - val_loss: 1.3217e-04 - 11s/epoch - 128us/sample
Epoch 57/90
84077/84077 - 11s - loss: 1.4273e-04 - val_loss: 1.3163e-04 - 11s/epoch - 128us/sample
Epoch 58/90
84077/84077 - 11s - loss: 1.4155e-04 - val_loss: 1.3062e-04 - 11s/epoch - 128us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.4038e-04 - val_loss: 1.2959e-04 - 11s/epoch - 128us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.4052e-04 - val_loss: 1.2956e-04 - 11s/epoch - 130us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.3997e-04 - val_loss: 1.3007e-04 - 11s/epoch - 128us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.3997e-04 - val_loss: 1.3010e-04 - 11s/epoch - 128us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.3945e-04 - val_loss: 1.2943e-04 - 11s/epoch - 128us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.3911e-04 - val_loss: 1.3029e-04 - 11s/epoch - 128us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.3927e-04 - val_loss: 1.2942e-04 - 11s/epoch - 129us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.3874e-04 - val_loss: 1.2917e-04 - 11s/epoch - 129us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.3818e-04 - val_loss: 1.2886e-04 - 11s/epoch - 128us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.3813e-04 - val_loss: 1.2824e-04 - 11s/epoch - 128us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.3771e-04 - val_loss: 1.2906e-04 - 11s/epoch - 128us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.3761e-04 - val_loss: 1.2758e-04 - 11s/epoch - 129us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.3732e-04 - val_loss: 1.2719e-04 - 11s/epoch - 130us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.3679e-04 - val_loss: 1.2821e-04 - 11s/epoch - 128us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.3692e-04 - val_loss: 1.2813e-04 - 11s/epoch - 128us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.3710e-04 - val_loss: 1.2682e-04 - 11s/epoch - 128us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.3655e-04 - val_loss: 1.2797e-04 - 11s/epoch - 128us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.3630e-04 - val_loss: 1.2695e-04 - 11s/epoch - 130us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.3612e-04 - val_loss: 1.2765e-04 - 11s/epoch - 128us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.3601e-04 - val_loss: 1.2613e-04 - 11s/epoch - 128us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.3570e-04 - val_loss: 1.2646e-04 - 11s/epoch - 128us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.3543e-04 - val_loss: 1.2676e-04 - 11s/epoch - 128us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.3513e-04 - val_loss: 1.2572e-04 - 11s/epoch - 129us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.3488e-04 - val_loss: 1.2600e-04 - 11s/epoch - 130us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.3493e-04 - val_loss: 1.2557e-04 - 11s/epoch - 128us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.3466e-04 - val_loss: 1.2655e-04 - 11s/epoch - 128us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.3483e-04 - val_loss: 1.2570e-04 - 11s/epoch - 128us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.3435e-04 - val_loss: 1.2581e-04 - 11s/epoch - 128us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.3423e-04 - val_loss: 1.2494e-04 - 11s/epoch - 129us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.3366e-04 - val_loss: 1.2459e-04 - 11s/epoch - 129us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.3368e-04 - val_loss: 1.2569e-04 - 11s/epoch - 128us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.3357e-04 - val_loss: 1.2638e-04 - 11s/epoch - 128us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001263778179389634
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 19:08:18.924050: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_39/outputlayer/BiasAdd' id:43632 op device:{requested: '', assigned: ''} def:{{{node decoder_model_39/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_39/outputlayer/MatMul, decoder_model_39/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03236633654807762
cosine 0.03197519990253945
MAE: 0.0024260948415347266
RMSE: 0.01018159010179492
r2: 0.919194325135168
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 90, 0.0006000000000000001, 0.2, 188, 0.00013357467148605666, 0.0001263778179389634, 0.03236633654807762, 0.03197519990253945, 0.0024260948415347266, 0.01018159010179492, 0.919194325135168, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 3
Fitness    = 511.49230331698664
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 511.49230331698664.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664]
[1.7999999999999998 85 0.0006000000000000001 32 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_102 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_102 (ReLU)               (None, 1697)         0           ['batch_normalization_102[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 19:08:37.262672: W tensorflow/c/c_api.cc:291] Operation '{name:'training_56/Adam/learning_rate/Assign' id:45422 op device:{requested: '', assigned: ''} def:{{{node training_56/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_56/Adam/learning_rate, training_56/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 19:08:56.691420: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_40/mul' id:44953 op device:{requested: '', assigned: ''} def:{{{node loss_40/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_40/mul/x, loss_40/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 26s - loss: 0.0055 - val_loss: 8.7616e-04 - 26s/epoch - 311us/sample
Epoch 2/85
84077/84077 - 16s - loss: 7.7105e-04 - val_loss: 0.0022 - 16s/epoch - 193us/sample
Epoch 3/85
84077/84077 - 16s - loss: 6.0566e-04 - val_loss: 4.6549e-04 - 16s/epoch - 193us/sample
Epoch 4/85
84077/84077 - 16s - loss: 5.7999e-04 - val_loss: 4.7187e-04 - 16s/epoch - 194us/sample
Epoch 5/85
84077/84077 - 16s - loss: 5.3025e-04 - val_loss: 4.2757e-04 - 16s/epoch - 195us/sample
Epoch 6/85
84077/84077 - 16s - loss: 4.4862e-04 - val_loss: 3.2359e-04 - 16s/epoch - 193us/sample
Epoch 7/85
84077/84077 - 16s - loss: 3.6414e-04 - val_loss: 2.9935e-04 - 16s/epoch - 193us/sample
Epoch 8/85
84077/84077 - 16s - loss: 3.2561e-04 - val_loss: 2.8197e-04 - 16s/epoch - 194us/sample
Epoch 9/85
84077/84077 - 16s - loss: 3.0409e-04 - val_loss: 2.5873e-04 - 16s/epoch - 196us/sample
Epoch 10/85
84077/84077 - 16s - loss: 2.8595e-04 - val_loss: 2.4604e-04 - 16s/epoch - 193us/sample
Epoch 11/85
84077/84077 - 16s - loss: 2.7257e-04 - val_loss: 2.4100e-04 - 16s/epoch - 193us/sample
Epoch 12/85
84077/84077 - 16s - loss: 2.6342e-04 - val_loss: 2.3154e-04 - 16s/epoch - 194us/sample
Epoch 13/85
84077/84077 - 16s - loss: 2.5634e-04 - val_loss: 2.2536e-04 - 16s/epoch - 195us/sample
Epoch 14/85
84077/84077 - 16s - loss: 2.5017e-04 - val_loss: 2.2174e-04 - 16s/epoch - 193us/sample
Epoch 15/85
84077/84077 - 16s - loss: 2.4502e-04 - val_loss: 2.1833e-04 - 16s/epoch - 193us/sample
Epoch 16/85
84077/84077 - 16s - loss: 2.4125e-04 - val_loss: 2.1405e-04 - 16s/epoch - 194us/sample
Epoch 17/85
84077/84077 - 16s - loss: 2.3761e-04 - val_loss: 2.0999e-04 - 16s/epoch - 195us/sample
Epoch 18/85
84077/84077 - 16s - loss: 2.3452e-04 - val_loss: 2.0728e-04 - 16s/epoch - 193us/sample
Epoch 19/85
84077/84077 - 16s - loss: 2.3215e-04 - val_loss: 2.0552e-04 - 16s/epoch - 193us/sample
Epoch 20/85
84077/84077 - 16s - loss: 2.2921e-04 - val_loss: 2.0355e-04 - 16s/epoch - 194us/sample
Epoch 21/85
84077/84077 - 16s - loss: 2.2742e-04 - val_loss: 2.0161e-04 - 16s/epoch - 195us/sample
Epoch 22/85
84077/84077 - 16s - loss: 2.2492e-04 - val_loss: 1.9930e-04 - 16s/epoch - 194us/sample
Epoch 23/85
84077/84077 - 16s - loss: 2.2176e-04 - val_loss: 1.9681e-04 - 16s/epoch - 193us/sample
Epoch 24/85
84077/84077 - 16s - loss: 2.1924e-04 - val_loss: 1.9513e-04 - 16s/epoch - 194us/sample
Epoch 25/85
84077/84077 - 16s - loss: 2.1730e-04 - val_loss: 1.9346e-04 - 16s/epoch - 195us/sample
Epoch 26/85
84077/84077 - 16s - loss: 2.1488e-04 - val_loss: 1.9099e-04 - 16s/epoch - 193us/sample
Epoch 27/85
84077/84077 - 16s - loss: 2.1346e-04 - val_loss: 1.9056e-04 - 16s/epoch - 194us/sample
Epoch 28/85
84077/84077 - 16s - loss: 2.1106e-04 - val_loss: 1.8845e-04 - 16s/epoch - 194us/sample
Epoch 29/85
84077/84077 - 16s - loss: 2.0879e-04 - val_loss: 1.8508e-04 - 16s/epoch - 195us/sample
Epoch 30/85
84077/84077 - 16s - loss: 2.0678e-04 - val_loss: 1.8500e-04 - 16s/epoch - 194us/sample
Epoch 31/85
84077/84077 - 16s - loss: 2.0647e-04 - val_loss: 1.8441e-04 - 16s/epoch - 193us/sample
Epoch 32/85
84077/84077 - 16s - loss: 2.0462e-04 - val_loss: 1.8192e-04 - 16s/epoch - 194us/sample
Epoch 33/85
84077/84077 - 16s - loss: 2.0222e-04 - val_loss: 1.7955e-04 - 16s/epoch - 195us/sample
Epoch 34/85
84077/84077 - 16s - loss: 2.0027e-04 - val_loss: 1.7771e-04 - 16s/epoch - 194us/sample
Epoch 35/85
84077/84077 - 16s - loss: 1.9845e-04 - val_loss: 1.7720e-04 - 16s/epoch - 193us/sample
Epoch 36/85
84077/84077 - 16s - loss: 1.9680e-04 - val_loss: 1.7533e-04 - 16s/epoch - 193us/sample
Epoch 37/85
84077/84077 - 16s - loss: 1.9502e-04 - val_loss: 1.7474e-04 - 16s/epoch - 194us/sample
Epoch 38/85
84077/84077 - 16s - loss: 1.9370e-04 - val_loss: 1.7369e-04 - 16s/epoch - 195us/sample
Epoch 39/85
84077/84077 - 16s - loss: 1.9276e-04 - val_loss: 1.7410e-04 - 16s/epoch - 193us/sample
Epoch 40/85
84077/84077 - 16s - loss: 1.9166e-04 - val_loss: 1.7220e-04 - 16s/epoch - 193us/sample
Epoch 41/85
84077/84077 - 16s - loss: 1.9017e-04 - val_loss: 1.6928e-04 - 16s/epoch - 194us/sample
Epoch 42/85
84077/84077 - 16s - loss: 1.8965e-04 - val_loss: 1.6998e-04 - 16s/epoch - 195us/sample
Epoch 43/85
84077/84077 - 16s - loss: 1.8795e-04 - val_loss: 1.6866e-04 - 16s/epoch - 194us/sample
Epoch 44/85
84077/84077 - 16s - loss: 1.8656e-04 - val_loss: 1.6746e-04 - 16s/epoch - 193us/sample
Epoch 45/85
84077/84077 - 16s - loss: 1.8562e-04 - val_loss: 1.6529e-04 - 16s/epoch - 194us/sample
Epoch 46/85
84077/84077 - 16s - loss: 1.8443e-04 - val_loss: 1.6799e-04 - 16s/epoch - 195us/sample
Epoch 47/85
84077/84077 - 16s - loss: 1.8287e-04 - val_loss: 1.6441e-04 - 16s/epoch - 194us/sample
Epoch 48/85
84077/84077 - 16s - loss: 1.8248e-04 - val_loss: 1.6458e-04 - 16s/epoch - 193us/sample
Epoch 49/85
84077/84077 - 16s - loss: 1.8131e-04 - val_loss: 1.6324e-04 - 16s/epoch - 194us/sample
Epoch 50/85
84077/84077 - 16s - loss: 1.8089e-04 - val_loss: 1.6211e-04 - 16s/epoch - 195us/sample
Epoch 51/85
84077/84077 - 16s - loss: 1.7999e-04 - val_loss: 1.6154e-04 - 16s/epoch - 194us/sample
Epoch 52/85
84077/84077 - 16s - loss: 1.7902e-04 - val_loss: 1.6102e-04 - 16s/epoch - 193us/sample
Epoch 53/85
84077/84077 - 16s - loss: 1.7850e-04 - val_loss: 1.6190e-04 - 16s/epoch - 193us/sample
Epoch 54/85
84077/84077 - 16s - loss: 1.7753e-04 - val_loss: 1.5983e-04 - 16s/epoch - 194us/sample
Epoch 55/85
84077/84077 - 16s - loss: 1.7672e-04 - val_loss: 1.5890e-04 - 16s/epoch - 194us/sample
Epoch 56/85
84077/84077 - 16s - loss: 1.7567e-04 - val_loss: 1.5731e-04 - 16s/epoch - 193us/sample
Epoch 57/85
84077/84077 - 16s - loss: 1.7516e-04 - val_loss: 1.5632e-04 - 16s/epoch - 194us/sample
Epoch 58/85
84077/84077 - 16s - loss: 1.7422e-04 - val_loss: 1.5720e-04 - 16s/epoch - 194us/sample
Epoch 59/85
84077/84077 - 16s - loss: 1.7420e-04 - val_loss: 1.5687e-04 - 16s/epoch - 194us/sample
Epoch 60/85
84077/84077 - 16s - loss: 1.7362e-04 - val_loss: 1.5581e-04 - 16s/epoch - 193us/sample
Epoch 61/85
84077/84077 - 16s - loss: 1.7295e-04 - val_loss: 1.5616e-04 - 16s/epoch - 194us/sample
Epoch 62/85
84077/84077 - 16s - loss: 1.7211e-04 - val_loss: 1.5465e-04 - 16s/epoch - 195us/sample
Epoch 63/85
84077/84077 - 16s - loss: 1.7251e-04 - val_loss: 1.5446e-04 - 16s/epoch - 194us/sample
Epoch 64/85
84077/84077 - 16s - loss: 1.7125e-04 - val_loss: 1.5430e-04 - 16s/epoch - 193us/sample
Epoch 65/85
84077/84077 - 16s - loss: 1.7140e-04 - val_loss: 1.5464e-04 - 16s/epoch - 194us/sample
Epoch 66/85
84077/84077 - 16s - loss: 1.7079e-04 - val_loss: 1.5385e-04 - 16s/epoch - 196us/sample
Epoch 67/85
84077/84077 - 16s - loss: 1.7039e-04 - val_loss: 1.5332e-04 - 16s/epoch - 194us/sample
Epoch 68/85
84077/84077 - 16s - loss: 1.7025e-04 - val_loss: 1.5318e-04 - 16s/epoch - 193us/sample
Epoch 69/85
84077/84077 - 16s - loss: 1.6951e-04 - val_loss: 1.5308e-04 - 16s/epoch - 194us/sample
Epoch 70/85
84077/84077 - 16s - loss: 1.6939e-04 - val_loss: 1.5216e-04 - 16s/epoch - 195us/sample
Epoch 71/85
84077/84077 - 16s - loss: 1.6868e-04 - val_loss: 1.5116e-04 - 16s/epoch - 193us/sample
Epoch 72/85
84077/84077 - 16s - loss: 1.6839e-04 - val_loss: 1.5223e-04 - 16s/epoch - 193us/sample
Epoch 73/85
84077/84077 - 16s - loss: 1.6748e-04 - val_loss: 1.5073e-04 - 16s/epoch - 194us/sample
Epoch 74/85
84077/84077 - 16s - loss: 1.6724e-04 - val_loss: 1.5137e-04 - 16s/epoch - 195us/sample
Epoch 75/85
84077/84077 - 16s - loss: 1.6698e-04 - val_loss: 1.5066e-04 - 16s/epoch - 194us/sample
Epoch 76/85
84077/84077 - 16s - loss: 1.6654e-04 - val_loss: 1.4904e-04 - 16s/epoch - 194us/sample
Epoch 77/85
84077/84077 - 16s - loss: 1.6639e-04 - val_loss: 1.5596e-04 - 16s/epoch - 194us/sample
Epoch 78/85
84077/84077 - 16s - loss: 1.6569e-04 - val_loss: 1.5054e-04 - 16s/epoch - 195us/sample
Epoch 79/85
84077/84077 - 16s - loss: 1.6534e-04 - val_loss: 1.4841e-04 - 16s/epoch - 194us/sample
Epoch 80/85
84077/84077 - 16s - loss: 1.6563e-04 - val_loss: 1.4863e-04 - 16s/epoch - 194us/sample
Epoch 81/85
84077/84077 - 16s - loss: 1.6517e-04 - val_loss: 1.4804e-04 - 16s/epoch - 195us/sample
Epoch 82/85
84077/84077 - 16s - loss: 1.6519e-04 - val_loss: 1.4815e-04 - 16s/epoch - 195us/sample
Epoch 83/85
84077/84077 - 16s - loss: 1.6451e-04 - val_loss: 1.4979e-04 - 16s/epoch - 193us/sample
Epoch 84/85
84077/84077 - 16s - loss: 1.6393e-04 - val_loss: 1.4744e-04 - 16s/epoch - 193us/sample
Epoch 85/85
84077/84077 - 16s - loss: 1.6480e-04 - val_loss: 1.4752e-04 - 16s/epoch - 195us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00014752410663393698
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 19:31:50.300740: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_40/outputlayer/BiasAdd' id:44917 op device:{requested: '', assigned: ''} def:{{{node decoder_model_40/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_40/outputlayer/MatMul, decoder_model_40/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0434628253904544
cosine 0.04293633562533987
MAE: 0.002687795727616469
RMSE: 0.01211864010088764
r2: 0.885303621797385
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 32, 85, 0.0006000000000000001, 0.2, 188, 0.00016479933749440062, 0.00014752410663393698, 0.0434628253904544, 0.04293633562533987, 0.002687795727616469, 0.01211864010088764, 0.885303621797385, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0004000000000000001 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_105 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_105 (ReLU)               (None, 1697)         0           ['batch_normalization_105[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 19:32:08.824700: W tensorflow/c/c_api.cc:291] Operation '{name:'training_58/Adam/dense_enc0_41/kernel/v/Assign' id:46828 op device:{requested: '', assigned: ''} def:{{{node training_58/Adam/dense_enc0_41/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_58/Adam/dense_enc0_41/kernel/v, training_58/Adam/dense_enc0_41/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 19:32:23.220324: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_41/mul' id:46238 op device:{requested: '', assigned: ''} def:{{{node loss_41/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_41/mul/x, loss_41/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0036 - val_loss: 8.4083e-04 - 21s/epoch - 249us/sample
Epoch 2/90
84077/84077 - 11s - loss: 9.9498e-04 - val_loss: 8.2802e-04 - 11s/epoch - 130us/sample
Epoch 3/90
84077/84077 - 11s - loss: 9.4693e-04 - val_loss: 6.3865e-04 - 11s/epoch - 131us/sample
Epoch 4/90
84077/84077 - 11s - loss: 6.6003e-04 - val_loss: 6.7310e-04 - 11s/epoch - 130us/sample
Epoch 5/90
84077/84077 - 11s - loss: 6.4200e-04 - val_loss: 0.0033 - 11s/epoch - 129us/sample
Epoch 6/90
84077/84077 - 11s - loss: 5.6012e-04 - val_loss: 4.4705e-04 - 11s/epoch - 129us/sample
Epoch 7/90
84077/84077 - 11s - loss: 4.0636e-04 - val_loss: 3.3417e-04 - 11s/epoch - 129us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.4449e-04 - val_loss: 2.9652e-04 - 11s/epoch - 129us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.1539e-04 - val_loss: 2.6706e-04 - 11s/epoch - 129us/sample
Epoch 10/90
84077/84077 - 11s - loss: 2.9448e-04 - val_loss: 2.4659e-04 - 11s/epoch - 131us/sample
Epoch 11/90
84077/84077 - 11s - loss: 2.7081e-04 - val_loss: 2.3182e-04 - 11s/epoch - 130us/sample
Epoch 12/90
84077/84077 - 11s - loss: 2.5343e-04 - val_loss: 2.1468e-04 - 11s/epoch - 129us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.3882e-04 - val_loss: 2.0603e-04 - 11s/epoch - 129us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.2687e-04 - val_loss: 1.9605e-04 - 11s/epoch - 129us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.1801e-04 - val_loss: 1.8957e-04 - 11s/epoch - 132us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.0701e-04 - val_loss: 1.7867e-04 - 11s/epoch - 130us/sample
Epoch 17/90
84077/84077 - 11s - loss: 1.9848e-04 - val_loss: 1.7355e-04 - 11s/epoch - 129us/sample
Epoch 18/90
84077/84077 - 11s - loss: 1.9202e-04 - val_loss: 1.6783e-04 - 11s/epoch - 129us/sample
Epoch 19/90
84077/84077 - 11s - loss: 1.8684e-04 - val_loss: 1.6498e-04 - 11s/epoch - 129us/sample
Epoch 20/90
84077/84077 - 11s - loss: 1.8374e-04 - val_loss: 1.5981e-04 - 11s/epoch - 131us/sample
Epoch 21/90
84077/84077 - 11s - loss: 1.7884e-04 - val_loss: 1.5786e-04 - 11s/epoch - 131us/sample
Epoch 22/90
84077/84077 - 11s - loss: 1.7635e-04 - val_loss: 1.5530e-04 - 11s/epoch - 129us/sample
Epoch 23/90
84077/84077 - 11s - loss: 1.7274e-04 - val_loss: 1.5510e-04 - 11s/epoch - 129us/sample
Epoch 24/90
84077/84077 - 11s - loss: 1.7040e-04 - val_loss: 1.5185e-04 - 11s/epoch - 129us/sample
Epoch 25/90
84077/84077 - 11s - loss: 1.6831e-04 - val_loss: 1.5107e-04 - 11s/epoch - 130us/sample
Epoch 26/90
84077/84077 - 11s - loss: 1.6594e-04 - val_loss: 1.5001e-04 - 11s/epoch - 132us/sample
Epoch 27/90
84077/84077 - 11s - loss: 1.6403e-04 - val_loss: 1.4947e-04 - 11s/epoch - 129us/sample
Epoch 28/90
84077/84077 - 11s - loss: 1.6248e-04 - val_loss: 1.4634e-04 - 11s/epoch - 129us/sample
Epoch 29/90
84077/84077 - 11s - loss: 1.6099e-04 - val_loss: 1.4614e-04 - 11s/epoch - 129us/sample
Epoch 30/90
84077/84077 - 11s - loss: 1.5974e-04 - val_loss: 1.4532e-04 - 11s/epoch - 129us/sample
Epoch 31/90
84077/84077 - 11s - loss: 1.5878e-04 - val_loss: 1.4415e-04 - 11s/epoch - 131us/sample
Epoch 32/90
84077/84077 - 11s - loss: 1.5678e-04 - val_loss: 1.4238e-04 - 11s/epoch - 130us/sample
Epoch 33/90
84077/84077 - 11s - loss: 1.5579e-04 - val_loss: 1.4168e-04 - 11s/epoch - 129us/sample
Epoch 34/90
84077/84077 - 11s - loss: 1.5580e-04 - val_loss: 1.4189e-04 - 11s/epoch - 129us/sample
Epoch 35/90
84077/84077 - 11s - loss: 1.5436e-04 - val_loss: 1.4041e-04 - 11s/epoch - 129us/sample
Epoch 36/90
84077/84077 - 11s - loss: 1.5358e-04 - val_loss: 1.4075e-04 - 11s/epoch - 129us/sample
Epoch 37/90
84077/84077 - 11s - loss: 1.5225e-04 - val_loss: 1.3999e-04 - 11s/epoch - 132us/sample
Epoch 38/90
84077/84077 - 11s - loss: 1.5150e-04 - val_loss: 1.3995e-04 - 11s/epoch - 129us/sample
Epoch 39/90
84077/84077 - 11s - loss: 1.5073e-04 - val_loss: 1.3742e-04 - 11s/epoch - 129us/sample
Epoch 40/90
84077/84077 - 11s - loss: 1.5004e-04 - val_loss: 1.3748e-04 - 11s/epoch - 129us/sample
Epoch 41/90
84077/84077 - 11s - loss: 1.4885e-04 - val_loss: 1.3553e-04 - 11s/epoch - 129us/sample
Epoch 42/90
84077/84077 - 11s - loss: 1.4868e-04 - val_loss: 1.3689e-04 - 11s/epoch - 131us/sample
Epoch 43/90
84077/84077 - 11s - loss: 1.4803e-04 - val_loss: 1.3584e-04 - 11s/epoch - 130us/sample
Epoch 44/90
84077/84077 - 11s - loss: 1.4746e-04 - val_loss: 1.3586e-04 - 11s/epoch - 129us/sample
Epoch 45/90
84077/84077 - 11s - loss: 1.4675e-04 - val_loss: 1.3581e-04 - 11s/epoch - 129us/sample
Epoch 46/90
84077/84077 - 11s - loss: 1.4594e-04 - val_loss: 1.3481e-04 - 11s/epoch - 129us/sample
Epoch 47/90
84077/84077 - 11s - loss: 1.4540e-04 - val_loss: 1.3373e-04 - 11s/epoch - 130us/sample
Epoch 48/90
84077/84077 - 11s - loss: 1.4567e-04 - val_loss: 1.3371e-04 - 11s/epoch - 132us/sample
Epoch 49/90
84077/84077 - 11s - loss: 1.4470e-04 - val_loss: 1.3387e-04 - 11s/epoch - 129us/sample
Epoch 50/90
84077/84077 - 11s - loss: 1.4406e-04 - val_loss: 1.3364e-04 - 11s/epoch - 129us/sample
Epoch 51/90
84077/84077 - 11s - loss: 1.4386e-04 - val_loss: 1.3289e-04 - 11s/epoch - 129us/sample
Epoch 52/90
84077/84077 - 11s - loss: 1.4311e-04 - val_loss: 1.3245e-04 - 11s/epoch - 129us/sample
Epoch 53/90
84077/84077 - 11s - loss: 1.4256e-04 - val_loss: 1.3301e-04 - 11s/epoch - 132us/sample
Epoch 54/90
84077/84077 - 11s - loss: 1.4251e-04 - val_loss: 1.3228e-04 - 11s/epoch - 130us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.4158e-04 - val_loss: 1.3203e-04 - 11s/epoch - 129us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.4110e-04 - val_loss: 1.3140e-04 - 11s/epoch - 129us/sample
Epoch 57/90
84077/84077 - 11s - loss: 1.4087e-04 - val_loss: 1.3119e-04 - 11s/epoch - 129us/sample
Epoch 58/90
84077/84077 - 11s - loss: 1.4077e-04 - val_loss: 1.3062e-04 - 11s/epoch - 131us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.4084e-04 - val_loss: 1.3269e-04 - 11s/epoch - 132us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.4039e-04 - val_loss: 1.3107e-04 - 11s/epoch - 129us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.3972e-04 - val_loss: 1.3102e-04 - 11s/epoch - 129us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.3935e-04 - val_loss: 1.2961e-04 - 11s/epoch - 129us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.3903e-04 - val_loss: 1.2980e-04 - 11s/epoch - 129us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.3871e-04 - val_loss: 1.3031e-04 - 11s/epoch - 132us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.3846e-04 - val_loss: 1.2902e-04 - 11s/epoch - 130us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.3836e-04 - val_loss: 1.3001e-04 - 11s/epoch - 129us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.3801e-04 - val_loss: 1.2898e-04 - 11s/epoch - 129us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.3778e-04 - val_loss: 1.2917e-04 - 11s/epoch - 129us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.3725e-04 - val_loss: 1.2854e-04 - 11s/epoch - 131us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.3749e-04 - val_loss: 1.2876e-04 - 11s/epoch - 132us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.3712e-04 - val_loss: 1.2861e-04 - 11s/epoch - 129us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.3637e-04 - val_loss: 1.3005e-04 - 11s/epoch - 129us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.3674e-04 - val_loss: 1.2888e-04 - 11s/epoch - 129us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.3625e-04 - val_loss: 1.2792e-04 - 11s/epoch - 129us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.3591e-04 - val_loss: 1.2770e-04 - 11s/epoch - 132us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.3587e-04 - val_loss: 1.2862e-04 - 11s/epoch - 130us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.3586e-04 - val_loss: 1.2755e-04 - 11s/epoch - 129us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.3514e-04 - val_loss: 1.2847e-04 - 11s/epoch - 129us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.3506e-04 - val_loss: 1.2692e-04 - 11s/epoch - 129us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.3474e-04 - val_loss: 1.2750e-04 - 11s/epoch - 131us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.3471e-04 - val_loss: 1.2627e-04 - 11s/epoch - 131us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.3409e-04 - val_loss: 1.2508e-04 - 11s/epoch - 129us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.3399e-04 - val_loss: 1.2697e-04 - 11s/epoch - 129us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.3397e-04 - val_loss: 1.2692e-04 - 11s/epoch - 129us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.3393e-04 - val_loss: 1.2704e-04 - 11s/epoch - 129us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.3365e-04 - val_loss: 1.2576e-04 - 11s/epoch - 132us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.3359e-04 - val_loss: 1.2570e-04 - 11s/epoch - 130us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.3367e-04 - val_loss: 1.2533e-04 - 11s/epoch - 129us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.3318e-04 - val_loss: 1.2546e-04 - 11s/epoch - 129us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.3285e-04 - val_loss: 1.2526e-04 - 11s/epoch - 129us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012526171090942624
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 19:48:37.535347: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_41/outputlayer/BiasAdd' id:46202 op device:{requested: '', assigned: ''} def:{{{node decoder_model_41/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_41/outputlayer/MatMul, decoder_model_41/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03195355885775635
cosine 0.031564197099821396
MAE: 0.0023316823638820203
RMSE: 0.010142328188191616
r2: 0.9201542175457565
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 64, 90, 0.0004000000000000001, 0.2, 188, 0.0001328455057303654, 0.00012526171090942624, 0.03195355885775635, 0.031564197099821396, 0.0023316823638820203, 0.010142328188191616, 0.9201542175457565, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0008 32 0] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_108 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_108 (ReLU)               (None, 1791)         0           ['batch_normalization_108[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 19:48:56.585663: W tensorflow/c/c_api.cc:291] Operation '{name:'training_60/Adam/batch_normalization_109/gamma/m/Assign' id:48113 op device:{requested: '', assigned: ''} def:{{{node training_60/Adam/batch_normalization_109/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_60/Adam/batch_normalization_109/gamma/m, training_60/Adam/batch_normalization_109/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 19:49:16.955616: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_42/mul' id:47535 op device:{requested: '', assigned: ''} def:{{{node loss_42/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_42/mul/x, loss_42/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0686 - val_loss: 0.0675 - 27s/epoch - 325us/sample
Epoch 2/85
84077/84077 - 17s - loss: 0.0669 - val_loss: 0.0674 - 17s/epoch - 201us/sample
Epoch 3/85
84077/84077 - 17s - loss: 0.0669 - val_loss: 0.0674 - 17s/epoch - 204us/sample
Epoch 4/85
84077/84077 - 17s - loss: 0.0669 - val_loss: 0.0674 - 17s/epoch - 201us/sample
Epoch 5/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 6/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 7/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 8/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 9/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 10/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 11/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 12/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 13/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 14/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 15/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 16/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 17/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 18/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 19/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 20/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 21/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 22/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 23/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 24/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 25/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 26/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 27/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 28/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 29/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 30/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 31/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 32/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 33/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 34/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 35/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 36/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 37/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 38/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 39/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 40/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 41/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 42/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 43/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 44/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 45/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 46/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 47/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 48/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 49/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 50/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 51/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 52/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 53/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 54/85
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 55/85
84077/84077 - 21s - loss: 0.0668 - val_loss: 0.0673 - 21s/epoch - 252us/sample
Epoch 56/85
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 211us/sample
Epoch 57/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 58/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 59/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 60/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 61/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 62/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 63/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 64/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 65/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 66/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 67/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 68/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 69/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 70/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 71/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 72/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 73/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 74/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 75/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 76/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 77/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 78/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 79/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 80/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 81/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 82/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 83/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 84/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 85/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573545209144
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 20:13:16.048458: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_42/outputlayer/BiasAdd' id:47487 op device:{requested: '', assigned: ''} def:{{{node decoder_model_42/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_42/outputlayer/MatMul, decoder_model_42/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.145009362554157
cosine 1.1407251589202583
MAE: 5.1896245950247435
RMSE: 5.376452178980802
r2: -21243.345828338373
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'binary_crossentropy', 32, 85, 0.0008, 0.2, 188, 0.06683691523171008, 0.06734573545209144, 1.145009362554157, 1.1407251589202583, 5.1896245950247435, 5.376452178980802, -21243.345828338373, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0004000000000000001 32 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_111 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_111 (ReLU)               (None, 1697)         0           ['batch_normalization_111[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 20:13:35.357116: W tensorflow/c/c_api.cc:291] Operation '{name:'training_62/Adam/dense_dec1_43/bias/v/Assign' id:49477 op device:{requested: '', assigned: ''} def:{{{node training_62/Adam/dense_dec1_43/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_62/Adam/dense_dec1_43/bias/v, training_62/Adam/dense_dec1_43/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 20:13:55.430978: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_43/mul' id:48853 op device:{requested: '', assigned: ''} def:{{{node loss_43/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_43/mul/x, loss_43/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0069 - val_loss: 0.0017 - 27s/epoch - 324us/sample
Epoch 2/90
84077/84077 - 17s - loss: 0.0036 - val_loss: 0.0026 - 17s/epoch - 199us/sample
Epoch 3/90
84077/84077 - 17s - loss: 0.0011 - val_loss: 9.5070e-04 - 17s/epoch - 197us/sample
Epoch 4/90
84077/84077 - 16s - loss: 8.9410e-04 - val_loss: 7.0346e-04 - 16s/epoch - 196us/sample
Epoch 5/90
84077/84077 - 17s - loss: 6.6750e-04 - val_loss: 5.3145e-04 - 17s/epoch - 197us/sample
Epoch 6/90
84077/84077 - 17s - loss: 5.5002e-04 - val_loss: 4.6635e-04 - 17s/epoch - 198us/sample
Epoch 7/90
84077/84077 - 17s - loss: 4.6824e-04 - val_loss: 3.9649e-04 - 17s/epoch - 198us/sample
Epoch 8/90
84077/84077 - 17s - loss: 4.1900e-04 - val_loss: 3.4891e-04 - 17s/epoch - 197us/sample
Epoch 9/90
84077/84077 - 17s - loss: 3.7938e-04 - val_loss: 3.2016e-04 - 17s/epoch - 196us/sample
Epoch 10/90
84077/84077 - 17s - loss: 3.5060e-04 - val_loss: 3.0625e-04 - 17s/epoch - 197us/sample
Epoch 11/90
84077/84077 - 17s - loss: 3.3166e-04 - val_loss: 2.9581e-04 - 17s/epoch - 198us/sample
Epoch 12/90
84077/84077 - 17s - loss: 3.1612e-04 - val_loss: 2.7839e-04 - 17s/epoch - 197us/sample
Epoch 13/90
84077/84077 - 16s - loss: 3.0664e-04 - val_loss: 2.7093e-04 - 16s/epoch - 196us/sample
Epoch 14/90
84077/84077 - 17s - loss: 2.9557e-04 - val_loss: 2.7004e-04 - 17s/epoch - 197us/sample
Epoch 15/90
84077/84077 - 17s - loss: 2.8830e-04 - val_loss: 2.6382e-04 - 17s/epoch - 199us/sample
Epoch 16/90
84077/84077 - 17s - loss: 2.8234e-04 - val_loss: 2.5752e-04 - 17s/epoch - 197us/sample
Epoch 17/90
84077/84077 - 16s - loss: 2.7676e-04 - val_loss: 2.5022e-04 - 16s/epoch - 196us/sample
Epoch 18/90
84077/84077 - 17s - loss: 2.7167e-04 - val_loss: 2.4493e-04 - 17s/epoch - 197us/sample
Epoch 19/90
84077/84077 - 17s - loss: 2.6761e-04 - val_loss: 2.4143e-04 - 17s/epoch - 199us/sample
Epoch 20/90
84077/84077 - 17s - loss: 2.6415e-04 - val_loss: 2.4466e-04 - 17s/epoch - 197us/sample
Epoch 21/90
84077/84077 - 16s - loss: 2.5997e-04 - val_loss: 2.3735e-04 - 16s/epoch - 196us/sample
Epoch 22/90
84077/84077 - 17s - loss: 2.5571e-04 - val_loss: 2.3732e-04 - 17s/epoch - 196us/sample
Epoch 23/90
84077/84077 - 17s - loss: 2.5375e-04 - val_loss: 2.3778e-04 - 17s/epoch - 199us/sample
Epoch 24/90
84077/84077 - 17s - loss: 2.5140e-04 - val_loss: 2.3127e-04 - 17s/epoch - 197us/sample
Epoch 25/90
84077/84077 - 17s - loss: 2.5036e-04 - val_loss: 2.3536e-04 - 17s/epoch - 196us/sample
Epoch 26/90
84077/84077 - 17s - loss: 2.4737e-04 - val_loss: 2.3757e-04 - 17s/epoch - 198us/sample
Epoch 27/90
84077/84077 - 17s - loss: 2.4436e-04 - val_loss: 2.3023e-04 - 17s/epoch - 199us/sample
Epoch 28/90
84077/84077 - 17s - loss: 2.4419e-04 - val_loss: 2.2455e-04 - 17s/epoch - 197us/sample
Epoch 29/90
84077/84077 - 17s - loss: 2.4141e-04 - val_loss: 2.3021e-04 - 17s/epoch - 196us/sample
Epoch 30/90
84077/84077 - 17s - loss: 2.3967e-04 - val_loss: 2.2658e-04 - 17s/epoch - 200us/sample
Epoch 31/90
84077/84077 - 17s - loss: 2.3725e-04 - val_loss: 2.2160e-04 - 17s/epoch - 197us/sample
Epoch 32/90
84077/84077 - 17s - loss: 2.3721e-04 - val_loss: 2.2159e-04 - 17s/epoch - 196us/sample
Epoch 33/90
84077/84077 - 17s - loss: 2.3498e-04 - val_loss: 2.2114e-04 - 17s/epoch - 198us/sample
Epoch 34/90
84077/84077 - 17s - loss: 2.3406e-04 - val_loss: 2.2080e-04 - 17s/epoch - 198us/sample
Epoch 35/90
84077/84077 - 17s - loss: 2.3353e-04 - val_loss: 2.1806e-04 - 17s/epoch - 196us/sample
Epoch 36/90
84077/84077 - 16s - loss: 2.3224e-04 - val_loss: 2.1627e-04 - 16s/epoch - 196us/sample
Epoch 37/90
84077/84077 - 17s - loss: 2.3086e-04 - val_loss: 2.1729e-04 - 17s/epoch - 198us/sample
Epoch 38/90
84077/84077 - 17s - loss: 2.2986e-04 - val_loss: 2.1713e-04 - 17s/epoch - 198us/sample
Epoch 39/90
84077/84077 - 16s - loss: 2.2913e-04 - val_loss: 2.1737e-04 - 16s/epoch - 196us/sample
Epoch 40/90
84077/84077 - 16s - loss: 2.2772e-04 - val_loss: 2.1268e-04 - 16s/epoch - 196us/sample
Epoch 41/90
84077/84077 - 17s - loss: 2.2749e-04 - val_loss: 2.1275e-04 - 17s/epoch - 199us/sample
Epoch 42/90
84077/84077 - 17s - loss: 2.2585e-04 - val_loss: 2.1312e-04 - 17s/epoch - 198us/sample
Epoch 43/90
84077/84077 - 17s - loss: 2.2464e-04 - val_loss: 2.1516e-04 - 17s/epoch - 196us/sample
Epoch 44/90
84077/84077 - 17s - loss: 2.2364e-04 - val_loss: 2.1041e-04 - 17s/epoch - 198us/sample
Epoch 45/90
84077/84077 - 17s - loss: 2.2250e-04 - val_loss: 2.1030e-04 - 17s/epoch - 198us/sample
Epoch 46/90
84077/84077 - 17s - loss: 2.2212e-04 - val_loss: 2.1164e-04 - 17s/epoch - 198us/sample
Epoch 47/90
84077/84077 - 16s - loss: 2.2089e-04 - val_loss: 2.0641e-04 - 16s/epoch - 196us/sample
Epoch 48/90
84077/84077 - 17s - loss: 2.2086e-04 - val_loss: 2.0832e-04 - 17s/epoch - 197us/sample
Epoch 49/90
84077/84077 - 17s - loss: 2.2013e-04 - val_loss: 2.1125e-04 - 17s/epoch - 197us/sample
Epoch 50/90
84077/84077 - 17s - loss: 2.1917e-04 - val_loss: 2.1235e-04 - 17s/epoch - 198us/sample
Epoch 51/90
84077/84077 - 16s - loss: 2.2124e-04 - val_loss: 2.0760e-04 - 16s/epoch - 196us/sample
Epoch 52/90
84077/84077 - 17s - loss: 2.1786e-04 - val_loss: 2.0757e-04 - 17s/epoch - 197us/sample
Epoch 53/90
84077/84077 - 17s - loss: 2.1850e-04 - val_loss: 2.0873e-04 - 17s/epoch - 198us/sample
Epoch 54/90
84077/84077 - 17s - loss: 2.1669e-04 - val_loss: 2.1029e-04 - 17s/epoch - 196us/sample
Epoch 55/90
84077/84077 - 17s - loss: 2.1649e-04 - val_loss: 2.1667e-04 - 17s/epoch - 197us/sample
Epoch 56/90
84077/84077 - 17s - loss: 2.1688e-04 - val_loss: 2.0657e-04 - 17s/epoch - 197us/sample
Epoch 57/90
84077/84077 - 17s - loss: 2.1483e-04 - val_loss: 2.0479e-04 - 17s/epoch - 198us/sample
Epoch 58/90
84077/84077 - 17s - loss: 2.1516e-04 - val_loss: 2.0084e-04 - 17s/epoch - 197us/sample
Epoch 59/90
84077/84077 - 17s - loss: 2.1465e-04 - val_loss: 2.0163e-04 - 17s/epoch - 197us/sample
Epoch 60/90
84077/84077 - 17s - loss: 2.1460e-04 - val_loss: 2.0291e-04 - 17s/epoch - 197us/sample
Epoch 61/90
84077/84077 - 17s - loss: 2.1413e-04 - val_loss: 2.0123e-04 - 17s/epoch - 198us/sample
Epoch 62/90
84077/84077 - 17s - loss: 2.1225e-04 - val_loss: 2.0321e-04 - 17s/epoch - 198us/sample
Epoch 63/90
84077/84077 - 17s - loss: 2.1351e-04 - val_loss: 2.0161e-04 - 17s/epoch - 196us/sample
Epoch 64/90
84077/84077 - 17s - loss: 2.1276e-04 - val_loss: 2.0399e-04 - 17s/epoch - 197us/sample
Epoch 65/90
84077/84077 - 17s - loss: 2.1162e-04 - val_loss: 2.0357e-04 - 17s/epoch - 199us/sample
Epoch 66/90
84077/84077 - 17s - loss: 2.1163e-04 - val_loss: 2.0103e-04 - 17s/epoch - 199us/sample
Epoch 67/90
84077/84077 - 16s - loss: 2.1068e-04 - val_loss: 1.9912e-04 - 16s/epoch - 196us/sample
Epoch 68/90
84077/84077 - 17s - loss: 2.1075e-04 - val_loss: 2.0402e-04 - 17s/epoch - 197us/sample
Epoch 69/90
84077/84077 - 17s - loss: 2.0972e-04 - val_loss: 1.9516e-04 - 17s/epoch - 198us/sample
Epoch 70/90
84077/84077 - 17s - loss: 2.1015e-04 - val_loss: 1.9532e-04 - 17s/epoch - 197us/sample
Epoch 71/90
84077/84077 - 17s - loss: 2.1379e-04 - val_loss: 1.9365e-04 - 17s/epoch - 196us/sample
Epoch 72/90
84077/84077 - 17s - loss: 2.0879e-04 - val_loss: 1.9720e-04 - 17s/epoch - 198us/sample
Epoch 73/90
84077/84077 - 17s - loss: 2.1073e-04 - val_loss: 1.9910e-04 - 17s/epoch - 198us/sample
Epoch 74/90
84077/84077 - 17s - loss: 2.0808e-04 - val_loss: 1.9678e-04 - 17s/epoch - 196us/sample
Epoch 75/90
84077/84077 - 17s - loss: 2.0833e-04 - val_loss: 1.9537e-04 - 17s/epoch - 197us/sample
Epoch 76/90
84077/84077 - 17s - loss: 2.0767e-04 - val_loss: 1.9618e-04 - 17s/epoch - 198us/sample
Epoch 77/90
84077/84077 - 17s - loss: 2.0941e-04 - val_loss: 1.9447e-04 - 17s/epoch - 198us/sample
Epoch 78/90
84077/84077 - 17s - loss: 2.0935e-04 - val_loss: 1.9810e-04 - 17s/epoch - 196us/sample
Epoch 79/90
84077/84077 - 17s - loss: 2.0726e-04 - val_loss: 1.9367e-04 - 17s/epoch - 197us/sample
Epoch 80/90
84077/84077 - 17s - loss: 2.0751e-04 - val_loss: 1.9241e-04 - 17s/epoch - 199us/sample
Epoch 81/90
84077/84077 - 17s - loss: 2.0683e-04 - val_loss: 1.9690e-04 - 17s/epoch - 197us/sample
Epoch 82/90
84077/84077 - 17s - loss: 2.0664e-04 - val_loss: 1.9324e-04 - 17s/epoch - 197us/sample
Epoch 83/90
84077/84077 - 17s - loss: 2.0552e-04 - val_loss: 1.9292e-04 - 17s/epoch - 197us/sample
Epoch 84/90
84077/84077 - 17s - loss: 2.0585e-04 - val_loss: 1.9249e-04 - 17s/epoch - 200us/sample
Epoch 85/90
84077/84077 - 17s - loss: 2.0672e-04 - val_loss: 2.0263e-04 - 17s/epoch - 197us/sample
Epoch 86/90
84077/84077 - 17s - loss: 2.1349e-04 - val_loss: 1.9105e-04 - 17s/epoch - 197us/sample
Epoch 87/90
84077/84077 - 17s - loss: 2.0530e-04 - val_loss: 1.8984e-04 - 17s/epoch - 197us/sample
Epoch 88/90
84077/84077 - 17s - loss: 2.0416e-04 - val_loss: 1.9010e-04 - 17s/epoch - 199us/sample
Epoch 89/90
84077/84077 - 17s - loss: 2.0457e-04 - val_loss: 1.8965e-04 - 17s/epoch - 197us/sample
Epoch 90/90
84077/84077 - 17s - loss: 2.0508e-04 - val_loss: 1.9338e-04 - 17s/epoch - 196us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001933804586680466
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 20:38:35.881315: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_43/outputlayer/BiasAdd' id:48824 op device:{requested: '', assigned: ''} def:{{{node decoder_model_43/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_43/outputlayer/MatMul, decoder_model_43/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.022993487146427617
cosine 0.02272079510474387
MAE: 0.002068086138635534
RMSE: 0.009270974988864263
r2: 0.9330356233452567
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 90, 0.0004000000000000001, 0.2, 188, 0.0002050815966146286, 0.0001933804586680466, 0.022993487146427617, 0.02272079510474387, 0.002068086138635534, 0.009270974988864263, 0.9330356233452567, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0004000000000000001 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_114 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_114 (ReLU)               (None, 1697)         0           ['batch_normalization_114[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE1.7999999999999998_cr0.2_bs32_ep90_loss_mse_lr0.0004000000000000001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 20:38:55.718315: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_44/kernel/Assign' id:49899 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_44/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_44/kernel, dense_dec0_44/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 20:39:03.295737: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_112_1/gamma/v/Assign' id:50883 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_112_1/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_112_1/gamma/v, batch_normalization_112_1/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 20:39:10.688537: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_45/outputlayer/BiasAdd' id:50526 op device:{requested: '', assigned: ''} def:{{{node decoder_model_45/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_45/outputlayer/MatMul, decoder_model_45/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.022946708213523324
cosine 0.02267314831649031
MAE: 0.0020715333507995187
RMSE: 0.009289529305146784
r2: 0.9327762617508955
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['1.7999999999999998custom_VAE', 'mse', 32, 90, 0.0004000000000000001, 0.2, 188, '--', '--', 0.022946708213523324, 0.02267314831649031, 0.0020715333507995187, 0.009289529305146784, 0.9327762617508955, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0008 64 0] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_117 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_117 (ReLU)               (None, 1697)         0           ['batch_normalization_117[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 20:39:29.996649: W tensorflow/c/c_api.cc:291] Operation '{name:'training_64/Adam/outputlayer_46/bias/v/Assign' id:52286 op device:{requested: '', assigned: ''} def:{{{node training_64/Adam/outputlayer_46/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_64/Adam/outputlayer_46/bias/v, training_64/Adam/outputlayer_46/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 20:39:45.548154: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_46/mul' id:51549 op device:{requested: '', assigned: ''} def:{{{node loss_46/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_46/mul/x, loss_46/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0703 - val_loss: 0.0679 - 23s/epoch - 273us/sample
Epoch 2/90
84077/84077 - 12s - loss: 0.0676 - val_loss: 0.0705 - 12s/epoch - 137us/sample
Epoch 3/90
84077/84077 - 11s - loss: 0.0676 - val_loss: 0.0676 - 11s/epoch - 136us/sample
Epoch 4/90
84077/84077 - 11s - loss: 0.0674 - val_loss: 0.0675 - 11s/epoch - 136us/sample
Epoch 5/90
84077/84077 - 11s - loss: 0.0674 - val_loss: 0.0675 - 11s/epoch - 136us/sample
Epoch 6/90
84077/84077 - 11s - loss: 0.0669 - val_loss: 0.0674 - 11s/epoch - 136us/sample
Epoch 7/90
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 139us/sample
Epoch 8/90
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 137us/sample
Epoch 9/90
84077/84077 - 11s - loss: 0.0669 - val_loss: 0.0674 - 11s/epoch - 136us/sample
Epoch 10/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0674 - 11s/epoch - 136us/sample
Epoch 11/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0674 - 11s/epoch - 136us/sample
Epoch 12/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 13/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 14/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 15/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 16/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 17/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 18/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 19/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 20/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 21/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 22/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 23/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 24/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 25/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 26/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 27/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 28/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 139us/sample
Epoch 29/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 30/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 31/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 32/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 33/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 34/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 35/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 36/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 37/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 38/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 39/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 40/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 41/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 42/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 43/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 44/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 45/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 46/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 47/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 48/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 49/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 50/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 51/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 52/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 53/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 54/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 55/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 56/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 57/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 58/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 59/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 137us/sample
Epoch 60/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 61/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 62/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 63/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 64/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 65/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 66/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 67/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 68/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 69/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 70/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 71/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 72/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 73/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 74/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 75/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 76/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 77/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 78/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 79/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 80/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 81/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 82/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 83/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 84/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 85/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 137us/sample
Epoch 86/90
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 138us/sample
Epoch 87/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 88/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 89/90
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 136us/sample
Epoch 90/90
84077/84077 - 11s - loss: 0.0669 - val_loss: 0.0675 - 11s/epoch - 136us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06747910052416693
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 20:56:53.433558: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_46/outputlayer/BiasAdd' id:51501 op device:{requested: '', assigned: ''} def:{{{node decoder_model_46/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_46/outputlayer/MatMul, decoder_model_46/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.265366620032177
cosine 1.1719811365007053
MAE: 7.7755005098303585
RMSE: 8.406789528610052
r2: -51926.98307037766
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'binary_crossentropy', 64, 90, 0.0008, 0.2, 188, 0.06685220975780759, 0.06747910052416693, 1.265366620032177, 1.1719811365007053, 7.7755005098303585, 8.406789528610052, -51926.98307037766, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.0008 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_120 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_120 (ReLU)               (None, 1603)         0           ['batch_normalization_120[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 20:57:14.125000: W tensorflow/c/c_api.cc:291] Operation '{name:'training_66/Adam/dense_dec0_47/kernel/m/Assign' id:53397 op device:{requested: '', assigned: ''} def:{{{node training_66/Adam/dense_dec0_47/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_66/Adam/dense_dec0_47/kernel/m, training_66/Adam/dense_dec0_47/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 20:57:34.587578: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_47/mul' id:52867 op device:{requested: '', assigned: ''} def:{{{node loss_47/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_47/mul/x, loss_47/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 28s - loss: 0.0058 - val_loss: 0.0019 - 28s/epoch - 335us/sample
Epoch 2/85
84077/84077 - 17s - loss: 0.0019 - val_loss: 0.0010 - 17s/epoch - 198us/sample
Epoch 3/85
84077/84077 - 17s - loss: 0.0011 - val_loss: 8.3095e-04 - 17s/epoch - 199us/sample
Epoch 4/85
84077/84077 - 17s - loss: 9.3803e-04 - val_loss: 6.6739e-04 - 17s/epoch - 198us/sample
Epoch 5/85
84077/84077 - 17s - loss: 6.9827e-04 - val_loss: 5.7856e-04 - 17s/epoch - 197us/sample
Epoch 6/85
84077/84077 - 17s - loss: 5.9225e-04 - val_loss: 5.1372e-04 - 17s/epoch - 197us/sample
Epoch 7/85
84077/84077 - 17s - loss: 5.0840e-04 - val_loss: 4.3274e-04 - 17s/epoch - 198us/sample
Epoch 8/85
84077/84077 - 17s - loss: 4.5895e-04 - val_loss: 3.9266e-04 - 17s/epoch - 199us/sample
Epoch 9/85
84077/84077 - 17s - loss: 4.1793e-04 - val_loss: 3.7051e-04 - 17s/epoch - 198us/sample
Epoch 10/85
84077/84077 - 17s - loss: 3.8821e-04 - val_loss: 3.4820e-04 - 17s/epoch - 198us/sample
Epoch 11/85
84077/84077 - 17s - loss: 3.6421e-04 - val_loss: 3.3439e-04 - 17s/epoch - 199us/sample
Epoch 12/85
84077/84077 - 17s - loss: 3.4991e-04 - val_loss: 3.3148e-04 - 17s/epoch - 199us/sample
Epoch 13/85
84077/84077 - 17s - loss: 3.3440e-04 - val_loss: 3.2287e-04 - 17s/epoch - 197us/sample
Epoch 14/85
84077/84077 - 17s - loss: 3.2273e-04 - val_loss: 3.0390e-04 - 17s/epoch - 197us/sample
Epoch 15/85
84077/84077 - 17s - loss: 3.1130e-04 - val_loss: 3.0164e-04 - 17s/epoch - 199us/sample
Epoch 16/85
84077/84077 - 17s - loss: 3.0303e-04 - val_loss: 2.9111e-04 - 17s/epoch - 199us/sample
Epoch 17/85
84077/84077 - 17s - loss: 2.9623e-04 - val_loss: 2.8431e-04 - 17s/epoch - 198us/sample
Epoch 18/85
84077/84077 - 17s - loss: 2.8991e-04 - val_loss: 2.8396e-04 - 17s/epoch - 198us/sample
Epoch 19/85
84077/84077 - 17s - loss: 2.8288e-04 - val_loss: 2.7876e-04 - 17s/epoch - 200us/sample
Epoch 20/85
84077/84077 - 17s - loss: 2.8020e-04 - val_loss: 2.8077e-04 - 17s/epoch - 198us/sample
Epoch 21/85
84077/84077 - 17s - loss: 2.7528e-04 - val_loss: 2.7381e-04 - 17s/epoch - 198us/sample
Epoch 22/85
84077/84077 - 17s - loss: 2.7257e-04 - val_loss: 2.7360e-04 - 17s/epoch - 198us/sample
Epoch 23/85
84077/84077 - 17s - loss: 2.6901e-04 - val_loss: 2.7202e-04 - 17s/epoch - 199us/sample
Epoch 24/85
84077/84077 - 17s - loss: 2.6650e-04 - val_loss: 2.6250e-04 - 17s/epoch - 198us/sample
Epoch 25/85
84077/84077 - 17s - loss: 2.6449e-04 - val_loss: 2.5647e-04 - 17s/epoch - 197us/sample
Epoch 26/85
84077/84077 - 17s - loss: 2.6145e-04 - val_loss: 2.6963e-04 - 17s/epoch - 198us/sample
Epoch 27/85
84077/84077 - 17s - loss: 2.5926e-04 - val_loss: 2.6503e-04 - 17s/epoch - 200us/sample
Epoch 28/85
84077/84077 - 17s - loss: 2.5735e-04 - val_loss: 2.6529e-04 - 17s/epoch - 198us/sample
Epoch 29/85
84077/84077 - 17s - loss: 2.5637e-04 - val_loss: 2.5732e-04 - 17s/epoch - 198us/sample
Epoch 30/85
84077/84077 - 17s - loss: 2.5643e-04 - val_loss: 2.6056e-04 - 17s/epoch - 198us/sample
Epoch 31/85
84077/84077 - 17s - loss: 2.5342e-04 - val_loss: 2.6199e-04 - 17s/epoch - 200us/sample
Epoch 32/85
84077/84077 - 17s - loss: 2.5091e-04 - val_loss: 2.5287e-04 - 17s/epoch - 198us/sample
Epoch 33/85
84077/84077 - 17s - loss: 2.5023e-04 - val_loss: 2.5740e-04 - 17s/epoch - 198us/sample
Epoch 34/85
84077/84077 - 17s - loss: 2.4883e-04 - val_loss: 2.5275e-04 - 17s/epoch - 199us/sample
Epoch 35/85
84077/84077 - 17s - loss: 2.4764e-04 - val_loss: 2.6033e-04 - 17s/epoch - 199us/sample
Epoch 36/85
84077/84077 - 17s - loss: 2.4691e-04 - val_loss: 2.5211e-04 - 17s/epoch - 197us/sample
Epoch 37/85
84077/84077 - 17s - loss: 2.4589e-04 - val_loss: 2.5614e-04 - 17s/epoch - 198us/sample
Epoch 38/85
84077/84077 - 17s - loss: 2.4439e-04 - val_loss: 2.5527e-04 - 17s/epoch - 200us/sample
Epoch 39/85
84077/84077 - 17s - loss: 2.4359e-04 - val_loss: 2.5220e-04 - 17s/epoch - 197us/sample
Epoch 40/85
84077/84077 - 17s - loss: 2.4268e-04 - val_loss: 2.5583e-04 - 17s/epoch - 197us/sample
Epoch 41/85
84077/84077 - 17s - loss: 2.4072e-04 - val_loss: 2.4576e-04 - 17s/epoch - 198us/sample
Epoch 42/85
84077/84077 - 17s - loss: 2.4097e-04 - val_loss: 2.4919e-04 - 17s/epoch - 200us/sample
Epoch 43/85
84077/84077 - 17s - loss: 2.3961e-04 - val_loss: 2.4994e-04 - 17s/epoch - 198us/sample
Epoch 44/85
84077/84077 - 17s - loss: 2.3872e-04 - val_loss: 2.4381e-04 - 17s/epoch - 197us/sample
Epoch 45/85
84077/84077 - 17s - loss: 2.3858e-04 - val_loss: 2.4565e-04 - 17s/epoch - 199us/sample
Epoch 46/85
84077/84077 - 17s - loss: 2.3669e-04 - val_loss: 2.4209e-04 - 17s/epoch - 199us/sample
Epoch 47/85
84077/84077 - 17s - loss: 2.3563e-04 - val_loss: 2.4874e-04 - 17s/epoch - 197us/sample
Epoch 48/85
84077/84077 - 17s - loss: 2.3541e-04 - val_loss: 2.4308e-04 - 17s/epoch - 198us/sample
Epoch 49/85
84077/84077 - 17s - loss: 2.3399e-04 - val_loss: 2.3688e-04 - 17s/epoch - 199us/sample
Epoch 50/85
84077/84077 - 17s - loss: 2.3450e-04 - val_loss: 2.4874e-04 - 17s/epoch - 198us/sample
Epoch 51/85
84077/84077 - 17s - loss: 2.3274e-04 - val_loss: 2.4194e-04 - 17s/epoch - 197us/sample
Epoch 52/85
84077/84077 - 17s - loss: 2.3310e-04 - val_loss: 2.4323e-04 - 17s/epoch - 198us/sample
Epoch 53/85
84077/84077 - 17s - loss: 2.3166e-04 - val_loss: 2.4011e-04 - 17s/epoch - 200us/sample
Epoch 54/85
84077/84077 - 17s - loss: 2.3076e-04 - val_loss: 2.3783e-04 - 17s/epoch - 198us/sample
Epoch 55/85
84077/84077 - 17s - loss: 2.3099e-04 - val_loss: 2.3821e-04 - 17s/epoch - 197us/sample
Epoch 56/85
84077/84077 - 17s - loss: 2.3117e-04 - val_loss: 2.4495e-04 - 17s/epoch - 198us/sample
Epoch 57/85
84077/84077 - 17s - loss: 2.2960e-04 - val_loss: 2.3424e-04 - 17s/epoch - 199us/sample
Epoch 58/85
84077/84077 - 17s - loss: 2.2855e-04 - val_loss: 2.3942e-04 - 17s/epoch - 197us/sample
Epoch 59/85
84077/84077 - 17s - loss: 2.2885e-04 - val_loss: 2.3644e-04 - 17s/epoch - 197us/sample
Epoch 60/85
84077/84077 - 17s - loss: 2.2781e-04 - val_loss: 2.4201e-04 - 17s/epoch - 199us/sample
Epoch 61/85
84077/84077 - 17s - loss: 2.2726e-04 - val_loss: 2.3323e-04 - 17s/epoch - 198us/sample
Epoch 62/85
84077/84077 - 17s - loss: 2.2672e-04 - val_loss: 2.3058e-04 - 17s/epoch - 197us/sample
Epoch 63/85
84077/84077 - 17s - loss: 2.2641e-04 - val_loss: 2.2954e-04 - 17s/epoch - 198us/sample
Epoch 64/85
84077/84077 - 17s - loss: 2.2678e-04 - val_loss: 2.3295e-04 - 17s/epoch - 199us/sample
Epoch 65/85
84077/84077 - 17s - loss: 2.2540e-04 - val_loss: 2.3529e-04 - 17s/epoch - 198us/sample
Epoch 66/85
84077/84077 - 17s - loss: 2.2626e-04 - val_loss: 2.3951e-04 - 17s/epoch - 198us/sample
Epoch 67/85
84077/84077 - 17s - loss: 2.2540e-04 - val_loss: 2.3403e-04 - 17s/epoch - 198us/sample
Epoch 68/85
84077/84077 - 17s - loss: 2.2449e-04 - val_loss: 2.3206e-04 - 17s/epoch - 199us/sample
Epoch 69/85
84077/84077 - 17s - loss: 2.2465e-04 - val_loss: 2.3505e-04 - 17s/epoch - 198us/sample
Epoch 70/85
84077/84077 - 17s - loss: 2.2406e-04 - val_loss: 2.2752e-04 - 17s/epoch - 197us/sample
Epoch 71/85
84077/84077 - 17s - loss: 2.2352e-04 - val_loss: 2.3003e-04 - 17s/epoch - 198us/sample
Epoch 72/85
84077/84077 - 17s - loss: 2.2448e-04 - val_loss: 2.3351e-04 - 17s/epoch - 200us/sample
Epoch 73/85
84077/84077 - 17s - loss: 2.2209e-04 - val_loss: 2.3119e-04 - 17s/epoch - 197us/sample
Epoch 74/85
84077/84077 - 17s - loss: 2.2272e-04 - val_loss: 2.2863e-04 - 17s/epoch - 197us/sample
Epoch 75/85
84077/84077 - 17s - loss: 2.2292e-04 - val_loss: 2.2974e-04 - 17s/epoch - 198us/sample
Epoch 76/85
84077/84077 - 17s - loss: 2.2168e-04 - val_loss: 2.3412e-04 - 17s/epoch - 199us/sample
Epoch 77/85
84077/84077 - 17s - loss: 2.2106e-04 - val_loss: 2.3856e-04 - 17s/epoch - 197us/sample
Epoch 78/85
84077/84077 - 17s - loss: 2.2094e-04 - val_loss: 2.2987e-04 - 17s/epoch - 198us/sample
Epoch 79/85
84077/84077 - 17s - loss: 2.2023e-04 - val_loss: 2.3019e-04 - 17s/epoch - 200us/sample
Epoch 80/85
84077/84077 - 17s - loss: 2.2067e-04 - val_loss: 2.2843e-04 - 17s/epoch - 198us/sample
Epoch 81/85
84077/84077 - 17s - loss: 2.2119e-04 - val_loss: 2.2978e-04 - 17s/epoch - 197us/sample
Epoch 82/85
84077/84077 - 17s - loss: 2.1940e-04 - val_loss: 2.2852e-04 - 17s/epoch - 198us/sample
Epoch 83/85
84077/84077 - 17s - loss: 2.1957e-04 - val_loss: 2.2857e-04 - 17s/epoch - 198us/sample
Epoch 84/85
84077/84077 - 17s - loss: 2.2271e-04 - val_loss: 2.2227e-04 - 17s/epoch - 199us/sample
Epoch 85/85
84077/84077 - 17s - loss: 2.1849e-04 - val_loss: 2.2524e-04 - 17s/epoch - 198us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00022524419287122443
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 21:20:58.668183: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_47/outputlayer/BiasAdd' id:52838 op device:{requested: '', assigned: ''} def:{{{node decoder_model_47/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_47/outputlayer/MatMul, decoder_model_47/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02738924965360238
cosine 0.027058518037677826
MAE: 0.002215627666676412
RMSE: 0.011076356289540654
r2: 0.9042910288565006
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 32, 85, 0.0008, 0.2, 188, 0.00021848552782496628, 0.00022524419287122443, 0.02738924965360238, 0.027058518037677826, 0.002215627666676412, 0.011076356289540654, 0.9042910288565006, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.0008 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_123 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_123 (ReLU)               (None, 1603)         0           ['batch_normalization_123[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE1.6999999999999997_cr0.2_bs32_ep85_loss_mse_lr0.0008_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 21:21:19.720305: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_125/gamma/Assign' id:53934 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_125/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_125/gamma, batch_normalization_125/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 21:21:27.958929: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_49/kernel/v/Assign' id:54866 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_49/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_49/kernel/v, bottleneck_zmean_49/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 21:21:35.969432: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_49/outputlayer/BiasAdd' id:54543 op device:{requested: '', assigned: ''} def:{{{node decoder_model_49/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_49/outputlayer/MatMul, decoder_model_49/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.027515658805332476
cosine 0.0271825419135845
MAE: 0.0022066924410971585
RMSE: 0.011042038641021407
r2: 0.9048839720796578
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['1.6999999999999997custom_VAE', 'mse', 32, 85, 0.0008, 0.2, 188, '--', '--', 0.027515658805332476, 0.0271825419135845, 0.0022066924410971585, 0.011042038641021407, 0.9048839720796578, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 85 0.0006000000000000001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_126 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_126 (ReLU)               (None, 1697)         0           ['batch_normalization_126[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 21:21:56.211287: W tensorflow/c/c_api.cc:291] Operation '{name:'training_68/Adam/dense_dec1_50/bias/v/Assign' id:56171 op device:{requested: '', assigned: ''} def:{{{node training_68/Adam/dense_dec1_50/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_68/Adam/dense_dec1_50/bias/v, training_68/Adam/dense_dec1_50/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 21:22:17.379051: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_50/mul' id:55547 op device:{requested: '', assigned: ''} def:{{{node loss_50/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_50/mul/x, loss_50/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 30s - loss: 0.0097 - val_loss: 0.0020 - 30s/epoch - 351us/sample
Epoch 2/85
84077/84077 - 17s - loss: 0.0019 - val_loss: 0.0031 - 17s/epoch - 205us/sample
Epoch 3/85
84077/84077 - 17s - loss: 0.0014 - val_loss: 0.0016 - 17s/epoch - 204us/sample
Epoch 4/85
84077/84077 - 17s - loss: 0.0012 - val_loss: 8.0165e-04 - 17s/epoch - 204us/sample
Epoch 5/85
84077/84077 - 17s - loss: 0.0016 - val_loss: 8.1139e-04 - 17s/epoch - 204us/sample
Epoch 6/85
84077/84077 - 17s - loss: 8.0916e-04 - val_loss: 6.8359e-04 - 17s/epoch - 204us/sample
Epoch 7/85
84077/84077 - 17s - loss: 9.9346e-04 - val_loss: 6.5981e-04 - 17s/epoch - 206us/sample
Epoch 8/85
84077/84077 - 17s - loss: 6.9648e-04 - val_loss: 5.8553e-04 - 17s/epoch - 205us/sample
Epoch 9/85
84077/84077 - 17s - loss: 6.4143e-04 - val_loss: 5.6909e-04 - 17s/epoch - 204us/sample
Epoch 10/85
84077/84077 - 17s - loss: 5.7212e-04 - val_loss: 4.9866e-04 - 17s/epoch - 204us/sample
Epoch 11/85
84077/84077 - 17s - loss: 5.4327e-04 - val_loss: 4.7870e-04 - 17s/epoch - 206us/sample
Epoch 12/85
84077/84077 - 17s - loss: 5.0359e-04 - val_loss: 4.4792e-04 - 17s/epoch - 205us/sample
Epoch 13/85
84077/84077 - 17s - loss: 4.8033e-04 - val_loss: 4.3564e-04 - 17s/epoch - 203us/sample
Epoch 14/85
84077/84077 - 17s - loss: 4.6454e-04 - val_loss: 4.2086e-04 - 17s/epoch - 204us/sample
Epoch 15/85
84077/84077 - 17s - loss: 4.5094e-04 - val_loss: 4.1082e-04 - 17s/epoch - 206us/sample
Epoch 16/85
84077/84077 - 17s - loss: 4.3848e-04 - val_loss: 3.9813e-04 - 17s/epoch - 205us/sample
Epoch 17/85
84077/84077 - 17s - loss: 4.2640e-04 - val_loss: 3.8925e-04 - 17s/epoch - 204us/sample
Epoch 18/85
84077/84077 - 17s - loss: 4.1574e-04 - val_loss: 3.7920e-04 - 17s/epoch - 204us/sample
Epoch 19/85
84077/84077 - 17s - loss: 4.0699e-04 - val_loss: 3.8183e-04 - 17s/epoch - 205us/sample
Epoch 20/85
84077/84077 - 17s - loss: 3.9921e-04 - val_loss: 3.6923e-04 - 17s/epoch - 206us/sample
Epoch 21/85
84077/84077 - 17s - loss: 3.9625e-04 - val_loss: 3.6562e-04 - 17s/epoch - 204us/sample
Epoch 22/85
84077/84077 - 17s - loss: 3.8984e-04 - val_loss: 3.6844e-04 - 17s/epoch - 204us/sample
Epoch 23/85
84077/84077 - 17s - loss: 3.8681e-04 - val_loss: 3.6768e-04 - 17s/epoch - 204us/sample
Epoch 24/85
84077/84077 - 17s - loss: 3.8082e-04 - val_loss: 3.5286e-04 - 17s/epoch - 206us/sample
Epoch 25/85
84077/84077 - 17s - loss: 3.7689e-04 - val_loss: 3.5539e-04 - 17s/epoch - 204us/sample
Epoch 26/85
84077/84077 - 17s - loss: 3.7309e-04 - val_loss: 3.5111e-04 - 17s/epoch - 204us/sample
Epoch 27/85
84077/84077 - 17s - loss: 3.6979e-04 - val_loss: 3.4587e-04 - 17s/epoch - 205us/sample
Epoch 28/85
84077/84077 - 17s - loss: 3.6738e-04 - val_loss: 3.4107e-04 - 17s/epoch - 207us/sample
Epoch 29/85
84077/84077 - 17s - loss: 3.6326e-04 - val_loss: 3.3521e-04 - 17s/epoch - 205us/sample
Epoch 30/85
84077/84077 - 17s - loss: 3.5831e-04 - val_loss: 3.3418e-04 - 17s/epoch - 204us/sample
Epoch 31/85
84077/84077 - 17s - loss: 3.5467e-04 - val_loss: 3.3102e-04 - 17s/epoch - 204us/sample
Epoch 32/85
84077/84077 - 17s - loss: 3.5315e-04 - val_loss: 3.3258e-04 - 17s/epoch - 206us/sample
Epoch 33/85
84077/84077 - 17s - loss: 3.4961e-04 - val_loss: 3.3296e-04 - 17s/epoch - 205us/sample
Epoch 34/85
84077/84077 - 17s - loss: 3.4623e-04 - val_loss: 3.2343e-04 - 17s/epoch - 204us/sample
Epoch 35/85
84077/84077 - 17s - loss: 3.4246e-04 - val_loss: 3.2039e-04 - 17s/epoch - 204us/sample
Epoch 36/85
84077/84077 - 17s - loss: 3.3824e-04 - val_loss: 3.1872e-04 - 17s/epoch - 207us/sample
Epoch 37/85
84077/84077 - 17s - loss: 3.3680e-04 - val_loss: 3.1425e-04 - 17s/epoch - 205us/sample
Epoch 38/85
84077/84077 - 17s - loss: 3.3377e-04 - val_loss: 3.1204e-04 - 17s/epoch - 204us/sample
Epoch 39/85
84077/84077 - 17s - loss: 3.3224e-04 - val_loss: 3.1264e-04 - 17s/epoch - 204us/sample
Epoch 40/85
84077/84077 - 17s - loss: 3.2960e-04 - val_loss: 3.1070e-04 - 17s/epoch - 207us/sample
Epoch 41/85
84077/84077 - 17s - loss: 3.2846e-04 - val_loss: 3.1139e-04 - 17s/epoch - 204us/sample
Epoch 42/85
84077/84077 - 17s - loss: 3.2796e-04 - val_loss: 3.0848e-04 - 17s/epoch - 204us/sample
Epoch 43/85
84077/84077 - 17s - loss: 3.2363e-04 - val_loss: 3.0538e-04 - 17s/epoch - 205us/sample
Epoch 44/85
84077/84077 - 17s - loss: 3.2336e-04 - val_loss: 3.0401e-04 - 17s/epoch - 206us/sample
Epoch 45/85
84077/84077 - 17s - loss: 3.2241e-04 - val_loss: 3.0101e-04 - 17s/epoch - 205us/sample
Epoch 46/85
84077/84077 - 17s - loss: 3.2044e-04 - val_loss: 3.0262e-04 - 17s/epoch - 204us/sample
Epoch 47/85
84077/84077 - 17s - loss: 3.1922e-04 - val_loss: 2.9966e-04 - 17s/epoch - 205us/sample
Epoch 48/85
84077/84077 - 17s - loss: 3.1773e-04 - val_loss: 2.9740e-04 - 17s/epoch - 206us/sample
Epoch 49/85
84077/84077 - 17s - loss: 3.1759e-04 - val_loss: 2.9614e-04 - 17s/epoch - 204us/sample
Epoch 50/85
84077/84077 - 17s - loss: 3.1377e-04 - val_loss: 2.9569e-04 - 17s/epoch - 205us/sample
Epoch 51/85
84077/84077 - 17s - loss: 3.1255e-04 - val_loss: 2.9410e-04 - 17s/epoch - 204us/sample
Epoch 52/85
84077/84077 - 17s - loss: 3.1220e-04 - val_loss: 2.9478e-04 - 17s/epoch - 207us/sample
Epoch 53/85
84077/84077 - 17s - loss: 3.1015e-04 - val_loss: 2.9292e-04 - 17s/epoch - 204us/sample
Epoch 54/85
84077/84077 - 17s - loss: 3.0933e-04 - val_loss: 2.8973e-04 - 17s/epoch - 204us/sample
Epoch 55/85
84077/84077 - 17s - loss: 3.0774e-04 - val_loss: 2.9065e-04 - 17s/epoch - 204us/sample
Epoch 56/85
84077/84077 - 17s - loss: 3.0684e-04 - val_loss: 2.9268e-04 - 17s/epoch - 206us/sample
Epoch 57/85
84077/84077 - 17s - loss: 3.0503e-04 - val_loss: 2.8935e-04 - 17s/epoch - 206us/sample
Epoch 58/85
84077/84077 - 17s - loss: 3.0465e-04 - val_loss: 2.8651e-04 - 17s/epoch - 204us/sample
Epoch 59/85
84077/84077 - 17s - loss: 3.0288e-04 - val_loss: 2.8582e-04 - 17s/epoch - 204us/sample
Epoch 60/85
84077/84077 - 17s - loss: 3.0187e-04 - val_loss: 2.8716e-04 - 17s/epoch - 207us/sample
Epoch 61/85
84077/84077 - 17s - loss: 3.0318e-04 - val_loss: 2.8401e-04 - 17s/epoch - 204us/sample
Epoch 62/85
84077/84077 - 17s - loss: 2.9995e-04 - val_loss: 2.8358e-04 - 17s/epoch - 204us/sample
Epoch 63/85
84077/84077 - 17s - loss: 2.9918e-04 - val_loss: 2.8251e-04 - 17s/epoch - 204us/sample
Epoch 64/85
84077/84077 - 17s - loss: 2.9840e-04 - val_loss: 2.8076e-04 - 17s/epoch - 206us/sample
Epoch 65/85
84077/84077 - 17s - loss: 2.9628e-04 - val_loss: 2.8092e-04 - 17s/epoch - 205us/sample
Epoch 66/85
84077/84077 - 17s - loss: 2.9537e-04 - val_loss: 2.7923e-04 - 17s/epoch - 204us/sample
Epoch 67/85
84077/84077 - 17s - loss: 2.9621e-04 - val_loss: 2.7817e-04 - 17s/epoch - 204us/sample
Epoch 68/85
84077/84077 - 17s - loss: 2.9377e-04 - val_loss: 2.7763e-04 - 17s/epoch - 206us/sample
Epoch 69/85
84077/84077 - 17s - loss: 2.9341e-04 - val_loss: 2.7868e-04 - 17s/epoch - 205us/sample
Epoch 70/85
84077/84077 - 17s - loss: 2.9198e-04 - val_loss: 2.7724e-04 - 17s/epoch - 204us/sample
Epoch 71/85
84077/84077 - 17s - loss: 2.9118e-04 - val_loss: 2.7665e-04 - 17s/epoch - 204us/sample
Epoch 72/85
84077/84077 - 17s - loss: 2.9198e-04 - val_loss: 2.7451e-04 - 17s/epoch - 205us/sample
Epoch 73/85
84077/84077 - 17s - loss: 2.8973e-04 - val_loss: 2.7466e-04 - 17s/epoch - 206us/sample
Epoch 74/85
84077/84077 - 17s - loss: 2.8896e-04 - val_loss: 2.7470e-04 - 17s/epoch - 204us/sample
Epoch 75/85
84077/84077 - 17s - loss: 2.8797e-04 - val_loss: 2.7032e-04 - 17s/epoch - 204us/sample
Epoch 76/85
84077/84077 - 17s - loss: 2.8510e-04 - val_loss: 2.6860e-04 - 17s/epoch - 204us/sample
Epoch 77/85
84077/84077 - 17s - loss: 2.8353e-04 - val_loss: 2.6749e-04 - 17s/epoch - 206us/sample
Epoch 78/85
84077/84077 - 17s - loss: 2.8225e-04 - val_loss: 2.6700e-04 - 17s/epoch - 204us/sample
Epoch 79/85
84077/84077 - 17s - loss: 2.8227e-04 - val_loss: 2.7075e-04 - 17s/epoch - 203us/sample
Epoch 80/85
84077/84077 - 17s - loss: 2.8081e-04 - val_loss: 2.6450e-04 - 17s/epoch - 204us/sample
Epoch 81/85
84077/84077 - 17s - loss: 2.8519e-04 - val_loss: 2.6677e-04 - 17s/epoch - 205us/sample
Epoch 82/85
84077/84077 - 17s - loss: 2.7944e-04 - val_loss: 2.6646e-04 - 17s/epoch - 205us/sample
Epoch 83/85
84077/84077 - 17s - loss: 2.7986e-04 - val_loss: 2.6424e-04 - 17s/epoch - 204us/sample
Epoch 84/85
84077/84077 - 17s - loss: 2.7863e-04 - val_loss: 2.6446e-04 - 17s/epoch - 204us/sample
Epoch 85/85
84077/84077 - 17s - loss: 2.7796e-04 - val_loss: 2.6340e-04 - 17s/epoch - 205us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002633996537490384
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 21:46:28.021177: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_50/outputlayer/BiasAdd' id:55518 op device:{requested: '', assigned: ''} def:{{{node decoder_model_50/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_50/outputlayer/MatMul, decoder_model_50/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04422286380911801
cosine 0.043677334572900774
MAE: 0.0026258784831505494
RMSE: 0.012803783997771483
r2: 0.8719814990983393
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 85, 0.0006000000000000001, 0.2, 188, 0.000277964064310745, 0.0002633996537490384, 0.04422286380911801, 0.043677334572900774, 0.0026258784831505494, 0.012803783997771483, 0.8719814990983393, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 4
Fitness    = 511.49230331698664
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 511.49230331698664.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664]
[1.7999999999999998 90 0.0004000000000000001 32 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_129 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_129 (ReLU)               (None, 1697)         0           ['batch_normalization_129[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 21:46:49.888850: W tensorflow/c/c_api.cc:291] Operation '{name:'training_70/Adam/dense_dec1_51/bias/v/Assign' id:57456 op device:{requested: '', assigned: ''} def:{{{node training_70/Adam/dense_dec1_51/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_70/Adam/dense_dec1_51/bias/v, training_70/Adam/dense_dec1_51/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 21:47:11.397026: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_51/mul' id:56809 op device:{requested: '', assigned: ''} def:{{{node loss_51/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_51/mul/x, loss_51/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 30s - loss: 0.0081 - val_loss: 9.3963e-04 - 30s/epoch - 356us/sample
Epoch 2/90
84077/84077 - 17s - loss: 0.0023 - val_loss: 6.5837e-04 - 17s/epoch - 207us/sample
Epoch 3/90
84077/84077 - 17s - loss: 7.3403e-04 - val_loss: 5.0738e-04 - 17s/epoch - 206us/sample
Epoch 4/90
84077/84077 - 17s - loss: 5.2989e-04 - val_loss: 5.3592e-04 - 17s/epoch - 205us/sample
Epoch 5/90
84077/84077 - 17s - loss: 5.5498e-04 - val_loss: 4.8018e-04 - 17s/epoch - 205us/sample
Epoch 6/90
84077/84077 - 17s - loss: 4.3426e-04 - val_loss: 3.6270e-04 - 17s/epoch - 206us/sample
Epoch 7/90
84077/84077 - 18s - loss: 3.8143e-04 - val_loss: 3.5032e-04 - 18s/epoch - 208us/sample
Epoch 8/90
84077/84077 - 17s - loss: 3.4048e-04 - val_loss: 2.8991e-04 - 17s/epoch - 205us/sample
Epoch 9/90
84077/84077 - 17s - loss: 3.1134e-04 - val_loss: 2.6743e-04 - 17s/epoch - 205us/sample
Epoch 10/90
84077/84077 - 17s - loss: 2.9224e-04 - val_loss: 2.5438e-04 - 17s/epoch - 207us/sample
Epoch 11/90
84077/84077 - 17s - loss: 2.7952e-04 - val_loss: 2.4563e-04 - 17s/epoch - 207us/sample
Epoch 12/90
84077/84077 - 17s - loss: 2.7034e-04 - val_loss: 2.3665e-04 - 17s/epoch - 205us/sample
Epoch 13/90
84077/84077 - 17s - loss: 2.6135e-04 - val_loss: 2.2837e-04 - 17s/epoch - 205us/sample
Epoch 14/90
84077/84077 - 17s - loss: 2.5556e-04 - val_loss: 2.2872e-04 - 17s/epoch - 206us/sample
Epoch 15/90
84077/84077 - 17s - loss: 2.5037e-04 - val_loss: 2.2020e-04 - 17s/epoch - 207us/sample
Epoch 16/90
84077/84077 - 17s - loss: 2.4628e-04 - val_loss: 2.1946e-04 - 17s/epoch - 206us/sample
Epoch 17/90
84077/84077 - 17s - loss: 2.4297e-04 - val_loss: 2.1686e-04 - 17s/epoch - 205us/sample
Epoch 18/90
84077/84077 - 17s - loss: 2.3822e-04 - val_loss: 2.1334e-04 - 17s/epoch - 206us/sample
Epoch 19/90
84077/84077 - 17s - loss: 2.3572e-04 - val_loss: 2.1079e-04 - 17s/epoch - 208us/sample
Epoch 20/90
84077/84077 - 17s - loss: 2.3126e-04 - val_loss: 2.0802e-04 - 17s/epoch - 205us/sample
Epoch 21/90
84077/84077 - 17s - loss: 2.2894e-04 - val_loss: 2.0627e-04 - 17s/epoch - 206us/sample
Epoch 22/90
84077/84077 - 17s - loss: 2.2559e-04 - val_loss: 2.0227e-04 - 17s/epoch - 207us/sample
Epoch 23/90
84077/84077 - 17s - loss: 2.2383e-04 - val_loss: 2.0086e-04 - 17s/epoch - 207us/sample
Epoch 24/90
84077/84077 - 17s - loss: 2.2054e-04 - val_loss: 1.9790e-04 - 17s/epoch - 206us/sample
Epoch 25/90
84077/84077 - 17s - loss: 2.1814e-04 - val_loss: 1.9567e-04 - 17s/epoch - 206us/sample
Epoch 26/90
84077/84077 - 17s - loss: 2.1491e-04 - val_loss: 1.9254e-04 - 17s/epoch - 207us/sample
Epoch 27/90
84077/84077 - 17s - loss: 2.1410e-04 - val_loss: 1.9202e-04 - 17s/epoch - 207us/sample
Epoch 28/90
84077/84077 - 17s - loss: 2.1183e-04 - val_loss: 1.8953e-04 - 17s/epoch - 206us/sample
Epoch 29/90
84077/84077 - 17s - loss: 2.0907e-04 - val_loss: 1.8789e-04 - 17s/epoch - 206us/sample
Epoch 30/90
84077/84077 - 17s - loss: 2.0729e-04 - val_loss: 1.8527e-04 - 17s/epoch - 207us/sample
Epoch 31/90
84077/84077 - 17s - loss: 2.0552e-04 - val_loss: 1.8410e-04 - 17s/epoch - 207us/sample
Epoch 32/90
84077/84077 - 17s - loss: 2.0418e-04 - val_loss: 1.8321e-04 - 17s/epoch - 206us/sample
Epoch 33/90
84077/84077 - 17s - loss: 2.0253e-04 - val_loss: 1.7820e-04 - 17s/epoch - 206us/sample
Epoch 34/90
84077/84077 - 17s - loss: 1.9887e-04 - val_loss: 1.7774e-04 - 17s/epoch - 207us/sample
Epoch 35/90
84077/84077 - 17s - loss: 1.9840e-04 - val_loss: 1.7701e-04 - 17s/epoch - 207us/sample
Epoch 36/90
84077/84077 - 17s - loss: 1.9747e-04 - val_loss: 1.7550e-04 - 17s/epoch - 205us/sample
Epoch 37/90
84077/84077 - 17s - loss: 1.9544e-04 - val_loss: 1.7381e-04 - 17s/epoch - 205us/sample
Epoch 38/90
84077/84077 - 17s - loss: 1.9422e-04 - val_loss: 1.7421e-04 - 17s/epoch - 207us/sample
Epoch 39/90
84077/84077 - 17s - loss: 1.9264e-04 - val_loss: 1.7493e-04 - 17s/epoch - 207us/sample
Epoch 40/90
84077/84077 - 17s - loss: 1.9353e-04 - val_loss: 1.7171e-04 - 17s/epoch - 206us/sample
Epoch 41/90
84077/84077 - 17s - loss: 1.9105e-04 - val_loss: 1.7017e-04 - 17s/epoch - 205us/sample
Epoch 42/90
84077/84077 - 17s - loss: 1.8937e-04 - val_loss: 1.7095e-04 - 17s/epoch - 207us/sample
Epoch 43/90
84077/84077 - 17s - loss: 1.8766e-04 - val_loss: 1.6895e-04 - 17s/epoch - 207us/sample
Epoch 44/90
84077/84077 - 17s - loss: 1.8738e-04 - val_loss: 1.7083e-04 - 17s/epoch - 205us/sample
Epoch 45/90
84077/84077 - 17s - loss: 1.8571e-04 - val_loss: 1.6843e-04 - 17s/epoch - 206us/sample
Epoch 46/90
84077/84077 - 17s - loss: 1.8450e-04 - val_loss: 1.6664e-04 - 17s/epoch - 207us/sample
Epoch 47/90
84077/84077 - 17s - loss: 1.8412e-04 - val_loss: 1.6743e-04 - 17s/epoch - 207us/sample
Epoch 48/90
84077/84077 - 17s - loss: 1.8312e-04 - val_loss: 1.6540e-04 - 17s/epoch - 206us/sample
Epoch 49/90
84077/84077 - 17s - loss: 1.8156e-04 - val_loss: 1.6222e-04 - 17s/epoch - 206us/sample
Epoch 50/90
84077/84077 - 17s - loss: 1.8124e-04 - val_loss: 1.6337e-04 - 17s/epoch - 206us/sample
Epoch 51/90
84077/84077 - 17s - loss: 1.8180e-04 - val_loss: 1.6428e-04 - 17s/epoch - 208us/sample
Epoch 52/90
84077/84077 - 17s - loss: 1.7960e-04 - val_loss: 1.6547e-04 - 17s/epoch - 206us/sample
Epoch 53/90
84077/84077 - 17s - loss: 1.7972e-04 - val_loss: 1.6268e-04 - 17s/epoch - 206us/sample
Epoch 54/90
84077/84077 - 17s - loss: 1.7837e-04 - val_loss: 1.6022e-04 - 17s/epoch - 206us/sample
Epoch 55/90
84077/84077 - 17s - loss: 1.7809e-04 - val_loss: 1.5952e-04 - 17s/epoch - 207us/sample
Epoch 56/90
84077/84077 - 17s - loss: 1.7678e-04 - val_loss: 1.5995e-04 - 17s/epoch - 205us/sample
Epoch 57/90
84077/84077 - 17s - loss: 1.7572e-04 - val_loss: 1.5790e-04 - 17s/epoch - 205us/sample
Epoch 58/90
84077/84077 - 17s - loss: 1.7670e-04 - val_loss: 1.5659e-04 - 17s/epoch - 206us/sample
Epoch 59/90
84077/84077 - 17s - loss: 1.7582e-04 - val_loss: 1.5727e-04 - 17s/epoch - 207us/sample
Epoch 60/90
84077/84077 - 17s - loss: 1.7443e-04 - val_loss: 1.5811e-04 - 17s/epoch - 205us/sample
Epoch 61/90
84077/84077 - 17s - loss: 1.7385e-04 - val_loss: 1.5623e-04 - 17s/epoch - 206us/sample
Epoch 62/90
84077/84077 - 18s - loss: 1.7327e-04 - val_loss: 1.5646e-04 - 18s/epoch - 208us/sample
Epoch 63/90
84077/84077 - 17s - loss: 1.7313e-04 - val_loss: 1.5802e-04 - 17s/epoch - 206us/sample
Epoch 64/90
84077/84077 - 17s - loss: 1.7318e-04 - val_loss: 1.5578e-04 - 17s/epoch - 206us/sample
Epoch 65/90
84077/84077 - 17s - loss: 1.7184e-04 - val_loss: 1.5725e-04 - 17s/epoch - 206us/sample
Epoch 66/90
84077/84077 - 17s - loss: 1.7218e-04 - val_loss: 1.6143e-04 - 17s/epoch - 208us/sample
Epoch 67/90
84077/84077 - 17s - loss: 1.7098e-04 - val_loss: 1.5494e-04 - 17s/epoch - 206us/sample
Epoch 68/90
84077/84077 - 17s - loss: 1.7099e-04 - val_loss: 1.5498e-04 - 17s/epoch - 206us/sample
Epoch 69/90
84077/84077 - 17s - loss: 1.7007e-04 - val_loss: 1.5359e-04 - 17s/epoch - 206us/sample
Epoch 70/90
84077/84077 - 17s - loss: 1.6991e-04 - val_loss: 1.5268e-04 - 17s/epoch - 208us/sample
Epoch 71/90
84077/84077 - 17s - loss: 1.6930e-04 - val_loss: 1.5409e-04 - 17s/epoch - 206us/sample
Epoch 72/90
84077/84077 - 17s - loss: 1.6946e-04 - val_loss: 1.5367e-04 - 17s/epoch - 205us/sample
Epoch 73/90
84077/84077 - 17s - loss: 1.6877e-04 - val_loss: 1.5347e-04 - 17s/epoch - 206us/sample
Epoch 74/90
84077/84077 - 17s - loss: 1.6849e-04 - val_loss: 1.5255e-04 - 17s/epoch - 207us/sample
Epoch 75/90
84077/84077 - 17s - loss: 1.6832e-04 - val_loss: 1.5146e-04 - 17s/epoch - 206us/sample
Epoch 76/90
84077/84077 - 17s - loss: 1.6794e-04 - val_loss: 1.5169e-04 - 17s/epoch - 205us/sample
Epoch 77/90
84077/84077 - 17s - loss: 1.6738e-04 - val_loss: 1.5209e-04 - 17s/epoch - 206us/sample
Epoch 78/90
84077/84077 - 17s - loss: 1.6718e-04 - val_loss: 1.5277e-04 - 17s/epoch - 208us/sample
Epoch 79/90
84077/84077 - 17s - loss: 1.6695e-04 - val_loss: 1.5040e-04 - 17s/epoch - 206us/sample
Epoch 80/90
84077/84077 - 17s - loss: 1.6697e-04 - val_loss: 1.5031e-04 - 17s/epoch - 206us/sample
Epoch 81/90
84077/84077 - 17s - loss: 1.6783e-04 - val_loss: 1.5241e-04 - 17s/epoch - 206us/sample
Epoch 82/90
84077/84077 - 17s - loss: 1.6658e-04 - val_loss: 1.5029e-04 - 17s/epoch - 208us/sample
Epoch 83/90
84077/84077 - 17s - loss: 1.6621e-04 - val_loss: 1.5158e-04 - 17s/epoch - 206us/sample
Epoch 84/90
84077/84077 - 17s - loss: 1.6548e-04 - val_loss: 1.5048e-04 - 17s/epoch - 205us/sample
Epoch 85/90
84077/84077 - 17s - loss: 1.6550e-04 - val_loss: 1.5000e-04 - 17s/epoch - 206us/sample
Epoch 86/90
84077/84077 - 17s - loss: 1.6516e-04 - val_loss: 1.4924e-04 - 17s/epoch - 207us/sample
Epoch 87/90
84077/84077 - 17s - loss: 1.6473e-04 - val_loss: 1.5009e-04 - 17s/epoch - 207us/sample
Epoch 88/90
84077/84077 - 17s - loss: 1.6648e-04 - val_loss: 1.5189e-04 - 17s/epoch - 206us/sample
Epoch 89/90
84077/84077 - 17s - loss: 1.6404e-04 - val_loss: 1.4954e-04 - 17s/epoch - 205us/sample
Epoch 90/90
84077/84077 - 17s - loss: 1.6376e-04 - val_loss: 1.5001e-04 - 17s/epoch - 207us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00015000888877439114
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 22:12:59.608692: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_51/outputlayer/BiasAdd' id:56773 op device:{requested: '', assigned: ''} def:{{{node decoder_model_51/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_51/outputlayer/MatMul, decoder_model_51/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.042032007974114986
cosine 0.04152453076711135
MAE: 0.002518980525356497
RMSE: 0.012168416884629133
r2: 0.8843839238270327
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 32, 90, 0.0004000000000000001, 0.2, 188, 0.00016375732464250415, 0.00015000888877439114, 0.042032007974114986, 0.04152453076711135, 0.002518980525356497, 0.012168416884629133, 0.8843839238270327, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.00020000000000000006 16 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_132 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_132 (ReLU)               (None, 1697)         0           ['batch_normalization_132[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 22:13:21.787074: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_52/kernel/Assign' id:57966 op device:{requested: '', assigned: ''} def:{{{node outputlayer_52/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_52/kernel, outputlayer_52/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 22:13:55.198218: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_52/mul' id:58094 op device:{requested: '', assigned: ''} def:{{{node loss_52/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_52/mul/x, loss_52/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 43s - loss: 0.0022 - val_loss: 6.4956e-04 - 43s/epoch - 510us/sample
Epoch 2/90
84077/84077 - 30s - loss: 7.2334e-04 - val_loss: 5.2169e-04 - 30s/epoch - 354us/sample
Epoch 3/90
84077/84077 - 30s - loss: 5.5496e-04 - val_loss: 4.4590e-04 - 30s/epoch - 354us/sample
Epoch 4/90
84077/84077 - 30s - loss: 4.4361e-04 - val_loss: 3.8775e-04 - 30s/epoch - 356us/sample
Epoch 5/90
84077/84077 - 30s - loss: 4.1174e-04 - val_loss: 3.9232e-04 - 30s/epoch - 353us/sample
Epoch 6/90
84077/84077 - 30s - loss: 3.9833e-04 - val_loss: 3.9454e-04 - 30s/epoch - 356us/sample
Epoch 7/90
84077/84077 - 30s - loss: 3.8906e-04 - val_loss: 3.9792e-04 - 30s/epoch - 354us/sample
Epoch 8/90
84077/84077 - 30s - loss: 3.8123e-04 - val_loss: 3.8465e-04 - 30s/epoch - 354us/sample
Epoch 9/90
84077/84077 - 30s - loss: 3.7514e-04 - val_loss: 4.3131e-04 - 30s/epoch - 355us/sample
Epoch 10/90
84077/84077 - 30s - loss: 3.6812e-04 - val_loss: 3.7685e-04 - 30s/epoch - 353us/sample
Epoch 11/90
84077/84077 - 30s - loss: 3.6329e-04 - val_loss: 4.0228e-04 - 30s/epoch - 356us/sample
Epoch 12/90
84077/84077 - 30s - loss: 3.5795e-04 - val_loss: 3.8317e-04 - 30s/epoch - 354us/sample
Epoch 13/90
84077/84077 - 30s - loss: 3.5329e-04 - val_loss: 3.8792e-04 - 30s/epoch - 355us/sample
Epoch 14/90
84077/84077 - 30s - loss: 3.5081e-04 - val_loss: 3.7621e-04 - 30s/epoch - 354us/sample
Epoch 15/90
84077/84077 - 30s - loss: 3.4868e-04 - val_loss: 3.6585e-04 - 30s/epoch - 354us/sample
Epoch 16/90
84077/84077 - 30s - loss: 3.4654e-04 - val_loss: 3.6335e-04 - 30s/epoch - 356us/sample
Epoch 17/90
84077/84077 - 30s - loss: 3.4421e-04 - val_loss: 3.6423e-04 - 30s/epoch - 353us/sample
Epoch 18/90
84077/84077 - 30s - loss: 3.4169e-04 - val_loss: 3.6209e-04 - 30s/epoch - 356us/sample
Epoch 19/90
84077/84077 - 30s - loss: 3.3991e-04 - val_loss: 3.5275e-04 - 30s/epoch - 353us/sample
Epoch 20/90
84077/84077 - 30s - loss: 3.3831e-04 - val_loss: 3.4199e-04 - 30s/epoch - 354us/sample
Epoch 21/90
84077/84077 - 30s - loss: 3.3759e-04 - val_loss: 3.5205e-04 - 30s/epoch - 355us/sample
Epoch 22/90
84077/84077 - 30s - loss: 3.3681e-04 - val_loss: 3.5183e-04 - 30s/epoch - 353us/sample
Epoch 23/90
84077/84077 - 30s - loss: 3.3572e-04 - val_loss: 3.4967e-04 - 30s/epoch - 356us/sample
Epoch 24/90
84077/84077 - 30s - loss: 3.3300e-04 - val_loss: 3.3939e-04 - 30s/epoch - 353us/sample
Epoch 25/90
84077/84077 - 30s - loss: 3.3155e-04 - val_loss: 3.3537e-04 - 30s/epoch - 356us/sample
Epoch 26/90
84077/84077 - 30s - loss: 3.2979e-04 - val_loss: 3.4466e-04 - 30s/epoch - 353us/sample
Epoch 27/90
84077/84077 - 30s - loss: 3.2866e-04 - val_loss: 3.3763e-04 - 30s/epoch - 354us/sample
Epoch 28/90
84077/84077 - 30s - loss: 3.2809e-04 - val_loss: 3.3398e-04 - 30s/epoch - 355us/sample
Epoch 29/90
84077/84077 - 30s - loss: 3.2805e-04 - val_loss: 3.3180e-04 - 30s/epoch - 354us/sample
Epoch 30/90
84077/84077 - 30s - loss: 3.2699e-04 - val_loss: 3.3265e-04 - 30s/epoch - 356us/sample
Epoch 31/90
84077/84077 - 30s - loss: 3.2665e-04 - val_loss: 3.2413e-04 - 30s/epoch - 353us/sample
Epoch 32/90
84077/84077 - 30s - loss: 3.2657e-04 - val_loss: 3.2780e-04 - 30s/epoch - 356us/sample
Epoch 33/90
84077/84077 - 30s - loss: 3.2565e-04 - val_loss: 3.2441e-04 - 30s/epoch - 353us/sample
Epoch 34/90
84077/84077 - 30s - loss: 3.2527e-04 - val_loss: 3.2803e-04 - 30s/epoch - 354us/sample
Epoch 35/90
84077/84077 - 30s - loss: 3.2448e-04 - val_loss: 3.2236e-04 - 30s/epoch - 356us/sample
Epoch 36/90
84077/84077 - 30s - loss: 3.2462e-04 - val_loss: 3.2031e-04 - 30s/epoch - 353us/sample
Epoch 37/90
84077/84077 - 30s - loss: 3.2396e-04 - val_loss: 3.1674e-04 - 30s/epoch - 356us/sample
Epoch 38/90
84077/84077 - 30s - loss: 3.2394e-04 - val_loss: 3.2372e-04 - 30s/epoch - 353us/sample
Epoch 39/90
84077/84077 - 30s - loss: 3.2312e-04 - val_loss: 3.2102e-04 - 30s/epoch - 356us/sample
Epoch 40/90
84077/84077 - 30s - loss: 3.2304e-04 - val_loss: 3.1847e-04 - 30s/epoch - 354us/sample
Epoch 41/90
84077/84077 - 30s - loss: 3.2270e-04 - val_loss: 3.1862e-04 - 30s/epoch - 353us/sample
Epoch 42/90
84077/84077 - 30s - loss: 3.2180e-04 - val_loss: 3.2105e-04 - 30s/epoch - 356us/sample
Epoch 43/90
84077/84077 - 30s - loss: 3.2197e-04 - val_loss: 3.1819e-04 - 30s/epoch - 353us/sample
Epoch 44/90
84077/84077 - 30s - loss: 3.2142e-04 - val_loss: 3.1841e-04 - 30s/epoch - 355us/sample
Epoch 45/90
84077/84077 - 30s - loss: 3.2104e-04 - val_loss: 3.2175e-04 - 30s/epoch - 355us/sample
Epoch 46/90
84077/84077 - 30s - loss: 3.2102e-04 - val_loss: 3.2095e-04 - 30s/epoch - 354us/sample
Epoch 47/90
84077/84077 - 30s - loss: 3.2049e-04 - val_loss: 3.1913e-04 - 30s/epoch - 357us/sample
Epoch 48/90
84077/84077 - 30s - loss: 3.2003e-04 - val_loss: 3.2633e-04 - 30s/epoch - 354us/sample
Epoch 49/90
84077/84077 - 30s - loss: 3.1985e-04 - val_loss: 3.2060e-04 - 30s/epoch - 356us/sample
Epoch 50/90
84077/84077 - 30s - loss: 3.1933e-04 - val_loss: 3.1871e-04 - 30s/epoch - 354us/sample
Epoch 51/90
84077/84077 - 30s - loss: 3.1921e-04 - val_loss: 3.2410e-04 - 30s/epoch - 354us/sample
Epoch 52/90
84077/84077 - 30s - loss: 3.1815e-04 - val_loss: 3.2258e-04 - 30s/epoch - 356us/sample
Epoch 53/90
84077/84077 - 30s - loss: 3.1775e-04 - val_loss: 3.2487e-04 - 30s/epoch - 354us/sample
Epoch 54/90
84077/84077 - 30s - loss: 3.1626e-04 - val_loss: 3.2658e-04 - 30s/epoch - 356us/sample
Epoch 55/90
84077/84077 - 30s - loss: 3.1515e-04 - val_loss: 3.1893e-04 - 30s/epoch - 353us/sample
Epoch 56/90
84077/84077 - 30s - loss: 3.1482e-04 - val_loss: 3.2435e-04 - 30s/epoch - 355us/sample
Epoch 57/90
84077/84077 - 30s - loss: 3.1448e-04 - val_loss: 3.2181e-04 - 30s/epoch - 353us/sample
Epoch 58/90
84077/84077 - 30s - loss: 3.1374e-04 - val_loss: 3.3158e-04 - 30s/epoch - 354us/sample
Epoch 59/90
84077/84077 - 30s - loss: 3.1326e-04 - val_loss: 3.3865e-04 - 30s/epoch - 357us/sample
Epoch 60/90
84077/84077 - 30s - loss: 3.1310e-04 - val_loss: 3.3231e-04 - 30s/epoch - 353us/sample
Epoch 61/90
84077/84077 - 30s - loss: 3.1247e-04 - val_loss: 3.2401e-04 - 30s/epoch - 356us/sample
Epoch 62/90
84077/84077 - 30s - loss: 3.1229e-04 - val_loss: 3.2947e-04 - 30s/epoch - 353us/sample
Epoch 63/90
84077/84077 - 30s - loss: 3.1210e-04 - val_loss: 3.3053e-04 - 30s/epoch - 356us/sample
Epoch 64/90
84077/84077 - 30s - loss: 3.1181e-04 - val_loss: 3.2878e-04 - 30s/epoch - 355us/sample
Epoch 65/90
84077/84077 - 30s - loss: 3.1151e-04 - val_loss: 3.3820e-04 - 30s/epoch - 354us/sample
Epoch 66/90
84077/84077 - 30s - loss: 3.1080e-04 - val_loss: 3.2571e-04 - 30s/epoch - 356us/sample
Epoch 67/90
84077/84077 - 30s - loss: 3.1079e-04 - val_loss: 3.2856e-04 - 30s/epoch - 353us/sample
Epoch 68/90
84077/84077 - 30s - loss: 3.1030e-04 - val_loss: 3.3638e-04 - 30s/epoch - 356us/sample
Epoch 69/90
84077/84077 - 30s - loss: 3.1026e-04 - val_loss: 3.4303e-04 - 30s/epoch - 354us/sample
Epoch 70/90
84077/84077 - 30s - loss: 3.1000e-04 - val_loss: 3.3299e-04 - 30s/epoch - 354us/sample
Epoch 71/90
84077/84077 - 30s - loss: 3.1009e-04 - val_loss: 3.3659e-04 - 30s/epoch - 355us/sample
Epoch 72/90
84077/84077 - 30s - loss: 3.0973e-04 - val_loss: 3.3820e-04 - 30s/epoch - 353us/sample
Epoch 73/90
84077/84077 - 30s - loss: 3.0957e-04 - val_loss: 3.3342e-04 - 30s/epoch - 356us/sample
Epoch 74/90
84077/84077 - 30s - loss: 3.0993e-04 - val_loss: 3.3031e-04 - 30s/epoch - 354us/sample
Epoch 75/90
84077/84077 - 30s - loss: 3.0943e-04 - val_loss: 3.3514e-04 - 30s/epoch - 354us/sample
Epoch 76/90
84077/84077 - 30s - loss: 3.0938e-04 - val_loss: 3.3935e-04 - 30s/epoch - 356us/sample
Epoch 77/90
84077/84077 - 30s - loss: 3.0965e-04 - val_loss: 3.3099e-04 - 30s/epoch - 354us/sample
Epoch 78/90
84077/84077 - 30s - loss: 3.0909e-04 - val_loss: 3.3587e-04 - 30s/epoch - 357us/sample
Epoch 79/90
84077/84077 - 30s - loss: 3.0909e-04 - val_loss: 3.4987e-04 - 30s/epoch - 353us/sample
Epoch 80/90
84077/84077 - 30s - loss: 3.0887e-04 - val_loss: 3.3314e-04 - 30s/epoch - 355us/sample
Epoch 81/90
84077/84077 - 30s - loss: 3.0871e-04 - val_loss: 3.4144e-04 - 30s/epoch - 355us/sample
Epoch 82/90
84077/84077 - 30s - loss: 3.0868e-04 - val_loss: 3.2660e-04 - 30s/epoch - 354us/sample
Epoch 83/90
84077/84077 - 30s - loss: 3.0872e-04 - val_loss: 3.3340e-04 - 30s/epoch - 356us/sample
Epoch 84/90
84077/84077 - 30s - loss: 3.0841e-04 - val_loss: 3.4900e-04 - 30s/epoch - 355us/sample
Epoch 85/90
84077/84077 - 30s - loss: 3.0812e-04 - val_loss: 3.2913e-04 - 30s/epoch - 356us/sample
Epoch 86/90
84077/84077 - 30s - loss: 3.0823e-04 - val_loss: 3.2402e-04 - 30s/epoch - 354us/sample
Epoch 87/90
84077/84077 - 30s - loss: 3.0813e-04 - val_loss: 3.2360e-04 - 30s/epoch - 354us/sample
Epoch 88/90
84077/84077 - 30s - loss: 3.0766e-04 - val_loss: 3.2271e-04 - 30s/epoch - 355us/sample
Epoch 89/90
84077/84077 - 30s - loss: 3.0728e-04 - val_loss: 3.2967e-04 - 30s/epoch - 354us/sample
Epoch 90/90
84077/84077 - 30s - loss: 3.0777e-04 - val_loss: 3.2891e-04 - 30s/epoch - 356us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00032890963434687137
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 22:58:14.798412: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_52/outputlayer/BiasAdd' id:58058 op device:{requested: '', assigned: ''} def:{{{node decoder_model_52/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_52/outputlayer/MatMul, decoder_model_52/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.1381726179956545
cosine 0.13628787585840724
MAE: 0.0041047603937621555
RMSE: 0.02404818103258676
r2: 0.5482365823028909
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 16, 90, 0.00020000000000000006, 0.2, 188, 0.00030776641944418707, 0.00032890963434687137, 0.1381726179956545, 0.13628787585840724, 0.0041047603937621555, 0.02404818103258676, 0.5482365823028909, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 16 0] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_135 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_135 (ReLU)               (None, 1886)         0           ['batch_normalization_135[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 22:58:37.395494: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_137/gamma/Assign' id:59181 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_137/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_137/gamma, batch_normalization_137/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 22:59:12.068296: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_53/mul' id:59391 op device:{requested: '', assigned: ''} def:{{{node loss_53/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_53/mul/x, loss_53/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 44s - loss: 0.0679 - val_loss: 0.0674 - 44s/epoch - 526us/sample
Epoch 2/90
84077/84077 - 31s - loss: 0.0700 - val_loss: 0.0684 - 31s/epoch - 364us/sample
Epoch 3/90
84077/84077 - 31s - loss: 0.0671 - val_loss: 0.0674 - 31s/epoch - 367us/sample
Epoch 4/90
84077/84077 - 31s - loss: 0.0670 - val_loss: 0.0674 - 31s/epoch - 364us/sample
Epoch 5/90
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 365us/sample
Epoch 6/90
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 366us/sample
Epoch 7/90
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 364us/sample
Epoch 8/90
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 9/90
84077/84077 - 45s - loss: 0.0668 - val_loss: 0.0673 - 45s/epoch - 540us/sample
Epoch 10/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 560us/sample
Epoch 11/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 570us/sample
Epoch 12/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 564us/sample
Epoch 13/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 557us/sample
Epoch 14/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 568us/sample
Epoch 15/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 568us/sample
Epoch 16/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 556us/sample
Epoch 17/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 568us/sample
Epoch 18/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 567us/sample
Epoch 19/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0674 - 47s/epoch - 563us/sample
Epoch 20/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 21/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 570us/sample
Epoch 22/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 564us/sample
Epoch 23/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 24/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 570us/sample
Epoch 25/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 26/90
84077/84077 - 44s - loss: 0.0668 - val_loss: 0.0673 - 44s/epoch - 528us/sample
Epoch 27/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 570us/sample
Epoch 28/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 29/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 556us/sample
Epoch 30/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 567us/sample
Epoch 31/90
84077/84077 - 44s - loss: 0.0668 - val_loss: 0.0673 - 44s/epoch - 527us/sample
Epoch 32/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 568us/sample
Epoch 33/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 560us/sample
Epoch 34/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 35/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 567us/sample
Epoch 36/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 564us/sample
Epoch 37/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 38/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 568us/sample
Epoch 39/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 40/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 561us/sample
Epoch 41/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 42/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 43/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 44/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 567us/sample
Epoch 45/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 562us/sample
Epoch 46/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 47/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 567us/sample
Epoch 48/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 568us/sample
Epoch 49/90
84077/84077 - 46s - loss: 0.0668 - val_loss: 0.0673 - 46s/epoch - 548us/sample
Epoch 50/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 560us/sample
Epoch 51/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 52/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 565us/sample
Epoch 53/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 562us/sample
Epoch 54/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 55/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 56/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 57/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 58/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 562us/sample
Epoch 59/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 564us/sample
Epoch 60/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 567us/sample
Epoch 61/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 564us/sample
Epoch 62/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 63/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 568us/sample
Epoch 64/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 567us/sample
Epoch 65/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 559us/sample
Epoch 66/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 67/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 68/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 554us/sample
Epoch 69/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 70/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 71/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 564us/sample
Epoch 72/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 560us/sample
Epoch 73/90
84077/84077 - 46s - loss: 0.0668 - val_loss: 0.0673 - 46s/epoch - 543us/sample
Epoch 74/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 570us/sample
Epoch 75/90
84077/84077 - 46s - loss: 0.0668 - val_loss: 0.0673 - 46s/epoch - 551us/sample
Epoch 76/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 570us/sample
Epoch 77/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 570us/sample
Epoch 78/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 560us/sample
Epoch 79/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 566us/sample
Epoch 80/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 81/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 563us/sample
Epoch 82/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 559us/sample
Epoch 83/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 571us/sample
Epoch 84/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 562us/sample
Epoch 85/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 562us/sample
Epoch 86/90
84077/84077 - 46s - loss: 0.0668 - val_loss: 0.0673 - 46s/epoch - 548us/sample
Epoch 87/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 88/90
84077/84077 - 47s - loss: 0.0668 - val_loss: 0.0673 - 47s/epoch - 555us/sample
Epoch 89/90
84077/84077 - 48s - loss: 0.0668 - val_loss: 0.0673 - 48s/epoch - 569us/sample
Epoch 90/90
84077/84077 - 46s - loss: 0.0668 - val_loss: 0.0673 - 46s/epoch - 553us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573594417109
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 00:07:31.758738: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_53/outputlayer/BiasAdd' id:59343 op device:{requested: '', assigned: ''} def:{{{node decoder_model_53/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_53/outputlayer/MatMul, decoder_model_53/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1728708892720514
cosine 1.1662410057863597
MAE: 5.468266636099734
RMSE: 6.604609378671773
r2: -31921.974553788237
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 16, 90, 0.001, 0.2, 188, 0.0668369098062745, 0.06734573594417109, 1.1728708892720514, 1.1662410057863597, 5.468266636099734, 6.604609378671773, -31921.974553788237, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 85 0.0004000000000000001 8 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_138 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_138 (ReLU)               (None, 1697)         0           ['batch_normalization_138[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 00:07:57.535888: W tensorflow/c/c_api.cc:291] Operation '{name:'training_76/Adam/dense_enc0_54/bias/m/Assign' id:61169 op device:{requested: '', assigned: ''} def:{{{node training_76/Adam/dense_enc0_54/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_76/Adam/dense_enc0_54/bias/m, training_76/Adam/dense_enc0_54/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 00:09:26.617117: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_54/mul' id:60709 op device:{requested: '', assigned: ''} def:{{{node loss_54/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_54/mul/x, loss_54/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 103s - loss: 0.0113 - val_loss: 0.0018 - 103s/epoch - 1ms/sample
Epoch 2/85
84077/84077 - 87s - loss: 0.0013 - val_loss: 0.0031 - 87s/epoch - 1ms/sample
Epoch 3/85
84077/84077 - 87s - loss: 9.9083e-04 - val_loss: 0.0021 - 87s/epoch - 1ms/sample
Epoch 4/85
84077/84077 - 85s - loss: 8.9381e-04 - val_loss: 0.0235 - 85s/epoch - 1ms/sample
Epoch 5/85
84077/84077 - 87s - loss: 8.4532e-04 - val_loss: 0.0579 - 87s/epoch - 1ms/sample
Epoch 6/85
84077/84077 - 87s - loss: 8.0676e-04 - val_loss: 0.0819 - 87s/epoch - 1ms/sample
Epoch 7/85
84077/84077 - 87s - loss: 7.7799e-04 - val_loss: 0.1095 - 87s/epoch - 1ms/sample
Epoch 8/85
84077/84077 - 87s - loss: 7.6170e-04 - val_loss: 0.1197 - 87s/epoch - 1ms/sample
Epoch 9/85
84077/84077 - 86s - loss: 7.4873e-04 - val_loss: 0.1293 - 86s/epoch - 1ms/sample
Epoch 10/85
84077/84077 - 87s - loss: 7.4009e-04 - val_loss: 0.1726 - 87s/epoch - 1ms/sample
Epoch 11/85
84077/84077 - 86s - loss: 7.3497e-04 - val_loss: 0.1618 - 86s/epoch - 1ms/sample
Epoch 12/85
84077/84077 - 87s - loss: 7.2904e-04 - val_loss: 0.1559 - 87s/epoch - 1ms/sample
Epoch 13/85
84077/84077 - 87s - loss: 7.2508e-04 - val_loss: 0.1574 - 87s/epoch - 1ms/sample
Epoch 14/85
84077/84077 - 86s - loss: 7.1917e-04 - val_loss: 0.1551 - 86s/epoch - 1ms/sample
Epoch 15/85
84077/84077 - 87s - loss: 7.1320e-04 - val_loss: 0.1559 - 87s/epoch - 1ms/sample
Epoch 16/85
84077/84077 - 86s - loss: 7.0783e-04 - val_loss: 0.1639 - 86s/epoch - 1ms/sample
Epoch 17/85
84077/84077 - 87s - loss: 7.0324e-04 - val_loss: 0.1682 - 87s/epoch - 1ms/sample
Epoch 18/85
84077/84077 - 87s - loss: 6.9871e-04 - val_loss: 0.1735 - 87s/epoch - 1ms/sample
Epoch 19/85
84077/84077 - 87s - loss: 6.9676e-04 - val_loss: 0.1700 - 87s/epoch - 1ms/sample
Epoch 20/85
84077/84077 - 87s - loss: 6.9495e-04 - val_loss: 0.1527 - 87s/epoch - 1ms/sample
Epoch 21/85
84077/84077 - 86s - loss: 6.9008e-04 - val_loss: 0.1734 - 86s/epoch - 1ms/sample
Epoch 22/85
84077/84077 - 87s - loss: 6.9084e-04 - val_loss: 0.1819 - 87s/epoch - 1ms/sample
Epoch 23/85
84077/84077 - 85s - loss: 6.8874e-04 - val_loss: 0.1827 - 85s/epoch - 1ms/sample
Epoch 24/85
84077/84077 - 86s - loss: 6.8637e-04 - val_loss: 0.1605 - 86s/epoch - 1ms/sample
Epoch 25/85
84077/84077 - 87s - loss: 6.8501e-04 - val_loss: 0.1559 - 87s/epoch - 1ms/sample
Epoch 26/85
84077/84077 - 87s - loss: 6.8311e-04 - val_loss: 0.1655 - 87s/epoch - 1ms/sample
Epoch 27/85
84077/84077 - 86s - loss: 6.8165e-04 - val_loss: 0.1771 - 86s/epoch - 1ms/sample
Epoch 28/85
84077/84077 - 87s - loss: 6.7970e-04 - val_loss: 0.1669 - 87s/epoch - 1ms/sample
Epoch 29/85
84077/84077 - 86s - loss: 6.7931e-04 - val_loss: 0.1534 - 86s/epoch - 1ms/sample
Epoch 30/85
84077/84077 - 87s - loss: 6.7837e-04 - val_loss: 0.1616 - 87s/epoch - 1ms/sample
Epoch 31/85
84077/84077 - 86s - loss: 6.7597e-04 - val_loss: 0.1477 - 86s/epoch - 1ms/sample
Epoch 32/85
84077/84077 - 87s - loss: 6.7577e-04 - val_loss: 0.1724 - 87s/epoch - 1ms/sample
Epoch 33/85
84077/84077 - 86s - loss: 6.7479e-04 - val_loss: 0.1594 - 86s/epoch - 1ms/sample
Epoch 34/85
84077/84077 - 86s - loss: 6.7347e-04 - val_loss: 0.1562 - 86s/epoch - 1ms/sample
Epoch 35/85
84077/84077 - 81s - loss: 6.7237e-04 - val_loss: 0.1429 - 81s/epoch - 959us/sample
Epoch 36/85
84077/84077 - 54s - loss: 6.7061e-04 - val_loss: 0.1546 - 54s/epoch - 646us/sample
Epoch 37/85
84077/84077 - 54s - loss: 6.7090e-04 - val_loss: 0.1412 - 54s/epoch - 644us/sample
Epoch 38/85
84077/84077 - 54s - loss: 6.6884e-04 - val_loss: 0.1416 - 54s/epoch - 643us/sample
Epoch 39/85
84077/84077 - 54s - loss: 6.6875e-04 - val_loss: 0.1437 - 54s/epoch - 645us/sample
Epoch 40/85
84077/84077 - 54s - loss: 6.6731e-04 - val_loss: 0.1384 - 54s/epoch - 645us/sample
Epoch 41/85
84077/84077 - 54s - loss: 6.6624e-04 - val_loss: 0.1338 - 54s/epoch - 645us/sample
Epoch 42/85
84077/84077 - 54s - loss: 6.6655e-04 - val_loss: 0.1343 - 54s/epoch - 643us/sample
Epoch 43/85
84077/84077 - 54s - loss: 6.6477e-04 - val_loss: 0.1463 - 54s/epoch - 646us/sample
Epoch 44/85
84077/84077 - 54s - loss: 6.6441e-04 - val_loss: 0.1689 - 54s/epoch - 646us/sample
Epoch 45/85
84077/84077 - 54s - loss: 6.6352e-04 - val_loss: 0.1376 - 54s/epoch - 644us/sample
Epoch 46/85
84077/84077 - 54s - loss: 6.6146e-04 - val_loss: 0.1601 - 54s/epoch - 644us/sample
Epoch 47/85
84077/84077 - 54s - loss: 6.6184e-04 - val_loss: 0.1571 - 54s/epoch - 644us/sample
Epoch 48/85
84077/84077 - 54s - loss: 6.6157e-04 - val_loss: 0.1341 - 54s/epoch - 646us/sample
Epoch 49/85
84077/84077 - 54s - loss: 6.6061e-04 - val_loss: 0.1307 - 54s/epoch - 646us/sample
Epoch 50/85
84077/84077 - 54s - loss: 6.6007e-04 - val_loss: 0.1405 - 54s/epoch - 644us/sample
Epoch 51/85
84077/84077 - 54s - loss: 6.5983e-04 - val_loss: 0.1312 - 54s/epoch - 645us/sample
Epoch 52/85
84077/84077 - 54s - loss: 6.6007e-04 - val_loss: 0.1287 - 54s/epoch - 644us/sample
Epoch 53/85
84077/84077 - 54s - loss: 6.5804e-04 - val_loss: 0.1198 - 54s/epoch - 644us/sample
Epoch 54/85
84077/84077 - 54s - loss: 6.5670e-04 - val_loss: 0.1143 - 54s/epoch - 644us/sample
Epoch 55/85
84077/84077 - 54s - loss: 6.5699e-04 - val_loss: 0.1471 - 54s/epoch - 646us/sample
Epoch 56/85
84077/84077 - 54s - loss: 6.5708e-04 - val_loss: 0.1151 - 54s/epoch - 646us/sample
Epoch 57/85
84077/84077 - 54s - loss: 6.5643e-04 - val_loss: 0.1261 - 54s/epoch - 642us/sample
Epoch 58/85
84077/84077 - 54s - loss: 6.5535e-04 - val_loss: 0.1312 - 54s/epoch - 645us/sample
Epoch 59/85
84077/84077 - 54s - loss: 6.5542e-04 - val_loss: 0.0981 - 54s/epoch - 645us/sample
Epoch 60/85
84077/84077 - 54s - loss: 6.5488e-04 - val_loss: 0.1294 - 54s/epoch - 645us/sample
Epoch 61/85
84077/84077 - 54s - loss: 6.5429e-04 - val_loss: 0.1109 - 54s/epoch - 645us/sample
Epoch 62/85
84077/84077 - 54s - loss: 6.5320e-04 - val_loss: 0.1300 - 54s/epoch - 645us/sample
Epoch 63/85
84077/84077 - 54s - loss: 6.5197e-04 - val_loss: 0.1329 - 54s/epoch - 645us/sample
Epoch 64/85
84077/84077 - 54s - loss: 6.5289e-04 - val_loss: 0.1032 - 54s/epoch - 645us/sample
Epoch 65/85
84077/84077 - 54s - loss: 6.5249e-04 - val_loss: 0.0992 - 54s/epoch - 646us/sample
Epoch 66/85
84077/84077 - 54s - loss: 6.5263e-04 - val_loss: 0.1351 - 54s/epoch - 645us/sample
Epoch 67/85
84077/84077 - 54s - loss: 6.5195e-04 - val_loss: 0.1393 - 54s/epoch - 646us/sample
Epoch 68/85
84077/84077 - 54s - loss: 6.5049e-04 - val_loss: 0.1032 - 54s/epoch - 644us/sample
Epoch 69/85
84077/84077 - 54s - loss: 6.4990e-04 - val_loss: 0.1095 - 54s/epoch - 646us/sample
Epoch 70/85
84077/84077 - 54s - loss: 6.5046e-04 - val_loss: 0.1147 - 54s/epoch - 645us/sample
Epoch 71/85
84077/84077 - 54s - loss: 6.5089e-04 - val_loss: 0.1238 - 54s/epoch - 647us/sample
Epoch 72/85
84077/84077 - 54s - loss: 6.5014e-04 - val_loss: 0.1290 - 54s/epoch - 645us/sample
Epoch 73/85
84077/84077 - 54s - loss: 6.4932e-04 - val_loss: 0.1028 - 54s/epoch - 647us/sample
Epoch 74/85
84077/84077 - 54s - loss: 6.4981e-04 - val_loss: 0.1109 - 54s/epoch - 643us/sample
Epoch 75/85
84077/84077 - 54s - loss: 6.4832e-04 - val_loss: 0.0971 - 54s/epoch - 645us/sample
Epoch 76/85
84077/84077 - 54s - loss: 6.4759e-04 - val_loss: 0.1066 - 54s/epoch - 645us/sample
Epoch 77/85
84077/84077 - 54s - loss: 6.4804e-04 - val_loss: 0.1028 - 54s/epoch - 645us/sample
Epoch 78/85
84077/84077 - 54s - loss: 6.4819e-04 - val_loss: 0.1061 - 54s/epoch - 645us/sample
Epoch 79/85
84077/84077 - 54s - loss: 6.4686e-04 - val_loss: 0.1016 - 54s/epoch - 646us/sample
Epoch 80/85
84077/84077 - 54s - loss: 6.4525e-04 - val_loss: 0.1063 - 54s/epoch - 644us/sample
Epoch 81/85
84077/84077 - 54s - loss: 6.4461e-04 - val_loss: 0.0956 - 54s/epoch - 643us/sample
Epoch 82/85
84077/84077 - 54s - loss: 6.4484e-04 - val_loss: 0.0811 - 54s/epoch - 645us/sample
Epoch 83/85
84077/84077 - 54s - loss: 6.4257e-04 - val_loss: 0.0824 - 54s/epoch - 646us/sample
Epoch 84/85
84077/84077 - 54s - loss: 6.4257e-04 - val_loss: 0.0972 - 54s/epoch - 645us/sample
Epoch 85/85
84077/84077 - 54s - loss: 6.4204e-04 - val_loss: 0.0877 - 54s/epoch - 645us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.08769810391669554
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 01:43:42.458490: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_54/outputlayer/BiasAdd' id:60680 op device:{requested: '', assigned: ''} def:{{{node decoder_model_54/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_54/outputlayer/MatMul, decoder_model_54/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.20107472139693394
cosine 0.19904934277496114
MAE: 0.032851138044556714
RMSE: 0.29555622337409815
r2: -67.16661176612442
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 8, 85, 0.0004000000000000001, 0.2, 188, 0.0006420441926758282, 0.08769810391669554, 0.20107472139693394, 0.19904934277496114, 0.032851138044556714, 0.29555622337409815, -67.16661176612442, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.00020000000000000006 32 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_141 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_141 (ReLU)               (None, 1791)         0           ['batch_normalization_141[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_141[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_141[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 01:44:05.812233: W tensorflow/c/c_api.cc:291] Operation '{name:'training_78/Adam/outputlayer_55/kernel/v/Assign' id:62663 op device:{requested: '', assigned: ''} def:{{{node training_78/Adam/outputlayer_55/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_78/Adam/outputlayer_55/kernel/v, training_78/Adam/outputlayer_55/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 01:44:28.408378: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_55/mul' id:61971 op device:{requested: '', assigned: ''} def:{{{node loss_55/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_55/mul/x, loss_55/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 32s - loss: 0.0031 - val_loss: 8.6315e-04 - 32s/epoch - 379us/sample
Epoch 2/90
84077/84077 - 18s - loss: 9.9947e-04 - val_loss: 7.3032e-04 - 18s/epoch - 212us/sample
Epoch 3/90
84077/84077 - 18s - loss: 6.7298e-04 - val_loss: 4.5595e-04 - 18s/epoch - 212us/sample
Epoch 4/90
84077/84077 - 18s - loss: 4.7369e-04 - val_loss: 3.8447e-04 - 18s/epoch - 212us/sample
Epoch 5/90
84077/84077 - 18s - loss: 3.9847e-04 - val_loss: 3.3560e-04 - 18s/epoch - 212us/sample
Epoch 6/90
84077/84077 - 18s - loss: 3.4823e-04 - val_loss: 3.0182e-04 - 18s/epoch - 214us/sample
Epoch 7/90
84077/84077 - 18s - loss: 3.1840e-04 - val_loss: 2.7685e-04 - 18s/epoch - 212us/sample
Epoch 8/90
84077/84077 - 18s - loss: 2.9978e-04 - val_loss: 2.5952e-04 - 18s/epoch - 212us/sample
Epoch 9/90
84077/84077 - 18s - loss: 2.8568e-04 - val_loss: 2.4844e-04 - 18s/epoch - 212us/sample
Epoch 10/90
84077/84077 - 18s - loss: 2.7544e-04 - val_loss: 2.4167e-04 - 18s/epoch - 213us/sample
Epoch 11/90
84077/84077 - 18s - loss: 2.6641e-04 - val_loss: 2.3718e-04 - 18s/epoch - 213us/sample
Epoch 12/90
84077/84077 - 18s - loss: 2.6075e-04 - val_loss: 2.2890e-04 - 18s/epoch - 211us/sample
Epoch 13/90
84077/84077 - 18s - loss: 2.5545e-04 - val_loss: 2.2385e-04 - 18s/epoch - 212us/sample
Epoch 14/90
84077/84077 - 18s - loss: 2.5028e-04 - val_loss: 2.2202e-04 - 18s/epoch - 212us/sample
Epoch 15/90
84077/84077 - 18s - loss: 2.4599e-04 - val_loss: 2.1962e-04 - 18s/epoch - 214us/sample
Epoch 16/90
84077/84077 - 18s - loss: 2.4230e-04 - val_loss: 2.1444e-04 - 18s/epoch - 212us/sample
Epoch 17/90
84077/84077 - 18s - loss: 2.3832e-04 - val_loss: 2.0938e-04 - 18s/epoch - 212us/sample
Epoch 18/90
84077/84077 - 18s - loss: 2.3454e-04 - val_loss: 2.0669e-04 - 18s/epoch - 213us/sample
Epoch 19/90
84077/84077 - 18s - loss: 2.3065e-04 - val_loss: 2.0388e-04 - 18s/epoch - 214us/sample
Epoch 20/90
84077/84077 - 18s - loss: 2.2858e-04 - val_loss: 2.0290e-04 - 18s/epoch - 212us/sample
Epoch 21/90
84077/84077 - 18s - loss: 2.2672e-04 - val_loss: 1.9968e-04 - 18s/epoch - 212us/sample
Epoch 22/90
84077/84077 - 18s - loss: 2.2408e-04 - val_loss: 1.9715e-04 - 18s/epoch - 212us/sample
Epoch 23/90
84077/84077 - 18s - loss: 2.2167e-04 - val_loss: 1.9528e-04 - 18s/epoch - 214us/sample
Epoch 24/90
84077/84077 - 18s - loss: 2.1943e-04 - val_loss: 1.9512e-04 - 18s/epoch - 212us/sample
Epoch 25/90
84077/84077 - 18s - loss: 2.1761e-04 - val_loss: 1.9149e-04 - 18s/epoch - 212us/sample
Epoch 26/90
84077/84077 - 18s - loss: 2.1534e-04 - val_loss: 1.9193e-04 - 18s/epoch - 213us/sample
Epoch 27/90
84077/84077 - 18s - loss: 2.1434e-04 - val_loss: 1.9081e-04 - 18s/epoch - 214us/sample
Epoch 28/90
84077/84077 - 18s - loss: 2.1241e-04 - val_loss: 1.8801e-04 - 18s/epoch - 212us/sample
Epoch 29/90
84077/84077 - 18s - loss: 2.1012e-04 - val_loss: 1.8456e-04 - 18s/epoch - 213us/sample
Epoch 30/90
84077/84077 - 18s - loss: 2.0876e-04 - val_loss: 1.8381e-04 - 18s/epoch - 214us/sample
Epoch 31/90
84077/84077 - 18s - loss: 2.0773e-04 - val_loss: 1.8708e-04 - 18s/epoch - 212us/sample
Epoch 32/90
84077/84077 - 18s - loss: 2.0622e-04 - val_loss: 1.8180e-04 - 18s/epoch - 212us/sample
Epoch 33/90
84077/84077 - 18s - loss: 2.0445e-04 - val_loss: 1.8089e-04 - 18s/epoch - 212us/sample
Epoch 34/90
84077/84077 - 18s - loss: 2.0259e-04 - val_loss: 1.7942e-04 - 18s/epoch - 213us/sample
Epoch 35/90
84077/84077 - 18s - loss: 2.0145e-04 - val_loss: 1.7797e-04 - 18s/epoch - 214us/sample
Epoch 36/90
84077/84077 - 18s - loss: 2.0020e-04 - val_loss: 1.7783e-04 - 18s/epoch - 213us/sample
Epoch 37/90
84077/84077 - 18s - loss: 1.9911e-04 - val_loss: 1.7585e-04 - 18s/epoch - 212us/sample
Epoch 38/90
84077/84077 - 18s - loss: 1.9807e-04 - val_loss: 1.7527e-04 - 18s/epoch - 214us/sample
Epoch 39/90
84077/84077 - 18s - loss: 1.9733e-04 - val_loss: 1.7498e-04 - 18s/epoch - 213us/sample
Epoch 40/90
84077/84077 - 18s - loss: 1.9613e-04 - val_loss: 1.7229e-04 - 18s/epoch - 212us/sample
Epoch 41/90
84077/84077 - 18s - loss: 1.9505e-04 - val_loss: 1.7258e-04 - 18s/epoch - 212us/sample
Epoch 42/90
84077/84077 - 18s - loss: 1.9329e-04 - val_loss: 1.7172e-04 - 18s/epoch - 212us/sample
Epoch 43/90
84077/84077 - 18s - loss: 1.9211e-04 - val_loss: 1.6914e-04 - 18s/epoch - 214us/sample
Epoch 44/90
84077/84077 - 18s - loss: 1.9095e-04 - val_loss: 1.6815e-04 - 18s/epoch - 212us/sample
Epoch 45/90
84077/84077 - 18s - loss: 1.8953e-04 - val_loss: 1.6948e-04 - 18s/epoch - 212us/sample
Epoch 46/90
84077/84077 - 18s - loss: 1.8864e-04 - val_loss: 1.6616e-04 - 18s/epoch - 212us/sample
Epoch 47/90
84077/84077 - 18s - loss: 1.8754e-04 - val_loss: 1.6677e-04 - 18s/epoch - 215us/sample
Epoch 48/90
84077/84077 - 18s - loss: 1.8606e-04 - val_loss: 1.6377e-04 - 18s/epoch - 212us/sample
Epoch 49/90
84077/84077 - 18s - loss: 1.8534e-04 - val_loss: 1.6303e-04 - 18s/epoch - 212us/sample
Epoch 50/90
84077/84077 - 18s - loss: 1.8453e-04 - val_loss: 1.6304e-04 - 18s/epoch - 213us/sample
Epoch 51/90
84077/84077 - 18s - loss: 1.8328e-04 - val_loss: 1.6237e-04 - 18s/epoch - 213us/sample
Epoch 52/90
84077/84077 - 18s - loss: 1.8250e-04 - val_loss: 1.6253e-04 - 18s/epoch - 214us/sample
Epoch 53/90
84077/84077 - 18s - loss: 1.8219e-04 - val_loss: 1.6166e-04 - 18s/epoch - 212us/sample
Epoch 54/90
84077/84077 - 18s - loss: 1.8094e-04 - val_loss: 1.5982e-04 - 18s/epoch - 212us/sample
Epoch 55/90
84077/84077 - 18s - loss: 1.7989e-04 - val_loss: 1.5921e-04 - 18s/epoch - 213us/sample
Epoch 56/90
84077/84077 - 18s - loss: 1.7940e-04 - val_loss: 1.5868e-04 - 18s/epoch - 214us/sample
Epoch 57/90
84077/84077 - 18s - loss: 1.7860e-04 - val_loss: 1.5943e-04 - 18s/epoch - 212us/sample
Epoch 58/90
84077/84077 - 18s - loss: 1.7813e-04 - val_loss: 1.5826e-04 - 18s/epoch - 212us/sample
Epoch 59/90
84077/84077 - 18s - loss: 1.7800e-04 - val_loss: 1.5809e-04 - 18s/epoch - 213us/sample
Epoch 60/90
84077/84077 - 18s - loss: 1.7698e-04 - val_loss: 1.5765e-04 - 18s/epoch - 214us/sample
Epoch 61/90
84077/84077 - 18s - loss: 1.7716e-04 - val_loss: 1.5674e-04 - 18s/epoch - 212us/sample
Epoch 62/90
84077/84077 - 18s - loss: 1.7619e-04 - val_loss: 1.5949e-04 - 18s/epoch - 212us/sample
Epoch 63/90
84077/84077 - 18s - loss: 1.7565e-04 - val_loss: 1.5660e-04 - 18s/epoch - 212us/sample
Epoch 64/90
84077/84077 - 18s - loss: 1.7518e-04 - val_loss: 1.5585e-04 - 18s/epoch - 215us/sample
Epoch 65/90
84077/84077 - 18s - loss: 1.7457e-04 - val_loss: 1.5584e-04 - 18s/epoch - 212us/sample
Epoch 66/90
84077/84077 - 18s - loss: 1.7447e-04 - val_loss: 1.5396e-04 - 18s/epoch - 212us/sample
Epoch 67/90
84077/84077 - 18s - loss: 1.7374e-04 - val_loss: 1.5565e-04 - 18s/epoch - 212us/sample
Epoch 68/90
84077/84077 - 18s - loss: 1.7319e-04 - val_loss: 1.5379e-04 - 18s/epoch - 214us/sample
Epoch 69/90
84077/84077 - 18s - loss: 1.7356e-04 - val_loss: 1.5336e-04 - 18s/epoch - 212us/sample
Epoch 70/90
84077/84077 - 18s - loss: 1.7244e-04 - val_loss: 1.5516e-04 - 18s/epoch - 212us/sample
Epoch 71/90
84077/84077 - 18s - loss: 1.7220e-04 - val_loss: 1.5354e-04 - 18s/epoch - 212us/sample
Epoch 72/90
84077/84077 - 18s - loss: 1.7175e-04 - val_loss: 1.5144e-04 - 18s/epoch - 214us/sample
Epoch 73/90
84077/84077 - 18s - loss: 1.7081e-04 - val_loss: 1.5275e-04 - 18s/epoch - 212us/sample
Epoch 74/90
84077/84077 - 18s - loss: 1.7184e-04 - val_loss: 1.5268e-04 - 18s/epoch - 212us/sample
Epoch 75/90
84077/84077 - 18s - loss: 1.7058e-04 - val_loss: 1.5214e-04 - 18s/epoch - 214us/sample
Epoch 76/90
84077/84077 - 18s - loss: 1.7016e-04 - val_loss: 1.5237e-04 - 18s/epoch - 213us/sample
Epoch 77/90
84077/84077 - 18s - loss: 1.6999e-04 - val_loss: 1.5532e-04 - 18s/epoch - 212us/sample
Epoch 78/90
84077/84077 - 18s - loss: 1.6994e-04 - val_loss: 1.4995e-04 - 18s/epoch - 212us/sample
Epoch 79/90
84077/84077 - 18s - loss: 1.6909e-04 - val_loss: 1.5208e-04 - 18s/epoch - 212us/sample
Epoch 80/90
84077/84077 - 18s - loss: 1.6905e-04 - val_loss: 1.4916e-04 - 18s/epoch - 214us/sample
Epoch 81/90
84077/84077 - 18s - loss: 1.6842e-04 - val_loss: 1.4970e-04 - 18s/epoch - 212us/sample
Epoch 82/90
84077/84077 - 18s - loss: 1.6834e-04 - val_loss: 1.4962e-04 - 18s/epoch - 212us/sample
Epoch 83/90
84077/84077 - 18s - loss: 1.6763e-04 - val_loss: 1.4958e-04 - 18s/epoch - 213us/sample
Epoch 84/90
84077/84077 - 18s - loss: 1.6697e-04 - val_loss: 1.4886e-04 - 18s/epoch - 214us/sample
Epoch 85/90
84077/84077 - 18s - loss: 1.6686e-04 - val_loss: 1.4961e-04 - 18s/epoch - 212us/sample
Epoch 86/90
84077/84077 - 18s - loss: 1.6646e-04 - val_loss: 1.4953e-04 - 18s/epoch - 212us/sample
Epoch 87/90
84077/84077 - 18s - loss: 1.6649e-04 - val_loss: 1.4847e-04 - 18s/epoch - 212us/sample
Epoch 88/90
84077/84077 - 18s - loss: 1.6559e-04 - val_loss: 1.4814e-04 - 18s/epoch - 215us/sample
Epoch 89/90
84077/84077 - 18s - loss: 1.6518e-04 - val_loss: 1.4886e-04 - 18s/epoch - 212us/sample
Epoch 90/90
84077/84077 - 18s - loss: 1.6510e-04 - val_loss: 1.4870e-04 - 18s/epoch - 212us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001486977990267334
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 02:11:04.804200: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_55/outputlayer/BiasAdd' id:61935 op device:{requested: '', assigned: ''} def:{{{node decoder_model_55/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_55/outputlayer/MatMul, decoder_model_55/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.041681609634563695
cosine 0.04117830443429038
MAE: 0.0024881943654486652
RMSE: 0.012054702574981023
r2: 0.8865493952359365
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 32, 90, 0.00020000000000000006, 0.2, 188, 0.00016510130065006123, 0.0001486977990267334, 0.041681609634563695, 0.04117830443429038, 0.0024881943654486652, 0.012054702574981023, 0.8865493952359365, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 80 0.001 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_144 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_144 (ReLU)               (None, 1697)         0           ['batch_normalization_144[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_144[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_144[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-13 02:11:28.695771: W tensorflow/c/c_api.cc:291] Operation '{name:'training_80/Adam/outputlayer_56/kernel/m/Assign' id:63804 op device:{requested: '', assigned: ''} def:{{{node training_80/Adam/outputlayer_56/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_80/Adam/outputlayer_56/kernel/m, training_80/Adam/outputlayer_56/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 02:11:45.341896: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_56/mul' id:63249 op device:{requested: '', assigned: ''} def:{{{node loss_56/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_56/mul/x, loss_56/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 26s - loss: 0.0990 - val_loss: 0.0016 - 26s/epoch - 305us/sample
Epoch 2/80
84077/84077 - 12s - loss: 0.0016 - val_loss: 0.0013 - 12s/epoch - 139us/sample
Epoch 3/80
84077/84077 - 12s - loss: 0.0027 - val_loss: 0.0013 - 12s/epoch - 139us/sample
Epoch 4/80
84077/84077 - 12s - loss: 0.0016 - val_loss: 0.0015 - 12s/epoch - 139us/sample
Epoch 5/80
84077/84077 - 12s - loss: 0.0010 - val_loss: 8.2420e-04 - 12s/epoch - 141us/sample
Epoch 6/80
84077/84077 - 12s - loss: 8.3233e-04 - val_loss: 0.0010 - 12s/epoch - 139us/sample
Epoch 7/80
84077/84077 - 12s - loss: 7.1826e-04 - val_loss: 6.0081e-04 - 12s/epoch - 139us/sample
Epoch 8/80
84077/84077 - 12s - loss: 6.6725e-04 - val_loss: 5.5601e-04 - 12s/epoch - 139us/sample
Epoch 9/80
84077/84077 - 12s - loss: 5.8425e-04 - val_loss: 4.9185e-04 - 12s/epoch - 139us/sample
Epoch 10/80
84077/84077 - 12s - loss: 6.1157e-04 - val_loss: 4.4323e-04 - 12s/epoch - 139us/sample
Epoch 11/80
84077/84077 - 12s - loss: 4.4639e-04 - val_loss: 3.9016e-04 - 12s/epoch - 141us/sample
Epoch 12/80
84077/84077 - 12s - loss: 4.0055e-04 - val_loss: 3.4876e-04 - 12s/epoch - 140us/sample
Epoch 13/80
84077/84077 - 12s - loss: 3.7475e-04 - val_loss: 3.2888e-04 - 12s/epoch - 139us/sample
Epoch 14/80
84077/84077 - 12s - loss: 3.4976e-04 - val_loss: 3.1357e-04 - 12s/epoch - 139us/sample
Epoch 15/80
84077/84077 - 12s - loss: 3.3101e-04 - val_loss: 2.9305e-04 - 12s/epoch - 139us/sample
Epoch 16/80
84077/84077 - 12s - loss: 3.1798e-04 - val_loss: 2.7959e-04 - 12s/epoch - 140us/sample
Epoch 17/80
84077/84077 - 12s - loss: 3.0687e-04 - val_loss: 2.7029e-04 - 12s/epoch - 141us/sample
Epoch 18/80
84077/84077 - 12s - loss: 2.9139e-04 - val_loss: 2.6558e-04 - 12s/epoch - 139us/sample
Epoch 19/80
84077/84077 - 12s - loss: 2.8048e-04 - val_loss: 2.5029e-04 - 12s/epoch - 139us/sample
Epoch 20/80
84077/84077 - 12s - loss: 2.7209e-04 - val_loss: 2.4023e-04 - 12s/epoch - 139us/sample
Epoch 21/80
84077/84077 - 12s - loss: 2.6452e-04 - val_loss: 2.4217e-04 - 12s/epoch - 139us/sample
Epoch 22/80
84077/84077 - 12s - loss: 2.5732e-04 - val_loss: 2.3430e-04 - 12s/epoch - 141us/sample
Epoch 23/80
84077/84077 - 12s - loss: 2.5068e-04 - val_loss: 2.2846e-04 - 12s/epoch - 140us/sample
Epoch 24/80
84077/84077 - 12s - loss: 2.4697e-04 - val_loss: 2.2427e-04 - 12s/epoch - 139us/sample
Epoch 25/80
84077/84077 - 12s - loss: 2.4311e-04 - val_loss: 2.2262e-04 - 12s/epoch - 139us/sample
Epoch 26/80
84077/84077 - 12s - loss: 2.3816e-04 - val_loss: 2.1870e-04 - 12s/epoch - 139us/sample
Epoch 27/80
84077/84077 - 12s - loss: 2.3482e-04 - val_loss: 2.1576e-04 - 12s/epoch - 139us/sample
Epoch 28/80
84077/84077 - 12s - loss: 2.3237e-04 - val_loss: 2.1216e-04 - 12s/epoch - 141us/sample
Epoch 29/80
84077/84077 - 12s - loss: 2.2930e-04 - val_loss: 2.1168e-04 - 12s/epoch - 139us/sample
Epoch 30/80
84077/84077 - 12s - loss: 2.3003e-04 - val_loss: 2.1085e-04 - 12s/epoch - 139us/sample
Epoch 31/80
84077/84077 - 12s - loss: 2.2440e-04 - val_loss: 2.0941e-04 - 12s/epoch - 139us/sample
Epoch 32/80
84077/84077 - 12s - loss: 2.2254e-04 - val_loss: 2.0613e-04 - 12s/epoch - 139us/sample
Epoch 33/80
84077/84077 - 12s - loss: 2.2071e-04 - val_loss: 2.0700e-04 - 12s/epoch - 140us/sample
Epoch 34/80
84077/84077 - 12s - loss: 2.1951e-04 - val_loss: 2.0403e-04 - 12s/epoch - 141us/sample
Epoch 35/80
84077/84077 - 12s - loss: 2.1831e-04 - val_loss: 2.1308e-04 - 12s/epoch - 139us/sample
Epoch 36/80
84077/84077 - 12s - loss: 2.1664e-04 - val_loss: 2.0200e-04 - 12s/epoch - 139us/sample
Epoch 37/80
84077/84077 - 12s - loss: 2.1562e-04 - val_loss: 2.0026e-04 - 12s/epoch - 139us/sample
Epoch 38/80
84077/84077 - 12s - loss: 2.1298e-04 - val_loss: 2.0027e-04 - 12s/epoch - 139us/sample
Epoch 39/80
84077/84077 - 12s - loss: 2.1262e-04 - val_loss: 1.9816e-04 - 12s/epoch - 141us/sample
Epoch 40/80
84077/84077 - 12s - loss: 2.1067e-04 - val_loss: 1.9684e-04 - 12s/epoch - 140us/sample
Epoch 41/80
84077/84077 - 12s - loss: 2.1033e-04 - val_loss: 2.0007e-04 - 12s/epoch - 139us/sample
Epoch 42/80
84077/84077 - 12s - loss: 2.0883e-04 - val_loss: 1.9592e-04 - 12s/epoch - 139us/sample
Epoch 43/80
84077/84077 - 12s - loss: 2.0754e-04 - val_loss: 1.9558e-04 - 12s/epoch - 139us/sample
Epoch 44/80
84077/84077 - 12s - loss: 2.0658e-04 - val_loss: 1.9505e-04 - 12s/epoch - 139us/sample
Epoch 45/80
84077/84077 - 12s - loss: 2.0543e-04 - val_loss: 1.9438e-04 - 12s/epoch - 141us/sample
Epoch 46/80
84077/84077 - 12s - loss: 2.0515e-04 - val_loss: 1.9259e-04 - 12s/epoch - 139us/sample
Epoch 47/80
84077/84077 - 12s - loss: 2.0393e-04 - val_loss: 1.9264e-04 - 12s/epoch - 139us/sample
Epoch 48/80
84077/84077 - 12s - loss: 2.0315e-04 - val_loss: 1.9289e-04 - 12s/epoch - 139us/sample
Epoch 49/80
84077/84077 - 12s - loss: 2.0254e-04 - val_loss: 1.9329e-04 - 12s/epoch - 139us/sample
Epoch 50/80
84077/84077 - 12s - loss: 2.0197e-04 - val_loss: 1.9163e-04 - 12s/epoch - 140us/sample
Epoch 51/80
84077/84077 - 12s - loss: 2.0076e-04 - val_loss: 1.9054e-04 - 12s/epoch - 141us/sample
Epoch 52/80
84077/84077 - 12s - loss: 1.9977e-04 - val_loss: 1.9033e-04 - 12s/epoch - 139us/sample
Epoch 53/80
84077/84077 - 12s - loss: 1.9962e-04 - val_loss: 1.8908e-04 - 12s/epoch - 139us/sample
Epoch 54/80
84077/84077 - 12s - loss: 1.9857e-04 - val_loss: 1.8860e-04 - 12s/epoch - 139us/sample
Epoch 55/80
84077/84077 - 12s - loss: 1.9885e-04 - val_loss: 1.8764e-04 - 12s/epoch - 139us/sample
Epoch 56/80
84077/84077 - 12s - loss: 1.9694e-04 - val_loss: 1.8810e-04 - 12s/epoch - 141us/sample
Epoch 57/80
84077/84077 - 12s - loss: 1.9706e-04 - val_loss: 1.8474e-04 - 12s/epoch - 140us/sample
Epoch 58/80
84077/84077 - 12s - loss: 1.9640e-04 - val_loss: 1.8830e-04 - 12s/epoch - 139us/sample
Epoch 59/80
84077/84077 - 12s - loss: 1.9678e-04 - val_loss: 1.8845e-04 - 12s/epoch - 139us/sample
Epoch 60/80
84077/84077 - 12s - loss: 1.9571e-04 - val_loss: 1.8545e-04 - 12s/epoch - 139us/sample
Epoch 61/80
84077/84077 - 12s - loss: 1.9510e-04 - val_loss: 1.8429e-04 - 12s/epoch - 139us/sample
Epoch 62/80
84077/84077 - 12s - loss: 1.9431e-04 - val_loss: 1.8764e-04 - 12s/epoch - 141us/sample
Epoch 63/80
84077/84077 - 12s - loss: 1.9429e-04 - val_loss: 1.8389e-04 - 12s/epoch - 140us/sample
Epoch 64/80
84077/84077 - 12s - loss: 1.9377e-04 - val_loss: 1.8543e-04 - 12s/epoch - 139us/sample
Epoch 65/80
84077/84077 - 12s - loss: 1.9366e-04 - val_loss: 1.8459e-04 - 12s/epoch - 139us/sample
Epoch 66/80
84077/84077 - 12s - loss: 1.9230e-04 - val_loss: 1.8465e-04 - 12s/epoch - 139us/sample
Epoch 67/80
84077/84077 - 12s - loss: 1.9192e-04 - val_loss: 1.8155e-04 - 12s/epoch - 139us/sample
Epoch 68/80
84077/84077 - 12s - loss: 1.9116e-04 - val_loss: 1.8174e-04 - 12s/epoch - 141us/sample
Epoch 69/80
84077/84077 - 12s - loss: 1.9182e-04 - val_loss: 1.8334e-04 - 12s/epoch - 139us/sample
Epoch 70/80
84077/84077 - 12s - loss: 1.9119e-04 - val_loss: 1.7871e-04 - 12s/epoch - 139us/sample
Epoch 71/80
84077/84077 - 12s - loss: 1.9029e-04 - val_loss: 1.8582e-04 - 12s/epoch - 139us/sample
Epoch 72/80
84077/84077 - 12s - loss: 1.9019e-04 - val_loss: 1.8149e-04 - 12s/epoch - 139us/sample
Epoch 73/80
84077/84077 - 12s - loss: 1.8994e-04 - val_loss: 1.7938e-04 - 12s/epoch - 140us/sample
Epoch 74/80
84077/84077 - 12s - loss: 1.8975e-04 - val_loss: 1.7976e-04 - 12s/epoch - 141us/sample
Epoch 75/80
84077/84077 - 12s - loss: 1.8859e-04 - val_loss: 1.7938e-04 - 12s/epoch - 139us/sample
Epoch 76/80
84077/84077 - 12s - loss: 1.8881e-04 - val_loss: 1.7899e-04 - 12s/epoch - 139us/sample
Epoch 77/80
84077/84077 - 12s - loss: 1.8789e-04 - val_loss: 1.8227e-04 - 12s/epoch - 139us/sample
Epoch 78/80
84077/84077 - 12s - loss: 1.8844e-04 - val_loss: 1.7832e-04 - 12s/epoch - 139us/sample
Epoch 79/80
84077/84077 - 12s - loss: 1.8802e-04 - val_loss: 1.7793e-04 - 12s/epoch - 141us/sample
Epoch 80/80
84077/84077 - 12s - loss: 1.8684e-04 - val_loss: 1.7702e-04 - 12s/epoch - 139us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017701832337976476
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 02:27:16.884363: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_56/outputlayer/BiasAdd' id:63220 op device:{requested: '', assigned: ''} def:{{{node decoder_model_56/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_56/outputlayer/MatMul, decoder_model_56/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.021147172591890688
cosine 0.020893858704731885
MAE: 0.002002281665927249
RMSE: 0.00847822247607947
r2: 0.9440942797117348
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 64, 80, 0.001, 0.2, 188, 0.0001868447409977251, 0.00017701832337976476, 0.021147172591890688, 0.020893858704731885, 0.002002281665927249, 0.00847822247607947, 0.9440942797117348, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 32 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_147 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_147 (ReLU)               (None, 1886)         0           ['batch_normalization_147[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_147[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_147[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 02:27:41.145346: W tensorflow/c/c_api.cc:291] Operation '{name:'training_82/Adam/batch_normalization_147/beta/m/Assign' id:65060 op device:{requested: '', assigned: ''} def:{{{node training_82/Adam/batch_normalization_147/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_82/Adam/batch_normalization_147/beta/m, training_82/Adam/batch_normalization_147/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 02:28:04.668849: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_57/mul' id:64523 op device:{requested: '', assigned: ''} def:{{{node loss_57/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_57/mul/x, loss_57/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 33s - loss: 0.0680 - val_loss: 0.0674 - 33s/epoch - 394us/sample
Epoch 2/90
84077/84077 - 19s - loss: 0.0669 - val_loss: 0.0674 - 19s/epoch - 221us/sample
Epoch 3/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0674 - 19s/epoch - 221us/sample
Epoch 4/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 5/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 6/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 7/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 8/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 9/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 10/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 11/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 12/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 13/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 14/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 15/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 16/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 17/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 18/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 19/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 20/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 21/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 22/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 23/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 24/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 25/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 26/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 27/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 28/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 29/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 30/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 31/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 32/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 33/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 34/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 35/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 36/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 37/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 38/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 39/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 40/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 41/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 220us/sample
Epoch 42/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 43/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 44/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 45/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 46/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 47/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 48/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 49/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 50/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 51/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 52/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 53/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 54/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 55/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 56/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 57/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 58/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 59/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 60/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 61/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 62/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 63/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 64/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 65/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 66/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 220us/sample
Epoch 67/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 68/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 69/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 70/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 71/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 72/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 73/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 224us/sample
Epoch 74/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 75/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 76/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 77/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 224us/sample
Epoch 78/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 79/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 80/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 81/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 82/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 83/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 84/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 85/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 86/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 87/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 88/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 221us/sample
Epoch 89/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 222us/sample
Epoch 90/90
84077/84077 - 19s - loss: 0.0668 - val_loss: 0.0673 - 19s/epoch - 223us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734574020859568
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 02:55:48.270218: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_57/outputlayer/BiasAdd' id:64475 op device:{requested: '', assigned: ''} def:{{{node decoder_model_57/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_57/outputlayer/MatMul, decoder_model_57/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.086659154991768
cosine 1.12673507289983
MAE: 5.346247893098944
RMSE: 5.4907234885545595
r2: -22115.742100458832
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 32, 90, 0.001, 0.2, 188, 0.06683691590271161, 0.06734574020859568, 1.086659154991768, 1.12673507289983, 5.346247893098944, 5.4907234885545595, -22115.742100458832, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.00020000000000000006 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_150 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_150 (ReLU)               (None, 1791)         0           ['batch_normalization_150[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_150[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_150[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 02:56:12.965087: W tensorflow/c/c_api.cc:291] Operation '{name:'training_84/Adam/batch_normalization_150/beta/m/Assign' id:66345 op device:{requested: '', assigned: ''} def:{{{node training_84/Adam/batch_normalization_150/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_84/Adam/batch_normalization_150/beta/m, training_84/Adam/batch_normalization_150/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 02:56:30.326543: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_58/mul' id:65848 op device:{requested: '', assigned: ''} def:{{{node loss_58/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_58/mul/x, loss_58/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0069 - val_loss: 8.7817e-04 - 27s/epoch - 319us/sample
Epoch 2/90
84077/84077 - 12s - loss: 8.7983e-04 - val_loss: 0.1402 - 12s/epoch - 141us/sample
Epoch 3/90
84077/84077 - 12s - loss: 0.0010 - val_loss: 7.9238e-04 - 12s/epoch - 141us/sample
Epoch 4/90
84077/84077 - 12s - loss: 7.3724e-04 - val_loss: 6.2921e-04 - 12s/epoch - 141us/sample
Epoch 5/90
84077/84077 - 12s - loss: 9.8601e-04 - val_loss: 7.1448e-04 - 12s/epoch - 141us/sample
Epoch 6/90
84077/84077 - 12s - loss: 5.4124e-04 - val_loss: 4.4060e-04 - 12s/epoch - 141us/sample
Epoch 7/90
84077/84077 - 12s - loss: 4.4611e-04 - val_loss: 3.7843e-04 - 12s/epoch - 142us/sample
Epoch 8/90
84077/84077 - 12s - loss: 3.7673e-04 - val_loss: 3.3748e-04 - 12s/epoch - 143us/sample
Epoch 9/90
84077/84077 - 12s - loss: 3.4602e-04 - val_loss: 3.1144e-04 - 12s/epoch - 141us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.2101e-04 - val_loss: 2.8258e-04 - 12s/epoch - 141us/sample
Epoch 11/90
84077/84077 - 12s - loss: 2.9322e-04 - val_loss: 2.6101e-04 - 12s/epoch - 141us/sample
Epoch 12/90
84077/84077 - 12s - loss: 2.7102e-04 - val_loss: 2.4258e-04 - 12s/epoch - 141us/sample
Epoch 13/90
84077/84077 - 12s - loss: 2.5241e-04 - val_loss: 2.2317e-04 - 12s/epoch - 143us/sample
Epoch 14/90
84077/84077 - 12s - loss: 2.3905e-04 - val_loss: 2.1314e-04 - 12s/epoch - 142us/sample
Epoch 15/90
84077/84077 - 12s - loss: 2.2769e-04 - val_loss: 2.0020e-04 - 12s/epoch - 141us/sample
Epoch 16/90
84077/84077 - 12s - loss: 2.1617e-04 - val_loss: 1.9170e-04 - 12s/epoch - 141us/sample
Epoch 17/90
84077/84077 - 12s - loss: 2.0729e-04 - val_loss: 1.8301e-04 - 12s/epoch - 141us/sample
Epoch 18/90
84077/84077 - 12s - loss: 1.9880e-04 - val_loss: 1.7782e-04 - 12s/epoch - 142us/sample
Epoch 19/90
84077/84077 - 12s - loss: 1.9229e-04 - val_loss: 1.7157e-04 - 12s/epoch - 143us/sample
Epoch 20/90
84077/84077 - 12s - loss: 1.8666e-04 - val_loss: 1.6664e-04 - 12s/epoch - 141us/sample
Epoch 21/90
84077/84077 - 12s - loss: 1.8238e-04 - val_loss: 1.6398e-04 - 12s/epoch - 141us/sample
Epoch 22/90
84077/84077 - 12s - loss: 1.7905e-04 - val_loss: 1.6074e-04 - 12s/epoch - 141us/sample
Epoch 23/90
84077/84077 - 12s - loss: 1.7586e-04 - val_loss: 1.5786e-04 - 12s/epoch - 141us/sample
Epoch 24/90
84077/84077 - 12s - loss: 1.7248e-04 - val_loss: 1.5706e-04 - 12s/epoch - 142us/sample
Epoch 25/90
84077/84077 - 12s - loss: 1.7003e-04 - val_loss: 1.5426e-04 - 12s/epoch - 143us/sample
Epoch 26/90
84077/84077 - 12s - loss: 1.6834e-04 - val_loss: 1.5262e-04 - 12s/epoch - 141us/sample
Epoch 27/90
84077/84077 - 12s - loss: 1.6612e-04 - val_loss: 1.5340e-04 - 12s/epoch - 141us/sample
Epoch 28/90
84077/84077 - 12s - loss: 1.6400e-04 - val_loss: 1.4981e-04 - 12s/epoch - 141us/sample
Epoch 29/90
84077/84077 - 12s - loss: 1.6245e-04 - val_loss: 1.4910e-04 - 12s/epoch - 141us/sample
Epoch 30/90
84077/84077 - 12s - loss: 1.6080e-04 - val_loss: 1.4741e-04 - 12s/epoch - 143us/sample
Epoch 31/90
84077/84077 - 12s - loss: 1.5954e-04 - val_loss: 1.4690e-04 - 12s/epoch - 142us/sample
Epoch 32/90
84077/84077 - 12s - loss: 1.5790e-04 - val_loss: 1.4564e-04 - 12s/epoch - 141us/sample
Epoch 33/90
84077/84077 - 12s - loss: 1.5635e-04 - val_loss: 1.4316e-04 - 12s/epoch - 148us/sample
Epoch 34/90
84077/84077 - 12s - loss: 1.5535e-04 - val_loss: 1.4312e-04 - 12s/epoch - 141us/sample
Epoch 35/90
84077/84077 - 12s - loss: 1.5465e-04 - val_loss: 1.4288e-04 - 12s/epoch - 141us/sample
Epoch 36/90
84077/84077 - 12s - loss: 1.5332e-04 - val_loss: 1.4252e-04 - 12s/epoch - 143us/sample
Epoch 37/90
84077/84077 - 12s - loss: 1.5232e-04 - val_loss: 1.4156e-04 - 12s/epoch - 141us/sample
Epoch 38/90
84077/84077 - 12s - loss: 1.5181e-04 - val_loss: 1.3968e-04 - 12s/epoch - 141us/sample
Epoch 39/90
84077/84077 - 12s - loss: 1.5077e-04 - val_loss: 1.3960e-04 - 12s/epoch - 141us/sample
Epoch 40/90
84077/84077 - 12s - loss: 1.5007e-04 - val_loss: 1.3860e-04 - 12s/epoch - 141us/sample
Epoch 41/90
84077/84077 - 12s - loss: 1.4913e-04 - val_loss: 1.3787e-04 - 12s/epoch - 142us/sample
Epoch 42/90
84077/84077 - 12s - loss: 1.4838e-04 - val_loss: 1.3957e-04 - 12s/epoch - 143us/sample
Epoch 43/90
84077/84077 - 12s - loss: 1.4800e-04 - val_loss: 1.3793e-04 - 12s/epoch - 142us/sample
Epoch 44/90
84077/84077 - 12s - loss: 1.4748e-04 - val_loss: 1.3738e-04 - 12s/epoch - 141us/sample
Epoch 45/90
84077/84077 - 12s - loss: 1.4660e-04 - val_loss: 1.3838e-04 - 12s/epoch - 141us/sample
Epoch 46/90
84077/84077 - 12s - loss: 1.4608e-04 - val_loss: 1.3702e-04 - 12s/epoch - 141us/sample
Epoch 47/90
84077/84077 - 12s - loss: 1.4571e-04 - val_loss: 1.3570e-04 - 12s/epoch - 143us/sample
Epoch 48/90
84077/84077 - 12s - loss: 1.4475e-04 - val_loss: 1.3624e-04 - 12s/epoch - 142us/sample
Epoch 49/90
84077/84077 - 12s - loss: 1.4438e-04 - val_loss: 1.3558e-04 - 12s/epoch - 141us/sample
Epoch 50/90
84077/84077 - 12s - loss: 1.4441e-04 - val_loss: 1.3497e-04 - 12s/epoch - 141us/sample
Epoch 51/90
84077/84077 - 12s - loss: 1.4356e-04 - val_loss: 1.3545e-04 - 12s/epoch - 141us/sample
Epoch 52/90
84077/84077 - 12s - loss: 1.4322e-04 - val_loss: 1.3358e-04 - 12s/epoch - 142us/sample
Epoch 53/90
84077/84077 - 12s - loss: 1.4278e-04 - val_loss: 1.3396e-04 - 12s/epoch - 143us/sample
Epoch 54/90
84077/84077 - 12s - loss: 1.4284e-04 - val_loss: 1.3408e-04 - 12s/epoch - 142us/sample
Epoch 55/90
84077/84077 - 12s - loss: 1.4203e-04 - val_loss: 1.3485e-04 - 12s/epoch - 141us/sample
Epoch 56/90
84077/84077 - 12s - loss: 1.4151e-04 - val_loss: 1.3265e-04 - 12s/epoch - 141us/sample
Epoch 57/90
84077/84077 - 12s - loss: 1.4135e-04 - val_loss: 1.3279e-04 - 12s/epoch - 141us/sample
Epoch 58/90
84077/84077 - 12s - loss: 1.4095e-04 - val_loss: 1.3242e-04 - 12s/epoch - 142us/sample
Epoch 59/90
84077/84077 - 12s - loss: 1.4048e-04 - val_loss: 1.3237e-04 - 12s/epoch - 143us/sample
Epoch 60/90
84077/84077 - 12s - loss: 1.4016e-04 - val_loss: 1.3214e-04 - 12s/epoch - 141us/sample
Epoch 61/90
84077/84077 - 12s - loss: 1.3992e-04 - val_loss: 1.3160e-04 - 12s/epoch - 141us/sample
Epoch 62/90
84077/84077 - 12s - loss: 1.3971e-04 - val_loss: 1.3128e-04 - 12s/epoch - 141us/sample
Epoch 63/90
84077/84077 - 12s - loss: 1.3929e-04 - val_loss: 1.3101e-04 - 12s/epoch - 141us/sample
Epoch 64/90
84077/84077 - 12s - loss: 1.3892e-04 - val_loss: 1.3087e-04 - 12s/epoch - 143us/sample
Epoch 65/90
84077/84077 - 12s - loss: 1.3884e-04 - val_loss: 1.2989e-04 - 12s/epoch - 142us/sample
Epoch 66/90
84077/84077 - 12s - loss: 1.3838e-04 - val_loss: 1.3265e-04 - 12s/epoch - 141us/sample
Epoch 67/90
84077/84077 - 12s - loss: 1.3839e-04 - val_loss: 1.3138e-04 - 12s/epoch - 141us/sample
Epoch 68/90
84077/84077 - 12s - loss: 1.3795e-04 - val_loss: 1.3076e-04 - 12s/epoch - 142us/sample
Epoch 69/90
84077/84077 - 12s - loss: 1.3775e-04 - val_loss: 1.3133e-04 - 12s/epoch - 142us/sample
Epoch 70/90
84077/84077 - 12s - loss: 1.3779e-04 - val_loss: 1.2976e-04 - 12s/epoch - 143us/sample
Epoch 71/90
84077/84077 - 12s - loss: 1.3728e-04 - val_loss: 1.3072e-04 - 12s/epoch - 142us/sample
Epoch 72/90
84077/84077 - 12s - loss: 1.3704e-04 - val_loss: 1.2987e-04 - 12s/epoch - 141us/sample
Epoch 73/90
84077/84077 - 12s - loss: 1.3678e-04 - val_loss: 1.2847e-04 - 12s/epoch - 141us/sample
Epoch 74/90
84077/84077 - 12s - loss: 1.3653e-04 - val_loss: 1.2924e-04 - 12s/epoch - 141us/sample
Epoch 75/90
84077/84077 - 12s - loss: 1.3661e-04 - val_loss: 1.2991e-04 - 12s/epoch - 142us/sample
Epoch 76/90
84077/84077 - 12s - loss: 1.3618e-04 - val_loss: 1.3105e-04 - 12s/epoch - 144us/sample
Epoch 77/90
84077/84077 - 12s - loss: 1.3588e-04 - val_loss: 1.2797e-04 - 12s/epoch - 142us/sample
Epoch 78/90
84077/84077 - 12s - loss: 1.3597e-04 - val_loss: 1.2953e-04 - 12s/epoch - 141us/sample
Epoch 79/90
84077/84077 - 12s - loss: 1.3555e-04 - val_loss: 1.2830e-04 - 12s/epoch - 141us/sample
Epoch 80/90
84077/84077 - 12s - loss: 1.3549e-04 - val_loss: 1.2766e-04 - 12s/epoch - 141us/sample
Epoch 81/90
84077/84077 - 12s - loss: 1.3527e-04 - val_loss: 1.2828e-04 - 12s/epoch - 142us/sample
Epoch 82/90
84077/84077 - 12s - loss: 1.3547e-04 - val_loss: 1.2864e-04 - 12s/epoch - 142us/sample
Epoch 83/90
84077/84077 - 12s - loss: 1.3506e-04 - val_loss: 1.2960e-04 - 12s/epoch - 141us/sample
Epoch 84/90
84077/84077 - 12s - loss: 1.3480e-04 - val_loss: 1.2797e-04 - 12s/epoch - 141us/sample
Epoch 85/90
84077/84077 - 12s - loss: 1.3473e-04 - val_loss: 1.2800e-04 - 12s/epoch - 141us/sample
Epoch 86/90
84077/84077 - 12s - loss: 1.3440e-04 - val_loss: 1.2832e-04 - 12s/epoch - 142us/sample
Epoch 87/90
84077/84077 - 12s - loss: 1.3438e-04 - val_loss: 1.2749e-04 - 12s/epoch - 143us/sample
Epoch 88/90
84077/84077 - 12s - loss: 1.3429e-04 - val_loss: 1.2777e-04 - 12s/epoch - 142us/sample
Epoch 89/90
84077/84077 - 12s - loss: 1.3409e-04 - val_loss: 1.2829e-04 - 12s/epoch - 141us/sample
Epoch 90/90
84077/84077 - 12s - loss: 1.3381e-04 - val_loss: 1.2752e-04 - 12s/epoch - 141us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012751935466778452
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 03:14:16.307798: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_58/outputlayer/BiasAdd' id:65812 op device:{requested: '', assigned: ''} def:{{{node decoder_model_58/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_58/outputlayer/MatMul, decoder_model_58/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03133381409685784
cosine 0.0309530082891928
MAE: 0.0022314932053893975
RMSE: 0.010308178719867792
r2: 0.9171744624418122
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 90, 0.00020000000000000006, 0.2, 188, 0.00013381457543463625, 0.00012751935466778452, 0.03133381409685784, 0.0309530082891928, 0.0022314932053893975, 0.010308178719867792, 0.9171744624418122, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 5
Fitness    = 511.49230331698664
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 511.49230331698664.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664]
[1.9 90 0.001 8 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_153 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_153 (ReLU)               (None, 1791)         0           ['batch_normalization_153[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_153[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_153[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE1.9_cr0.2_bs8_ep90_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-13 03:14:41.875664: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_59/kernel/Assign' id:66917 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_59/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_59/kernel, dense_dec0_59/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-13 03:14:52.203087: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_60/kernel/v/Assign' id:67839 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_60/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_60/kernel/v, dense_enc0_60/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 03:15:02.517558: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_60/outputlayer/BiasAdd' id:67544 op device:{requested: '', assigned: ''} def:{{{node decoder_model_60/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_60/outputlayer/MatMul, decoder_model_60/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.19538799688106703
cosine 0.19346942726325989
MAE: 0.025078957618136652
RMSE: 0.28527986460279997
r2: -62.575439486346454
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['1.9custom_VAE', 'mse', 8, 90, 0.001, 0.2, 188, '--', '--', 0.19538799688106703, 0.19346942726325989, 0.025078957618136652, 0.28527986460279997, -62.575439486346454, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0006000000000000001 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_156 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_156 (ReLU)               (None, 1697)         0           ['batch_normalization_156[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_156[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_156[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 03:15:26.856623: W tensorflow/c/c_api.cc:291] Operation '{name:'training_86/Adam/batch_normalization_157/beta/m/Assign' id:69098 op device:{requested: '', assigned: ''} def:{{{node training_86/Adam/batch_normalization_157/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_86/Adam/batch_normalization_157/beta/m, training_86/Adam/batch_normalization_157/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 03:15:44.564210: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_61/mul' id:68555 op device:{requested: '', assigned: ''} def:{{{node loss_61/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_61/mul/x, loss_61/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 28s - loss: 0.0037 - val_loss: 9.6949e-04 - 28s/epoch - 329us/sample
Epoch 2/90
84077/84077 - 12s - loss: 8.3327e-04 - val_loss: 7.5325e-04 - 12s/epoch - 146us/sample
Epoch 3/90
84077/84077 - 12s - loss: 0.0017 - val_loss: 0.0021 - 12s/epoch - 146us/sample
Epoch 4/90
84077/84077 - 12s - loss: 7.7931e-04 - val_loss: 6.3534e-04 - 12s/epoch - 146us/sample
Epoch 5/90
84077/84077 - 12s - loss: 6.3862e-04 - val_loss: 4.9362e-04 - 12s/epoch - 146us/sample
Epoch 6/90
84077/84077 - 12s - loss: 5.3204e-04 - val_loss: 4.0532e-04 - 12s/epoch - 148us/sample
Epoch 7/90
84077/84077 - 12s - loss: 4.3663e-04 - val_loss: 3.6924e-04 - 12s/epoch - 146us/sample
Epoch 8/90
84077/84077 - 12s - loss: 3.7535e-04 - val_loss: 3.4007e-04 - 12s/epoch - 146us/sample
Epoch 9/90
84077/84077 - 12s - loss: 3.3921e-04 - val_loss: 2.8381e-04 - 12s/epoch - 146us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.1137e-04 - val_loss: 2.6269e-04 - 12s/epoch - 146us/sample
Epoch 11/90
84077/84077 - 12s - loss: 3.0222e-04 - val_loss: 2.4984e-04 - 12s/epoch - 146us/sample
Epoch 12/90
84077/84077 - 12s - loss: 2.6796e-04 - val_loss: 2.3389e-04 - 12s/epoch - 148us/sample
Epoch 13/90
84077/84077 - 12s - loss: 2.5664e-04 - val_loss: 2.1688e-04 - 12s/epoch - 146us/sample
Epoch 14/90
84077/84077 - 12s - loss: 2.4006e-04 - val_loss: 2.0925e-04 - 12s/epoch - 146us/sample
Epoch 15/90
84077/84077 - 12s - loss: 2.3102e-04 - val_loss: 1.9929e-04 - 12s/epoch - 146us/sample
Epoch 16/90
84077/84077 - 12s - loss: 2.2024e-04 - val_loss: 1.9249e-04 - 12s/epoch - 146us/sample
Epoch 17/90
84077/84077 - 12s - loss: 2.1117e-04 - val_loss: 1.8629e-04 - 12s/epoch - 148us/sample
Epoch 18/90
84077/84077 - 12s - loss: 2.0344e-04 - val_loss: 1.7568e-04 - 12s/epoch - 146us/sample
Epoch 19/90
84077/84077 - 12s - loss: 1.9719e-04 - val_loss: 1.6881e-04 - 12s/epoch - 146us/sample
Epoch 20/90
84077/84077 - 12s - loss: 1.9108e-04 - val_loss: 1.6783e-04 - 12s/epoch - 146us/sample
Epoch 21/90
84077/84077 - 12s - loss: 1.8562e-04 - val_loss: 1.6277e-04 - 12s/epoch - 146us/sample
Epoch 22/90
84077/84077 - 12s - loss: 1.8187e-04 - val_loss: 1.5842e-04 - 12s/epoch - 147us/sample
Epoch 23/90
84077/84077 - 12s - loss: 1.7843e-04 - val_loss: 1.5545e-04 - 12s/epoch - 148us/sample
Epoch 24/90
84077/84077 - 12s - loss: 1.7572e-04 - val_loss: 1.5454e-04 - 12s/epoch - 146us/sample
Epoch 25/90
84077/84077 - 12s - loss: 1.7240e-04 - val_loss: 1.5284e-04 - 12s/epoch - 146us/sample
Epoch 26/90
84077/84077 - 12s - loss: 1.6939e-04 - val_loss: 1.5140e-04 - 12s/epoch - 146us/sample
Epoch 27/90
84077/84077 - 12s - loss: 1.6699e-04 - val_loss: 1.5087e-04 - 12s/epoch - 146us/sample
Epoch 28/90
84077/84077 - 12s - loss: 1.6540e-04 - val_loss: 1.4919e-04 - 12s/epoch - 148us/sample
Epoch 29/90
84077/84077 - 12s - loss: 1.6468e-04 - val_loss: 1.4774e-04 - 12s/epoch - 146us/sample
Epoch 30/90
84077/84077 - 12s - loss: 1.6193e-04 - val_loss: 1.4608e-04 - 12s/epoch - 146us/sample
Epoch 31/90
84077/84077 - 12s - loss: 1.6045e-04 - val_loss: 1.4387e-04 - 12s/epoch - 146us/sample
Epoch 32/90
84077/84077 - 12s - loss: 1.5855e-04 - val_loss: 1.4251e-04 - 12s/epoch - 146us/sample
Epoch 33/90
84077/84077 - 12s - loss: 1.5864e-04 - val_loss: 1.4314e-04 - 12s/epoch - 147us/sample
Epoch 34/90
84077/84077 - 12s - loss: 1.5718e-04 - val_loss: 1.4279e-04 - 12s/epoch - 147us/sample
Epoch 35/90
84077/84077 - 12s - loss: 1.5588e-04 - val_loss: 1.4239e-04 - 12s/epoch - 146us/sample
Epoch 36/90
84077/84077 - 12s - loss: 1.5504e-04 - val_loss: 1.4052e-04 - 12s/epoch - 146us/sample
Epoch 37/90
84077/84077 - 12s - loss: 1.5410e-04 - val_loss: 1.3846e-04 - 12s/epoch - 146us/sample
Epoch 38/90
84077/84077 - 12s - loss: 1.5326e-04 - val_loss: 1.3965e-04 - 12s/epoch - 146us/sample
Epoch 39/90
84077/84077 - 12s - loss: 1.5223e-04 - val_loss: 1.4084e-04 - 12s/epoch - 148us/sample
Epoch 40/90
84077/84077 - 12s - loss: 1.5210e-04 - val_loss: 1.3782e-04 - 12s/epoch - 146us/sample
Epoch 41/90
84077/84077 - 12s - loss: 1.5121e-04 - val_loss: 1.3765e-04 - 12s/epoch - 146us/sample
Epoch 42/90
84077/84077 - 12s - loss: 1.5061e-04 - val_loss: 1.3747e-04 - 12s/epoch - 146us/sample
Epoch 43/90
84077/84077 - 12s - loss: 1.4943e-04 - val_loss: 1.3572e-04 - 12s/epoch - 146us/sample
Epoch 44/90
84077/84077 - 12s - loss: 1.4914e-04 - val_loss: 1.3842e-04 - 12s/epoch - 147us/sample
Epoch 45/90
84077/84077 - 12s - loss: 1.4861e-04 - val_loss: 1.3515e-04 - 12s/epoch - 147us/sample
Epoch 46/90
84077/84077 - 12s - loss: 1.4829e-04 - val_loss: 1.3752e-04 - 12s/epoch - 146us/sample
Epoch 47/90
84077/84077 - 12s - loss: 1.4770e-04 - val_loss: 1.3454e-04 - 12s/epoch - 146us/sample
Epoch 48/90
84077/84077 - 12s - loss: 1.4722e-04 - val_loss: 1.3530e-04 - 12s/epoch - 146us/sample
Epoch 49/90
84077/84077 - 12s - loss: 1.4632e-04 - val_loss: 1.3518e-04 - 12s/epoch - 146us/sample
Epoch 50/90
84077/84077 - 12s - loss: 1.4628e-04 - val_loss: 1.3383e-04 - 12s/epoch - 148us/sample
Epoch 51/90
84077/84077 - 12s - loss: 1.4543e-04 - val_loss: 1.3291e-04 - 12s/epoch - 146us/sample
Epoch 52/90
84077/84077 - 12s - loss: 1.4539e-04 - val_loss: 1.3414e-04 - 12s/epoch - 146us/sample
Epoch 53/90
84077/84077 - 12s - loss: 1.4465e-04 - val_loss: 1.3304e-04 - 12s/epoch - 146us/sample
Epoch 54/90
84077/84077 - 12s - loss: 1.4427e-04 - val_loss: 1.3341e-04 - 12s/epoch - 146us/sample
Epoch 55/90
84077/84077 - 12s - loss: 1.4439e-04 - val_loss: 1.3303e-04 - 12s/epoch - 148us/sample
Epoch 56/90
84077/84077 - 12s - loss: 1.4356e-04 - val_loss: 1.3281e-04 - 12s/epoch - 146us/sample
Epoch 57/90
84077/84077 - 12s - loss: 1.4333e-04 - val_loss: 1.3350e-04 - 12s/epoch - 146us/sample
Epoch 58/90
84077/84077 - 12s - loss: 1.4297e-04 - val_loss: 1.3153e-04 - 12s/epoch - 146us/sample
Epoch 59/90
84077/84077 - 12s - loss: 1.4217e-04 - val_loss: 1.3028e-04 - 12s/epoch - 146us/sample
Epoch 60/90
84077/84077 - 12s - loss: 1.4229e-04 - val_loss: 1.3148e-04 - 12s/epoch - 147us/sample
Epoch 61/90
84077/84077 - 12s - loss: 1.4184e-04 - val_loss: 1.3216e-04 - 12s/epoch - 148us/sample
Epoch 62/90
84077/84077 - 12s - loss: 1.4213e-04 - val_loss: 1.3085e-04 - 12s/epoch - 146us/sample
Epoch 63/90
84077/84077 - 12s - loss: 1.4157e-04 - val_loss: 1.3036e-04 - 12s/epoch - 146us/sample
Epoch 64/90
84077/84077 - 12s - loss: 1.4105e-04 - val_loss: 1.3055e-04 - 12s/epoch - 146us/sample
Epoch 65/90
84077/84077 - 12s - loss: 1.4061e-04 - val_loss: 1.2962e-04 - 12s/epoch - 146us/sample
Epoch 66/90
84077/84077 - 12s - loss: 1.4024e-04 - val_loss: 1.3003e-04 - 12s/epoch - 148us/sample
Epoch 67/90
84077/84077 - 12s - loss: 1.4065e-04 - val_loss: 1.2952e-04 - 12s/epoch - 146us/sample
Epoch 68/90
84077/84077 - 12s - loss: 1.4005e-04 - val_loss: 1.3022e-04 - 12s/epoch - 146us/sample
Epoch 69/90
84077/84077 - 12s - loss: 1.3956e-04 - val_loss: 1.3026e-04 - 12s/epoch - 146us/sample
Epoch 70/90
84077/84077 - 12s - loss: 1.3952e-04 - val_loss: 1.2881e-04 - 12s/epoch - 146us/sample
Epoch 71/90
84077/84077 - 12s - loss: 1.3936e-04 - val_loss: 1.2848e-04 - 12s/epoch - 148us/sample
Epoch 72/90
84077/84077 - 12s - loss: 1.3894e-04 - val_loss: 1.2817e-04 - 12s/epoch - 147us/sample
Epoch 73/90
84077/84077 - 12s - loss: 1.3877e-04 - val_loss: 1.2951e-04 - 12s/epoch - 146us/sample
Epoch 74/90
84077/84077 - 12s - loss: 1.3880e-04 - val_loss: 1.2933e-04 - 12s/epoch - 146us/sample
Epoch 75/90
84077/84077 - 12s - loss: 1.3830e-04 - val_loss: 1.2967e-04 - 12s/epoch - 146us/sample
Epoch 76/90
84077/84077 - 12s - loss: 1.3785e-04 - val_loss: 1.2776e-04 - 12s/epoch - 146us/sample
Epoch 77/90
84077/84077 - 12s - loss: 1.3798e-04 - val_loss: 1.2949e-04 - 12s/epoch - 148us/sample
Epoch 78/90
84077/84077 - 12s - loss: 1.3775e-04 - val_loss: 1.2808e-04 - 12s/epoch - 146us/sample
Epoch 79/90
84077/84077 - 12s - loss: 1.3762e-04 - val_loss: 1.2634e-04 - 12s/epoch - 146us/sample
Epoch 80/90
84077/84077 - 12s - loss: 1.3750e-04 - val_loss: 1.2857e-04 - 12s/epoch - 146us/sample
Epoch 81/90
84077/84077 - 12s - loss: 1.3671e-04 - val_loss: 1.2685e-04 - 12s/epoch - 146us/sample
Epoch 82/90
84077/84077 - 12s - loss: 1.3720e-04 - val_loss: 1.2759e-04 - 12s/epoch - 148us/sample
Epoch 83/90
84077/84077 - 12s - loss: 1.3634e-04 - val_loss: 1.2679e-04 - 12s/epoch - 146us/sample
Epoch 84/90
84077/84077 - 12s - loss: 1.3648e-04 - val_loss: 1.2653e-04 - 12s/epoch - 146us/sample
Epoch 85/90
84077/84077 - 12s - loss: 1.3690e-04 - val_loss: 1.2670e-04 - 12s/epoch - 146us/sample
Epoch 86/90
84077/84077 - 12s - loss: 1.3656e-04 - val_loss: 1.2651e-04 - 12s/epoch - 146us/sample
Epoch 87/90
84077/84077 - 12s - loss: 1.3638e-04 - val_loss: 1.2588e-04 - 12s/epoch - 147us/sample
Epoch 88/90
84077/84077 - 12s - loss: 1.3607e-04 - val_loss: 1.2672e-04 - 12s/epoch - 147us/sample
Epoch 89/90
84077/84077 - 12s - loss: 1.3584e-04 - val_loss: 1.2630e-04 - 12s/epoch - 146us/sample
Epoch 90/90
84077/84077 - 12s - loss: 1.3556e-04 - val_loss: 1.2498e-04 - 12s/epoch - 146us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012497709978019349
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 03:34:04.380816: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_61/outputlayer/BiasAdd' id:68519 op device:{requested: '', assigned: ''} def:{{{node decoder_model_61/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_61/outputlayer/MatMul, decoder_model_61/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030594729251771594
cosine 0.030221332124183285
MAE: 0.0024354389317177367
RMSE: 0.00972780821655218
r2: 0.9263245221158397
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 64, 90, 0.0006000000000000001, 0.2, 188, 0.00013555767841663777, 0.00012497709978019349, 0.030594729251771594, 0.030221332124183285, 0.0024354389317177367, 0.00972780821655218, 0.9263245221158397, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0004000000000000001 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_159 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_159 (ReLU)               (None, 1791)         0           ['batch_normalization_159[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_159[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_159[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 03:34:30.386068: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_161/gamma/Assign' id:69642 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_161/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_161/gamma, batch_normalization_161/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 03:34:54.319970: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_62/mul' id:69833 op device:{requested: '', assigned: ''} def:{{{node loss_62/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_62/mul/x, loss_62/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 34s - loss: 0.0063 - val_loss: 0.0052 - 34s/epoch - 408us/sample
Epoch 2/90
84077/84077 - 19s - loss: 0.0026 - val_loss: 0.0014 - 19s/epoch - 223us/sample
Epoch 3/90
84077/84077 - 19s - loss: 0.0017 - val_loss: 9.7215e-04 - 19s/epoch - 225us/sample
Epoch 4/90
84077/84077 - 19s - loss: 0.0013 - val_loss: 7.5888e-04 - 19s/epoch - 224us/sample
Epoch 5/90
84077/84077 - 19s - loss: 8.8310e-04 - val_loss: 7.9612e-04 - 19s/epoch - 223us/sample
Epoch 6/90
84077/84077 - 19s - loss: 7.9396e-04 - val_loss: 5.8813e-04 - 19s/epoch - 223us/sample
Epoch 7/90
84077/84077 - 19s - loss: 6.1117e-04 - val_loss: 5.2901e-04 - 19s/epoch - 224us/sample
Epoch 8/90
84077/84077 - 19s - loss: 6.6139e-04 - val_loss: 5.1037e-04 - 19s/epoch - 224us/sample
Epoch 9/90
84077/84077 - 19s - loss: 5.1321e-04 - val_loss: 4.6684e-04 - 19s/epoch - 223us/sample
Epoch 10/90
84077/84077 - 19s - loss: 4.7933e-04 - val_loss: 4.1245e-04 - 19s/epoch - 223us/sample
Epoch 11/90
84077/84077 - 19s - loss: 4.3926e-04 - val_loss: 4.0495e-04 - 19s/epoch - 224us/sample
Epoch 12/90
84077/84077 - 19s - loss: 4.1375e-04 - val_loss: 3.9170e-04 - 19s/epoch - 224us/sample
Epoch 13/90
84077/84077 - 19s - loss: 3.9410e-04 - val_loss: 3.7656e-04 - 19s/epoch - 222us/sample
Epoch 14/90
84077/84077 - 19s - loss: 3.7783e-04 - val_loss: 3.6857e-04 - 19s/epoch - 223us/sample
Epoch 15/90
84077/84077 - 19s - loss: 3.6524e-04 - val_loss: 3.5143e-04 - 19s/epoch - 223us/sample
Epoch 16/90
84077/84077 - 19s - loss: 3.5175e-04 - val_loss: 3.3974e-04 - 19s/epoch - 224us/sample
Epoch 17/90
84077/84077 - 19s - loss: 3.3982e-04 - val_loss: 3.3056e-04 - 19s/epoch - 223us/sample
Epoch 18/90
84077/84077 - 19s - loss: 3.3114e-04 - val_loss: 3.3472e-04 - 19s/epoch - 223us/sample
Epoch 19/90
84077/84077 - 19s - loss: 3.2849e-04 - val_loss: 3.1779e-04 - 19s/epoch - 223us/sample
Epoch 20/90
84077/84077 - 19s - loss: 3.1918e-04 - val_loss: 3.2531e-04 - 19s/epoch - 224us/sample
Epoch 21/90
84077/84077 - 19s - loss: 3.1324e-04 - val_loss: 3.1671e-04 - 19s/epoch - 223us/sample
Epoch 22/90
84077/84077 - 19s - loss: 3.0761e-04 - val_loss: 3.1489e-04 - 19s/epoch - 223us/sample
Epoch 23/90
84077/84077 - 19s - loss: 3.0323e-04 - val_loss: 3.1805e-04 - 19s/epoch - 225us/sample
Epoch 24/90
84077/84077 - 19s - loss: 2.9977e-04 - val_loss: 3.1020e-04 - 19s/epoch - 223us/sample
Epoch 25/90
84077/84077 - 19s - loss: 2.9692e-04 - val_loss: 3.0424e-04 - 19s/epoch - 223us/sample
Epoch 26/90
84077/84077 - 19s - loss: 2.9457e-04 - val_loss: 3.0698e-04 - 19s/epoch - 223us/sample
Epoch 27/90
84077/84077 - 19s - loss: 2.9317e-04 - val_loss: 3.0217e-04 - 19s/epoch - 225us/sample
Epoch 28/90
84077/84077 - 19s - loss: 2.9765e-04 - val_loss: 3.1109e-04 - 19s/epoch - 223us/sample
Epoch 29/90
84077/84077 - 19s - loss: 2.8950e-04 - val_loss: 3.0236e-04 - 19s/epoch - 222us/sample
Epoch 30/90
84077/84077 - 19s - loss: 2.8653e-04 - val_loss: 2.9687e-04 - 19s/epoch - 223us/sample
Epoch 31/90
84077/84077 - 19s - loss: 2.8458e-04 - val_loss: 2.9184e-04 - 19s/epoch - 225us/sample
Epoch 32/90
84077/84077 - 19s - loss: 2.8270e-04 - val_loss: 2.9642e-04 - 19s/epoch - 223us/sample
Epoch 33/90
84077/84077 - 19s - loss: 2.8015e-04 - val_loss: 2.9486e-04 - 19s/epoch - 223us/sample
Epoch 34/90
84077/84077 - 19s - loss: 2.7906e-04 - val_loss: 2.9128e-04 - 19s/epoch - 223us/sample
Epoch 35/90
84077/84077 - 19s - loss: 2.7706e-04 - val_loss: 2.8909e-04 - 19s/epoch - 225us/sample
Epoch 36/90
84077/84077 - 19s - loss: 2.7572e-04 - val_loss: 2.8645e-04 - 19s/epoch - 223us/sample
Epoch 37/90
84077/84077 - 19s - loss: 2.7357e-04 - val_loss: 2.9094e-04 - 19s/epoch - 222us/sample
Epoch 38/90
84077/84077 - 19s - loss: 2.7260e-04 - val_loss: 2.9122e-04 - 19s/epoch - 223us/sample
Epoch 39/90
84077/84077 - 19s - loss: 2.7115e-04 - val_loss: 2.8609e-04 - 19s/epoch - 224us/sample
Epoch 40/90
84077/84077 - 19s - loss: 2.7051e-04 - val_loss: 2.8117e-04 - 19s/epoch - 224us/sample
Epoch 41/90
84077/84077 - 19s - loss: 2.6926e-04 - val_loss: 2.7941e-04 - 19s/epoch - 222us/sample
Epoch 42/90
84077/84077 - 19s - loss: 2.6838e-04 - val_loss: 2.8648e-04 - 19s/epoch - 223us/sample
Epoch 43/90
84077/84077 - 19s - loss: 2.6730e-04 - val_loss: 2.7948e-04 - 19s/epoch - 223us/sample
Epoch 44/90
84077/84077 - 19s - loss: 2.6637e-04 - val_loss: 2.8500e-04 - 19s/epoch - 224us/sample
Epoch 45/90
84077/84077 - 19s - loss: 2.6469e-04 - val_loss: 2.7693e-04 - 19s/epoch - 222us/sample
Epoch 46/90
84077/84077 - 19s - loss: 2.6399e-04 - val_loss: 2.6974e-04 - 19s/epoch - 223us/sample
Epoch 47/90
84077/84077 - 19s - loss: 2.6328e-04 - val_loss: 2.7679e-04 - 19s/epoch - 224us/sample
Epoch 48/90
84077/84077 - 19s - loss: 2.6213e-04 - val_loss: 2.7173e-04 - 19s/epoch - 225us/sample
Epoch 49/90
84077/84077 - 19s - loss: 2.6052e-04 - val_loss: 2.7708e-04 - 19s/epoch - 223us/sample
Epoch 50/90
84077/84077 - 19s - loss: 2.5961e-04 - val_loss: 2.7680e-04 - 19s/epoch - 223us/sample
Epoch 51/90
84077/84077 - 19s - loss: 2.5953e-04 - val_loss: 2.7541e-04 - 19s/epoch - 223us/sample
Epoch 52/90
84077/84077 - 19s - loss: 2.5772e-04 - val_loss: 2.7118e-04 - 19s/epoch - 225us/sample
Epoch 53/90
84077/84077 - 19s - loss: 2.5773e-04 - val_loss: 2.7168e-04 - 19s/epoch - 223us/sample
Epoch 54/90
84077/84077 - 19s - loss: 2.5715e-04 - val_loss: 2.6956e-04 - 19s/epoch - 223us/sample
Epoch 55/90
84077/84077 - 19s - loss: 2.5591e-04 - val_loss: 2.7563e-04 - 19s/epoch - 223us/sample
Epoch 56/90
84077/84077 - 19s - loss: 2.5608e-04 - val_loss: 2.7350e-04 - 19s/epoch - 225us/sample
Epoch 57/90
84077/84077 - 19s - loss: 2.5512e-04 - val_loss: 2.6851e-04 - 19s/epoch - 223us/sample
Epoch 58/90
84077/84077 - 19s - loss: 2.5455e-04 - val_loss: 2.7238e-04 - 19s/epoch - 223us/sample
Epoch 59/90
84077/84077 - 19s - loss: 2.5384e-04 - val_loss: 2.7307e-04 - 19s/epoch - 224us/sample
Epoch 60/90
84077/84077 - 19s - loss: 2.5378e-04 - val_loss: 2.6399e-04 - 19s/epoch - 225us/sample
Epoch 61/90
84077/84077 - 19s - loss: 2.5250e-04 - val_loss: 2.6110e-04 - 19s/epoch - 222us/sample
Epoch 62/90
84077/84077 - 19s - loss: 2.5173e-04 - val_loss: 2.6561e-04 - 19s/epoch - 223us/sample
Epoch 63/90
84077/84077 - 19s - loss: 2.5131e-04 - val_loss: 2.6686e-04 - 19s/epoch - 223us/sample
Epoch 64/90
84077/84077 - 19s - loss: 2.5168e-04 - val_loss: 2.6745e-04 - 19s/epoch - 225us/sample
Epoch 65/90
84077/84077 - 19s - loss: 2.5144e-04 - val_loss: 2.6545e-04 - 19s/epoch - 223us/sample
Epoch 66/90
84077/84077 - 19s - loss: 2.5073e-04 - val_loss: 2.6900e-04 - 19s/epoch - 223us/sample
Epoch 67/90
84077/84077 - 19s - loss: 2.5060e-04 - val_loss: 2.7240e-04 - 19s/epoch - 223us/sample
Epoch 68/90
84077/84077 - 19s - loss: 2.4909e-04 - val_loss: 2.6478e-04 - 19s/epoch - 225us/sample
Epoch 69/90
84077/84077 - 19s - loss: 2.4916e-04 - val_loss: 2.6314e-04 - 19s/epoch - 223us/sample
Epoch 70/90
84077/84077 - 19s - loss: 2.4905e-04 - val_loss: 2.6326e-04 - 19s/epoch - 223us/sample
Epoch 71/90
84077/84077 - 19s - loss: 2.4816e-04 - val_loss: 2.6434e-04 - 19s/epoch - 223us/sample
Epoch 72/90
84077/84077 - 19s - loss: 2.4836e-04 - val_loss: 2.6428e-04 - 19s/epoch - 225us/sample
Epoch 73/90
84077/84077 - 19s - loss: 2.4773e-04 - val_loss: 2.6432e-04 - 19s/epoch - 223us/sample
Epoch 74/90
84077/84077 - 19s - loss: 2.4708e-04 - val_loss: 2.6423e-04 - 19s/epoch - 223us/sample
Epoch 75/90
84077/84077 - 19s - loss: 2.4746e-04 - val_loss: 2.6429e-04 - 19s/epoch - 223us/sample
Epoch 76/90
84077/84077 - 19s - loss: 2.4764e-04 - val_loss: 2.6110e-04 - 19s/epoch - 225us/sample
Epoch 77/90
84077/84077 - 19s - loss: 2.4631e-04 - val_loss: 2.7093e-04 - 19s/epoch - 222us/sample
Epoch 78/90
84077/84077 - 19s - loss: 2.4653e-04 - val_loss: 2.6242e-04 - 19s/epoch - 223us/sample
Epoch 79/90
84077/84077 - 19s - loss: 2.4653e-04 - val_loss: 2.6541e-04 - 19s/epoch - 224us/sample
Epoch 80/90
84077/84077 - 19s - loss: 2.4546e-04 - val_loss: 2.6097e-04 - 19s/epoch - 225us/sample
Epoch 81/90
84077/84077 - 19s - loss: 2.4564e-04 - val_loss: 2.6337e-04 - 19s/epoch - 223us/sample
Epoch 82/90
84077/84077 - 19s - loss: 2.4471e-04 - val_loss: 2.6148e-04 - 19s/epoch - 223us/sample
Epoch 83/90
84077/84077 - 19s - loss: 2.4472e-04 - val_loss: 2.5997e-04 - 19s/epoch - 223us/sample
Epoch 84/90
84077/84077 - 19s - loss: 2.4490e-04 - val_loss: 2.6377e-04 - 19s/epoch - 225us/sample
Epoch 85/90
84077/84077 - 19s - loss: 2.4431e-04 - val_loss: 2.6053e-04 - 19s/epoch - 223us/sample
Epoch 86/90
84077/84077 - 19s - loss: 2.4382e-04 - val_loss: 2.6175e-04 - 19s/epoch - 223us/sample
Epoch 87/90
84077/84077 - 19s - loss: 2.4439e-04 - val_loss: 2.6533e-04 - 19s/epoch - 223us/sample
Epoch 88/90
84077/84077 - 19s - loss: 2.4338e-04 - val_loss: 2.6393e-04 - 19s/epoch - 226us/sample
Epoch 89/90
84077/84077 - 19s - loss: 2.4303e-04 - val_loss: 2.6860e-04 - 19s/epoch - 223us/sample
Epoch 90/90
84077/84077 - 19s - loss: 2.4229e-04 - val_loss: 2.6225e-04 - 19s/epoch - 223us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00026225198023674704
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 04:02:52.032296: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_62/outputlayer/BiasAdd' id:69804 op device:{requested: '', assigned: ''} def:{{{node decoder_model_62/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_62/outputlayer/MatMul, decoder_model_62/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.034686130600771685
cosine 0.0342724814377566
MAE: 0.002396374475524397
RMSE: 0.012838556176528905
r2: 0.8713407189809128
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 32, 90, 0.0004000000000000001, 0.2, 188, 0.00024229116312130545, 0.00026225198023674704, 0.034686130600771685, 0.0342724814377566, 0.002396374475524397, 0.012838556176528905, 0.8713407189809128, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 80 0.001 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_162 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_162 (ReLU)               (None, 1603)         0           ['batch_normalization_162[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_162[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_162[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE1.6999999999999997_cr0.2_bs64_ep80_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-13 04:03:18.823014: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_201/moving_variance/Assign' id:71338 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_201/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_201/moving_variance, batch_normalization_201/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-13 04:03:29.869575: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_64/bias/m/Assign' id:71734 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_64/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_64/bias/m, bottleneck_zmean_64/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 04:03:40.778159: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_64/outputlayer/BiasAdd' id:71513 op device:{requested: '', assigned: ''} def:{{{node decoder_model_64/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_64/outputlayer/MatMul, decoder_model_64/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031074383764154143
cosine 0.030697570606358013
MAE: 0.002209557258205984
RMSE: 0.010439627284234138
r2: 0.9150049418904866
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['1.6999999999999997custom_VAE', 'logcosh', 64, 80, 0.001, 0.2, 188, '--', '--', 0.031074383764154143, 0.030697570606358013, 0.002209557258205984, 0.010439627284234138, 0.9150049418904866, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0004000000000000001 32 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_165 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_165 (ReLU)               (None, 1791)         0           ['batch_normalization_165[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_165[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_165[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 04:04:06.269114: W tensorflow/c/c_api.cc:291] Operation '{name:'training_90/Adam/batch_normalization_167_1/beta/v/Assign' id:73216 op device:{requested: '', assigned: ''} def:{{{node training_90/Adam/batch_normalization_167_1/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_90/Adam/batch_normalization_167_1/beta/v, training_90/Adam/batch_normalization_167_1/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 04:04:30.803459: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_65/mul' id:72531 op device:{requested: '', assigned: ''} def:{{{node loss_65/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_65/mul/x, loss_65/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 35s - loss: 0.0035 - val_loss: 0.0020 - 35s/epoch - 421us/sample
Epoch 2/90
84077/84077 - 19s - loss: 0.0014 - val_loss: 9.2333e-04 - 19s/epoch - 225us/sample
Epoch 3/90
84077/84077 - 19s - loss: 7.2849e-04 - val_loss: 4.7771e-04 - 19s/epoch - 225us/sample
Epoch 4/90
84077/84077 - 19s - loss: 6.0304e-04 - val_loss: 0.0015 - 19s/epoch - 227us/sample
Epoch 5/90
84077/84077 - 19s - loss: 4.9817e-04 - val_loss: 3.7546e-04 - 19s/epoch - 225us/sample
Epoch 6/90
84077/84077 - 19s - loss: 3.9433e-04 - val_loss: 3.4073e-04 - 19s/epoch - 225us/sample
Epoch 7/90
84077/84077 - 19s - loss: 3.6027e-04 - val_loss: 3.3187e-04 - 19s/epoch - 226us/sample
Epoch 8/90
84077/84077 - 19s - loss: 3.5302e-04 - val_loss: 2.9848e-04 - 19s/epoch - 227us/sample
Epoch 9/90
84077/84077 - 19s - loss: 3.2684e-04 - val_loss: 2.9314e-04 - 19s/epoch - 225us/sample
Epoch 10/90
84077/84077 - 19s - loss: 3.1212e-04 - val_loss: 2.8281e-04 - 19s/epoch - 225us/sample
Epoch 11/90
84077/84077 - 19s - loss: 3.0207e-04 - val_loss: 2.7351e-04 - 19s/epoch - 225us/sample
Epoch 12/90
84077/84077 - 19s - loss: 2.9631e-04 - val_loss: 2.6905e-04 - 19s/epoch - 227us/sample
Epoch 13/90
84077/84077 - 19s - loss: 2.9128e-04 - val_loss: 2.6715e-04 - 19s/epoch - 225us/sample
Epoch 14/90
84077/84077 - 19s - loss: 2.8715e-04 - val_loss: 2.6744e-04 - 19s/epoch - 225us/sample
Epoch 15/90
84077/84077 - 19s - loss: 2.8369e-04 - val_loss: 2.6280e-04 - 19s/epoch - 225us/sample
Epoch 16/90
84077/84077 - 19s - loss: 2.8017e-04 - val_loss: 2.6009e-04 - 19s/epoch - 226us/sample
Epoch 17/90
84077/84077 - 19s - loss: 2.7769e-04 - val_loss: 2.5773e-04 - 19s/epoch - 225us/sample
Epoch 18/90
84077/84077 - 19s - loss: 2.7544e-04 - val_loss: 2.5762e-04 - 19s/epoch - 225us/sample
Epoch 19/90
84077/84077 - 19s - loss: 2.7290e-04 - val_loss: 2.5778e-04 - 19s/epoch - 226us/sample
Epoch 20/90
84077/84077 - 19s - loss: 2.7125e-04 - val_loss: 2.5645e-04 - 19s/epoch - 226us/sample
Epoch 21/90
84077/84077 - 19s - loss: 2.6939e-04 - val_loss: 2.5633e-04 - 19s/epoch - 225us/sample
Epoch 22/90
84077/84077 - 19s - loss: 2.6836e-04 - val_loss: 2.5666e-04 - 19s/epoch - 225us/sample
Epoch 23/90
84077/84077 - 19s - loss: 2.6745e-04 - val_loss: 2.5944e-04 - 19s/epoch - 227us/sample
Epoch 24/90
84077/84077 - 19s - loss: 2.6601e-04 - val_loss: 2.5550e-04 - 19s/epoch - 225us/sample
Epoch 25/90
84077/84077 - 19s - loss: 2.6553e-04 - val_loss: 2.5266e-04 - 19s/epoch - 225us/sample
Epoch 26/90
84077/84077 - 19s - loss: 2.6389e-04 - val_loss: 2.5095e-04 - 19s/epoch - 225us/sample
Epoch 27/90
84077/84077 - 19s - loss: 2.6285e-04 - val_loss: 2.5283e-04 - 19s/epoch - 227us/sample
Epoch 28/90
84077/84077 - 19s - loss: 2.6186e-04 - val_loss: 2.5007e-04 - 19s/epoch - 225us/sample
Epoch 29/90
84077/84077 - 19s - loss: 2.6132e-04 - val_loss: 2.4891e-04 - 19s/epoch - 225us/sample
Epoch 30/90
84077/84077 - 19s - loss: 2.6071e-04 - val_loss: 2.4774e-04 - 19s/epoch - 225us/sample
Epoch 31/90
84077/84077 - 19s - loss: 2.6009e-04 - val_loss: 2.4779e-04 - 19s/epoch - 228us/sample
Epoch 32/90
84077/84077 - 19s - loss: 2.5978e-04 - val_loss: 2.4702e-04 - 19s/epoch - 225us/sample
Epoch 33/90
84077/84077 - 19s - loss: 2.5895e-04 - val_loss: 2.4433e-04 - 19s/epoch - 225us/sample
Epoch 34/90
84077/84077 - 19s - loss: 2.5829e-04 - val_loss: 2.4572e-04 - 19s/epoch - 226us/sample
Epoch 35/90
84077/84077 - 19s - loss: 2.5756e-04 - val_loss: 2.4454e-04 - 19s/epoch - 227us/sample
Epoch 36/90
84077/84077 - 19s - loss: 2.5724e-04 - val_loss: 2.4334e-04 - 19s/epoch - 225us/sample
Epoch 37/90
84077/84077 - 19s - loss: 2.5672e-04 - val_loss: 2.4303e-04 - 19s/epoch - 225us/sample
Epoch 38/90
84077/84077 - 19s - loss: 2.5620e-04 - val_loss: 2.4139e-04 - 19s/epoch - 225us/sample
Epoch 39/90
84077/84077 - 19s - loss: 2.5575e-04 - val_loss: 2.4310e-04 - 19s/epoch - 227us/sample
Epoch 40/90
84077/84077 - 19s - loss: 2.5491e-04 - val_loss: 2.4035e-04 - 19s/epoch - 225us/sample
Epoch 41/90
84077/84077 - 19s - loss: 2.5499e-04 - val_loss: 2.4050e-04 - 19s/epoch - 225us/sample
Epoch 42/90
84077/84077 - 19s - loss: 2.5417e-04 - val_loss: 2.4166e-04 - 19s/epoch - 225us/sample
Epoch 43/90
84077/84077 - 19s - loss: 2.5429e-04 - val_loss: 2.4043e-04 - 19s/epoch - 227us/sample
Epoch 44/90
84077/84077 - 19s - loss: 2.5400e-04 - val_loss: 2.4022e-04 - 19s/epoch - 225us/sample
Epoch 45/90
84077/84077 - 19s - loss: 2.5362e-04 - val_loss: 2.3896e-04 - 19s/epoch - 225us/sample
Epoch 46/90
84077/84077 - 19s - loss: 2.5302e-04 - val_loss: 2.3979e-04 - 19s/epoch - 225us/sample
Epoch 47/90
84077/84077 - 19s - loss: 2.5280e-04 - val_loss: 2.3971e-04 - 19s/epoch - 227us/sample
Epoch 48/90
84077/84077 - 19s - loss: 2.5244e-04 - val_loss: 2.4145e-04 - 19s/epoch - 225us/sample
Epoch 49/90
84077/84077 - 19s - loss: 2.5212e-04 - val_loss: 2.3850e-04 - 19s/epoch - 225us/sample
Epoch 50/90
84077/84077 - 19s - loss: 2.5186e-04 - val_loss: 2.3801e-04 - 19s/epoch - 226us/sample
Epoch 51/90
84077/84077 - 19s - loss: 2.5176e-04 - val_loss: 2.3823e-04 - 19s/epoch - 227us/sample
Epoch 52/90
84077/84077 - 19s - loss: 2.5134e-04 - val_loss: 2.3801e-04 - 19s/epoch - 225us/sample
Epoch 53/90
84077/84077 - 19s - loss: 2.5087e-04 - val_loss: 2.3713e-04 - 19s/epoch - 226us/sample
Epoch 54/90
84077/84077 - 19s - loss: 2.5069e-04 - val_loss: 2.3682e-04 - 19s/epoch - 225us/sample
Epoch 55/90
84077/84077 - 19s - loss: 2.5039e-04 - val_loss: 2.3714e-04 - 19s/epoch - 227us/sample
Epoch 56/90
84077/84077 - 19s - loss: 2.5025e-04 - val_loss: 2.3678e-04 - 19s/epoch - 225us/sample
Epoch 57/90
84077/84077 - 19s - loss: 2.5015e-04 - val_loss: 2.3647e-04 - 19s/epoch - 225us/sample
Epoch 58/90
84077/84077 - 19s - loss: 2.4987e-04 - val_loss: 2.3741e-04 - 19s/epoch - 226us/sample
Epoch 59/90
84077/84077 - 19s - loss: 2.4962e-04 - val_loss: 2.3575e-04 - 19s/epoch - 227us/sample
Epoch 60/90
84077/84077 - 19s - loss: 2.4950e-04 - val_loss: 2.3657e-04 - 19s/epoch - 225us/sample
Epoch 61/90
84077/84077 - 19s - loss: 2.4942e-04 - val_loss: 2.3471e-04 - 19s/epoch - 225us/sample
Epoch 62/90
84077/84077 - 19s - loss: 2.4908e-04 - val_loss: 2.3481e-04 - 19s/epoch - 225us/sample
Epoch 63/90
84077/84077 - 19s - loss: 2.4850e-04 - val_loss: 2.3470e-04 - 19s/epoch - 227us/sample
Epoch 64/90
84077/84077 - 19s - loss: 2.4885e-04 - val_loss: 2.3552e-04 - 19s/epoch - 226us/sample
Epoch 65/90
84077/84077 - 19s - loss: 2.4863e-04 - val_loss: 2.3584e-04 - 19s/epoch - 225us/sample
Epoch 66/90
84077/84077 - 19s - loss: 2.4828e-04 - val_loss: 2.3495e-04 - 19s/epoch - 226us/sample
Epoch 67/90
84077/84077 - 19s - loss: 2.4805e-04 - val_loss: 2.3378e-04 - 19s/epoch - 227us/sample
Epoch 68/90
84077/84077 - 19s - loss: 2.4781e-04 - val_loss: 2.3386e-04 - 19s/epoch - 225us/sample
Epoch 69/90
84077/84077 - 19s - loss: 2.4789e-04 - val_loss: 2.3445e-04 - 19s/epoch - 225us/sample
Epoch 70/90
84077/84077 - 19s - loss: 2.4762e-04 - val_loss: 2.3365e-04 - 19s/epoch - 225us/sample
Epoch 71/90
84077/84077 - 19s - loss: 2.4748e-04 - val_loss: 2.3228e-04 - 19s/epoch - 227us/sample
Epoch 72/90
84077/84077 - 19s - loss: 2.4754e-04 - val_loss: 2.3279e-04 - 19s/epoch - 225us/sample
Epoch 73/90
84077/84077 - 19s - loss: 2.4730e-04 - val_loss: 2.3354e-04 - 19s/epoch - 225us/sample
Epoch 74/90
84077/84077 - 19s - loss: 2.4696e-04 - val_loss: 2.3332e-04 - 19s/epoch - 225us/sample
Epoch 75/90
84077/84077 - 19s - loss: 2.4681e-04 - val_loss: 2.3395e-04 - 19s/epoch - 227us/sample
Epoch 76/90
84077/84077 - 19s - loss: 2.4673e-04 - val_loss: 2.3406e-04 - 19s/epoch - 225us/sample
Epoch 77/90
84077/84077 - 19s - loss: 2.4661e-04 - val_loss: 2.3227e-04 - 19s/epoch - 225us/sample
Epoch 78/90
84077/84077 - 19s - loss: 2.4676e-04 - val_loss: 2.3362e-04 - 19s/epoch - 225us/sample
Epoch 79/90
84077/84077 - 19s - loss: 2.4636e-04 - val_loss: 2.3271e-04 - 19s/epoch - 227us/sample
Epoch 80/90
84077/84077 - 19s - loss: 2.4577e-04 - val_loss: 2.3274e-04 - 19s/epoch - 225us/sample
Epoch 81/90
84077/84077 - 19s - loss: 2.4605e-04 - val_loss: 2.3228e-04 - 19s/epoch - 225us/sample
Epoch 82/90
84077/84077 - 19s - loss: 2.4556e-04 - val_loss: 2.3180e-04 - 19s/epoch - 225us/sample
Epoch 83/90
84077/84077 - 19s - loss: 2.4518e-04 - val_loss: 2.3236e-04 - 19s/epoch - 227us/sample
Epoch 84/90
84077/84077 - 19s - loss: 2.4556e-04 - val_loss: 2.3364e-04 - 19s/epoch - 225us/sample
Epoch 85/90
84077/84077 - 19s - loss: 2.4478e-04 - val_loss: 2.3096e-04 - 19s/epoch - 225us/sample
Epoch 86/90
84077/84077 - 19s - loss: 2.4488e-04 - val_loss: 2.3197e-04 - 19s/epoch - 225us/sample
Epoch 87/90
84077/84077 - 19s - loss: 2.4499e-04 - val_loss: 2.3157e-04 - 19s/epoch - 227us/sample
Epoch 88/90
84077/84077 - 19s - loss: 2.4517e-04 - val_loss: 2.3238e-04 - 19s/epoch - 225us/sample
Epoch 89/90
84077/84077 - 19s - loss: 2.4451e-04 - val_loss: 2.3237e-04 - 19s/epoch - 225us/sample
Epoch 90/90
84077/84077 - 19s - loss: 2.4473e-04 - val_loss: 2.3228e-04 - 19s/epoch - 225us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002322818696058681
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 04:32:45.136706: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_65/outputlayer/BiasAdd' id:72495 op device:{requested: '', assigned: ''} def:{{{node decoder_model_65/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_65/outputlayer/MatMul, decoder_model_65/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.09534564036858299
cosine 0.09407727965879743
MAE: 0.0034391165004444024
RMSE: 0.018795030856677053
r2: 0.7240417250696326
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 32, 90, 0.0004000000000000001, 0.2, 188, 0.0002447317981057236, 0.0002322818696058681, 0.09534564036858299, 0.09407727965879743, 0.0034391165004444024, 0.018795030856677053, 0.7240417250696326, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 90 0.0004000000000000001 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_168 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_168 (ReLU)               (None, 1603)         0           ['batch_normalization_168[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_168[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_168[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 04:33:12.357381: W tensorflow/c/c_api.cc:291] Operation '{name:'training_92/Adam/batch_normalization_170/gamma/m/Assign' id:74350 op device:{requested: '', assigned: ''} def:{{{node training_92/Adam/batch_normalization_170/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_92/Adam/batch_normalization_170/gamma/m, training_92/Adam/batch_normalization_170/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 04:33:30.620546: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_66/mul' id:73809 op device:{requested: '', assigned: ''} def:{{{node loss_66/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_66/mul/x, loss_66/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 29s - loss: 0.8068 - val_loss: 0.0016 - 29s/epoch - 345us/sample
Epoch 2/90
84077/84077 - 13s - loss: 0.0013 - val_loss: 0.0022 - 13s/epoch - 150us/sample
Epoch 3/90
84077/84077 - 12s - loss: 0.0011 - val_loss: 9.4165e-04 - 12s/epoch - 149us/sample
Epoch 4/90
84077/84077 - 12s - loss: 0.0055 - val_loss: 0.0013 - 12s/epoch - 149us/sample
Epoch 5/90
84077/84077 - 12s - loss: 0.0012 - val_loss: 9.5597e-04 - 12s/epoch - 148us/sample
Epoch 6/90
84077/84077 - 12s - loss: 9.4978e-04 - val_loss: 9.1741e-04 - 12s/epoch - 148us/sample
Epoch 7/90
84077/84077 - 13s - loss: 0.1080 - val_loss: 0.0023 - 13s/epoch - 149us/sample
Epoch 8/90
84077/84077 - 13s - loss: 0.0014 - val_loss: 0.0016 - 13s/epoch - 151us/sample
Epoch 9/90
84077/84077 - 13s - loss: 0.0026 - val_loss: 0.0016 - 13s/epoch - 149us/sample
Epoch 10/90
84077/84077 - 13s - loss: 0.0012 - val_loss: 8.8504e-04 - 13s/epoch - 149us/sample
Epoch 11/90
84077/84077 - 12s - loss: 0.0015 - val_loss: 0.0011 - 12s/epoch - 148us/sample
Epoch 12/90
84077/84077 - 13s - loss: 0.0261 - val_loss: 0.0012 - 13s/epoch - 149us/sample
Epoch 13/90
84077/84077 - 13s - loss: 9.2624e-04 - val_loss: 7.5215e-04 - 13s/epoch - 150us/sample
Epoch 14/90
84077/84077 - 13s - loss: 0.0077 - val_loss: 8.7738e-04 - 13s/epoch - 150us/sample
Epoch 15/90
84077/84077 - 12s - loss: 8.1938e-04 - val_loss: 7.0929e-04 - 12s/epoch - 148us/sample
Epoch 16/90
84077/84077 - 12s - loss: 0.0010 - val_loss: 7.0557e-04 - 12s/epoch - 148us/sample
Epoch 17/90
84077/84077 - 13s - loss: 7.2161e-04 - val_loss: 6.5883e-04 - 13s/epoch - 149us/sample
Epoch 18/90
84077/84077 - 13s - loss: 6.8442e-04 - val_loss: 6.2309e-04 - 13s/epoch - 149us/sample
Epoch 19/90
84077/84077 - 13s - loss: 7.2904e-04 - val_loss: 6.2056e-04 - 13s/epoch - 151us/sample
Epoch 20/90
84077/84077 - 12s - loss: 6.4442e-04 - val_loss: 5.9290e-04 - 12s/epoch - 149us/sample
Epoch 21/90
84077/84077 - 12s - loss: 6.3705e-04 - val_loss: 0.0011 - 12s/epoch - 148us/sample
Epoch 22/90
84077/84077 - 12s - loss: 6.2396e-04 - val_loss: 5.7224e-04 - 12s/epoch - 148us/sample
Epoch 23/90
84077/84077 - 12s - loss: 6.2103e-04 - val_loss: 5.5808e-04 - 12s/epoch - 149us/sample
Epoch 24/90
84077/84077 - 13s - loss: 5.9022e-04 - val_loss: 5.4678e-04 - 13s/epoch - 151us/sample
Epoch 25/90
84077/84077 - 13s - loss: 6.5820e-04 - val_loss: 5.9953e-04 - 13s/epoch - 149us/sample
Epoch 26/90
84077/84077 - 12s - loss: 5.7987e-04 - val_loss: 5.3714e-04 - 12s/epoch - 148us/sample
Epoch 27/90
84077/84077 - 12s - loss: 5.8796e-04 - val_loss: 5.3298e-04 - 12s/epoch - 149us/sample
Epoch 28/90
84077/84077 - 12s - loss: 5.7147e-04 - val_loss: 5.2806e-04 - 12s/epoch - 148us/sample
Epoch 29/90
84077/84077 - 13s - loss: 5.5609e-04 - val_loss: 5.1879e-04 - 13s/epoch - 150us/sample
Epoch 30/90
84077/84077 - 13s - loss: 5.4990e-04 - val_loss: 5.1502e-04 - 13s/epoch - 150us/sample
Epoch 31/90
84077/84077 - 12s - loss: 6.7969e-04 - val_loss: 0.0032 - 12s/epoch - 149us/sample
Epoch 32/90
84077/84077 - 12s - loss: 6.9425e-04 - val_loss: 5.3369e-04 - 12s/epoch - 148us/sample
Epoch 33/90
84077/84077 - 12s - loss: 5.6266e-04 - val_loss: 5.1854e-04 - 12s/epoch - 149us/sample
Epoch 34/90
84077/84077 - 13s - loss: 5.5141e-04 - val_loss: 5.1251e-04 - 13s/epoch - 149us/sample
Epoch 35/90
84077/84077 - 13s - loss: 5.6791e-04 - val_loss: 5.3543e-04 - 13s/epoch - 151us/sample
Epoch 36/90
84077/84077 - 12s - loss: 5.4098e-04 - val_loss: 5.0453e-04 - 12s/epoch - 148us/sample
Epoch 37/90
84077/84077 - 12s - loss: 5.3416e-04 - val_loss: 5.0335e-04 - 12s/epoch - 148us/sample
Epoch 38/90
84077/84077 - 12s - loss: 5.3069e-04 - val_loss: 4.9965e-04 - 12s/epoch - 148us/sample
Epoch 39/90
84077/84077 - 13s - loss: 5.2818e-04 - val_loss: 4.9697e-04 - 13s/epoch - 149us/sample
Epoch 40/90
84077/84077 - 13s - loss: 5.2519e-04 - val_loss: 4.9283e-04 - 13s/epoch - 150us/sample
Epoch 41/90
84077/84077 - 13s - loss: 5.2092e-04 - val_loss: 4.9163e-04 - 13s/epoch - 149us/sample
Epoch 42/90
84077/84077 - 12s - loss: 5.2120e-04 - val_loss: 4.8794e-04 - 12s/epoch - 148us/sample
Epoch 43/90
84077/84077 - 12s - loss: 5.1490e-04 - val_loss: 4.8404e-04 - 12s/epoch - 149us/sample
Epoch 44/90
84077/84077 - 12s - loss: 5.0974e-04 - val_loss: 4.7701e-04 - 12s/epoch - 148us/sample
Epoch 45/90
84077/84077 - 13s - loss: 5.0636e-04 - val_loss: 4.7566e-04 - 13s/epoch - 149us/sample
Epoch 46/90
84077/84077 - 13s - loss: 5.0374e-04 - val_loss: 4.7365e-04 - 13s/epoch - 151us/sample
Epoch 47/90
84077/84077 - 12s - loss: 5.0030e-04 - val_loss: 4.7121e-04 - 12s/epoch - 148us/sample
Epoch 48/90
84077/84077 - 12s - loss: 4.9729e-04 - val_loss: 4.6980e-04 - 12s/epoch - 149us/sample
Epoch 49/90
84077/84077 - 12s - loss: 4.9519e-04 - val_loss: 4.6831e-04 - 12s/epoch - 148us/sample
Epoch 50/90
84077/84077 - 13s - loss: 4.9394e-04 - val_loss: 4.6625e-04 - 13s/epoch - 149us/sample
Epoch 51/90
84077/84077 - 13s - loss: 4.9252e-04 - val_loss: 4.6731e-04 - 13s/epoch - 151us/sample
Epoch 52/90
84077/84077 - 12s - loss: 4.9029e-04 - val_loss: 4.6327e-04 - 12s/epoch - 149us/sample
Epoch 53/90
84077/84077 - 13s - loss: 4.8719e-04 - val_loss: 4.6114e-04 - 13s/epoch - 149us/sample
Epoch 54/90
84077/84077 - 12s - loss: 4.8585e-04 - val_loss: 4.6170e-04 - 12s/epoch - 148us/sample
Epoch 55/90
84077/84077 - 13s - loss: 4.8367e-04 - val_loss: 4.5832e-04 - 13s/epoch - 149us/sample
Epoch 56/90
84077/84077 - 13s - loss: 4.8255e-04 - val_loss: 4.5824e-04 - 13s/epoch - 151us/sample
Epoch 57/90
84077/84077 - 13s - loss: 4.8136e-04 - val_loss: 4.5682e-04 - 13s/epoch - 149us/sample
Epoch 58/90
84077/84077 - 12s - loss: 4.8097e-04 - val_loss: 4.5624e-04 - 12s/epoch - 149us/sample
Epoch 59/90
84077/84077 - 12s - loss: 4.8045e-04 - val_loss: 4.5682e-04 - 12s/epoch - 148us/sample
Epoch 60/90
84077/84077 - 13s - loss: 4.7913e-04 - val_loss: 4.5652e-04 - 13s/epoch - 149us/sample
Epoch 61/90
84077/84077 - 13s - loss: 4.7854e-04 - val_loss: 4.5567e-04 - 13s/epoch - 149us/sample
Epoch 62/90
84077/84077 - 13s - loss: 4.7774e-04 - val_loss: 4.5418e-04 - 13s/epoch - 150us/sample
Epoch 63/90
84077/84077 - 13s - loss: 4.7703e-04 - val_loss: 4.5449e-04 - 13s/epoch - 149us/sample
Epoch 64/90
84077/84077 - 12s - loss: 4.7610e-04 - val_loss: 4.5465e-04 - 12s/epoch - 148us/sample
Epoch 65/90
84077/84077 - 13s - loss: 4.7474e-04 - val_loss: 4.5322e-04 - 13s/epoch - 149us/sample
Epoch 66/90
84077/84077 - 12s - loss: 4.7386e-04 - val_loss: 4.5184e-04 - 12s/epoch - 149us/sample
Epoch 67/90
84077/84077 - 13s - loss: 4.7242e-04 - val_loss: 4.5016e-04 - 13s/epoch - 151us/sample
Epoch 68/90
84077/84077 - 12s - loss: 4.6996e-04 - val_loss: 4.4901e-04 - 12s/epoch - 148us/sample
Epoch 69/90
84077/84077 - 12s - loss: 4.6925e-04 - val_loss: 4.4756e-04 - 12s/epoch - 148us/sample
Epoch 70/90
84077/84077 - 13s - loss: 4.6841e-04 - val_loss: 4.4755e-04 - 13s/epoch - 149us/sample
Epoch 71/90
84077/84077 - 12s - loss: 4.6733e-04 - val_loss: 4.4521e-04 - 12s/epoch - 149us/sample
Epoch 72/90
84077/84077 - 13s - loss: 4.6731e-04 - val_loss: 4.4552e-04 - 13s/epoch - 151us/sample
Epoch 73/90
84077/84077 - 13s - loss: 4.6656e-04 - val_loss: 4.4399e-04 - 13s/epoch - 149us/sample
Epoch 74/90
84077/84077 - 12s - loss: 4.6556e-04 - val_loss: 4.4466e-04 - 12s/epoch - 148us/sample
Epoch 75/90
84077/84077 - 12s - loss: 4.6550e-04 - val_loss: 4.4596e-04 - 12s/epoch - 149us/sample
Epoch 76/90
84077/84077 - 12s - loss: 4.6472e-04 - val_loss: 4.4504e-04 - 12s/epoch - 148us/sample
Epoch 77/90
84077/84077 - 13s - loss: 4.6454e-04 - val_loss: 4.4424e-04 - 13s/epoch - 149us/sample
Epoch 78/90
84077/84077 - 13s - loss: 4.6445e-04 - val_loss: 4.4443e-04 - 13s/epoch - 150us/sample
Epoch 79/90
84077/84077 - 13s - loss: 4.6412e-04 - val_loss: 4.4428e-04 - 13s/epoch - 149us/sample
Epoch 80/90
84077/84077 - 12s - loss: 4.6351e-04 - val_loss: 4.4238e-04 - 12s/epoch - 148us/sample
Epoch 81/90
84077/84077 - 12s - loss: 4.6291e-04 - val_loss: 4.4257e-04 - 12s/epoch - 148us/sample
Epoch 82/90
84077/84077 - 13s - loss: 4.6170e-04 - val_loss: 4.4081e-04 - 13s/epoch - 149us/sample
Epoch 83/90
84077/84077 - 13s - loss: 4.6089e-04 - val_loss: 4.4091e-04 - 13s/epoch - 151us/sample
Epoch 84/90
84077/84077 - 13s - loss: 4.6116e-04 - val_loss: 4.4098e-04 - 13s/epoch - 149us/sample
Epoch 85/90
84077/84077 - 12s - loss: 4.6077e-04 - val_loss: 4.4051e-04 - 12s/epoch - 148us/sample
Epoch 86/90
84077/84077 - 12s - loss: 4.5991e-04 - val_loss: 4.4011e-04 - 12s/epoch - 148us/sample
Epoch 87/90
84077/84077 - 13s - loss: 4.5984e-04 - val_loss: 4.4002e-04 - 13s/epoch - 149us/sample
Epoch 88/90
84077/84077 - 13s - loss: 4.5898e-04 - val_loss: 4.4180e-04 - 13s/epoch - 151us/sample
Epoch 89/90
84077/84077 - 13s - loss: 4.5926e-04 - val_loss: 4.4084e-04 - 13s/epoch - 149us/sample
Epoch 90/90
84077/84077 - 12s - loss: 4.5897e-04 - val_loss: 4.4065e-04 - 12s/epoch - 148us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0004406482044718956
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 04:52:11.663276: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_66/outputlayer/BiasAdd' id:73780 op device:{requested: '', assigned: ''} def:{{{node decoder_model_66/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_66/outputlayer/MatMul, decoder_model_66/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.09738091974413703
cosine 0.09608675207781248
MAE: 0.0036781622999396127
RMSE: 0.018919722097252102
r2: 0.7204004435077714
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 64, 90, 0.0004000000000000001, 0.2, 188, 0.0004589655687327982, 0.0004406482044718956, 0.09738091974413703, 0.09608675207781248, 0.0036781622999396127, 0.018919722097252102, 0.7204004435077714, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 80 0.00020000000000000006 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_171 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_171 (ReLU)               (None, 1603)         0           ['batch_normalization_171[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_171[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_171[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-13 04:52:39.487679: W tensorflow/c/c_api.cc:291] Operation '{name:'training_94/Adam/bottleneck_zlog_67/bias/m/Assign' id:75592 op device:{requested: '', assigned: ''} def:{{{node training_94/Adam/bottleneck_zlog_67/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_94/Adam/bottleneck_zlog_67/bias/m, training_94/Adam/bottleneck_zlog_67/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 04:52:58.190792: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_67/mul' id:75071 op device:{requested: '', assigned: ''} def:{{{node loss_67/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_67/mul/x, loss_67/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 30s - loss: 0.0422 - val_loss: 0.0011 - 30s/epoch - 352us/sample
Epoch 2/80
84077/84077 - 13s - loss: 8.1023e-04 - val_loss: 9.4623e-04 - 13s/epoch - 151us/sample
Epoch 3/80
84077/84077 - 13s - loss: 8.8621e-04 - val_loss: 0.0027 - 13s/epoch - 151us/sample
Epoch 4/80
84077/84077 - 13s - loss: 7.3919e-04 - val_loss: 9.3657e-04 - 13s/epoch - 151us/sample
Epoch 5/80
84077/84077 - 13s - loss: 6.2275e-04 - val_loss: 5.9628e-04 - 13s/epoch - 154us/sample
Epoch 6/80
84077/84077 - 13s - loss: 5.1533e-04 - val_loss: 4.8378e-04 - 13s/epoch - 152us/sample
Epoch 7/80
84077/84077 - 13s - loss: 4.1468e-04 - val_loss: 3.4688e-04 - 13s/epoch - 151us/sample
Epoch 8/80
84077/84077 - 13s - loss: 3.6697e-04 - val_loss: 3.2043e-04 - 13s/epoch - 151us/sample
Epoch 9/80
84077/84077 - 13s - loss: 3.3366e-04 - val_loss: 2.8346e-04 - 13s/epoch - 151us/sample
Epoch 10/80
84077/84077 - 13s - loss: 3.2413e-04 - val_loss: 2.7404e-04 - 13s/epoch - 152us/sample
Epoch 11/80
84077/84077 - 13s - loss: 2.9524e-04 - val_loss: 2.6686e-04 - 13s/epoch - 153us/sample
Epoch 12/80
84077/84077 - 13s - loss: 2.7322e-04 - val_loss: 2.4147e-04 - 13s/epoch - 151us/sample
Epoch 13/80
84077/84077 - 13s - loss: 2.5706e-04 - val_loss: 2.3882e-04 - 13s/epoch - 151us/sample
Epoch 14/80
84077/84077 - 13s - loss: 2.4567e-04 - val_loss: 2.1330e-04 - 13s/epoch - 151us/sample
Epoch 15/80
84077/84077 - 13s - loss: 2.3399e-04 - val_loss: 2.0492e-04 - 13s/epoch - 151us/sample
Epoch 16/80
84077/84077 - 13s - loss: 2.2207e-04 - val_loss: 1.9826e-04 - 13s/epoch - 153us/sample
Epoch 17/80
84077/84077 - 13s - loss: 2.1323e-04 - val_loss: 1.8753e-04 - 13s/epoch - 151us/sample
Epoch 18/80
84077/84077 - 13s - loss: 2.0558e-04 - val_loss: 1.7872e-04 - 13s/epoch - 151us/sample
Epoch 19/80
84077/84077 - 13s - loss: 1.9677e-04 - val_loss: 1.7563e-04 - 13s/epoch - 151us/sample
Epoch 20/80
84077/84077 - 13s - loss: 1.9233e-04 - val_loss: 1.6888e-04 - 13s/epoch - 151us/sample
Epoch 21/80
84077/84077 - 13s - loss: 1.8771e-04 - val_loss: 1.7023e-04 - 13s/epoch - 151us/sample
Epoch 22/80
84077/84077 - 13s - loss: 1.8389e-04 - val_loss: 1.6385e-04 - 13s/epoch - 154us/sample
Epoch 23/80
84077/84077 - 13s - loss: 1.7962e-04 - val_loss: 1.6129e-04 - 13s/epoch - 151us/sample
Epoch 24/80
84077/84077 - 13s - loss: 1.7673e-04 - val_loss: 1.5791e-04 - 13s/epoch - 151us/sample
Epoch 25/80
84077/84077 - 13s - loss: 1.7354e-04 - val_loss: 1.5522e-04 - 13s/epoch - 151us/sample
Epoch 26/80
84077/84077 - 13s - loss: 1.7166e-04 - val_loss: 1.5349e-04 - 13s/epoch - 151us/sample
Epoch 27/80
84077/84077 - 13s - loss: 1.6921e-04 - val_loss: 1.5102e-04 - 13s/epoch - 153us/sample
Epoch 28/80
84077/84077 - 13s - loss: 1.6796e-04 - val_loss: 1.4870e-04 - 13s/epoch - 152us/sample
Epoch 29/80
84077/84077 - 13s - loss: 1.6609e-04 - val_loss: 1.4968e-04 - 13s/epoch - 151us/sample
Epoch 30/80
84077/84077 - 13s - loss: 1.6427e-04 - val_loss: 1.4762e-04 - 13s/epoch - 151us/sample
Epoch 31/80
84077/84077 - 13s - loss: 1.6279e-04 - val_loss: 1.4732e-04 - 13s/epoch - 151us/sample
Epoch 32/80
84077/84077 - 13s - loss: 1.6143e-04 - val_loss: 1.4649e-04 - 13s/epoch - 152us/sample
Epoch 33/80
84077/84077 - 13s - loss: 1.6002e-04 - val_loss: 1.4597e-04 - 13s/epoch - 153us/sample
Epoch 34/80
84077/84077 - 13s - loss: 1.5872e-04 - val_loss: 1.4330e-04 - 13s/epoch - 151us/sample
Epoch 35/80
84077/84077 - 13s - loss: 1.5762e-04 - val_loss: 1.4150e-04 - 13s/epoch - 151us/sample
Epoch 36/80
84077/84077 - 13s - loss: 1.5608e-04 - val_loss: 1.4258e-04 - 13s/epoch - 151us/sample
Epoch 37/80
84077/84077 - 13s - loss: 1.5514e-04 - val_loss: 1.4155e-04 - 13s/epoch - 151us/sample
Epoch 38/80
84077/84077 - 13s - loss: 1.5473e-04 - val_loss: 1.4040e-04 - 13s/epoch - 154us/sample
Epoch 39/80
84077/84077 - 13s - loss: 1.5316e-04 - val_loss: 1.4086e-04 - 13s/epoch - 151us/sample
Epoch 40/80
84077/84077 - 13s - loss: 1.5325e-04 - val_loss: 1.3881e-04 - 13s/epoch - 151us/sample
Epoch 41/80
84077/84077 - 13s - loss: 1.5206e-04 - val_loss: 1.3875e-04 - 13s/epoch - 151us/sample
Epoch 42/80
84077/84077 - 13s - loss: 1.5116e-04 - val_loss: 1.3887e-04 - 13s/epoch - 151us/sample
Epoch 43/80
84077/84077 - 13s - loss: 1.5010e-04 - val_loss: 1.3648e-04 - 13s/epoch - 152us/sample
Epoch 44/80
84077/84077 - 13s - loss: 1.5001e-04 - val_loss: 1.3700e-04 - 13s/epoch - 153us/sample
Epoch 45/80
84077/84077 - 13s - loss: 1.4892e-04 - val_loss: 1.3640e-04 - 13s/epoch - 151us/sample
Epoch 46/80
84077/84077 - 13s - loss: 1.4819e-04 - val_loss: 1.3718e-04 - 13s/epoch - 151us/sample
Epoch 47/80
84077/84077 - 13s - loss: 1.4809e-04 - val_loss: 1.3625e-04 - 13s/epoch - 151us/sample
Epoch 48/80
84077/84077 - 13s - loss: 1.4721e-04 - val_loss: 1.3530e-04 - 13s/epoch - 151us/sample
Epoch 49/80
84077/84077 - 13s - loss: 1.4662e-04 - val_loss: 1.3634e-04 - 13s/epoch - 153us/sample
Epoch 50/80
84077/84077 - 13s - loss: 1.4614e-04 - val_loss: 1.3442e-04 - 13s/epoch - 152us/sample
Epoch 51/80
84077/84077 - 13s - loss: 1.4530e-04 - val_loss: 1.3379e-04 - 13s/epoch - 151us/sample
Epoch 52/80
84077/84077 - 13s - loss: 1.4534e-04 - val_loss: 1.3353e-04 - 13s/epoch - 151us/sample
Epoch 53/80
84077/84077 - 13s - loss: 1.4490e-04 - val_loss: 1.3404e-04 - 13s/epoch - 151us/sample
Epoch 54/80
84077/84077 - 13s - loss: 1.4483e-04 - val_loss: 1.3358e-04 - 13s/epoch - 152us/sample
Epoch 55/80
84077/84077 - 13s - loss: 1.4398e-04 - val_loss: 1.3269e-04 - 13s/epoch - 153us/sample
Epoch 56/80
84077/84077 - 13s - loss: 1.4369e-04 - val_loss: 1.3268e-04 - 13s/epoch - 151us/sample
Epoch 57/80
84077/84077 - 13s - loss: 1.4332e-04 - val_loss: 1.3228e-04 - 13s/epoch - 151us/sample
Epoch 58/80
84077/84077 - 13s - loss: 1.4293e-04 - val_loss: 1.3212e-04 - 13s/epoch - 151us/sample
Epoch 59/80
84077/84077 - 13s - loss: 1.4303e-04 - val_loss: 1.3053e-04 - 13s/epoch - 151us/sample
Epoch 60/80
84077/84077 - 13s - loss: 1.4210e-04 - val_loss: 1.3183e-04 - 13s/epoch - 153us/sample
Epoch 61/80
84077/84077 - 13s - loss: 1.4224e-04 - val_loss: 1.3097e-04 - 13s/epoch - 151us/sample
Epoch 62/80
84077/84077 - 13s - loss: 1.4170e-04 - val_loss: 1.3248e-04 - 13s/epoch - 151us/sample
Epoch 63/80
84077/84077 - 13s - loss: 1.4129e-04 - val_loss: 1.3153e-04 - 13s/epoch - 151us/sample
Epoch 64/80
84077/84077 - 13s - loss: 1.4197e-04 - val_loss: 1.3118e-04 - 13s/epoch - 151us/sample
Epoch 65/80
84077/84077 - 13s - loss: 1.4059e-04 - val_loss: 1.3089e-04 - 13s/epoch - 151us/sample
Epoch 66/80
84077/84077 - 13s - loss: 1.4074e-04 - val_loss: 1.2923e-04 - 13s/epoch - 154us/sample
Epoch 67/80
84077/84077 - 13s - loss: 1.4008e-04 - val_loss: 1.2974e-04 - 13s/epoch - 151us/sample
Epoch 68/80
84077/84077 - 13s - loss: 1.4048e-04 - val_loss: 1.2823e-04 - 13s/epoch - 151us/sample
Epoch 69/80
84077/84077 - 13s - loss: 1.3928e-04 - val_loss: 1.2878e-04 - 13s/epoch - 151us/sample
Epoch 70/80
84077/84077 - 13s - loss: 1.3963e-04 - val_loss: 1.2936e-04 - 13s/epoch - 151us/sample
Epoch 71/80
84077/84077 - 13s - loss: 1.3938e-04 - val_loss: 1.2978e-04 - 13s/epoch - 153us/sample
Epoch 72/80
84077/84077 - 13s - loss: 1.3891e-04 - val_loss: 1.2937e-04 - 13s/epoch - 152us/sample
Epoch 73/80
84077/84077 - 13s - loss: 1.3894e-04 - val_loss: 1.2931e-04 - 13s/epoch - 151us/sample
Epoch 74/80
84077/84077 - 13s - loss: 1.3817e-04 - val_loss: 1.3076e-04 - 13s/epoch - 151us/sample
Epoch 75/80
84077/84077 - 13s - loss: 1.3852e-04 - val_loss: 1.2883e-04 - 13s/epoch - 151us/sample
Epoch 76/80
84077/84077 - 13s - loss: 1.3809e-04 - val_loss: 1.2930e-04 - 13s/epoch - 152us/sample
Epoch 77/80
84077/84077 - 13s - loss: 1.3815e-04 - val_loss: 1.2846e-04 - 13s/epoch - 153us/sample
Epoch 78/80
84077/84077 - 13s - loss: 1.3778e-04 - val_loss: 1.2753e-04 - 13s/epoch - 151us/sample
Epoch 79/80
84077/84077 - 13s - loss: 1.3773e-04 - val_loss: 1.2874e-04 - 13s/epoch - 151us/sample
Epoch 80/80
84077/84077 - 13s - loss: 1.3751e-04 - val_loss: 1.2848e-04 - 13s/epoch - 151us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012847834271993415
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 05:09:50.733169: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_67/outputlayer/BiasAdd' id:75035 op device:{requested: '', assigned: ''} def:{{{node decoder_model_67/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_67/outputlayer/MatMul, decoder_model_67/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.032104132554127716
cosine 0.03170982965775997
MAE: 0.0025765234467705154
RMSE: 0.010287902919855577
r2: 0.9174887146343885
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'logcosh', 64, 80, 0.00020000000000000006, 0.2, 188, 0.00013751227854566115, 0.00012847834271993415, 0.032104132554127716, 0.03170982965775997, 0.0025765234467705154, 0.010287902919855577, 0.9174887146343885, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.001 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_174 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_174 (ReLU)               (None, 1791)         0           ['batch_normalization_174[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_174[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_174[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 05:10:18.930336: W tensorflow/c/c_api.cc:291] Operation '{name:'training_96/Adam/learning_rate/Assign' id:76795 op device:{requested: '', assigned: ''} def:{{{node training_96/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_96/Adam/learning_rate, training_96/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 05:11:21.632182: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_68/mul' id:76349 op device:{requested: '', assigned: ''} def:{{{node loss_68/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_68/mul/x, loss_68/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 77s - loss: 0.0042 - val_loss: 0.0016 - 77s/epoch - 913us/sample
Epoch 2/85
84077/84077 - 59s - loss: 0.0013 - val_loss: 0.0012 - 59s/epoch - 696us/sample
Epoch 3/85
84077/84077 - 59s - loss: 0.0011 - val_loss: 0.0012 - 59s/epoch - 697us/sample
Epoch 4/85
84077/84077 - 59s - loss: 0.0010 - val_loss: 0.0017 - 59s/epoch - 697us/sample
Epoch 5/85
84077/84077 - 59s - loss: 9.4569e-04 - val_loss: 0.0042 - 59s/epoch - 697us/sample
Epoch 6/85
84077/84077 - 58s - loss: 8.8664e-04 - val_loss: 0.0110 - 58s/epoch - 694us/sample
Epoch 7/85
84077/84077 - 59s - loss: 8.3915e-04 - val_loss: 0.0223 - 59s/epoch - 697us/sample
Epoch 8/85
84077/84077 - 59s - loss: 8.1482e-04 - val_loss: 0.0365 - 59s/epoch - 698us/sample
Epoch 9/85
84077/84077 - 59s - loss: 8.0070e-04 - val_loss: 0.0515 - 59s/epoch - 699us/sample
Epoch 10/85
84077/84077 - 58s - loss: 7.9169e-04 - val_loss: 0.0593 - 58s/epoch - 695us/sample
Epoch 11/85
84077/84077 - 58s - loss: 7.8408e-04 - val_loss: 0.0668 - 58s/epoch - 696us/sample
Epoch 12/85
84077/84077 - 59s - loss: 7.7657e-04 - val_loss: 0.0642 - 59s/epoch - 698us/sample
Epoch 13/85
84077/84077 - 59s - loss: 7.7326e-04 - val_loss: 0.0944 - 59s/epoch - 697us/sample
Epoch 14/85
84077/84077 - 59s - loss: 7.6860e-04 - val_loss: 0.1151 - 59s/epoch - 697us/sample
Epoch 15/85
84077/84077 - 59s - loss: 7.6485e-04 - val_loss: 0.0972 - 59s/epoch - 697us/sample
Epoch 16/85
84077/84077 - 59s - loss: 7.5916e-04 - val_loss: 0.1001 - 59s/epoch - 698us/sample
Epoch 17/85
84077/84077 - 59s - loss: 7.5626e-04 - val_loss: 0.0807 - 59s/epoch - 698us/sample
Epoch 18/85
84077/84077 - 59s - loss: 7.5114e-04 - val_loss: 0.0864 - 59s/epoch - 697us/sample
Epoch 19/85
84077/84077 - 58s - loss: 7.4773e-04 - val_loss: 0.1174 - 58s/epoch - 694us/sample
Epoch 20/85
84077/84077 - 59s - loss: 7.4513e-04 - val_loss: 0.0944 - 59s/epoch - 697us/sample
Epoch 21/85
84077/84077 - 59s - loss: 7.4292e-04 - val_loss: 0.0954 - 59s/epoch - 698us/sample
Epoch 22/85
84077/84077 - 59s - loss: 7.4102e-04 - val_loss: 0.1128 - 59s/epoch - 698us/sample
Epoch 23/85
84077/84077 - 59s - loss: 7.3773e-04 - val_loss: 0.0951 - 59s/epoch - 697us/sample
Epoch 24/85
84077/84077 - 58s - loss: 7.3621e-04 - val_loss: 0.1034 - 58s/epoch - 696us/sample
Epoch 25/85
84077/84077 - 59s - loss: 7.3428e-04 - val_loss: 0.1463 - 59s/epoch - 697us/sample
Epoch 26/85
84077/84077 - 59s - loss: 7.3221e-04 - val_loss: 0.1321 - 59s/epoch - 697us/sample
Epoch 27/85
84077/84077 - 59s - loss: 7.3054e-04 - val_loss: 0.1301 - 59s/epoch - 698us/sample
Epoch 28/85
84077/84077 - 59s - loss: 7.2919e-04 - val_loss: 0.1232 - 59s/epoch - 696us/sample
Epoch 29/85
84077/84077 - 59s - loss: 7.2738e-04 - val_loss: 0.1152 - 59s/epoch - 698us/sample
Epoch 30/85
84077/84077 - 59s - loss: 7.2377e-04 - val_loss: 0.1166 - 59s/epoch - 698us/sample
Epoch 31/85
84077/84077 - 59s - loss: 7.2148e-04 - val_loss: 0.1201 - 59s/epoch - 697us/sample
Epoch 32/85
84077/84077 - 58s - loss: 7.1857e-04 - val_loss: 0.1091 - 58s/epoch - 695us/sample
Epoch 33/85
84077/84077 - 59s - loss: 7.1873e-04 - val_loss: 0.1107 - 59s/epoch - 697us/sample
Epoch 34/85
84077/84077 - 58s - loss: 7.1613e-04 - val_loss: 0.1007 - 58s/epoch - 696us/sample
Epoch 35/85
84077/84077 - 59s - loss: 7.1633e-04 - val_loss: 0.1118 - 59s/epoch - 697us/sample
Epoch 36/85
84077/84077 - 58s - loss: 7.1474e-04 - val_loss: 0.0891 - 58s/epoch - 695us/sample
Epoch 37/85
84077/84077 - 59s - loss: 7.1444e-04 - val_loss: 0.1008 - 59s/epoch - 697us/sample
Epoch 38/85
84077/84077 - 59s - loss: 7.1361e-04 - val_loss: 0.0795 - 59s/epoch - 698us/sample
Epoch 39/85
84077/84077 - 59s - loss: 7.1333e-04 - val_loss: 0.0913 - 59s/epoch - 697us/sample
Epoch 40/85
84077/84077 - 59s - loss: 7.1326e-04 - val_loss: 0.0834 - 59s/epoch - 698us/sample
Epoch 41/85
84077/84077 - 59s - loss: 7.1106e-04 - val_loss: 0.0641 - 59s/epoch - 699us/sample
Epoch 42/85
84077/84077 - 58s - loss: 7.1074e-04 - val_loss: 0.0796 - 58s/epoch - 694us/sample
Epoch 43/85
84077/84077 - 59s - loss: 7.1047e-04 - val_loss: 0.0759 - 59s/epoch - 696us/sample
Epoch 44/85
84077/84077 - 59s - loss: 7.1020e-04 - val_loss: 0.0649 - 59s/epoch - 696us/sample
Epoch 45/85
84077/84077 - 59s - loss: 7.0810e-04 - val_loss: 0.0679 - 59s/epoch - 697us/sample
Epoch 46/85
84077/84077 - 59s - loss: 7.0706e-04 - val_loss: 0.0560 - 59s/epoch - 698us/sample
Epoch 47/85
84077/84077 - 58s - loss: 7.0768e-04 - val_loss: 0.0738 - 58s/epoch - 695us/sample
Epoch 48/85
84077/84077 - 59s - loss: 7.0598e-04 - val_loss: 0.0719 - 59s/epoch - 697us/sample
Epoch 49/85
84077/84077 - 59s - loss: 7.0698e-04 - val_loss: 0.0538 - 59s/epoch - 697us/sample
Epoch 50/85
84077/84077 - 59s - loss: 7.0592e-04 - val_loss: 0.0567 - 59s/epoch - 698us/sample
Epoch 51/85
84077/84077 - 58s - loss: 7.0583e-04 - val_loss: 0.0468 - 58s/epoch - 695us/sample
Epoch 52/85
84077/84077 - 59s - loss: 7.0484e-04 - val_loss: 0.0563 - 59s/epoch - 699us/sample
Epoch 53/85
84077/84077 - 59s - loss: 7.0337e-04 - val_loss: 0.0612 - 59s/epoch - 697us/sample
Epoch 54/85
84077/84077 - 59s - loss: 7.0180e-04 - val_loss: 0.0446 - 59s/epoch - 698us/sample
Epoch 55/85
84077/84077 - 58s - loss: 6.9929e-04 - val_loss: 0.0511 - 58s/epoch - 696us/sample
Epoch 56/85
84077/84077 - 59s - loss: 6.9873e-04 - val_loss: 0.0500 - 59s/epoch - 699us/sample
Epoch 57/85
84077/84077 - 59s - loss: 6.9730e-04 - val_loss: 0.0494 - 59s/epoch - 699us/sample
Epoch 58/85
84077/84077 - 59s - loss: 6.9518e-04 - val_loss: 0.0545 - 59s/epoch - 696us/sample
Epoch 59/85
84077/84077 - 60s - loss: 6.9671e-04 - val_loss: 0.0585 - 60s/epoch - 717us/sample
Epoch 60/85
84077/84077 - 58s - loss: 6.9544e-04 - val_loss: 0.0476 - 58s/epoch - 695us/sample
Epoch 61/85
84077/84077 - 59s - loss: 6.9512e-04 - val_loss: 0.0465 - 59s/epoch - 696us/sample
Epoch 62/85
84077/84077 - 59s - loss: 6.9491e-04 - val_loss: 0.0429 - 59s/epoch - 698us/sample
Epoch 63/85
84077/84077 - 59s - loss: 6.9455e-04 - val_loss: 0.0485 - 59s/epoch - 697us/sample
Epoch 64/85
84077/84077 - 59s - loss: 6.9368e-04 - val_loss: 0.0392 - 59s/epoch - 696us/sample
Epoch 65/85
84077/84077 - 59s - loss: 6.9311e-04 - val_loss: 0.0526 - 59s/epoch - 697us/sample
Epoch 66/85
84077/84077 - 59s - loss: 6.9358e-04 - val_loss: 0.0474 - 59s/epoch - 696us/sample
Epoch 67/85
84077/84077 - 59s - loss: 6.9250e-04 - val_loss: 0.0386 - 59s/epoch - 697us/sample
Epoch 68/85
84077/84077 - 59s - loss: 6.9235e-04 - val_loss: 0.0383 - 59s/epoch - 697us/sample
Epoch 69/85
84077/84077 - 59s - loss: 6.9162e-04 - val_loss: 0.0388 - 59s/epoch - 697us/sample
Epoch 70/85
84077/84077 - 59s - loss: 6.9097e-04 - val_loss: 0.0305 - 59s/epoch - 697us/sample
Epoch 71/85
84077/84077 - 59s - loss: 6.9203e-04 - val_loss: 0.0276 - 59s/epoch - 699us/sample
Epoch 72/85
84077/84077 - 59s - loss: 6.9158e-04 - val_loss: 0.0398 - 59s/epoch - 697us/sample
Epoch 73/85
84077/84077 - 58s - loss: 6.9025e-04 - val_loss: 0.0345 - 58s/epoch - 695us/sample
Epoch 74/85
84077/84077 - 59s - loss: 6.9117e-04 - val_loss: 0.0303 - 59s/epoch - 697us/sample
Epoch 75/85
84077/84077 - 59s - loss: 6.8980e-04 - val_loss: 0.0254 - 59s/epoch - 697us/sample
Epoch 76/85
84077/84077 - 59s - loss: 6.8852e-04 - val_loss: 0.0279 - 59s/epoch - 696us/sample
Epoch 77/85
84077/84077 - 58s - loss: 6.8850e-04 - val_loss: 0.0324 - 58s/epoch - 696us/sample
Epoch 78/85
84077/84077 - 58s - loss: 6.8824e-04 - val_loss: 0.0280 - 58s/epoch - 695us/sample
Epoch 79/85
84077/84077 - 59s - loss: 6.8822e-04 - val_loss: 0.0322 - 59s/epoch - 697us/sample
Epoch 80/85
84077/84077 - 59s - loss: 6.8845e-04 - val_loss: 0.0320 - 59s/epoch - 697us/sample
Epoch 81/85
84077/84077 - 58s - loss: 6.8748e-04 - val_loss: 0.0244 - 58s/epoch - 695us/sample
Epoch 82/85
84077/84077 - 59s - loss: 6.8698e-04 - val_loss: 0.0294 - 59s/epoch - 697us/sample
Epoch 83/85
84077/84077 - 58s - loss: 6.8778e-04 - val_loss: 0.0319 - 58s/epoch - 696us/sample
Epoch 84/85
84077/84077 - 59s - loss: 6.8631e-04 - val_loss: 0.0253 - 59s/epoch - 698us/sample
Epoch 85/85
84077/84077 - 58s - loss: 6.8689e-04 - val_loss: 0.0327 - 58s/epoch - 694us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.032736295076350105
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 06:33:33.263645: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_68/outputlayer/BiasAdd' id:76320 op device:{requested: '', assigned: ''} def:{{{node decoder_model_68/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_68/outputlayer/MatMul, decoder_model_68/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2027198950410379
cosine 0.200368131914323
MAE: 0.01518065223558062
RMSE: 0.1800281286837852
r2: -24.317539517918416
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 8, 85, 0.001, 0.2, 188, 0.0006868921774249792, 0.032736295076350105, 0.2027198950410379, 0.200368131914323, 0.01518065223558062, 0.1800281286837852, -24.317539517918416, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 6
Fitness    = 511.49230331698664
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 511.49230331698664.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664]
[2.0 80 0.001 64 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_177 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_177 (ReLU)               (None, 1886)         0           ['batch_normalization_177[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_177[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_177[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-13 06:34:01.681599: W tensorflow/c/c_api.cc:291] Operation '{name:'training_98/Adam/iter/Assign' id:78060 op device:{requested: '', assigned: ''} def:{{{node training_98/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_98/Adam/iter, training_98/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 06:34:20.697787: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_69/mul' id:77611 op device:{requested: '', assigned: ''} def:{{{node loss_69/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_69/mul/x, loss_69/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 30s - loss: 0.0043 - val_loss: 0.0011 - 30s/epoch - 361us/sample
Epoch 2/80
84077/84077 - 13s - loss: 0.0035 - val_loss: 0.0016 - 13s/epoch - 154us/sample
Epoch 3/80
84077/84077 - 13s - loss: 8.2315e-04 - val_loss: 5.6914e-04 - 13s/epoch - 153us/sample
Epoch 4/80
84077/84077 - 13s - loss: 7.5968e-04 - val_loss: 8.1899e-04 - 13s/epoch - 153us/sample
Epoch 5/80
84077/84077 - 13s - loss: 6.0996e-04 - val_loss: 4.9630e-04 - 13s/epoch - 153us/sample
Epoch 6/80
84077/84077 - 13s - loss: 4.4472e-04 - val_loss: 4.0137e-04 - 13s/epoch - 153us/sample
Epoch 7/80
84077/84077 - 13s - loss: 4.1076e-04 - val_loss: 3.8510e-04 - 13s/epoch - 153us/sample
Epoch 8/80
84077/84077 - 13s - loss: 3.7904e-04 - val_loss: 3.1505e-04 - 13s/epoch - 154us/sample
Epoch 9/80
84077/84077 - 13s - loss: 3.3293e-04 - val_loss: 2.9115e-04 - 13s/epoch - 153us/sample
Epoch 10/80
84077/84077 - 13s - loss: 3.2277e-04 - val_loss: 2.6603e-04 - 13s/epoch - 153us/sample
Epoch 11/80
84077/84077 - 13s - loss: 2.7926e-04 - val_loss: 2.4362e-04 - 13s/epoch - 153us/sample
Epoch 12/80
84077/84077 - 13s - loss: 2.6098e-04 - val_loss: 2.3006e-04 - 13s/epoch - 153us/sample
Epoch 13/80
84077/84077 - 13s - loss: 2.4177e-04 - val_loss: 2.1420e-04 - 13s/epoch - 155us/sample
Epoch 14/80
84077/84077 - 13s - loss: 2.2448e-04 - val_loss: 1.9950e-04 - 13s/epoch - 153us/sample
Epoch 15/80
84077/84077 - 13s - loss: 2.1334e-04 - val_loss: 1.8456e-04 - 13s/epoch - 153us/sample
Epoch 16/80
84077/84077 - 13s - loss: 2.0474e-04 - val_loss: 1.8268e-04 - 13s/epoch - 152us/sample
Epoch 17/80
84077/84077 - 13s - loss: 1.9489e-04 - val_loss: 1.7060e-04 - 13s/epoch - 153us/sample
Epoch 18/80
84077/84077 - 13s - loss: 1.8758e-04 - val_loss: 1.6507e-04 - 13s/epoch - 154us/sample
Epoch 19/80
84077/84077 - 13s - loss: 1.8228e-04 - val_loss: 1.6146e-04 - 13s/epoch - 153us/sample
Epoch 20/80
84077/84077 - 13s - loss: 1.7739e-04 - val_loss: 1.5786e-04 - 13s/epoch - 153us/sample
Epoch 21/80
84077/84077 - 13s - loss: 1.7369e-04 - val_loss: 1.5698e-04 - 13s/epoch - 153us/sample
Epoch 22/80
84077/84077 - 13s - loss: 1.7073e-04 - val_loss: 1.5293e-04 - 13s/epoch - 153us/sample
Epoch 23/80
84077/84077 - 13s - loss: 1.6957e-04 - val_loss: 1.5109e-04 - 13s/epoch - 153us/sample
Epoch 24/80
84077/84077 - 13s - loss: 1.6549e-04 - val_loss: 1.4769e-04 - 13s/epoch - 155us/sample
Epoch 25/80
84077/84077 - 13s - loss: 1.6390e-04 - val_loss: 1.4714e-04 - 13s/epoch - 153us/sample
Epoch 26/80
84077/84077 - 13s - loss: 1.6212e-04 - val_loss: 1.4658e-04 - 13s/epoch - 152us/sample
Epoch 27/80
84077/84077 - 13s - loss: 1.6004e-04 - val_loss: 1.4585e-04 - 13s/epoch - 153us/sample
Epoch 28/80
84077/84077 - 13s - loss: 1.5836e-04 - val_loss: 1.4387e-04 - 13s/epoch - 153us/sample
Epoch 29/80
84077/84077 - 13s - loss: 1.5704e-04 - val_loss: 1.4214e-04 - 13s/epoch - 155us/sample
Epoch 30/80
84077/84077 - 13s - loss: 1.5614e-04 - val_loss: 1.4303e-04 - 13s/epoch - 153us/sample
Epoch 31/80
84077/84077 - 13s - loss: 1.5438e-04 - val_loss: 1.4062e-04 - 13s/epoch - 153us/sample
Epoch 32/80
84077/84077 - 13s - loss: 1.5313e-04 - val_loss: 1.4227e-04 - 13s/epoch - 152us/sample
Epoch 33/80
84077/84077 - 13s - loss: 1.5254e-04 - val_loss: 1.3874e-04 - 13s/epoch - 153us/sample
Epoch 34/80
84077/84077 - 13s - loss: 1.5118e-04 - val_loss: 1.3793e-04 - 13s/epoch - 154us/sample
Epoch 35/80
84077/84077 - 13s - loss: 1.5031e-04 - val_loss: 1.3752e-04 - 13s/epoch - 154us/sample
Epoch 36/80
84077/84077 - 13s - loss: 1.5002e-04 - val_loss: 1.3751e-04 - 13s/epoch - 153us/sample
Epoch 37/80
84077/84077 - 13s - loss: 1.4908e-04 - val_loss: 1.3601e-04 - 13s/epoch - 153us/sample
Epoch 38/80
84077/84077 - 13s - loss: 1.4778e-04 - val_loss: 1.3615e-04 - 13s/epoch - 153us/sample
Epoch 39/80
84077/84077 - 13s - loss: 1.4738e-04 - val_loss: 1.3521e-04 - 13s/epoch - 153us/sample
Epoch 40/80
84077/84077 - 13s - loss: 1.4660e-04 - val_loss: 1.3584e-04 - 13s/epoch - 155us/sample
Epoch 41/80
84077/84077 - 13s - loss: 1.4542e-04 - val_loss: 1.3492e-04 - 13s/epoch - 153us/sample
Epoch 42/80
84077/84077 - 13s - loss: 1.4521e-04 - val_loss: 1.3473e-04 - 13s/epoch - 152us/sample
Epoch 43/80
84077/84077 - 13s - loss: 1.4464e-04 - val_loss: 1.3390e-04 - 13s/epoch - 153us/sample
Epoch 44/80
84077/84077 - 13s - loss: 1.4420e-04 - val_loss: 1.3344e-04 - 13s/epoch - 153us/sample
Epoch 45/80
84077/84077 - 13s - loss: 1.4355e-04 - val_loss: 1.3336e-04 - 13s/epoch - 155us/sample
Epoch 46/80
84077/84077 - 13s - loss: 1.4312e-04 - val_loss: 1.3130e-04 - 13s/epoch - 153us/sample
Epoch 47/80
84077/84077 - 13s - loss: 1.4305e-04 - val_loss: 1.3212e-04 - 13s/epoch - 153us/sample
Epoch 48/80
84077/84077 - 13s - loss: 1.4299e-04 - val_loss: 1.3315e-04 - 13s/epoch - 153us/sample
Epoch 49/80
84077/84077 - 13s - loss: 1.4210e-04 - val_loss: 1.3161e-04 - 13s/epoch - 152us/sample
Epoch 50/80
84077/84077 - 13s - loss: 1.4145e-04 - val_loss: 1.3213e-04 - 13s/epoch - 154us/sample
Epoch 51/80
84077/84077 - 13s - loss: 1.4111e-04 - val_loss: 1.3018e-04 - 13s/epoch - 155us/sample
Epoch 52/80
84077/84077 - 13s - loss: 1.4069e-04 - val_loss: 1.3169e-04 - 13s/epoch - 153us/sample
Epoch 53/80
84077/84077 - 13s - loss: 1.4093e-04 - val_loss: 1.3023e-04 - 13s/epoch - 153us/sample
Epoch 54/80
84077/84077 - 13s - loss: 1.4035e-04 - val_loss: 1.3005e-04 - 13s/epoch - 153us/sample
Epoch 55/80
84077/84077 - 13s - loss: 1.3979e-04 - val_loss: 1.3083e-04 - 13s/epoch - 153us/sample
Epoch 56/80
84077/84077 - 13s - loss: 1.3917e-04 - val_loss: 1.3075e-04 - 13s/epoch - 155us/sample
Epoch 57/80
84077/84077 - 13s - loss: 1.3921e-04 - val_loss: 1.3025e-04 - 13s/epoch - 153us/sample
Epoch 58/80
84077/84077 - 13s - loss: 1.3860e-04 - val_loss: 1.2947e-04 - 13s/epoch - 153us/sample
Epoch 59/80
84077/84077 - 13s - loss: 1.3829e-04 - val_loss: 1.2952e-04 - 13s/epoch - 153us/sample
Epoch 60/80
84077/84077 - 13s - loss: 1.3810e-04 - val_loss: 1.2916e-04 - 13s/epoch - 152us/sample
Epoch 61/80
84077/84077 - 13s - loss: 1.3805e-04 - val_loss: 1.3000e-04 - 13s/epoch - 154us/sample
Epoch 62/80
84077/84077 - 13s - loss: 1.3804e-04 - val_loss: 1.2804e-04 - 13s/epoch - 154us/sample
Epoch 63/80
84077/84077 - 13s - loss: 1.3710e-04 - val_loss: 1.2942e-04 - 13s/epoch - 153us/sample
Epoch 64/80
84077/84077 - 13s - loss: 1.3703e-04 - val_loss: 1.2981e-04 - 13s/epoch - 153us/sample
Epoch 65/80
84077/84077 - 13s - loss: 1.3685e-04 - val_loss: 1.2812e-04 - 13s/epoch - 153us/sample
Epoch 66/80
84077/84077 - 13s - loss: 1.3658e-04 - val_loss: 1.2870e-04 - 13s/epoch - 153us/sample
Epoch 67/80
84077/84077 - 13s - loss: 1.3644e-04 - val_loss: 1.2829e-04 - 13s/epoch - 155us/sample
Epoch 68/80
84077/84077 - 13s - loss: 1.3628e-04 - val_loss: 1.2773e-04 - 13s/epoch - 153us/sample
Epoch 69/80
84077/84077 - 13s - loss: 1.3609e-04 - val_loss: 1.2793e-04 - 13s/epoch - 153us/sample
Epoch 70/80
84077/84077 - 13s - loss: 1.3545e-04 - val_loss: 1.2670e-04 - 13s/epoch - 153us/sample
Epoch 71/80
84077/84077 - 13s - loss: 1.3539e-04 - val_loss: 1.2795e-04 - 13s/epoch - 153us/sample
Epoch 72/80
84077/84077 - 13s - loss: 1.3519e-04 - val_loss: 1.2771e-04 - 13s/epoch - 155us/sample
Epoch 73/80
84077/84077 - 13s - loss: 1.3484e-04 - val_loss: 1.2713e-04 - 13s/epoch - 153us/sample
Epoch 74/80
84077/84077 - 13s - loss: 1.3483e-04 - val_loss: 1.2650e-04 - 13s/epoch - 153us/sample
Epoch 75/80
84077/84077 - 13s - loss: 1.3480e-04 - val_loss: 1.2728e-04 - 13s/epoch - 153us/sample
Epoch 76/80
84077/84077 - 13s - loss: 1.3424e-04 - val_loss: 1.2604e-04 - 13s/epoch - 153us/sample
Epoch 77/80
84077/84077 - 13s - loss: 1.3405e-04 - val_loss: 1.2644e-04 - 13s/epoch - 153us/sample
Epoch 78/80
84077/84077 - 13s - loss: 1.3353e-04 - val_loss: 1.2600e-04 - 13s/epoch - 154us/sample
Epoch 79/80
84077/84077 - 13s - loss: 1.3390e-04 - val_loss: 1.2578e-04 - 13s/epoch - 153us/sample
Epoch 80/80
84077/84077 - 13s - loss: 1.3348e-04 - val_loss: 1.2624e-04 - 13s/epoch - 153us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.000126240509866068
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 06:51:24.228495: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_69/outputlayer/BiasAdd' id:77575 op device:{requested: '', assigned: ''} def:{{{node decoder_model_69/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_69/outputlayer/MatMul, decoder_model_69/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030509070653772914
cosine 0.03013381147186769
MAE: 0.002418926069744389
RMSE: 0.010006814728182841
r2: 0.9220375930321922
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 80, 0.001, 0.2, 188, 0.00013347594696912899, 0.000126240509866068, 0.030509070653772914, 0.03013381147186769, 0.002418926069744389, 0.010006814728182841, 0.9220375930321922, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5999999999999996 80 0.001 32 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1508)         1423552     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_180 (Batch  (None, 1508)        6032        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_180 (ReLU)               (None, 1508)         0           ['batch_normalization_180[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          283692      ['re_lu_180[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          283692      ['re_lu_180[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1750315     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,747,283
Trainable params: 3,740,875
Non-trainable params: 6,408
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-13 06:51:56.659686: W tensorflow/c/c_api.cc:291] Operation '{name:'training_100/Adam/batch_normalization_181/gamma/v/Assign' id:79551 op device:{requested: '', assigned: ''} def:{{{node training_100/Adam/batch_normalization_181/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_100/Adam/batch_normalization_181/gamma/v, training_100/Adam/batch_normalization_181/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 06:52:24.713862: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_70/mul' id:78896 op device:{requested: '', assigned: ''} def:{{{node loss_70/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_70/mul/x, loss_70/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 42s - loss: 0.0031 - val_loss: 9.3776e-04 - 42s/epoch - 500us/sample
Epoch 2/80
84077/84077 - 21s - loss: 0.0011 - val_loss: 6.3487e-04 - 21s/epoch - 244us/sample
Epoch 3/80
84077/84077 - 21s - loss: 5.8391e-04 - val_loss: 5.0171e-04 - 21s/epoch - 245us/sample
Epoch 4/80
84077/84077 - 20s - loss: 5.0121e-04 - val_loss: 4.6413e-04 - 20s/epoch - 243us/sample
Epoch 5/80
84077/84077 - 20s - loss: 4.4153e-04 - val_loss: 3.7214e-04 - 20s/epoch - 243us/sample
Epoch 6/80
84077/84077 - 21s - loss: 3.8332e-04 - val_loss: 3.2448e-04 - 21s/epoch - 245us/sample
Epoch 7/80
84077/84077 - 20s - loss: 3.4462e-04 - val_loss: 2.9752e-04 - 20s/epoch - 244us/sample
Epoch 8/80
84077/84077 - 20s - loss: 3.1812e-04 - val_loss: 2.7236e-04 - 20s/epoch - 244us/sample
Epoch 9/80
84077/84077 - 20s - loss: 2.9820e-04 - val_loss: 2.5865e-04 - 20s/epoch - 243us/sample
Epoch 10/80
84077/84077 - 21s - loss: 2.8388e-04 - val_loss: 2.4691e-04 - 21s/epoch - 245us/sample
Epoch 11/80
84077/84077 - 20s - loss: 2.7257e-04 - val_loss: 2.4047e-04 - 20s/epoch - 244us/sample
Epoch 12/80
84077/84077 - 20s - loss: 2.6466e-04 - val_loss: 2.3590e-04 - 20s/epoch - 244us/sample
Epoch 13/80
84077/84077 - 21s - loss: 2.5796e-04 - val_loss: 2.3139e-04 - 21s/epoch - 245us/sample
Epoch 14/80
84077/84077 - 21s - loss: 2.5100e-04 - val_loss: 2.2225e-04 - 21s/epoch - 244us/sample
Epoch 15/80
84077/84077 - 20s - loss: 2.4448e-04 - val_loss: 2.2073e-04 - 20s/epoch - 243us/sample
Epoch 16/80
84077/84077 - 20s - loss: 2.3799e-04 - val_loss: 2.1559e-04 - 20s/epoch - 243us/sample
Epoch 17/80
84077/84077 - 21s - loss: 2.3316e-04 - val_loss: 2.1314e-04 - 21s/epoch - 245us/sample
Epoch 18/80
84077/84077 - 21s - loss: 2.3040e-04 - val_loss: 2.0882e-04 - 21s/epoch - 244us/sample
Epoch 19/80
84077/84077 - 20s - loss: 2.2662e-04 - val_loss: 2.0731e-04 - 20s/epoch - 244us/sample
Epoch 20/80
84077/84077 - 20s - loss: 2.2349e-04 - val_loss: 2.0662e-04 - 20s/epoch - 243us/sample
Epoch 21/80
84077/84077 - 21s - loss: 2.2032e-04 - val_loss: 2.0486e-04 - 21s/epoch - 246us/sample
Epoch 22/80
84077/84077 - 20s - loss: 2.1778e-04 - val_loss: 2.0521e-04 - 20s/epoch - 243us/sample
Epoch 23/80
84077/84077 - 21s - loss: 2.1541e-04 - val_loss: 1.9902e-04 - 21s/epoch - 244us/sample
Epoch 24/80
84077/84077 - 21s - loss: 2.1237e-04 - val_loss: 1.9820e-04 - 21s/epoch - 244us/sample
Epoch 25/80
84077/84077 - 21s - loss: 2.1003e-04 - val_loss: 1.9655e-04 - 21s/epoch - 245us/sample
Epoch 26/80
84077/84077 - 20s - loss: 2.0934e-04 - val_loss: 1.9355e-04 - 20s/epoch - 244us/sample
Epoch 27/80
84077/84077 - 21s - loss: 2.0680e-04 - val_loss: 1.9396e-04 - 21s/epoch - 244us/sample
Epoch 28/80
84077/84077 - 21s - loss: 2.0456e-04 - val_loss: 1.9052e-04 - 21s/epoch - 245us/sample
Epoch 29/80
84077/84077 - 21s - loss: 2.0294e-04 - val_loss: 1.9128e-04 - 21s/epoch - 244us/sample
Epoch 30/80
84077/84077 - 20s - loss: 2.0115e-04 - val_loss: 1.8692e-04 - 20s/epoch - 244us/sample
Epoch 31/80
84077/84077 - 20s - loss: 1.9888e-04 - val_loss: 1.8655e-04 - 20s/epoch - 243us/sample
Epoch 32/80
84077/84077 - 21s - loss: 1.9733e-04 - val_loss: 1.8552e-04 - 21s/epoch - 246us/sample
Epoch 33/80
84077/84077 - 20s - loss: 1.9605e-04 - val_loss: 1.8552e-04 - 20s/epoch - 243us/sample
Epoch 34/80
84077/84077 - 20s - loss: 1.9418e-04 - val_loss: 1.8284e-04 - 20s/epoch - 243us/sample
Epoch 35/80
84077/84077 - 21s - loss: 1.9377e-04 - val_loss: 1.8224e-04 - 21s/epoch - 245us/sample
Epoch 36/80
84077/84077 - 21s - loss: 1.9119e-04 - val_loss: 1.7952e-04 - 21s/epoch - 244us/sample
Epoch 37/80
84077/84077 - 20s - loss: 1.8946e-04 - val_loss: 1.7670e-04 - 20s/epoch - 244us/sample
Epoch 38/80
84077/84077 - 20s - loss: 1.8777e-04 - val_loss: 1.7551e-04 - 20s/epoch - 244us/sample
Epoch 39/80
84077/84077 - 21s - loss: 1.8686e-04 - val_loss: 1.7425e-04 - 21s/epoch - 246us/sample
Epoch 40/80
84077/84077 - 20s - loss: 1.8610e-04 - val_loss: 1.7508e-04 - 20s/epoch - 243us/sample
Epoch 41/80
84077/84077 - 20s - loss: 1.8451e-04 - val_loss: 1.7330e-04 - 20s/epoch - 243us/sample
Epoch 42/80
84077/84077 - 21s - loss: 1.8415e-04 - val_loss: 1.7583e-04 - 21s/epoch - 245us/sample
Epoch 43/80
84077/84077 - 21s - loss: 1.8356e-04 - val_loss: 1.7068e-04 - 21s/epoch - 244us/sample
Epoch 44/80
84077/84077 - 20s - loss: 1.8252e-04 - val_loss: 1.7012e-04 - 20s/epoch - 243us/sample
Epoch 45/80
84077/84077 - 20s - loss: 1.8170e-04 - val_loss: 1.7184e-04 - 20s/epoch - 243us/sample
Epoch 46/80
84077/84077 - 21s - loss: 1.8166e-04 - val_loss: 1.6992e-04 - 21s/epoch - 245us/sample
Epoch 47/80
84077/84077 - 20s - loss: 1.8064e-04 - val_loss: 1.6780e-04 - 20s/epoch - 243us/sample
Epoch 48/80
84077/84077 - 20s - loss: 1.7960e-04 - val_loss: 1.6697e-04 - 20s/epoch - 243us/sample
Epoch 49/80
84077/84077 - 21s - loss: 1.7884e-04 - val_loss: 1.6374e-04 - 21s/epoch - 244us/sample
Epoch 50/80
84077/84077 - 21s - loss: 1.7829e-04 - val_loss: 1.6435e-04 - 21s/epoch - 245us/sample
Epoch 51/80
84077/84077 - 20s - loss: 1.7760e-04 - val_loss: 1.6383e-04 - 20s/epoch - 243us/sample
Epoch 52/80
84077/84077 - 21s - loss: 1.7802e-04 - val_loss: 1.6550e-04 - 21s/epoch - 244us/sample
Epoch 53/80
84077/84077 - 21s - loss: 1.7687e-04 - val_loss: 1.6438e-04 - 21s/epoch - 246us/sample
Epoch 54/80
84077/84077 - 20s - loss: 1.7642e-04 - val_loss: 1.6234e-04 - 20s/epoch - 243us/sample
Epoch 55/80
84077/84077 - 20s - loss: 1.7612e-04 - val_loss: 1.6294e-04 - 20s/epoch - 244us/sample
Epoch 56/80
84077/84077 - 20s - loss: 1.7554e-04 - val_loss: 1.6265e-04 - 20s/epoch - 244us/sample
Epoch 57/80
84077/84077 - 21s - loss: 1.7409e-04 - val_loss: 1.6234e-04 - 21s/epoch - 246us/sample
Epoch 58/80
84077/84077 - 20s - loss: 1.7442e-04 - val_loss: 1.6044e-04 - 20s/epoch - 243us/sample
Epoch 59/80
84077/84077 - 20s - loss: 1.7392e-04 - val_loss: 1.6284e-04 - 20s/epoch - 243us/sample
Epoch 60/80
84077/84077 - 21s - loss: 1.7358e-04 - val_loss: 1.6035e-04 - 21s/epoch - 245us/sample
Epoch 61/80
84077/84077 - 21s - loss: 1.7267e-04 - val_loss: 1.5842e-04 - 21s/epoch - 244us/sample
Epoch 62/80
84077/84077 - 20s - loss: 1.7230e-04 - val_loss: 1.6139e-04 - 20s/epoch - 243us/sample
Epoch 63/80
84077/84077 - 20s - loss: 1.7203e-04 - val_loss: 1.6016e-04 - 20s/epoch - 243us/sample
Epoch 64/80
84077/84077 - 21s - loss: 1.7136e-04 - val_loss: 1.5779e-04 - 21s/epoch - 245us/sample
Epoch 65/80
84077/84077 - 20s - loss: 1.7130e-04 - val_loss: 1.5913e-04 - 20s/epoch - 244us/sample
Epoch 66/80
84077/84077 - 20s - loss: 1.7058e-04 - val_loss: 1.5656e-04 - 20s/epoch - 243us/sample
Epoch 67/80
84077/84077 - 20s - loss: 1.7109e-04 - val_loss: 1.5739e-04 - 20s/epoch - 244us/sample
Epoch 68/80
84077/84077 - 21s - loss: 1.7088e-04 - val_loss: 1.5768e-04 - 21s/epoch - 245us/sample
Epoch 69/80
84077/84077 - 20s - loss: 1.7013e-04 - val_loss: 1.5698e-04 - 20s/epoch - 243us/sample
Epoch 70/80
84077/84077 - 21s - loss: 1.6942e-04 - val_loss: 1.5559e-04 - 21s/epoch - 244us/sample
Epoch 71/80
84077/84077 - 21s - loss: 1.6901e-04 - val_loss: 1.5483e-04 - 21s/epoch - 245us/sample
Epoch 72/80
84077/84077 - 21s - loss: 1.6842e-04 - val_loss: 1.5575e-04 - 21s/epoch - 244us/sample
Epoch 73/80
84077/84077 - 20s - loss: 1.6811e-04 - val_loss: 1.5739e-04 - 20s/epoch - 244us/sample
Epoch 74/80
84077/84077 - 20s - loss: 1.6773e-04 - val_loss: 1.5486e-04 - 20s/epoch - 244us/sample
Epoch 75/80
84077/84077 - 21s - loss: 1.6775e-04 - val_loss: 1.5386e-04 - 21s/epoch - 245us/sample
Epoch 76/80
84077/84077 - 20s - loss: 1.6765e-04 - val_loss: 1.5419e-04 - 20s/epoch - 243us/sample
Epoch 77/80
84077/84077 - 20s - loss: 1.6677e-04 - val_loss: 1.5298e-04 - 20s/epoch - 243us/sample
Epoch 78/80
84077/84077 - 21s - loss: 1.6655e-04 - val_loss: 1.5257e-04 - 21s/epoch - 245us/sample
Epoch 79/80
84077/84077 - 21s - loss: 1.6679e-04 - val_loss: 1.5327e-04 - 21s/epoch - 244us/sample
Epoch 80/80
84077/84077 - 20s - loss: 1.6708e-04 - val_loss: 1.5369e-04 - 20s/epoch - 243us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00015368860292920252
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 07:19:33.416318: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_70/outputlayer/BiasAdd' id:78860 op device:{requested: '', assigned: ''} def:{{{node decoder_model_70/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_70/outputlayer/MatMul, decoder_model_70/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.043834258964460714
cosine 0.04330944009277192
MAE: 0.0025145960388503457
RMSE: 0.012524620113925793
r2: 0.8774914618153261
RMSE zero-vector: 0.04004287452915337
['1.5999999999999996custom_VAE', 'logcosh', 32, 80, 0.001, 0.2, 188, 0.0001670823697480593, 0.00015368860292920252, 0.043834258964460714, 0.04330944009277192, 0.0025145960388503457, 0.012524620113925793, 0.8774914618153261, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_183 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_183 (ReLU)               (None, 1980)         0           ['batch_normalization_183[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_183[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_183[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 07:20:10.758414: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_183/moving_variance/Assign' id:79783 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_183/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_183/moving_variance, batch_normalization_183/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 07:20:32.237729: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_71/mul' id:80177 op device:{requested: '', assigned: ''} def:{{{node loss_71/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_71/mul/x, loss_71/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 35s - loss: 0.0175 - val_loss: 0.0014 - 35s/epoch - 422us/sample
Epoch 2/90
84077/84077 - 13s - loss: 0.0023 - val_loss: 0.0019 - 13s/epoch - 160us/sample
Epoch 3/90
84077/84077 - 13s - loss: 0.0015 - val_loss: 0.0011 - 13s/epoch - 159us/sample
Epoch 4/90
84077/84077 - 13s - loss: 0.0037 - val_loss: 0.0016 - 13s/epoch - 158us/sample
Epoch 5/90
84077/84077 - 13s - loss: 0.0013 - val_loss: 9.0705e-04 - 13s/epoch - 159us/sample
Epoch 6/90
84077/84077 - 13s - loss: 8.7739e-04 - val_loss: 7.5381e-04 - 13s/epoch - 160us/sample
Epoch 7/90
84077/84077 - 13s - loss: 9.0325e-04 - val_loss: 6.8609e-04 - 13s/epoch - 160us/sample
Epoch 8/90
84077/84077 - 14s - loss: 7.1622e-04 - val_loss: 5.4472e-04 - 14s/epoch - 161us/sample
Epoch 9/90
84077/84077 - 13s - loss: 6.0889e-04 - val_loss: 4.8004e-04 - 13s/epoch - 160us/sample
Epoch 10/90
84077/84077 - 13s - loss: 5.5151e-04 - val_loss: 4.4233e-04 - 13s/epoch - 159us/sample
Epoch 11/90
84077/84077 - 13s - loss: 5.0614e-04 - val_loss: 3.7921e-04 - 13s/epoch - 160us/sample
Epoch 12/90
84077/84077 - 13s - loss: 4.3442e-04 - val_loss: 3.5632e-04 - 13s/epoch - 160us/sample
Epoch 13/90
84077/84077 - 14s - loss: 3.9674e-04 - val_loss: 3.3700e-04 - 14s/epoch - 161us/sample
Epoch 14/90
84077/84077 - 13s - loss: 3.5923e-04 - val_loss: 2.9737e-04 - 13s/epoch - 160us/sample
Epoch 15/90
84077/84077 - 13s - loss: 3.3705e-04 - val_loss: 2.9465e-04 - 13s/epoch - 159us/sample
Epoch 16/90
84077/84077 - 13s - loss: 3.1892e-04 - val_loss: 2.6910e-04 - 13s/epoch - 160us/sample
Epoch 17/90
84077/84077 - 13s - loss: 2.9914e-04 - val_loss: 2.6597e-04 - 13s/epoch - 160us/sample
Epoch 18/90
84077/84077 - 13s - loss: 2.9442e-04 - val_loss: 2.4991e-04 - 13s/epoch - 160us/sample
Epoch 19/90
84077/84077 - 13s - loss: 2.7743e-04 - val_loss: 2.4627e-04 - 13s/epoch - 161us/sample
Epoch 20/90
84077/84077 - 13s - loss: 2.6880e-04 - val_loss: 2.3505e-04 - 13s/epoch - 160us/sample
Epoch 21/90
84077/84077 - 13s - loss: 2.6169e-04 - val_loss: 2.3195e-04 - 13s/epoch - 160us/sample
Epoch 22/90
84077/84077 - 13s - loss: 2.5426e-04 - val_loss: 2.2688e-04 - 13s/epoch - 160us/sample
Epoch 23/90
84077/84077 - 13s - loss: 2.4958e-04 - val_loss: 2.2353e-04 - 13s/epoch - 160us/sample
Epoch 24/90
84077/84077 - 14s - loss: 2.4294e-04 - val_loss: 2.1857e-04 - 14s/epoch - 161us/sample
Epoch 25/90
84077/84077 - 13s - loss: 2.3864e-04 - val_loss: 2.1586e-04 - 13s/epoch - 159us/sample
Epoch 26/90
84077/84077 - 13s - loss: 2.3438e-04 - val_loss: 2.1358e-04 - 13s/epoch - 159us/sample
Epoch 27/90
84077/84077 - 13s - loss: 2.3072e-04 - val_loss: 2.1128e-04 - 13s/epoch - 160us/sample
Epoch 28/90
84077/84077 - 13s - loss: 2.2843e-04 - val_loss: 2.0697e-04 - 13s/epoch - 160us/sample
Epoch 29/90
84077/84077 - 14s - loss: 2.2530e-04 - val_loss: 2.0699e-04 - 14s/epoch - 161us/sample
Epoch 30/90
84077/84077 - 13s - loss: 2.2129e-04 - val_loss: 2.0350e-04 - 13s/epoch - 160us/sample
Epoch 31/90
84077/84077 - 13s - loss: 2.1953e-04 - val_loss: 2.0212e-04 - 13s/epoch - 160us/sample
Epoch 32/90
84077/84077 - 13s - loss: 2.1719e-04 - val_loss: 2.0091e-04 - 13s/epoch - 159us/sample
Epoch 33/90
84077/84077 - 13s - loss: 2.1613e-04 - val_loss: 2.0059e-04 - 13s/epoch - 160us/sample
Epoch 34/90
84077/84077 - 14s - loss: 2.1356e-04 - val_loss: 1.9730e-04 - 14s/epoch - 161us/sample
Epoch 35/90
84077/84077 - 13s - loss: 2.1207e-04 - val_loss: 1.9617e-04 - 13s/epoch - 160us/sample
Epoch 36/90
84077/84077 - 13s - loss: 2.0934e-04 - val_loss: 1.9337e-04 - 13s/epoch - 159us/sample
Epoch 37/90
84077/84077 - 13s - loss: 2.0794e-04 - val_loss: 1.9307e-04 - 13s/epoch - 160us/sample
Epoch 38/90
84077/84077 - 13s - loss: 2.0615e-04 - val_loss: 1.9106e-04 - 13s/epoch - 159us/sample
Epoch 39/90
84077/84077 - 14s - loss: 2.0769e-04 - val_loss: 1.8863e-04 - 14s/epoch - 161us/sample
Epoch 40/90
84077/84077 - 13s - loss: 2.0339e-04 - val_loss: 1.8846e-04 - 13s/epoch - 160us/sample
Epoch 41/90
84077/84077 - 13s - loss: 2.0264e-04 - val_loss: 1.8853e-04 - 13s/epoch - 160us/sample
Epoch 42/90
84077/84077 - 13s - loss: 2.0151e-04 - val_loss: 1.8510e-04 - 13s/epoch - 160us/sample
Epoch 43/90
84077/84077 - 13s - loss: 2.0147e-04 - val_loss: 1.8488e-04 - 13s/epoch - 160us/sample
Epoch 44/90
84077/84077 - 13s - loss: 1.9989e-04 - val_loss: 1.8380e-04 - 13s/epoch - 160us/sample
Epoch 45/90
84077/84077 - 14s - loss: 1.9848e-04 - val_loss: 1.8542e-04 - 14s/epoch - 161us/sample
Epoch 46/90
84077/84077 - 13s - loss: 1.9800e-04 - val_loss: 1.8151e-04 - 13s/epoch - 159us/sample
Epoch 47/90
84077/84077 - 13s - loss: 2.0272e-04 - val_loss: 1.8621e-04 - 13s/epoch - 160us/sample
Epoch 48/90
84077/84077 - 13s - loss: 1.9700e-04 - val_loss: 1.8298e-04 - 13s/epoch - 159us/sample
Epoch 49/90
84077/84077 - 13s - loss: 1.9548e-04 - val_loss: 1.8222e-04 - 13s/epoch - 160us/sample
Epoch 50/90
84077/84077 - 14s - loss: 1.9463e-04 - val_loss: 1.8215e-04 - 14s/epoch - 161us/sample
Epoch 51/90
84077/84077 - 13s - loss: 1.9966e-04 - val_loss: 1.8763e-04 - 13s/epoch - 160us/sample
Epoch 52/90
84077/84077 - 13s - loss: 1.9506e-04 - val_loss: 1.8068e-04 - 13s/epoch - 159us/sample
Epoch 53/90
84077/84077 - 13s - loss: 1.9480e-04 - val_loss: 1.8106e-04 - 13s/epoch - 160us/sample
Epoch 54/90
84077/84077 - 13s - loss: 1.9143e-04 - val_loss: 1.7874e-04 - 13s/epoch - 160us/sample
Epoch 55/90
84077/84077 - 14s - loss: 1.9042e-04 - val_loss: 1.7866e-04 - 14s/epoch - 161us/sample
Epoch 56/90
84077/84077 - 13s - loss: 1.9137e-04 - val_loss: 1.7894e-04 - 13s/epoch - 160us/sample
Epoch 57/90
84077/84077 - 13s - loss: 1.8932e-04 - val_loss: 1.7598e-04 - 13s/epoch - 159us/sample
Epoch 58/90
84077/84077 - 13s - loss: 1.8916e-04 - val_loss: 1.7732e-04 - 13s/epoch - 160us/sample
Epoch 59/90
84077/84077 - 13s - loss: 1.8871e-04 - val_loss: 1.7569e-04 - 13s/epoch - 160us/sample
Epoch 60/90
84077/84077 - 14s - loss: 1.8785e-04 - val_loss: 1.7477e-04 - 14s/epoch - 161us/sample
Epoch 61/90
84077/84077 - 13s - loss: 1.8743e-04 - val_loss: 1.7308e-04 - 13s/epoch - 160us/sample
Epoch 62/90
84077/84077 - 13s - loss: 1.8603e-04 - val_loss: 1.7497e-04 - 13s/epoch - 159us/sample
Epoch 63/90
84077/84077 - 13s - loss: 1.8771e-04 - val_loss: 1.7242e-04 - 13s/epoch - 160us/sample
Epoch 64/90
84077/84077 - 13s - loss: 1.8570e-04 - val_loss: 1.7326e-04 - 13s/epoch - 160us/sample
Epoch 65/90
84077/84077 - 14s - loss: 1.8738e-04 - val_loss: 1.7423e-04 - 14s/epoch - 161us/sample
Epoch 66/90
84077/84077 - 14s - loss: 1.8479e-04 - val_loss: 1.7192e-04 - 14s/epoch - 161us/sample
Epoch 67/90
84077/84077 - 13s - loss: 1.8392e-04 - val_loss: 1.7094e-04 - 13s/epoch - 159us/sample
Epoch 68/90
84077/84077 - 13s - loss: 1.8377e-04 - val_loss: 1.7144e-04 - 13s/epoch - 159us/sample
Epoch 69/90
84077/84077 - 13s - loss: 1.8620e-04 - val_loss: 1.7057e-04 - 13s/epoch - 160us/sample
Epoch 70/90
84077/84077 - 13s - loss: 1.8354e-04 - val_loss: 1.7482e-04 - 13s/epoch - 160us/sample
Epoch 71/90
84077/84077 - 14s - loss: 1.8327e-04 - val_loss: 1.7083e-04 - 14s/epoch - 161us/sample
Epoch 72/90
84077/84077 - 13s - loss: 1.8239e-04 - val_loss: 1.7122e-04 - 13s/epoch - 159us/sample
Epoch 73/90
84077/84077 - 13s - loss: 1.8112e-04 - val_loss: 1.7131e-04 - 13s/epoch - 160us/sample
Epoch 74/90
84077/84077 - 13s - loss: 1.8607e-04 - val_loss: 1.6975e-04 - 13s/epoch - 160us/sample
Epoch 75/90
84077/84077 - 13s - loss: 1.8123e-04 - val_loss: 1.6991e-04 - 13s/epoch - 160us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.8056e-04 - val_loss: 1.6969e-04 - 14s/epoch - 161us/sample
Epoch 77/90
84077/84077 - 13s - loss: 1.7969e-04 - val_loss: 1.6767e-04 - 13s/epoch - 159us/sample
Epoch 78/90
84077/84077 - 13s - loss: 1.8068e-04 - val_loss: 1.6888e-04 - 13s/epoch - 159us/sample
Epoch 79/90
84077/84077 - 13s - loss: 1.7886e-04 - val_loss: 1.6812e-04 - 13s/epoch - 160us/sample
Epoch 80/90
84077/84077 - 13s - loss: 1.7941e-04 - val_loss: 1.6584e-04 - 13s/epoch - 160us/sample
Epoch 81/90
84077/84077 - 14s - loss: 1.7860e-04 - val_loss: 1.6893e-04 - 14s/epoch - 161us/sample
Epoch 82/90
84077/84077 - 13s - loss: 1.7876e-04 - val_loss: 1.6707e-04 - 13s/epoch - 160us/sample
Epoch 83/90
84077/84077 - 13s - loss: 1.7813e-04 - val_loss: 1.6630e-04 - 13s/epoch - 160us/sample
Epoch 84/90
84077/84077 - 13s - loss: 1.7752e-04 - val_loss: 1.6552e-04 - 13s/epoch - 160us/sample
Epoch 85/90
84077/84077 - 13s - loss: 1.7737e-04 - val_loss: 1.6367e-04 - 13s/epoch - 160us/sample
Epoch 86/90
84077/84077 - 13s - loss: 1.7659e-04 - val_loss: 1.6738e-04 - 13s/epoch - 160us/sample
Epoch 87/90
84077/84077 - 14s - loss: 1.8387e-04 - val_loss: 1.6567e-04 - 14s/epoch - 161us/sample
Epoch 88/90
84077/84077 - 13s - loss: 1.7691e-04 - val_loss: 1.6275e-04 - 13s/epoch - 159us/sample
Epoch 89/90
84077/84077 - 13s - loss: 1.7668e-04 - val_loss: 1.6497e-04 - 13s/epoch - 160us/sample
Epoch 90/90
84077/84077 - 13s - loss: 1.7659e-04 - val_loss: 1.6516e-04 - 13s/epoch - 160us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00016516251877508837
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 07:40:36.172483: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_71/outputlayer/BiasAdd' id:80148 op device:{requested: '', assigned: ''} def:{{{node decoder_model_71/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_71/outputlayer/MatMul, decoder_model_71/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.019863657710966084
cosine 0.019624278327677495
MAE: 0.0018446053764318124
RMSE: 0.007959633878481314
r2: 0.9507060175198037
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.001, 0.2, 188, 0.0001765904373756653, 0.00016516251877508837, 0.019863657710966084, 0.019624278327677495, 0.0018446053764318124, 0.007959633878481314, 0.9507060175198037, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 80 0.0012000000000000001 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_186 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_186 (ReLU)               (None, 1886)         0           ['batch_normalization_186[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_186[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_186[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-13 07:41:13.383727: W tensorflow/c/c_api.cc:291] Operation '{name:'training_104/Adam/batch_normalization_186/gamma/m/Assign' id:81929 op device:{requested: '', assigned: ''} def:{{{node training_104/Adam/batch_normalization_186/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_104/Adam/batch_normalization_186/gamma/m, training_104/Adam/batch_normalization_186/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 07:41:34.918945: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_72/mul' id:81439 op device:{requested: '', assigned: ''} def:{{{node loss_72/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_72/mul/x, loss_72/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 36s - loss: 0.0045 - val_loss: 8.4748e-04 - 36s/epoch - 423us/sample
Epoch 2/80
84077/84077 - 14s - loss: 9.6948e-04 - val_loss: 0.0010 - 14s/epoch - 161us/sample
Epoch 3/80
84077/84077 - 13s - loss: 0.0011 - val_loss: 5.9131e-04 - 13s/epoch - 159us/sample
Epoch 4/80
84077/84077 - 13s - loss: 0.0060 - val_loss: 7.6325e-04 - 13s/epoch - 160us/sample
Epoch 5/80
84077/84077 - 14s - loss: 6.2580e-04 - val_loss: 7.1807e-04 - 14s/epoch - 161us/sample
Epoch 6/80
84077/84077 - 14s - loss: 4.9569e-04 - val_loss: 3.9629e-04 - 14s/epoch - 161us/sample
Epoch 7/80
84077/84077 - 14s - loss: 4.0735e-04 - val_loss: 3.5247e-04 - 14s/epoch - 162us/sample
Epoch 8/80
84077/84077 - 14s - loss: 3.6328e-04 - val_loss: 3.2100e-04 - 14s/epoch - 162us/sample
Epoch 9/80
84077/84077 - 13s - loss: 3.4906e-04 - val_loss: 2.9894e-04 - 13s/epoch - 160us/sample
Epoch 10/80
84077/84077 - 14s - loss: 3.1182e-04 - val_loss: 2.7075e-04 - 14s/epoch - 161us/sample
Epoch 11/80
84077/84077 - 14s - loss: 2.9145e-04 - val_loss: 2.5062e-04 - 14s/epoch - 161us/sample
Epoch 12/80
84077/84077 - 14s - loss: 2.7190e-04 - val_loss: 2.3568e-04 - 14s/epoch - 161us/sample
Epoch 13/80
84077/84077 - 14s - loss: 2.5670e-04 - val_loss: 2.2675e-04 - 14s/epoch - 163us/sample
Epoch 14/80
84077/84077 - 14s - loss: 2.4169e-04 - val_loss: 2.1175e-04 - 14s/epoch - 161us/sample
Epoch 15/80
84077/84077 - 14s - loss: 2.3017e-04 - val_loss: 2.0686e-04 - 14s/epoch - 161us/sample
Epoch 16/80
84077/84077 - 14s - loss: 2.1953e-04 - val_loss: 1.9416e-04 - 14s/epoch - 161us/sample
Epoch 17/80
84077/84077 - 14s - loss: 2.1062e-04 - val_loss: 1.8804e-04 - 14s/epoch - 161us/sample
Epoch 18/80
84077/84077 - 14s - loss: 2.0182e-04 - val_loss: 1.7907e-04 - 14s/epoch - 163us/sample
Epoch 19/80
84077/84077 - 14s - loss: 1.9457e-04 - val_loss: 1.7239e-04 - 14s/epoch - 161us/sample
Epoch 20/80
84077/84077 - 14s - loss: 1.9027e-04 - val_loss: 1.6796e-04 - 14s/epoch - 161us/sample
Epoch 21/80
84077/84077 - 14s - loss: 1.8537e-04 - val_loss: 1.6647e-04 - 14s/epoch - 161us/sample
Epoch 22/80
84077/84077 - 14s - loss: 1.8020e-04 - val_loss: 1.6177e-04 - 14s/epoch - 161us/sample
Epoch 23/80
84077/84077 - 14s - loss: 1.7776e-04 - val_loss: 1.5869e-04 - 14s/epoch - 163us/sample
Epoch 24/80
84077/84077 - 13s - loss: 1.7371e-04 - val_loss: 1.5625e-04 - 13s/epoch - 161us/sample
Epoch 25/80
84077/84077 - 14s - loss: 1.7143e-04 - val_loss: 1.5375e-04 - 14s/epoch - 161us/sample
Epoch 26/80
84077/84077 - 14s - loss: 1.6894e-04 - val_loss: 1.4984e-04 - 14s/epoch - 161us/sample
Epoch 27/80
84077/84077 - 14s - loss: 1.6648e-04 - val_loss: 1.5094e-04 - 14s/epoch - 161us/sample
Epoch 28/80
84077/84077 - 14s - loss: 1.6482e-04 - val_loss: 1.4784e-04 - 14s/epoch - 163us/sample
Epoch 29/80
84077/84077 - 14s - loss: 1.6318e-04 - val_loss: 1.4677e-04 - 14s/epoch - 161us/sample
Epoch 30/80
84077/84077 - 14s - loss: 1.6154e-04 - val_loss: 1.4688e-04 - 14s/epoch - 161us/sample
Epoch 31/80
84077/84077 - 14s - loss: 1.6022e-04 - val_loss: 1.4473e-04 - 14s/epoch - 161us/sample
Epoch 32/80
84077/84077 - 14s - loss: 1.5886e-04 - val_loss: 1.4543e-04 - 14s/epoch - 161us/sample
Epoch 33/80
84077/84077 - 14s - loss: 1.5777e-04 - val_loss: 1.4316e-04 - 14s/epoch - 162us/sample
Epoch 34/80
84077/84077 - 14s - loss: 1.5680e-04 - val_loss: 1.4444e-04 - 14s/epoch - 161us/sample
Epoch 35/80
84077/84077 - 14s - loss: 1.5619e-04 - val_loss: 1.4251e-04 - 14s/epoch - 161us/sample
Epoch 36/80
84077/84077 - 14s - loss: 1.5509e-04 - val_loss: 1.4168e-04 - 14s/epoch - 161us/sample
Epoch 37/80
84077/84077 - 14s - loss: 1.5354e-04 - val_loss: 1.3997e-04 - 14s/epoch - 161us/sample
Epoch 38/80
84077/84077 - 14s - loss: 1.5217e-04 - val_loss: 1.3911e-04 - 14s/epoch - 163us/sample
Epoch 39/80
84077/84077 - 14s - loss: 1.5159e-04 - val_loss: 1.3836e-04 - 14s/epoch - 161us/sample
Epoch 40/80
84077/84077 - 14s - loss: 1.5106e-04 - val_loss: 1.3884e-04 - 14s/epoch - 161us/sample
Epoch 41/80
84077/84077 - 14s - loss: 1.4971e-04 - val_loss: 1.3837e-04 - 14s/epoch - 161us/sample
Epoch 42/80
84077/84077 - 14s - loss: 1.4917e-04 - val_loss: 1.3695e-04 - 14s/epoch - 161us/sample
Epoch 43/80
84077/84077 - 14s - loss: 1.4886e-04 - val_loss: 1.3529e-04 - 14s/epoch - 162us/sample
Epoch 44/80
84077/84077 - 14s - loss: 1.4810e-04 - val_loss: 1.3613e-04 - 14s/epoch - 161us/sample
Epoch 45/80
84077/84077 - 13s - loss: 1.4722e-04 - val_loss: 1.3527e-04 - 13s/epoch - 160us/sample
Epoch 46/80
84077/84077 - 14s - loss: 1.4668e-04 - val_loss: 1.3458e-04 - 14s/epoch - 161us/sample
Epoch 47/80
84077/84077 - 14s - loss: 1.4585e-04 - val_loss: 1.3611e-04 - 14s/epoch - 161us/sample
Epoch 48/80
84077/84077 - 14s - loss: 1.4588e-04 - val_loss: 1.3423e-04 - 14s/epoch - 161us/sample
Epoch 49/80
84077/84077 - 14s - loss: 1.4559e-04 - val_loss: 1.3428e-04 - 14s/epoch - 163us/sample
Epoch 50/80
84077/84077 - 13s - loss: 1.4467e-04 - val_loss: 1.3296e-04 - 13s/epoch - 160us/sample
Epoch 51/80
84077/84077 - 14s - loss: 1.4394e-04 - val_loss: 1.3347e-04 - 14s/epoch - 161us/sample
Epoch 52/80
84077/84077 - 14s - loss: 1.4358e-04 - val_loss: 1.3260e-04 - 14s/epoch - 161us/sample
Epoch 53/80
84077/84077 - 14s - loss: 1.4306e-04 - val_loss: 1.3423e-04 - 14s/epoch - 161us/sample
Epoch 54/80
84077/84077 - 14s - loss: 1.4251e-04 - val_loss: 1.3282e-04 - 14s/epoch - 162us/sample
Epoch 55/80
84077/84077 - 14s - loss: 1.4246e-04 - val_loss: 1.3164e-04 - 14s/epoch - 161us/sample
Epoch 56/80
84077/84077 - 14s - loss: 1.4232e-04 - val_loss: 1.3256e-04 - 14s/epoch - 161us/sample
Epoch 57/80
84077/84077 - 14s - loss: 1.4136e-04 - val_loss: 1.3209e-04 - 14s/epoch - 161us/sample
Epoch 58/80
84077/84077 - 14s - loss: 1.4122e-04 - val_loss: 1.3324e-04 - 14s/epoch - 161us/sample
Epoch 59/80
84077/84077 - 14s - loss: 1.4081e-04 - val_loss: 1.3074e-04 - 14s/epoch - 162us/sample
Epoch 60/80
84077/84077 - 14s - loss: 1.4072e-04 - val_loss: 1.3003e-04 - 14s/epoch - 161us/sample
Epoch 61/80
84077/84077 - 14s - loss: 1.4044e-04 - val_loss: 1.3129e-04 - 14s/epoch - 161us/sample
Epoch 62/80
84077/84077 - 14s - loss: 1.4014e-04 - val_loss: 1.3100e-04 - 14s/epoch - 161us/sample
Epoch 63/80
84077/84077 - 14s - loss: 1.3960e-04 - val_loss: 1.2954e-04 - 14s/epoch - 161us/sample
Epoch 64/80
84077/84077 - 14s - loss: 1.3915e-04 - val_loss: 1.2869e-04 - 14s/epoch - 163us/sample
Epoch 65/80
84077/84077 - 14s - loss: 1.3900e-04 - val_loss: 1.2930e-04 - 14s/epoch - 161us/sample
Epoch 66/80
84077/84077 - 14s - loss: 1.3854e-04 - val_loss: 1.2886e-04 - 14s/epoch - 161us/sample
Epoch 67/80
84077/84077 - 14s - loss: 1.3830e-04 - val_loss: 1.2872e-04 - 14s/epoch - 161us/sample
Epoch 68/80
84077/84077 - 14s - loss: 1.3800e-04 - val_loss: 1.2935e-04 - 14s/epoch - 161us/sample
Epoch 69/80
84077/84077 - 14s - loss: 1.3782e-04 - val_loss: 1.2900e-04 - 14s/epoch - 162us/sample
Epoch 70/80
84077/84077 - 14s - loss: 1.3755e-04 - val_loss: 1.2777e-04 - 14s/epoch - 161us/sample
Epoch 71/80
84077/84077 - 13s - loss: 1.3740e-04 - val_loss: 1.3001e-04 - 13s/epoch - 160us/sample
Epoch 72/80
84077/84077 - 14s - loss: 1.3702e-04 - val_loss: 1.2760e-04 - 14s/epoch - 161us/sample
Epoch 73/80
84077/84077 - 14s - loss: 1.3685e-04 - val_loss: 1.2887e-04 - 14s/epoch - 161us/sample
Epoch 74/80
84077/84077 - 14s - loss: 1.3648e-04 - val_loss: 1.2793e-04 - 14s/epoch - 161us/sample
Epoch 75/80
84077/84077 - 14s - loss: 1.3691e-04 - val_loss: 1.2827e-04 - 14s/epoch - 163us/sample
Epoch 76/80
84077/84077 - 13s - loss: 1.3640e-04 - val_loss: 1.2626e-04 - 13s/epoch - 160us/sample
Epoch 77/80
84077/84077 - 14s - loss: 1.3598e-04 - val_loss: 1.2724e-04 - 14s/epoch - 161us/sample
Epoch 78/80
84077/84077 - 14s - loss: 1.3554e-04 - val_loss: 1.2630e-04 - 14s/epoch - 161us/sample
Epoch 79/80
84077/84077 - 14s - loss: 1.3573e-04 - val_loss: 1.2669e-04 - 14s/epoch - 161us/sample
Epoch 80/80
84077/84077 - 14s - loss: 1.3555e-04 - val_loss: 1.2663e-04 - 14s/epoch - 162us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012663180398158068
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 07:59:32.956054: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_72/outputlayer/BiasAdd' id:81403 op device:{requested: '', assigned: ''} def:{{{node decoder_model_72/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_72/outputlayer/MatMul, decoder_model_72/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030823647680053808
cosine 0.03045307389389807
MAE: 0.0024124218689422438
RMSE: 0.009954777134746934
r2: 0.9226964729272733
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 80, 0.0012000000000000001, 0.2, 188, 0.00013555373847767136, 0.00012663180398158068, 0.030823647680053808, 0.03045307389389807, 0.0024124218689422438, 0.009954777134746934, 0.9226964729272733, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 80 0.001 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_189 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_189 (ReLU)               (None, 1697)         0           ['batch_normalization_189[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_189[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_189[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-13 08:00:09.939224: W tensorflow/c/c_api.cc:291] Operation '{name:'training_106/Adam/batch_normalization_191/gamma/v/Assign' id:83402 op device:{requested: '', assigned: ''} def:{{{node training_106/Adam/batch_normalization_191/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_106/Adam/batch_normalization_191/gamma/v, training_106/Adam/batch_normalization_191/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 08:00:31.526018: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_73/mul' id:82724 op device:{requested: '', assigned: ''} def:{{{node loss_73/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_73/mul/x, loss_73/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 36s - loss: 0.0139 - val_loss: 0.0014 - 36s/epoch - 423us/sample
Epoch 2/80
84077/84077 - 14s - loss: 0.0010 - val_loss: 8.0281e-04 - 14s/epoch - 161us/sample
Epoch 3/80
84077/84077 - 14s - loss: 0.0290 - val_loss: 7.2566e-04 - 14s/epoch - 161us/sample
Epoch 4/80
84077/84077 - 14s - loss: 9.5836e-04 - val_loss: 6.4694e-04 - 14s/epoch - 161us/sample
Epoch 5/80
84077/84077 - 14s - loss: 6.7801e-04 - val_loss: 9.3315e-04 - 14s/epoch - 161us/sample
Epoch 6/80
84077/84077 - 14s - loss: 5.3368e-04 - val_loss: 4.6438e-04 - 14s/epoch - 162us/sample
Epoch 7/80
84077/84077 - 14s - loss: 4.4925e-04 - val_loss: 3.8258e-04 - 14s/epoch - 161us/sample
Epoch 8/80
84077/84077 - 14s - loss: 3.6987e-04 - val_loss: 3.3899e-04 - 14s/epoch - 161us/sample
Epoch 9/80
84077/84077 - 14s - loss: 4.7036e-04 - val_loss: 3.8763e-04 - 14s/epoch - 161us/sample
Epoch 10/80
84077/84077 - 14s - loss: 3.4254e-04 - val_loss: 2.9554e-04 - 14s/epoch - 161us/sample
Epoch 11/80
84077/84077 - 14s - loss: 3.0456e-04 - val_loss: 2.6571e-04 - 14s/epoch - 162us/sample
Epoch 12/80
84077/84077 - 14s - loss: 2.8544e-04 - val_loss: 2.5440e-04 - 14s/epoch - 161us/sample
Epoch 13/80
84077/84077 - 14s - loss: 4.1077e-04 - val_loss: 2.8599e-04 - 14s/epoch - 161us/sample
Epoch 14/80
84077/84077 - 14s - loss: 2.7097e-04 - val_loss: 2.3347e-04 - 14s/epoch - 161us/sample
Epoch 15/80
84077/84077 - 13s - loss: 2.5206e-04 - val_loss: 2.2825e-04 - 13s/epoch - 161us/sample
Epoch 16/80
84077/84077 - 14s - loss: 2.4104e-04 - val_loss: 2.1396e-04 - 14s/epoch - 163us/sample
Epoch 17/80
84077/84077 - 14s - loss: 2.3164e-04 - val_loss: 2.0614e-04 - 14s/epoch - 161us/sample
Epoch 18/80
84077/84077 - 14s - loss: 2.2162e-04 - val_loss: 1.9814e-04 - 14s/epoch - 161us/sample
Epoch 19/80
84077/84077 - 14s - loss: 2.1342e-04 - val_loss: 1.9390e-04 - 14s/epoch - 161us/sample
Epoch 20/80
84077/84077 - 14s - loss: 2.0777e-04 - val_loss: 1.8381e-04 - 14s/epoch - 161us/sample
Epoch 21/80
84077/84077 - 14s - loss: 2.0213e-04 - val_loss: 1.8048e-04 - 14s/epoch - 162us/sample
Epoch 22/80
84077/84077 - 14s - loss: 1.9558e-04 - val_loss: 1.7406e-04 - 14s/epoch - 162us/sample
Epoch 23/80
84077/84077 - 13s - loss: 1.9021e-04 - val_loss: 1.7048e-04 - 13s/epoch - 161us/sample
Epoch 24/80
84077/84077 - 14s - loss: 1.8943e-04 - val_loss: 1.6773e-04 - 14s/epoch - 161us/sample
Epoch 25/80
84077/84077 - 14s - loss: 1.8346e-04 - val_loss: 1.6380e-04 - 14s/epoch - 161us/sample
Epoch 26/80
84077/84077 - 14s - loss: 1.7905e-04 - val_loss: 1.6031e-04 - 14s/epoch - 162us/sample
Epoch 27/80
84077/84077 - 14s - loss: 1.7625e-04 - val_loss: 1.5790e-04 - 14s/epoch - 161us/sample
Epoch 28/80
84077/84077 - 14s - loss: 1.7375e-04 - val_loss: 1.5596e-04 - 14s/epoch - 161us/sample
Epoch 29/80
84077/84077 - 14s - loss: 1.7090e-04 - val_loss: 1.5325e-04 - 14s/epoch - 161us/sample
Epoch 30/80
84077/84077 - 14s - loss: 1.6868e-04 - val_loss: 1.5241e-04 - 14s/epoch - 161us/sample
Epoch 31/80
84077/84077 - 14s - loss: 1.6695e-04 - val_loss: 1.4974e-04 - 14s/epoch - 161us/sample
Epoch 32/80
84077/84077 - 14s - loss: 1.6485e-04 - val_loss: 1.5065e-04 - 14s/epoch - 162us/sample
Epoch 33/80
84077/84077 - 14s - loss: 1.6389e-04 - val_loss: 1.4952e-04 - 14s/epoch - 161us/sample
Epoch 34/80
84077/84077 - 14s - loss: 1.6241e-04 - val_loss: 1.4786e-04 - 14s/epoch - 161us/sample
Epoch 35/80
84077/84077 - 14s - loss: 1.6087e-04 - val_loss: 1.4610e-04 - 14s/epoch - 161us/sample
Epoch 36/80
84077/84077 - 14s - loss: 1.5960e-04 - val_loss: 1.4632e-04 - 14s/epoch - 161us/sample
Epoch 37/80
84077/84077 - 14s - loss: 1.6002e-04 - val_loss: 1.4616e-04 - 14s/epoch - 163us/sample
Epoch 38/80
84077/84077 - 14s - loss: 1.5798e-04 - val_loss: 1.4321e-04 - 14s/epoch - 161us/sample
Epoch 39/80
84077/84077 - 14s - loss: 1.5613e-04 - val_loss: 1.4321e-04 - 14s/epoch - 161us/sample
Epoch 40/80
84077/84077 - 14s - loss: 1.5543e-04 - val_loss: 1.4290e-04 - 14s/epoch - 161us/sample
Epoch 41/80
84077/84077 - 14s - loss: 1.5448e-04 - val_loss: 1.4272e-04 - 14s/epoch - 161us/sample
Epoch 42/80
84077/84077 - 14s - loss: 1.5363e-04 - val_loss: 1.4255e-04 - 14s/epoch - 163us/sample
Epoch 43/80
84077/84077 - 14s - loss: 1.5300e-04 - val_loss: 1.4198e-04 - 14s/epoch - 161us/sample
Epoch 44/80
84077/84077 - 14s - loss: 1.5253e-04 - val_loss: 1.4001e-04 - 14s/epoch - 161us/sample
Epoch 45/80
84077/84077 - 13s - loss: 1.5166e-04 - val_loss: 1.3871e-04 - 13s/epoch - 160us/sample
Epoch 46/80
84077/84077 - 14s - loss: 1.5102e-04 - val_loss: 1.4009e-04 - 14s/epoch - 161us/sample
Epoch 47/80
84077/84077 - 14s - loss: 1.5027e-04 - val_loss: 1.3951e-04 - 14s/epoch - 162us/sample
Epoch 48/80
84077/84077 - 14s - loss: 1.4980e-04 - val_loss: 1.3873e-04 - 14s/epoch - 161us/sample
Epoch 49/80
84077/84077 - 14s - loss: 1.4902e-04 - val_loss: 1.3712e-04 - 14s/epoch - 161us/sample
Epoch 50/80
84077/84077 - 14s - loss: 1.4811e-04 - val_loss: 1.3841e-04 - 14s/epoch - 161us/sample
Epoch 51/80
84077/84077 - 14s - loss: 1.4853e-04 - val_loss: 1.3905e-04 - 14s/epoch - 161us/sample
Epoch 52/80
84077/84077 - 14s - loss: 1.4793e-04 - val_loss: 1.3804e-04 - 14s/epoch - 162us/sample
Epoch 53/80
84077/84077 - 14s - loss: 1.4733e-04 - val_loss: 1.3622e-04 - 14s/epoch - 161us/sample
Epoch 54/80
84077/84077 - 14s - loss: 1.4718e-04 - val_loss: 1.3695e-04 - 14s/epoch - 161us/sample
Epoch 55/80
84077/84077 - 14s - loss: 1.4659e-04 - val_loss: 1.3584e-04 - 14s/epoch - 161us/sample
Epoch 56/80
84077/84077 - 14s - loss: 1.4580e-04 - val_loss: 1.3605e-04 - 14s/epoch - 161us/sample
Epoch 57/80
84077/84077 - 14s - loss: 1.4508e-04 - val_loss: 1.3497e-04 - 14s/epoch - 162us/sample
Epoch 58/80
84077/84077 - 14s - loss: 1.4503e-04 - val_loss: 1.3472e-04 - 14s/epoch - 162us/sample
Epoch 59/80
84077/84077 - 13s - loss: 1.4455e-04 - val_loss: 1.3511e-04 - 13s/epoch - 160us/sample
Epoch 60/80
84077/84077 - 14s - loss: 1.4437e-04 - val_loss: 1.3553e-04 - 14s/epoch - 161us/sample
Epoch 61/80
84077/84077 - 14s - loss: 1.4406e-04 - val_loss: 1.3480e-04 - 14s/epoch - 161us/sample
Epoch 62/80
84077/84077 - 14s - loss: 1.4354e-04 - val_loss: 1.3330e-04 - 14s/epoch - 161us/sample
Epoch 63/80
84077/84077 - 14s - loss: 1.4336e-04 - val_loss: 1.3345e-04 - 14s/epoch - 163us/sample
Epoch 64/80
84077/84077 - 14s - loss: 1.4313e-04 - val_loss: 1.3381e-04 - 14s/epoch - 161us/sample
Epoch 65/80
84077/84077 - 14s - loss: 1.4271e-04 - val_loss: 1.3385e-04 - 14s/epoch - 161us/sample
Epoch 66/80
84077/84077 - 14s - loss: 1.4250e-04 - val_loss: 1.3384e-04 - 14s/epoch - 161us/sample
Epoch 67/80
84077/84077 - 14s - loss: 1.4184e-04 - val_loss: 1.3304e-04 - 14s/epoch - 161us/sample
Epoch 68/80
84077/84077 - 14s - loss: 1.4231e-04 - val_loss: 1.3403e-04 - 14s/epoch - 163us/sample
Epoch 69/80
84077/84077 - 14s - loss: 1.4193e-04 - val_loss: 1.3385e-04 - 14s/epoch - 161us/sample
Epoch 70/80
84077/84077 - 14s - loss: 1.4126e-04 - val_loss: 1.3246e-04 - 14s/epoch - 161us/sample
Epoch 71/80
84077/84077 - 14s - loss: 1.4112e-04 - val_loss: 1.3171e-04 - 14s/epoch - 161us/sample
Epoch 72/80
84077/84077 - 14s - loss: 1.4090e-04 - val_loss: 1.3315e-04 - 14s/epoch - 161us/sample
Epoch 73/80
84077/84077 - 14s - loss: 1.4038e-04 - val_loss: 1.3207e-04 - 14s/epoch - 163us/sample
Epoch 74/80
84077/84077 - 14s - loss: 1.4041e-04 - val_loss: 1.3144e-04 - 14s/epoch - 161us/sample
Epoch 75/80
84077/84077 - 14s - loss: 1.4015e-04 - val_loss: 1.3122e-04 - 14s/epoch - 161us/sample
Epoch 76/80
84077/84077 - 14s - loss: 1.4030e-04 - val_loss: 1.3477e-04 - 14s/epoch - 161us/sample
Epoch 77/80
84077/84077 - 13s - loss: 1.3968e-04 - val_loss: 1.3151e-04 - 13s/epoch - 160us/sample
Epoch 78/80
84077/84077 - 14s - loss: 1.3946e-04 - val_loss: 1.3111e-04 - 14s/epoch - 163us/sample
Epoch 79/80
84077/84077 - 14s - loss: 1.3938e-04 - val_loss: 1.3228e-04 - 14s/epoch - 161us/sample
Epoch 80/80
84077/84077 - 14s - loss: 1.3892e-04 - val_loss: 1.3053e-04 - 14s/epoch - 161us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00013052876587050247
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 08:18:29.578678: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_73/outputlayer/BiasAdd' id:82688 op device:{requested: '', assigned: ''} def:{{{node decoder_model_73/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_73/outputlayer/MatMul, decoder_model_73/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03338742233746575
cosine 0.03297604651137852
MAE: 0.002429662528366748
RMSE: 0.010643674436014737
r2: 0.9116604130014786
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 64, 80, 0.001, 0.2, 188, 0.00013892130916743175, 0.00013052876587050247, 0.03338742233746575, 0.03297604651137852, 0.002429662528366748, 0.010643674436014737, 0.9116604130014786, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.001 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_192 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_192 (ReLU)               (None, 1603)         0           ['batch_normalization_192[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_192[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_192[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE1.6999999999999997_cr0.2_bs64_ep85_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-13 08:19:07.070106: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_74/bias/Assign' id:83800 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_74/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_74/bias, dense_dec0_74/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-13 08:19:22.652228: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_123_1/gamma/v/Assign' id:84729 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_123_1/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_123_1/gamma/v, batch_normalization_123_1/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 08:19:38.284689: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_75/outputlayer/BiasAdd' id:84420 op device:{requested: '', assigned: ''} def:{{{node decoder_model_75/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_75/outputlayer/MatMul, decoder_model_75/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.020610162710834178
cosine 0.020362288510067435
MAE: 0.0022790662720063573
RMSE: 0.008209531543781333
r2: 0.9476352505002325
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['1.6999999999999997custom_VAE', 'mse', 64, 85, 0.001, 0.2, 188, '--', '--', 0.020610162710834178, 0.020362288510067435, 0.0022790662720063573, 0.008209531543781333, 0.9476352505002325, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_195 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_195 (ReLU)               (None, 1980)         0           ['batch_normalization_195[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_195[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_195[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.1_cr0.2_bs32_ep90_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-13 08:20:13.576616: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_76/bias/Assign' id:85107 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_76/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_76/bias, bottleneck_zlog_76/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-13 08:20:29.318920: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_77/kernel/m/Assign' id:86075 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_77/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_77/kernel/m, dense_dec1_77/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 08:20:45.088930: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_77/outputlayer/BiasAdd' id:85842 op device:{requested: '', assigned: ''} def:{{{node decoder_model_77/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_77/outputlayer/MatMul, decoder_model_77/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02771582175254036
cosine 0.027389282115549637
MAE: 0.002111048604466704
RMSE: 0.010193691006611012
r2: 0.9189397476133714
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.1custom_VAE', 'mse', 32, 90, 0.001, 0.2, 188, '--', '--', 0.02771582175254036, 0.027389282115549637, 0.002111048604466704, 0.010193691006611012, 0.9189397476133714, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_198 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_198 (ReLU)               (None, 1886)         0           ['batch_normalization_198[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_198[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_198[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.0_cr0.2_bs64_ep90_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-13 08:21:20.307970: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_198/moving_variance/Assign' id:86452 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_198/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_198/moving_variance, batch_normalization_198/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-13 08:21:36.111580: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_79/kernel/m/Assign' id:87561 op device:{requested: '', assigned: ''} def:{{{node outputlayer_79/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_79/kernel/m, outputlayer_79/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 08:21:51.836956: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_79/outputlayer/BiasAdd' id:87271 op device:{requested: '', assigned: ''} def:{{{node decoder_model_79/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_79/outputlayer/MatMul, decoder_model_79/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030444980563441994
cosine 0.030075373267885935
MAE: 0.0021819469771193084
RMSE: 0.010149013483564444
r2: 0.9197154358709417
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.0custom_VAE', 'logcosh', 64, 90, 0.001, 0.2, 188, '--', '--', 0.030444980563441994, 0.030075373267885935, 0.0021819469771193084, 0.010149013483564444, 0.9197154358709417, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 7
Fitness    = 541.8276370289637
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 541.8276370289637.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664]
[2.2 90 0.001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_201 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_201 (ReLU)               (None, 2074)         0           ['batch_normalization_201[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_201[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_201[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 08:22:26.468204: W tensorflow/c/c_api.cc:291] Operation '{name:'training_108/Adam/decay/Assign' id:88723 op device:{requested: '', assigned: ''} def:{{{node training_108/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_108/Adam/decay, training_108/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 08:22:48.378489: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_80/mul' id:88282 op device:{requested: '', assigned: ''} def:{{{node loss_80/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_80/mul/x, loss_80/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 36s - loss: 0.0082 - val_loss: 0.0018 - 36s/epoch - 431us/sample
Epoch 2/90
84077/84077 - 13s - loss: 0.0015 - val_loss: 0.0014 - 13s/epoch - 159us/sample
Epoch 3/90
84077/84077 - 13s - loss: 0.0227 - val_loss: 0.0014 - 13s/epoch - 159us/sample
Epoch 4/90
84077/84077 - 13s - loss: 0.0013 - val_loss: 0.0010 - 13s/epoch - 159us/sample
Epoch 5/90
84077/84077 - 13s - loss: 9.9971e-04 - val_loss: 8.8968e-04 - 13s/epoch - 159us/sample
Epoch 6/90
84077/84077 - 14s - loss: 8.2972e-04 - val_loss: 6.4612e-04 - 14s/epoch - 161us/sample
Epoch 7/90
84077/84077 - 13s - loss: 7.9404e-04 - val_loss: 5.9451e-04 - 13s/epoch - 160us/sample
Epoch 8/90
84077/84077 - 13s - loss: 6.4267e-04 - val_loss: 5.0713e-04 - 13s/epoch - 160us/sample
Epoch 9/90
84077/84077 - 13s - loss: 5.5576e-04 - val_loss: 4.5228e-04 - 13s/epoch - 159us/sample
Epoch 10/90
84077/84077 - 13s - loss: 5.1656e-04 - val_loss: 4.1364e-04 - 13s/epoch - 160us/sample
Epoch 11/90
84077/84077 - 13s - loss: 4.6218e-04 - val_loss: 3.8085e-04 - 13s/epoch - 160us/sample
Epoch 12/90
84077/84077 - 14s - loss: 4.4314e-04 - val_loss: 3.6958e-04 - 14s/epoch - 161us/sample
Epoch 13/90
84077/84077 - 13s - loss: 3.8826e-04 - val_loss: 3.2776e-04 - 13s/epoch - 159us/sample
Epoch 14/90
84077/84077 - 13s - loss: 3.6233e-04 - val_loss: 3.0993e-04 - 13s/epoch - 159us/sample
Epoch 15/90
84077/84077 - 13s - loss: 3.5847e-04 - val_loss: 2.8585e-04 - 13s/epoch - 159us/sample
Epoch 16/90
84077/84077 - 13s - loss: 3.2054e-04 - val_loss: 2.6860e-04 - 13s/epoch - 160us/sample
Epoch 17/90
84077/84077 - 14s - loss: 3.0590e-04 - val_loss: 2.7613e-04 - 14s/epoch - 162us/sample
Epoch 18/90
84077/84077 - 13s - loss: 2.9393e-04 - val_loss: 2.4839e-04 - 13s/epoch - 159us/sample
Epoch 19/90
84077/84077 - 13s - loss: 2.8569e-04 - val_loss: 2.4732e-04 - 13s/epoch - 159us/sample
Epoch 20/90
84077/84077 - 13s - loss: 2.7318e-04 - val_loss: 2.3947e-04 - 13s/epoch - 159us/sample
Epoch 21/90
84077/84077 - 13s - loss: 2.6469e-04 - val_loss: 2.3267e-04 - 13s/epoch - 160us/sample
Epoch 22/90
84077/84077 - 14s - loss: 2.6023e-04 - val_loss: 2.2992e-04 - 14s/epoch - 161us/sample
Epoch 23/90
84077/84077 - 13s - loss: 2.5768e-04 - val_loss: 2.2397e-04 - 13s/epoch - 159us/sample
Epoch 24/90
84077/84077 - 13s - loss: 2.4871e-04 - val_loss: 2.2124e-04 - 13s/epoch - 159us/sample
Epoch 25/90
84077/84077 - 13s - loss: 2.4414e-04 - val_loss: 2.2100e-04 - 13s/epoch - 159us/sample
Epoch 26/90
84077/84077 - 13s - loss: 2.4019e-04 - val_loss: 2.1409e-04 - 13s/epoch - 160us/sample
Epoch 27/90
84077/84077 - 14s - loss: 2.3844e-04 - val_loss: 2.1809e-04 - 14s/epoch - 161us/sample
Epoch 28/90
84077/84077 - 13s - loss: 2.3297e-04 - val_loss: 2.1015e-04 - 13s/epoch - 160us/sample
Epoch 29/90
84077/84077 - 13s - loss: 2.3182e-04 - val_loss: 2.1205e-04 - 13s/epoch - 159us/sample
Epoch 30/90
84077/84077 - 13s - loss: 2.3754e-04 - val_loss: 2.1550e-04 - 13s/epoch - 159us/sample
Epoch 31/90
84077/84077 - 13s - loss: 2.2896e-04 - val_loss: 2.0592e-04 - 13s/epoch - 159us/sample
Epoch 32/90
84077/84077 - 13s - loss: 2.2658e-04 - val_loss: 2.0818e-04 - 13s/epoch - 160us/sample
Epoch 33/90
84077/84077 - 14s - loss: 2.2273e-04 - val_loss: 2.0446e-04 - 14s/epoch - 161us/sample
Epoch 34/90
84077/84077 - 13s - loss: 2.2315e-04 - val_loss: 2.1022e-04 - 13s/epoch - 159us/sample
Epoch 35/90
84077/84077 - 13s - loss: 2.1917e-04 - val_loss: 1.9716e-04 - 13s/epoch - 160us/sample
Epoch 36/90
84077/84077 - 13s - loss: 2.1812e-04 - val_loss: 1.9988e-04 - 13s/epoch - 159us/sample
Epoch 37/90
84077/84077 - 13s - loss: 2.1551e-04 - val_loss: 1.9776e-04 - 13s/epoch - 159us/sample
Epoch 38/90
84077/84077 - 14s - loss: 2.1581e-04 - val_loss: 1.9628e-04 - 14s/epoch - 161us/sample
Epoch 39/90
84077/84077 - 13s - loss: 2.1339e-04 - val_loss: 1.9667e-04 - 13s/epoch - 159us/sample
Epoch 40/90
84077/84077 - 13s - loss: 2.1435e-04 - val_loss: 1.9385e-04 - 13s/epoch - 159us/sample
Epoch 41/90
84077/84077 - 13s - loss: 2.1433e-04 - val_loss: 1.9466e-04 - 13s/epoch - 160us/sample
Epoch 42/90
84077/84077 - 13s - loss: 2.1048e-04 - val_loss: 1.9059e-04 - 13s/epoch - 159us/sample
Epoch 43/90
84077/84077 - 14s - loss: 2.0842e-04 - val_loss: 1.8973e-04 - 14s/epoch - 162us/sample
Epoch 44/90
84077/84077 - 13s - loss: 2.0734e-04 - val_loss: 1.8866e-04 - 13s/epoch - 160us/sample
Epoch 45/90
84077/84077 - 13s - loss: 2.0619e-04 - val_loss: 1.9249e-04 - 13s/epoch - 159us/sample
Epoch 46/90
84077/84077 - 13s - loss: 2.1397e-04 - val_loss: 2.1457e-04 - 13s/epoch - 160us/sample
Epoch 47/90
84077/84077 - 13s - loss: 2.0572e-04 - val_loss: 1.8988e-04 - 13s/epoch - 159us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.0340e-04 - val_loss: 1.9363e-04 - 14s/epoch - 161us/sample
Epoch 49/90
84077/84077 - 13s - loss: 2.0236e-04 - val_loss: 1.9229e-04 - 13s/epoch - 160us/sample
Epoch 50/90
84077/84077 - 13s - loss: 2.0454e-04 - val_loss: 1.8464e-04 - 13s/epoch - 159us/sample
Epoch 51/90
84077/84077 - 13s - loss: 2.0224e-04 - val_loss: 1.9857e-04 - 13s/epoch - 159us/sample
Epoch 52/90
84077/84077 - 13s - loss: 2.0029e-04 - val_loss: 1.8532e-04 - 13s/epoch - 160us/sample
Epoch 53/90
84077/84077 - 13s - loss: 2.0203e-04 - val_loss: 1.9959e-04 - 13s/epoch - 160us/sample
Epoch 54/90
84077/84077 - 14s - loss: 1.9928e-04 - val_loss: 1.9125e-04 - 14s/epoch - 162us/sample
Epoch 55/90
84077/84077 - 13s - loss: 1.9934e-04 - val_loss: 1.8259e-04 - 13s/epoch - 160us/sample
Epoch 56/90
84077/84077 - 13s - loss: 2.0002e-04 - val_loss: 1.8113e-04 - 13s/epoch - 159us/sample
Epoch 57/90
84077/84077 - 13s - loss: 1.9690e-04 - val_loss: 1.8331e-04 - 13s/epoch - 160us/sample
Epoch 58/90
84077/84077 - 13s - loss: 1.9609e-04 - val_loss: 1.7959e-04 - 13s/epoch - 160us/sample
Epoch 59/90
84077/84077 - 14s - loss: 1.9542e-04 - val_loss: 1.8173e-04 - 14s/epoch - 161us/sample
Epoch 60/90
84077/84077 - 13s - loss: 1.9790e-04 - val_loss: 2.2240e-04 - 13s/epoch - 159us/sample
Epoch 61/90
84077/84077 - 13s - loss: 1.9645e-04 - val_loss: 1.7655e-04 - 13s/epoch - 159us/sample
Epoch 62/90
84077/84077 - 13s - loss: 1.9470e-04 - val_loss: 1.7996e-04 - 13s/epoch - 160us/sample
Epoch 63/90
84077/84077 - 13s - loss: 1.9452e-04 - val_loss: 1.7867e-04 - 13s/epoch - 160us/sample
Epoch 64/90
84077/84077 - 14s - loss: 1.9370e-04 - val_loss: 1.7662e-04 - 14s/epoch - 161us/sample
Epoch 65/90
84077/84077 - 13s - loss: 1.9324e-04 - val_loss: 1.7641e-04 - 13s/epoch - 160us/sample
Epoch 66/90
84077/84077 - 13s - loss: 1.9280e-04 - val_loss: 1.7608e-04 - 13s/epoch - 160us/sample
Epoch 67/90
84077/84077 - 13s - loss: 1.9383e-04 - val_loss: 1.7684e-04 - 13s/epoch - 159us/sample
Epoch 68/90
84077/84077 - 13s - loss: 1.9125e-04 - val_loss: 1.7471e-04 - 13s/epoch - 160us/sample
Epoch 69/90
84077/84077 - 13s - loss: 1.9294e-04 - val_loss: 1.9166e-04 - 13s/epoch - 161us/sample
Epoch 70/90
84077/84077 - 13s - loss: 1.9625e-04 - val_loss: 1.7751e-04 - 13s/epoch - 160us/sample
Epoch 71/90
84077/84077 - 13s - loss: 1.9138e-04 - val_loss: 1.7546e-04 - 13s/epoch - 159us/sample
Epoch 72/90
84077/84077 - 13s - loss: 1.9076e-04 - val_loss: 1.7268e-04 - 13s/epoch - 159us/sample
Epoch 73/90
84077/84077 - 13s - loss: 1.9061e-04 - val_loss: 1.7393e-04 - 13s/epoch - 159us/sample
Epoch 74/90
84077/84077 - 13s - loss: 2.1699e-04 - val_loss: 1.8557e-04 - 13s/epoch - 160us/sample
Epoch 75/90
84077/84077 - 14s - loss: 1.9188e-04 - val_loss: 1.7517e-04 - 14s/epoch - 161us/sample
Epoch 76/90
84077/84077 - 13s - loss: 1.9650e-04 - val_loss: 1.7578e-04 - 13s/epoch - 160us/sample
Epoch 77/90
84077/84077 - 13s - loss: 1.8909e-04 - val_loss: 1.7678e-04 - 13s/epoch - 160us/sample
Epoch 78/90
84077/84077 - 13s - loss: 1.8806e-04 - val_loss: 1.7227e-04 - 13s/epoch - 159us/sample
Epoch 79/90
84077/84077 - 13s - loss: 1.9178e-04 - val_loss: 1.7633e-04 - 13s/epoch - 160us/sample
Epoch 80/90
84077/84077 - 14s - loss: 1.8815e-04 - val_loss: 1.7240e-04 - 14s/epoch - 161us/sample
Epoch 81/90
84077/84077 - 13s - loss: 1.8770e-04 - val_loss: 1.7609e-04 - 13s/epoch - 160us/sample
Epoch 82/90
84077/84077 - 13s - loss: 1.8766e-04 - val_loss: 1.7314e-04 - 13s/epoch - 159us/sample
Epoch 83/90
84077/84077 - 13s - loss: 1.8664e-04 - val_loss: 1.7582e-04 - 13s/epoch - 159us/sample
Epoch 84/90
84077/84077 - 13s - loss: 1.9125e-04 - val_loss: 1.7226e-04 - 13s/epoch - 160us/sample
Epoch 85/90
84077/84077 - 14s - loss: 1.8698e-04 - val_loss: 1.7056e-04 - 14s/epoch - 161us/sample
Epoch 86/90
84077/84077 - 13s - loss: 1.8548e-04 - val_loss: 1.7266e-04 - 13s/epoch - 160us/sample
Epoch 87/90
84077/84077 - 13s - loss: 1.8600e-04 - val_loss: 1.7338e-04 - 13s/epoch - 159us/sample
Epoch 88/90
84077/84077 - 13s - loss: 1.8668e-04 - val_loss: 1.7272e-04 - 13s/epoch - 159us/sample
Epoch 89/90
84077/84077 - 13s - loss: 1.8431e-04 - val_loss: 1.7248e-04 - 13s/epoch - 160us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.8423e-04 - val_loss: 1.7226e-04 - 14s/epoch - 161us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017225550475063868
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 08:42:51.957871: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_80/outputlayer/BiasAdd' id:88253 op device:{requested: '', assigned: ''} def:{{{node decoder_model_80/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_80/outputlayer/MatMul, decoder_model_80/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02114189792171586
cosine 0.020884617337605833
MAE: 0.002412347047947593
RMSE: 0.008391883897603281
r2: 0.9452368982703828
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 90, 0.001, 0.2, 188, 0.00018423199757631778, 0.00017225550475063868, 0.02114189792171586, 0.020884617337605833, 0.002412347047947593, 0.008391883897603281, 0.9452368982703828, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_204 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_204 (ReLU)               (None, 1886)         0           ['batch_normalization_204[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_204[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_204[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 08:43:29.273463: W tensorflow/c/c_api.cc:291] Operation '{name:'training_110/Adam/dense_dec0_81/kernel/v/Assign' id:90178 op device:{requested: '', assigned: ''} def:{{{node training_110/Adam/dense_dec0_81/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_110/Adam/dense_dec0_81/kernel/v, training_110/Adam/dense_dec0_81/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 08:43:51.256541: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_81/mul' id:89537 op device:{requested: '', assigned: ''} def:{{{node loss_81/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_81/mul/x, loss_81/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 36s - loss: 0.0254 - val_loss: 0.0021 - 36s/epoch - 434us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0015 - val_loss: 0.0025 - 14s/epoch - 161us/sample
Epoch 3/85
84077/84077 - 14s - loss: 683.7225 - val_loss: 0.0056 - 14s/epoch - 162us/sample
Epoch 4/85
84077/84077 - 14s - loss: 0.0017 - val_loss: 0.0011 - 14s/epoch - 162us/sample
Epoch 5/85
84077/84077 - 14s - loss: 9.7450e-04 - val_loss: 9.8445e-04 - 14s/epoch - 162us/sample
Epoch 6/85
84077/84077 - 14s - loss: 8.8507e-04 - val_loss: 8.3420e-04 - 14s/epoch - 164us/sample
Epoch 7/85
84077/84077 - 14s - loss: 0.0010 - val_loss: 7.0243e-04 - 14s/epoch - 162us/sample
Epoch 8/85
84077/84077 - 14s - loss: 0.0016 - val_loss: 0.0010 - 14s/epoch - 162us/sample
Epoch 9/85
84077/84077 - 14s - loss: 7.1338e-04 - val_loss: 5.7188e-04 - 14s/epoch - 162us/sample
Epoch 10/85
84077/84077 - 14s - loss: 6.0535e-04 - val_loss: 4.9375e-04 - 14s/epoch - 162us/sample
Epoch 11/85
84077/84077 - 14s - loss: 5.3136e-04 - val_loss: 4.7794e-04 - 14s/epoch - 164us/sample
Epoch 12/85
84077/84077 - 14s - loss: 7.7351e-04 - val_loss: 7.6164e-04 - 14s/epoch - 162us/sample
Epoch 13/85
84077/84077 - 14s - loss: 4.8967e-04 - val_loss: 4.4641e-04 - 14s/epoch - 162us/sample
Epoch 14/85
84077/84077 - 14s - loss: 4.4611e-04 - val_loss: 5.7982e-04 - 14s/epoch - 162us/sample
Epoch 15/85
84077/84077 - 14s - loss: 4.1962e-04 - val_loss: 3.8527e-04 - 14s/epoch - 162us/sample
Epoch 16/85
84077/84077 - 14s - loss: 4.0136e-04 - val_loss: 3.5118e-04 - 14s/epoch - 164us/sample
Epoch 17/85
84077/84077 - 14s - loss: 3.7528e-04 - val_loss: 3.3019e-04 - 14s/epoch - 162us/sample
Epoch 18/85
84077/84077 - 14s - loss: 3.5791e-04 - val_loss: 3.2586e-04 - 14s/epoch - 162us/sample
Epoch 19/85
84077/84077 - 14s - loss: 3.4253e-04 - val_loss: 3.1147e-04 - 14s/epoch - 162us/sample
Epoch 20/85
84077/84077 - 14s - loss: 3.2859e-04 - val_loss: 3.5968e-04 - 14s/epoch - 162us/sample
Epoch 21/85
84077/84077 - 14s - loss: 3.2014e-04 - val_loss: 2.8628e-04 - 14s/epoch - 164us/sample
Epoch 22/85
84077/84077 - 14s - loss: 3.0763e-04 - val_loss: 2.8045e-04 - 14s/epoch - 162us/sample
Epoch 23/85
84077/84077 - 14s - loss: 2.9726e-04 - val_loss: 2.7387e-04 - 14s/epoch - 162us/sample
Epoch 24/85
84077/84077 - 14s - loss: 2.9091e-04 - val_loss: 2.6037e-04 - 14s/epoch - 162us/sample
Epoch 25/85
84077/84077 - 14s - loss: 2.8362e-04 - val_loss: 2.6006e-04 - 14s/epoch - 162us/sample
Epoch 26/85
84077/84077 - 14s - loss: 2.7732e-04 - val_loss: 2.5195e-04 - 14s/epoch - 163us/sample
Epoch 27/85
84077/84077 - 14s - loss: 2.7163e-04 - val_loss: 2.5255e-04 - 14s/epoch - 163us/sample
Epoch 28/85
84077/84077 - 14s - loss: 2.6727e-04 - val_loss: 2.4874e-04 - 14s/epoch - 162us/sample
Epoch 29/85
84077/84077 - 14s - loss: 2.6070e-04 - val_loss: 2.4214e-04 - 14s/epoch - 162us/sample
Epoch 30/85
84077/84077 - 14s - loss: 2.5778e-04 - val_loss: 2.4187e-04 - 14s/epoch - 162us/sample
Epoch 31/85
84077/84077 - 14s - loss: 2.5365e-04 - val_loss: 2.3989e-04 - 14s/epoch - 164us/sample
Epoch 32/85
84077/84077 - 14s - loss: 2.4911e-04 - val_loss: 2.3334e-04 - 14s/epoch - 163us/sample
Epoch 33/85
84077/84077 - 14s - loss: 2.4532e-04 - val_loss: 2.4038e-04 - 14s/epoch - 162us/sample
Epoch 34/85
84077/84077 - 14s - loss: 2.4204e-04 - val_loss: 2.2598e-04 - 14s/epoch - 162us/sample
Epoch 35/85
84077/84077 - 14s - loss: 2.3955e-04 - val_loss: 2.2611e-04 - 14s/epoch - 162us/sample
Epoch 36/85
84077/84077 - 14s - loss: 2.3584e-04 - val_loss: 2.2115e-04 - 14s/epoch - 164us/sample
Epoch 37/85
84077/84077 - 14s - loss: 2.3128e-04 - val_loss: 2.2053e-04 - 14s/epoch - 163us/sample
Epoch 38/85
84077/84077 - 14s - loss: 2.2913e-04 - val_loss: 2.1668e-04 - 14s/epoch - 162us/sample
Epoch 39/85
84077/84077 - 14s - loss: 2.2614e-04 - val_loss: 2.1369e-04 - 14s/epoch - 162us/sample
Epoch 40/85
84077/84077 - 14s - loss: 2.2445e-04 - val_loss: 2.1267e-04 - 14s/epoch - 162us/sample
Epoch 41/85
84077/84077 - 14s - loss: 2.2224e-04 - val_loss: 2.1059e-04 - 14s/epoch - 163us/sample
Epoch 42/85
84077/84077 - 14s - loss: 2.2016e-04 - val_loss: 2.1143e-04 - 14s/epoch - 163us/sample
Epoch 43/85
84077/84077 - 14s - loss: 2.1872e-04 - val_loss: 2.0861e-04 - 14s/epoch - 162us/sample
Epoch 44/85
84077/84077 - 14s - loss: 2.1762e-04 - val_loss: 2.0885e-04 - 14s/epoch - 162us/sample
Epoch 45/85
84077/84077 - 14s - loss: 2.1538e-04 - val_loss: 2.0658e-04 - 14s/epoch - 162us/sample
Epoch 46/85
84077/84077 - 14s - loss: 2.1466e-04 - val_loss: 2.0765e-04 - 14s/epoch - 163us/sample
Epoch 47/85
84077/84077 - 14s - loss: 2.1377e-04 - val_loss: 2.0576e-04 - 14s/epoch - 163us/sample
Epoch 48/85
84077/84077 - 14s - loss: 2.1246e-04 - val_loss: 2.0611e-04 - 14s/epoch - 162us/sample
Epoch 49/85
84077/84077 - 14s - loss: 2.1157e-04 - val_loss: 2.0228e-04 - 14s/epoch - 162us/sample
Epoch 50/85
84077/84077 - 14s - loss: 2.1031e-04 - val_loss: 2.0636e-04 - 14s/epoch - 162us/sample
Epoch 51/85
84077/84077 - 14s - loss: 2.0953e-04 - val_loss: 2.0370e-04 - 14s/epoch - 163us/sample
Epoch 52/85
84077/84077 - 14s - loss: 2.0845e-04 - val_loss: 2.0248e-04 - 14s/epoch - 164us/sample
Epoch 53/85
84077/84077 - 14s - loss: 2.0762e-04 - val_loss: 2.0206e-04 - 14s/epoch - 162us/sample
Epoch 54/85
84077/84077 - 14s - loss: 2.0776e-04 - val_loss: 2.0148e-04 - 14s/epoch - 162us/sample
Epoch 55/85
84077/84077 - 14s - loss: 2.0568e-04 - val_loss: 1.9934e-04 - 14s/epoch - 162us/sample
Epoch 56/85
84077/84077 - 14s - loss: 2.0520e-04 - val_loss: 2.0187e-04 - 14s/epoch - 162us/sample
Epoch 57/85
84077/84077 - 14s - loss: 2.0494e-04 - val_loss: 2.0304e-04 - 14s/epoch - 164us/sample
Epoch 58/85
84077/84077 - 14s - loss: 2.0373e-04 - val_loss: 1.9970e-04 - 14s/epoch - 162us/sample
Epoch 59/85
84077/84077 - 14s - loss: 2.0301e-04 - val_loss: 1.9911e-04 - 14s/epoch - 162us/sample
Epoch 60/85
84077/84077 - 14s - loss: 2.0230e-04 - val_loss: 1.9842e-04 - 14s/epoch - 162us/sample
Epoch 61/85
84077/84077 - 14s - loss: 2.0187e-04 - val_loss: 1.9820e-04 - 14s/epoch - 162us/sample
Epoch 62/85
84077/84077 - 14s - loss: 2.0142e-04 - val_loss: 1.9792e-04 - 14s/epoch - 164us/sample
Epoch 63/85
84077/84077 - 14s - loss: 2.0020e-04 - val_loss: 1.9679e-04 - 14s/epoch - 162us/sample
Epoch 64/85
84077/84077 - 14s - loss: 2.0012e-04 - val_loss: 1.9778e-04 - 14s/epoch - 162us/sample
Epoch 65/85
84077/84077 - 14s - loss: 1.9910e-04 - val_loss: 1.9300e-04 - 14s/epoch - 162us/sample
Epoch 66/85
84077/84077 - 14s - loss: 1.9881e-04 - val_loss: 1.9489e-04 - 14s/epoch - 162us/sample
Epoch 67/85
84077/84077 - 14s - loss: 1.9824e-04 - val_loss: 1.9263e-04 - 14s/epoch - 164us/sample
Epoch 68/85
84077/84077 - 14s - loss: 1.9761e-04 - val_loss: 1.9394e-04 - 14s/epoch - 162us/sample
Epoch 69/85
84077/84077 - 14s - loss: 1.9749e-04 - val_loss: 1.9433e-04 - 14s/epoch - 162us/sample
Epoch 70/85
84077/84077 - 14s - loss: 1.9719e-04 - val_loss: 1.9159e-04 - 14s/epoch - 162us/sample
Epoch 71/85
84077/84077 - 14s - loss: 1.9769e-04 - val_loss: 1.9361e-04 - 14s/epoch - 163us/sample
Epoch 72/85
84077/84077 - 14s - loss: 1.9620e-04 - val_loss: 1.9464e-04 - 14s/epoch - 165us/sample
Epoch 73/85
84077/84077 - 14s - loss: 1.9573e-04 - val_loss: 1.9287e-04 - 14s/epoch - 162us/sample
Epoch 74/85
84077/84077 - 14s - loss: 1.9460e-04 - val_loss: 1.9427e-04 - 14s/epoch - 162us/sample
Epoch 75/85
84077/84077 - 14s - loss: 1.9510e-04 - val_loss: 1.9147e-04 - 14s/epoch - 162us/sample
Epoch 76/85
84077/84077 - 14s - loss: 1.9446e-04 - val_loss: 1.9399e-04 - 14s/epoch - 162us/sample
Epoch 77/85
84077/84077 - 14s - loss: 1.9379e-04 - val_loss: 1.9110e-04 - 14s/epoch - 164us/sample
Epoch 78/85
84077/84077 - 14s - loss: 1.9402e-04 - val_loss: 1.8893e-04 - 14s/epoch - 162us/sample
Epoch 79/85
84077/84077 - 14s - loss: 1.9348e-04 - val_loss: 1.9103e-04 - 14s/epoch - 162us/sample
Epoch 80/85
84077/84077 - 14s - loss: 1.9298e-04 - val_loss: 1.8956e-04 - 14s/epoch - 162us/sample
Epoch 81/85
84077/84077 - 14s - loss: 1.9285e-04 - val_loss: 1.8970e-04 - 14s/epoch - 162us/sample
Epoch 82/85
84077/84077 - 14s - loss: 1.9222e-04 - val_loss: 1.9014e-04 - 14s/epoch - 164us/sample
Epoch 83/85
84077/84077 - 14s - loss: 1.9235e-04 - val_loss: 1.8812e-04 - 14s/epoch - 162us/sample
Epoch 84/85
84077/84077 - 14s - loss: 1.9175e-04 - val_loss: 1.8862e-04 - 14s/epoch - 162us/sample
Epoch 85/85
84077/84077 - 14s - loss: 1.9112e-04 - val_loss: 1.8735e-04 - 14s/epoch - 162us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018735276155389214
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 09:03:06.029742: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_81/outputlayer/BiasAdd' id:89508 op device:{requested: '', assigned: ''} def:{{{node decoder_model_81/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_81/outputlayer/MatMul, decoder_model_81/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02181970656678021
cosine 0.021567865735915952
MAE: 0.0019285954815695453
RMSE: 0.009262239066366425
r2: 0.9331890183696473
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 85, 0.001, 0.2, 188, 0.00019112243964290954, 0.00018735276155389214, 0.02181970656678021, 0.021567865735915952, 0.0019285954815695453, 0.009262239066366425, 0.9331890183696473, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0012000000000000001 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_207 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_207 (ReLU)               (None, 1980)         0           ['batch_normalization_207[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_207[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_207[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 09:03:43.503873: W tensorflow/c/c_api.cc:291] Operation '{name:'training_112/Adam/batch_normalization_209/beta/v/Assign' id:91454 op device:{requested: '', assigned: ''} def:{{{node training_112/Adam/batch_normalization_209/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_112/Adam/batch_normalization_209/beta/v, training_112/Adam/batch_normalization_209/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 09:04:05.448565: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_82/mul' id:90792 op device:{requested: '', assigned: ''} def:{{{node loss_82/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_82/mul/x, loss_82/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 37s - loss: 0.0359 - val_loss: 0.0017 - 37s/epoch - 436us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0016 - val_loss: 0.0015 - 14s/epoch - 163us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0030 - val_loss: 0.0012 - 14s/epoch - 161us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 162us/sample
Epoch 5/90
84077/84077 - 14s - loss: 0.0011 - val_loss: 8.2735e-04 - 14s/epoch - 162us/sample
Epoch 6/90
84077/84077 - 14s - loss: 8.6710e-04 - val_loss: 7.1720e-04 - 14s/epoch - 162us/sample
Epoch 7/90
84077/84077 - 14s - loss: 8.7591e-04 - val_loss: 6.5541e-04 - 14s/epoch - 161us/sample
Epoch 8/90
84077/84077 - 14s - loss: 6.6142e-04 - val_loss: 5.6133e-04 - 14s/epoch - 164us/sample
Epoch 9/90
84077/84077 - 14s - loss: 5.7995e-04 - val_loss: 4.9736e-04 - 14s/epoch - 162us/sample
Epoch 10/90
84077/84077 - 14s - loss: 5.1735e-04 - val_loss: 4.4039e-04 - 14s/epoch - 162us/sample
Epoch 11/90
84077/84077 - 14s - loss: 4.6431e-04 - val_loss: 3.8474e-04 - 14s/epoch - 161us/sample
Epoch 12/90
84077/84077 - 14s - loss: 4.1336e-04 - val_loss: 3.5187e-04 - 14s/epoch - 162us/sample
Epoch 13/90
84077/84077 - 14s - loss: 3.8292e-04 - val_loss: 3.5109e-04 - 14s/epoch - 164us/sample
Epoch 14/90
84077/84077 - 14s - loss: 3.5370e-04 - val_loss: 3.0514e-04 - 14s/epoch - 162us/sample
Epoch 15/90
84077/84077 - 14s - loss: 3.3245e-04 - val_loss: 2.8432e-04 - 14s/epoch - 162us/sample
Epoch 16/90
84077/84077 - 14s - loss: 3.2220e-04 - val_loss: 2.7687e-04 - 14s/epoch - 161us/sample
Epoch 17/90
84077/84077 - 14s - loss: 3.0256e-04 - val_loss: 2.6340e-04 - 14s/epoch - 162us/sample
Epoch 18/90
84077/84077 - 14s - loss: 2.9148e-04 - val_loss: 2.5980e-04 - 14s/epoch - 164us/sample
Epoch 19/90
84077/84077 - 14s - loss: 2.8021e-04 - val_loss: 2.4629e-04 - 14s/epoch - 162us/sample
Epoch 20/90
84077/84077 - 14s - loss: 2.7316e-04 - val_loss: 2.4179e-04 - 14s/epoch - 162us/sample
Epoch 21/90
84077/84077 - 14s - loss: 2.6330e-04 - val_loss: 2.3380e-04 - 14s/epoch - 161us/sample
Epoch 22/90
84077/84077 - 14s - loss: 2.5848e-04 - val_loss: 2.3091e-04 - 14s/epoch - 162us/sample
Epoch 23/90
84077/84077 - 14s - loss: 2.5229e-04 - val_loss: 2.3035e-04 - 14s/epoch - 164us/sample
Epoch 24/90
84077/84077 - 14s - loss: 2.4739e-04 - val_loss: 2.2769e-04 - 14s/epoch - 162us/sample
Epoch 25/90
84077/84077 - 14s - loss: 2.4308e-04 - val_loss: 2.2125e-04 - 14s/epoch - 162us/sample
Epoch 26/90
84077/84077 - 14s - loss: 2.3879e-04 - val_loss: 2.1642e-04 - 14s/epoch - 162us/sample
Epoch 27/90
84077/84077 - 14s - loss: 2.3559e-04 - val_loss: 2.1474e-04 - 14s/epoch - 161us/sample
Epoch 28/90
84077/84077 - 14s - loss: 2.3227e-04 - val_loss: 2.1640e-04 - 14s/epoch - 164us/sample
Epoch 29/90
84077/84077 - 14s - loss: 2.2820e-04 - val_loss: 2.1273e-04 - 14s/epoch - 162us/sample
Epoch 30/90
84077/84077 - 14s - loss: 2.2622e-04 - val_loss: 2.1158e-04 - 14s/epoch - 162us/sample
Epoch 31/90
84077/84077 - 14s - loss: 2.2395e-04 - val_loss: 2.1384e-04 - 14s/epoch - 161us/sample
Epoch 32/90
84077/84077 - 14s - loss: 2.2240e-04 - val_loss: 2.0655e-04 - 14s/epoch - 162us/sample
Epoch 33/90
84077/84077 - 14s - loss: 2.2034e-04 - val_loss: 2.0668e-04 - 14s/epoch - 163us/sample
Epoch 34/90
84077/84077 - 14s - loss: 2.1815e-04 - val_loss: 2.0639e-04 - 14s/epoch - 163us/sample
Epoch 35/90
84077/84077 - 14s - loss: 2.1648e-04 - val_loss: 2.1092e-04 - 14s/epoch - 161us/sample
Epoch 36/90
84077/84077 - 14s - loss: 2.1485e-04 - val_loss: 2.0474e-04 - 14s/epoch - 162us/sample
Epoch 37/90
84077/84077 - 14s - loss: 2.1294e-04 - val_loss: 2.0118e-04 - 14s/epoch - 162us/sample
Epoch 38/90
84077/84077 - 14s - loss: 2.1183e-04 - val_loss: 2.0003e-04 - 14s/epoch - 162us/sample
Epoch 39/90
84077/84077 - 14s - loss: 2.1106e-04 - val_loss: 1.9865e-04 - 14s/epoch - 172us/sample
Epoch 40/90
84077/84077 - 14s - loss: 2.0988e-04 - val_loss: 1.9987e-04 - 14s/epoch - 162us/sample
Epoch 41/90
84077/84077 - 14s - loss: 2.0788e-04 - val_loss: 1.9920e-04 - 14s/epoch - 162us/sample
Epoch 42/90
84077/84077 - 14s - loss: 2.0774e-04 - val_loss: 1.9825e-04 - 14s/epoch - 162us/sample
Epoch 43/90
84077/84077 - 14s - loss: 2.0688e-04 - val_loss: 2.0007e-04 - 14s/epoch - 162us/sample
Epoch 44/90
84077/84077 - 14s - loss: 2.0492e-04 - val_loss: 1.9700e-04 - 14s/epoch - 164us/sample
Epoch 45/90
84077/84077 - 14s - loss: 2.0431e-04 - val_loss: 1.9667e-04 - 14s/epoch - 162us/sample
Epoch 46/90
84077/84077 - 14s - loss: 2.0327e-04 - val_loss: 1.9433e-04 - 14s/epoch - 162us/sample
Epoch 47/90
84077/84077 - 14s - loss: 2.0290e-04 - val_loss: 1.9392e-04 - 14s/epoch - 161us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.0114e-04 - val_loss: 1.9300e-04 - 14s/epoch - 163us/sample
Epoch 49/90
84077/84077 - 14s - loss: 2.0091e-04 - val_loss: 1.9209e-04 - 14s/epoch - 164us/sample
Epoch 50/90
84077/84077 - 14s - loss: 1.9990e-04 - val_loss: 1.9166e-04 - 14s/epoch - 162us/sample
Epoch 51/90
84077/84077 - 14s - loss: 1.9908e-04 - val_loss: 1.8858e-04 - 14s/epoch - 161us/sample
Epoch 52/90
84077/84077 - 14s - loss: 1.9870e-04 - val_loss: 1.8984e-04 - 14s/epoch - 162us/sample
Epoch 53/90
84077/84077 - 14s - loss: 1.9800e-04 - val_loss: 1.8782e-04 - 14s/epoch - 162us/sample
Epoch 54/90
84077/84077 - 14s - loss: 1.9702e-04 - val_loss: 1.9229e-04 - 14s/epoch - 164us/sample
Epoch 55/90
84077/84077 - 14s - loss: 1.9616e-04 - val_loss: 1.8572e-04 - 14s/epoch - 162us/sample
Epoch 56/90
84077/84077 - 14s - loss: 1.9671e-04 - val_loss: 1.8657e-04 - 14s/epoch - 162us/sample
Epoch 57/90
84077/84077 - 14s - loss: 1.9552e-04 - val_loss: 1.8489e-04 - 14s/epoch - 162us/sample
Epoch 58/90
84077/84077 - 14s - loss: 1.9476e-04 - val_loss: 1.8596e-04 - 14s/epoch - 162us/sample
Epoch 59/90
84077/84077 - 14s - loss: 1.9436e-04 - val_loss: 1.8248e-04 - 14s/epoch - 164us/sample
Epoch 60/90
84077/84077 - 14s - loss: 1.9364e-04 - val_loss: 1.8548e-04 - 14s/epoch - 162us/sample
Epoch 61/90
84077/84077 - 14s - loss: 1.9319e-04 - val_loss: 1.8669e-04 - 14s/epoch - 162us/sample
Epoch 62/90
84077/84077 - 14s - loss: 1.9297e-04 - val_loss: 1.7937e-04 - 14s/epoch - 162us/sample
Epoch 63/90
84077/84077 - 14s - loss: 1.9150e-04 - val_loss: 1.8397e-04 - 14s/epoch - 162us/sample
Epoch 64/90
84077/84077 - 14s - loss: 1.9225e-04 - val_loss: 1.8125e-04 - 14s/epoch - 164us/sample
Epoch 65/90
84077/84077 - 14s - loss: 1.9099e-04 - val_loss: 1.8065e-04 - 14s/epoch - 162us/sample
Epoch 66/90
84077/84077 - 14s - loss: 1.9103e-04 - val_loss: 1.8103e-04 - 14s/epoch - 162us/sample
Epoch 67/90
84077/84077 - 14s - loss: 1.9139e-04 - val_loss: 1.7958e-04 - 14s/epoch - 161us/sample
Epoch 68/90
84077/84077 - 14s - loss: 1.9009e-04 - val_loss: 1.7865e-04 - 14s/epoch - 162us/sample
Epoch 69/90
84077/84077 - 14s - loss: 1.9030e-04 - val_loss: 1.7945e-04 - 14s/epoch - 164us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.8921e-04 - val_loss: 1.8111e-04 - 14s/epoch - 163us/sample
Epoch 71/90
84077/84077 - 14s - loss: 1.8888e-04 - val_loss: 1.7696e-04 - 14s/epoch - 161us/sample
Epoch 72/90
84077/84077 - 14s - loss: 1.8880e-04 - val_loss: 1.8210e-04 - 14s/epoch - 162us/sample
Epoch 73/90
84077/84077 - 14s - loss: 1.8839e-04 - val_loss: 1.8096e-04 - 14s/epoch - 162us/sample
Epoch 74/90
84077/84077 - 14s - loss: 1.8830e-04 - val_loss: 1.7853e-04 - 14s/epoch - 164us/sample
Epoch 75/90
84077/84077 - 14s - loss: 1.8746e-04 - val_loss: 1.7632e-04 - 14s/epoch - 162us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.8748e-04 - val_loss: 1.7818e-04 - 14s/epoch - 162us/sample
Epoch 77/90
84077/84077 - 14s - loss: 1.8713e-04 - val_loss: 1.7431e-04 - 14s/epoch - 162us/sample
Epoch 78/90
84077/84077 - 14s - loss: 1.8678e-04 - val_loss: 1.7676e-04 - 14s/epoch - 162us/sample
Epoch 79/90
84077/84077 - 14s - loss: 1.8670e-04 - val_loss: 1.7546e-04 - 14s/epoch - 163us/sample
Epoch 80/90
84077/84077 - 14s - loss: 1.8652e-04 - val_loss: 1.7680e-04 - 14s/epoch - 163us/sample
Epoch 81/90
84077/84077 - 14s - loss: 1.8570e-04 - val_loss: 1.7640e-04 - 14s/epoch - 162us/sample
Epoch 82/90
84077/84077 - 14s - loss: 1.8561e-04 - val_loss: 1.7728e-04 - 14s/epoch - 161us/sample
Epoch 83/90
84077/84077 - 14s - loss: 1.8576e-04 - val_loss: 1.7729e-04 - 14s/epoch - 162us/sample
Epoch 84/90
84077/84077 - 14s - loss: 1.8500e-04 - val_loss: 1.7640e-04 - 14s/epoch - 163us/sample
Epoch 85/90
84077/84077 - 14s - loss: 1.8502e-04 - val_loss: 1.7564e-04 - 14s/epoch - 163us/sample
Epoch 86/90
84077/84077 - 14s - loss: 1.8489e-04 - val_loss: 1.7311e-04 - 14s/epoch - 162us/sample
Epoch 87/90
84077/84077 - 14s - loss: 1.8442e-04 - val_loss: 1.7714e-04 - 14s/epoch - 161us/sample
Epoch 88/90
84077/84077 - 14s - loss: 1.8424e-04 - val_loss: 1.7352e-04 - 14s/epoch - 163us/sample
Epoch 89/90
84077/84077 - 14s - loss: 1.8441e-04 - val_loss: 1.7534e-04 - 14s/epoch - 162us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.8366e-04 - val_loss: 1.7425e-04 - 14s/epoch - 164us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017425403281189663
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 09:24:27.890256: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_82/outputlayer/BiasAdd' id:90763 op device:{requested: '', assigned: ''} def:{{{node decoder_model_82/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_82/outputlayer/MatMul, decoder_model_82/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.020635337240175397
cosine 0.020384633501997682
MAE: 0.0018520175826935322
RMSE: 0.008651094291908632
r2: 0.9422373453465464
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.0012000000000000001, 0.2, 188, 0.00018365847456467728, 0.00017425403281189663, 0.020635337240175397, 0.020384633501997682, 0.0018520175826935322, 0.008651094291908632, 0.9422373453465464, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.0008 16 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_210 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_210 (ReLU)               (None, 1603)         0           ['batch_normalization_210[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_210[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_210[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 09:25:05.363482: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_83/bias/Assign' id:91765 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_83/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_83/bias, dense_dec1_83/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 09:25:49.169998: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_83/mul' id:92047 op device:{requested: '', assigned: ''} def:{{{node loss_83/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_83/mul/x, loss_83/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 60s - loss: 0.0050 - val_loss: 0.0015 - 60s/epoch - 713us/sample
Epoch 2/85
84077/84077 - 37s - loss: 0.0012 - val_loss: 8.1713e-04 - 37s/epoch - 435us/sample
Epoch 3/85
84077/84077 - 37s - loss: 8.1366e-04 - val_loss: 6.6452e-04 - 37s/epoch - 436us/sample
Epoch 4/85
84077/84077 - 37s - loss: 6.5816e-04 - val_loss: 6.0144e-04 - 37s/epoch - 436us/sample
Epoch 5/85
84077/84077 - 37s - loss: 5.8743e-04 - val_loss: 5.5367e-04 - 37s/epoch - 437us/sample
Epoch 6/85
84077/84077 - 37s - loss: 5.4010e-04 - val_loss: 5.3979e-04 - 37s/epoch - 435us/sample
Epoch 7/85
84077/84077 - 37s - loss: 5.0854e-04 - val_loss: 4.8871e-04 - 37s/epoch - 437us/sample
Epoch 8/85
84077/84077 - 37s - loss: 4.8382e-04 - val_loss: 4.5566e-04 - 37s/epoch - 435us/sample
Epoch 9/85
84077/84077 - 37s - loss: 4.6713e-04 - val_loss: 4.5031e-04 - 37s/epoch - 438us/sample
Epoch 10/85
84077/84077 - 37s - loss: 4.5356e-04 - val_loss: 4.4062e-04 - 37s/epoch - 436us/sample
Epoch 11/85
84077/84077 - 37s - loss: 4.4004e-04 - val_loss: 4.0699e-04 - 37s/epoch - 437us/sample
Epoch 12/85
84077/84077 - 37s - loss: 4.2980e-04 - val_loss: 3.9954e-04 - 37s/epoch - 436us/sample
Epoch 13/85
84077/84077 - 37s - loss: 4.1505e-04 - val_loss: 3.9078e-04 - 37s/epoch - 438us/sample
Epoch 14/85
84077/84077 - 37s - loss: 4.0433e-04 - val_loss: 3.7738e-04 - 37s/epoch - 434us/sample
Epoch 15/85
84077/84077 - 37s - loss: 3.9694e-04 - val_loss: 3.7666e-04 - 37s/epoch - 437us/sample
Epoch 16/85
84077/84077 - 37s - loss: 3.8418e-04 - val_loss: 3.6245e-04 - 37s/epoch - 437us/sample
Epoch 17/85
84077/84077 - 37s - loss: 3.8060e-04 - val_loss: 3.5598e-04 - 37s/epoch - 434us/sample
Epoch 18/85
84077/84077 - 37s - loss: 3.7126e-04 - val_loss: 3.4599e-04 - 37s/epoch - 438us/sample
Epoch 19/85
84077/84077 - 37s - loss: 3.6177e-04 - val_loss: 3.3334e-04 - 37s/epoch - 435us/sample
Epoch 20/85
84077/84077 - 37s - loss: 3.5983e-04 - val_loss: 3.3285e-04 - 37s/epoch - 438us/sample
Epoch 21/85
84077/84077 - 37s - loss: 3.5161e-04 - val_loss: 3.3059e-04 - 37s/epoch - 436us/sample
Epoch 22/85
84077/84077 - 37s - loss: 3.4596e-04 - val_loss: 3.2098e-04 - 37s/epoch - 438us/sample
Epoch 23/85
84077/84077 - 37s - loss: 3.4522e-04 - val_loss: 3.2523e-04 - 37s/epoch - 434us/sample
Epoch 24/85
84077/84077 - 37s - loss: 3.4390e-04 - val_loss: 3.2196e-04 - 37s/epoch - 439us/sample
Epoch 25/85
84077/84077 - 37s - loss: 3.3415e-04 - val_loss: 3.1333e-04 - 37s/epoch - 435us/sample
Epoch 26/85
84077/84077 - 37s - loss: 3.2981e-04 - val_loss: 3.0804e-04 - 37s/epoch - 435us/sample
Epoch 27/85
84077/84077 - 37s - loss: 3.3133e-04 - val_loss: 3.1044e-04 - 37s/epoch - 438us/sample
Epoch 28/85
84077/84077 - 37s - loss: 3.2490e-04 - val_loss: 3.0481e-04 - 37s/epoch - 435us/sample
Epoch 29/85
84077/84077 - 37s - loss: 3.2159e-04 - val_loss: 3.1601e-04 - 37s/epoch - 436us/sample
Epoch 30/85
84077/84077 - 37s - loss: 3.2225e-04 - val_loss: 3.0350e-04 - 37s/epoch - 435us/sample
Epoch 31/85
84077/84077 - 37s - loss: 3.2098e-04 - val_loss: 3.1406e-04 - 37s/epoch - 438us/sample
Epoch 32/85
84077/84077 - 37s - loss: 3.1638e-04 - val_loss: 3.0473e-04 - 37s/epoch - 435us/sample
Epoch 33/85
84077/84077 - 37s - loss: 3.1385e-04 - val_loss: 2.9833e-04 - 37s/epoch - 438us/sample
Epoch 34/85
84077/84077 - 37s - loss: 3.1160e-04 - val_loss: 2.9659e-04 - 37s/epoch - 436us/sample
Epoch 35/85
84077/84077 - 37s - loss: 3.0815e-04 - val_loss: 2.9308e-04 - 37s/epoch - 437us/sample
Epoch 36/85
84077/84077 - 37s - loss: 3.0921e-04 - val_loss: 2.9909e-04 - 37s/epoch - 436us/sample
Epoch 37/85
84077/84077 - 37s - loss: 3.1141e-04 - val_loss: 2.9182e-04 - 37s/epoch - 438us/sample
Epoch 38/85
84077/84077 - 37s - loss: 3.0514e-04 - val_loss: 2.9036e-04 - 37s/epoch - 435us/sample
Epoch 39/85
84077/84077 - 37s - loss: 3.0342e-04 - val_loss: 2.9592e-04 - 37s/epoch - 437us/sample
Epoch 40/85
84077/84077 - 37s - loss: 3.0117e-04 - val_loss: 2.8264e-04 - 37s/epoch - 437us/sample
Epoch 41/85
84077/84077 - 37s - loss: 3.0159e-04 - val_loss: 2.8048e-04 - 37s/epoch - 436us/sample
Epoch 42/85
84077/84077 - 37s - loss: 3.0267e-04 - val_loss: 2.9842e-04 - 37s/epoch - 438us/sample
Epoch 43/85
84077/84077 - 37s - loss: 2.9748e-04 - val_loss: 2.8880e-04 - 37s/epoch - 436us/sample
Epoch 44/85
84077/84077 - 37s - loss: 2.9942e-04 - val_loss: 2.7749e-04 - 37s/epoch - 437us/sample
Epoch 45/85
84077/84077 - 37s - loss: 2.9382e-04 - val_loss: 2.7841e-04 - 37s/epoch - 436us/sample
Epoch 46/85
84077/84077 - 37s - loss: 2.9612e-04 - val_loss: 2.7999e-04 - 37s/epoch - 438us/sample
Epoch 47/85
84077/84077 - 37s - loss: 2.9602e-04 - val_loss: 2.8883e-04 - 37s/epoch - 435us/sample
Epoch 48/85
84077/84077 - 37s - loss: 2.9315e-04 - val_loss: 2.7994e-04 - 37s/epoch - 437us/sample
Epoch 49/85
84077/84077 - 37s - loss: 2.9184e-04 - val_loss: 2.8227e-04 - 37s/epoch - 436us/sample
Epoch 50/85
84077/84077 - 37s - loss: 2.8850e-04 - val_loss: 2.7156e-04 - 37s/epoch - 438us/sample
Epoch 51/85
84077/84077 - 37s - loss: 2.9002e-04 - val_loss: 2.8241e-04 - 37s/epoch - 436us/sample
Epoch 52/85
84077/84077 - 37s - loss: 2.8736e-04 - val_loss: 2.7934e-04 - 37s/epoch - 438us/sample
Epoch 53/85
84077/84077 - 37s - loss: 2.8649e-04 - val_loss: 2.7730e-04 - 37s/epoch - 434us/sample
Epoch 54/85
84077/84077 - 37s - loss: 2.8638e-04 - val_loss: 2.7356e-04 - 37s/epoch - 439us/sample
Epoch 55/85
84077/84077 - 37s - loss: 2.8371e-04 - val_loss: 2.7637e-04 - 37s/epoch - 434us/sample
Epoch 56/85
84077/84077 - 37s - loss: 2.8452e-04 - val_loss: 2.8783e-04 - 37s/epoch - 436us/sample
Epoch 57/85
84077/84077 - 37s - loss: 2.8284e-04 - val_loss: 2.7384e-04 - 37s/epoch - 437us/sample
Epoch 58/85
84077/84077 - 37s - loss: 2.8278e-04 - val_loss: 2.7613e-04 - 37s/epoch - 436us/sample
Epoch 59/85
84077/84077 - 37s - loss: 2.8890e-04 - val_loss: 3.5718e-04 - 37s/epoch - 437us/sample
Epoch 60/85
84077/84077 - 37s - loss: 2.8389e-04 - val_loss: 2.7064e-04 - 37s/epoch - 435us/sample
Epoch 61/85
84077/84077 - 37s - loss: 2.8154e-04 - val_loss: 2.7254e-04 - 37s/epoch - 438us/sample
Epoch 62/85
84077/84077 - 37s - loss: 2.7823e-04 - val_loss: 2.7180e-04 - 37s/epoch - 435us/sample
Epoch 63/85
84077/84077 - 37s - loss: 2.7907e-04 - val_loss: 2.6275e-04 - 37s/epoch - 438us/sample
Epoch 64/85
84077/84077 - 37s - loss: 2.7599e-04 - val_loss: 2.6564e-04 - 37s/epoch - 435us/sample
Epoch 65/85
84077/84077 - 37s - loss: 2.8384e-04 - val_loss: 2.8066e-04 - 37s/epoch - 438us/sample
Epoch 66/85
84077/84077 - 37s - loss: 2.7741e-04 - val_loss: 2.6258e-04 - 37s/epoch - 435us/sample
Epoch 67/85
84077/84077 - 37s - loss: 2.7709e-04 - val_loss: 2.6956e-04 - 37s/epoch - 438us/sample
Epoch 68/85
84077/84077 - 37s - loss: 2.7656e-04 - val_loss: 2.6236e-04 - 37s/epoch - 435us/sample
Epoch 69/85
84077/84077 - 37s - loss: 2.7487e-04 - val_loss: 2.6119e-04 - 37s/epoch - 439us/sample
Epoch 70/85
84077/84077 - 37s - loss: 2.7635e-04 - val_loss: 2.6439e-04 - 37s/epoch - 435us/sample
Epoch 71/85
84077/84077 - 37s - loss: 2.7324e-04 - val_loss: 2.5970e-04 - 37s/epoch - 437us/sample
Epoch 72/85
84077/84077 - 37s - loss: 2.7321e-04 - val_loss: 2.5951e-04 - 37s/epoch - 436us/sample
Epoch 73/85
84077/84077 - 37s - loss: 2.7492e-04 - val_loss: 2.6160e-04 - 37s/epoch - 438us/sample
Epoch 74/85
84077/84077 - 37s - loss: 2.7578e-04 - val_loss: 2.6831e-04 - 37s/epoch - 435us/sample
Epoch 75/85
84077/84077 - 37s - loss: 2.7509e-04 - val_loss: 2.6144e-04 - 37s/epoch - 436us/sample
Epoch 76/85
84077/84077 - 37s - loss: 2.7469e-04 - val_loss: 2.6665e-04 - 37s/epoch - 438us/sample
Epoch 77/85
84077/84077 - 37s - loss: 2.7090e-04 - val_loss: 2.5827e-04 - 37s/epoch - 436us/sample
Epoch 78/85
84077/84077 - 37s - loss: 2.7368e-04 - val_loss: 2.6192e-04 - 37s/epoch - 436us/sample
Epoch 79/85
84077/84077 - 37s - loss: 2.7031e-04 - val_loss: 2.6416e-04 - 37s/epoch - 436us/sample
Epoch 80/85
84077/84077 - 37s - loss: 2.7050e-04 - val_loss: 2.6366e-04 - 37s/epoch - 437us/sample
Epoch 81/85
84077/84077 - 37s - loss: 2.6984e-04 - val_loss: 2.5745e-04 - 37s/epoch - 436us/sample
Epoch 82/85
84077/84077 - 37s - loss: 2.6971e-04 - val_loss: 2.6026e-04 - 37s/epoch - 438us/sample
Epoch 83/85
84077/84077 - 37s - loss: 2.6808e-04 - val_loss: 2.5955e-04 - 37s/epoch - 434us/sample
Epoch 84/85
84077/84077 - 37s - loss: 2.7047e-04 - val_loss: 2.6032e-04 - 37s/epoch - 438us/sample
Epoch 85/85
84077/84077 - 37s - loss: 2.6800e-04 - val_loss: 2.6343e-04 - 37s/epoch - 435us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.000263429618432294
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 10:17:20.584458: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_83/outputlayer/BiasAdd' id:92018 op device:{requested: '', assigned: ''} def:{{{node decoder_model_83/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_83/outputlayer/MatMul, decoder_model_83/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.034548826174770904
cosine 0.034142005091201987
MAE: 0.00253607747721503
RMSE: 0.012142157704334685
r2: 0.8848856866186446
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 16, 85, 0.0008, 0.2, 188, 0.0002679974565649047, 0.000263429618432294, 0.034548826174770904, 0.034142005091201987, 0.00253607747721503, 0.012142157704334685, 0.8848856866186446, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.001 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_213 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_213 (ReLU)               (None, 2074)         0           ['batch_normalization_213[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_213[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_213[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 10:17:57.887501: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_84/bias/Assign' id:92962 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_84/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_84/bias, bottleneck_zmean_84/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 10:18:20.052422: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_84/mul' id:93309 op device:{requested: '', assigned: ''} def:{{{node loss_84/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_84/mul/x, loss_84/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 37s - loss: 0.0050 - val_loss: 0.0012 - 37s/epoch - 442us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0010 - val_loss: 7.6057e-04 - 14s/epoch - 170us/sample
Epoch 3/85
84077/84077 - 14s - loss: 0.0010 - val_loss: 8.6294e-04 - 14s/epoch - 168us/sample
Epoch 4/85
84077/84077 - 14s - loss: 7.3503e-04 - val_loss: 5.9196e-04 - 14s/epoch - 168us/sample
Epoch 5/85
84077/84077 - 14s - loss: 7.5123e-04 - val_loss: 4.9653e-04 - 14s/epoch - 168us/sample
Epoch 6/85
84077/84077 - 14s - loss: 5.0350e-04 - val_loss: 7.4587e-04 - 14s/epoch - 168us/sample
Epoch 7/85
84077/84077 - 14s - loss: 4.3952e-04 - val_loss: 3.6554e-04 - 14s/epoch - 171us/sample
Epoch 8/85
84077/84077 - 14s - loss: 3.6818e-04 - val_loss: 3.1669e-04 - 14s/epoch - 169us/sample
Epoch 9/85
84077/84077 - 14s - loss: 3.3047e-04 - val_loss: 2.8249e-04 - 14s/epoch - 168us/sample
Epoch 10/85
84077/84077 - 14s - loss: 3.0509e-04 - val_loss: 2.6355e-04 - 14s/epoch - 169us/sample
Epoch 11/85
84077/84077 - 14s - loss: 2.8281e-04 - val_loss: 2.5236e-04 - 14s/epoch - 168us/sample
Epoch 12/85
84077/84077 - 14s - loss: 2.6341e-04 - val_loss: 2.3579e-04 - 14s/epoch - 171us/sample
Epoch 13/85
84077/84077 - 14s - loss: 2.4549e-04 - val_loss: 2.0721e-04 - 14s/epoch - 168us/sample
Epoch 14/85
84077/84077 - 14s - loss: 2.3362e-04 - val_loss: 2.0095e-04 - 14s/epoch - 168us/sample
Epoch 15/85
84077/84077 - 14s - loss: 2.2208e-04 - val_loss: 1.9519e-04 - 14s/epoch - 169us/sample
Epoch 16/85
84077/84077 - 14s - loss: 2.1335e-04 - val_loss: 1.8506e-04 - 14s/epoch - 168us/sample
Epoch 17/85
84077/84077 - 14s - loss: 2.0446e-04 - val_loss: 1.7928e-04 - 14s/epoch - 171us/sample
Epoch 18/85
84077/84077 - 14s - loss: 1.9809e-04 - val_loss: 1.7381e-04 - 14s/epoch - 169us/sample
Epoch 19/85
84077/84077 - 14s - loss: 1.9193e-04 - val_loss: 1.6703e-04 - 14s/epoch - 168us/sample
Epoch 20/85
84077/84077 - 14s - loss: 1.8636e-04 - val_loss: 1.6491e-04 - 14s/epoch - 169us/sample
Epoch 21/85
84077/84077 - 14s - loss: 1.8206e-04 - val_loss: 1.6182e-04 - 14s/epoch - 168us/sample
Epoch 22/85
84077/84077 - 14s - loss: 1.7838e-04 - val_loss: 1.5690e-04 - 14s/epoch - 171us/sample
Epoch 23/85
84077/84077 - 14s - loss: 1.7469e-04 - val_loss: 1.5496e-04 - 14s/epoch - 168us/sample
Epoch 24/85
84077/84077 - 14s - loss: 1.7208e-04 - val_loss: 1.5254e-04 - 14s/epoch - 169us/sample
Epoch 25/85
84077/84077 - 14s - loss: 1.6937e-04 - val_loss: 1.5035e-04 - 14s/epoch - 168us/sample
Epoch 26/85
84077/84077 - 14s - loss: 1.6752e-04 - val_loss: 1.4907e-04 - 14s/epoch - 169us/sample
Epoch 27/85
84077/84077 - 14s - loss: 1.6468e-04 - val_loss: 1.4709e-04 - 14s/epoch - 171us/sample
Epoch 28/85
84077/84077 - 14s - loss: 1.6313e-04 - val_loss: 1.4705e-04 - 14s/epoch - 169us/sample
Epoch 29/85
84077/84077 - 14s - loss: 1.6164e-04 - val_loss: 1.4482e-04 - 14s/epoch - 168us/sample
Epoch 30/85
84077/84077 - 14s - loss: 1.5949e-04 - val_loss: 1.4260e-04 - 14s/epoch - 169us/sample
Epoch 31/85
84077/84077 - 14s - loss: 1.5844e-04 - val_loss: 1.4221e-04 - 14s/epoch - 169us/sample
Epoch 32/85
84077/84077 - 14s - loss: 1.5717e-04 - val_loss: 1.4181e-04 - 14s/epoch - 171us/sample
Epoch 33/85
84077/84077 - 14s - loss: 1.5637e-04 - val_loss: 1.4172e-04 - 14s/epoch - 169us/sample
Epoch 34/85
84077/84077 - 14s - loss: 1.5469e-04 - val_loss: 1.4156e-04 - 14s/epoch - 169us/sample
Epoch 35/85
84077/84077 - 14s - loss: 1.5327e-04 - val_loss: 1.4018e-04 - 14s/epoch - 168us/sample
Epoch 36/85
84077/84077 - 14s - loss: 1.5220e-04 - val_loss: 1.3810e-04 - 14s/epoch - 169us/sample
Epoch 37/85
84077/84077 - 14s - loss: 1.5159e-04 - val_loss: 1.3749e-04 - 14s/epoch - 171us/sample
Epoch 38/85
84077/84077 - 14s - loss: 1.5067e-04 - val_loss: 1.3537e-04 - 14s/epoch - 169us/sample
Epoch 39/85
84077/84077 - 14s - loss: 1.4906e-04 - val_loss: 1.3628e-04 - 14s/epoch - 169us/sample
Epoch 40/85
84077/84077 - 14s - loss: 1.4898e-04 - val_loss: 1.3632e-04 - 14s/epoch - 168us/sample
Epoch 41/85
84077/84077 - 14s - loss: 1.4807e-04 - val_loss: 1.3552e-04 - 14s/epoch - 169us/sample
Epoch 42/85
84077/84077 - 14s - loss: 1.4707e-04 - val_loss: 1.3460e-04 - 14s/epoch - 171us/sample
Epoch 43/85
84077/84077 - 14s - loss: 1.4683e-04 - val_loss: 1.3603e-04 - 14s/epoch - 168us/sample
Epoch 44/85
84077/84077 - 14s - loss: 1.4624e-04 - val_loss: 1.3399e-04 - 14s/epoch - 169us/sample
Epoch 45/85
84077/84077 - 14s - loss: 1.4523e-04 - val_loss: 1.3277e-04 - 14s/epoch - 168us/sample
Epoch 46/85
84077/84077 - 14s - loss: 1.4501e-04 - val_loss: 1.3412e-04 - 14s/epoch - 169us/sample
Epoch 47/85
84077/84077 - 14s - loss: 1.4411e-04 - val_loss: 1.3361e-04 - 14s/epoch - 171us/sample
Epoch 48/85
84077/84077 - 14s - loss: 1.4336e-04 - val_loss: 1.3374e-04 - 14s/epoch - 168us/sample
Epoch 49/85
84077/84077 - 14s - loss: 1.4336e-04 - val_loss: 1.3142e-04 - 14s/epoch - 168us/sample
Epoch 50/85
84077/84077 - 14s - loss: 1.4293e-04 - val_loss: 1.3149e-04 - 14s/epoch - 169us/sample
Epoch 51/85
84077/84077 - 14s - loss: 1.4194e-04 - val_loss: 1.3013e-04 - 14s/epoch - 169us/sample
Epoch 52/85
84077/84077 - 14s - loss: 1.4164e-04 - val_loss: 1.3099e-04 - 14s/epoch - 171us/sample
Epoch 53/85
84077/84077 - 14s - loss: 1.4100e-04 - val_loss: 1.3139e-04 - 14s/epoch - 168us/sample
Epoch 54/85
84077/84077 - 14s - loss: 1.4099e-04 - val_loss: 1.2932e-04 - 14s/epoch - 169us/sample
Epoch 55/85
84077/84077 - 14s - loss: 1.4045e-04 - val_loss: 1.3002e-04 - 14s/epoch - 168us/sample
Epoch 56/85
84077/84077 - 14s - loss: 1.4027e-04 - val_loss: 1.2839e-04 - 14s/epoch - 169us/sample
Epoch 57/85
84077/84077 - 14s - loss: 1.4098e-04 - val_loss: 1.2898e-04 - 14s/epoch - 171us/sample
Epoch 58/85
84077/84077 - 14s - loss: 1.3974e-04 - val_loss: 1.3067e-04 - 14s/epoch - 169us/sample
Epoch 59/85
84077/84077 - 14s - loss: 1.3898e-04 - val_loss: 1.2854e-04 - 14s/epoch - 168us/sample
Epoch 60/85
84077/84077 - 14s - loss: 1.3840e-04 - val_loss: 1.2937e-04 - 14s/epoch - 169us/sample
Epoch 61/85
84077/84077 - 14s - loss: 1.3836e-04 - val_loss: 1.2907e-04 - 14s/epoch - 169us/sample
Epoch 62/85
84077/84077 - 14s - loss: 1.3795e-04 - val_loss: 1.2939e-04 - 14s/epoch - 171us/sample
Epoch 63/85
84077/84077 - 14s - loss: 1.3775e-04 - val_loss: 1.2769e-04 - 14s/epoch - 168us/sample
Epoch 64/85
84077/84077 - 14s - loss: 1.3708e-04 - val_loss: 1.2891e-04 - 14s/epoch - 168us/sample
Epoch 65/85
84077/84077 - 14s - loss: 1.3731e-04 - val_loss: 1.2715e-04 - 14s/epoch - 168us/sample
Epoch 66/85
84077/84077 - 14s - loss: 1.3714e-04 - val_loss: 1.2673e-04 - 14s/epoch - 169us/sample
Epoch 67/85
84077/84077 - 14s - loss: 1.3648e-04 - val_loss: 1.2746e-04 - 14s/epoch - 171us/sample
Epoch 68/85
84077/84077 - 14s - loss: 1.3601e-04 - val_loss: 1.2868e-04 - 14s/epoch - 168us/sample
Epoch 69/85
84077/84077 - 14s - loss: 1.3580e-04 - val_loss: 1.2666e-04 - 14s/epoch - 168us/sample
Epoch 70/85
84077/84077 - 14s - loss: 1.3587e-04 - val_loss: 1.2717e-04 - 14s/epoch - 169us/sample
Epoch 71/85
84077/84077 - 14s - loss: 1.3537e-04 - val_loss: 1.2716e-04 - 14s/epoch - 169us/sample
Epoch 72/85
84077/84077 - 14s - loss: 1.3532e-04 - val_loss: 1.2576e-04 - 14s/epoch - 171us/sample
Epoch 73/85
84077/84077 - 14s - loss: 1.3487e-04 - val_loss: 1.2532e-04 - 14s/epoch - 169us/sample
Epoch 74/85
84077/84077 - 14s - loss: 1.3485e-04 - val_loss: 1.2586e-04 - 14s/epoch - 168us/sample
Epoch 75/85
84077/84077 - 14s - loss: 1.3459e-04 - val_loss: 1.2507e-04 - 14s/epoch - 169us/sample
Epoch 76/85
84077/84077 - 14s - loss: 1.3424e-04 - val_loss: 1.2566e-04 - 14s/epoch - 170us/sample
Epoch 77/85
84077/84077 - 14s - loss: 1.3411e-04 - val_loss: 1.2609e-04 - 14s/epoch - 170us/sample
Epoch 78/85
84077/84077 - 14s - loss: 1.3394e-04 - val_loss: 1.2535e-04 - 14s/epoch - 169us/sample
Epoch 79/85
84077/84077 - 14s - loss: 1.3371e-04 - val_loss: 1.2500e-04 - 14s/epoch - 168us/sample
Epoch 80/85
84077/84077 - 14s - loss: 1.3362e-04 - val_loss: 1.2637e-04 - 14s/epoch - 169us/sample
Epoch 81/85
84077/84077 - 14s - loss: 1.3334e-04 - val_loss: 1.2562e-04 - 14s/epoch - 170us/sample
Epoch 82/85
84077/84077 - 14s - loss: 1.3326e-04 - val_loss: 1.2517e-04 - 14s/epoch - 170us/sample
Epoch 83/85
84077/84077 - 14s - loss: 1.3313e-04 - val_loss: 1.2435e-04 - 14s/epoch - 169us/sample
Epoch 84/85
84077/84077 - 14s - loss: 1.3279e-04 - val_loss: 1.2589e-04 - 14s/epoch - 169us/sample
Epoch 85/85
84077/84077 - 14s - loss: 1.3271e-04 - val_loss: 1.2451e-04 - 14s/epoch - 169us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012450536142180236
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 10:38:21.515999: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_84/outputlayer/BiasAdd' id:93273 op device:{requested: '', assigned: ''} def:{{{node decoder_model_84/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_84/outputlayer/MatMul, decoder_model_84/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.028380176901801373
cosine 0.028028898538069908
MAE: 0.0022993958582032564
RMSE: 0.009795451073882683
r2: 0.9252494182625678
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 64, 85, 0.001, 0.2, 188, 0.00013270713823293037, 0.00012450536142180236, 0.028380176901801373, 0.028028898538069908, 0.0022993958582032564, 0.009795451073882683, 0.9252494182625678, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.001 16 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_216 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_216 (ReLU)               (None, 2074)         0           ['batch_normalization_216[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_216[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_216[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 10:38:58.991169: W tensorflow/c/c_api.cc:291] Operation '{name:'training_118/Adam/batch_normalization_216/gamma/m/Assign' id:95054 op device:{requested: '', assigned: ''} def:{{{node training_118/Adam/batch_normalization_216/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_118/Adam/batch_normalization_216/gamma/m, training_118/Adam/batch_normalization_216/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 10:39:43.930736: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_85/mul' id:94587 op device:{requested: '', assigned: ''} def:{{{node loss_85/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_85/mul/x, loss_85/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 61s - loss: 0.0072 - val_loss: 0.0096 - 61s/epoch - 730us/sample
Epoch 2/90
84077/84077 - 37s - loss: 0.0015 - val_loss: 9.7882e-04 - 37s/epoch - 445us/sample
Epoch 3/90
84077/84077 - 38s - loss: 0.0038 - val_loss: 0.0012 - 38s/epoch - 450us/sample
Epoch 4/90
84077/84077 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 444us/sample
Epoch 5/90
84077/84077 - 38s - loss: 0.0011 - val_loss: 8.6899e-04 - 38s/epoch - 450us/sample
Epoch 6/90
84077/84077 - 37s - loss: 9.4900e-04 - val_loss: 7.9289e-04 - 37s/epoch - 445us/sample
Epoch 7/90
84077/84077 - 38s - loss: 8.6649e-04 - val_loss: 7.9734e-04 - 38s/epoch - 449us/sample
Epoch 8/90
84077/84077 - 37s - loss: 8.2688e-04 - val_loss: 7.3502e-04 - 37s/epoch - 446us/sample
Epoch 9/90
84077/84077 - 38s - loss: 7.8550e-04 - val_loss: 7.1334e-04 - 38s/epoch - 447us/sample
Epoch 10/90
84077/84077 - 38s - loss: 7.5506e-04 - val_loss: 6.9179e-04 - 38s/epoch - 448us/sample
Epoch 11/90
84077/84077 - 37s - loss: 7.2202e-04 - val_loss: 6.8474e-04 - 37s/epoch - 445us/sample
Epoch 12/90
84077/84077 - 38s - loss: 6.9914e-04 - val_loss: 6.7689e-04 - 38s/epoch - 449us/sample
Epoch 13/90
84077/84077 - 37s - loss: 6.7544e-04 - val_loss: 6.7710e-04 - 37s/epoch - 445us/sample
Epoch 14/90
84077/84077 - 38s - loss: 6.5458e-04 - val_loss: 6.6961e-04 - 38s/epoch - 450us/sample
Epoch 15/90
84077/84077 - 37s - loss: 6.3583e-04 - val_loss: 6.6275e-04 - 37s/epoch - 446us/sample
Epoch 16/90
84077/84077 - 38s - loss: 6.2175e-04 - val_loss: 6.4453e-04 - 38s/epoch - 450us/sample
Epoch 17/90
84077/84077 - 38s - loss: 6.0894e-04 - val_loss: 6.4624e-04 - 38s/epoch - 446us/sample
Epoch 18/90
84077/84077 - 38s - loss: 6.0266e-04 - val_loss: 6.3376e-04 - 38s/epoch - 450us/sample
Epoch 19/90
84077/84077 - 37s - loss: 5.9475e-04 - val_loss: 6.3873e-04 - 37s/epoch - 445us/sample
Epoch 20/90
84077/84077 - 38s - loss: 5.9097e-04 - val_loss: 6.1686e-04 - 38s/epoch - 449us/sample
Epoch 21/90
84077/84077 - 37s - loss: 5.8617e-04 - val_loss: 6.1222e-04 - 37s/epoch - 444us/sample
Epoch 22/90
84077/84077 - 38s - loss: 5.7761e-04 - val_loss: 5.9136e-04 - 38s/epoch - 449us/sample
Epoch 23/90
84077/84077 - 38s - loss: 5.7080e-04 - val_loss: 5.9119e-04 - 38s/epoch - 446us/sample
Epoch 24/90
84077/84077 - 38s - loss: 5.6726e-04 - val_loss: 5.9332e-04 - 38s/epoch - 448us/sample
Epoch 25/90
84077/84077 - 38s - loss: 5.6204e-04 - val_loss: 5.9518e-04 - 38s/epoch - 447us/sample
Epoch 26/90
84077/84077 - 37s - loss: 5.5896e-04 - val_loss: 6.0179e-04 - 37s/epoch - 445us/sample
Epoch 27/90
84077/84077 - 38s - loss: 5.5641e-04 - val_loss: 5.8979e-04 - 38s/epoch - 447us/sample
Epoch 28/90
84077/84077 - 37s - loss: 5.5161e-04 - val_loss: 5.7445e-04 - 37s/epoch - 444us/sample
Epoch 29/90
84077/84077 - 38s - loss: 5.4942e-04 - val_loss: 5.6444e-04 - 38s/epoch - 448us/sample
Epoch 30/90
84077/84077 - 37s - loss: 5.4593e-04 - val_loss: 5.6919e-04 - 37s/epoch - 446us/sample
Epoch 31/90
84077/84077 - 38s - loss: 5.4290e-04 - val_loss: 5.7624e-04 - 38s/epoch - 449us/sample
Epoch 32/90
84077/84077 - 37s - loss: 5.4027e-04 - val_loss: 5.7161e-04 - 37s/epoch - 445us/sample
Epoch 33/90
84077/84077 - 38s - loss: 5.3833e-04 - val_loss: 5.5177e-04 - 38s/epoch - 450us/sample
Epoch 34/90
84077/84077 - 37s - loss: 5.3705e-04 - val_loss: 5.4846e-04 - 37s/epoch - 445us/sample
Epoch 35/90
84077/84077 - 38s - loss: 5.3422e-04 - val_loss: 5.5185e-04 - 38s/epoch - 450us/sample
Epoch 36/90
84077/84077 - 37s - loss: 5.3230e-04 - val_loss: 5.6176e-04 - 37s/epoch - 445us/sample
Epoch 37/90
84077/84077 - 38s - loss: 5.2940e-04 - val_loss: 5.3546e-04 - 38s/epoch - 450us/sample
Epoch 38/90
84077/84077 - 37s - loss: 5.2864e-04 - val_loss: 5.3901e-04 - 37s/epoch - 446us/sample
Epoch 39/90
84077/84077 - 38s - loss: 5.2764e-04 - val_loss: 5.2738e-04 - 38s/epoch - 448us/sample
Epoch 40/90
84077/84077 - 38s - loss: 5.2607e-04 - val_loss: 5.3531e-04 - 38s/epoch - 446us/sample
Epoch 41/90
84077/84077 - 38s - loss: 5.2436e-04 - val_loss: 5.3403e-04 - 38s/epoch - 447us/sample
Epoch 42/90
84077/84077 - 38s - loss: 5.2421e-04 - val_loss: 5.3697e-04 - 38s/epoch - 447us/sample
Epoch 43/90
84077/84077 - 37s - loss: 5.2269e-04 - val_loss: 5.3190e-04 - 37s/epoch - 446us/sample
Epoch 44/90
84077/84077 - 38s - loss: 5.2103e-04 - val_loss: 5.3279e-04 - 38s/epoch - 448us/sample
Epoch 45/90
84077/84077 - 38s - loss: 5.2020e-04 - val_loss: 5.2477e-04 - 38s/epoch - 446us/sample
Epoch 46/90
84077/84077 - 38s - loss: 5.1865e-04 - val_loss: 5.1609e-04 - 38s/epoch - 449us/sample
Epoch 47/90
84077/84077 - 37s - loss: 5.1739e-04 - val_loss: 5.1919e-04 - 37s/epoch - 445us/sample
Epoch 48/90
84077/84077 - 38s - loss: 5.1612e-04 - val_loss: 5.2774e-04 - 38s/epoch - 449us/sample
Epoch 49/90
84077/84077 - 38s - loss: 5.1388e-04 - val_loss: 5.1993e-04 - 38s/epoch - 446us/sample
Epoch 50/90
84077/84077 - 38s - loss: 5.1082e-04 - val_loss: 5.1625e-04 - 38s/epoch - 449us/sample
Epoch 51/90
84077/84077 - 37s - loss: 5.0974e-04 - val_loss: 5.0954e-04 - 37s/epoch - 445us/sample
Epoch 52/90
84077/84077 - 38s - loss: 5.0800e-04 - val_loss: 5.0140e-04 - 38s/epoch - 450us/sample
Epoch 53/90
84077/84077 - 37s - loss: 5.0575e-04 - val_loss: 5.2303e-04 - 37s/epoch - 445us/sample
Epoch 54/90
84077/84077 - 38s - loss: 5.0265e-04 - val_loss: 4.9195e-04 - 38s/epoch - 450us/sample
Epoch 55/90
84077/84077 - 37s - loss: 4.9976e-04 - val_loss: 4.9142e-04 - 37s/epoch - 444us/sample
Epoch 56/90
84077/84077 - 38s - loss: 4.9761e-04 - val_loss: 5.0625e-04 - 38s/epoch - 447us/sample
Epoch 57/90
84077/84077 - 38s - loss: 4.9702e-04 - val_loss: 5.0488e-04 - 38s/epoch - 448us/sample
Epoch 58/90
84077/84077 - 37s - loss: 4.9466e-04 - val_loss: 5.0510e-04 - 37s/epoch - 445us/sample
Epoch 59/90
84077/84077 - 38s - loss: 4.9331e-04 - val_loss: 4.9631e-04 - 38s/epoch - 449us/sample
Epoch 60/90
84077/84077 - 37s - loss: 4.9176e-04 - val_loss: 4.8938e-04 - 37s/epoch - 445us/sample
Epoch 61/90
84077/84077 - 38s - loss: 4.8929e-04 - val_loss: 4.7626e-04 - 38s/epoch - 450us/sample
Epoch 62/90
84077/84077 - 38s - loss: 4.8816e-04 - val_loss: 4.6936e-04 - 38s/epoch - 446us/sample
Epoch 63/90
84077/84077 - 38s - loss: 4.8615e-04 - val_loss: 4.7883e-04 - 38s/epoch - 449us/sample
Epoch 64/90
84077/84077 - 37s - loss: 4.8568e-04 - val_loss: 4.7264e-04 - 37s/epoch - 445us/sample
Epoch 65/90
84077/84077 - 38s - loss: 4.8330e-04 - val_loss: 4.7403e-04 - 38s/epoch - 450us/sample
Epoch 66/90
84077/84077 - 37s - loss: 4.8209e-04 - val_loss: 4.7252e-04 - 37s/epoch - 446us/sample
Epoch 67/90
84077/84077 - 38s - loss: 4.8232e-04 - val_loss: 4.5833e-04 - 38s/epoch - 448us/sample
Epoch 68/90
84077/84077 - 38s - loss: 4.8035e-04 - val_loss: 4.6795e-04 - 38s/epoch - 446us/sample
Epoch 69/90
84077/84077 - 37s - loss: 4.7952e-04 - val_loss: 4.6315e-04 - 37s/epoch - 446us/sample
Epoch 70/90
84077/84077 - 38s - loss: 4.7862e-04 - val_loss: 4.6155e-04 - 38s/epoch - 449us/sample
Epoch 71/90
84077/84077 - 37s - loss: 4.7794e-04 - val_loss: 4.6343e-04 - 37s/epoch - 445us/sample
Epoch 72/90
84077/84077 - 38s - loss: 4.7575e-04 - val_loss: 4.5651e-04 - 38s/epoch - 448us/sample
Epoch 73/90
84077/84077 - 38s - loss: 4.7369e-04 - val_loss: 4.5903e-04 - 38s/epoch - 447us/sample
Epoch 74/90
84077/84077 - 38s - loss: 4.7483e-04 - val_loss: 4.5060e-04 - 38s/epoch - 449us/sample
Epoch 75/90
84077/84077 - 37s - loss: 4.7180e-04 - val_loss: 4.5398e-04 - 37s/epoch - 445us/sample
Epoch 76/90
84077/84077 - 38s - loss: 4.7214e-04 - val_loss: 4.6718e-04 - 38s/epoch - 449us/sample
Epoch 77/90
84077/84077 - 38s - loss: 4.7052e-04 - val_loss: 4.5353e-04 - 38s/epoch - 446us/sample
Epoch 78/90
84077/84077 - 38s - loss: 4.6942e-04 - val_loss: 4.5681e-04 - 38s/epoch - 452us/sample
Epoch 79/90
84077/84077 - 37s - loss: 4.6854e-04 - val_loss: 4.6830e-04 - 37s/epoch - 445us/sample
Epoch 80/90
84077/84077 - 38s - loss: 4.6843e-04 - val_loss: 4.4357e-04 - 38s/epoch - 450us/sample
Epoch 81/90
84077/84077 - 38s - loss: 4.6817e-04 - val_loss: 4.5451e-04 - 38s/epoch - 448us/sample
Epoch 82/90
84077/84077 - 38s - loss: 4.6766e-04 - val_loss: 4.5219e-04 - 38s/epoch - 450us/sample
Epoch 83/90
84077/84077 - 37s - loss: 4.6664e-04 - val_loss: 4.4922e-04 - 37s/epoch - 446us/sample
Epoch 84/90
84077/84077 - 38s - loss: 4.6699e-04 - val_loss: 4.4316e-04 - 38s/epoch - 451us/sample
Epoch 85/90
84077/84077 - 37s - loss: 4.6673e-04 - val_loss: 4.4709e-04 - 37s/epoch - 444us/sample
Epoch 86/90
84077/84077 - 38s - loss: 4.6600e-04 - val_loss: 4.4274e-04 - 38s/epoch - 452us/sample
Epoch 87/90
84077/84077 - 37s - loss: 4.6598e-04 - val_loss: 4.5434e-04 - 37s/epoch - 444us/sample
Epoch 88/90
84077/84077 - 38s - loss: 4.6571e-04 - val_loss: 4.4359e-04 - 38s/epoch - 447us/sample
Epoch 89/90
84077/84077 - 38s - loss: 4.6348e-04 - val_loss: 4.4011e-04 - 38s/epoch - 449us/sample
Epoch 90/90
84077/84077 - 37s - loss: 4.6282e-04 - val_loss: 4.4078e-04 - 37s/epoch - 445us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00044078318701571106
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 11:35:39.995522: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_85/outputlayer/BiasAdd' id:94558 op device:{requested: '', assigned: ''} def:{{{node decoder_model_85/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_85/outputlayer/MatMul, decoder_model_85/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.09508614487028025
cosine 0.09389471643571128
MAE: 0.0037029871500317097
RMSE: 0.018709817669815713
r2: 0.7265708774567624
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 16, 90, 0.001, 0.2, 188, 0.0004628245215280255, 0.00044078318701571106, 0.09508614487028025, 0.09389471643571128, 0.0037029871500317097, 0.018709817669815713, 0.7265708774567624, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 8
Fitness    = 541.8276370289637
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 541.8276370289637.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 541.8276370289637]
[2.1 90 0.0008 64 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_219 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_219 (ReLU)               (None, 1980)         0           ['batch_normalization_219[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_219[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_219[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 11:36:17.702511: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_86/kernel/Assign' id:95633 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_86/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_86/kernel, dense_dec0_86/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 11:36:40.125127: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_86/mul' id:95842 op device:{requested: '', assigned: ''} def:{{{node loss_86/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_86/mul/x, loss_86/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 38s - loss: 0.0144 - val_loss: 0.0020 - 38s/epoch - 446us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0017 - val_loss: 0.0015 - 14s/epoch - 167us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0024 - val_loss: 0.0011 - 14s/epoch - 165us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0016 - val_loss: 0.0010 - 14s/epoch - 165us/sample
Epoch 5/90
84077/84077 - 14s - loss: 0.0010 - val_loss: 9.6222e-04 - 14s/epoch - 166us/sample
Epoch 6/90
84077/84077 - 14s - loss: 8.7776e-04 - val_loss: 7.3491e-04 - 14s/epoch - 166us/sample
Epoch 7/90
84077/84077 - 14s - loss: 7.3813e-04 - val_loss: 6.0944e-04 - 14s/epoch - 168us/sample
Epoch 8/90
84077/84077 - 14s - loss: 8.3805e-04 - val_loss: 6.1394e-04 - 14s/epoch - 165us/sample
Epoch 9/90
84077/84077 - 14s - loss: 6.0028e-04 - val_loss: 4.8620e-04 - 14s/epoch - 165us/sample
Epoch 10/90
84077/84077 - 14s - loss: 5.1393e-04 - val_loss: 4.5027e-04 - 14s/epoch - 166us/sample
Epoch 11/90
84077/84077 - 14s - loss: 4.7370e-04 - val_loss: 6.1678e-04 - 14s/epoch - 165us/sample
Epoch 12/90
84077/84077 - 14s - loss: 4.3039e-04 - val_loss: 3.6765e-04 - 14s/epoch - 168us/sample
Epoch 13/90
84077/84077 - 14s - loss: 3.9124e-04 - val_loss: 3.3310e-04 - 14s/epoch - 165us/sample
Epoch 14/90
84077/84077 - 14s - loss: 3.6268e-04 - val_loss: 3.1211e-04 - 14s/epoch - 166us/sample
Epoch 15/90
84077/84077 - 14s - loss: 3.4407e-04 - val_loss: 3.0686e-04 - 14s/epoch - 166us/sample
Epoch 16/90
84077/84077 - 14s - loss: 3.2161e-04 - val_loss: 2.9071e-04 - 14s/epoch - 166us/sample
Epoch 17/90
84077/84077 - 14s - loss: 3.0613e-04 - val_loss: 2.7733e-04 - 14s/epoch - 168us/sample
Epoch 18/90
84077/84077 - 14s - loss: 2.9439e-04 - val_loss: 2.6792e-04 - 14s/epoch - 166us/sample
Epoch 19/90
84077/84077 - 14s - loss: 2.8663e-04 - val_loss: 2.6094e-04 - 14s/epoch - 167us/sample
Epoch 20/90
84077/84077 - 14s - loss: 2.7689e-04 - val_loss: 2.5360e-04 - 14s/epoch - 165us/sample
Epoch 21/90
84077/84077 - 14s - loss: 2.7001e-04 - val_loss: 2.5002e-04 - 14s/epoch - 166us/sample
Epoch 22/90
84077/84077 - 14s - loss: 2.6386e-04 - val_loss: 2.4354e-04 - 14s/epoch - 168us/sample
Epoch 23/90
84077/84077 - 14s - loss: 2.5785e-04 - val_loss: 2.4372e-04 - 14s/epoch - 166us/sample
Epoch 24/90
84077/84077 - 14s - loss: 2.5353e-04 - val_loss: 2.3558e-04 - 14s/epoch - 166us/sample
Epoch 25/90
84077/84077 - 14s - loss: 2.4967e-04 - val_loss: 2.3849e-04 - 14s/epoch - 166us/sample
Epoch 26/90
84077/84077 - 14s - loss: 2.4660e-04 - val_loss: 2.3152e-04 - 14s/epoch - 166us/sample
Epoch 27/90
84077/84077 - 14s - loss: 2.4319e-04 - val_loss: 2.2739e-04 - 14s/epoch - 169us/sample
Epoch 28/90
84077/84077 - 14s - loss: 2.3994e-04 - val_loss: 2.2503e-04 - 14s/epoch - 166us/sample
Epoch 29/90
84077/84077 - 14s - loss: 2.3737e-04 - val_loss: 2.3002e-04 - 14s/epoch - 166us/sample
Epoch 30/90
84077/84077 - 14s - loss: 2.3581e-04 - val_loss: 2.2574e-04 - 14s/epoch - 166us/sample
Epoch 31/90
84077/84077 - 14s - loss: 2.3327e-04 - val_loss: 2.1942e-04 - 14s/epoch - 165us/sample
Epoch 32/90
84077/84077 - 14s - loss: 2.3132e-04 - val_loss: 2.2240e-04 - 14s/epoch - 169us/sample
Epoch 33/90
84077/84077 - 14s - loss: 2.2861e-04 - val_loss: 2.1850e-04 - 14s/epoch - 166us/sample
Epoch 34/90
84077/84077 - 14s - loss: 2.2780e-04 - val_loss: 2.1944e-04 - 14s/epoch - 165us/sample
Epoch 35/90
84077/84077 - 14s - loss: 2.2509e-04 - val_loss: 2.1368e-04 - 14s/epoch - 165us/sample
Epoch 36/90
84077/84077 - 14s - loss: 2.2382e-04 - val_loss: 2.1738e-04 - 14s/epoch - 167us/sample
Epoch 37/90
84077/84077 - 14s - loss: 2.2199e-04 - val_loss: 2.1454e-04 - 14s/epoch - 168us/sample
Epoch 38/90
84077/84077 - 14s - loss: 2.2045e-04 - val_loss: 2.1203e-04 - 14s/epoch - 166us/sample
Epoch 39/90
84077/84077 - 14s - loss: 2.1880e-04 - val_loss: 2.0889e-04 - 14s/epoch - 166us/sample
Epoch 40/90
84077/84077 - 14s - loss: 2.1811e-04 - val_loss: 2.1169e-04 - 14s/epoch - 166us/sample
Epoch 41/90
84077/84077 - 14s - loss: 2.1674e-04 - val_loss: 2.0719e-04 - 14s/epoch - 165us/sample
Epoch 42/90
84077/84077 - 14s - loss: 2.1577e-04 - val_loss: 2.0721e-04 - 14s/epoch - 168us/sample
Epoch 43/90
84077/84077 - 14s - loss: 2.1452e-04 - val_loss: 2.0627e-04 - 14s/epoch - 166us/sample
Epoch 44/90
84077/84077 - 14s - loss: 2.1385e-04 - val_loss: 2.0568e-04 - 14s/epoch - 166us/sample
Epoch 45/90
84077/84077 - 14s - loss: 2.1212e-04 - val_loss: 2.0549e-04 - 14s/epoch - 166us/sample
Epoch 46/90
84077/84077 - 14s - loss: 2.1155e-04 - val_loss: 2.0685e-04 - 14s/epoch - 166us/sample
Epoch 47/90
84077/84077 - 14s - loss: 2.1069e-04 - val_loss: 2.0709e-04 - 14s/epoch - 168us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.1033e-04 - val_loss: 2.0353e-04 - 14s/epoch - 166us/sample
Epoch 49/90
84077/84077 - 14s - loss: 2.0875e-04 - val_loss: 2.0231e-04 - 14s/epoch - 167us/sample
Epoch 50/90
84077/84077 - 14s - loss: 2.0781e-04 - val_loss: 2.0186e-04 - 14s/epoch - 166us/sample
Epoch 51/90
84077/84077 - 14s - loss: 2.0773e-04 - val_loss: 2.0180e-04 - 14s/epoch - 165us/sample
Epoch 52/90
84077/84077 - 14s - loss: 2.0733e-04 - val_loss: 2.0044e-04 - 14s/epoch - 167us/sample
Epoch 53/90
84077/84077 - 14s - loss: 2.0565e-04 - val_loss: 2.0229e-04 - 14s/epoch - 168us/sample
Epoch 54/90
84077/84077 - 14s - loss: 2.0602e-04 - val_loss: 2.0321e-04 - 14s/epoch - 166us/sample
Epoch 55/90
84077/84077 - 14s - loss: 2.0515e-04 - val_loss: 2.0084e-04 - 14s/epoch - 165us/sample
Epoch 56/90
84077/84077 - 14s - loss: 2.0466e-04 - val_loss: 1.9729e-04 - 14s/epoch - 166us/sample
Epoch 57/90
84077/84077 - 14s - loss: 2.0421e-04 - val_loss: 1.9733e-04 - 14s/epoch - 167us/sample
Epoch 58/90
84077/84077 - 14s - loss: 2.0294e-04 - val_loss: 1.9738e-04 - 14s/epoch - 168us/sample
Epoch 59/90
84077/84077 - 14s - loss: 2.0291e-04 - val_loss: 2.0165e-04 - 14s/epoch - 166us/sample
Epoch 60/90
84077/84077 - 14s - loss: 2.0242e-04 - val_loss: 2.0059e-04 - 14s/epoch - 166us/sample
Epoch 61/90
84077/84077 - 14s - loss: 2.0156e-04 - val_loss: 1.9866e-04 - 14s/epoch - 166us/sample
Epoch 62/90
84077/84077 - 14s - loss: 2.0159e-04 - val_loss: 1.9501e-04 - 14s/epoch - 166us/sample
Epoch 63/90
84077/84077 - 14s - loss: 2.0175e-04 - val_loss: 1.9664e-04 - 14s/epoch - 168us/sample
Epoch 64/90
84077/84077 - 14s - loss: 2.0080e-04 - val_loss: 1.9602e-04 - 14s/epoch - 165us/sample
Epoch 65/90
84077/84077 - 14s - loss: 1.9995e-04 - val_loss: 1.9578e-04 - 14s/epoch - 166us/sample
Epoch 66/90
84077/84077 - 14s - loss: 2.0041e-04 - val_loss: 1.9497e-04 - 14s/epoch - 166us/sample
Epoch 67/90
84077/84077 - 14s - loss: 1.9949e-04 - val_loss: 1.9468e-04 - 14s/epoch - 166us/sample
Epoch 68/90
84077/84077 - 14s - loss: 1.9916e-04 - val_loss: 1.9421e-04 - 14s/epoch - 168us/sample
Epoch 69/90
84077/84077 - 14s - loss: 1.9919e-04 - val_loss: 1.9596e-04 - 14s/epoch - 165us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.9827e-04 - val_loss: 1.9262e-04 - 14s/epoch - 166us/sample
Epoch 71/90
84077/84077 - 14s - loss: 1.9846e-04 - val_loss: 1.9435e-04 - 14s/epoch - 166us/sample
Epoch 72/90
84077/84077 - 14s - loss: 1.9742e-04 - val_loss: 1.9001e-04 - 14s/epoch - 166us/sample
Epoch 73/90
84077/84077 - 14s - loss: 1.9720e-04 - val_loss: 1.8892e-04 - 14s/epoch - 168us/sample
Epoch 74/90
84077/84077 - 14s - loss: 1.9654e-04 - val_loss: 1.9345e-04 - 14s/epoch - 166us/sample
Epoch 75/90
84077/84077 - 14s - loss: 1.9661e-04 - val_loss: 1.9337e-04 - 14s/epoch - 166us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.9579e-04 - val_loss: 1.9156e-04 - 14s/epoch - 165us/sample
Epoch 77/90
84077/84077 - 14s - loss: 1.9558e-04 - val_loss: 1.9215e-04 - 14s/epoch - 166us/sample
Epoch 78/90
84077/84077 - 14s - loss: 1.9625e-04 - val_loss: 1.9324e-04 - 14s/epoch - 168us/sample
Epoch 79/90
84077/84077 - 14s - loss: 1.9555e-04 - val_loss: 1.9034e-04 - 14s/epoch - 167us/sample
Epoch 80/90
84077/84077 - 14s - loss: 1.9488e-04 - val_loss: 1.9117e-04 - 14s/epoch - 166us/sample
Epoch 81/90
84077/84077 - 14s - loss: 1.9514e-04 - val_loss: 1.9162e-04 - 14s/epoch - 166us/sample
Epoch 82/90
84077/84077 - 14s - loss: 1.9443e-04 - val_loss: 1.9117e-04 - 14s/epoch - 166us/sample
Epoch 83/90
84077/84077 - 14s - loss: 1.9471e-04 - val_loss: 1.9113e-04 - 14s/epoch - 169us/sample
Epoch 84/90
84077/84077 - 14s - loss: 1.9428e-04 - val_loss: 1.8939e-04 - 14s/epoch - 166us/sample
Epoch 85/90
84077/84077 - 14s - loss: 1.9307e-04 - val_loss: 1.8993e-04 - 14s/epoch - 166us/sample
Epoch 86/90
84077/84077 - 14s - loss: 1.9313e-04 - val_loss: 1.8902e-04 - 14s/epoch - 165us/sample
Epoch 87/90
84077/84077 - 14s - loss: 1.9306e-04 - val_loss: 1.8630e-04 - 14s/epoch - 166us/sample
Epoch 88/90
84077/84077 - 14s - loss: 1.9272e-04 - val_loss: 1.9102e-04 - 14s/epoch - 169us/sample
Epoch 89/90
84077/84077 - 14s - loss: 1.9300e-04 - val_loss: 1.8808e-04 - 14s/epoch - 166us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.9282e-04 - val_loss: 1.8705e-04 - 14s/epoch - 165us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018705448220883027
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 11:57:32.562384: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_86/outputlayer/BiasAdd' id:95813 op device:{requested: '', assigned: ''} def:{{{node decoder_model_86/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_86/outputlayer/MatMul, decoder_model_86/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02203973174362809
cosine 0.0217778280077708
MAE: 0.0019218101262374703
RMSE: 0.009259532607454475
r2: 0.9332128338151683
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.0008, 0.2, 188, 0.00019282442844156832, 0.00018705448220883027, 0.02203973174362809, 0.0217778280077708, 0.0019218101262374703, 0.009259532607454475, 0.9332128338151683, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0012000000000000001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_222 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_222 (ReLU)               (None, 2074)         0           ['batch_normalization_222[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_222[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_222[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 11:58:10.476687: W tensorflow/c/c_api.cc:291] Operation '{name:'training_122/Adam/dense_dec0_87/bias/v/Assign' id:97745 op device:{requested: '', assigned: ''} def:{{{node training_122/Adam/dense_dec0_87/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_122/Adam/dense_dec0_87/bias/v, training_122/Adam/dense_dec0_87/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 11:58:32.938406: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_87/mul' id:97097 op device:{requested: '', assigned: ''} def:{{{node loss_87/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_87/mul/x, loss_87/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 38s - loss: 0.0110 - val_loss: 0.0021 - 38s/epoch - 447us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 168us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0022 - val_loss: 0.0012 - 14s/epoch - 169us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0014 - val_loss: 9.6948e-04 - 14s/epoch - 168us/sample
Epoch 5/90
84077/84077 - 14s - loss: 9.5228e-04 - val_loss: 8.9080e-04 - 14s/epoch - 167us/sample
Epoch 6/90
84077/84077 - 14s - loss: 7.9974e-04 - val_loss: 6.2794e-04 - 14s/epoch - 168us/sample
Epoch 7/90
84077/84077 - 14s - loss: 6.6103e-04 - val_loss: 5.3846e-04 - 14s/epoch - 168us/sample
Epoch 8/90
84077/84077 - 14s - loss: 5.7920e-04 - val_loss: 5.2508e-04 - 14s/epoch - 169us/sample
Epoch 9/90
84077/84077 - 14s - loss: 5.2354e-04 - val_loss: 4.4270e-04 - 14s/epoch - 168us/sample
Epoch 10/90
84077/84077 - 14s - loss: 4.7225e-04 - val_loss: 3.9556e-04 - 14s/epoch - 167us/sample
Epoch 11/90
84077/84077 - 14s - loss: 4.3104e-04 - val_loss: 3.5159e-04 - 14s/epoch - 168us/sample
Epoch 12/90
84077/84077 - 14s - loss: 3.9786e-04 - val_loss: 3.3149e-04 - 14s/epoch - 167us/sample
Epoch 13/90
84077/84077 - 14s - loss: 3.6692e-04 - val_loss: 3.1426e-04 - 14s/epoch - 169us/sample
Epoch 14/90
84077/84077 - 14s - loss: 3.4630e-04 - val_loss: 2.9154e-04 - 14s/epoch - 168us/sample
Epoch 15/90
84077/84077 - 14s - loss: 3.2824e-04 - val_loss: 2.8447e-04 - 14s/epoch - 167us/sample
Epoch 16/90
84077/84077 - 14s - loss: 3.0999e-04 - val_loss: 2.8055e-04 - 14s/epoch - 167us/sample
Epoch 17/90
84077/84077 - 14s - loss: 3.0108e-04 - val_loss: 2.7772e-04 - 14s/epoch - 167us/sample
Epoch 18/90
84077/84077 - 14s - loss: 2.8897e-04 - val_loss: 2.5779e-04 - 14s/epoch - 168us/sample
Epoch 19/90
84077/84077 - 14s - loss: 3.1261e-04 - val_loss: 2.5782e-04 - 14s/epoch - 170us/sample
Epoch 20/90
84077/84077 - 14s - loss: 2.7493e-04 - val_loss: 2.4586e-04 - 14s/epoch - 167us/sample
Epoch 21/90
84077/84077 - 14s - loss: 2.6781e-04 - val_loss: 2.4204e-04 - 14s/epoch - 167us/sample
Epoch 22/90
84077/84077 - 14s - loss: 2.6080e-04 - val_loss: 2.3680e-04 - 14s/epoch - 167us/sample
Epoch 23/90
84077/84077 - 14s - loss: 2.6258e-04 - val_loss: 2.3282e-04 - 14s/epoch - 169us/sample
Epoch 24/90
84077/84077 - 14s - loss: 2.5284e-04 - val_loss: 2.3244e-04 - 14s/epoch - 169us/sample
Epoch 25/90
84077/84077 - 14s - loss: 2.4908e-04 - val_loss: 2.2743e-04 - 14s/epoch - 167us/sample
Epoch 26/90
84077/84077 - 14s - loss: 2.4380e-04 - val_loss: 2.2612e-04 - 14s/epoch - 167us/sample
Epoch 27/90
84077/84077 - 14s - loss: 2.4298e-04 - val_loss: 2.2691e-04 - 14s/epoch - 167us/sample
Epoch 28/90
84077/84077 - 14s - loss: 2.3982e-04 - val_loss: 2.2251e-04 - 14s/epoch - 170us/sample
Epoch 29/90
84077/84077 - 14s - loss: 2.3511e-04 - val_loss: 2.2173e-04 - 14s/epoch - 169us/sample
Epoch 30/90
84077/84077 - 14s - loss: 2.3375e-04 - val_loss: 2.1798e-04 - 14s/epoch - 167us/sample
Epoch 31/90
84077/84077 - 14s - loss: 2.3197e-04 - val_loss: 2.1887e-04 - 14s/epoch - 167us/sample
Epoch 32/90
84077/84077 - 14s - loss: 2.2838e-04 - val_loss: 2.1634e-04 - 14s/epoch - 169us/sample
Epoch 33/90
84077/84077 - 14s - loss: 2.2603e-04 - val_loss: 2.1261e-04 - 14s/epoch - 168us/sample
Epoch 34/90
84077/84077 - 14s - loss: 2.2497e-04 - val_loss: 2.1180e-04 - 14s/epoch - 170us/sample
Epoch 35/90
84077/84077 - 14s - loss: 2.2293e-04 - val_loss: 2.0625e-04 - 14s/epoch - 167us/sample
Epoch 36/90
84077/84077 - 14s - loss: 2.2263e-04 - val_loss: 2.0690e-04 - 14s/epoch - 168us/sample
Epoch 37/90
84077/84077 - 14s - loss: 2.3264e-04 - val_loss: 2.0487e-04 - 14s/epoch - 167us/sample
Epoch 38/90
84077/84077 - 14s - loss: 2.1866e-04 - val_loss: 2.0319e-04 - 14s/epoch - 168us/sample
Epoch 39/90
84077/84077 - 14s - loss: 2.1743e-04 - val_loss: 2.0096e-04 - 14s/epoch - 170us/sample
Epoch 40/90
84077/84077 - 14s - loss: 2.2567e-04 - val_loss: 2.0959e-04 - 14s/epoch - 168us/sample
Epoch 41/90
84077/84077 - 14s - loss: 2.1661e-04 - val_loss: 2.0095e-04 - 14s/epoch - 167us/sample
Epoch 42/90
84077/84077 - 14s - loss: 2.1373e-04 - val_loss: 2.0242e-04 - 14s/epoch - 167us/sample
Epoch 43/90
84077/84077 - 14s - loss: 2.1294e-04 - val_loss: 2.0062e-04 - 14s/epoch - 168us/sample
Epoch 44/90
84077/84077 - 14s - loss: 2.1230e-04 - val_loss: 1.9859e-04 - 14s/epoch - 170us/sample
Epoch 45/90
84077/84077 - 14s - loss: 2.0999e-04 - val_loss: 1.9840e-04 - 14s/epoch - 168us/sample
Epoch 46/90
84077/84077 - 14s - loss: 2.1017e-04 - val_loss: 1.9720e-04 - 14s/epoch - 167us/sample
Epoch 47/90
84077/84077 - 14s - loss: 2.0856e-04 - val_loss: 1.9449e-04 - 14s/epoch - 167us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.1951e-04 - val_loss: 2.0054e-04 - 14s/epoch - 168us/sample
Epoch 49/90
84077/84077 - 14s - loss: 2.0773e-04 - val_loss: 1.9424e-04 - 14s/epoch - 170us/sample
Epoch 50/90
84077/84077 - 14s - loss: 2.1023e-04 - val_loss: 1.9255e-04 - 14s/epoch - 167us/sample
Epoch 51/90
84077/84077 - 14s - loss: 2.0472e-04 - val_loss: 1.9299e-04 - 14s/epoch - 167us/sample
Epoch 52/90
84077/84077 - 14s - loss: 2.0346e-04 - val_loss: 1.8940e-04 - 14s/epoch - 167us/sample
Epoch 53/90
84077/84077 - 14s - loss: 2.0494e-04 - val_loss: 1.9085e-04 - 14s/epoch - 168us/sample
Epoch 54/90
84077/84077 - 14s - loss: 2.0322e-04 - val_loss: 2.0845e-04 - 14s/epoch - 170us/sample
Epoch 55/90
84077/84077 - 14s - loss: 2.0247e-04 - val_loss: 1.8817e-04 - 14s/epoch - 167us/sample
Epoch 56/90
84077/84077 - 14s - loss: 2.0193e-04 - val_loss: 1.8776e-04 - 14s/epoch - 167us/sample
Epoch 57/90
84077/84077 - 14s - loss: 2.0050e-04 - val_loss: 1.9005e-04 - 14s/epoch - 168us/sample
Epoch 58/90
84077/84077 - 14s - loss: 2.0160e-04 - val_loss: 1.8692e-04 - 14s/epoch - 168us/sample
Epoch 59/90
84077/84077 - 14s - loss: 1.9964e-04 - val_loss: 1.8764e-04 - 14s/epoch - 170us/sample
Epoch 60/90
84077/84077 - 14s - loss: 1.9878e-04 - val_loss: 1.8750e-04 - 14s/epoch - 167us/sample
Epoch 61/90
84077/84077 - 14s - loss: 1.9746e-04 - val_loss: 1.8647e-04 - 14s/epoch - 167us/sample
Epoch 62/90
84077/84077 - 14s - loss: 1.9808e-04 - val_loss: 1.8683e-04 - 14s/epoch - 167us/sample
Epoch 63/90
84077/84077 - 14s - loss: 1.9781e-04 - val_loss: 1.8533e-04 - 14s/epoch - 167us/sample
Epoch 64/90
84077/84077 - 14s - loss: 1.9678e-04 - val_loss: 1.8537e-04 - 14s/epoch - 170us/sample
Epoch 65/90
84077/84077 - 14s - loss: 1.9690e-04 - val_loss: 1.8407e-04 - 14s/epoch - 167us/sample
Epoch 66/90
84077/84077 - 14s - loss: 1.9694e-04 - val_loss: 1.8379e-04 - 14s/epoch - 167us/sample
Epoch 67/90
84077/84077 - 14s - loss: 1.9706e-04 - val_loss: 2.0858e-04 - 14s/epoch - 167us/sample
Epoch 68/90
84077/84077 - 14s - loss: 1.9788e-04 - val_loss: 1.8562e-04 - 14s/epoch - 168us/sample
Epoch 69/90
84077/84077 - 14s - loss: 2.0147e-04 - val_loss: 1.8483e-04 - 14s/epoch - 170us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.9436e-04 - val_loss: 1.8104e-04 - 14s/epoch - 168us/sample
Epoch 71/90
84077/84077 - 14s - loss: 1.9339e-04 - val_loss: 1.8117e-04 - 14s/epoch - 167us/sample
Epoch 72/90
84077/84077 - 14s - loss: 1.9325e-04 - val_loss: 1.8086e-04 - 14s/epoch - 168us/sample
Epoch 73/90
84077/84077 - 14s - loss: 1.9292e-04 - val_loss: 1.8224e-04 - 14s/epoch - 167us/sample
Epoch 74/90
84077/84077 - 14s - loss: 1.9209e-04 - val_loss: 1.7888e-04 - 14s/epoch - 170us/sample
Epoch 75/90
84077/84077 - 14s - loss: 1.9611e-04 - val_loss: 1.8200e-04 - 14s/epoch - 167us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.9336e-04 - val_loss: 1.7889e-04 - 14s/epoch - 167us/sample
Epoch 77/90
84077/84077 - 14s - loss: 1.9199e-04 - val_loss: 1.8075e-04 - 14s/epoch - 167us/sample
Epoch 78/90
84077/84077 - 14s - loss: 1.9113e-04 - val_loss: 1.8068e-04 - 14s/epoch - 167us/sample
Epoch 79/90
84077/84077 - 14s - loss: 1.9110e-04 - val_loss: 1.7704e-04 - 14s/epoch - 170us/sample
Epoch 80/90
84077/84077 - 14s - loss: 1.9128e-04 - val_loss: 1.7795e-04 - 14s/epoch - 167us/sample
Epoch 81/90
84077/84077 - 14s - loss: 1.9055e-04 - val_loss: 1.8038e-04 - 14s/epoch - 167us/sample
Epoch 82/90
84077/84077 - 14s - loss: 1.8953e-04 - val_loss: 1.7846e-04 - 14s/epoch - 167us/sample
Epoch 83/90
84077/84077 - 15s - loss: 1.8967e-04 - val_loss: 1.7966e-04 - 15s/epoch - 175us/sample
Epoch 84/90
84077/84077 - 14s - loss: 1.9118e-04 - val_loss: 1.7723e-04 - 14s/epoch - 170us/sample
Epoch 85/90
84077/84077 - 14s - loss: 1.8891e-04 - val_loss: 1.7758e-04 - 14s/epoch - 168us/sample
Epoch 86/90
84077/84077 - 14s - loss: 1.8835e-04 - val_loss: 1.7673e-04 - 14s/epoch - 167us/sample
Epoch 87/90
84077/84077 - 14s - loss: 1.8916e-04 - val_loss: 2.0668e-04 - 14s/epoch - 167us/sample
Epoch 88/90
84077/84077 - 14s - loss: 1.9090e-04 - val_loss: 1.7580e-04 - 14s/epoch - 167us/sample
Epoch 89/90
84077/84077 - 14s - loss: 1.8888e-04 - val_loss: 1.7706e-04 - 14s/epoch - 170us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.8740e-04 - val_loss: 1.7367e-04 - 14s/epoch - 167us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017367058937884253
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 12:19:38.104329: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_87/outputlayer/BiasAdd' id:97068 op device:{requested: '', assigned: ''} def:{{{node decoder_model_87/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_87/outputlayer/MatMul, decoder_model_87/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01987620401172614
cosine 0.019636531636003058
MAE: 0.0018819753308948258
RMSE: 0.008219707338932156
r2: 0.9476647129836014
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 90, 0.0012000000000000001, 0.2, 188, 0.00018740366151283124, 0.00017367058937884253, 0.01987620401172614, 0.019636531636003058, 0.0018819753308948258, 0.008219707338932156, 0.9476647129836014, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0012000000000000001 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_225 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_225 (ReLU)               (None, 2074)         0           ['batch_normalization_225[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_225[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_225[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.2_cr0.2_bs64_ep90_loss_mse_lr0.0012000000000000001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-13 12:20:18.591488: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_225/moving_variance/Assign' id:97958 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_225/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_225/moving_variance, batch_normalization_225/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-13 12:20:36.115537: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_89/kernel/v/Assign' id:99139 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_89/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_89/kernel/v, dense_dec0_89/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 12:20:53.472410: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_89/outputlayer/BiasAdd' id:98770 op device:{requested: '', assigned: ''} def:{{{node decoder_model_89/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_89/outputlayer/MatMul, decoder_model_89/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.020062632531745634
cosine 0.019818696234196014
MAE: 0.0018823675316808487
RMSE: 0.008229356294479342
r2: 0.9475562396940258
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.2custom_VAE', 'mse', 64, 90, 0.0012000000000000001, 0.2, 188, '--', '--', 0.020062632531745634, 0.019818696234196014, 0.0018823675316808487, 0.008229356294479342, 0.9475562396940258, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 90 0.0012000000000000001 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_228 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_228 (ReLU)               (None, 2168)         0           ['batch_normalization_228[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_228[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_228[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 12:21:32.355795: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_229/moving_mean/Assign' id:99511 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_229/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_229/moving_mean, batch_normalization_229/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 12:21:55.617915: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_90/mul' id:99774 op device:{requested: '', assigned: ''} def:{{{node loss_90/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_90/mul/x, loss_90/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 40s - loss: 0.0151 - val_loss: 0.0017 - 40s/epoch - 470us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0019 - val_loss: 0.0014 - 14s/epoch - 171us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0021 - val_loss: 0.0015 - 14s/epoch - 168us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0014 - val_loss: 0.0058 - 14s/epoch - 169us/sample
Epoch 5/90
84077/84077 - 14s - loss: 0.0016 - val_loss: 0.0013 - 14s/epoch - 169us/sample
Epoch 6/90
84077/84077 - 14s - loss: 0.0012 - val_loss: 8.7211e-04 - 14s/epoch - 169us/sample
Epoch 7/90
84077/84077 - 14s - loss: 8.6930e-04 - val_loss: 6.8586e-04 - 14s/epoch - 170us/sample
Epoch 8/90
84077/84077 - 14s - loss: 6.7717e-04 - val_loss: 5.7938e-04 - 14s/epoch - 170us/sample
Epoch 9/90
84077/84077 - 14s - loss: 6.2650e-04 - val_loss: 5.5861e-04 - 14s/epoch - 169us/sample
Epoch 10/90
84077/84077 - 14s - loss: 5.4951e-04 - val_loss: 4.7541e-04 - 14s/epoch - 169us/sample
Epoch 11/90
84077/84077 - 14s - loss: 4.8315e-04 - val_loss: 4.3342e-04 - 14s/epoch - 169us/sample
Epoch 12/90
84077/84077 - 14s - loss: 4.5067e-04 - val_loss: 3.8354e-04 - 14s/epoch - 169us/sample
Epoch 13/90
84077/84077 - 14s - loss: 4.1807e-04 - val_loss: 3.5419e-04 - 14s/epoch - 171us/sample
Epoch 14/90
84077/84077 - 14s - loss: 3.9095e-04 - val_loss: 3.3633e-04 - 14s/epoch - 169us/sample
Epoch 15/90
84077/84077 - 14s - loss: 3.7254e-04 - val_loss: 3.2731e-04 - 14s/epoch - 169us/sample
Epoch 16/90
84077/84077 - 14s - loss: 3.5732e-04 - val_loss: 3.0104e-04 - 14s/epoch - 169us/sample
Epoch 17/90
84077/84077 - 14s - loss: 3.4008e-04 - val_loss: 2.9517e-04 - 14s/epoch - 169us/sample
Epoch 18/90
84077/84077 - 14s - loss: 3.2971e-04 - val_loss: 2.8431e-04 - 14s/epoch - 171us/sample
Epoch 19/90
84077/84077 - 14s - loss: 3.1779e-04 - val_loss: 2.8437e-04 - 14s/epoch - 170us/sample
Epoch 20/90
84077/84077 - 14s - loss: 3.1065e-04 - val_loss: 2.7274e-04 - 14s/epoch - 169us/sample
Epoch 21/90
84077/84077 - 14s - loss: 3.0197e-04 - val_loss: 2.6354e-04 - 14s/epoch - 169us/sample
Epoch 22/90
84077/84077 - 14s - loss: 2.9492e-04 - val_loss: 2.6087e-04 - 14s/epoch - 170us/sample
Epoch 23/90
84077/84077 - 14s - loss: 2.8923e-04 - val_loss: 2.5109e-04 - 14s/epoch - 172us/sample
Epoch 24/90
84077/84077 - 14s - loss: 2.8534e-04 - val_loss: 2.4672e-04 - 14s/epoch - 169us/sample
Epoch 25/90
84077/84077 - 14s - loss: 2.7851e-04 - val_loss: 2.3991e-04 - 14s/epoch - 169us/sample
Epoch 26/90
84077/84077 - 14s - loss: 2.7418e-04 - val_loss: 2.4130e-04 - 14s/epoch - 169us/sample
Epoch 27/90
84077/84077 - 14s - loss: 2.7072e-04 - val_loss: 2.3497e-04 - 14s/epoch - 169us/sample
Epoch 28/90
84077/84077 - 14s - loss: 2.6587e-04 - val_loss: 2.3226e-04 - 14s/epoch - 171us/sample
Epoch 29/90
84077/84077 - 14s - loss: 2.6258e-04 - val_loss: 2.2760e-04 - 14s/epoch - 169us/sample
Epoch 30/90
84077/84077 - 14s - loss: 2.5965e-04 - val_loss: 2.2603e-04 - 14s/epoch - 169us/sample
Epoch 31/90
84077/84077 - 14s - loss: 2.5679e-04 - val_loss: 2.2872e-04 - 14s/epoch - 169us/sample
Epoch 32/90
84077/84077 - 14s - loss: 2.5529e-04 - val_loss: 2.2304e-04 - 14s/epoch - 170us/sample
Epoch 33/90
84077/84077 - 14s - loss: 2.5342e-04 - val_loss: 2.2608e-04 - 14s/epoch - 171us/sample
Epoch 34/90
84077/84077 - 14s - loss: 2.5101e-04 - val_loss: 2.1804e-04 - 14s/epoch - 170us/sample
Epoch 35/90
84077/84077 - 14s - loss: 2.4906e-04 - val_loss: 2.2071e-04 - 14s/epoch - 169us/sample
Epoch 36/90
84077/84077 - 14s - loss: 2.4819e-04 - val_loss: 2.1812e-04 - 14s/epoch - 169us/sample
Epoch 37/90
84077/84077 - 14s - loss: 2.4747e-04 - val_loss: 2.1565e-04 - 14s/epoch - 169us/sample
Epoch 38/90
84077/84077 - 14s - loss: 2.4446e-04 - val_loss: 2.1601e-04 - 14s/epoch - 171us/sample
Epoch 39/90
84077/84077 - 14s - loss: 2.4271e-04 - val_loss: 2.1655e-04 - 14s/epoch - 169us/sample
Epoch 40/90
84077/84077 - 14s - loss: 2.4195e-04 - val_loss: 2.1454e-04 - 14s/epoch - 169us/sample
Epoch 41/90
84077/84077 - 14s - loss: 2.4123e-04 - val_loss: 2.1183e-04 - 14s/epoch - 169us/sample
Epoch 42/90
84077/84077 - 14s - loss: 2.3901e-04 - val_loss: 2.1094e-04 - 14s/epoch - 169us/sample
Epoch 43/90
84077/84077 - 14s - loss: 2.3879e-04 - val_loss: 2.1383e-04 - 14s/epoch - 171us/sample
Epoch 44/90
84077/84077 - 14s - loss: 2.3659e-04 - val_loss: 2.0996e-04 - 14s/epoch - 170us/sample
Epoch 45/90
84077/84077 - 14s - loss: 2.3659e-04 - val_loss: 2.1130e-04 - 14s/epoch - 169us/sample
Epoch 46/90
84077/84077 - 14s - loss: 2.3499e-04 - val_loss: 2.0896e-04 - 14s/epoch - 169us/sample
Epoch 47/90
84077/84077 - 14s - loss: 2.3406e-04 - val_loss: 2.0875e-04 - 14s/epoch - 169us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.3353e-04 - val_loss: 2.0821e-04 - 14s/epoch - 171us/sample
Epoch 49/90
84077/84077 - 14s - loss: 2.3186e-04 - val_loss: 2.0603e-04 - 14s/epoch - 170us/sample
Epoch 50/90
84077/84077 - 14s - loss: 2.3100e-04 - val_loss: 2.0752e-04 - 14s/epoch - 169us/sample
Epoch 51/90
84077/84077 - 14s - loss: 2.3057e-04 - val_loss: 2.0479e-04 - 14s/epoch - 169us/sample
Epoch 52/90
84077/84077 - 14s - loss: 2.2935e-04 - val_loss: 2.0602e-04 - 14s/epoch - 169us/sample
Epoch 53/90
84077/84077 - 14s - loss: 2.2860e-04 - val_loss: 2.0636e-04 - 14s/epoch - 170us/sample
Epoch 54/90
84077/84077 - 14s - loss: 2.2915e-04 - val_loss: 2.0465e-04 - 14s/epoch - 170us/sample
Epoch 55/90
84077/84077 - 14s - loss: 2.2771e-04 - val_loss: 2.0619e-04 - 14s/epoch - 169us/sample
Epoch 56/90
84077/84077 - 14s - loss: 2.2722e-04 - val_loss: 2.0225e-04 - 14s/epoch - 169us/sample
Epoch 57/90
84077/84077 - 14s - loss: 2.2659e-04 - val_loss: 2.0513e-04 - 14s/epoch - 169us/sample
Epoch 58/90
84077/84077 - 14s - loss: 2.2472e-04 - val_loss: 2.0119e-04 - 14s/epoch - 170us/sample
Epoch 59/90
84077/84077 - 14s - loss: 2.2457e-04 - val_loss: 2.0339e-04 - 14s/epoch - 171us/sample
Epoch 60/90
84077/84077 - 14s - loss: 2.2375e-04 - val_loss: 2.0358e-04 - 14s/epoch - 169us/sample
Epoch 61/90
84077/84077 - 14s - loss: 2.2358e-04 - val_loss: 2.0004e-04 - 14s/epoch - 169us/sample
Epoch 62/90
84077/84077 - 14s - loss: 2.2286e-04 - val_loss: 2.0288e-04 - 14s/epoch - 169us/sample
Epoch 63/90
84077/84077 - 14s - loss: 2.2292e-04 - val_loss: 1.9991e-04 - 14s/epoch - 169us/sample
Epoch 64/90
84077/84077 - 14s - loss: 2.2114e-04 - val_loss: 2.0114e-04 - 14s/epoch - 172us/sample
Epoch 65/90
84077/84077 - 14s - loss: 2.2055e-04 - val_loss: 1.9960e-04 - 14s/epoch - 169us/sample
Epoch 66/90
84077/84077 - 14s - loss: 2.2046e-04 - val_loss: 2.0106e-04 - 14s/epoch - 169us/sample
Epoch 67/90
84077/84077 - 14s - loss: 2.2032e-04 - val_loss: 2.0019e-04 - 14s/epoch - 169us/sample
Epoch 68/90
84077/84077 - 14s - loss: 2.1979e-04 - val_loss: 1.9810e-04 - 14s/epoch - 170us/sample
Epoch 69/90
84077/84077 - 14s - loss: 2.1915e-04 - val_loss: 1.9615e-04 - 14s/epoch - 171us/sample
Epoch 70/90
84077/84077 - 14s - loss: 2.1791e-04 - val_loss: 1.9665e-04 - 14s/epoch - 169us/sample
Epoch 71/90
84077/84077 - 14s - loss: 2.1848e-04 - val_loss: 1.9677e-04 - 14s/epoch - 169us/sample
Epoch 72/90
84077/84077 - 14s - loss: 2.1714e-04 - val_loss: 1.9333e-04 - 14s/epoch - 169us/sample
Epoch 73/90
84077/84077 - 14s - loss: 2.1746e-04 - val_loss: 1.9857e-04 - 14s/epoch - 169us/sample
Epoch 74/90
84077/84077 - 14s - loss: 2.1660e-04 - val_loss: 1.9692e-04 - 14s/epoch - 172us/sample
Epoch 75/90
84077/84077 - 14s - loss: 2.1622e-04 - val_loss: 1.9531e-04 - 14s/epoch - 169us/sample
Epoch 76/90
84077/84077 - 14s - loss: 2.1512e-04 - val_loss: 1.9421e-04 - 14s/epoch - 169us/sample
Epoch 77/90
84077/84077 - 14s - loss: 2.1520e-04 - val_loss: 1.9436e-04 - 14s/epoch - 169us/sample
Epoch 78/90
84077/84077 - 14s - loss: 2.1488e-04 - val_loss: 1.9579e-04 - 14s/epoch - 169us/sample
Epoch 79/90
84077/84077 - 14s - loss: 2.1439e-04 - val_loss: 1.9569e-04 - 14s/epoch - 171us/sample
Epoch 80/90
84077/84077 - 14s - loss: 2.1409e-04 - val_loss: 1.9535e-04 - 14s/epoch - 169us/sample
Epoch 81/90
84077/84077 - 14s - loss: 2.1320e-04 - val_loss: 1.9381e-04 - 14s/epoch - 169us/sample
Epoch 82/90
84077/84077 - 14s - loss: 2.1319e-04 - val_loss: 1.9368e-04 - 14s/epoch - 169us/sample
Epoch 83/90
84077/84077 - 14s - loss: 2.1262e-04 - val_loss: 1.9342e-04 - 14s/epoch - 169us/sample
Epoch 84/90
84077/84077 - 14s - loss: 2.1170e-04 - val_loss: 1.9151e-04 - 14s/epoch - 171us/sample
Epoch 85/90
84077/84077 - 14s - loss: 2.1201e-04 - val_loss: 1.9163e-04 - 14s/epoch - 169us/sample
Epoch 86/90
84077/84077 - 14s - loss: 2.1148e-04 - val_loss: 1.9182e-04 - 14s/epoch - 169us/sample
Epoch 87/90
84077/84077 - 14s - loss: 2.1109e-04 - val_loss: 1.9445e-04 - 14s/epoch - 169us/sample
Epoch 88/90
84077/84077 - 14s - loss: 2.1038e-04 - val_loss: 1.9057e-04 - 14s/epoch - 169us/sample
Epoch 89/90
84077/84077 - 14s - loss: 2.0991e-04 - val_loss: 1.9124e-04 - 14s/epoch - 172us/sample
Epoch 90/90
84077/84077 - 14s - loss: 2.1014e-04 - val_loss: 1.9049e-04 - 14s/epoch - 169us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00019049488057531843
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 12:43:12.982059: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_90/outputlayer/BiasAdd' id:99745 op device:{requested: '', assigned: ''} def:{{{node decoder_model_90/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_90/outputlayer/MatMul, decoder_model_90/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02502656476854899
cosine 0.024716436879380504
MAE: 0.0028202641128886635
RMSE: 0.009315233109542909
r2: 0.9324291781280387
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'mse', 64, 90, 0.0012000000000000001, 0.2, 188, 0.00021013891656013637, 0.00019049488057531843, 0.02502656476854899, 0.024716436879380504, 0.0028202641128886635, 0.009315233109542909, 0.9324291781280387, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.001 8 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_231 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_231 (ReLU)               (None, 2074)         0           ['batch_normalization_231[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_231[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_231[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 12:43:54.149506: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_232/moving_variance/Assign' id:100774 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_232/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_232/moving_variance, batch_normalization_232/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 12:45:09.937028: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_91/mul' id:101032 op device:{requested: '', assigned: ''} def:{{{node loss_91/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_91/mul/x, loss_91/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 96s - loss: 0.0362 - val_loss: 0.0032 - 96s/epoch - 1ms/sample
Epoch 2/90
84077/84077 - 69s - loss: 0.0013 - val_loss: 0.0012 - 69s/epoch - 824us/sample
Epoch 3/90
84077/84077 - 69s - loss: 0.0011 - val_loss: 0.0121 - 69s/epoch - 823us/sample
Epoch 4/90
84077/84077 - 69s - loss: 9.1553e-04 - val_loss: 0.0853 - 69s/epoch - 824us/sample
Epoch 5/90
84077/84077 - 69s - loss: 8.6224e-04 - val_loss: 0.2147 - 69s/epoch - 827us/sample
Epoch 6/90
84077/84077 - 69s - loss: 8.3154e-04 - val_loss: 0.7184 - 69s/epoch - 823us/sample
Epoch 7/90
84077/84077 - 69s - loss: 7.9933e-04 - val_loss: 1.7382 - 69s/epoch - 825us/sample
Epoch 8/90
84077/84077 - 69s - loss: 7.5831e-04 - val_loss: 2.0501 - 69s/epoch - 823us/sample
Epoch 9/90
84077/84077 - 69s - loss: 7.2397e-04 - val_loss: 1.4041 - 69s/epoch - 824us/sample
Epoch 10/90
84077/84077 - 69s - loss: 6.9649e-04 - val_loss: 1.6987 - 69s/epoch - 825us/sample
Epoch 11/90
84077/84077 - 69s - loss: 6.6711e-04 - val_loss: 0.9018 - 69s/epoch - 822us/sample
Epoch 12/90
84077/84077 - 69s - loss: 6.4453e-04 - val_loss: 1.2802 - 69s/epoch - 826us/sample
Epoch 13/90
84077/84077 - 69s - loss: 6.2374e-04 - val_loss: 0.6944 - 69s/epoch - 823us/sample
Epoch 14/90
84077/84077 - 69s - loss: 6.0811e-04 - val_loss: 0.5557 - 69s/epoch - 826us/sample
Epoch 15/90
84077/84077 - 69s - loss: 5.9610e-04 - val_loss: 0.3564 - 69s/epoch - 826us/sample
Epoch 16/90
84077/84077 - 69s - loss: 5.8581e-04 - val_loss: 0.2316 - 69s/epoch - 825us/sample
Epoch 17/90
84077/84077 - 69s - loss: 5.7664e-04 - val_loss: 0.1847 - 69s/epoch - 821us/sample
Epoch 18/90
84077/84077 - 69s - loss: 5.7167e-04 - val_loss: 0.1610 - 69s/epoch - 824us/sample
Epoch 19/90
84077/84077 - 69s - loss: 5.6835e-04 - val_loss: 0.1015 - 69s/epoch - 824us/sample
Epoch 20/90
84077/84077 - 69s - loss: 5.6461e-04 - val_loss: 0.0908 - 69s/epoch - 822us/sample
Epoch 21/90
84077/84077 - 69s - loss: 5.6163e-04 - val_loss: 0.0779 - 69s/epoch - 822us/sample
Epoch 22/90
84077/84077 - 69s - loss: 5.5870e-04 - val_loss: 0.0651 - 69s/epoch - 821us/sample
Epoch 23/90
84077/84077 - 69s - loss: 5.5694e-04 - val_loss: 0.0401 - 69s/epoch - 824us/sample
Epoch 24/90
84077/84077 - 69s - loss: 5.5568e-04 - val_loss: 0.0264 - 69s/epoch - 824us/sample
Epoch 25/90
84077/84077 - 69s - loss: 5.5330e-04 - val_loss: 0.0182 - 69s/epoch - 824us/sample
Epoch 26/90
84077/84077 - 69s - loss: 5.5298e-04 - val_loss: 0.0150 - 69s/epoch - 822us/sample
Epoch 27/90
84077/84077 - 69s - loss: 5.5269e-04 - val_loss: 0.0099 - 69s/epoch - 823us/sample
Epoch 28/90
84077/84077 - 69s - loss: 5.5195e-04 - val_loss: 0.0113 - 69s/epoch - 822us/sample
Epoch 29/90
84077/84077 - 69s - loss: 5.4989e-04 - val_loss: 0.0117 - 69s/epoch - 823us/sample
Epoch 30/90
84077/84077 - 69s - loss: 5.4950e-04 - val_loss: 0.0087 - 69s/epoch - 822us/sample
Epoch 31/90
84077/84077 - 69s - loss: 5.5019e-04 - val_loss: 0.0093 - 69s/epoch - 822us/sample
Epoch 32/90
84077/84077 - 69s - loss: 5.4888e-04 - val_loss: 0.0099 - 69s/epoch - 826us/sample
Epoch 33/90
84077/84077 - 69s - loss: 5.4873e-04 - val_loss: 0.0086 - 69s/epoch - 824us/sample
Epoch 34/90
84077/84077 - 69s - loss: 5.4722e-04 - val_loss: 0.0080 - 69s/epoch - 822us/sample
Epoch 35/90
84077/84077 - 69s - loss: 5.4788e-04 - val_loss: 0.0088 - 69s/epoch - 824us/sample
Epoch 36/90
84077/84077 - 69s - loss: 5.4645e-04 - val_loss: 0.0087 - 69s/epoch - 823us/sample
Epoch 37/90
84077/84077 - 69s - loss: 5.4692e-04 - val_loss: 0.0095 - 69s/epoch - 821us/sample
Epoch 38/90
84077/84077 - 69s - loss: 5.4595e-04 - val_loss: 0.0096 - 69s/epoch - 825us/sample
Epoch 39/90
84077/84077 - 69s - loss: 5.4659e-04 - val_loss: 0.0082 - 69s/epoch - 823us/sample
Epoch 40/90
84077/84077 - 69s - loss: 5.4575e-04 - val_loss: 0.0088 - 69s/epoch - 825us/sample
Epoch 41/90
84077/84077 - 69s - loss: 5.4598e-04 - val_loss: 0.0078 - 69s/epoch - 823us/sample
Epoch 42/90
84077/84077 - 69s - loss: 5.4486e-04 - val_loss: 0.0092 - 69s/epoch - 826us/sample
Epoch 43/90
84077/84077 - 69s - loss: 5.4508e-04 - val_loss: 0.0109 - 69s/epoch - 825us/sample
Epoch 44/90
84077/84077 - 69s - loss: 5.4459e-04 - val_loss: 0.0072 - 69s/epoch - 823us/sample
Epoch 45/90
84077/84077 - 69s - loss: 5.4440e-04 - val_loss: 0.0075 - 69s/epoch - 824us/sample
Epoch 46/90
84077/84077 - 69s - loss: 5.4425e-04 - val_loss: 0.0092 - 69s/epoch - 824us/sample
Epoch 47/90
84077/84077 - 69s - loss: 5.4327e-04 - val_loss: 0.0082 - 69s/epoch - 823us/sample
Epoch 48/90
84077/84077 - 69s - loss: 5.4387e-04 - val_loss: 0.0075 - 69s/epoch - 823us/sample
Epoch 49/90
84077/84077 - 69s - loss: 5.4343e-04 - val_loss: 0.0068 - 69s/epoch - 821us/sample
Epoch 50/90
84077/84077 - 70s - loss: 5.4249e-04 - val_loss: 0.0087 - 70s/epoch - 827us/sample
Epoch 51/90
84077/84077 - 69s - loss: 5.4320e-04 - val_loss: 0.0086 - 69s/epoch - 826us/sample
Epoch 52/90
84077/84077 - 69s - loss: 5.4222e-04 - val_loss: 0.0066 - 69s/epoch - 822us/sample
Epoch 53/90
84077/84077 - 69s - loss: 5.4257e-04 - val_loss: 0.0092 - 69s/epoch - 825us/sample
Epoch 54/90
84077/84077 - 69s - loss: 5.4203e-04 - val_loss: 0.0074 - 69s/epoch - 822us/sample
Epoch 55/90
84077/84077 - 69s - loss: 5.4285e-04 - val_loss: 0.0087 - 69s/epoch - 822us/sample
Epoch 56/90
84077/84077 - 69s - loss: 5.4225e-04 - val_loss: 0.0085 - 69s/epoch - 825us/sample
Epoch 57/90
84077/84077 - 69s - loss: 5.4176e-04 - val_loss: 0.0071 - 69s/epoch - 826us/sample
Epoch 58/90
84077/84077 - 69s - loss: 5.4226e-04 - val_loss: 0.0080 - 69s/epoch - 826us/sample
Epoch 59/90
84077/84077 - 69s - loss: 5.4109e-04 - val_loss: 0.0088 - 69s/epoch - 821us/sample
Epoch 60/90
84077/84077 - 69s - loss: 5.4165e-04 - val_loss: 0.0065 - 69s/epoch - 825us/sample
Epoch 61/90
84077/84077 - 69s - loss: 5.4079e-04 - val_loss: 0.0078 - 69s/epoch - 825us/sample
Epoch 62/90
84077/84077 - 69s - loss: 5.4010e-04 - val_loss: 0.0074 - 69s/epoch - 825us/sample
Epoch 63/90
84077/84077 - 70s - loss: 5.4060e-04 - val_loss: 0.0072 - 70s/epoch - 829us/sample
Epoch 64/90
84077/84077 - 69s - loss: 5.3905e-04 - val_loss: 0.0083 - 69s/epoch - 824us/sample
Epoch 65/90
84077/84077 - 69s - loss: 5.3919e-04 - val_loss: 0.0076 - 69s/epoch - 821us/sample
Epoch 66/90
84077/84077 - 70s - loss: 5.3940e-04 - val_loss: 0.0078 - 70s/epoch - 828us/sample
Epoch 67/90
84077/84077 - 69s - loss: 5.3937e-04 - val_loss: 0.0098 - 69s/epoch - 826us/sample
Epoch 68/90
84077/84077 - 70s - loss: 5.3947e-04 - val_loss: 0.0076 - 70s/epoch - 827us/sample
Epoch 69/90
84077/84077 - 69s - loss: 5.3906e-04 - val_loss: 0.0088 - 69s/epoch - 826us/sample
Epoch 70/90
84077/84077 - 69s - loss: 5.3806e-04 - val_loss: 0.0081 - 69s/epoch - 825us/sample
Epoch 71/90
84077/84077 - 69s - loss: 5.3930e-04 - val_loss: 0.0098 - 69s/epoch - 822us/sample
Epoch 72/90
84077/84077 - 69s - loss: 5.3988e-04 - val_loss: 0.0073 - 69s/epoch - 825us/sample
Epoch 73/90
84077/84077 - 69s - loss: 5.3906e-04 - val_loss: 0.0064 - 69s/epoch - 824us/sample
Epoch 74/90
84077/84077 - 69s - loss: 5.3843e-04 - val_loss: 0.0085 - 69s/epoch - 824us/sample
Epoch 75/90
84077/84077 - 69s - loss: 5.3823e-04 - val_loss: 0.0073 - 69s/epoch - 826us/sample
Epoch 76/90
84077/84077 - 69s - loss: 5.3892e-04 - val_loss: 0.0068 - 69s/epoch - 822us/sample
Epoch 77/90
84077/84077 - 69s - loss: 5.3826e-04 - val_loss: 0.0068 - 69s/epoch - 823us/sample
Epoch 78/90
84077/84077 - 69s - loss: 5.3858e-04 - val_loss: 0.0078 - 69s/epoch - 824us/sample
Epoch 79/90
84077/84077 - 69s - loss: 5.3744e-04 - val_loss: 0.0087 - 69s/epoch - 825us/sample
Epoch 80/90
84077/84077 - 69s - loss: 5.3795e-04 - val_loss: 0.0075 - 69s/epoch - 825us/sample
Epoch 81/90
84077/84077 - 69s - loss: 5.3777e-04 - val_loss: 0.0083 - 69s/epoch - 821us/sample
Epoch 82/90
84077/84077 - 69s - loss: 5.3767e-04 - val_loss: 0.0081 - 69s/epoch - 825us/sample
Epoch 83/90
84077/84077 - 69s - loss: 5.3720e-04 - val_loss: 0.0074 - 69s/epoch - 826us/sample
Epoch 84/90
84077/84077 - 69s - loss: 5.3762e-04 - val_loss: 0.0079 - 69s/epoch - 825us/sample
Epoch 85/90
84077/84077 - 69s - loss: 5.3784e-04 - val_loss: 0.0090 - 69s/epoch - 826us/sample
Epoch 86/90
84077/84077 - 69s - loss: 5.3785e-04 - val_loss: 0.0077 - 69s/epoch - 824us/sample
Epoch 87/90
84077/84077 - 69s - loss: 5.3705e-04 - val_loss: 0.0075 - 69s/epoch - 822us/sample
Epoch 88/90
84077/84077 - 69s - loss: 5.3598e-04 - val_loss: 0.0081 - 69s/epoch - 826us/sample
Epoch 89/90
84077/84077 - 69s - loss: 5.3743e-04 - val_loss: 0.0073 - 69s/epoch - 825us/sample
Epoch 90/90
84077/84077 - 69s - loss: 5.3645e-04 - val_loss: 0.0079 - 69s/epoch - 822us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.007922051332465702
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 14:28:08.225841: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_91/outputlayer/BiasAdd' id:101003 op device:{requested: '', assigned: ''} def:{{{node decoder_model_91/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_91/outputlayer/MatMul, decoder_model_91/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.29437181523441086
cosine 0.2901476793696146
MAE: 0.012168270065725318
RMSE: 0.08670256974934587
r2: -4.87221102843271
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 8, 90, 0.001, 0.2, 188, 0.0005364533985261823, 0.007922051332465702, 0.29437181523441086, 0.2901476793696146, 0.012168270065725318, 0.08670256974934587, -4.87221102843271, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.001 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_234 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_234 (ReLU)               (None, 1980)         0           ['batch_normalization_234[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_234[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_234[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 14:28:49.471217: W tensorflow/c/c_api.cc:291] Operation '{name:'training_128/Adam/batch_normalization_235/gamma/v/Assign' id:102916 op device:{requested: '', assigned: ''} def:{{{node training_128/Adam/batch_normalization_235/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_128/Adam/batch_normalization_235/gamma/v, training_128/Adam/batch_normalization_235/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 14:29:13.013600: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_92/mul' id:102287 op device:{requested: '', assigned: ''} def:{{{node loss_92/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_92/mul/x, loss_92/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 40s - loss: 0.0069 - val_loss: 0.0020 - 40s/epoch - 474us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0018 - val_loss: 0.0014 - 14s/epoch - 172us/sample
Epoch 3/85
84077/84077 - 15s - loss: 0.0020 - val_loss: 0.0024 - 15s/epoch - 174us/sample
Epoch 4/85
84077/84077 - 14s - loss: 0.0011 - val_loss: 8.7752e-04 - 14s/epoch - 172us/sample
Epoch 5/85
84077/84077 - 14s - loss: 9.0415e-04 - val_loss: 7.1986e-04 - 14s/epoch - 172us/sample
Epoch 6/85
84077/84077 - 14s - loss: 7.8632e-04 - val_loss: 6.3625e-04 - 14s/epoch - 171us/sample
Epoch 7/85
84077/84077 - 14s - loss: 6.6504e-04 - val_loss: 5.9171e-04 - 14s/epoch - 172us/sample
Epoch 8/85
84077/84077 - 15s - loss: 5.6263e-04 - val_loss: 4.4019e-04 - 15s/epoch - 174us/sample
Epoch 9/85
84077/84077 - 14s - loss: 5.3209e-04 - val_loss: 3.9099e-04 - 14s/epoch - 172us/sample
Epoch 10/85
84077/84077 - 14s - loss: 4.3487e-04 - val_loss: 3.5220e-04 - 14s/epoch - 171us/sample
Epoch 11/85
84077/84077 - 14s - loss: 3.8698e-04 - val_loss: 3.3665e-04 - 14s/epoch - 172us/sample
Epoch 12/85
84077/84077 - 14s - loss: 3.7791e-04 - val_loss: 3.0516e-04 - 14s/epoch - 172us/sample
Epoch 13/85
84077/84077 - 15s - loss: 3.3179e-04 - val_loss: 2.8374e-04 - 15s/epoch - 174us/sample
Epoch 14/85
84077/84077 - 14s - loss: 3.0842e-04 - val_loss: 2.6590e-04 - 14s/epoch - 171us/sample
Epoch 15/85
84077/84077 - 14s - loss: 2.9624e-04 - val_loss: 2.5643e-04 - 14s/epoch - 171us/sample
Epoch 16/85
84077/84077 - 14s - loss: 2.8455e-04 - val_loss: 2.5042e-04 - 14s/epoch - 172us/sample
Epoch 17/85
84077/84077 - 14s - loss: 2.7562e-04 - val_loss: 2.5712e-04 - 14s/epoch - 172us/sample
Epoch 18/85
84077/84077 - 15s - loss: 2.6820e-04 - val_loss: 2.4063e-04 - 15s/epoch - 174us/sample
Epoch 19/85
84077/84077 - 14s - loss: 2.6041e-04 - val_loss: 2.3059e-04 - 14s/epoch - 171us/sample
Epoch 20/85
84077/84077 - 14s - loss: 2.5574e-04 - val_loss: 2.3009e-04 - 14s/epoch - 171us/sample
Epoch 21/85
84077/84077 - 14s - loss: 2.5058e-04 - val_loss: 2.2353e-04 - 14s/epoch - 172us/sample
Epoch 22/85
84077/84077 - 14s - loss: 2.4599e-04 - val_loss: 2.2063e-04 - 14s/epoch - 172us/sample
Epoch 23/85
84077/84077 - 15s - loss: 2.4019e-04 - val_loss: 2.1618e-04 - 15s/epoch - 174us/sample
Epoch 24/85
84077/84077 - 14s - loss: 2.3824e-04 - val_loss: 2.1190e-04 - 14s/epoch - 172us/sample
Epoch 25/85
84077/84077 - 14s - loss: 2.3560e-04 - val_loss: 2.1447e-04 - 14s/epoch - 172us/sample
Epoch 26/85
84077/84077 - 14s - loss: 2.3170e-04 - val_loss: 2.1054e-04 - 14s/epoch - 172us/sample
Epoch 27/85
84077/84077 - 14s - loss: 2.2968e-04 - val_loss: 2.0614e-04 - 14s/epoch - 172us/sample
Epoch 28/85
84077/84077 - 15s - loss: 2.2676e-04 - val_loss: 2.0150e-04 - 15s/epoch - 174us/sample
Epoch 29/85
84077/84077 - 14s - loss: 2.2517e-04 - val_loss: 2.0092e-04 - 14s/epoch - 171us/sample
Epoch 30/85
84077/84077 - 14s - loss: 2.2219e-04 - val_loss: 1.9970e-04 - 14s/epoch - 172us/sample
Epoch 31/85
84077/84077 - 14s - loss: 2.2070e-04 - val_loss: 2.0035e-04 - 14s/epoch - 171us/sample
Epoch 32/85
84077/84077 - 14s - loss: 2.1841e-04 - val_loss: 1.9890e-04 - 14s/epoch - 172us/sample
Epoch 33/85
84077/84077 - 15s - loss: 2.1696e-04 - val_loss: 1.9201e-04 - 15s/epoch - 174us/sample
Epoch 34/85
84077/84077 - 14s - loss: 2.1512e-04 - val_loss: 1.9376e-04 - 14s/epoch - 172us/sample
Epoch 35/85
84077/84077 - 14s - loss: 2.1475e-04 - val_loss: 1.9491e-04 - 14s/epoch - 171us/sample
Epoch 36/85
84077/84077 - 14s - loss: 2.1282e-04 - val_loss: 1.9146e-04 - 14s/epoch - 172us/sample
Epoch 37/85
84077/84077 - 14s - loss: 2.1258e-04 - val_loss: 1.9150e-04 - 14s/epoch - 172us/sample
Epoch 38/85
84077/84077 - 15s - loss: 2.1087e-04 - val_loss: 1.8994e-04 - 15s/epoch - 174us/sample
Epoch 39/85
84077/84077 - 14s - loss: 2.0910e-04 - val_loss: 1.8947e-04 - 14s/epoch - 171us/sample
Epoch 40/85
84077/84077 - 14s - loss: 2.0796e-04 - val_loss: 1.8722e-04 - 14s/epoch - 172us/sample
Epoch 41/85
84077/84077 - 14s - loss: 2.0623e-04 - val_loss: 1.8852e-04 - 14s/epoch - 172us/sample
Epoch 42/85
84077/84077 - 14s - loss: 2.0618e-04 - val_loss: 1.8570e-04 - 14s/epoch - 172us/sample
Epoch 43/85
84077/84077 - 15s - loss: 2.0416e-04 - val_loss: 1.8894e-04 - 15s/epoch - 174us/sample
Epoch 44/85
84077/84077 - 14s - loss: 2.0723e-04 - val_loss: 2.1037e-04 - 14s/epoch - 171us/sample
Epoch 45/85
84077/84077 - 14s - loss: 2.0418e-04 - val_loss: 1.8552e-04 - 14s/epoch - 172us/sample
Epoch 46/85
84077/84077 - 14s - loss: 2.0280e-04 - val_loss: 1.8420e-04 - 14s/epoch - 171us/sample
Epoch 47/85
84077/84077 - 15s - loss: 2.0134e-04 - val_loss: 1.8330e-04 - 15s/epoch - 173us/sample
Epoch 48/85
84077/84077 - 15s - loss: 2.0081e-04 - val_loss: 1.8525e-04 - 15s/epoch - 173us/sample
Epoch 49/85
84077/84077 - 14s - loss: 1.9960e-04 - val_loss: 1.8477e-04 - 14s/epoch - 172us/sample
Epoch 50/85
84077/84077 - 14s - loss: 1.9916e-04 - val_loss: 1.8075e-04 - 14s/epoch - 171us/sample
Epoch 51/85
84077/84077 - 14s - loss: 1.9826e-04 - val_loss: 1.8259e-04 - 14s/epoch - 172us/sample
Epoch 52/85
84077/84077 - 15s - loss: 2.0077e-04 - val_loss: 1.8852e-04 - 15s/epoch - 173us/sample
Epoch 53/85
84077/84077 - 15s - loss: 1.9815e-04 - val_loss: 1.8390e-04 - 15s/epoch - 173us/sample
Epoch 54/85
84077/84077 - 14s - loss: 1.9662e-04 - val_loss: 1.8172e-04 - 14s/epoch - 171us/sample
Epoch 55/85
84077/84077 - 14s - loss: 1.9680e-04 - val_loss: 1.8039e-04 - 14s/epoch - 172us/sample
Epoch 56/85
84077/84077 - 14s - loss: 1.9491e-04 - val_loss: 1.8483e-04 - 14s/epoch - 171us/sample
Epoch 57/85
84077/84077 - 15s - loss: 1.9426e-04 - val_loss: 1.8022e-04 - 15s/epoch - 174us/sample
Epoch 58/85
84077/84077 - 14s - loss: 1.9640e-04 - val_loss: 1.8689e-04 - 14s/epoch - 172us/sample
Epoch 59/85
84077/84077 - 14s - loss: 1.9459e-04 - val_loss: 1.7930e-04 - 14s/epoch - 171us/sample
Epoch 60/85
84077/84077 - 16s - loss: 1.9278e-04 - val_loss: 1.8048e-04 - 16s/epoch - 188us/sample
Epoch 61/85
84077/84077 - 15s - loss: 1.9266e-04 - val_loss: 1.8472e-04 - 15s/epoch - 182us/sample
Epoch 62/85
84077/84077 - 16s - loss: 1.9231e-04 - val_loss: 1.7753e-04 - 16s/epoch - 191us/sample
Epoch 63/85
84077/84077 - 16s - loss: 1.9058e-04 - val_loss: 1.7559e-04 - 16s/epoch - 190us/sample
Epoch 64/85
84077/84077 - 15s - loss: 1.9056e-04 - val_loss: 1.7740e-04 - 15s/epoch - 178us/sample
Epoch 65/85
84077/84077 - 15s - loss: 1.9078e-04 - val_loss: 1.7831e-04 - 15s/epoch - 178us/sample
Epoch 66/85
84077/84077 - 15s - loss: 1.9054e-04 - val_loss: 1.7771e-04 - 15s/epoch - 179us/sample
Epoch 67/85
84077/84077 - 15s - loss: 1.9016e-04 - val_loss: 1.7439e-04 - 15s/epoch - 181us/sample
Epoch 68/85
84077/84077 - 15s - loss: 1.8976e-04 - val_loss: 1.7584e-04 - 15s/epoch - 178us/sample
Epoch 69/85
84077/84077 - 15s - loss: 1.8893e-04 - val_loss: 1.7585e-04 - 15s/epoch - 178us/sample
Epoch 70/85
84077/84077 - 15s - loss: 1.8897e-04 - val_loss: 1.7541e-04 - 15s/epoch - 183us/sample
Epoch 71/85
84077/84077 - 15s - loss: 1.8804e-04 - val_loss: 1.7416e-04 - 15s/epoch - 177us/sample
Epoch 72/85
84077/84077 - 15s - loss: 1.8756e-04 - val_loss: 1.7745e-04 - 15s/epoch - 177us/sample
Epoch 73/85
84077/84077 - 15s - loss: 1.8772e-04 - val_loss: 1.7317e-04 - 15s/epoch - 176us/sample
Epoch 74/85
84077/84077 - 15s - loss: 1.8685e-04 - val_loss: 1.7600e-04 - 15s/epoch - 178us/sample
Epoch 75/85
84077/84077 - 15s - loss: 1.8637e-04 - val_loss: 1.7432e-04 - 15s/epoch - 178us/sample
Epoch 76/85
84077/84077 - 15s - loss: 1.8700e-04 - val_loss: 1.7593e-04 - 15s/epoch - 181us/sample
Epoch 77/85
84077/84077 - 15s - loss: 1.8646e-04 - val_loss: 1.7135e-04 - 15s/epoch - 178us/sample
Epoch 78/85
84077/84077 - 15s - loss: 1.8603e-04 - val_loss: 1.7132e-04 - 15s/epoch - 178us/sample
Epoch 79/85
84077/84077 - 15s - loss: 1.8830e-04 - val_loss: 1.7489e-04 - 15s/epoch - 178us/sample
Epoch 80/85
84077/84077 - 15s - loss: 1.8664e-04 - val_loss: 1.7579e-04 - 15s/epoch - 175us/sample
Epoch 81/85
84077/84077 - 15s - loss: 1.8432e-04 - val_loss: 1.7325e-04 - 15s/epoch - 179us/sample
Epoch 82/85
84077/84077 - 15s - loss: 1.8413e-04 - val_loss: 1.7213e-04 - 15s/epoch - 174us/sample
Epoch 83/85
84077/84077 - 15s - loss: 1.8464e-04 - val_loss: 1.7630e-04 - 15s/epoch - 178us/sample
Epoch 84/85
84077/84077 - 15s - loss: 1.8438e-04 - val_loss: 1.7084e-04 - 15s/epoch - 178us/sample
Epoch 85/85
84077/84077 - 15s - loss: 1.8394e-04 - val_loss: 1.7379e-04 - 15s/epoch - 178us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001737882825931027
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 14:49:53.256450: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_92/outputlayer/BiasAdd' id:102258 op device:{requested: '', assigned: ''} def:{{{node decoder_model_92/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_92/outputlayer/MatMul, decoder_model_92/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.021003698627143016
cosine 0.020748214796203843
MAE: 0.002418395808066866
RMSE: 0.008348522203629596
r2: 0.9459014712279319
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.001, 0.2, 188, 0.00018394454843015148, 0.0001737882825931027, 0.021003698627143016, 0.020748214796203843, 0.002418395808066866, 0.008348522203629596, 0.9459014712279319, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 90 0.0008 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_237 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_237 (ReLU)               (None, 2168)         0           ['batch_normalization_237[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_237[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_237[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 14:50:34.924386: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_239/beta/Assign' id:103358 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_239/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_239/beta, batch_normalization_239/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 14:50:59.348202: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_93/mul' id:103542 op device:{requested: '', assigned: ''} def:{{{node loss_93/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_93/mul/x, loss_93/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 41s - loss: 0.0075 - val_loss: 0.0019 - 41s/epoch - 488us/sample
Epoch 2/90
84077/84077 - 15s - loss: 0.0015 - val_loss: 0.0012 - 15s/epoch - 178us/sample
Epoch 3/90
84077/84077 - 15s - loss: 0.0016 - val_loss: 9.6101e-04 - 15s/epoch - 179us/sample
Epoch 4/90
84077/84077 - 15s - loss: 0.0013 - val_loss: 8.6866e-04 - 15s/epoch - 181us/sample
Epoch 5/90
84077/84077 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 182us/sample
Epoch 6/90
84077/84077 - 15s - loss: 7.9944e-04 - val_loss: 5.9403e-04 - 15s/epoch - 183us/sample
Epoch 7/90
84077/84077 - 15s - loss: 6.1109e-04 - val_loss: 5.3746e-04 - 15s/epoch - 180us/sample
Epoch 8/90
84077/84077 - 15s - loss: 5.5813e-04 - val_loss: 4.4208e-04 - 15s/epoch - 181us/sample
Epoch 9/90
84077/84077 - 15s - loss: 5.3237e-04 - val_loss: 4.0318e-04 - 15s/epoch - 181us/sample
Epoch 10/90
84077/84077 - 15s - loss: 4.3626e-04 - val_loss: 3.6908e-04 - 15s/epoch - 181us/sample
Epoch 11/90
84077/84077 - 15s - loss: 4.2777e-04 - val_loss: 3.4918e-04 - 15s/epoch - 179us/sample
Epoch 12/90
84077/84077 - 15s - loss: 3.7806e-04 - val_loss: 3.2066e-04 - 15s/epoch - 180us/sample
Epoch 13/90
84077/84077 - 15s - loss: 3.5763e-04 - val_loss: 3.0000e-04 - 15s/epoch - 181us/sample
Epoch 14/90
84077/84077 - 15s - loss: 3.4378e-04 - val_loss: 2.9452e-04 - 15s/epoch - 181us/sample
Epoch 15/90
84077/84077 - 15s - loss: 3.1774e-04 - val_loss: 2.9769e-04 - 15s/epoch - 184us/sample
Epoch 16/90
84077/84077 - 15s - loss: 3.0758e-04 - val_loss: 3.2585e-04 - 15s/epoch - 181us/sample
Epoch 17/90
84077/84077 - 15s - loss: 2.9769e-04 - val_loss: 2.5754e-04 - 15s/epoch - 181us/sample
Epoch 18/90
84077/84077 - 15s - loss: 3.0415e-04 - val_loss: 2.7103e-04 - 15s/epoch - 180us/sample
Epoch 19/90
84077/84077 - 15s - loss: 2.8714e-04 - val_loss: 2.5093e-04 - 15s/epoch - 179us/sample
Epoch 20/90
84077/84077 - 15s - loss: 2.7889e-04 - val_loss: 2.4397e-04 - 15s/epoch - 181us/sample
Epoch 21/90
84077/84077 - 15s - loss: 2.6633e-04 - val_loss: 2.3918e-04 - 15s/epoch - 178us/sample
Epoch 22/90
84077/84077 - 15s - loss: 2.6295e-04 - val_loss: 2.3977e-04 - 15s/epoch - 181us/sample
Epoch 23/90
84077/84077 - 15s - loss: 2.5719e-04 - val_loss: 2.3000e-04 - 15s/epoch - 181us/sample
Epoch 24/90
84077/84077 - 15s - loss: 2.6735e-04 - val_loss: 2.3517e-04 - 15s/epoch - 184us/sample
Epoch 25/90
84077/84077 - 15s - loss: 2.5368e-04 - val_loss: 2.3920e-04 - 15s/epoch - 181us/sample
Epoch 26/90
84077/84077 - 15s - loss: 2.5084e-04 - val_loss: 2.2872e-04 - 15s/epoch - 181us/sample
Epoch 27/90
84077/84077 - 15s - loss: 2.4643e-04 - val_loss: 2.3058e-04 - 15s/epoch - 181us/sample
Epoch 28/90
84077/84077 - 15s - loss: 2.5127e-04 - val_loss: 2.2646e-04 - 15s/epoch - 179us/sample
Epoch 29/90
84077/84077 - 15s - loss: 2.4191e-04 - val_loss: 2.1748e-04 - 15s/epoch - 181us/sample
Epoch 30/90
84077/84077 - 15s - loss: 2.3621e-04 - val_loss: 2.1678e-04 - 15s/epoch - 177us/sample
Epoch 31/90
84077/84077 - 15s - loss: 2.3536e-04 - val_loss: 2.1206e-04 - 15s/epoch - 180us/sample
Epoch 32/90
84077/84077 - 15s - loss: 3.2384e-04 - val_loss: 3.2104e-04 - 15s/epoch - 181us/sample
Epoch 33/90
84077/84077 - 15s - loss: 4.1741e-04 - val_loss: 2.6881e-04 - 15s/epoch - 182us/sample
Epoch 34/90
84077/84077 - 15s - loss: 2.7205e-04 - val_loss: 2.3656e-04 - 15s/epoch - 182us/sample
Epoch 35/90
84077/84077 - 15s - loss: 2.5518e-04 - val_loss: 2.2418e-04 - 15s/epoch - 180us/sample
Epoch 36/90
84077/84077 - 15s - loss: 2.4601e-04 - val_loss: 2.2148e-04 - 15s/epoch - 181us/sample
Epoch 37/90
84077/84077 - 15s - loss: 2.3895e-04 - val_loss: 2.1624e-04 - 15s/epoch - 179us/sample
Epoch 38/90
84077/84077 - 15s - loss: 2.3930e-04 - val_loss: 2.1213e-04 - 15s/epoch - 181us/sample
Epoch 39/90
84077/84077 - 15s - loss: 2.3376e-04 - val_loss: 2.0999e-04 - 15s/epoch - 180us/sample
Epoch 40/90
84077/84077 - 15s - loss: 2.3454e-04 - val_loss: 2.1265e-04 - 15s/epoch - 181us/sample
Epoch 41/90
84077/84077 - 15s - loss: 2.3044e-04 - val_loss: 2.0502e-04 - 15s/epoch - 181us/sample
Epoch 42/90
84077/84077 - 15s - loss: 2.3293e-04 - val_loss: 2.0917e-04 - 15s/epoch - 181us/sample
Epoch 43/90
84077/84077 - 15s - loss: 2.2546e-04 - val_loss: 1.9985e-04 - 15s/epoch - 184us/sample
Epoch 44/90
84077/84077 - 15s - loss: 2.2907e-04 - val_loss: 2.0251e-04 - 15s/epoch - 181us/sample
Epoch 45/90
84077/84077 - 15s - loss: 2.2151e-04 - val_loss: 2.0151e-04 - 15s/epoch - 181us/sample
Epoch 46/90
84077/84077 - 15s - loss: 2.2174e-04 - val_loss: 1.9880e-04 - 15s/epoch - 179us/sample
Epoch 47/90
84077/84077 - 15s - loss: 2.2164e-04 - val_loss: 1.9832e-04 - 15s/epoch - 179us/sample
Epoch 48/90
84077/84077 - 15s - loss: 2.1959e-04 - val_loss: 2.0031e-04 - 15s/epoch - 182us/sample
Epoch 49/90
84077/84077 - 15s - loss: 2.2351e-04 - val_loss: 1.9895e-04 - 15s/epoch - 180us/sample
Epoch 50/90
84077/84077 - 15s - loss: 2.1650e-04 - val_loss: 1.9403e-04 - 15s/epoch - 180us/sample
Epoch 51/90
84077/84077 - 15s - loss: 2.1534e-04 - val_loss: 1.9201e-04 - 15s/epoch - 180us/sample
Epoch 52/90
84077/84077 - 15s - loss: 2.1312e-04 - val_loss: 1.9456e-04 - 15s/epoch - 184us/sample
Epoch 53/90
84077/84077 - 15s - loss: 2.1212e-04 - val_loss: 1.8964e-04 - 15s/epoch - 181us/sample
Epoch 54/90
84077/84077 - 15s - loss: 2.1077e-04 - val_loss: 1.9055e-04 - 15s/epoch - 181us/sample
Epoch 55/90
84077/84077 - 15s - loss: 2.1108e-04 - val_loss: 1.9168e-04 - 15s/epoch - 178us/sample
Epoch 56/90
84077/84077 - 15s - loss: 2.1804e-04 - val_loss: 1.9404e-04 - 15s/epoch - 179us/sample
Epoch 57/90
84077/84077 - 15s - loss: 2.1134e-04 - val_loss: 1.9081e-04 - 15s/epoch - 178us/sample
Epoch 58/90
84077/84077 - 15s - loss: 2.0893e-04 - val_loss: 1.8829e-04 - 15s/epoch - 181us/sample
Epoch 59/90
84077/84077 - 15s - loss: 2.0741e-04 - val_loss: 1.8654e-04 - 15s/epoch - 180us/sample
Epoch 60/90
84077/84077 - 15s - loss: 2.0560e-04 - val_loss: 1.8830e-04 - 15s/epoch - 181us/sample
Epoch 61/90
84077/84077 - 15s - loss: 2.0536e-04 - val_loss: 1.8576e-04 - 15s/epoch - 182us/sample
Epoch 62/90
84077/84077 - 15s - loss: 2.0617e-04 - val_loss: 1.8902e-04 - 15s/epoch - 183us/sample
Epoch 63/90
84077/84077 - 15s - loss: 2.0821e-04 - val_loss: 1.8528e-04 - 15s/epoch - 181us/sample
Epoch 64/90
84077/84077 - 15s - loss: 2.0327e-04 - val_loss: 1.8209e-04 - 15s/epoch - 181us/sample
Epoch 65/90
84077/84077 - 15s - loss: 2.0480e-04 - val_loss: 1.8427e-04 - 15s/epoch - 179us/sample
Epoch 66/90
84077/84077 - 15s - loss: 2.0196e-04 - val_loss: 1.8333e-04 - 15s/epoch - 181us/sample
Epoch 67/90
84077/84077 - 15s - loss: 2.0318e-04 - val_loss: 1.8246e-04 - 15s/epoch - 178us/sample
Epoch 68/90
84077/84077 - 15s - loss: 2.0241e-04 - val_loss: 1.8433e-04 - 15s/epoch - 181us/sample
Epoch 69/90
84077/84077 - 15s - loss: 1.9992e-04 - val_loss: 1.8792e-04 - 15s/epoch - 181us/sample
Epoch 70/90
84077/84077 - 15s - loss: 1.9993e-04 - val_loss: 1.8242e-04 - 15s/epoch - 181us/sample
Epoch 71/90
84077/84077 - 15s - loss: 1.9990e-04 - val_loss: 1.8763e-04 - 15s/epoch - 183us/sample
Epoch 72/90
84077/84077 - 15s - loss: 1.9991e-04 - val_loss: 1.8384e-04 - 15s/epoch - 181us/sample
Epoch 73/90
84077/84077 - 15s - loss: 1.9723e-04 - val_loss: 1.7843e-04 - 15s/epoch - 181us/sample
Epoch 74/90
84077/84077 - 15s - loss: 1.9724e-04 - val_loss: 1.8024e-04 - 15s/epoch - 178us/sample
Epoch 75/90
84077/84077 - 15s - loss: 1.9714e-04 - val_loss: 1.7811e-04 - 15s/epoch - 179us/sample
Epoch 76/90
84077/84077 - 15s - loss: 1.9529e-04 - val_loss: 1.8261e-04 - 15s/epoch - 182us/sample
Epoch 77/90
84077/84077 - 15s - loss: 2.0537e-04 - val_loss: 1.8021e-04 - 15s/epoch - 181us/sample
Epoch 78/90
84077/84077 - 15s - loss: 1.9604e-04 - val_loss: 1.8215e-04 - 15s/epoch - 181us/sample
Epoch 79/90
84077/84077 - 15s - loss: 1.9738e-04 - val_loss: 1.8116e-04 - 15s/epoch - 181us/sample
Epoch 80/90
84077/84077 - 15s - loss: 1.9604e-04 - val_loss: 1.8527e-04 - 15s/epoch - 184us/sample
Epoch 81/90
84077/84077 - 15s - loss: 2.1215e-04 - val_loss: 1.7883e-04 - 15s/epoch - 181us/sample
Epoch 82/90
84077/84077 - 15s - loss: 1.9509e-04 - val_loss: 1.7769e-04 - 15s/epoch - 181us/sample
Epoch 83/90
84077/84077 - 15s - loss: 2.0222e-04 - val_loss: 1.9326e-04 - 15s/epoch - 178us/sample
Epoch 84/90
84077/84077 - 15s - loss: 1.9557e-04 - val_loss: 1.7883e-04 - 15s/epoch - 179us/sample
Epoch 85/90
84077/84077 - 15s - loss: 1.9364e-04 - val_loss: 1.7719e-04 - 15s/epoch - 180us/sample
Epoch 86/90
84077/84077 - 15s - loss: 1.9300e-04 - val_loss: 1.7698e-04 - 15s/epoch - 181us/sample
Epoch 87/90
84077/84077 - 15s - loss: 1.9446e-04 - val_loss: 1.7714e-04 - 15s/epoch - 181us/sample
Epoch 88/90
84077/84077 - 15s - loss: 1.9838e-04 - val_loss: 1.8714e-04 - 15s/epoch - 181us/sample
Epoch 89/90
84077/84077 - 15s - loss: 1.9417e-04 - val_loss: 1.7746e-04 - 15s/epoch - 181us/sample
Epoch 90/90
84077/84077 - 15s - loss: 1.9466e-04 - val_loss: 1.8100e-04 - 15s/epoch - 183us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018100272256044484
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 15:13:39.940069: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_93/outputlayer/BiasAdd' id:103513 op device:{requested: '', assigned: ''} def:{{{node decoder_model_93/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_93/outputlayer/MatMul, decoder_model_93/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02216022474588257
cosine 0.021893655745706918
MAE: 0.0020335961055663212
RMSE: 0.008542351297519752
r2: 0.9431588429499932
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'mse', 64, 90, 0.0008, 0.2, 188, 0.0001946579626054533, 0.00018100272256044484, 0.02216022474588257, 0.021893655745706918, 0.0020335961055663212, 0.008542351297519752, 0.9431588429499932, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0008 64 0] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_240 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_240 (ReLU)               (None, 1980)         0           ['batch_normalization_240[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_240[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_240[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 15:14:22.267523: W tensorflow/c/c_api.cc:291] Operation '{name:'training_132/Adam/batch_normalization_241/gamma/m/Assign' id:105394 op device:{requested: '', assigned: ''} def:{{{node training_132/Adam/batch_normalization_241/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_132/Adam/batch_normalization_241/gamma/m, training_132/Adam/batch_normalization_241/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 15:14:47.344131: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_94/mul' id:104816 op device:{requested: '', assigned: ''} def:{{{node loss_94/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_94/mul/x, loss_94/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 42s - loss: 0.0710 - val_loss: 0.0839 - 42s/epoch - 500us/sample
Epoch 2/90
84077/84077 - 15s - loss: 0.0675 - val_loss: 0.0676 - 15s/epoch - 184us/sample
Epoch 3/90
84077/84077 - 15s - loss: 0.0670 - val_loss: 0.0674 - 15s/epoch - 184us/sample
Epoch 4/90
84077/84077 - 16s - loss: 0.0669 - val_loss: 0.0674 - 16s/epoch - 187us/sample
Epoch 5/90
84077/84077 - 15s - loss: 0.0669 - val_loss: 0.0674 - 15s/epoch - 184us/sample
Epoch 6/90
84077/84077 - 15s - loss: 0.0669 - val_loss: 0.0674 - 15s/epoch - 184us/sample
Epoch 7/90
84077/84077 - 15s - loss: 0.0669 - val_loss: 0.0674 - 15s/epoch - 181us/sample
Epoch 8/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0674 - 15s/epoch - 182us/sample
Epoch 9/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0674 - 15s/epoch - 183us/sample
Epoch 10/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 11/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 12/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 13/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 14/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 15/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 16/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 17/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 18/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 19/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 20/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 21/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 184us/sample
Epoch 22/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 23/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 24/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 25/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 183us/sample
Epoch 26/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 27/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 28/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 29/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 30/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 31/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 32/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 190us/sample
Epoch 33/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 34/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 183us/sample
Epoch 35/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 36/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 37/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 38/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 184us/sample
Epoch 39/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 40/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 41/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 42/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 43/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 44/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 45/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 46/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 47/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 48/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 49/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 50/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 51/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 52/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 53/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 54/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 55/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 56/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 57/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 184us/sample
Epoch 58/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 59/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 60/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 61/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 62/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 63/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 64/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 65/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 66/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 67/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 184us/sample
Epoch 68/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 69/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 70/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 71/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 72/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 73/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 181us/sample
Epoch 74/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 184us/sample
Epoch 75/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 76/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 77/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 78/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 79/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 80/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 81/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
Epoch 82/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 180us/sample
Epoch 83/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 84/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 85/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 86/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 87/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 88/90
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 89/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 90/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 182us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734575069300293
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 15:37:52.095155: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_94/outputlayer/BiasAdd' id:104768 op device:{requested: '', assigned: ''} def:{{{node decoder_model_94/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_94/outputlayer/MatMul, decoder_model_94/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2011715577566406
cosine 1.1482762081759943
MAE: 5.581310012604612
RMSE: 5.754660383659182
r2: -24258.775422643914
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'binary_crossentropy', 64, 90, 0.0008, 0.2, 188, 0.0668369264087773, 0.06734575069300293, 1.2011715577566406, 1.1482762081759943, 5.581310012604612, 5.754660383659182, -24258.775422643914, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 9
Fitness    = 541.8276370289637
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 541.8276370289637.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 541.8276370289637, 541.8276370289637]
[2.2 85 0.001 64 0] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_243 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_243 (ReLU)               (None, 2074)         0           ['batch_normalization_243[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_243[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_243[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 15:38:34.118306: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_95/kernel/Assign' id:105701 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_95/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_95/kernel, dense_enc0_95/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 15:38:59.478652: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_95/mul' id:106153 op device:{requested: '', assigned: ''} def:{{{node loss_95/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_95/mul/x, loss_95/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 43s - loss: 0.0710 - val_loss: 0.0738 - 43s/epoch - 506us/sample
Epoch 2/85
84077/84077 - 16s - loss: 0.0678 - val_loss: 0.0791 - 16s/epoch - 188us/sample
Epoch 3/85
84077/84077 - 16s - loss: 0.0673 - val_loss: 0.0685 - 16s/epoch - 188us/sample
Epoch 4/85
84077/84077 - 16s - loss: 0.0670 - val_loss: 0.0675 - 16s/epoch - 188us/sample
Epoch 5/85
84077/84077 - 16s - loss: 0.0669 - val_loss: 0.0674 - 16s/epoch - 186us/sample
Epoch 6/85
84077/84077 - 16s - loss: 0.0669 - val_loss: 0.0674 - 16s/epoch - 188us/sample
Epoch 7/85
84077/84077 - 16s - loss: 0.0669 - val_loss: 0.0674 - 16s/epoch - 188us/sample
Epoch 8/85
84077/84077 - 16s - loss: 0.0669 - val_loss: 0.0674 - 16s/epoch - 191us/sample
Epoch 9/85
84077/84077 - 16s - loss: 0.0669 - val_loss: 0.0674 - 16s/epoch - 188us/sample
Epoch 10/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0674 - 16s/epoch - 188us/sample
Epoch 11/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0674 - 16s/epoch - 188us/sample
Epoch 12/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 13/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 14/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 15/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 16/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 17/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 190us/sample
Epoch 18/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 19/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 20/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 21/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 22/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 23/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 24/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 25/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 26/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 27/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 28/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 29/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 30/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 31/85
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 197us/sample
Epoch 32/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 192us/sample
Epoch 33/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 34/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 35/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 36/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 37/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 38/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 39/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 40/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 41/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 42/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 43/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 44/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 45/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 190us/sample
Epoch 46/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 47/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 48/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 49/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 50/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 51/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 52/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 53/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 54/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 55/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 56/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 57/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 58/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 59/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 190us/sample
Epoch 60/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 61/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 62/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 63/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 64/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 65/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 66/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 67/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 68/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 69/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 70/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 71/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 72/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 73/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 74/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 186us/sample
Epoch 75/85
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 184us/sample
Epoch 76/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
Epoch 77/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 191us/sample
Epoch 78/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 79/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 80/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 81/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 188us/sample
Epoch 82/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 189us/sample
Epoch 83/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 84/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 185us/sample
Epoch 85/85
84077/84077 - 16s - loss: 0.0668 - val_loss: 0.0673 - 16s/epoch - 187us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734576811118749
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 16:01:16.312272: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_95/outputlayer/BiasAdd' id:106105 op device:{requested: '', assigned: ''} def:{{{node decoder_model_95/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_95/outputlayer/MatMul, decoder_model_95/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1628101430295874
cosine 1.1427951210270055
MAE: 5.801994307449799
RMSE: 5.976408655862377
r2: -26241.880331030698
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'binary_crossentropy', 64, 85, 0.001, 0.2, 188, 0.06683693239373527, 0.06734576811118749, 1.1628101430295874, 1.1427951210270055, 5.801994307449799, 5.976408655862377, -26241.880331030698, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0008 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_246 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_246 (ReLU)               (None, 1980)         0           ['batch_normalization_246[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_246[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_246[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-13 16:01:58.712353: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_96/bias/Assign' id:107154 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_96/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_96/bias, bottleneck_zlog_96/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 16:02:23.977362: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_96/mul' id:107471 op device:{requested: '', assigned: ''} def:{{{node loss_96/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_96/mul/x, loss_96/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 43s - loss: 0.0089 - val_loss: 0.0019 - 43s/epoch - 507us/sample
Epoch 2/85
84077/84077 - 15s - loss: 0.0017 - val_loss: 0.0055 - 15s/epoch - 181us/sample
Epoch 3/85
84077/84077 - 15s - loss: 0.0332 - val_loss: 0.0037 - 15s/epoch - 179us/sample
Epoch 4/85
84077/84077 - 15s - loss: 0.0020 - val_loss: 0.0013 - 15s/epoch - 179us/sample
Epoch 5/85
84077/84077 - 15s - loss: 0.0011 - val_loss: 8.1299e-04 - 15s/epoch - 179us/sample
Epoch 6/85
84077/84077 - 15s - loss: 8.3635e-04 - val_loss: 6.9195e-04 - 15s/epoch - 184us/sample
Epoch 7/85
84077/84077 - 15s - loss: 7.5073e-04 - val_loss: 6.0407e-04 - 15s/epoch - 182us/sample
Epoch 8/85
84077/84077 - 15s - loss: 6.4573e-04 - val_loss: 7.1106e-04 - 15s/epoch - 181us/sample
Epoch 9/85
84077/84077 - 15s - loss: 5.7510e-04 - val_loss: 4.6296e-04 - 15s/epoch - 182us/sample
Epoch 10/85
84077/84077 - 15s - loss: 5.2706e-04 - val_loss: 4.3705e-04 - 15s/epoch - 182us/sample
Epoch 11/85
84077/84077 - 15s - loss: 4.5547e-04 - val_loss: 3.7735e-04 - 15s/epoch - 184us/sample
Epoch 12/85
84077/84077 - 15s - loss: 4.1474e-04 - val_loss: 3.5017e-04 - 15s/epoch - 179us/sample
Epoch 13/85
84077/84077 - 15s - loss: 3.9138e-04 - val_loss: 3.2122e-04 - 15s/epoch - 180us/sample
Epoch 14/85
84077/84077 - 15s - loss: 3.6299e-04 - val_loss: 3.0592e-04 - 15s/epoch - 181us/sample
Epoch 15/85
84077/84077 - 16s - loss: 3.3626e-04 - val_loss: 2.8929e-04 - 16s/epoch - 185us/sample
Epoch 16/85
84077/84077 - 15s - loss: 3.4124e-04 - val_loss: 2.8120e-04 - 15s/epoch - 182us/sample
Epoch 17/85
84077/84077 - 15s - loss: 3.1414e-04 - val_loss: 2.6670e-04 - 15s/epoch - 182us/sample
Epoch 18/85
84077/84077 - 15s - loss: 2.9758e-04 - val_loss: 2.6360e-04 - 15s/epoch - 182us/sample
Epoch 19/85
84077/84077 - 15s - loss: 2.9003e-04 - val_loss: 2.5472e-04 - 15s/epoch - 181us/sample
Epoch 20/85
84077/84077 - 15s - loss: 2.7909e-04 - val_loss: 2.5033e-04 - 15s/epoch - 183us/sample
Epoch 21/85
84077/84077 - 15s - loss: 2.6958e-04 - val_loss: 2.4086e-04 - 15s/epoch - 179us/sample
Epoch 22/85
84077/84077 - 15s - loss: 2.6362e-04 - val_loss: 2.3893e-04 - 15s/epoch - 179us/sample
Epoch 23/85
84077/84077 - 15s - loss: 2.9541e-04 - val_loss: 3.9300e-04 - 15s/epoch - 179us/sample
Epoch 24/85
84077/84077 - 15s - loss: 2.7776e-04 - val_loss: 2.3218e-04 - 15s/epoch - 182us/sample
Epoch 25/85
84077/84077 - 16s - loss: 2.5710e-04 - val_loss: 2.2664e-04 - 16s/epoch - 184us/sample
Epoch 26/85
84077/84077 - 15s - loss: 2.4734e-04 - val_loss: 2.2415e-04 - 15s/epoch - 182us/sample
Epoch 27/85
84077/84077 - 15s - loss: 2.4558e-04 - val_loss: 2.2254e-04 - 15s/epoch - 183us/sample
Epoch 28/85
84077/84077 - 15s - loss: 2.4149e-04 - val_loss: 2.2191e-04 - 15s/epoch - 184us/sample
Epoch 29/85
84077/84077 - 16s - loss: 2.3827e-04 - val_loss: 2.1834e-04 - 16s/epoch - 184us/sample
Epoch 30/85
84077/84077 - 15s - loss: 2.3480e-04 - val_loss: 2.1398e-04 - 15s/epoch - 181us/sample
Epoch 31/85
84077/84077 - 15s - loss: 2.3163e-04 - val_loss: 2.1436e-04 - 15s/epoch - 179us/sample
Epoch 32/85
84077/84077 - 15s - loss: 2.3580e-04 - val_loss: 2.3228e-04 - 15s/epoch - 178us/sample
Epoch 33/85
84077/84077 - 15s - loss: 2.3116e-04 - val_loss: 2.1046e-04 - 15s/epoch - 182us/sample
Epoch 34/85
84077/84077 - 16s - loss: 2.2476e-04 - val_loss: 2.0829e-04 - 16s/epoch - 186us/sample
Epoch 35/85
84077/84077 - 15s - loss: 2.4015e-04 - val_loss: 2.1588e-04 - 15s/epoch - 182us/sample
Epoch 36/85
84077/84077 - 15s - loss: 2.2466e-04 - val_loss: 2.0765e-04 - 15s/epoch - 182us/sample
Epoch 37/85
84077/84077 - 15s - loss: 2.2006e-04 - val_loss: 2.0763e-04 - 15s/epoch - 183us/sample
Epoch 38/85
84077/84077 - 15s - loss: 2.1995e-04 - val_loss: 2.0450e-04 - 15s/epoch - 183us/sample
Epoch 39/85
84077/84077 - 16s - loss: 2.1783e-04 - val_loss: 2.0287e-04 - 16s/epoch - 185us/sample
Epoch 40/85
84077/84077 - 15s - loss: 2.1732e-04 - val_loss: 2.0076e-04 - 15s/epoch - 179us/sample
Epoch 41/85
84077/84077 - 15s - loss: 2.1474e-04 - val_loss: 2.1055e-04 - 15s/epoch - 177us/sample
Epoch 42/85
84077/84077 - 15s - loss: 2.1398e-04 - val_loss: 2.0167e-04 - 15s/epoch - 182us/sample
Epoch 43/85
84077/84077 - 15s - loss: 2.1217e-04 - val_loss: 1.9889e-04 - 15s/epoch - 184us/sample
Epoch 44/85
84077/84077 - 15s - loss: 2.1225e-04 - val_loss: 1.9924e-04 - 15s/epoch - 183us/sample
Epoch 45/85
84077/84077 - 15s - loss: 2.1004e-04 - val_loss: 1.9864e-04 - 15s/epoch - 182us/sample
Epoch 46/85
84077/84077 - 15s - loss: 2.0869e-04 - val_loss: 1.9505e-04 - 15s/epoch - 182us/sample
Epoch 47/85
84077/84077 - 15s - loss: 2.0765e-04 - val_loss: 1.9682e-04 - 15s/epoch - 182us/sample
Epoch 48/85
84077/84077 - 15s - loss: 2.0670e-04 - val_loss: 1.9513e-04 - 15s/epoch - 183us/sample
Epoch 49/85
84077/84077 - 15s - loss: 2.0575e-04 - val_loss: 1.9528e-04 - 15s/epoch - 179us/sample
Epoch 50/85
84077/84077 - 15s - loss: 2.0590e-04 - val_loss: 1.9492e-04 - 15s/epoch - 179us/sample
Epoch 51/85
84077/84077 - 15s - loss: 2.0346e-04 - val_loss: 1.9306e-04 - 15s/epoch - 180us/sample
Epoch 52/85
84077/84077 - 15s - loss: 2.0264e-04 - val_loss: 1.9585e-04 - 15s/epoch - 182us/sample
Epoch 53/85
84077/84077 - 16s - loss: 2.0225e-04 - val_loss: 2.0108e-04 - 16s/epoch - 184us/sample
Epoch 54/85
84077/84077 - 15s - loss: 2.0177e-04 - val_loss: 1.9208e-04 - 15s/epoch - 182us/sample
Epoch 55/85
84077/84077 - 15s - loss: 2.0021e-04 - val_loss: 1.9026e-04 - 15s/epoch - 182us/sample
Epoch 56/85
84077/84077 - 15s - loss: 2.0125e-04 - val_loss: 1.9039e-04 - 15s/epoch - 182us/sample
Epoch 57/85
84077/84077 - 15s - loss: 1.9940e-04 - val_loss: 1.8721e-04 - 15s/epoch - 182us/sample
Epoch 58/85
84077/84077 - 15s - loss: 1.9832e-04 - val_loss: 1.8929e-04 - 15s/epoch - 181us/sample
Epoch 59/85
84077/84077 - 15s - loss: 1.9824e-04 - val_loss: 1.8988e-04 - 15s/epoch - 179us/sample
Epoch 60/85
84077/84077 - 15s - loss: 1.9889e-04 - val_loss: 2.0889e-04 - 15s/epoch - 182us/sample
Epoch 61/85
84077/84077 - 15s - loss: 1.9822e-04 - val_loss: 1.8883e-04 - 15s/epoch - 182us/sample
Epoch 62/85
84077/84077 - 16s - loss: 1.9671e-04 - val_loss: 1.8507e-04 - 16s/epoch - 185us/sample
Epoch 63/85
84077/84077 - 15s - loss: 1.9482e-04 - val_loss: 1.8853e-04 - 15s/epoch - 183us/sample
Epoch 64/85
84077/84077 - 15s - loss: 1.9463e-04 - val_loss: 1.8456e-04 - 15s/epoch - 182us/sample
Epoch 65/85
84077/84077 - 15s - loss: 1.9435e-04 - val_loss: 1.8710e-04 - 15s/epoch - 182us/sample
Epoch 66/85
84077/84077 - 15s - loss: 1.9443e-04 - val_loss: 1.9011e-04 - 15s/epoch - 181us/sample
Epoch 67/85
84077/84077 - 15s - loss: 1.9516e-04 - val_loss: 1.8631e-04 - 15s/epoch - 182us/sample
Epoch 68/85
84077/84077 - 15s - loss: 1.9822e-04 - val_loss: 1.9553e-04 - 15s/epoch - 178us/sample
Epoch 69/85
84077/84077 - 15s - loss: 1.9485e-04 - val_loss: 1.8481e-04 - 15s/epoch - 179us/sample
Epoch 70/85
84077/84077 - 15s - loss: 1.9177e-04 - val_loss: 1.8388e-04 - 15s/epoch - 183us/sample
Epoch 71/85
84077/84077 - 15s - loss: 1.9168e-04 - val_loss: 1.8340e-04 - 15s/epoch - 182us/sample
Epoch 72/85
84077/84077 - 16s - loss: 1.9135e-04 - val_loss: 1.8204e-04 - 16s/epoch - 184us/sample
Epoch 73/85
84077/84077 - 15s - loss: 1.9084e-04 - val_loss: 1.7992e-04 - 15s/epoch - 182us/sample
Epoch 74/85
84077/84077 - 15s - loss: 1.9004e-04 - val_loss: 1.8412e-04 - 15s/epoch - 181us/sample
Epoch 75/85
84077/84077 - 15s - loss: 1.9034e-04 - val_loss: 1.8076e-04 - 15s/epoch - 181us/sample
Epoch 76/85
84077/84077 - 15s - loss: 1.8922e-04 - val_loss: 1.7798e-04 - 15s/epoch - 182us/sample
Epoch 77/85
84077/84077 - 15s - loss: 1.8884e-04 - val_loss: 1.7904e-04 - 15s/epoch - 181us/sample
Epoch 78/85
84077/84077 - 15s - loss: 1.8828e-04 - val_loss: 1.7847e-04 - 15s/epoch - 182us/sample
Epoch 79/85
84077/84077 - 15s - loss: 1.8979e-04 - val_loss: 1.8249e-04 - 15s/epoch - 182us/sample
Epoch 80/85
84077/84077 - 15s - loss: 1.8822e-04 - val_loss: 1.7793e-04 - 15s/epoch - 182us/sample
Epoch 81/85
84077/84077 - 16s - loss: 1.8789e-04 - val_loss: 1.7590e-04 - 16s/epoch - 186us/sample
Epoch 82/85
84077/84077 - 15s - loss: 1.8682e-04 - val_loss: 1.7554e-04 - 15s/epoch - 182us/sample
Epoch 83/85
84077/84077 - 15s - loss: 1.8713e-04 - val_loss: 1.7483e-04 - 15s/epoch - 182us/sample
Epoch 84/85
84077/84077 - 15s - loss: 1.8747e-04 - val_loss: 1.7558e-04 - 15s/epoch - 181us/sample
Epoch 85/85
84077/84077 - 15s - loss: 1.8677e-04 - val_loss: 1.7531e-04 - 15s/epoch - 181us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017530514602678973
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 16:23:56.125409: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_96/outputlayer/BiasAdd' id:107442 op device:{requested: '', assigned: ''} def:{{{node decoder_model_96/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_96/outputlayer/MatMul, decoder_model_96/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.020955967698511168
cosine 0.020698875412489083
MAE: 0.0020203865433402217
RMSE: 0.008498961254930276
r2: 0.9440034965953216
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.0008, 0.2, 188, 0.00018676936710269363, 0.00017530514602678973, 0.020955967698511168, 0.020698875412489083, 0.0020203865433402217, 0.008498961254930276, 0.9440034965953216, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 90 0.0012000000000000001 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_249 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_249 (ReLU)               (None, 2168)         0           ['batch_normalization_249[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_249[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_249[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 16:24:39.104943: W tensorflow/c/c_api.cc:291] Operation '{name:'training_138/Adam/dense_dec0_97/kernel/m/Assign' id:109283 op device:{requested: '', assigned: ''} def:{{{node training_138/Adam/dense_dec0_97/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_138/Adam/dense_dec0_97/kernel/m, training_138/Adam/dense_dec0_97/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 16:25:04.892830: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_97/mul' id:108733 op device:{requested: '', assigned: ''} def:{{{node loss_97/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_97/mul/x, loss_97/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 43s - loss: 0.0055 - val_loss: 0.0010 - 43s/epoch - 514us/sample
Epoch 2/90
84077/84077 - 16s - loss: 0.0013 - val_loss: 7.7131e-04 - 16s/epoch - 187us/sample
Epoch 3/90
84077/84077 - 16s - loss: 9.2252e-04 - val_loss: 6.6303e-04 - 16s/epoch - 187us/sample
Epoch 4/90
84077/84077 - 16s - loss: 8.5836e-04 - val_loss: 8.6120e-04 - 16s/epoch - 187us/sample
Epoch 5/90
84077/84077 - 16s - loss: 0.0037 - val_loss: 6.0295e-04 - 16s/epoch - 186us/sample
Epoch 6/90
84077/84077 - 16s - loss: 5.1791e-04 - val_loss: 4.8588e-04 - 16s/epoch - 186us/sample
Epoch 7/90
84077/84077 - 16s - loss: 4.0809e-04 - val_loss: 4.3633e-04 - 16s/epoch - 186us/sample
Epoch 8/90
84077/84077 - 16s - loss: 3.7980e-04 - val_loss: 3.3799e-04 - 16s/epoch - 187us/sample
Epoch 9/90
84077/84077 - 16s - loss: 3.4973e-04 - val_loss: 2.9150e-04 - 16s/epoch - 187us/sample
Epoch 10/90
84077/84077 - 16s - loss: 3.3341e-04 - val_loss: 2.6745e-04 - 16s/epoch - 189us/sample
Epoch 11/90
84077/84077 - 16s - loss: 3.0609e-04 - val_loss: 2.6529e-04 - 16s/epoch - 187us/sample
Epoch 12/90
84077/84077 - 16s - loss: 2.8325e-04 - val_loss: 2.4442e-04 - 16s/epoch - 187us/sample
Epoch 13/90
84077/84077 - 15s - loss: 2.6103e-04 - val_loss: 2.2506e-04 - 15s/epoch - 184us/sample
Epoch 14/90
84077/84077 - 16s - loss: 2.4729e-04 - val_loss: 2.1340e-04 - 16s/epoch - 187us/sample
Epoch 15/90
84077/84077 - 15s - loss: 2.3244e-04 - val_loss: 2.0168e-04 - 15s/epoch - 183us/sample
Epoch 16/90
84077/84077 - 16s - loss: 2.2006e-04 - val_loss: 1.9075e-04 - 16s/epoch - 187us/sample
Epoch 17/90
84077/84077 - 16s - loss: 2.1129e-04 - val_loss: 1.8368e-04 - 16s/epoch - 187us/sample
Epoch 18/90
84077/84077 - 16s - loss: 2.0285e-04 - val_loss: 1.7524e-04 - 16s/epoch - 187us/sample
Epoch 19/90
84077/84077 - 17s - loss: 1.9562e-04 - val_loss: 1.7168e-04 - 17s/epoch - 200us/sample
Epoch 20/90
84077/84077 - 17s - loss: 1.8994e-04 - val_loss: 1.6845e-04 - 17s/epoch - 203us/sample
Epoch 21/90
84077/84077 - 16s - loss: 1.8462e-04 - val_loss: 1.6468e-04 - 16s/epoch - 187us/sample
Epoch 22/90
84077/84077 - 15s - loss: 1.8097e-04 - val_loss: 1.6150e-04 - 15s/epoch - 183us/sample
Epoch 23/90
84077/84077 - 16s - loss: 1.7704e-04 - val_loss: 1.5797e-04 - 16s/epoch - 188us/sample
Epoch 24/90
84077/84077 - 16s - loss: 1.7431e-04 - val_loss: 1.5634e-04 - 16s/epoch - 186us/sample
Epoch 25/90
84077/84077 - 16s - loss: 1.7167e-04 - val_loss: 1.5403e-04 - 16s/epoch - 187us/sample
Epoch 26/90
84077/84077 - 16s - loss: 1.6994e-04 - val_loss: 1.5297e-04 - 16s/epoch - 187us/sample
Epoch 27/90
84077/84077 - 16s - loss: 1.6705e-04 - val_loss: 1.5185e-04 - 16s/epoch - 188us/sample
Epoch 28/90
84077/84077 - 16s - loss: 1.6597e-04 - val_loss: 1.4801e-04 - 16s/epoch - 192us/sample
Epoch 29/90
84077/84077 - 16s - loss: 1.6398e-04 - val_loss: 1.4828e-04 - 16s/epoch - 187us/sample
Epoch 30/90
84077/84077 - 16s - loss: 1.6268e-04 - val_loss: 1.4726e-04 - 16s/epoch - 188us/sample
Epoch 31/90
84077/84077 - 16s - loss: 1.6083e-04 - val_loss: 1.4581e-04 - 16s/epoch - 186us/sample
Epoch 32/90
84077/84077 - 16s - loss: 1.5944e-04 - val_loss: 1.4531e-04 - 16s/epoch - 186us/sample
Epoch 33/90
84077/84077 - 16s - loss: 1.5851e-04 - val_loss: 1.4419e-04 - 16s/epoch - 185us/sample
Epoch 34/90
84077/84077 - 16s - loss: 1.5741e-04 - val_loss: 1.4298e-04 - 16s/epoch - 188us/sample
Epoch 35/90
84077/84077 - 16s - loss: 1.5584e-04 - val_loss: 1.4201e-04 - 16s/epoch - 187us/sample
Epoch 36/90
84077/84077 - 16s - loss: 1.5547e-04 - val_loss: 1.4189e-04 - 16s/epoch - 187us/sample
Epoch 37/90
84077/84077 - 16s - loss: 1.5431e-04 - val_loss: 1.4208e-04 - 16s/epoch - 189us/sample
Epoch 38/90
84077/84077 - 16s - loss: 1.5304e-04 - val_loss: 1.3993e-04 - 16s/epoch - 188us/sample
Epoch 39/90
84077/84077 - 16s - loss: 1.5171e-04 - val_loss: 1.3856e-04 - 16s/epoch - 187us/sample
Epoch 40/90
84077/84077 - 15s - loss: 1.5165e-04 - val_loss: 1.3934e-04 - 15s/epoch - 184us/sample
Epoch 41/90
84077/84077 - 16s - loss: 1.5079e-04 - val_loss: 1.3785e-04 - 16s/epoch - 185us/sample
Epoch 42/90
84077/84077 - 16s - loss: 1.5006e-04 - val_loss: 1.3733e-04 - 16s/epoch - 186us/sample
Epoch 43/90
84077/84077 - 16s - loss: 1.4965e-04 - val_loss: 1.3695e-04 - 16s/epoch - 186us/sample
Epoch 44/90
84077/84077 - 16s - loss: 1.4860e-04 - val_loss: 1.3645e-04 - 16s/epoch - 186us/sample
Epoch 45/90
84077/84077 - 16s - loss: 1.4760e-04 - val_loss: 1.3696e-04 - 16s/epoch - 187us/sample
Epoch 46/90
84077/84077 - 16s - loss: 1.4745e-04 - val_loss: 1.3578e-04 - 16s/epoch - 191us/sample
Epoch 47/90
84077/84077 - 16s - loss: 1.4678e-04 - val_loss: 1.3448e-04 - 16s/epoch - 187us/sample
Epoch 48/90
84077/84077 - 16s - loss: 1.4578e-04 - val_loss: 1.3512e-04 - 16s/epoch - 187us/sample
Epoch 49/90
84077/84077 - 16s - loss: 1.4609e-04 - val_loss: 1.3426e-04 - 16s/epoch - 185us/sample
Epoch 50/90
84077/84077 - 16s - loss: 1.4572e-04 - val_loss: 1.3418e-04 - 16s/epoch - 186us/sample
Epoch 51/90
84077/84077 - 16s - loss: 1.4460e-04 - val_loss: 1.3277e-04 - 16s/epoch - 190us/sample
Epoch 52/90
84077/84077 - 16s - loss: 1.4400e-04 - val_loss: 1.3313e-04 - 16s/epoch - 188us/sample
Epoch 53/90
84077/84077 - 16s - loss: 1.4344e-04 - val_loss: 1.3480e-04 - 16s/epoch - 188us/sample
Epoch 54/90
84077/84077 - 16s - loss: 1.4318e-04 - val_loss: 1.3354e-04 - 16s/epoch - 187us/sample
Epoch 55/90
84077/84077 - 16s - loss: 1.4282e-04 - val_loss: 1.3122e-04 - 16s/epoch - 189us/sample
Epoch 56/90
84077/84077 - 16s - loss: 1.4264e-04 - val_loss: 1.3188e-04 - 16s/epoch - 187us/sample
Epoch 57/90
84077/84077 - 16s - loss: 1.4217e-04 - val_loss: 1.3129e-04 - 16s/epoch - 186us/sample
Epoch 58/90
84077/84077 - 15s - loss: 1.4139e-04 - val_loss: 1.3219e-04 - 15s/epoch - 184us/sample
Epoch 59/90
84077/84077 - 16s - loss: 1.4119e-04 - val_loss: 1.3050e-04 - 16s/epoch - 185us/sample
Epoch 60/90
84077/84077 - 16s - loss: 1.4113e-04 - val_loss: 1.3014e-04 - 16s/epoch - 189us/sample
Epoch 61/90
84077/84077 - 16s - loss: 1.4066e-04 - val_loss: 1.3010e-04 - 16s/epoch - 187us/sample
Epoch 62/90
84077/84077 - 16s - loss: 1.3996e-04 - val_loss: 1.3089e-04 - 16s/epoch - 185us/sample
Epoch 63/90
84077/84077 - 15s - loss: 1.3996e-04 - val_loss: 1.3031e-04 - 15s/epoch - 181us/sample
Epoch 64/90
84077/84077 - 15s - loss: 1.3949e-04 - val_loss: 1.2954e-04 - 15s/epoch - 183us/sample
Epoch 65/90
84077/84077 - 15s - loss: 1.3921e-04 - val_loss: 1.2985e-04 - 15s/epoch - 183us/sample
Epoch 66/90
84077/84077 - 15s - loss: 1.3906e-04 - val_loss: 1.2899e-04 - 15s/epoch - 180us/sample
Epoch 67/90
84077/84077 - 15s - loss: 1.3879e-04 - val_loss: 1.2851e-04 - 15s/epoch - 180us/sample
Epoch 68/90
84077/84077 - 15s - loss: 1.3863e-04 - val_loss: 1.2958e-04 - 15s/epoch - 180us/sample
Epoch 69/90
84077/84077 - 15s - loss: 1.3780e-04 - val_loss: 1.2990e-04 - 15s/epoch - 184us/sample
Epoch 70/90
84077/84077 - 15s - loss: 1.3808e-04 - val_loss: 1.2780e-04 - 15s/epoch - 181us/sample
Epoch 71/90
84077/84077 - 15s - loss: 1.3760e-04 - val_loss: 1.2920e-04 - 15s/epoch - 180us/sample
Epoch 72/90
84077/84077 - 15s - loss: 1.3721e-04 - val_loss: 1.2836e-04 - 15s/epoch - 183us/sample
Epoch 73/90
84077/84077 - 16s - loss: 1.3692e-04 - val_loss: 1.2810e-04 - 16s/epoch - 186us/sample
Epoch 74/90
84077/84077 - 16s - loss: 1.3667e-04 - val_loss: 1.2713e-04 - 16s/epoch - 185us/sample
Epoch 75/90
84077/84077 - 16s - loss: 1.3669e-04 - val_loss: 1.2708e-04 - 16s/epoch - 187us/sample
Epoch 76/90
84077/84077 - 17s - loss: 1.3622e-04 - val_loss: 1.2681e-04 - 17s/epoch - 202us/sample
Epoch 77/90
84077/84077 - 17s - loss: 1.3629e-04 - val_loss: 1.2785e-04 - 17s/epoch - 202us/sample
Epoch 78/90
84077/84077 - 17s - loss: 1.3595e-04 - val_loss: 1.2763e-04 - 17s/epoch - 204us/sample
Epoch 79/90
84077/84077 - 17s - loss: 1.3586e-04 - val_loss: 1.2762e-04 - 17s/epoch - 203us/sample
Epoch 80/90
84077/84077 - 17s - loss: 1.3552e-04 - val_loss: 1.2734e-04 - 17s/epoch - 202us/sample
Epoch 81/90
84077/84077 - 17s - loss: 1.3537e-04 - val_loss: 1.2788e-04 - 17s/epoch - 202us/sample
Epoch 82/90
84077/84077 - 17s - loss: 1.3509e-04 - val_loss: 1.2770e-04 - 17s/epoch - 204us/sample
Epoch 83/90
84077/84077 - 17s - loss: 1.3499e-04 - val_loss: 1.2733e-04 - 17s/epoch - 203us/sample
Epoch 84/90
84077/84077 - 17s - loss: 1.3471e-04 - val_loss: 1.2600e-04 - 17s/epoch - 202us/sample
Epoch 85/90
84077/84077 - 16s - loss: 1.3465e-04 - val_loss: 1.2818e-04 - 16s/epoch - 188us/sample
Epoch 86/90
84077/84077 - 16s - loss: 1.3479e-04 - val_loss: 1.2559e-04 - 16s/epoch - 185us/sample
Epoch 87/90
84077/84077 - 15s - loss: 1.3411e-04 - val_loss: 1.2695e-04 - 15s/epoch - 183us/sample
Epoch 88/90
84077/84077 - 15s - loss: 1.3404e-04 - val_loss: 1.2717e-04 - 15s/epoch - 180us/sample
Epoch 89/90
84077/84077 - 15s - loss: 1.3399e-04 - val_loss: 1.2553e-04 - 15s/epoch - 180us/sample
Epoch 90/90
84077/84077 - 15s - loss: 1.3385e-04 - val_loss: 1.2651e-04 - 15s/epoch - 180us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012651308544610275
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 16:48:39.498182: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_97/outputlayer/BiasAdd' id:108697 op device:{requested: '', assigned: ''} def:{{{node decoder_model_97/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_97/outputlayer/MatMul, decoder_model_97/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030829783422008195
cosine 0.03045620164616491
MAE: 0.002384482098102394
RMSE: 0.010005908742864314
r2: 0.9220176498183509
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'logcosh', 64, 90, 0.0012000000000000001, 0.2, 188, 0.0001338519675285065, 0.00012651308544610275, 0.030829783422008195, 0.03045620164616491, 0.002384482098102394, 0.010005908742864314, 0.9220176498183509, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0008 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_252 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_252 (ReLU)               (None, 2074)         0           ['batch_normalization_252[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_252[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_252[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 16:49:22.291889: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_98/kernel/Assign' id:109689 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_98/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_98/kernel, bottleneck_zlog_98/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 16:49:47.667819: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_98/mul' id:110018 op device:{requested: '', assigned: ''} def:{{{node loss_98/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_98/mul/x, loss_98/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 43s - loss: 0.0036 - val_loss: 0.0011 - 43s/epoch - 513us/sample
Epoch 2/90
84077/84077 - 16s - loss: 0.0011 - val_loss: 6.9732e-04 - 16s/epoch - 185us/sample
Epoch 3/90
84077/84077 - 15s - loss: 8.9627e-04 - val_loss: 6.2693e-04 - 15s/epoch - 179us/sample
Epoch 4/90
84077/84077 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 179us/sample
Epoch 5/90
84077/84077 - 15s - loss: 7.5831e-04 - val_loss: 7.9781e-04 - 15s/epoch - 180us/sample
Epoch 6/90
84077/84077 - 16s - loss: 6.2143e-04 - val_loss: 4.5898e-04 - 16s/epoch - 187us/sample
Epoch 7/90
84077/84077 - 16s - loss: 4.5842e-04 - val_loss: 4.1889e-04 - 16s/epoch - 188us/sample
Epoch 8/90
84077/84077 - 15s - loss: 4.2660e-04 - val_loss: 3.9225e-04 - 15s/epoch - 179us/sample
Epoch 9/90
84077/84077 - 16s - loss: 4.0749e-04 - val_loss: 3.7004e-04 - 16s/epoch - 188us/sample
Epoch 10/90
84077/84077 - 17s - loss: 3.7600e-04 - val_loss: 3.4347e-04 - 17s/epoch - 199us/sample
Epoch 11/90
84077/84077 - 16s - loss: 3.6274e-04 - val_loss: 4.0855e-04 - 16s/epoch - 184us/sample
Epoch 12/90
84077/84077 - 16s - loss: 3.2545e-04 - val_loss: 2.9300e-04 - 16s/epoch - 191us/sample
Epoch 13/90
84077/84077 - 16s - loss: 3.0521e-04 - val_loss: 2.7187e-04 - 16s/epoch - 188us/sample
Epoch 14/90
84077/84077 - 15s - loss: 2.8763e-04 - val_loss: 2.6057e-04 - 15s/epoch - 179us/sample
Epoch 15/90
84077/84077 - 17s - loss: 2.7349e-04 - val_loss: 2.4708e-04 - 17s/epoch - 200us/sample
Epoch 16/90
84077/84077 - 17s - loss: 2.6413e-04 - val_loss: 2.3769e-04 - 17s/epoch - 206us/sample
Epoch 17/90
84077/84077 - 16s - loss: 2.5504e-04 - val_loss: 2.3027e-04 - 16s/epoch - 195us/sample
Epoch 18/90
84077/84077 - 15s - loss: 2.4835e-04 - val_loss: 2.2483e-04 - 15s/epoch - 179us/sample
Epoch 19/90
84077/84077 - 15s - loss: 2.4318e-04 - val_loss: 2.1907e-04 - 15s/epoch - 182us/sample
Epoch 20/90
84077/84077 - 15s - loss: 2.3560e-04 - val_loss: 2.1169e-04 - 15s/epoch - 180us/sample
Epoch 21/90
84077/84077 - 15s - loss: 2.2959e-04 - val_loss: 2.0500e-04 - 15s/epoch - 179us/sample
Epoch 22/90
84077/84077 - 15s - loss: 2.2432e-04 - val_loss: 2.0176e-04 - 15s/epoch - 180us/sample
Epoch 23/90
84077/84077 - 15s - loss: 2.1904e-04 - val_loss: 1.9781e-04 - 15s/epoch - 182us/sample
Epoch 24/90
84077/84077 - 15s - loss: 2.1477e-04 - val_loss: 1.9270e-04 - 15s/epoch - 183us/sample
Epoch 25/90
84077/84077 - 15s - loss: 2.1039e-04 - val_loss: 1.8914e-04 - 15s/epoch - 180us/sample
Epoch 26/90
84077/84077 - 16s - loss: 2.0630e-04 - val_loss: 1.8576e-04 - 16s/epoch - 188us/sample
Epoch 27/90
84077/84077 - 17s - loss: 2.0200e-04 - val_loss: 1.8230e-04 - 17s/epoch - 206us/sample
Epoch 28/90
84077/84077 - 18s - loss: 1.9837e-04 - val_loss: 1.7778e-04 - 18s/epoch - 209us/sample
Epoch 29/90
84077/84077 - 16s - loss: 1.9526e-04 - val_loss: 1.7739e-04 - 16s/epoch - 191us/sample
Epoch 30/90
84077/84077 - 15s - loss: 1.9244e-04 - val_loss: 1.7283e-04 - 15s/epoch - 180us/sample
Epoch 31/90
84077/84077 - 15s - loss: 1.8918e-04 - val_loss: 1.7055e-04 - 15s/epoch - 179us/sample
Epoch 32/90
84077/84077 - 15s - loss: 1.8612e-04 - val_loss: 1.6919e-04 - 15s/epoch - 180us/sample
Epoch 33/90
84077/84077 - 15s - loss: 1.8400e-04 - val_loss: 1.6579e-04 - 15s/epoch - 182us/sample
Epoch 34/90
84077/84077 - 15s - loss: 1.8193e-04 - val_loss: 1.6515e-04 - 15s/epoch - 179us/sample
Epoch 35/90
84077/84077 - 15s - loss: 1.7878e-04 - val_loss: 1.6324e-04 - 15s/epoch - 179us/sample
Epoch 36/90
84077/84077 - 15s - loss: 1.7686e-04 - val_loss: 1.6080e-04 - 15s/epoch - 179us/sample
Epoch 37/90
84077/84077 - 15s - loss: 1.7441e-04 - val_loss: 1.6043e-04 - 15s/epoch - 180us/sample
Epoch 38/90
84077/84077 - 15s - loss: 1.7307e-04 - val_loss: 1.5763e-04 - 15s/epoch - 182us/sample
Epoch 39/90
84077/84077 - 15s - loss: 1.7111e-04 - val_loss: 1.5656e-04 - 15s/epoch - 179us/sample
Epoch 40/90
84077/84077 - 15s - loss: 1.6967e-04 - val_loss: 1.5668e-04 - 15s/epoch - 180us/sample
Epoch 41/90
84077/84077 - 15s - loss: 1.6825e-04 - val_loss: 1.5357e-04 - 15s/epoch - 179us/sample
Epoch 42/90
84077/84077 - 15s - loss: 1.6665e-04 - val_loss: 1.5282e-04 - 15s/epoch - 182us/sample
Epoch 43/90
84077/84077 - 15s - loss: 1.6643e-04 - val_loss: 1.5264e-04 - 15s/epoch - 180us/sample
Epoch 44/90
84077/84077 - 15s - loss: 1.6504e-04 - val_loss: 1.5190e-04 - 15s/epoch - 179us/sample
Epoch 45/90
84077/84077 - 15s - loss: 1.6418e-04 - val_loss: 1.5171e-04 - 15s/epoch - 179us/sample
Epoch 46/90
84077/84077 - 15s - loss: 1.6296e-04 - val_loss: 1.4963e-04 - 15s/epoch - 180us/sample
Epoch 47/90
84077/84077 - 15s - loss: 1.6237e-04 - val_loss: 1.4956e-04 - 15s/epoch - 182us/sample
Epoch 48/90
84077/84077 - 15s - loss: 1.6238e-04 - val_loss: 1.4781e-04 - 15s/epoch - 180us/sample
Epoch 49/90
84077/84077 - 15s - loss: 1.6070e-04 - val_loss: 1.4800e-04 - 15s/epoch - 179us/sample
Epoch 50/90
84077/84077 - 15s - loss: 1.6027e-04 - val_loss: 1.4713e-04 - 15s/epoch - 179us/sample
Epoch 51/90
84077/84077 - 15s - loss: 1.5933e-04 - val_loss: 1.4562e-04 - 15s/epoch - 181us/sample
Epoch 52/90
84077/84077 - 16s - loss: 1.5877e-04 - val_loss: 1.4645e-04 - 16s/epoch - 194us/sample
Epoch 53/90
84077/84077 - 17s - loss: 1.5817e-04 - val_loss: 1.4709e-04 - 17s/epoch - 206us/sample
Epoch 54/90
84077/84077 - 15s - loss: 1.5758e-04 - val_loss: 1.4612e-04 - 15s/epoch - 183us/sample
Epoch 55/90
84077/84077 - 15s - loss: 1.5711e-04 - val_loss: 1.4592e-04 - 15s/epoch - 179us/sample
Epoch 56/90
84077/84077 - 15s - loss: 1.5614e-04 - val_loss: 1.4773e-04 - 15s/epoch - 182us/sample
Epoch 57/90
84077/84077 - 15s - loss: 1.5547e-04 - val_loss: 1.4389e-04 - 15s/epoch - 179us/sample
Epoch 58/90
84077/84077 - 15s - loss: 1.5548e-04 - val_loss: 1.4395e-04 - 15s/epoch - 179us/sample
Epoch 59/90
84077/84077 - 15s - loss: 1.5492e-04 - val_loss: 1.4353e-04 - 15s/epoch - 180us/sample
Epoch 60/90
84077/84077 - 15s - loss: 1.5414e-04 - val_loss: 1.4277e-04 - 15s/epoch - 180us/sample
Epoch 61/90
84077/84077 - 15s - loss: 1.5368e-04 - val_loss: 1.4298e-04 - 15s/epoch - 182us/sample
Epoch 62/90
84077/84077 - 15s - loss: 1.5339e-04 - val_loss: 1.4320e-04 - 15s/epoch - 179us/sample
Epoch 63/90
84077/84077 - 15s - loss: 1.5291e-04 - val_loss: 1.4127e-04 - 15s/epoch - 179us/sample
Epoch 64/90
84077/84077 - 15s - loss: 1.5270e-04 - val_loss: 1.4197e-04 - 15s/epoch - 179us/sample
Epoch 65/90
84077/84077 - 15s - loss: 1.5208e-04 - val_loss: 1.4218e-04 - 15s/epoch - 180us/sample
Epoch 66/90
84077/84077 - 15s - loss: 1.5202e-04 - val_loss: 1.4153e-04 - 15s/epoch - 182us/sample
Epoch 67/90
84077/84077 - 15s - loss: 1.5119e-04 - val_loss: 1.4131e-04 - 15s/epoch - 179us/sample
Epoch 68/90
84077/84077 - 15s - loss: 1.5128e-04 - val_loss: 1.4010e-04 - 15s/epoch - 179us/sample
Epoch 69/90
84077/84077 - 15s - loss: 1.5074e-04 - val_loss: 1.4022e-04 - 15s/epoch - 179us/sample
Epoch 70/90
84077/84077 - 15s - loss: 1.5050e-04 - val_loss: 1.3955e-04 - 15s/epoch - 182us/sample
Epoch 71/90
84077/84077 - 15s - loss: 1.4990e-04 - val_loss: 1.4066e-04 - 15s/epoch - 180us/sample
Epoch 72/90
84077/84077 - 15s - loss: 1.4956e-04 - val_loss: 1.3961e-04 - 15s/epoch - 179us/sample
Epoch 73/90
84077/84077 - 15s - loss: 1.4951e-04 - val_loss: 1.3952e-04 - 15s/epoch - 180us/sample
Epoch 74/90
84077/84077 - 15s - loss: 1.4872e-04 - val_loss: 1.3808e-04 - 15s/epoch - 180us/sample
Epoch 75/90
84077/84077 - 15s - loss: 1.4908e-04 - val_loss: 1.3779e-04 - 15s/epoch - 182us/sample
Epoch 76/90
84077/84077 - 15s - loss: 1.4826e-04 - val_loss: 1.3844e-04 - 15s/epoch - 180us/sample
Epoch 77/90
84077/84077 - 15s - loss: 1.4849e-04 - val_loss: 1.3767e-04 - 15s/epoch - 179us/sample
Epoch 78/90
84077/84077 - 15s - loss: 1.4809e-04 - val_loss: 1.3680e-04 - 15s/epoch - 179us/sample
Epoch 79/90
84077/84077 - 15s - loss: 1.4767e-04 - val_loss: 1.3741e-04 - 15s/epoch - 180us/sample
Epoch 80/90
84077/84077 - 15s - loss: 1.4778e-04 - val_loss: 1.3766e-04 - 15s/epoch - 182us/sample
Epoch 81/90
84077/84077 - 15s - loss: 1.4754e-04 - val_loss: 1.3686e-04 - 15s/epoch - 179us/sample
Epoch 82/90
84077/84077 - 15s - loss: 1.4651e-04 - val_loss: 1.3687e-04 - 15s/epoch - 179us/sample
Epoch 83/90
84077/84077 - 15s - loss: 1.4641e-04 - val_loss: 1.3857e-04 - 15s/epoch - 180us/sample
Epoch 84/90
84077/84077 - 15s - loss: 1.4637e-04 - val_loss: 1.3627e-04 - 15s/epoch - 181us/sample
Epoch 85/90
84077/84077 - 15s - loss: 1.4612e-04 - val_loss: 1.3540e-04 - 15s/epoch - 180us/sample
Epoch 86/90
84077/84077 - 15s - loss: 1.4599e-04 - val_loss: 1.3617e-04 - 15s/epoch - 180us/sample
Epoch 87/90
84077/84077 - 16s - loss: 1.4569e-04 - val_loss: 1.3635e-04 - 16s/epoch - 185us/sample
Epoch 88/90
84077/84077 - 15s - loss: 1.4571e-04 - val_loss: 1.3499e-04 - 15s/epoch - 184us/sample
Epoch 89/90
84077/84077 - 16s - loss: 1.4525e-04 - val_loss: 1.3494e-04 - 16s/epoch - 184us/sample
Epoch 90/90
84077/84077 - 15s - loss: 1.4534e-04 - val_loss: 1.3517e-04 - 15s/epoch - 181us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00013517409395295417
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 17:12:46.650596: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_98/outputlayer/BiasAdd' id:109982 op device:{requested: '', assigned: ''} def:{{{node decoder_model_98/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_98/outputlayer/MatMul, decoder_model_98/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.037962577742264365
cosine 0.037503523182071466
MAE: 0.0023050568680066967
RMSE: 0.011307169744736032
r2: 0.9002142497818502
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 64, 90, 0.0008, 0.2, 188, 0.00014534463878861266, 0.00013517409395295417, 0.037962577742264365, 0.037503523182071466, 0.0023050568680066967, 0.011307169744736032, 0.9002142497818502, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 16 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_255 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_255 (ReLU)               (None, 1886)         0           ['batch_normalization_255[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_255[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_255[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 17:13:29.798092: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_99/bias/Assign' id:110870 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_99/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_99/bias, dense_enc0_99/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 17:14:19.705795: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_99/mul' id:111296 op device:{requested: '', assigned: ''} def:{{{node loss_99/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_99/mul/x, loss_99/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 69s - loss: 0.0051 - val_loss: 0.0014 - 69s/epoch - 826us/sample
Epoch 2/90
84077/84077 - 42s - loss: 0.0014 - val_loss: 0.0011 - 42s/epoch - 495us/sample
Epoch 3/90
84077/84077 - 41s - loss: 0.0011 - val_loss: 9.5982e-04 - 41s/epoch - 492us/sample
Epoch 4/90
84077/84077 - 41s - loss: 9.6408e-04 - val_loss: 9.2116e-04 - 41s/epoch - 492us/sample
Epoch 5/90
84077/84077 - 41s - loss: 7.8296e-04 - val_loss: 9.1373e-04 - 41s/epoch - 491us/sample
Epoch 6/90
84077/84077 - 41s - loss: 7.0481e-04 - val_loss: 9.5360e-04 - 41s/epoch - 493us/sample
Epoch 7/90
84077/84077 - 41s - loss: 6.6344e-04 - val_loss: 0.0011 - 41s/epoch - 491us/sample
Epoch 8/90
84077/84077 - 41s - loss: 6.3551e-04 - val_loss: 0.0012 - 41s/epoch - 493us/sample
Epoch 9/90
84077/84077 - 41s - loss: 6.1745e-04 - val_loss: 0.0014 - 41s/epoch - 493us/sample
Epoch 10/90
84077/84077 - 42s - loss: 6.0219e-04 - val_loss: 0.0018 - 42s/epoch - 494us/sample
Epoch 11/90
84077/84077 - 42s - loss: 5.9306e-04 - val_loss: 0.0018 - 42s/epoch - 496us/sample
Epoch 12/90
84077/84077 - 41s - loss: 5.8651e-04 - val_loss: 0.0018 - 41s/epoch - 493us/sample
Epoch 13/90
84077/84077 - 41s - loss: 5.7935e-04 - val_loss: 0.0016 - 41s/epoch - 492us/sample
Epoch 14/90
84077/84077 - 41s - loss: 5.7301e-04 - val_loss: 0.0017 - 41s/epoch - 492us/sample
Epoch 15/90
84077/84077 - 42s - loss: 5.6768e-04 - val_loss: 0.0015 - 42s/epoch - 495us/sample
Epoch 16/90
84077/84077 - 41s - loss: 5.6227e-04 - val_loss: 0.0016 - 41s/epoch - 491us/sample
Epoch 17/90
84077/84077 - 42s - loss: 5.5572e-04 - val_loss: 0.0015 - 42s/epoch - 494us/sample
Epoch 18/90
84077/84077 - 41s - loss: 5.4917e-04 - val_loss: 0.0013 - 41s/epoch - 492us/sample
Epoch 19/90
84077/84077 - 42s - loss: 5.4504e-04 - val_loss: 0.0014 - 42s/epoch - 496us/sample
Epoch 20/90
84077/84077 - 41s - loss: 5.4126e-04 - val_loss: 0.0013 - 41s/epoch - 492us/sample
Epoch 21/90
84077/84077 - 42s - loss: 5.3827e-04 - val_loss: 0.0012 - 42s/epoch - 495us/sample
Epoch 22/90
84077/84077 - 41s - loss: 5.3563e-04 - val_loss: 0.0011 - 41s/epoch - 490us/sample
Epoch 23/90
84077/84077 - 41s - loss: 5.3221e-04 - val_loss: 0.0010 - 41s/epoch - 493us/sample
Epoch 24/90
84077/84077 - 41s - loss: 5.2910e-04 - val_loss: 0.0011 - 41s/epoch - 492us/sample
Epoch 25/90
84077/84077 - 42s - loss: 5.2729e-04 - val_loss: 0.0011 - 42s/epoch - 494us/sample
Epoch 26/90
84077/84077 - 41s - loss: 5.2506e-04 - val_loss: 0.0010 - 41s/epoch - 490us/sample
Epoch 27/90
84077/84077 - 41s - loss: 5.2377e-04 - val_loss: 8.9183e-04 - 41s/epoch - 494us/sample
Epoch 28/90
84077/84077 - 41s - loss: 5.2095e-04 - val_loss: 8.9510e-04 - 41s/epoch - 491us/sample
Epoch 29/90
84077/84077 - 41s - loss: 5.1830e-04 - val_loss: 8.1318e-04 - 41s/epoch - 494us/sample
Epoch 30/90
84077/84077 - 41s - loss: 5.1757e-04 - val_loss: 7.8149e-04 - 41s/epoch - 493us/sample
Epoch 31/90
84077/84077 - 41s - loss: 5.1485e-04 - val_loss: 7.9980e-04 - 41s/epoch - 493us/sample
Epoch 32/90
84077/84077 - 42s - loss: 5.1346e-04 - val_loss: 7.6047e-04 - 42s/epoch - 494us/sample
Epoch 33/90
84077/84077 - 41s - loss: 5.1201e-04 - val_loss: 7.6528e-04 - 41s/epoch - 492us/sample
Epoch 34/90
84077/84077 - 41s - loss: 5.0932e-04 - val_loss: 7.2360e-04 - 41s/epoch - 493us/sample
Epoch 35/90
84077/84077 - 41s - loss: 5.0774e-04 - val_loss: 7.2642e-04 - 41s/epoch - 492us/sample
Epoch 36/90
84077/84077 - 41s - loss: 5.0735e-04 - val_loss: 6.7729e-04 - 41s/epoch - 493us/sample
Epoch 37/90
84077/84077 - 41s - loss: 5.0592e-04 - val_loss: 7.0863e-04 - 41s/epoch - 493us/sample
Epoch 38/90
84077/84077 - 42s - loss: 5.0529e-04 - val_loss: 6.9966e-04 - 42s/epoch - 494us/sample
Epoch 39/90
84077/84077 - 41s - loss: 5.0361e-04 - val_loss: 7.0111e-04 - 41s/epoch - 492us/sample
Epoch 40/90
84077/84077 - 42s - loss: 5.0282e-04 - val_loss: 6.8777e-04 - 42s/epoch - 494us/sample
Epoch 41/90
84077/84077 - 41s - loss: 5.0145e-04 - val_loss: 6.8402e-04 - 41s/epoch - 493us/sample
Epoch 42/90
84077/84077 - 42s - loss: 5.0158e-04 - val_loss: 6.9901e-04 - 42s/epoch - 494us/sample
Epoch 43/90
84077/84077 - 41s - loss: 5.0053e-04 - val_loss: 6.5528e-04 - 41s/epoch - 491us/sample
Epoch 44/90
84077/84077 - 42s - loss: 5.0041e-04 - val_loss: 6.2987e-04 - 42s/epoch - 496us/sample
Epoch 45/90
84077/84077 - 42s - loss: 4.9909e-04 - val_loss: 6.8474e-04 - 42s/epoch - 494us/sample
Epoch 46/90
84077/84077 - 41s - loss: 4.9899e-04 - val_loss: 6.3490e-04 - 41s/epoch - 493us/sample
Epoch 47/90
84077/84077 - 41s - loss: 4.9871e-04 - val_loss: 6.4685e-04 - 41s/epoch - 493us/sample
Epoch 48/90
84077/84077 - 42s - loss: 4.9831e-04 - val_loss: 6.1707e-04 - 42s/epoch - 496us/sample
Epoch 49/90
84077/84077 - 41s - loss: 4.9719e-04 - val_loss: 6.3205e-04 - 41s/epoch - 493us/sample
Epoch 50/90
84077/84077 - 42s - loss: 4.9634e-04 - val_loss: 6.1221e-04 - 42s/epoch - 494us/sample
Epoch 51/90
84077/84077 - 41s - loss: 4.9590e-04 - val_loss: 6.3206e-04 - 41s/epoch - 492us/sample
Epoch 52/90
84077/84077 - 41s - loss: 4.9587e-04 - val_loss: 6.3450e-04 - 41s/epoch - 492us/sample
Epoch 53/90
84077/84077 - 42s - loss: 4.9457e-04 - val_loss: 6.1855e-04 - 42s/epoch - 495us/sample
Epoch 54/90
84077/84077 - 42s - loss: 4.9476e-04 - val_loss: 6.0455e-04 - 42s/epoch - 494us/sample
Epoch 55/90
84077/84077 - 41s - loss: 4.9438e-04 - val_loss: 6.1671e-04 - 41s/epoch - 493us/sample
Epoch 56/90
84077/84077 - 41s - loss: 4.9441e-04 - val_loss: 6.1069e-04 - 41s/epoch - 493us/sample
Epoch 57/90
84077/84077 - 41s - loss: 4.9372e-04 - val_loss: 6.3606e-04 - 41s/epoch - 493us/sample
Epoch 58/90
84077/84077 - 41s - loss: 4.9225e-04 - val_loss: 6.3186e-04 - 41s/epoch - 493us/sample
Epoch 59/90
84077/84077 - 41s - loss: 4.9236e-04 - val_loss: 6.1863e-04 - 41s/epoch - 493us/sample
Epoch 60/90
84077/84077 - 42s - loss: 4.9096e-04 - val_loss: 6.0435e-04 - 42s/epoch - 494us/sample
Epoch 61/90
84077/84077 - 42s - loss: 4.9090e-04 - val_loss: 6.1217e-04 - 42s/epoch - 494us/sample
Epoch 62/90
84077/84077 - 41s - loss: 4.9073e-04 - val_loss: 6.0338e-04 - 41s/epoch - 493us/sample
Epoch 63/90
84077/84077 - 42s - loss: 4.8976e-04 - val_loss: 5.9518e-04 - 42s/epoch - 495us/sample
Epoch 64/90
84077/84077 - 41s - loss: 4.8978e-04 - val_loss: 5.9562e-04 - 41s/epoch - 492us/sample
Epoch 65/90
84077/84077 - 42s - loss: 4.8957e-04 - val_loss: 5.8719e-04 - 42s/epoch - 496us/sample
Epoch 66/90
84077/84077 - 41s - loss: 4.8941e-04 - val_loss: 5.8367e-04 - 41s/epoch - 491us/sample
Epoch 67/90
84077/84077 - 42s - loss: 4.8899e-04 - val_loss: 5.8035e-04 - 42s/epoch - 496us/sample
Epoch 68/90
84077/84077 - 41s - loss: 4.8822e-04 - val_loss: 5.7047e-04 - 41s/epoch - 491us/sample
Epoch 69/90
84077/84077 - 42s - loss: 4.8768e-04 - val_loss: 5.6679e-04 - 42s/epoch - 495us/sample
Epoch 70/90
84077/84077 - 41s - loss: 4.8824e-04 - val_loss: 5.8455e-04 - 41s/epoch - 494us/sample
Epoch 71/90
84077/84077 - 42s - loss: 4.8801e-04 - val_loss: 5.6756e-04 - 42s/epoch - 494us/sample
Epoch 72/90
84077/84077 - 41s - loss: 4.8713e-04 - val_loss: 5.6894e-04 - 41s/epoch - 490us/sample
Epoch 73/90
84077/84077 - 42s - loss: 4.8713e-04 - val_loss: 5.6875e-04 - 42s/epoch - 496us/sample
Epoch 74/90
84077/84077 - 41s - loss: 4.8685e-04 - val_loss: 5.8242e-04 - 41s/epoch - 491us/sample
Epoch 75/90
84077/84077 - 41s - loss: 4.8595e-04 - val_loss: 5.6992e-04 - 41s/epoch - 493us/sample
Epoch 76/90
84077/84077 - 41s - loss: 4.8519e-04 - val_loss: 5.7447e-04 - 41s/epoch - 492us/sample
Epoch 77/90
84077/84077 - 42s - loss: 4.8425e-04 - val_loss: 5.6618e-04 - 42s/epoch - 495us/sample
Epoch 78/90
84077/84077 - 41s - loss: 4.8489e-04 - val_loss: 5.6425e-04 - 41s/epoch - 489us/sample
Epoch 79/90
84077/84077 - 42s - loss: 4.8369e-04 - val_loss: 5.8121e-04 - 42s/epoch - 496us/sample
Epoch 80/90
84077/84077 - 41s - loss: 4.8245e-04 - val_loss: 5.7774e-04 - 41s/epoch - 490us/sample
Epoch 81/90
84077/84077 - 41s - loss: 4.8124e-04 - val_loss: 5.7535e-04 - 41s/epoch - 492us/sample
Epoch 82/90
84077/84077 - 41s - loss: 4.8125e-04 - val_loss: 5.9548e-04 - 41s/epoch - 493us/sample
Epoch 83/90
84077/84077 - 42s - loss: 4.8026e-04 - val_loss: 5.6921e-04 - 42s/epoch - 496us/sample
Epoch 84/90
84077/84077 - 42s - loss: 4.8042e-04 - val_loss: 5.8627e-04 - 42s/epoch - 495us/sample
Epoch 85/90
84077/84077 - 41s - loss: 4.7995e-04 - val_loss: 5.8074e-04 - 41s/epoch - 490us/sample
Epoch 86/90
84077/84077 - 41s - loss: 4.8004e-04 - val_loss: 5.8292e-04 - 41s/epoch - 493us/sample
Epoch 87/90
84077/84077 - 42s - loss: 4.7847e-04 - val_loss: 5.7953e-04 - 42s/epoch - 494us/sample
Epoch 88/90
84077/84077 - 41s - loss: 4.7939e-04 - val_loss: 5.7616e-04 - 41s/epoch - 492us/sample
Epoch 89/90
84077/84077 - 41s - loss: 4.7905e-04 - val_loss: 5.8477e-04 - 41s/epoch - 493us/sample
Epoch 90/90
84077/84077 - 42s - loss: 4.7895e-04 - val_loss: 5.5657e-04 - 42s/epoch - 494us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0005565666550257686
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 18:16:00.951699: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_99/outputlayer/BiasAdd' id:111267 op device:{requested: '', assigned: ''} def:{{{node decoder_model_99/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_99/outputlayer/MatMul, decoder_model_99/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10269252277426935
cosine 0.1013323984514502
MAE: 0.0039721033376149
RMSE: 0.021569156551969382
r2: 0.6365808786841938
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 16, 90, 0.001, 0.2, 188, 0.00047894839760840907, 0.0005565666550257686, 0.10269252277426935, 0.1013323984514502, 0.0039721033376149, 0.021569156551969382, 0.6365808786841938, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0012000000000000001 16 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_258 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_258 (ReLU)               (None, 1980)         0           ['batch_normalization_258[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_258[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_258[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-13 18:16:44.489304: W tensorflow/c/c_api.cc:291] Operation '{name:'training_144/Adam/dense_enc0_100/bias/m/Assign' id:113011 op device:{requested: '', assigned: ''} def:{{{node training_144/Adam/dense_enc0_100/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_144/Adam/dense_enc0_100/bias/m, training_144/Adam/dense_enc0_100/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-13 18:17:34.769395: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_100/mul' id:112551 op device:{requested: '', assigned: ''} def:{{{node loss_100/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_100/mul/x, loss_100/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 70s - loss: 0.0047 - val_loss: 1.8199 - 70s/epoch - 833us/sample
Epoch 2/90
84077/84077 - 42s - loss: 0.0013 - val_loss: 9.1276e-04 - 42s/epoch - 497us/sample
Epoch 3/90
84077/84077 - 42s - loss: 9.3004e-04 - val_loss: 7.6576e-04 - 42s/epoch - 495us/sample
Epoch 4/90
84077/84077 - 42s - loss: 7.8662e-04 - val_loss: 7.1151e-04 - 42s/epoch - 497us/sample
Epoch 5/90
84077/84077 - 42s - loss: 7.1193e-04 - val_loss: 6.7531e-04 - 42s/epoch - 494us/sample
Epoch 6/90
84077/84077 - 42s - loss: 6.5384e-04 - val_loss: 6.6313e-04 - 42s/epoch - 498us/sample
Epoch 7/90
84077/84077 - 42s - loss: 6.1984e-04 - val_loss: 6.9103e-04 - 42s/epoch - 496us/sample
Epoch 8/90
84077/84077 - 42s - loss: 5.9371e-04 - val_loss: 6.8963e-04 - 42s/epoch - 496us/sample
Epoch 9/90
84077/84077 - 42s - loss: 5.7233e-04 - val_loss: 7.1089e-04 - 42s/epoch - 494us/sample
Epoch 10/90
84077/84077 - 42s - loss: 5.5502e-04 - val_loss: 6.9341e-04 - 42s/epoch - 495us/sample
Epoch 11/90
84077/84077 - 42s - loss: 5.4274e-04 - val_loss: 6.7499e-04 - 42s/epoch - 494us/sample
Epoch 12/90
84077/84077 - 42s - loss: 5.3044e-04 - val_loss: 6.6762e-04 - 42s/epoch - 495us/sample
Epoch 13/90
84077/84077 - 42s - loss: 5.2070e-04 - val_loss: 6.1332e-04 - 42s/epoch - 497us/sample
Epoch 14/90
84077/84077 - 41s - loss: 5.0977e-04 - val_loss: 6.1775e-04 - 41s/epoch - 493us/sample
Epoch 15/90
84077/84077 - 42s - loss: 5.0018e-04 - val_loss: 6.1174e-04 - 42s/epoch - 497us/sample
Epoch 16/90
84077/84077 - 42s - loss: 4.9206e-04 - val_loss: 5.8226e-04 - 42s/epoch - 505us/sample
Epoch 17/90
84077/84077 - 42s - loss: 4.8753e-04 - val_loss: 5.7261e-04 - 42s/epoch - 498us/sample
Epoch 18/90
84077/84077 - 41s - loss: 4.8118e-04 - val_loss: 5.7778e-04 - 41s/epoch - 493us/sample
Epoch 19/90
84077/84077 - 42s - loss: 4.7570e-04 - val_loss: 5.7407e-04 - 42s/epoch - 498us/sample
Epoch 20/90
84077/84077 - 42s - loss: 4.7208e-04 - val_loss: 5.3958e-04 - 42s/epoch - 496us/sample
Epoch 21/90
84077/84077 - 42s - loss: 4.6310e-04 - val_loss: 5.3848e-04 - 42s/epoch - 498us/sample
Epoch 22/90
84077/84077 - 42s - loss: 4.6006e-04 - val_loss: 5.3230e-04 - 42s/epoch - 495us/sample
Epoch 23/90
84077/84077 - 42s - loss: 4.5465e-04 - val_loss: 5.1831e-04 - 42s/epoch - 499us/sample
Epoch 24/90
84077/84077 - 41s - loss: 4.5251e-04 - val_loss: 5.0543e-04 - 41s/epoch - 493us/sample
Epoch 25/90
84077/84077 - 42s - loss: 4.4708e-04 - val_loss: 5.0803e-04 - 42s/epoch - 496us/sample
Epoch 26/90
84077/84077 - 42s - loss: 4.4432e-04 - val_loss: 4.9017e-04 - 42s/epoch - 496us/sample
Epoch 27/90
84077/84077 - 42s - loss: 4.4048e-04 - val_loss: 4.7959e-04 - 42s/epoch - 496us/sample
Epoch 28/90
84077/84077 - 42s - loss: 4.3605e-04 - val_loss: 4.5979e-04 - 42s/epoch - 495us/sample
Epoch 29/90
84077/84077 - 42s - loss: 4.3069e-04 - val_loss: 4.5017e-04 - 42s/epoch - 497us/sample
Epoch 30/90
84077/84077 - 42s - loss: 4.2537e-04 - val_loss: 4.4220e-04 - 42s/epoch - 496us/sample
Epoch 31/90
84077/84077 - 42s - loss: 4.2266e-04 - val_loss: 4.2838e-04 - 42s/epoch - 496us/sample
Epoch 32/90
84077/84077 - 42s - loss: 4.1870e-04 - val_loss: 4.4717e-04 - 42s/epoch - 498us/sample
Epoch 33/90
84077/84077 - 42s - loss: 4.1476e-04 - val_loss: 4.1188e-04 - 42s/epoch - 495us/sample
Epoch 34/90
84077/84077 - 42s - loss: 4.1203e-04 - val_loss: 4.2258e-04 - 42s/epoch - 496us/sample
Epoch 35/90
84077/84077 - 42s - loss: 4.0823e-04 - val_loss: 4.0301e-04 - 42s/epoch - 495us/sample
Epoch 36/90
84077/84077 - 42s - loss: 4.0456e-04 - val_loss: 3.8835e-04 - 42s/epoch - 498us/sample
Epoch 37/90
84077/84077 - 41s - loss: 4.0559e-04 - val_loss: 3.9425e-04 - 41s/epoch - 493us/sample
Epoch 38/90
84077/84077 - 42s - loss: 4.0009e-04 - val_loss: 3.8576e-04 - 42s/epoch - 497us/sample
Epoch 39/90
84077/84077 - 42s - loss: 3.9803e-04 - val_loss: 3.8759e-04 - 42s/epoch - 495us/sample
Epoch 40/90
84077/84077 - 42s - loss: 3.9598e-04 - val_loss: 3.7588e-04 - 42s/epoch - 498us/sample
Epoch 41/90
84077/84077 - 42s - loss: 3.9440e-04 - val_loss: 3.8139e-04 - 42s/epoch - 494us/sample
Epoch 42/90
84077/84077 - 42s - loss: 3.9078e-04 - val_loss: 3.7857e-04 - 42s/epoch - 498us/sample
Epoch 43/90
84077/84077 - 42s - loss: 3.9072e-04 - val_loss: 3.7454e-04 - 42s/epoch - 494us/sample
Epoch 44/90
84077/84077 - 42s - loss: 3.8647e-04 - val_loss: 3.6429e-04 - 42s/epoch - 495us/sample
Epoch 45/90
84077/84077 - 42s - loss: 3.8413e-04 - val_loss: 3.9519e-04 - 42s/epoch - 495us/sample
Epoch 46/90
84077/84077 - 42s - loss: 3.8181e-04 - val_loss: 3.7220e-04 - 42s/epoch - 499us/sample
Epoch 47/90
84077/84077 - 41s - loss: 3.8137e-04 - val_loss: 3.6823e-04 - 41s/epoch - 492us/sample
Epoch 48/90
84077/84077 - 42s - loss: 3.7855e-04 - val_loss: 3.6541e-04 - 42s/epoch - 498us/sample
Epoch 49/90
84077/84077 - 42s - loss: 3.7505e-04 - val_loss: 3.5690e-04 - 42s/epoch - 495us/sample
Epoch 50/90
84077/84077 - 42s - loss: 3.7895e-04 - val_loss: 3.5575e-04 - 42s/epoch - 499us/sample
Epoch 51/90
84077/84077 - 42s - loss: 3.7167e-04 - val_loss: 3.5295e-04 - 42s/epoch - 496us/sample
Epoch 52/90
84077/84077 - 42s - loss: 3.6906e-04 - val_loss: 3.5029e-04 - 42s/epoch - 495us/sample
Epoch 53/90
84077/84077 - 42s - loss: 3.6528e-04 - val_loss: 3.5073e-04 - 42s/epoch - 497us/sample
Epoch 54/90
84077/84077 - 41s - loss: 3.6336e-04 - val_loss: 3.4600e-04 - 41s/epoch - 492us/sample
Epoch 55/90
84077/84077 - 42s - loss: 3.6132e-04 - val_loss: 3.3732e-04 - 42s/epoch - 499us/sample
Epoch 56/90
84077/84077 - 42s - loss: 3.5940e-04 - val_loss: 3.4153e-04 - 42s/epoch - 494us/sample
Epoch 57/90
84077/84077 - 42s - loss: 3.5637e-04 - val_loss: 3.3645e-04 - 42s/epoch - 497us/sample
Epoch 58/90
84077/84077 - 42s - loss: 3.5463e-04 - val_loss: 3.3968e-04 - 42s/epoch - 494us/sample
Epoch 59/90
84077/84077 - 42s - loss: 3.5025e-04 - val_loss: 3.3309e-04 - 42s/epoch - 496us/sample
Epoch 60/90
84077/84077 - 41s - loss: 3.5029e-04 - val_loss: 3.3160e-04 - 41s/epoch - 493us/sample
Epoch 61/90
84077/84077 - 42s - loss: 3.4817e-04 - val_loss: 3.2787e-04 - 42s/epoch - 498us/sample
Epoch 62/90
84077/84077 - 42s - loss: 3.4628e-04 - val_loss: 3.2782e-04 - 42s/epoch - 495us/sample
Epoch 63/90
84077/84077 - 42s - loss: 3.4223e-04 - val_loss: 3.2718e-04 - 42s/epoch - 496us/sample
Epoch 64/90
84077/84077 - 41s - loss: 3.4088e-04 - val_loss: 3.2516e-04 - 41s/epoch - 492us/sample
Epoch 65/90
84077/84077 - 42s - loss: 3.3735e-04 - val_loss: 3.1920e-04 - 42s/epoch - 497us/sample
Epoch 66/90
84077/84077 - 42s - loss: 3.3851e-04 - val_loss: 3.2273e-04 - 42s/epoch - 494us/sample
Epoch 67/90
84077/84077 - 42s - loss: 3.3502e-04 - val_loss: 3.1601e-04 - 42s/epoch - 497us/sample
Epoch 68/90
84077/84077 - 42s - loss: 3.3732e-04 - val_loss: 3.1321e-04 - 42s/epoch - 494us/sample
Epoch 69/90
84077/84077 - 42s - loss: 3.3242e-04 - val_loss: 3.1509e-04 - 42s/epoch - 498us/sample
Epoch 70/90
84077/84077 - 42s - loss: 3.2980e-04 - val_loss: 3.1211e-04 - 42s/epoch - 494us/sample
Epoch 71/90
84077/84077 - 42s - loss: 3.3159e-04 - val_loss: 3.1592e-04 - 42s/epoch - 497us/sample
Epoch 72/90
84077/84077 - 42s - loss: 3.2694e-04 - val_loss: 3.0610e-04 - 42s/epoch - 496us/sample
Epoch 73/90
84077/84077 - 41s - loss: 3.2814e-04 - val_loss: 3.0380e-04 - 41s/epoch - 493us/sample
Epoch 74/90
84077/84077 - 41s - loss: 3.2688e-04 - val_loss: 3.7433e-04 - 41s/epoch - 492us/sample
Epoch 75/90
84077/84077 - 41s - loss: 3.3061e-04 - val_loss: 3.1662e-04 - 41s/epoch - 489us/sample
Epoch 76/90
84077/84077 - 41s - loss: 3.2652e-04 - val_loss: 3.0538e-04 - 41s/epoch - 493us/sample
Epoch 77/90
84077/84077 - 41s - loss: 3.2197e-04 - val_loss: 3.0781e-04 - 41s/epoch - 489us/sample
Epoch 78/90
84077/84077 - 41s - loss: 3.2556e-04 - val_loss: 3.1480e-04 - 41s/epoch - 493us/sample
Epoch 79/90
84077/84077 - 41s - loss: 3.2703e-04 - val_loss: 3.0741e-04 - 41s/epoch - 490us/sample
Epoch 80/90
84077/84077 - 41s - loss: 3.2336e-04 - val_loss: 3.0273e-04 - 41s/epoch - 493us/sample
Epoch 81/90
84077/84077 - 41s - loss: 3.2178e-04 - val_loss: 3.1089e-04 - 41s/epoch - 489us/sample
Epoch 82/90
84077/84077 - 42s - loss: 3.1994e-04 - val_loss: 2.9794e-04 - 42s/epoch - 494us/sample
Epoch 83/90
84077/84077 - 41s - loss: 3.1837e-04 - val_loss: 2.9576e-04 - 41s/epoch - 488us/sample
Epoch 84/90
84077/84077 - 41s - loss: 3.2073e-04 - val_loss: 2.9764e-04 - 41s/epoch - 492us/sample
Epoch 85/90
84077/84077 - 41s - loss: 3.1796e-04 - val_loss: 2.9638e-04 - 41s/epoch - 489us/sample
Epoch 86/90
84077/84077 - 41s - loss: 3.1515e-04 - val_loss: 2.9720e-04 - 41s/epoch - 492us/sample
Epoch 87/90
84077/84077 - 41s - loss: 3.1523e-04 - val_loss: 2.9836e-04 - 41s/epoch - 490us/sample
Epoch 88/90
84077/84077 - 41s - loss: 3.1740e-04 - val_loss: 2.9260e-04 - 41s/epoch - 493us/sample
Epoch 89/90
84077/84077 - 41s - loss: 3.1767e-04 - val_loss: 2.9914e-04 - 41s/epoch - 489us/sample
Epoch 90/90
84077/84077 - 41s - loss: 3.1208e-04 - val_loss: 2.9860e-04 - 41s/epoch - 492us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002985965273790843
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-13 19:19:29.638910: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_100/outputlayer/BiasAdd' id:112522 op device:{requested: '', assigned: ''} def:{{{node decoder_model_100/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_100/outputlayer/MatMul, decoder_model_100/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04655811628517024
cosine 0.04600742130938592
MAE: 0.002887478655406623
RMSE: 0.01381228315339575
r2: 0.8510061390665047
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 16, 90, 0.0012000000000000001, 0.2, 188, 0.00031207738900009067, 0.0002985965273790843, 0.04655811628517024, 0.04600742130938592, 0.002887478655406623, 0.01381228315339575, 0.8510061390665047, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 10
Fitness    = 541.8276370289637
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 541.8276370289637.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 541.8276370289637, 541.8276370289637, 541.8276370289637]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:3345: UserWarning: Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.
  warnings.warn("Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.")
Best solutions :  [[2.0 90 0.001 64 1]
 [2.0 90 0.001 64 1]
 [2.0 90 0.001 64 1]
 [2.0 90 0.001 64 1]
 [2.0 90 0.001 64 1]
 [2.0 90 0.001 64 1]
 [2.0 90 0.001 64 1]
 [2.1 90 0.001 64 1]
 [2.1 90 0.001 64 1]
 [2.1 90 0.001 64 1]
 [2.1 90 0.001 64 1]]
Best solutions fitness :  [511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 511.49230331698664, 541.8276370289637, 541.8276370289637, 541.8276370289637, 541.8276370289637]
Solution 0: [2.0, 85, 0.0012, 64, 1], Fitness: 495.0672872731068
Solution 1: [2.0, 90, 0.001, 32, 1], Fitness: 452.2927008743113
Solution 2: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 3: [2.1, 100, 0.0008, 64, 0], Fitness: 0.17644362015488807
Solution 4: [2.0, 85, 0.0012, 64, 2], Fitness: 440.7871161543128
Solution 5: [1.9, 85, 0.0012, 64, 1], Fitness: 321.5325259482288
Solution 6: [2.0, 90, 0.0008, 32, 1], Fitness: 306.9000014320915
Solution 7: [1.8, 90, 0.0008, 16, 2], Fitness: 320.99913246330436
Solution 8: [2.2, 90, 0.0006, 64, 0], Fitness: 0.1730025820305872
Solution 9: [1.8, 95, 0.0012, 64, 1], Fitness: 362.8932654734305
Solution 10: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 11: [2.1, 85, 0.0014, 64, 1], Fitness: 376.3834620928492
Solution 12: [1.8, 90, 0.0008, 32, 0], Fitness: 0.1992040138031251
Solution 13: [1.9, 90, 0.0008, 32, 2], Fitness: 369.16980492870323
Solution 14: [1.9, 85, 0.0012, 16, 2], Fitness: 209.74907114792106
Solution 15: [1.9, 85, 0.0012, 64, 1], Fitness: 321.5325259482288
Solution 16: [1.9000000000000001, 95, 0.0012, 32, 2], Fitness: 333.1183925121379
Solution 17: [2.1, 90, 0.001, 64, 2], Fitness: 377.607602795782
Solution 18: [2.0, 80, 0.0012, 64, 2], Fitness: 449.3742726683899
Solution 19: [1.9, 95, 0.0012, 64, 1], Fitness: 446.30358683854723
Solution 20: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 21: [2.0, 80, 0.0012, 64, 0], Fitness: 0.19490042401326935
Solution 22: [2.1, 80, 0.0009999999999999998, 32, 2], Fitness: 417.383196454871
Solution 23: [1.9, 90, 0.0006000000000000001, 32, 2], Fitness: 274.7281094708558
Solution 24: [2.0, 90, 0.001, 32, 1], Fitness: 452.2927008743113
Solution 25: [2.0, 80, 0.0009999999999999998, 64, 2], Fitness: 420.4448839990982
Solution 26: [2.1, 90, 0.0008, 32, 2], Fitness: 344.2575728726588
Solution 27: [1.9, 85, 0.0008, 64, 2], Fitness: 434.6324456406333
Solution 28: [1.9, 90, 0.0008, 32, 1], Fitness: 460.0808439767788
Solution 29: [2.2, 80, 0.0012, 64, 2], Fitness: 405.5501327224097
Solution 30: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 31: [1.9, 90, 0.0008, 64, 2], Fitness: 425.50618018432624
Solution 32: [1.7999999999999998, 90, 0.0006000000000000001, 32, 2], Fitness: 328.1834663171097
Solution 33: [1.9, 90, 0.0008, 64, 1], Fitness: 497.92118534818957
Solution 34: [1.7999999999999998, 90, 0.0006000000000000001, 32, 1], Fitness: 396.69673703236435
Solution 35: [2.0, 85, 0.0008, 32, 2], Fitness: 384.4375933707684
Solution 36: [2.0, 90, 0.0006000000000000001, 32, 1], Fitness: 422.6027083090863
Solution 37: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 38: [1.7999999999999998, 85, 0.0008, 32, 1], Fitness: 482.223661545531
Solution 39: [1.9, 90, 0.0006000000000000001, 64, 2], Fitness: 412.01521378030253
Solution 40: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 41: [1.7999999999999998, 85, 0.0006000000000000001, 32, 2], Fitness: 371.9137120492481
Solution 42: [1.7999999999999998, 90, 0.0004000000000000001, 64, 2], Fitness: 428.6910277556232
Solution 43: [1.9, 85, 0.0008, 32, 0], Fitness: 0.19269212811010736
Solution 44: [1.7999999999999998, 90, 0.0004000000000000001, 32, 1], Fitness: 483.3051564781413
Solution 45: [1.7999999999999998, 90, 0.0004000000000000001, 32, 1], Fitness: 482.501282603839
Solution 46: [1.7999999999999998, 90, 0.0008, 64, 0], Fitness: 0.12860906768981098
Solution 47: [1.6999999999999997, 85, 0.0008, 32, 1], Fitness: 451.13575682261035
Solution 48: [1.6999999999999997, 85, 0.0008, 32, 1], Fitness: 452.9616451026254
Solution 49: [1.7999999999999998, 85, 0.0006000000000000001, 32, 1], Fitness: 380.6799615643617
Solution 50: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 51: [1.7999999999999998, 90, 0.0004000000000000001, 32, 2], Fitness: 396.82846352891227
Solution 52: [1.7999999999999998, 90, 0.00020000000000000006, 16, 2], Fitness: 243.5602431937555
Solution 53: [2.0, 90, 0.001, 16, 0], Fitness: 0.1828732729536359
Solution 54: [1.7999999999999998, 85, 0.0004000000000000001, 8, 1], Fitness: 30.439419152681005
Solution 55: [1.7999999999999998, 90, 0.0004000000000000001, 64, 2], Fitness: 428.6910277556232
Solution 56: [1.9, 90, 0.00020000000000000006, 32, 2], Fitness: 401.7364067187878
Solution 57: [1.7999999999999998, 80, 0.001, 64, 1], Fitness: 499.18092747937914
Solution 58: [2.0, 90, 0.001, 32, 0], Fitness: 0.1870470342843226
Solution 59: [1.9, 90, 0.00020000000000000006, 64, 2], Fitness: 447.9296947403597
Solution 60: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 61: [1.9, 90, 0.001, 8, 1], Fitness: 39.872475672640164
Solution 62: [1.7999999999999998, 90, 0.0006000000000000001, 64, 2], Fitness: 410.43507677616225
Solution 63: [1.9, 90, 0.0004000000000000001, 32, 1], Fitness: 417.1229860872119
Solution 64: [1.6999999999999997, 80, 0.001, 64, 2], Fitness: 452.3746201496573
Solution 65: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 66: [1.9, 90, 0.0004000000000000001, 32, 2], Fitness: 290.6878298658832
Solution 67: [1.6999999999999997, 90, 0.0004000000000000001, 64, 1], Fitness: 271.8010020967037
Solution 68: [1.6999999999999997, 80, 0.00020000000000000006, 64, 2], Fitness: 387.96931265666694
Solution 69: [1.9, 85, 0.001, 8, 1], Fitness: 65.86898345993862
Solution 70: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 71: [2.0, 80, 0.001, 64, 2], Fitness: 413.23576472136915
Solution 72: [1.5999999999999996, 80, 0.001, 32, 2], Fitness: 397.52010440317383
Solution 73: [1.9, 90, 0.001, 8, 1], Fitness: 39.872475672640164
Solution 74: [2.1, 90, 0.001, 64, 1], Fitness: 541.8276370289637
Solution 75: [2.0, 80, 0.0012000000000000001, 64, 2], Fitness: 414.34944004973346
Solution 76: [1.7999999999999998, 80, 0.001, 64, 2], Fitness: 411.410464566604
Solution 77: [1.6999999999999997, 85, 0.001, 64, 1], Fitness: 438.58374305938236
Solution 78: [2.1, 90, 0.001, 32, 1], Fitness: 473.47395220220403
Solution 79: [2.0, 90, 0.001, 64, 2], Fitness: 458.09633054836456
Solution 80: [2.1, 90, 0.001, 64, 1], Fitness: 541.8276370289637
Solution 81: [1.6999999999999997, 85, 0.001, 64, 1], Fitness: 438.58374305938236
Solution 82: [2.2, 90, 0.001, 64, 1], Fitness: 414.36228612475776
Solution 83: [2.0, 85, 0.001, 64, 1], Fitness: 518.2433362595738
Solution 84: [2.1, 90, 0.0012000000000000001, 64, 1], Fitness: 539.6602867342509
Solution 85: [1.6999999999999997, 85, 0.0008, 16, 1], Fitness: 394.1543011519332
Solution 86: [2.0, 90, 0.001, 32, 1], Fitness: 452.2927008743113
Solution 87: [2.1, 90, 0.001, 64, 1], Fitness: 541.8276370289637
Solution 88: [2.2, 85, 0.001, 64, 2], Fitness: 434.7077901544556
Solution 89: [2.2, 90, 0.001, 16, 1], Fitness: 269.9793383439354
Solution 90: [2.1, 90, 0.001, 64, 1], Fitness: 541.8276370289637
Solution 91: [2.1, 90, 0.0008, 64, 1], Fitness: 520.0721518753321
Solution 92: [2.2, 90, 0.0012000000000000001, 64, 1], Fitness: 531.0744031493928
Solution 93: [2.2, 90, 0.0012000000000000001, 64, 1], Fitness: 530.9638098664312
Solution 94: [2.3000000000000003, 90, 0.0012000000000000001, 64, 1], Fitness: 354.4510403799487
Solution 95: [2.2, 90, 0.001, 8, 1], Fitness: 82.17419735112088
Solution 96: [2.1, 85, 0.001, 64, 1], Fitness: 413.32633406479084
Solution 97: [2.2, 85, 0.001, 64, 2], Fitness: 434.7077901544556
Solution 98: [2.3000000000000003, 90, 0.0008, 64, 1], Fitness: 491.4980409449148
Solution 99: [2.1, 90, 0.0008, 64, 0], Fitness: 0.1791693739592077
Solution 100: [2.1, 90, 0.001, 64, 1], Fitness: 541.8276370289637
Solution 101: [2.2, 85, 0.001, 64, 0], Fitness: 0.17235449996245147
Solution 102: [2.1, 85, 0.0008, 64, 1], Fitness: 494.7099322960561
Solution 103: [2.2, 90, 0.001, 64, 1], Fitness: 414.36228612475776
Solution 104: [2.0, 90, 0.001, 64, 1], Fitness: 511.49230331698664
Solution 105: [2.3000000000000003, 90, 0.0012000000000000001, 64, 2], Fitness: 419.20247517073426
Solution 106: [2.2, 90, 0.0008, 64, 2], Fitness: 433.64065035585065
Solution 107: [2.0, 90, 0.001, 16, 1], Fitness: 251.69242152163895
Solution 108: [2.1, 90, 0.0012000000000000001, 16, 1], Fitness: 346.2030083304271
Solution 109: [2.1, 90, 0.0012000000000000001, 64, 1], Fitness: 539.6602867342509
Traceback (most recent call last):
  File "genetic.py", line 239, in <module>
    
  File "genetic.py", line 201, in genetic_hypertune_autoencoder
    for i in range(len(ga_instance.solutions)):
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 3677, in save
    pickle.dump(self, file)
AttributeError: Can't pickle local object 'genetic_hypertune_autoencoder.<locals>.mutation_func'
Mon Feb 13 19:20:23 CET 2023
done
