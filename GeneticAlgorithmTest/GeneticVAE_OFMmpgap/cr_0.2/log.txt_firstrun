start
Sat Feb 11 02:45:09 CET 2023
2023-02-11 02:45:10.620737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-11 02:45:10.764965: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
         OFM: s^1 - s^1  OFM: s^1 - s^2  ...  OFM: f^14 - f^13  OFM: f^14 - f^14
id                                       ...                                    
id40482        0.000000        0.000000  ...               0.0          0.000000
id40486        0.000000        0.000000  ...               0.0          0.000000
id40487        0.000000        0.000000  ...               0.0          0.000022
id40490        0.000000        0.080160  ...               0.0          0.000000
id40496        0.000000        0.000000  ...               0.0          0.000000
...                 ...             ...  ...               ...               ...
id83741        0.174544        0.746256  ...               0.0          0.000000
id83742        0.000000        0.000000  ...               0.0          1.815492
id83743        0.007311        0.241355  ...               0.0          0.000000
id83744        0.000000        0.000000  ...               0.0          0.439306
id83745        0.000000        0.203940  ...               0.0          0.000000

[93419 rows x 1024 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.0 90 0.0005 8 0]
 [1.0 90 0.001 16 2]
 [1.0 10 0.001 64 2]
 [2.0 90 0.001 32 1]
 [0.5 10 0.0005 16 2]
 [1.5 10 0.0005 32 0]
 [2.0 170 0.0005 8 1]
 [0.5 30 0.0005 32 2]
 [1.5 170 0.0005 16 2]
 [1.0 10 0.0005 16 0]]
[1.0 90 0.0005 8 0] 0
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 943)          890192      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 943)         3772        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 943)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          177472      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          177472      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1108475     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,357,383
Trainable params: 2,353,235
Non-trainable params: 4,148
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-11 02:45:37.983547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-11 02:45:38.569446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9634 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5
2023-02-11 02:45:38.594621: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-11 02:45:38.907150: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/outputlayer/bias/v/Assign' id:1168 op device:{requested: '', assigned: ''} def:{{{node training/Adam/outputlayer/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/outputlayer/bias/v, training/Adam/outputlayer/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 02:46:09.039211: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:449 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 33s - loss: 0.0675 - val_loss: 0.0674 - 33s/epoch - 387us/sample
Epoch 2/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 3/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 4/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 5/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 6/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 7/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 8/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 9/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 10/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 11/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 12/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 13/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 14/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 15/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 16/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 17/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 18/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 19/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 20/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 21/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 22/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 23/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 354us/sample
Epoch 24/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 25/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 26/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 27/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 28/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 29/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 30/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 31/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 32/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 354us/sample
Epoch 33/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 34/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 35/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 36/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 37/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 38/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 39/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 353us/sample
Epoch 40/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 352us/sample
Epoch 41/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 352us/sample
Epoch 42/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 353us/sample
Epoch 43/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 44/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 45/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 46/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 47/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 352us/sample
Epoch 48/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 352us/sample
Epoch 49/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 354us/sample
Epoch 50/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 359us/sample
Epoch 51/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 52/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 53/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 353us/sample
Epoch 54/90
84077/84077 - 29s - loss: 0.0668 - val_loss: 0.0673 - 29s/epoch - 351us/sample
Epoch 55/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 353us/sample
Epoch 56/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 57/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 58/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 59/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 60/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 61/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 62/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 63/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 64/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 65/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 66/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 353us/sample
Epoch 67/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 68/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 69/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 70/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 71/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 72/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 73/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 74/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 75/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 76/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 77/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 78/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 355us/sample
Epoch 79/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 80/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 81/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 82/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 83/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 84/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 85/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 353us/sample
Epoch 86/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 353us/sample
Epoch 87/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 358us/sample
Epoch 88/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 89/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 90/90
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 359us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573474627215
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 03:30:33.269973: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:401 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.0453806949996571
cosine 1.1201994170511664
MAE: 5.662988121353952
RMSE: 6.450051248068176
r2: -30783.511492008856
RMSE zero-vector: 0.04004287452915337
['1.0custom_VAE', 'binary_crossentropy', 8, 90, 0.0005, 0.2, 188, 0.06683691787211708, 0.06734573474627215, 1.0453806949996571, 1.1201994170511664, 5.662988121353952, 6.450051248068176, -30783.511492008856, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 90 0.001 16 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 943)          890192      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 943)         3772        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 943)          0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          177472      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          177472      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1108475     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,357,383
Trainable params: 2,353,235
Non-trainable params: 4,148
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 03:30:38.780358: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_5/moving_variance/Assign' id:1559 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_5/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_5/moving_variance, batch_normalization_5/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 03:30:55.062760: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1736 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0018 - val_loss: 5.9668e-04 - 17s/epoch - 206us/sample
Epoch 2/90
84077/84077 - 17s - loss: 5.1927e-04 - val_loss: 4.1696e-04 - 17s/epoch - 197us/sample
Epoch 3/90
84077/84077 - 17s - loss: 4.1068e-04 - val_loss: 4.1526e-04 - 17s/epoch - 197us/sample
Epoch 4/90
84077/84077 - 16s - loss: 3.5946e-04 - val_loss: 3.7906e-04 - 16s/epoch - 196us/sample
Epoch 5/90
84077/84077 - 16s - loss: 3.3144e-04 - val_loss: 3.6904e-04 - 16s/epoch - 193us/sample
Epoch 6/90
84077/84077 - 16s - loss: 3.1335e-04 - val_loss: 3.4734e-04 - 16s/epoch - 195us/sample
Epoch 7/90
84077/84077 - 16s - loss: 2.9967e-04 - val_loss: 3.5566e-04 - 16s/epoch - 195us/sample
Epoch 8/90
84077/84077 - 17s - loss: 2.8933e-04 - val_loss: 3.1690e-04 - 17s/epoch - 197us/sample
Epoch 9/90
84077/84077 - 17s - loss: 2.8107e-04 - val_loss: 3.2308e-04 - 17s/epoch - 197us/sample
Epoch 10/90
84077/84077 - 17s - loss: 2.7357e-04 - val_loss: 3.0971e-04 - 17s/epoch - 198us/sample
Epoch 11/90
84077/84077 - 17s - loss: 2.6914e-04 - val_loss: 3.0909e-04 - 17s/epoch - 197us/sample
Epoch 12/90
84077/84077 - 17s - loss: 2.6453e-04 - val_loss: 3.0308e-04 - 17s/epoch - 198us/sample
Epoch 13/90
84077/84077 - 17s - loss: 2.6088e-04 - val_loss: 2.9696e-04 - 17s/epoch - 198us/sample
Epoch 14/90
84077/84077 - 17s - loss: 2.5731e-04 - val_loss: 2.9566e-04 - 17s/epoch - 197us/sample
Epoch 15/90
84077/84077 - 16s - loss: 2.5292e-04 - val_loss: 2.7577e-04 - 16s/epoch - 196us/sample
Epoch 16/90
84077/84077 - 17s - loss: 2.4875e-04 - val_loss: 2.8299e-04 - 17s/epoch - 197us/sample
Epoch 17/90
84077/84077 - 16s - loss: 2.4444e-04 - val_loss: 2.7599e-04 - 16s/epoch - 195us/sample
Epoch 18/90
84077/84077 - 17s - loss: 2.4139e-04 - val_loss: 2.6669e-04 - 17s/epoch - 197us/sample
Epoch 19/90
84077/84077 - 17s - loss: 2.3775e-04 - val_loss: 2.6837e-04 - 17s/epoch - 198us/sample
Epoch 20/90
84077/84077 - 17s - loss: 2.3556e-04 - val_loss: 2.6053e-04 - 17s/epoch - 198us/sample
Epoch 21/90
84077/84077 - 17s - loss: 2.3342e-04 - val_loss: 2.5907e-04 - 17s/epoch - 198us/sample
Epoch 22/90
84077/84077 - 17s - loss: 2.3135e-04 - val_loss: 2.5976e-04 - 17s/epoch - 198us/sample
Epoch 23/90
84077/84077 - 17s - loss: 2.2921e-04 - val_loss: 2.5448e-04 - 17s/epoch - 197us/sample
Epoch 24/90
84077/84077 - 17s - loss: 2.2754e-04 - val_loss: 2.5314e-04 - 17s/epoch - 197us/sample
Epoch 25/90
84077/84077 - 17s - loss: 2.2613e-04 - val_loss: 2.4699e-04 - 17s/epoch - 197us/sample
Epoch 26/90
84077/84077 - 17s - loss: 2.2400e-04 - val_loss: 2.4772e-04 - 17s/epoch - 198us/sample
Epoch 27/90
84077/84077 - 17s - loss: 2.2270e-04 - val_loss: 2.4423e-04 - 17s/epoch - 197us/sample
Epoch 28/90
84077/84077 - 17s - loss: 2.2217e-04 - val_loss: 2.4437e-04 - 17s/epoch - 198us/sample
Epoch 29/90
84077/84077 - 17s - loss: 2.2057e-04 - val_loss: 2.3990e-04 - 17s/epoch - 198us/sample
Epoch 30/90
84077/84077 - 17s - loss: 2.1935e-04 - val_loss: 2.3643e-04 - 17s/epoch - 197us/sample
Epoch 31/90
84077/84077 - 17s - loss: 2.1786e-04 - val_loss: 2.3524e-04 - 17s/epoch - 197us/sample
Epoch 32/90
84077/84077 - 17s - loss: 2.1676e-04 - val_loss: 2.4305e-04 - 17s/epoch - 198us/sample
Epoch 33/90
84077/84077 - 17s - loss: 2.1485e-04 - val_loss: 2.3373e-04 - 17s/epoch - 198us/sample
Epoch 34/90
84077/84077 - 17s - loss: 2.1439e-04 - val_loss: 2.4459e-04 - 17s/epoch - 197us/sample
Epoch 35/90
84077/84077 - 17s - loss: 2.1250e-04 - val_loss: 2.3368e-04 - 17s/epoch - 198us/sample
Epoch 36/90
84077/84077 - 16s - loss: 2.1261e-04 - val_loss: 2.3395e-04 - 16s/epoch - 193us/sample
Epoch 37/90
84077/84077 - 16s - loss: 2.1093e-04 - val_loss: 2.3260e-04 - 16s/epoch - 192us/sample
Epoch 38/90
84077/84077 - 16s - loss: 2.1050e-04 - val_loss: 2.3000e-04 - 16s/epoch - 193us/sample
Epoch 39/90
84077/84077 - 17s - loss: 2.0871e-04 - val_loss: 2.2896e-04 - 17s/epoch - 198us/sample
Epoch 40/90
84077/84077 - 17s - loss: 2.0740e-04 - val_loss: 2.2806e-04 - 17s/epoch - 198us/sample
Epoch 41/90
84077/84077 - 17s - loss: 2.0702e-04 - val_loss: 2.2656e-04 - 17s/epoch - 198us/sample
Epoch 42/90
84077/84077 - 17s - loss: 2.0576e-04 - val_loss: 2.2628e-04 - 17s/epoch - 197us/sample
Epoch 43/90
84077/84077 - 17s - loss: 2.0563e-04 - val_loss: 2.2436e-04 - 17s/epoch - 196us/sample
Epoch 44/90
84077/84077 - 17s - loss: 2.0436e-04 - val_loss: 2.2900e-04 - 17s/epoch - 197us/sample
Epoch 45/90
84077/84077 - 17s - loss: 2.0366e-04 - val_loss: 2.1859e-04 - 17s/epoch - 199us/sample
Epoch 46/90
84077/84077 - 17s - loss: 2.0337e-04 - val_loss: 2.1939e-04 - 17s/epoch - 198us/sample
Epoch 47/90
84077/84077 - 17s - loss: 2.0226e-04 - val_loss: 2.2088e-04 - 17s/epoch - 197us/sample
Epoch 48/90
84077/84077 - 16s - loss: 2.0255e-04 - val_loss: 2.1326e-04 - 16s/epoch - 195us/sample
Epoch 49/90
84077/84077 - 16s - loss: 2.0166e-04 - val_loss: 2.2246e-04 - 16s/epoch - 191us/sample
Epoch 50/90
84077/84077 - 16s - loss: 2.0086e-04 - val_loss: 2.2171e-04 - 16s/epoch - 192us/sample
Epoch 51/90
84077/84077 - 17s - loss: 2.0027e-04 - val_loss: 2.1933e-04 - 17s/epoch - 197us/sample
Epoch 52/90
84077/84077 - 17s - loss: 1.9982e-04 - val_loss: 2.1449e-04 - 17s/epoch - 198us/sample
Epoch 53/90
84077/84077 - 17s - loss: 1.9954e-04 - val_loss: 2.1862e-04 - 17s/epoch - 197us/sample
Epoch 54/90
84077/84077 - 17s - loss: 1.9888e-04 - val_loss: 2.0900e-04 - 17s/epoch - 198us/sample
Epoch 55/90
84077/84077 - 17s - loss: 1.9860e-04 - val_loss: 2.1274e-04 - 17s/epoch - 198us/sample
Epoch 56/90
84077/84077 - 17s - loss: 1.9776e-04 - val_loss: 2.0955e-04 - 17s/epoch - 197us/sample
Epoch 57/90
84077/84077 - 17s - loss: 1.9724e-04 - val_loss: 2.1317e-04 - 17s/epoch - 197us/sample
Epoch 58/90
84077/84077 - 17s - loss: 1.9744e-04 - val_loss: 2.1668e-04 - 17s/epoch - 198us/sample
Epoch 59/90
84077/84077 - 17s - loss: 1.9657e-04 - val_loss: 2.0920e-04 - 17s/epoch - 198us/sample
Epoch 60/90
84077/84077 - 16s - loss: 1.9635e-04 - val_loss: 2.0949e-04 - 16s/epoch - 195us/sample
Epoch 61/90
84077/84077 - 17s - loss: 1.9670e-04 - val_loss: 2.0890e-04 - 17s/epoch - 196us/sample
Epoch 62/90
84077/84077 - 16s - loss: 1.9524e-04 - val_loss: 2.1018e-04 - 16s/epoch - 195us/sample
Epoch 63/90
84077/84077 - 17s - loss: 1.9524e-04 - val_loss: 2.0686e-04 - 17s/epoch - 197us/sample
Epoch 64/90
84077/84077 - 17s - loss: 1.9492e-04 - val_loss: 2.0184e-04 - 17s/epoch - 198us/sample
Epoch 65/90
84077/84077 - 17s - loss: 1.9461e-04 - val_loss: 2.0779e-04 - 17s/epoch - 197us/sample
Epoch 66/90
84077/84077 - 17s - loss: 1.9402e-04 - val_loss: 2.0863e-04 - 17s/epoch - 197us/sample
Epoch 67/90
84077/84077 - 17s - loss: 1.9448e-04 - val_loss: 2.0808e-04 - 17s/epoch - 198us/sample
Epoch 68/90
84077/84077 - 17s - loss: 1.9384e-04 - val_loss: 2.0130e-04 - 17s/epoch - 197us/sample
Epoch 69/90
84077/84077 - 17s - loss: 1.9341e-04 - val_loss: 2.0376e-04 - 17s/epoch - 198us/sample
Epoch 70/90
84077/84077 - 17s - loss: 1.9339e-04 - val_loss: 2.0633e-04 - 17s/epoch - 198us/sample
Epoch 71/90
84077/84077 - 17s - loss: 1.9311e-04 - val_loss: 2.0873e-04 - 17s/epoch - 197us/sample
Epoch 72/90
84077/84077 - 17s - loss: 1.9267e-04 - val_loss: 1.9833e-04 - 17s/epoch - 198us/sample
Epoch 73/90
84077/84077 - 17s - loss: 1.9312e-04 - val_loss: 1.9825e-04 - 17s/epoch - 198us/sample
Epoch 74/90
84077/84077 - 17s - loss: 1.9282e-04 - val_loss: 2.0685e-04 - 17s/epoch - 197us/sample
Epoch 75/90
84077/84077 - 17s - loss: 1.9204e-04 - val_loss: 2.0047e-04 - 17s/epoch - 197us/sample
Epoch 76/90
84077/84077 - 17s - loss: 1.9209e-04 - val_loss: 2.0011e-04 - 17s/epoch - 199us/sample
Epoch 77/90
84077/84077 - 17s - loss: 1.9218e-04 - val_loss: 1.9686e-04 - 17s/epoch - 197us/sample
Epoch 78/90
84077/84077 - 17s - loss: 1.9136e-04 - val_loss: 2.0010e-04 - 17s/epoch - 198us/sample
Epoch 79/90
84077/84077 - 17s - loss: 1.9110e-04 - val_loss: 2.0180e-04 - 17s/epoch - 198us/sample
Epoch 80/90
84077/84077 - 17s - loss: 1.9128e-04 - val_loss: 1.9980e-04 - 17s/epoch - 197us/sample
Epoch 81/90
84077/84077 - 17s - loss: 1.9061e-04 - val_loss: 1.9840e-04 - 17s/epoch - 197us/sample
Epoch 82/90
84077/84077 - 17s - loss: 1.9036e-04 - val_loss: 1.9994e-04 - 17s/epoch - 199us/sample
Epoch 83/90
84077/84077 - 17s - loss: 1.9062e-04 - val_loss: 1.9964e-04 - 17s/epoch - 197us/sample
Epoch 84/90
84077/84077 - 17s - loss: 1.9062e-04 - val_loss: 2.0045e-04 - 17s/epoch - 198us/sample
Epoch 85/90
84077/84077 - 17s - loss: 1.9044e-04 - val_loss: 1.9789e-04 - 17s/epoch - 198us/sample
Epoch 86/90
84077/84077 - 17s - loss: 1.8967e-04 - val_loss: 1.9640e-04 - 17s/epoch - 197us/sample
Epoch 87/90
84077/84077 - 17s - loss: 1.8925e-04 - val_loss: 2.0226e-04 - 17s/epoch - 198us/sample
Epoch 88/90
84077/84077 - 17s - loss: 1.8929e-04 - val_loss: 1.9481e-04 - 17s/epoch - 198us/sample
Epoch 89/90
84077/84077 - 17s - loss: 1.8946e-04 - val_loss: 1.9808e-04 - 17s/epoch - 198us/sample
Epoch 90/90
84077/84077 - 17s - loss: 1.8897e-04 - val_loss: 1.9455e-04 - 17s/epoch - 197us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.000194553599365368
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 03:55:29.918126: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1700 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05652637737040695
cosine 0.055868256843774236
MAE: 0.003187810559622128
RMSE: 0.015589259309287137
r2: 0.8101838619299855
RMSE zero-vector: 0.04004287452915337
['1.0custom_VAE', 'logcosh', 16, 90, 0.001, 0.2, 188, 0.00018897025761213296, 0.000194553599365368, 0.05652637737040695, 0.055868256843774236, 0.003187810559622128, 0.015589259309287137, 0.8101838619299855, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 10 0.001 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 943)          890192      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 943)         3772        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 943)          0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          177472      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          177472      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1108475     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,357,383
Trainable params: 2,353,235
Non-trainable params: 4,148
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/10
2023-02-11 03:55:35.362269: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/batch_normalization_7/gamma/v/Assign' id:3611 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/batch_normalization_7/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/batch_normalization_7/gamma/v, training_4/Adam/batch_normalization_7/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 03:55:42.356964: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2977 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 8s - loss: 0.0030 - val_loss: 7.6194e-04 - 8s/epoch - 92us/sample
Epoch 2/10
84077/84077 - 7s - loss: 6.7086e-04 - val_loss: 6.1993e-04 - 7s/epoch - 82us/sample
Epoch 3/10
84077/84077 - 7s - loss: 5.9179e-04 - val_loss: 5.5747e-04 - 7s/epoch - 80us/sample
Epoch 4/10
84077/84077 - 7s - loss: 5.6882e-04 - val_loss: 0.0018 - 7s/epoch - 81us/sample
Epoch 5/10
84077/84077 - 7s - loss: 5.5582e-04 - val_loss: 4.5030e-04 - 7s/epoch - 81us/sample
Epoch 6/10
84077/84077 - 7s - loss: 4.4237e-04 - val_loss: 4.1206e-04 - 7s/epoch - 81us/sample
Epoch 7/10
84077/84077 - 7s - loss: 3.7795e-04 - val_loss: 3.1753e-04 - 7s/epoch - 81us/sample
Epoch 8/10
84077/84077 - 7s - loss: 3.3269e-04 - val_loss: 2.9136e-04 - 7s/epoch - 82us/sample
Epoch 9/10
84077/84077 - 7s - loss: 3.0519e-04 - val_loss: 2.7049e-04 - 7s/epoch - 81us/sample
Epoch 10/10
84077/84077 - 7s - loss: 2.8636e-04 - val_loss: 2.4838e-04 - 7s/epoch - 82us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002483844348928608
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 03:56:44.402323: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2941 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10266724065171466
cosine 0.10133698339850837
MAE: 0.00544483845134329
RMSE: 0.018694928734972003
r2: 0.7274031905883457
RMSE zero-vector: 0.04004287452915337
['1.0custom_VAE', 'logcosh', 64, 10, 0.001, 0.2, 188, 0.00028635822968652773, 0.0002483844348928608, 0.10266724065171466, 0.10133698339850837, 0.00544483845134329, 0.018694928734972003, 0.7274031905883457, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 1886)        7544        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 1886)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 03:56:50.115342: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/batch_normalization_9/beta/v/Assign' id:4822 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/batch_normalization_9/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/batch_normalization_9/beta/v, training_6/Adam/batch_normalization_9/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 03:57:00.545362: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4231 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 11s - loss: 0.0059 - val_loss: 37.8422 - 11s/epoch - 136us/sample
Epoch 2/90
84077/84077 - 10s - loss: 0.0041 - val_loss: 0.1668 - 10s/epoch - 122us/sample
Epoch 3/90
84077/84077 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 122us/sample
Epoch 4/90
84077/84077 - 10s - loss: 9.2456e-04 - val_loss: 7.6021e-04 - 10s/epoch - 122us/sample
Epoch 5/90
84077/84077 - 10s - loss: 7.9041e-04 - val_loss: 6.3692e-04 - 10s/epoch - 122us/sample
Epoch 6/90
84077/84077 - 10s - loss: 7.0323e-04 - val_loss: 5.6032e-04 - 10s/epoch - 122us/sample
Epoch 7/90
84077/84077 - 10s - loss: 6.0242e-04 - val_loss: 5.1310e-04 - 10s/epoch - 122us/sample
Epoch 8/90
84077/84077 - 10s - loss: 5.6371e-04 - val_loss: 4.7785e-04 - 10s/epoch - 122us/sample
Epoch 9/90
84077/84077 - 10s - loss: 5.2242e-04 - val_loss: 4.5677e-04 - 10s/epoch - 122us/sample
Epoch 10/90
84077/84077 - 10s - loss: 4.9344e-04 - val_loss: 4.3282e-04 - 10s/epoch - 122us/sample
Epoch 11/90
84077/84077 - 10s - loss: 4.7194e-04 - val_loss: 4.1665e-04 - 10s/epoch - 122us/sample
Epoch 12/90
84077/84077 - 10s - loss: 4.5509e-04 - val_loss: 4.1708e-04 - 10s/epoch - 121us/sample
Epoch 13/90
84077/84077 - 10s - loss: 4.3980e-04 - val_loss: 3.9577e-04 - 10s/epoch - 122us/sample
Epoch 14/90
84077/84077 - 10s - loss: 4.2901e-04 - val_loss: 3.9145e-04 - 10s/epoch - 121us/sample
Epoch 15/90
84077/84077 - 10s - loss: 4.1509e-04 - val_loss: 3.8051e-04 - 10s/epoch - 121us/sample
Epoch 16/90
84077/84077 - 10s - loss: 4.0449e-04 - val_loss: 3.5918e-04 - 10s/epoch - 121us/sample
Epoch 17/90
84077/84077 - 10s - loss: 3.9487e-04 - val_loss: 3.6019e-04 - 10s/epoch - 122us/sample
Epoch 18/90
84077/84077 - 10s - loss: 3.8620e-04 - val_loss: 3.4938e-04 - 10s/epoch - 123us/sample
Epoch 19/90
84077/84077 - 10s - loss: 3.7671e-04 - val_loss: 3.4109e-04 - 10s/epoch - 121us/sample
Epoch 20/90
84077/84077 - 10s - loss: 3.7099e-04 - val_loss: 3.3138e-04 - 10s/epoch - 121us/sample
Epoch 21/90
84077/84077 - 10s - loss: 3.6193e-04 - val_loss: 3.2597e-04 - 10s/epoch - 121us/sample
Epoch 22/90
84077/84077 - 10s - loss: 3.5529e-04 - val_loss: 3.1935e-04 - 10s/epoch - 122us/sample
Epoch 23/90
84077/84077 - 10s - loss: 3.4775e-04 - val_loss: 3.1582e-04 - 10s/epoch - 122us/sample
Epoch 24/90
84077/84077 - 10s - loss: 3.4295e-04 - val_loss: 3.1077e-04 - 10s/epoch - 122us/sample
Epoch 25/90
84077/84077 - 10s - loss: 3.4130e-04 - val_loss: 3.0510e-04 - 10s/epoch - 122us/sample
Epoch 26/90
84077/84077 - 10s - loss: 3.3624e-04 - val_loss: 3.0564e-04 - 10s/epoch - 122us/sample
Epoch 27/90
84077/84077 - 10s - loss: 3.2718e-04 - val_loss: 2.9494e-04 - 10s/epoch - 123us/sample
Epoch 28/90
84077/84077 - 10s - loss: 3.2610e-04 - val_loss: 2.9284e-04 - 10s/epoch - 122us/sample
Epoch 29/90
84077/84077 - 10s - loss: 3.1761e-04 - val_loss: 2.8790e-04 - 10s/epoch - 122us/sample
Epoch 30/90
84077/84077 - 10s - loss: 3.1667e-04 - val_loss: 2.8325e-04 - 10s/epoch - 122us/sample
Epoch 31/90
84077/84077 - 10s - loss: 3.1953e-04 - val_loss: 2.8202e-04 - 10s/epoch - 122us/sample
Epoch 32/90
84077/84077 - 10s - loss: 3.0894e-04 - val_loss: 2.7584e-04 - 10s/epoch - 121us/sample
Epoch 33/90
84077/84077 - 10s - loss: 3.0648e-04 - val_loss: 2.7533e-04 - 10s/epoch - 121us/sample
Epoch 34/90
84077/84077 - 10s - loss: 3.0148e-04 - val_loss: 2.7488e-04 - 10s/epoch - 120us/sample
Epoch 35/90
84077/84077 - 10s - loss: 2.9813e-04 - val_loss: 2.8204e-04 - 10s/epoch - 120us/sample
Epoch 36/90
84077/84077 - 10s - loss: 2.9814e-04 - val_loss: 2.6616e-04 - 10s/epoch - 121us/sample
Epoch 37/90
84077/84077 - 10s - loss: 2.9651e-04 - val_loss: 2.6686e-04 - 10s/epoch - 122us/sample
Epoch 38/90
84077/84077 - 10s - loss: 2.8952e-04 - val_loss: 2.5731e-04 - 10s/epoch - 122us/sample
Epoch 39/90
84077/84077 - 10s - loss: 2.9105e-04 - val_loss: 2.5846e-04 - 10s/epoch - 122us/sample
Epoch 40/90
84077/84077 - 10s - loss: 2.8823e-04 - val_loss: 2.5345e-04 - 10s/epoch - 121us/sample
Epoch 41/90
84077/84077 - 10s - loss: 2.8109e-04 - val_loss: 2.5136e-04 - 10s/epoch - 122us/sample
Epoch 42/90
84077/84077 - 10s - loss: 2.8326e-04 - val_loss: 2.5082e-04 - 10s/epoch - 122us/sample
Epoch 43/90
84077/84077 - 10s - loss: 2.8357e-04 - val_loss: 2.4986e-04 - 10s/epoch - 122us/sample
Epoch 44/90
84077/84077 - 10s - loss: 2.7490e-04 - val_loss: 2.5051e-04 - 10s/epoch - 122us/sample
Epoch 45/90
84077/84077 - 10s - loss: 2.7420e-04 - val_loss: 2.4442e-04 - 10s/epoch - 122us/sample
Epoch 46/90
84077/84077 - 10s - loss: 2.7686e-04 - val_loss: 2.4390e-04 - 10s/epoch - 122us/sample
Epoch 47/90
84077/84077 - 10s - loss: 2.6879e-04 - val_loss: 2.4069e-04 - 10s/epoch - 122us/sample
Epoch 48/90
84077/84077 - 10s - loss: 2.6702e-04 - val_loss: 2.3979e-04 - 10s/epoch - 122us/sample
Epoch 49/90
84077/84077 - 10s - loss: 2.6484e-04 - val_loss: 2.3977e-04 - 10s/epoch - 122us/sample
Epoch 50/90
84077/84077 - 10s - loss: 2.7173e-04 - val_loss: 2.4083e-04 - 10s/epoch - 122us/sample
Epoch 51/90
84077/84077 - 10s - loss: 2.6913e-04 - val_loss: 2.3752e-04 - 10s/epoch - 122us/sample
Epoch 52/90
84077/84077 - 10s - loss: 2.6104e-04 - val_loss: 2.3679e-04 - 10s/epoch - 122us/sample
Epoch 53/90
84077/84077 - 10s - loss: 2.6394e-04 - val_loss: 2.3486e-04 - 10s/epoch - 123us/sample
Epoch 54/90
84077/84077 - 10s - loss: 2.5628e-04 - val_loss: 2.3409e-04 - 10s/epoch - 122us/sample
Epoch 55/90
84077/84077 - 10s - loss: 2.5573e-04 - val_loss: 2.3171e-04 - 10s/epoch - 122us/sample
Epoch 56/90
84077/84077 - 10s - loss: 2.5973e-04 - val_loss: 2.3379e-04 - 10s/epoch - 122us/sample
Epoch 57/90
84077/84077 - 10s - loss: 2.5454e-04 - val_loss: 2.3265e-04 - 10s/epoch - 123us/sample
Epoch 58/90
84077/84077 - 10s - loss: 2.5737e-04 - val_loss: 2.3096e-04 - 10s/epoch - 122us/sample
Epoch 59/90
84077/84077 - 10s - loss: 2.5165e-04 - val_loss: 2.2712e-04 - 10s/epoch - 121us/sample
Epoch 60/90
84077/84077 - 10s - loss: 2.5246e-04 - val_loss: 2.3229e-04 - 10s/epoch - 121us/sample
Epoch 61/90
84077/84077 - 10s - loss: 2.5457e-04 - val_loss: 2.2529e-04 - 10s/epoch - 122us/sample
Epoch 62/90
84077/84077 - 10s - loss: 2.4808e-04 - val_loss: 2.2438e-04 - 10s/epoch - 122us/sample
Epoch 63/90
84077/84077 - 10s - loss: 2.5017e-04 - val_loss: 2.2642e-04 - 10s/epoch - 122us/sample
Epoch 64/90
84077/84077 - 10s - loss: 2.4666e-04 - val_loss: 2.2382e-04 - 10s/epoch - 122us/sample
Epoch 65/90
84077/84077 - 10s - loss: 2.4501e-04 - val_loss: 2.2594e-04 - 10s/epoch - 122us/sample
Epoch 66/90
84077/84077 - 10s - loss: 2.4344e-04 - val_loss: 2.2254e-04 - 10s/epoch - 123us/sample
Epoch 67/90
84077/84077 - 10s - loss: 2.4728e-04 - val_loss: 2.2367e-04 - 10s/epoch - 121us/sample
Epoch 68/90
84077/84077 - 10s - loss: 2.4606e-04 - val_loss: 2.1944e-04 - 10s/epoch - 121us/sample
Epoch 69/90
84077/84077 - 10s - loss: 2.4285e-04 - val_loss: 2.2042e-04 - 10s/epoch - 121us/sample
Epoch 70/90
84077/84077 - 10s - loss: 2.4271e-04 - val_loss: 2.2295e-04 - 10s/epoch - 121us/sample
Epoch 71/90
84077/84077 - 10s - loss: 2.4232e-04 - val_loss: 2.1910e-04 - 10s/epoch - 122us/sample
Epoch 72/90
84077/84077 - 10s - loss: 2.4022e-04 - val_loss: 2.2195e-04 - 10s/epoch - 122us/sample
Epoch 73/90
84077/84077 - 10s - loss: 2.4030e-04 - val_loss: 2.1721e-04 - 10s/epoch - 122us/sample
Epoch 74/90
84077/84077 - 10s - loss: 2.4206e-04 - val_loss: 2.1824e-04 - 10s/epoch - 122us/sample
Epoch 75/90
84077/84077 - 10s - loss: 2.3691e-04 - val_loss: 2.1644e-04 - 10s/epoch - 123us/sample
Epoch 76/90
84077/84077 - 10s - loss: 2.4338e-04 - val_loss: 2.1893e-04 - 10s/epoch - 123us/sample
Epoch 77/90
84077/84077 - 10s - loss: 2.4318e-04 - val_loss: 2.1796e-04 - 10s/epoch - 122us/sample
Epoch 78/90
84077/84077 - 10s - loss: 2.3701e-04 - val_loss: 2.1689e-04 - 10s/epoch - 122us/sample
Epoch 79/90
84077/84077 - 10s - loss: 2.3672e-04 - val_loss: 2.1568e-04 - 10s/epoch - 122us/sample
Epoch 80/90
84077/84077 - 10s - loss: 2.4241e-04 - val_loss: 2.1982e-04 - 10s/epoch - 123us/sample
Epoch 81/90
84077/84077 - 10s - loss: 2.3540e-04 - val_loss: 2.1643e-04 - 10s/epoch - 122us/sample
Epoch 82/90
84077/84077 - 10s - loss: 2.3396e-04 - val_loss: 2.1334e-04 - 10s/epoch - 122us/sample
Epoch 83/90
84077/84077 - 10s - loss: 2.3749e-04 - val_loss: 2.1458e-04 - 10s/epoch - 122us/sample
Epoch 84/90
84077/84077 - 10s - loss: 2.3538e-04 - val_loss: 2.1421e-04 - 10s/epoch - 122us/sample
Epoch 85/90
84077/84077 - 10s - loss: 2.3483e-04 - val_loss: 2.1395e-04 - 10s/epoch - 123us/sample
Epoch 86/90
84077/84077 - 10s - loss: 2.3491e-04 - val_loss: 2.2343e-04 - 10s/epoch - 122us/sample
Epoch 87/90
84077/84077 - 10s - loss: 2.3226e-04 - val_loss: 2.1554e-04 - 10s/epoch - 122us/sample
Epoch 88/90
84077/84077 - 10s - loss: 2.3677e-04 - val_loss: 2.2137e-04 - 10s/epoch - 122us/sample
Epoch 89/90
84077/84077 - 10s - loss: 2.3082e-04 - val_loss: 2.1277e-04 - 10s/epoch - 123us/sample
Epoch 90/90
84077/84077 - 10s - loss: 2.3345e-04 - val_loss: 2.1450e-04 - 10s/epoch - 122us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00021449564168935718
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 04:12:13.636913: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4202 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030607379691979505
cosine 0.030235215840788802
MAE: 0.002207073597278724
RMSE: 0.0104408243077193
r2: 0.9149111734991274
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 90, 0.001, 0.2, 188, 0.00023345179849439, 0.00021449564168935718, 0.030607379691979505, 0.030235215840788802, 0.002207073597278724, 0.0104408243077193, 0.9149111734991274, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 10 0.0005 16 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 471)          444624      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 471)         1884        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 471)          0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          88736       ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          88736       ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          572283      ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,196,263
Trainable params: 1,194,003
Non-trainable params: 2,260
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/10
2023-02-11 04:12:20.026209: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_4/bias/Assign' id:5275 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_4/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_4/bias, dense_dec0_4/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 04:12:36.083935: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5476 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0015 - val_loss: 5.4431e-04 - 17s/epoch - 208us/sample
Epoch 2/10
84077/84077 - 16s - loss: 5.0874e-04 - val_loss: 3.9856e-04 - 16s/epoch - 192us/sample
Epoch 3/10
84077/84077 - 16s - loss: 4.0125e-04 - val_loss: 3.5958e-04 - 16s/epoch - 192us/sample
Epoch 4/10
84077/84077 - 16s - loss: 3.5782e-04 - val_loss: 3.5275e-04 - 16s/epoch - 193us/sample
Epoch 5/10
84077/84077 - 16s - loss: 3.3131e-04 - val_loss: 3.3107e-04 - 16s/epoch - 192us/sample
Epoch 6/10
84077/84077 - 16s - loss: 3.1281e-04 - val_loss: 3.2007e-04 - 16s/epoch - 193us/sample
Epoch 7/10
84077/84077 - 16s - loss: 2.9951e-04 - val_loss: 3.0831e-04 - 16s/epoch - 192us/sample
Epoch 8/10
84077/84077 - 16s - loss: 2.8962e-04 - val_loss: 2.9787e-04 - 16s/epoch - 192us/sample
Epoch 9/10
84077/84077 - 16s - loss: 2.8067e-04 - val_loss: 2.8656e-04 - 16s/epoch - 192us/sample
Epoch 10/10
84077/84077 - 16s - loss: 2.7303e-04 - val_loss: 2.8449e-04 - 16s/epoch - 192us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00028448951266201746
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 04:15:02.566781: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:5440 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0971093374338592
cosine 0.09594302445896026
MAE: 0.004398380603191383
RMSE: 0.020812001123865012
r2: 0.6616633704919775
RMSE zero-vector: 0.04004287452915337
['0.5custom_VAE', 'logcosh', 16, 10, 0.0005, 0.2, 188, 0.0002730320777275023, 0.00028448951266201746, 0.0971093374338592, 0.09594302445896026, 0.004398380603191383, 0.020812001123865012, 0.6616633704919775, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 10 0.0005 32 0] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 1414)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          266020      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          266020      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1643531     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,516,043
Trainable params: 3,510,011
Non-trainable params: 6,032
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/10
2023-02-11 04:15:08.995524: W tensorflow/c/c_api.cc:291] Operation '{name:'training_10/Adam/bottleneck_zlog_5/bias/v/Assign' id:7430 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/bottleneck_zlog_5/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_10/Adam/bottleneck_zlog_5/bias/v, training_10/Adam/bottleneck_zlog_5/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 04:15:19.890702: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:6752 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 2.0486 - val_loss: 0.0808 - 12s/epoch - 145us/sample
Epoch 2/10
84077/84077 - 11s - loss: 0.0730 - val_loss: 0.0701 - 11s/epoch - 126us/sample
Epoch 3/10
84077/84077 - 11s - loss: 0.0682 - val_loss: 0.0680 - 11s/epoch - 126us/sample
Epoch 4/10
84077/84077 - 11s - loss: 0.0672 - val_loss: 0.0675 - 11s/epoch - 126us/sample
Epoch 5/10
84077/84077 - 11s - loss: 0.0670 - val_loss: 0.0674 - 11s/epoch - 127us/sample
Epoch 6/10
84077/84077 - 11s - loss: 0.0669 - val_loss: 0.0674 - 11s/epoch - 128us/sample
Epoch 7/10
84077/84077 - 11s - loss: 0.0669 - val_loss: 0.0674 - 11s/epoch - 128us/sample
Epoch 8/10
84077/84077 - 11s - loss: 0.0669 - val_loss: 0.0674 - 11s/epoch - 127us/sample
Epoch 9/10
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0674 - 11s/epoch - 127us/sample
Epoch 10/10
84077/84077 - 11s - loss: 0.0668 - val_loss: 0.0673 - 11s/epoch - 127us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734829691576258
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 04:16:56.733235: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:6704 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2230584316268713
cosine 1.1708157742023926
MAE: 4.323554055900023
RMSE: 4.774119561397557
r2: -16790.374375100262
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'binary_crossentropy', 32, 10, 0.0005, 0.2, 188, 0.06684421076788435, 0.06734829691576258, 1.2230584316268713, 1.1708157742023926, 4.323554055900023, 4.774119561397557, -16790.374375100262, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 170 0.0005 8 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 1886)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/170
2023-02-11 04:17:03.368468: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_6/bias/Assign' id:7957 op device:{requested: '', assigned: ''} def:{{{node outputlayer_6/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_6/bias, outputlayer_6/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 04:17:34.540452: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:8073 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 34s - loss: 0.0048 - val_loss: 0.0015 - 34s/epoch - 400us/sample
Epoch 2/170
84077/84077 - 32s - loss: 0.0014 - val_loss: 0.0012 - 32s/epoch - 376us/sample
Epoch 3/170
84077/84077 - 31s - loss: 0.0011 - val_loss: 0.0028 - 31s/epoch - 374us/sample
Epoch 4/170
84077/84077 - 31s - loss: 9.6083e-04 - val_loss: 0.0080 - 31s/epoch - 374us/sample
Epoch 5/170
84077/84077 - 32s - loss: 9.0888e-04 - val_loss: 0.0425 - 32s/epoch - 378us/sample
Epoch 6/170
84077/84077 - 32s - loss: 8.7124e-04 - val_loss: 0.0677 - 32s/epoch - 378us/sample
Epoch 7/170
84077/84077 - 32s - loss: 8.4995e-04 - val_loss: 0.1059 - 32s/epoch - 376us/sample
Epoch 8/170
84077/84077 - 32s - loss: 8.3124e-04 - val_loss: 0.1606 - 32s/epoch - 377us/sample
Epoch 9/170
84077/84077 - 32s - loss: 8.1121e-04 - val_loss: 0.1940 - 32s/epoch - 376us/sample
Epoch 10/170
84077/84077 - 31s - loss: 7.9921e-04 - val_loss: 0.2925 - 31s/epoch - 371us/sample
Epoch 11/170
84077/84077 - 32s - loss: 7.8615e-04 - val_loss: 0.2379 - 32s/epoch - 375us/sample
Epoch 12/170
84077/84077 - 32s - loss: 7.7735e-04 - val_loss: 0.3179 - 32s/epoch - 377us/sample
Epoch 13/170
84077/84077 - 32s - loss: 7.7020e-04 - val_loss: 0.3690 - 32s/epoch - 377us/sample
Epoch 14/170
84077/84077 - 32s - loss: 7.6161e-04 - val_loss: 0.4328 - 32s/epoch - 375us/sample
Epoch 15/170
84077/84077 - 31s - loss: 7.5688e-04 - val_loss: 0.3641 - 31s/epoch - 373us/sample
Epoch 16/170
84077/84077 - 32s - loss: 7.5148e-04 - val_loss: 0.4895 - 32s/epoch - 376us/sample
Epoch 17/170
84077/84077 - 32s - loss: 7.4740e-04 - val_loss: 0.3830 - 32s/epoch - 376us/sample
Epoch 18/170
84077/84077 - 32s - loss: 7.4109e-04 - val_loss: 0.2985 - 32s/epoch - 377us/sample
Epoch 19/170
84077/84077 - 32s - loss: 7.3975e-04 - val_loss: 0.3726 - 32s/epoch - 377us/sample
Epoch 20/170
84077/84077 - 31s - loss: 7.3718e-04 - val_loss: 0.3268 - 31s/epoch - 374us/sample
Epoch 21/170
84077/84077 - 31s - loss: 7.3366e-04 - val_loss: 0.2537 - 31s/epoch - 372us/sample
Epoch 22/170
84077/84077 - 32s - loss: 7.3180e-04 - val_loss: 0.2976 - 32s/epoch - 375us/sample
Epoch 23/170
84077/84077 - 32s - loss: 7.2819e-04 - val_loss: 0.2830 - 32s/epoch - 376us/sample
Epoch 24/170
84077/84077 - 32s - loss: 7.2576e-04 - val_loss: 0.2413 - 32s/epoch - 379us/sample
Epoch 25/170
84077/84077 - 32s - loss: 7.2457e-04 - val_loss: 0.2903 - 32s/epoch - 376us/sample
Epoch 26/170
84077/84077 - 32s - loss: 7.2121e-04 - val_loss: 0.2955 - 32s/epoch - 377us/sample
Epoch 27/170
84077/84077 - 31s - loss: 7.2048e-04 - val_loss: 0.2296 - 31s/epoch - 373us/sample
Epoch 28/170
84077/84077 - 31s - loss: 7.1893e-04 - val_loss: 0.2727 - 31s/epoch - 373us/sample
Epoch 29/170
84077/84077 - 32s - loss: 7.1639e-04 - val_loss: 0.2242 - 32s/epoch - 377us/sample
Epoch 30/170
84077/84077 - 32s - loss: 7.1397e-04 - val_loss: 0.2083 - 32s/epoch - 376us/sample
Epoch 31/170
84077/84077 - 32s - loss: 7.1267e-04 - val_loss: 0.2387 - 32s/epoch - 376us/sample
Epoch 32/170
84077/84077 - 32s - loss: 7.1188e-04 - val_loss: 0.3058 - 32s/epoch - 375us/sample
Epoch 33/170
84077/84077 - 31s - loss: 7.1016e-04 - val_loss: 0.2845 - 31s/epoch - 374us/sample
Epoch 34/170
84077/84077 - 31s - loss: 7.0783e-04 - val_loss: 0.3528 - 31s/epoch - 370us/sample
Epoch 35/170
84077/84077 - 31s - loss: 7.0755e-04 - val_loss: 0.2635 - 31s/epoch - 373us/sample
Epoch 36/170
84077/84077 - 32s - loss: 7.0503e-04 - val_loss: 0.2420 - 32s/epoch - 377us/sample
Epoch 37/170
84077/84077 - 32s - loss: 7.0576e-04 - val_loss: 0.1985 - 32s/epoch - 377us/sample
Epoch 38/170
84077/84077 - 32s - loss: 7.0444e-04 - val_loss: 0.2345 - 32s/epoch - 376us/sample
Epoch 39/170
84077/84077 - 32s - loss: 7.0268e-04 - val_loss: 0.2494 - 32s/epoch - 376us/sample
Epoch 40/170
84077/84077 - 31s - loss: 7.0232e-04 - val_loss: 0.2334 - 31s/epoch - 371us/sample
Epoch 41/170
84077/84077 - 31s - loss: 7.0040e-04 - val_loss: 0.1948 - 31s/epoch - 373us/sample
Epoch 42/170
84077/84077 - 32s - loss: 7.0027e-04 - val_loss: 0.2312 - 32s/epoch - 379us/sample
Epoch 43/170
84077/84077 - 32s - loss: 6.9984e-04 - val_loss: 0.2219 - 32s/epoch - 377us/sample
Epoch 44/170
84077/84077 - 32s - loss: 6.9882e-04 - val_loss: 0.2394 - 32s/epoch - 375us/sample
Epoch 45/170
84077/84077 - 32s - loss: 6.9674e-04 - val_loss: 0.2348 - 32s/epoch - 375us/sample
Epoch 46/170
84077/84077 - 32s - loss: 6.9897e-04 - val_loss: 0.2154 - 32s/epoch - 375us/sample
Epoch 47/170
84077/84077 - 31s - loss: 6.9630e-04 - val_loss: 0.1758 - 31s/epoch - 370us/sample
Epoch 48/170
84077/84077 - 32s - loss: 6.9517e-04 - val_loss: 0.1649 - 32s/epoch - 375us/sample
Epoch 49/170
84077/84077 - 32s - loss: 6.9441e-04 - val_loss: 0.1792 - 32s/epoch - 377us/sample
Epoch 50/170
84077/84077 - 31s - loss: 6.9516e-04 - val_loss: 0.1950 - 31s/epoch - 374us/sample
Epoch 51/170
84077/84077 - 32s - loss: 6.9477e-04 - val_loss: 0.2120 - 32s/epoch - 375us/sample
Epoch 52/170
84077/84077 - 32s - loss: 6.9427e-04 - val_loss: 0.1865 - 32s/epoch - 376us/sample
Epoch 53/170
84077/84077 - 31s - loss: 6.9361e-04 - val_loss: 0.2276 - 31s/epoch - 371us/sample
Epoch 54/170
84077/84077 - 32s - loss: 6.9250e-04 - val_loss: 0.1969 - 32s/epoch - 375us/sample
Epoch 55/170
84077/84077 - 32s - loss: 6.9119e-04 - val_loss: 0.2294 - 32s/epoch - 377us/sample
Epoch 56/170
84077/84077 - 32s - loss: 6.9281e-04 - val_loss: 0.2195 - 32s/epoch - 375us/sample
Epoch 57/170
84077/84077 - 32s - loss: 6.9087e-04 - val_loss: 0.1679 - 32s/epoch - 376us/sample
Epoch 58/170
84077/84077 - 32s - loss: 6.9051e-04 - val_loss: 0.2121 - 32s/epoch - 376us/sample
Epoch 59/170
84077/84077 - 32s - loss: 6.8944e-04 - val_loss: 0.1837 - 32s/epoch - 376us/sample
Epoch 60/170
84077/84077 - 32s - loss: 6.8940e-04 - val_loss: 0.1770 - 32s/epoch - 375us/sample
Epoch 61/170
84077/84077 - 32s - loss: 6.8918e-04 - val_loss: 0.1689 - 32s/epoch - 376us/sample
Epoch 62/170
84077/84077 - 32s - loss: 6.8937e-04 - val_loss: 0.1483 - 32s/epoch - 376us/sample
Epoch 63/170
84077/84077 - 32s - loss: 6.8877e-04 - val_loss: 0.1575 - 32s/epoch - 378us/sample
Epoch 64/170
84077/84077 - 32s - loss: 6.8738e-04 - val_loss: 0.1681 - 32s/epoch - 379us/sample
Epoch 65/170
84077/84077 - 31s - loss: 6.8683e-04 - val_loss: 0.1528 - 31s/epoch - 374us/sample
Epoch 66/170
84077/84077 - 32s - loss: 6.8731e-04 - val_loss: 0.1433 - 32s/epoch - 377us/sample
Epoch 67/170
84077/84077 - 32s - loss: 6.8548e-04 - val_loss: 0.1565 - 32s/epoch - 377us/sample
Epoch 68/170
84077/84077 - 31s - loss: 6.8605e-04 - val_loss: 0.1681 - 31s/epoch - 374us/sample
Epoch 69/170
84077/84077 - 32s - loss: 6.8548e-04 - val_loss: 0.1320 - 32s/epoch - 377us/sample
Epoch 70/170
84077/84077 - 32s - loss: 6.8442e-04 - val_loss: 0.1183 - 32s/epoch - 375us/sample
Epoch 71/170
84077/84077 - 32s - loss: 6.8440e-04 - val_loss: 0.1317 - 32s/epoch - 377us/sample
Epoch 72/170
84077/84077 - 32s - loss: 6.8328e-04 - val_loss: 0.1681 - 32s/epoch - 377us/sample
Epoch 73/170
84077/84077 - 32s - loss: 6.8503e-04 - val_loss: 0.1616 - 32s/epoch - 375us/sample
Epoch 74/170
84077/84077 - 31s - loss: 6.8299e-04 - val_loss: 0.1507 - 31s/epoch - 370us/sample
Epoch 75/170
84077/84077 - 31s - loss: 6.8116e-04 - val_loss: 0.1585 - 31s/epoch - 374us/sample
Epoch 76/170
84077/84077 - 32s - loss: 6.8252e-04 - val_loss: 0.1254 - 32s/epoch - 377us/sample
Epoch 77/170
84077/84077 - 32s - loss: 6.8144e-04 - val_loss: 0.1220 - 32s/epoch - 376us/sample
Epoch 78/170
84077/84077 - 32s - loss: 6.8162e-04 - val_loss: 0.1155 - 32s/epoch - 376us/sample
Epoch 79/170
84077/84077 - 31s - loss: 6.8102e-04 - val_loss: 0.1201 - 31s/epoch - 373us/sample
Epoch 80/170
84077/84077 - 32s - loss: 6.8089e-04 - val_loss: 0.1393 - 32s/epoch - 375us/sample
Epoch 81/170
84077/84077 - 31s - loss: 6.8055e-04 - val_loss: 0.1358 - 31s/epoch - 373us/sample
Epoch 82/170
84077/84077 - 32s - loss: 6.8136e-04 - val_loss: 0.1519 - 32s/epoch - 377us/sample
Epoch 83/170
84077/84077 - 32s - loss: 6.7947e-04 - val_loss: 0.1375 - 32s/epoch - 376us/sample
Epoch 84/170
84077/84077 - 32s - loss: 6.8023e-04 - val_loss: 0.1318 - 32s/epoch - 376us/sample
Epoch 85/170
84077/84077 - 31s - loss: 6.7994e-04 - val_loss: 0.1123 - 31s/epoch - 371us/sample
Epoch 86/170
84077/84077 - 31s - loss: 6.7941e-04 - val_loss: 0.1237 - 31s/epoch - 371us/sample
Epoch 87/170
84077/84077 - 32s - loss: 6.7841e-04 - val_loss: 0.1123 - 32s/epoch - 376us/sample
Epoch 88/170
84077/84077 - 32s - loss: 6.7804e-04 - val_loss: 0.1172 - 32s/epoch - 376us/sample
Epoch 89/170
84077/84077 - 31s - loss: 6.7893e-04 - val_loss: 0.1307 - 31s/epoch - 374us/sample
Epoch 90/170
84077/84077 - 32s - loss: 6.7843e-04 - val_loss: 0.1046 - 32s/epoch - 376us/sample
Epoch 91/170
84077/84077 - 31s - loss: 6.7709e-04 - val_loss: 0.1003 - 31s/epoch - 373us/sample
Epoch 92/170
84077/84077 - 31s - loss: 6.7727e-04 - val_loss: 0.1140 - 31s/epoch - 371us/sample
Epoch 93/170
84077/84077 - 32s - loss: 6.7708e-04 - val_loss: 0.1179 - 32s/epoch - 376us/sample
Epoch 94/170
84077/84077 - 32s - loss: 6.7652e-04 - val_loss: 0.0948 - 32s/epoch - 377us/sample
Epoch 95/170
84077/84077 - 32s - loss: 6.7637e-04 - val_loss: 0.1249 - 32s/epoch - 377us/sample
Epoch 96/170
84077/84077 - 31s - loss: 6.7594e-04 - val_loss: 0.1116 - 31s/epoch - 375us/sample
Epoch 97/170
84077/84077 - 31s - loss: 6.7508e-04 - val_loss: 0.0987 - 31s/epoch - 373us/sample
Epoch 98/170
84077/84077 - 31s - loss: 6.7525e-04 - val_loss: 0.0854 - 31s/epoch - 367us/sample
Epoch 99/170
84077/84077 - 31s - loss: 6.7444e-04 - val_loss: 0.1197 - 31s/epoch - 373us/sample
Epoch 100/170
84077/84077 - 32s - loss: 6.7510e-04 - val_loss: 0.0854 - 32s/epoch - 376us/sample
Epoch 101/170
84077/84077 - 32s - loss: 6.7335e-04 - val_loss: 0.1002 - 32s/epoch - 377us/sample
Epoch 102/170
84077/84077 - 32s - loss: 6.7438e-04 - val_loss: 0.1096 - 32s/epoch - 376us/sample
Epoch 103/170
84077/84077 - 32s - loss: 6.7314e-04 - val_loss: 0.0954 - 32s/epoch - 376us/sample
Epoch 104/170
84077/84077 - 31s - loss: 6.7171e-04 - val_loss: 0.0913 - 31s/epoch - 373us/sample
Epoch 105/170
84077/84077 - 31s - loss: 6.7169e-04 - val_loss: 0.1118 - 31s/epoch - 373us/sample
Epoch 106/170
84077/84077 - 32s - loss: 6.7255e-04 - val_loss: 0.1074 - 32s/epoch - 377us/sample
Epoch 107/170
84077/84077 - 32s - loss: 6.7126e-04 - val_loss: 0.1065 - 32s/epoch - 376us/sample
Epoch 108/170
84077/84077 - 32s - loss: 6.7102e-04 - val_loss: 0.0943 - 32s/epoch - 377us/sample
Epoch 109/170
84077/84077 - 32s - loss: 6.7145e-04 - val_loss: 0.1180 - 32s/epoch - 376us/sample
Epoch 110/170
84077/84077 - 32s - loss: 6.7111e-04 - val_loss: 0.0918 - 32s/epoch - 375us/sample
Epoch 111/170
84077/84077 - 31s - loss: 6.7112e-04 - val_loss: 0.1098 - 31s/epoch - 370us/sample
Epoch 112/170
84077/84077 - 31s - loss: 6.6939e-04 - val_loss: 0.0892 - 31s/epoch - 373us/sample
Epoch 113/170
84077/84077 - 32s - loss: 6.6974e-04 - val_loss: 0.0823 - 32s/epoch - 378us/sample
Epoch 114/170
84077/84077 - 32s - loss: 6.6942e-04 - val_loss: 0.1000 - 32s/epoch - 377us/sample
Epoch 115/170
84077/84077 - 32s - loss: 6.6972e-04 - val_loss: 0.1023 - 32s/epoch - 378us/sample
Epoch 116/170
84077/84077 - 32s - loss: 6.6857e-04 - val_loss: 0.0922 - 32s/epoch - 376us/sample
Epoch 117/170
84077/84077 - 31s - loss: 6.6969e-04 - val_loss: 0.0906 - 31s/epoch - 370us/sample
Epoch 118/170
84077/84077 - 31s - loss: 6.6893e-04 - val_loss: 0.0726 - 31s/epoch - 365us/sample
Epoch 119/170
84077/84077 - 32s - loss: 6.6660e-04 - val_loss: 0.0945 - 32s/epoch - 376us/sample
Epoch 120/170
84077/84077 - 32s - loss: 6.6804e-04 - val_loss: 0.0880 - 32s/epoch - 377us/sample
Epoch 121/170
84077/84077 - 32s - loss: 6.6881e-04 - val_loss: 0.0834 - 32s/epoch - 376us/sample
Epoch 122/170
84077/84077 - 32s - loss: 6.6785e-04 - val_loss: 0.0879 - 32s/epoch - 377us/sample
Epoch 123/170
84077/84077 - 31s - loss: 6.6694e-04 - val_loss: 0.0847 - 31s/epoch - 374us/sample
Epoch 124/170
84077/84077 - 31s - loss: 6.6719e-04 - val_loss: 0.0810 - 31s/epoch - 370us/sample
Epoch 125/170
84077/84077 - 32s - loss: 6.6753e-04 - val_loss: 0.0870 - 32s/epoch - 376us/sample
Epoch 126/170
84077/84077 - 32s - loss: 6.6620e-04 - val_loss: 0.0842 - 32s/epoch - 379us/sample
Epoch 127/170
84077/84077 - 32s - loss: 6.6756e-04 - val_loss: 0.0797 - 32s/epoch - 376us/sample
Epoch 128/170
84077/84077 - 32s - loss: 6.6728e-04 - val_loss: 0.0724 - 32s/epoch - 376us/sample
Epoch 129/170
84077/84077 - 31s - loss: 6.6642e-04 - val_loss: 0.1007 - 31s/epoch - 374us/sample
Epoch 130/170
84077/84077 - 32s - loss: 6.6604e-04 - val_loss: 0.0663 - 32s/epoch - 376us/sample
Epoch 131/170
84077/84077 - 32s - loss: 6.6776e-04 - val_loss: 0.0775 - 32s/epoch - 379us/sample
Epoch 132/170
84077/84077 - 32s - loss: 6.6557e-04 - val_loss: 0.0938 - 32s/epoch - 376us/sample
Epoch 133/170
84077/84077 - 32s - loss: 6.6623e-04 - val_loss: 0.0834 - 32s/epoch - 375us/sample
Epoch 134/170
84077/84077 - 32s - loss: 6.6610e-04 - val_loss: 0.0711 - 32s/epoch - 376us/sample
Epoch 135/170
84077/84077 - 32s - loss: 6.6502e-04 - val_loss: 0.0670 - 32s/epoch - 377us/sample
Epoch 136/170
84077/84077 - 32s - loss: 6.6585e-04 - val_loss: 0.0846 - 32s/epoch - 376us/sample
Epoch 137/170
84077/84077 - 32s - loss: 6.6555e-04 - val_loss: 0.0713 - 32s/epoch - 377us/sample
Epoch 138/170
84077/84077 - 32s - loss: 6.6488e-04 - val_loss: 0.0775 - 32s/epoch - 376us/sample
Epoch 139/170
84077/84077 - 32s - loss: 6.6684e-04 - val_loss: 0.0726 - 32s/epoch - 375us/sample
Epoch 140/170
84077/84077 - 32s - loss: 6.6580e-04 - val_loss: 0.0682 - 32s/epoch - 375us/sample
Epoch 141/170
84077/84077 - 32s - loss: 6.6475e-04 - val_loss: 0.0710 - 32s/epoch - 377us/sample
Epoch 142/170
84077/84077 - 32s - loss: 6.6471e-04 - val_loss: 0.0908 - 32s/epoch - 378us/sample
Epoch 143/170
84077/84077 - 32s - loss: 6.6492e-04 - val_loss: 0.0732 - 32s/epoch - 379us/sample
Epoch 144/170
84077/84077 - 32s - loss: 6.6521e-04 - val_loss: 0.0719 - 32s/epoch - 378us/sample
Epoch 145/170
84077/84077 - 32s - loss: 6.6634e-04 - val_loss: 0.0792 - 32s/epoch - 379us/sample
Epoch 146/170
84077/84077 - 32s - loss: 6.6549e-04 - val_loss: 0.0759 - 32s/epoch - 379us/sample
Epoch 147/170
84077/84077 - 32s - loss: 6.6513e-04 - val_loss: 0.0717 - 32s/epoch - 377us/sample
Epoch 148/170
84077/84077 - 32s - loss: 6.6490e-04 - val_loss: 0.0776 - 32s/epoch - 378us/sample
Epoch 149/170
84077/84077 - 32s - loss: 6.6361e-04 - val_loss: 0.0763 - 32s/epoch - 379us/sample
Epoch 150/170
84077/84077 - 32s - loss: 6.6313e-04 - val_loss: 0.0723 - 32s/epoch - 377us/sample
Epoch 151/170
84077/84077 - 32s - loss: 6.6383e-04 - val_loss: 0.0699 - 32s/epoch - 379us/sample
Epoch 152/170
84077/84077 - 32s - loss: 6.6429e-04 - val_loss: 0.0714 - 32s/epoch - 377us/sample
Epoch 153/170
84077/84077 - 32s - loss: 6.6372e-04 - val_loss: 0.0670 - 32s/epoch - 377us/sample
Epoch 154/170
84077/84077 - 32s - loss: 6.6378e-04 - val_loss: 0.0528 - 32s/epoch - 378us/sample
Epoch 155/170
84077/84077 - 32s - loss: 6.6413e-04 - val_loss: 0.0640 - 32s/epoch - 379us/sample
Epoch 156/170
84077/84077 - 32s - loss: 6.6337e-04 - val_loss: 0.0673 - 32s/epoch - 378us/sample
Epoch 157/170
84077/84077 - 31s - loss: 6.6374e-04 - val_loss: 0.0600 - 31s/epoch - 373us/sample
Epoch 158/170
84077/84077 - 31s - loss: 6.6380e-04 - val_loss: 0.0626 - 31s/epoch - 374us/sample
Epoch 159/170
84077/84077 - 32s - loss: 6.6187e-04 - val_loss: 0.0597 - 32s/epoch - 379us/sample
Epoch 160/170
84077/84077 - 32s - loss: 6.6316e-04 - val_loss: 0.0609 - 32s/epoch - 378us/sample
Epoch 161/170
84077/84077 - 32s - loss: 6.6369e-04 - val_loss: 0.0704 - 32s/epoch - 379us/sample
Epoch 162/170
84077/84077 - 32s - loss: 6.6233e-04 - val_loss: 0.0702 - 32s/epoch - 377us/sample
Epoch 163/170
84077/84077 - 32s - loss: 6.6345e-04 - val_loss: 0.0712 - 32s/epoch - 378us/sample
Epoch 164/170
84077/84077 - 32s - loss: 6.6308e-04 - val_loss: 0.0614 - 32s/epoch - 379us/sample
Epoch 165/170
84077/84077 - 32s - loss: 6.6348e-04 - val_loss: 0.0620 - 32s/epoch - 380us/sample
Epoch 166/170
84077/84077 - 32s - loss: 6.6259e-04 - val_loss: 0.0708 - 32s/epoch - 376us/sample
Epoch 167/170
84077/84077 - 32s - loss: 6.6328e-04 - val_loss: 0.0638 - 32s/epoch - 376us/sample
Epoch 168/170
84077/84077 - 31s - loss: 6.6176e-04 - val_loss: 0.0674 - 31s/epoch - 370us/sample
Epoch 169/170
84077/84077 - 31s - loss: 6.6217e-04 - val_loss: 0.0679 - 31s/epoch - 371us/sample
Epoch 170/170
84077/84077 - 32s - loss: 6.6115e-04 - val_loss: 0.0558 - 32s/epoch - 379us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.05580671406828056
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 05:46:33.059383: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:8044 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.20156039627881916
cosine 0.19940241394860253
MAE: 0.02115240722267319
RMSE: 0.23723981478244197
r2: -42.96404427722522
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 8, 170, 0.0005, 0.2, 188, 0.0006611504526911487, 0.05580671406828056, 0.20156039627881916, 0.19940241394860253, 0.02115240722267319, 0.23723981478244197, -42.96404427722522, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 30 0.0005 32 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 471)          444624      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 471)         1884        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 471)          0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          88736       ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          88736       ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          572283      ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,196,263
Trainable params: 1,194,003
Non-trainable params: 2,260
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/30
2023-02-11 05:46:39.939772: W tensorflow/c/c_api.cc:291] Operation '{name:'training_14/Adam/batch_normalization_23/gamma/v/Assign' id:9971 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/batch_normalization_23/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/batch_normalization_23/gamma/v, training_14/Adam/batch_normalization_23/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 05:46:50.753518: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:9315 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0018 - val_loss: 6.4627e-04 - 12s/epoch - 147us/sample
Epoch 2/30
84077/84077 - 10s - loss: 5.7843e-04 - val_loss: 5.4271e-04 - 10s/epoch - 124us/sample
Epoch 3/30
84077/84077 - 10s - loss: 4.7893e-04 - val_loss: 4.0480e-04 - 10s/epoch - 123us/sample
Epoch 4/30
84077/84077 - 10s - loss: 4.1981e-04 - val_loss: 3.6340e-04 - 10s/epoch - 124us/sample
Epoch 5/30
84077/84077 - 10s - loss: 3.6584e-04 - val_loss: 3.0913e-04 - 10s/epoch - 124us/sample
Epoch 6/30
84077/84077 - 10s - loss: 3.2200e-04 - val_loss: 2.7639e-04 - 10s/epoch - 125us/sample
Epoch 7/30
84077/84077 - 10s - loss: 2.9431e-04 - val_loss: 2.5277e-04 - 10s/epoch - 124us/sample
Epoch 8/30
84077/84077 - 10s - loss: 2.7433e-04 - val_loss: 2.4307e-04 - 10s/epoch - 124us/sample
Epoch 9/30
84077/84077 - 10s - loss: 2.5986e-04 - val_loss: 2.3207e-04 - 10s/epoch - 124us/sample
Epoch 10/30
84077/84077 - 10s - loss: 2.5057e-04 - val_loss: 2.2028e-04 - 10s/epoch - 123us/sample
Epoch 11/30
84077/84077 - 10s - loss: 2.4119e-04 - val_loss: 2.1781e-04 - 10s/epoch - 125us/sample
Epoch 12/30
84077/84077 - 10s - loss: 2.3577e-04 - val_loss: 2.0830e-04 - 10s/epoch - 123us/sample
Epoch 13/30
84077/84077 - 10s - loss: 2.2883e-04 - val_loss: 2.0164e-04 - 10s/epoch - 124us/sample
Epoch 14/30
84077/84077 - 10s - loss: 2.2299e-04 - val_loss: 1.9715e-04 - 10s/epoch - 123us/sample
Epoch 15/30
84077/84077 - 10s - loss: 2.1762e-04 - val_loss: 1.9749e-04 - 10s/epoch - 125us/sample
Epoch 16/30
84077/84077 - 10s - loss: 2.1309e-04 - val_loss: 1.9860e-04 - 10s/epoch - 124us/sample
Epoch 17/30
84077/84077 - 10s - loss: 2.0941e-04 - val_loss: 1.8954e-04 - 10s/epoch - 124us/sample
Epoch 18/30
84077/84077 - 10s - loss: 2.0573e-04 - val_loss: 1.8615e-04 - 10s/epoch - 124us/sample
Epoch 19/30
84077/84077 - 10s - loss: 2.0306e-04 - val_loss: 1.8288e-04 - 10s/epoch - 124us/sample
Epoch 20/30
84077/84077 - 10s - loss: 1.9993e-04 - val_loss: 1.8412e-04 - 10s/epoch - 124us/sample
Epoch 21/30
84077/84077 - 10s - loss: 1.9732e-04 - val_loss: 1.8058e-04 - 10s/epoch - 124us/sample
Epoch 22/30
84077/84077 - 10s - loss: 1.9485e-04 - val_loss: 1.7901e-04 - 10s/epoch - 124us/sample
Epoch 23/30
84077/84077 - 10s - loss: 1.9193e-04 - val_loss: 1.7649e-04 - 10s/epoch - 124us/sample
Epoch 24/30
84077/84077 - 10s - loss: 1.9045e-04 - val_loss: 1.7812e-04 - 10s/epoch - 125us/sample
Epoch 25/30
84077/84077 - 10s - loss: 1.8887e-04 - val_loss: 1.7411e-04 - 10s/epoch - 124us/sample
Epoch 26/30
84077/84077 - 10s - loss: 1.8663e-04 - val_loss: 1.7236e-04 - 10s/epoch - 123us/sample
Epoch 27/30
84077/84077 - 10s - loss: 1.8525e-04 - val_loss: 1.7081e-04 - 10s/epoch - 122us/sample
Epoch 28/30
84077/84077 - 10s - loss: 1.8378e-04 - val_loss: 1.6996e-04 - 10s/epoch - 122us/sample
Epoch 29/30
84077/84077 - 10s - loss: 1.8248e-04 - val_loss: 1.6551e-04 - 10s/epoch - 121us/sample
Epoch 30/30
84077/84077 - 10s - loss: 1.8051e-04 - val_loss: 1.6799e-04 - 10s/epoch - 121us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001679920947924994
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 05:51:53.063676: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:9279 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04532368326668244
cosine 0.04477646399611311
MAE: 0.0030793277516527697
RMSE: 0.01336025616777801
r2: 0.8606162506708372
RMSE zero-vector: 0.04004287452915337
['0.5custom_VAE', 'logcosh', 32, 30, 0.0005, 0.2, 188, 0.00018050760615099398, 0.0001679920947924994, 0.04532368326668244, 0.04477646399611311, 0.0030793277516527697, 0.01336025616777801, 0.8606162506708372, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 170 0.0005 16 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 1414)         0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          266020      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          266020      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1643531     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,516,043
Trainable params: 3,510,011
Non-trainable params: 6,032
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/170
2023-02-11 05:52:00.092889: W tensorflow/c/c_api.cc:291] Operation '{name:'training_16/Adam/batch_normalization_26/gamma/v/Assign' id:11254 op device:{requested: '', assigned: ''} def:{{{node training_16/Adam/batch_normalization_26/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_16/Adam/batch_normalization_26/gamma/v, training_16/Adam/batch_normalization_26/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 05:52:18.285268: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/mul' id:10576 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0056 - val_loss: 6.6591e-04 - 20s/epoch - 240us/sample
Epoch 2/170
84077/84077 - 18s - loss: 6.6245e-04 - val_loss: 4.7676e-04 - 18s/epoch - 212us/sample
Epoch 3/170
84077/84077 - 18s - loss: 5.2607e-04 - val_loss: 4.4094e-04 - 18s/epoch - 212us/sample
Epoch 4/170
84077/84077 - 18s - loss: 4.3070e-04 - val_loss: 3.9122e-04 - 18s/epoch - 212us/sample
Epoch 5/170
84077/84077 - 18s - loss: 3.9703e-04 - val_loss: 4.5088e-04 - 18s/epoch - 213us/sample
Epoch 6/170
84077/84077 - 18s - loss: 3.7595e-04 - val_loss: 5.0725e-04 - 18s/epoch - 212us/sample
Epoch 7/170
84077/84077 - 18s - loss: 3.6268e-04 - val_loss: 6.3490e-04 - 18s/epoch - 213us/sample
Epoch 8/170
84077/84077 - 18s - loss: 3.5260e-04 - val_loss: 5.8395e-04 - 18s/epoch - 212us/sample
Epoch 9/170
84077/84077 - 18s - loss: 3.4534e-04 - val_loss: 5.7454e-04 - 18s/epoch - 209us/sample
Epoch 10/170
84077/84077 - 18s - loss: 3.4031e-04 - val_loss: 6.5451e-04 - 18s/epoch - 210us/sample
Epoch 11/170
84077/84077 - 18s - loss: 3.3598e-04 - val_loss: 6.8882e-04 - 18s/epoch - 209us/sample
Epoch 12/170
84077/84077 - 18s - loss: 3.3200e-04 - val_loss: 6.4101e-04 - 18s/epoch - 212us/sample
Epoch 13/170
84077/84077 - 18s - loss: 3.2822e-04 - val_loss: 5.9102e-04 - 18s/epoch - 214us/sample
Epoch 14/170
84077/84077 - 18s - loss: 3.2502e-04 - val_loss: 6.5214e-04 - 18s/epoch - 212us/sample
Epoch 15/170
84077/84077 - 18s - loss: 3.2267e-04 - val_loss: 5.5992e-04 - 18s/epoch - 213us/sample
Epoch 16/170
84077/84077 - 18s - loss: 3.2014e-04 - val_loss: 5.6567e-04 - 18s/epoch - 213us/sample
Epoch 17/170
84077/84077 - 18s - loss: 3.1681e-04 - val_loss: 5.7389e-04 - 18s/epoch - 211us/sample
Epoch 18/170
84077/84077 - 18s - loss: 3.1424e-04 - val_loss: 5.2526e-04 - 18s/epoch - 212us/sample
Epoch 19/170
84077/84077 - 18s - loss: 3.1246e-04 - val_loss: 5.4437e-04 - 18s/epoch - 211us/sample
Epoch 20/170
84077/84077 - 18s - loss: 3.0951e-04 - val_loss: 5.1793e-04 - 18s/epoch - 211us/sample
Epoch 21/170
84077/84077 - 18s - loss: 3.0807e-04 - val_loss: 5.2370e-04 - 18s/epoch - 211us/sample
Epoch 22/170
84077/84077 - 18s - loss: 3.0588e-04 - val_loss: 4.7142e-04 - 18s/epoch - 212us/sample
Epoch 23/170
84077/84077 - 18s - loss: 3.0411e-04 - val_loss: 4.5975e-04 - 18s/epoch - 212us/sample
Epoch 24/170
84077/84077 - 18s - loss: 3.0235e-04 - val_loss: 4.6846e-04 - 18s/epoch - 212us/sample
Epoch 25/170
84077/84077 - 18s - loss: 3.0157e-04 - val_loss: 4.5664e-04 - 18s/epoch - 212us/sample
Epoch 26/170
84077/84077 - 18s - loss: 3.0061e-04 - val_loss: 4.5515e-04 - 18s/epoch - 212us/sample
Epoch 27/170
84077/84077 - 18s - loss: 2.9922e-04 - val_loss: 4.3575e-04 - 18s/epoch - 213us/sample
Epoch 28/170
84077/84077 - 18s - loss: 2.9856e-04 - val_loss: 4.3174e-04 - 18s/epoch - 212us/sample
Epoch 29/170
84077/84077 - 18s - loss: 2.9714e-04 - val_loss: 4.1107e-04 - 18s/epoch - 210us/sample
Epoch 30/170
84077/84077 - 17s - loss: 2.9625e-04 - val_loss: 4.2334e-04 - 17s/epoch - 206us/sample
Epoch 31/170
84077/84077 - 17s - loss: 2.9542e-04 - val_loss: 3.9305e-04 - 17s/epoch - 207us/sample
Epoch 32/170
84077/84077 - 18s - loss: 2.9412e-04 - val_loss: 3.8331e-04 - 18s/epoch - 212us/sample
Epoch 33/170
84077/84077 - 18s - loss: 2.9336e-04 - val_loss: 3.8473e-04 - 18s/epoch - 212us/sample
Epoch 34/170
84077/84077 - 18s - loss: 2.9234e-04 - val_loss: 3.5936e-04 - 18s/epoch - 213us/sample
Epoch 35/170
84077/84077 - 18s - loss: 2.9217e-04 - val_loss: 3.6506e-04 - 18s/epoch - 212us/sample
Epoch 36/170
84077/84077 - 18s - loss: 2.9093e-04 - val_loss: 3.4820e-04 - 18s/epoch - 212us/sample
Epoch 37/170
84077/84077 - 18s - loss: 2.8972e-04 - val_loss: 3.3869e-04 - 18s/epoch - 212us/sample
Epoch 38/170
84077/84077 - 18s - loss: 2.8951e-04 - val_loss: 3.4738e-04 - 18s/epoch - 212us/sample
Epoch 39/170
84077/84077 - 18s - loss: 2.8888e-04 - val_loss: 3.3327e-04 - 18s/epoch - 214us/sample
Epoch 40/170
84077/84077 - 18s - loss: 2.8825e-04 - val_loss: 3.2938e-04 - 18s/epoch - 211us/sample
Epoch 41/170
84077/84077 - 18s - loss: 2.8807e-04 - val_loss: 3.2948e-04 - 18s/epoch - 211us/sample
Epoch 42/170
84077/84077 - 18s - loss: 2.8737e-04 - val_loss: 3.3639e-04 - 18s/epoch - 211us/sample
Epoch 43/170
84077/84077 - 18s - loss: 2.8671e-04 - val_loss: 3.3003e-04 - 18s/epoch - 210us/sample
Epoch 44/170
84077/84077 - 18s - loss: 2.8621e-04 - val_loss: 3.2952e-04 - 18s/epoch - 211us/sample
Epoch 45/170
84077/84077 - 18s - loss: 2.8648e-04 - val_loss: 3.2029e-04 - 18s/epoch - 212us/sample
Epoch 46/170
84077/84077 - 18s - loss: 2.8541e-04 - val_loss: 3.1530e-04 - 18s/epoch - 212us/sample
Epoch 47/170
84077/84077 - 18s - loss: 2.8534e-04 - val_loss: 3.1991e-04 - 18s/epoch - 213us/sample
Epoch 48/170
84077/84077 - 18s - loss: 2.8468e-04 - val_loss: 3.1540e-04 - 18s/epoch - 212us/sample
Epoch 49/170
84077/84077 - 18s - loss: 2.8411e-04 - val_loss: 3.2602e-04 - 18s/epoch - 209us/sample
Epoch 50/170
84077/84077 - 18s - loss: 2.8409e-04 - val_loss: 3.0760e-04 - 18s/epoch - 209us/sample
Epoch 51/170
84077/84077 - 18s - loss: 2.8377e-04 - val_loss: 3.0470e-04 - 18s/epoch - 209us/sample
Epoch 52/170
84077/84077 - 18s - loss: 2.8385e-04 - val_loss: 3.0281e-04 - 18s/epoch - 211us/sample
Epoch 53/170
84077/84077 - 18s - loss: 2.8349e-04 - val_loss: 3.0408e-04 - 18s/epoch - 210us/sample
Epoch 54/170
84077/84077 - 18s - loss: 2.8277e-04 - val_loss: 2.9756e-04 - 18s/epoch - 211us/sample
Epoch 55/170
84077/84077 - 18s - loss: 2.8231e-04 - val_loss: 3.0005e-04 - 18s/epoch - 210us/sample
Epoch 56/170
84077/84077 - 18s - loss: 2.8202e-04 - val_loss: 3.0357e-04 - 18s/epoch - 213us/sample
Epoch 57/170
84077/84077 - 18s - loss: 2.8154e-04 - val_loss: 3.0524e-04 - 18s/epoch - 212us/sample
Epoch 58/170
84077/84077 - 18s - loss: 2.8113e-04 - val_loss: 2.9944e-04 - 18s/epoch - 213us/sample
Epoch 59/170
84077/84077 - 18s - loss: 2.8086e-04 - val_loss: 2.9773e-04 - 18s/epoch - 211us/sample
Epoch 60/170
84077/84077 - 18s - loss: 2.8031e-04 - val_loss: 2.9741e-04 - 18s/epoch - 211us/sample
Epoch 61/170
84077/84077 - 18s - loss: 2.8016e-04 - val_loss: 3.0209e-04 - 18s/epoch - 211us/sample
Epoch 62/170
84077/84077 - 18s - loss: 2.7952e-04 - val_loss: 2.9489e-04 - 18s/epoch - 213us/sample
Epoch 63/170
84077/84077 - 18s - loss: 2.7941e-04 - val_loss: 2.9110e-04 - 18s/epoch - 212us/sample
Epoch 64/170
84077/84077 - 18s - loss: 2.7887e-04 - val_loss: 3.0361e-04 - 18s/epoch - 212us/sample
Epoch 65/170
84077/84077 - 18s - loss: 2.7824e-04 - val_loss: 2.8740e-04 - 18s/epoch - 211us/sample
Epoch 66/170
84077/84077 - 18s - loss: 2.7830e-04 - val_loss: 2.8697e-04 - 18s/epoch - 211us/sample
Epoch 67/170
84077/84077 - 18s - loss: 2.7788e-04 - val_loss: 2.8867e-04 - 18s/epoch - 212us/sample
Epoch 68/170
84077/84077 - 18s - loss: 2.7749e-04 - val_loss: 2.9192e-04 - 18s/epoch - 212us/sample
Epoch 69/170
84077/84077 - 18s - loss: 2.7711e-04 - val_loss: 2.8613e-04 - 18s/epoch - 212us/sample
Epoch 70/170
84077/84077 - 18s - loss: 2.7706e-04 - val_loss: 2.8407e-04 - 18s/epoch - 212us/sample
Epoch 71/170
84077/84077 - 18s - loss: 2.7668e-04 - val_loss: 2.9039e-04 - 18s/epoch - 212us/sample
Epoch 72/170
84077/84077 - 18s - loss: 2.7677e-04 - val_loss: 2.8657e-04 - 18s/epoch - 211us/sample
Epoch 73/170
84077/84077 - 18s - loss: 2.7638e-04 - val_loss: 2.8775e-04 - 18s/epoch - 211us/sample
Epoch 74/170
84077/84077 - 18s - loss: 2.7607e-04 - val_loss: 2.7993e-04 - 18s/epoch - 211us/sample
Epoch 75/170
84077/84077 - 18s - loss: 2.7621e-04 - val_loss: 2.8639e-04 - 18s/epoch - 211us/sample
Epoch 76/170
84077/84077 - 18s - loss: 2.7594e-04 - val_loss: 2.8298e-04 - 18s/epoch - 212us/sample
Epoch 77/170
84077/84077 - 18s - loss: 2.7569e-04 - val_loss: 2.8598e-04 - 18s/epoch - 213us/sample
Epoch 78/170
84077/84077 - 18s - loss: 2.7572e-04 - val_loss: 2.8338e-04 - 18s/epoch - 212us/sample
Epoch 79/170
84077/84077 - 18s - loss: 2.7490e-04 - val_loss: 2.8006e-04 - 18s/epoch - 211us/sample
Epoch 80/170
84077/84077 - 18s - loss: 2.7523e-04 - val_loss: 2.8065e-04 - 18s/epoch - 213us/sample
Epoch 81/170
84077/84077 - 18s - loss: 2.7484e-04 - val_loss: 2.7929e-04 - 18s/epoch - 212us/sample
Epoch 82/170
84077/84077 - 18s - loss: 2.7430e-04 - val_loss: 2.8214e-04 - 18s/epoch - 213us/sample
Epoch 83/170
84077/84077 - 18s - loss: 2.7416e-04 - val_loss: 2.8261e-04 - 18s/epoch - 212us/sample
Epoch 84/170
84077/84077 - 18s - loss: 2.7429e-04 - val_loss: 2.8045e-04 - 18s/epoch - 210us/sample
Epoch 85/170
84077/84077 - 18s - loss: 2.7401e-04 - val_loss: 2.7775e-04 - 18s/epoch - 211us/sample
Epoch 86/170
84077/84077 - 17s - loss: 2.7391e-04 - val_loss: 2.7847e-04 - 17s/epoch - 207us/sample
Epoch 87/170
84077/84077 - 17s - loss: 2.7353e-04 - val_loss: 2.8075e-04 - 17s/epoch - 208us/sample
Epoch 88/170
84077/84077 - 17s - loss: 2.7291e-04 - val_loss: 2.8162e-04 - 17s/epoch - 208us/sample
Epoch 89/170
84077/84077 - 18s - loss: 2.7302e-04 - val_loss: 2.7460e-04 - 18s/epoch - 213us/sample
Epoch 90/170
84077/84077 - 18s - loss: 2.7290e-04 - val_loss: 2.7360e-04 - 18s/epoch - 212us/sample
Epoch 91/170
84077/84077 - 18s - loss: 2.7235e-04 - val_loss: 2.7410e-04 - 18s/epoch - 212us/sample
Epoch 92/170
84077/84077 - 18s - loss: 2.7158e-04 - val_loss: 2.7287e-04 - 18s/epoch - 212us/sample
Epoch 93/170
84077/84077 - 18s - loss: 2.7123e-04 - val_loss: 2.7540e-04 - 18s/epoch - 211us/sample
Epoch 94/170
84077/84077 - 18s - loss: 2.7041e-04 - val_loss: 2.7263e-04 - 18s/epoch - 213us/sample
Epoch 95/170
84077/84077 - 18s - loss: 2.7045e-04 - val_loss: 2.7609e-04 - 18s/epoch - 211us/sample
Epoch 96/170
84077/84077 - 18s - loss: 2.7017e-04 - val_loss: 2.7458e-04 - 18s/epoch - 210us/sample
Epoch 97/170
84077/84077 - 18s - loss: 2.7021e-04 - val_loss: 2.7061e-04 - 18s/epoch - 209us/sample
Epoch 98/170
84077/84077 - 18s - loss: 2.7004e-04 - val_loss: 2.6997e-04 - 18s/epoch - 208us/sample
Epoch 99/170
84077/84077 - 18s - loss: 2.6989e-04 - val_loss: 2.7271e-04 - 18s/epoch - 208us/sample
Epoch 100/170
84077/84077 - 18s - loss: 2.6913e-04 - val_loss: 2.7262e-04 - 18s/epoch - 211us/sample
Epoch 101/170
84077/84077 - 18s - loss: 2.6895e-04 - val_loss: 2.7325e-04 - 18s/epoch - 213us/sample
Epoch 102/170
84077/84077 - 18s - loss: 2.6892e-04 - val_loss: 2.7079e-04 - 18s/epoch - 212us/sample
Epoch 103/170
84077/84077 - 18s - loss: 2.6887e-04 - val_loss: 2.6223e-04 - 18s/epoch - 211us/sample
Epoch 104/170
84077/84077 - 18s - loss: 2.6823e-04 - val_loss: 2.6762e-04 - 18s/epoch - 212us/sample
Epoch 105/170
84077/84077 - 18s - loss: 2.6789e-04 - val_loss: 2.6557e-04 - 18s/epoch - 212us/sample
Epoch 106/170
84077/84077 - 18s - loss: 2.6779e-04 - val_loss: 2.6959e-04 - 18s/epoch - 212us/sample
Epoch 107/170
84077/84077 - 18s - loss: 2.6735e-04 - val_loss: 2.6808e-04 - 18s/epoch - 212us/sample
Epoch 108/170
84077/84077 - 18s - loss: 2.6747e-04 - val_loss: 2.6730e-04 - 18s/epoch - 212us/sample
Epoch 109/170
84077/84077 - 18s - loss: 2.6815e-04 - val_loss: 2.6891e-04 - 18s/epoch - 209us/sample
Epoch 110/170
84077/84077 - 17s - loss: 2.6763e-04 - val_loss: 2.6482e-04 - 17s/epoch - 208us/sample
Epoch 111/170
84077/84077 - 18s - loss: 2.6716e-04 - val_loss: 2.6900e-04 - 18s/epoch - 209us/sample
Epoch 112/170
84077/84077 - 18s - loss: 2.6686e-04 - val_loss: 2.6863e-04 - 18s/epoch - 212us/sample
Epoch 113/170
84077/84077 - 18s - loss: 2.6689e-04 - val_loss: 2.6665e-04 - 18s/epoch - 212us/sample
Epoch 114/170
84077/84077 - 18s - loss: 2.6680e-04 - val_loss: 2.6436e-04 - 18s/epoch - 213us/sample
Epoch 115/170
84077/84077 - 18s - loss: 2.6676e-04 - val_loss: 2.6304e-04 - 18s/epoch - 212us/sample
Epoch 116/170
84077/84077 - 18s - loss: 2.6667e-04 - val_loss: 2.6592e-04 - 18s/epoch - 212us/sample
Epoch 117/170
84077/84077 - 18s - loss: 2.6708e-04 - val_loss: 2.7037e-04 - 18s/epoch - 212us/sample
Epoch 118/170
84077/84077 - 18s - loss: 2.6608e-04 - val_loss: 2.6666e-04 - 18s/epoch - 212us/sample
Epoch 119/170
84077/84077 - 18s - loss: 2.6620e-04 - val_loss: 2.7047e-04 - 18s/epoch - 211us/sample
Epoch 120/170
84077/84077 - 18s - loss: 2.6617e-04 - val_loss: 2.6504e-04 - 18s/epoch - 211us/sample
Epoch 121/170
84077/84077 - 17s - loss: 2.6571e-04 - val_loss: 2.6170e-04 - 17s/epoch - 208us/sample
Epoch 122/170
84077/84077 - 17s - loss: 2.6599e-04 - val_loss: 2.6501e-04 - 17s/epoch - 208us/sample
Epoch 123/170
84077/84077 - 18s - loss: 2.6580e-04 - val_loss: 2.6514e-04 - 18s/epoch - 213us/sample
Epoch 124/170
84077/84077 - 18s - loss: 2.6521e-04 - val_loss: 2.6023e-04 - 18s/epoch - 212us/sample
Epoch 125/170
84077/84077 - 18s - loss: 2.6576e-04 - val_loss: 2.6082e-04 - 18s/epoch - 213us/sample
Epoch 126/170
84077/84077 - 18s - loss: 2.6587e-04 - val_loss: 2.6316e-04 - 18s/epoch - 211us/sample
Epoch 127/170
84077/84077 - 18s - loss: 2.6548e-04 - val_loss: 2.6271e-04 - 18s/epoch - 212us/sample
Epoch 128/170
84077/84077 - 18s - loss: 2.6537e-04 - val_loss: 2.5886e-04 - 18s/epoch - 212us/sample
Epoch 129/170
84077/84077 - 18s - loss: 2.6532e-04 - val_loss: 2.6301e-04 - 18s/epoch - 212us/sample
Epoch 130/170
84077/84077 - 18s - loss: 2.6578e-04 - val_loss: 2.6179e-04 - 18s/epoch - 212us/sample
Epoch 131/170
84077/84077 - 18s - loss: 2.6460e-04 - val_loss: 2.5863e-04 - 18s/epoch - 212us/sample
Epoch 132/170
84077/84077 - 18s - loss: 2.6503e-04 - val_loss: 2.6203e-04 - 18s/epoch - 208us/sample
Epoch 133/170
84077/84077 - 17s - loss: 2.6508e-04 - val_loss: 2.5909e-04 - 17s/epoch - 205us/sample
Epoch 134/170
84077/84077 - 18s - loss: 2.6506e-04 - val_loss: 2.5848e-04 - 18s/epoch - 208us/sample
Epoch 135/170
84077/84077 - 18s - loss: 2.6460e-04 - val_loss: 2.5927e-04 - 18s/epoch - 211us/sample
Epoch 136/170
84077/84077 - 18s - loss: 2.6432e-04 - val_loss: 2.6073e-04 - 18s/epoch - 211us/sample
Epoch 137/170
84077/84077 - 18s - loss: 2.6435e-04 - val_loss: 2.6153e-04 - 18s/epoch - 211us/sample
Epoch 138/170
84077/84077 - 18s - loss: 2.6427e-04 - val_loss: 2.5898e-04 - 18s/epoch - 213us/sample
Epoch 139/170
84077/84077 - 18s - loss: 2.6416e-04 - val_loss: 2.5948e-04 - 18s/epoch - 212us/sample
Epoch 140/170
84077/84077 - 18s - loss: 2.6395e-04 - val_loss: 2.5811e-04 - 18s/epoch - 213us/sample
Epoch 141/170
84077/84077 - 18s - loss: 2.6331e-04 - val_loss: 2.5789e-04 - 18s/epoch - 212us/sample
Epoch 142/170
84077/84077 - 18s - loss: 2.6416e-04 - val_loss: 2.5857e-04 - 18s/epoch - 212us/sample
Epoch 143/170
84077/84077 - 18s - loss: 2.6383e-04 - val_loss: 2.5631e-04 - 18s/epoch - 211us/sample
Epoch 144/170
84077/84077 - 17s - loss: 2.6400e-04 - val_loss: 2.5900e-04 - 17s/epoch - 208us/sample
Epoch 145/170
84077/84077 - 17s - loss: 2.6379e-04 - val_loss: 2.6175e-04 - 17s/epoch - 208us/sample
Epoch 146/170
84077/84077 - 18s - loss: 2.6346e-04 - val_loss: 2.5526e-04 - 18s/epoch - 213us/sample
Epoch 147/170
84077/84077 - 18s - loss: 2.6361e-04 - val_loss: 2.5631e-04 - 18s/epoch - 212us/sample
Epoch 148/170
84077/84077 - 18s - loss: 2.6336e-04 - val_loss: 2.5570e-04 - 18s/epoch - 213us/sample
Epoch 149/170
84077/84077 - 18s - loss: 2.6246e-04 - val_loss: 2.5751e-04 - 18s/epoch - 212us/sample
Epoch 150/170
84077/84077 - 18s - loss: 2.6249e-04 - val_loss: 2.5697e-04 - 18s/epoch - 212us/sample
Epoch 151/170
84077/84077 - 18s - loss: 2.6288e-04 - val_loss: 2.5797e-04 - 18s/epoch - 211us/sample
Epoch 152/170
84077/84077 - 18s - loss: 2.6292e-04 - val_loss: 2.5676e-04 - 18s/epoch - 211us/sample
Epoch 153/170
84077/84077 - 18s - loss: 2.6260e-04 - val_loss: 2.5893e-04 - 18s/epoch - 210us/sample
Epoch 154/170
84077/84077 - 17s - loss: 2.6270e-04 - val_loss: 2.5696e-04 - 17s/epoch - 208us/sample
Epoch 155/170
84077/84077 - 17s - loss: 2.6248e-04 - val_loss: 2.5411e-04 - 17s/epoch - 207us/sample
Epoch 156/170
84077/84077 - 17s - loss: 2.6305e-04 - val_loss: 2.5446e-04 - 17s/epoch - 208us/sample
Epoch 157/170
84077/84077 - 18s - loss: 2.6220e-04 - val_loss: 2.5354e-04 - 18s/epoch - 212us/sample
Epoch 158/170
84077/84077 - 18s - loss: 2.6229e-04 - val_loss: 2.6142e-04 - 18s/epoch - 212us/sample
Epoch 159/170
84077/84077 - 18s - loss: 2.6221e-04 - val_loss: 2.5488e-04 - 18s/epoch - 211us/sample
Epoch 160/170
84077/84077 - 18s - loss: 2.6238e-04 - val_loss: 2.5685e-04 - 18s/epoch - 213us/sample
Epoch 161/170
84077/84077 - 18s - loss: 2.6186e-04 - val_loss: 2.5431e-04 - 18s/epoch - 212us/sample
Epoch 162/170
84077/84077 - 18s - loss: 2.6209e-04 - val_loss: 2.5243e-04 - 18s/epoch - 211us/sample
Epoch 163/170
84077/84077 - 18s - loss: 2.6180e-04 - val_loss: 2.5386e-04 - 18s/epoch - 213us/sample
Epoch 164/170
84077/84077 - 18s - loss: 2.6206e-04 - val_loss: 2.5360e-04 - 18s/epoch - 211us/sample
Epoch 165/170
84077/84077 - 18s - loss: 2.6186e-04 - val_loss: 2.5581e-04 - 18s/epoch - 210us/sample
Epoch 166/170
84077/84077 - 17s - loss: 2.6191e-04 - val_loss: 2.5209e-04 - 17s/epoch - 206us/sample
Epoch 167/170
84077/84077 - 17s - loss: 2.6183e-04 - val_loss: 2.5410e-04 - 17s/epoch - 208us/sample
Epoch 168/170
84077/84077 - 18s - loss: 2.6172e-04 - val_loss: 2.5737e-04 - 18s/epoch - 211us/sample
Epoch 169/170
84077/84077 - 18s - loss: 2.6118e-04 - val_loss: 2.5527e-04 - 18s/epoch - 212us/sample
Epoch 170/170
84077/84077 - 18s - loss: 2.6067e-04 - val_loss: 2.5579e-04 - 18s/epoch - 213us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002557896154271122
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 06:42:20.655508: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_8/outputlayer/BiasAdd' id:10540 op device:{requested: '', assigned: ''} def:{{{node decoder_model_8/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_8/outputlayer/MatMul, decoder_model_8/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10443533029225775
cosine 0.10308426265214368
MAE: 0.0037005285210436937
RMSE: 0.020045286837609397
r2: 0.6862407800632334
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'logcosh', 16, 170, 0.0005, 0.2, 188, 0.0002606720205146588, 0.0002557896154271122, 0.10443533029225775, 0.10308426265214368, 0.0037005285210436937, 0.020045286837609397, 0.6862407800632334, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 10 0.0005 16 0] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 943)          890192      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 943)         3772        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 943)          0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          177472      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          177472      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1108475     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,357,383
Trainable params: 2,353,235
Non-trainable params: 4,148
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/10
2023-02-11 06:42:28.130942: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/batch_normalization_28/beta/m/Assign' id:12430 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/batch_normalization_28/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/batch_normalization_28/beta/m, training_18/Adam/batch_normalization_28/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 06:42:47.008072: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_9/mul' id:11853 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0684 - val_loss: 0.0681 - 21s/epoch - 250us/sample
Epoch 2/10
84077/84077 - 18s - loss: 0.0671 - val_loss: 0.0674 - 18s/epoch - 218us/sample
Epoch 3/10
84077/84077 - 18s - loss: 0.0669 - val_loss: 0.0674 - 18s/epoch - 220us/sample
Epoch 4/10
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 219us/sample
Epoch 5/10
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 217us/sample
Epoch 6/10
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 218us/sample
Epoch 7/10
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 217us/sample
Epoch 8/10
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 219us/sample
Epoch 9/10
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 219us/sample
Epoch 10/10
84077/84077 - 18s - loss: 0.0668 - val_loss: 0.0673 - 18s/epoch - 218us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573215826807
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 06:45:33.897996: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:11805 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.125718213789161
cosine 1.1540593493460503
MAE: 3.788519294711987
RMSE: 4.2064596076600465
r2: -13010.142086664102
RMSE zero-vector: 0.04004287452915337
['1.0custom_VAE', 'binary_crossentropy', 16, 10, 0.0005, 0.2, 188, 0.06683692710911078, 0.06734573215826807, 1.125718213789161, 1.1540593493460503, 3.788519294711987, 4.2064596076600465, -13010.142086664102, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 1980)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 06:45:41.535038: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_10/bias/Assign' id:12830 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_10/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_10/bias, bottleneck_zlog_10/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 06:45:52.949915: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:13147 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 13s - loss: 0.0079 - val_loss: 0.0018 - 13s/epoch - 159us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0038 - val_loss: 0.0011 - 11s/epoch - 127us/sample
Epoch 3/90
84077/84077 - 11s - loss: 0.0012 - val_loss: 9.8703e-04 - 11s/epoch - 128us/sample
Epoch 4/90
84077/84077 - 11s - loss: 9.5899e-04 - val_loss: 7.4787e-04 - 11s/epoch - 127us/sample
Epoch 5/90
84077/84077 - 11s - loss: 8.0977e-04 - val_loss: 6.2852e-04 - 11s/epoch - 128us/sample
Epoch 6/90
84077/84077 - 11s - loss: 6.4782e-04 - val_loss: 5.3746e-04 - 11s/epoch - 128us/sample
Epoch 7/90
84077/84077 - 11s - loss: 5.6820e-04 - val_loss: 4.8354e-04 - 11s/epoch - 128us/sample
Epoch 8/90
84077/84077 - 11s - loss: 5.1175e-04 - val_loss: 4.4167e-04 - 11s/epoch - 127us/sample
Epoch 9/90
84077/84077 - 11s - loss: 4.6639e-04 - val_loss: 4.2100e-04 - 11s/epoch - 126us/sample
Epoch 10/90
84077/84077 - 11s - loss: 4.3710e-04 - val_loss: 3.9647e-04 - 11s/epoch - 127us/sample
Epoch 11/90
84077/84077 - 11s - loss: 4.1643e-04 - val_loss: 3.8321e-04 - 11s/epoch - 127us/sample
Epoch 12/90
84077/84077 - 11s - loss: 4.0094e-04 - val_loss: 3.5476e-04 - 11s/epoch - 127us/sample
Epoch 13/90
84077/84077 - 11s - loss: 3.9232e-04 - val_loss: 3.7193e-04 - 11s/epoch - 127us/sample
Epoch 14/90
84077/84077 - 11s - loss: 3.8333e-04 - val_loss: 3.4330e-04 - 11s/epoch - 128us/sample
Epoch 15/90
84077/84077 - 11s - loss: 3.5824e-04 - val_loss: 3.2516e-04 - 11s/epoch - 128us/sample
Epoch 16/90
84077/84077 - 11s - loss: 3.4925e-04 - val_loss: 3.1730e-04 - 11s/epoch - 128us/sample
Epoch 17/90
84077/84077 - 11s - loss: 3.5186e-04 - val_loss: 3.1670e-04 - 11s/epoch - 127us/sample
Epoch 18/90
84077/84077 - 11s - loss: 3.3342e-04 - val_loss: 3.1736e-04 - 11s/epoch - 128us/sample
Epoch 19/90
84077/84077 - 11s - loss: 3.2785e-04 - val_loss: 3.0738e-04 - 11s/epoch - 129us/sample
Epoch 20/90
84077/84077 - 11s - loss: 3.1987e-04 - val_loss: 3.0071e-04 - 11s/epoch - 128us/sample
Epoch 21/90
84077/84077 - 11s - loss: 3.1405e-04 - val_loss: 2.9500e-04 - 11s/epoch - 126us/sample
Epoch 22/90
84077/84077 - 11s - loss: 3.0641e-04 - val_loss: 2.9054e-04 - 11s/epoch - 126us/sample
Epoch 23/90
84077/84077 - 11s - loss: 3.0217e-04 - val_loss: 2.8744e-04 - 11s/epoch - 128us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.9811e-04 - val_loss: 2.8677e-04 - 11s/epoch - 126us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.9349e-04 - val_loss: 2.8624e-04 - 11s/epoch - 127us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.9129e-04 - val_loss: 2.7409e-04 - 11s/epoch - 127us/sample
Epoch 27/90
84077/84077 - 11s - loss: 2.8769e-04 - val_loss: 2.7341e-04 - 11s/epoch - 128us/sample
Epoch 28/90
84077/84077 - 11s - loss: 2.8377e-04 - val_loss: 2.6676e-04 - 11s/epoch - 127us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.8015e-04 - val_loss: 2.7071e-04 - 11s/epoch - 126us/sample
Epoch 30/90
84077/84077 - 11s - loss: 2.7807e-04 - val_loss: 2.5967e-04 - 11s/epoch - 127us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.7414e-04 - val_loss: 2.6741e-04 - 11s/epoch - 128us/sample
Epoch 32/90
84077/84077 - 11s - loss: 2.7205e-04 - val_loss: 2.6629e-04 - 11s/epoch - 128us/sample
Epoch 33/90
84077/84077 - 11s - loss: 2.7240e-04 - val_loss: 2.5956e-04 - 11s/epoch - 127us/sample
Epoch 34/90
84077/84077 - 11s - loss: 2.6903e-04 - val_loss: 2.5908e-04 - 11s/epoch - 128us/sample
Epoch 35/90
84077/84077 - 11s - loss: 2.6396e-04 - val_loss: 2.5261e-04 - 11s/epoch - 128us/sample
Epoch 36/90
84077/84077 - 11s - loss: 2.6633e-04 - val_loss: 2.5450e-04 - 11s/epoch - 128us/sample
Epoch 37/90
84077/84077 - 11s - loss: 2.6221e-04 - val_loss: 2.4801e-04 - 11s/epoch - 127us/sample
Epoch 38/90
84077/84077 - 11s - loss: 2.6223e-04 - val_loss: 2.5476e-04 - 11s/epoch - 127us/sample
Epoch 39/90
84077/84077 - 11s - loss: 2.6035e-04 - val_loss: 2.4891e-04 - 11s/epoch - 129us/sample
Epoch 40/90
84077/84077 - 11s - loss: 2.5659e-04 - val_loss: 2.4620e-04 - 11s/epoch - 126us/sample
Epoch 41/90
84077/84077 - 11s - loss: 2.5838e-04 - val_loss: 2.4501e-04 - 11s/epoch - 126us/sample
Epoch 42/90
84077/84077 - 11s - loss: 2.5891e-04 - val_loss: 2.4314e-04 - 11s/epoch - 126us/sample
Epoch 43/90
84077/84077 - 11s - loss: 2.5668e-04 - val_loss: 2.4314e-04 - 11s/epoch - 127us/sample
Epoch 44/90
84077/84077 - 11s - loss: 2.5261e-04 - val_loss: 2.3547e-04 - 11s/epoch - 127us/sample
Epoch 45/90
84077/84077 - 11s - loss: 2.5282e-04 - val_loss: 2.3347e-04 - 11s/epoch - 128us/sample
Epoch 46/90
84077/84077 - 11s - loss: 2.5034e-04 - val_loss: 2.3643e-04 - 11s/epoch - 128us/sample
Epoch 47/90
84077/84077 - 11s - loss: 2.4997e-04 - val_loss: 2.3169e-04 - 11s/epoch - 129us/sample
Epoch 48/90
84077/84077 - 11s - loss: 2.4750e-04 - val_loss: 2.3538e-04 - 11s/epoch - 128us/sample
Epoch 49/90
84077/84077 - 11s - loss: 2.4640e-04 - val_loss: 2.3223e-04 - 11s/epoch - 127us/sample
Epoch 50/90
84077/84077 - 11s - loss: 2.4453e-04 - val_loss: 2.3088e-04 - 11s/epoch - 127us/sample
Epoch 51/90
84077/84077 - 11s - loss: 2.4438e-04 - val_loss: 2.2961e-04 - 11s/epoch - 128us/sample
Epoch 52/90
84077/84077 - 11s - loss: 2.4236e-04 - val_loss: 2.2851e-04 - 11s/epoch - 127us/sample
Epoch 53/90
84077/84077 - 11s - loss: 2.4133e-04 - val_loss: 2.2693e-04 - 11s/epoch - 128us/sample
Epoch 54/90
84077/84077 - 11s - loss: 2.3954e-04 - val_loss: 2.3075e-04 - 11s/epoch - 127us/sample
Epoch 55/90
84077/84077 - 11s - loss: 2.4055e-04 - val_loss: 2.3124e-04 - 11s/epoch - 128us/sample
Epoch 56/90
84077/84077 - 11s - loss: 2.4149e-04 - val_loss: 2.3610e-04 - 11s/epoch - 128us/sample
Epoch 57/90
84077/84077 - 11s - loss: 2.4152e-04 - val_loss: 2.2893e-04 - 11s/epoch - 128us/sample
Epoch 58/90
84077/84077 - 11s - loss: 2.3881e-04 - val_loss: 2.3013e-04 - 11s/epoch - 128us/sample
Epoch 59/90
84077/84077 - 11s - loss: 2.3710e-04 - val_loss: 2.2549e-04 - 11s/epoch - 127us/sample
Epoch 60/90
84077/84077 - 11s - loss: 2.3511e-04 - val_loss: 2.2544e-04 - 11s/epoch - 128us/sample
Epoch 61/90
84077/84077 - 11s - loss: 2.3448e-04 - val_loss: 2.2434e-04 - 11s/epoch - 127us/sample
Epoch 62/90
84077/84077 - 11s - loss: 2.3618e-04 - val_loss: 2.2715e-04 - 11s/epoch - 127us/sample
Epoch 63/90
84077/84077 - 11s - loss: 2.3924e-04 - val_loss: 2.2611e-04 - 11s/epoch - 126us/sample
Epoch 64/90
84077/84077 - 11s - loss: 2.3378e-04 - val_loss: 2.2753e-04 - 11s/epoch - 128us/sample
Epoch 65/90
84077/84077 - 11s - loss: 2.3388e-04 - val_loss: 2.2116e-04 - 11s/epoch - 127us/sample
Epoch 66/90
84077/84077 - 11s - loss: 2.3634e-04 - val_loss: 2.2338e-04 - 11s/epoch - 128us/sample
Epoch 67/90
84077/84077 - 11s - loss: 2.3382e-04 - val_loss: 2.2813e-04 - 11s/epoch - 127us/sample
Epoch 68/90
84077/84077 - 11s - loss: 2.3613e-04 - val_loss: 2.2252e-04 - 11s/epoch - 128us/sample
Epoch 69/90
84077/84077 - 11s - loss: 2.3096e-04 - val_loss: 2.2068e-04 - 11s/epoch - 128us/sample
Epoch 70/90
84077/84077 - 11s - loss: 2.3169e-04 - val_loss: 2.2581e-04 - 11s/epoch - 128us/sample
Epoch 71/90
84077/84077 - 11s - loss: 2.3029e-04 - val_loss: 2.2364e-04 - 11s/epoch - 127us/sample
Epoch 72/90
84077/84077 - 11s - loss: 2.3004e-04 - val_loss: 2.2217e-04 - 11s/epoch - 128us/sample
Epoch 73/90
84077/84077 - 11s - loss: 2.2962e-04 - val_loss: 2.2949e-04 - 11s/epoch - 126us/sample
Epoch 74/90
84077/84077 - 11s - loss: 2.3109e-04 - val_loss: 2.2275e-04 - 11s/epoch - 126us/sample
Epoch 75/90
84077/84077 - 11s - loss: 2.3097e-04 - val_loss: 2.2118e-04 - 11s/epoch - 126us/sample
Epoch 76/90
84077/84077 - 11s - loss: 2.2760e-04 - val_loss: 2.1786e-04 - 11s/epoch - 127us/sample
Epoch 77/90
84077/84077 - 11s - loss: 2.2716e-04 - val_loss: 2.1712e-04 - 11s/epoch - 128us/sample
Epoch 78/90
84077/84077 - 11s - loss: 2.2707e-04 - val_loss: 2.2292e-04 - 11s/epoch - 128us/sample
Epoch 79/90
84077/84077 - 11s - loss: 2.2717e-04 - val_loss: 2.2109e-04 - 11s/epoch - 127us/sample
Epoch 80/90
84077/84077 - 11s - loss: 2.2546e-04 - val_loss: 2.1543e-04 - 11s/epoch - 128us/sample
Epoch 81/90
84077/84077 - 11s - loss: 2.3123e-04 - val_loss: 2.2732e-04 - 11s/epoch - 128us/sample
Epoch 82/90
84077/84077 - 11s - loss: 2.2474e-04 - val_loss: 2.1490e-04 - 11s/epoch - 128us/sample
Epoch 83/90
84077/84077 - 11s - loss: 2.2469e-04 - val_loss: 2.1860e-04 - 11s/epoch - 127us/sample
Epoch 84/90
84077/84077 - 11s - loss: 2.2455e-04 - val_loss: 2.1636e-04 - 11s/epoch - 127us/sample
Epoch 85/90
84077/84077 - 11s - loss: 2.2321e-04 - val_loss: 2.1591e-04 - 11s/epoch - 129us/sample
Epoch 86/90
84077/84077 - 11s - loss: 2.2334e-04 - val_loss: 2.1173e-04 - 11s/epoch - 128us/sample
Epoch 87/90
84077/84077 - 11s - loss: 2.2578e-04 - val_loss: 2.1899e-04 - 11s/epoch - 128us/sample
Epoch 88/90
84077/84077 - 11s - loss: 2.2227e-04 - val_loss: 2.1227e-04 - 11s/epoch - 128us/sample
Epoch 89/90
84077/84077 - 11s - loss: 2.2428e-04 - val_loss: 2.1972e-04 - 11s/epoch - 129us/sample
Epoch 90/90
84077/84077 - 11s - loss: 2.2171e-04 - val_loss: 2.1240e-04 - 11s/epoch - 128us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00021239713473658026
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 07:01:48.130162: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:13118 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02756693891205884
cosine 0.027242755652497348
MAE: 0.0021107163276864807
RMSE: 0.010155998180347209
r2: 0.9195376903304509
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 90, 0.001, 0.2, 188, 0.00022170984543351047, 0.00021239713473658026, 0.02756693891205884, 0.027242755652497348, 0.0021107163276864807, 0.010155998180347209, 0.9195376903304509, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0008 32 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 1886)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 07:01:56.294343: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/bottleneck_zmean_11/bias/v/Assign' id:15035 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/bottleneck_zmean_11/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/bottleneck_zmean_11/bias/v, training_22/Adam/bottleneck_zmean_11/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 07:02:07.966524: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:14412 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0028 - val_loss: 8.4751e-04 - 14s/epoch - 164us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0119 - val_loss: 6.7145e-04 - 11s/epoch - 128us/sample
Epoch 3/90
84077/84077 - 11s - loss: 8.2869e-04 - val_loss: 6.3465e-04 - 11s/epoch - 127us/sample
Epoch 4/90
84077/84077 - 11s - loss: 6.4215e-04 - val_loss: 4.6793e-04 - 11s/epoch - 127us/sample
Epoch 5/90
84077/84077 - 11s - loss: 4.8607e-04 - val_loss: 4.0726e-04 - 11s/epoch - 129us/sample
Epoch 6/90
84077/84077 - 11s - loss: 4.5658e-04 - val_loss: 3.6757e-04 - 11s/epoch - 129us/sample
Epoch 7/90
84077/84077 - 11s - loss: 3.8009e-04 - val_loss: 3.2822e-04 - 11s/epoch - 130us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.4719e-04 - val_loss: 3.0391e-04 - 11s/epoch - 130us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.2713e-04 - val_loss: 2.8970e-04 - 11s/epoch - 131us/sample
Epoch 10/90
84077/84077 - 11s - loss: 3.0910e-04 - val_loss: 2.7550e-04 - 11s/epoch - 130us/sample
Epoch 11/90
84077/84077 - 11s - loss: 2.9432e-04 - val_loss: 2.6616e-04 - 11s/epoch - 130us/sample
Epoch 12/90
84077/84077 - 11s - loss: 2.8309e-04 - val_loss: 2.5430e-04 - 11s/epoch - 130us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.7517e-04 - val_loss: 2.4917e-04 - 11s/epoch - 130us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.6736e-04 - val_loss: 2.4369e-04 - 11s/epoch - 131us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.6099e-04 - val_loss: 2.3624e-04 - 11s/epoch - 130us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.5529e-04 - val_loss: 2.3112e-04 - 11s/epoch - 130us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.5131e-04 - val_loss: 2.3080e-04 - 11s/epoch - 130us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.4738e-04 - val_loss: 2.2548e-04 - 11s/epoch - 130us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.4371e-04 - val_loss: 2.2175e-04 - 11s/epoch - 130us/sample
Epoch 20/90
84077/84077 - 11s - loss: 2.4054e-04 - val_loss: 2.1991e-04 - 11s/epoch - 129us/sample
Epoch 21/90
84077/84077 - 11s - loss: 2.3774e-04 - val_loss: 2.1626e-04 - 11s/epoch - 128us/sample
Epoch 22/90
84077/84077 - 11s - loss: 2.3578e-04 - val_loss: 2.1602e-04 - 11s/epoch - 128us/sample
Epoch 23/90
84077/84077 - 11s - loss: 2.3356e-04 - val_loss: 2.1574e-04 - 11s/epoch - 129us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.3060e-04 - val_loss: 2.1516e-04 - 11s/epoch - 131us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.2921e-04 - val_loss: 2.0844e-04 - 11s/epoch - 131us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.2763e-04 - val_loss: 2.0800e-04 - 11s/epoch - 131us/sample
Epoch 27/90
84077/84077 - 11s - loss: 2.2487e-04 - val_loss: 2.0811e-04 - 11s/epoch - 130us/sample
Epoch 28/90
84077/84077 - 11s - loss: 2.2320e-04 - val_loss: 2.0722e-04 - 11s/epoch - 130us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.2159e-04 - val_loss: 2.0275e-04 - 11s/epoch - 131us/sample
Epoch 30/90
84077/84077 - 11s - loss: 2.1945e-04 - val_loss: 2.0388e-04 - 11s/epoch - 130us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.1960e-04 - val_loss: 2.0391e-04 - 11s/epoch - 130us/sample
Epoch 32/90
84077/84077 - 11s - loss: 2.1754e-04 - val_loss: 2.0066e-04 - 11s/epoch - 130us/sample
Epoch 33/90
84077/84077 - 11s - loss: 2.1642e-04 - val_loss: 2.0361e-04 - 11s/epoch - 130us/sample
Epoch 34/90
84077/84077 - 11s - loss: 2.1610e-04 - val_loss: 1.9859e-04 - 11s/epoch - 130us/sample
Epoch 35/90
84077/84077 - 11s - loss: 2.1536e-04 - val_loss: 1.9961e-04 - 11s/epoch - 130us/sample
Epoch 36/90
84077/84077 - 11s - loss: 2.1343e-04 - val_loss: 1.9706e-04 - 11s/epoch - 130us/sample
Epoch 37/90
84077/84077 - 11s - loss: 2.1272e-04 - val_loss: 1.9615e-04 - 11s/epoch - 129us/sample
Epoch 38/90
84077/84077 - 11s - loss: 2.1224e-04 - val_loss: 1.9517e-04 - 11s/epoch - 128us/sample
Epoch 39/90
84077/84077 - 11s - loss: 2.1176e-04 - val_loss: 1.9662e-04 - 11s/epoch - 129us/sample
Epoch 40/90
84077/84077 - 11s - loss: 2.1016e-04 - val_loss: 1.9659e-04 - 11s/epoch - 129us/sample
Epoch 41/90
84077/84077 - 11s - loss: 2.0839e-04 - val_loss: 1.9440e-04 - 11s/epoch - 128us/sample
Epoch 42/90
84077/84077 - 11s - loss: 2.0810e-04 - val_loss: 1.9211e-04 - 11s/epoch - 130us/sample
Epoch 43/90
84077/84077 - 11s - loss: 2.0744e-04 - val_loss: 1.9152e-04 - 11s/epoch - 130us/sample
Epoch 44/90
84077/84077 - 11s - loss: 2.0716e-04 - val_loss: 1.8955e-04 - 11s/epoch - 131us/sample
Epoch 45/90
84077/84077 - 11s - loss: 2.0623e-04 - val_loss: 1.9307e-04 - 11s/epoch - 129us/sample
Epoch 46/90
84077/84077 - 11s - loss: 2.0539e-04 - val_loss: 1.9127e-04 - 11s/epoch - 131us/sample
Epoch 47/90
84077/84077 - 11s - loss: 2.0489e-04 - val_loss: 1.9212e-04 - 11s/epoch - 130us/sample
Epoch 48/90
84077/84077 - 11s - loss: 2.0452e-04 - val_loss: 1.9214e-04 - 11s/epoch - 130us/sample
Epoch 49/90
84077/84077 - 11s - loss: 2.0417e-04 - val_loss: 1.8938e-04 - 11s/epoch - 131us/sample
Epoch 50/90
84077/84077 - 11s - loss: 2.0276e-04 - val_loss: 1.9099e-04 - 11s/epoch - 131us/sample
Epoch 51/90
84077/84077 - 11s - loss: 2.0228e-04 - val_loss: 1.8667e-04 - 11s/epoch - 130us/sample
Epoch 52/90
84077/84077 - 11s - loss: 2.0287e-04 - val_loss: 1.8785e-04 - 11s/epoch - 130us/sample
Epoch 53/90
84077/84077 - 11s - loss: 2.0004e-04 - val_loss: 1.8704e-04 - 11s/epoch - 130us/sample
Epoch 54/90
84077/84077 - 11s - loss: 1.9940e-04 - val_loss: 1.8468e-04 - 11s/epoch - 131us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.9875e-04 - val_loss: 1.8518e-04 - 11s/epoch - 130us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.9933e-04 - val_loss: 1.8498e-04 - 11s/epoch - 130us/sample
Epoch 57/90
84077/84077 - 11s - loss: 1.9846e-04 - val_loss: 1.8808e-04 - 11s/epoch - 130us/sample
Epoch 58/90
84077/84077 - 11s - loss: 1.9691e-04 - val_loss: 1.8610e-04 - 11s/epoch - 130us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.9821e-04 - val_loss: 1.8403e-04 - 11s/epoch - 131us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.9631e-04 - val_loss: 1.8570e-04 - 11s/epoch - 130us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.9590e-04 - val_loss: 1.8235e-04 - 11s/epoch - 130us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.9512e-04 - val_loss: 1.8082e-04 - 11s/epoch - 129us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.9507e-04 - val_loss: 1.8192e-04 - 11s/epoch - 130us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.9567e-04 - val_loss: 1.8236e-04 - 11s/epoch - 130us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.9407e-04 - val_loss: 1.8023e-04 - 11s/epoch - 130us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.9272e-04 - val_loss: 1.7937e-04 - 11s/epoch - 130us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.9314e-04 - val_loss: 1.8198e-04 - 11s/epoch - 130us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.9260e-04 - val_loss: 1.7863e-04 - 11s/epoch - 130us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.9189e-04 - val_loss: 1.8006e-04 - 11s/epoch - 130us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.9385e-04 - val_loss: 1.7630e-04 - 11s/epoch - 130us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.9137e-04 - val_loss: 1.8049e-04 - 11s/epoch - 130us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.9069e-04 - val_loss: 1.7422e-04 - 11s/epoch - 130us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.9110e-04 - val_loss: 1.7850e-04 - 11s/epoch - 131us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.9356e-04 - val_loss: 1.7530e-04 - 11s/epoch - 131us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.9076e-04 - val_loss: 1.7615e-04 - 11s/epoch - 130us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.9311e-04 - val_loss: 1.7798e-04 - 11s/epoch - 130us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.9108e-04 - val_loss: 1.7631e-04 - 11s/epoch - 130us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.8909e-04 - val_loss: 1.7791e-04 - 11s/epoch - 131us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.8935e-04 - val_loss: 1.7723e-04 - 11s/epoch - 130us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.8973e-04 - val_loss: 1.7550e-04 - 11s/epoch - 130us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.8859e-04 - val_loss: 1.7422e-04 - 11s/epoch - 130us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.8883e-04 - val_loss: 1.7526e-04 - 11s/epoch - 130us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.8868e-04 - val_loss: 1.7427e-04 - 11s/epoch - 131us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.8790e-04 - val_loss: 1.7459e-04 - 11s/epoch - 130us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.8830e-04 - val_loss: 1.8391e-04 - 11s/epoch - 130us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.8903e-04 - val_loss: 1.7297e-04 - 11s/epoch - 130us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.8879e-04 - val_loss: 1.7799e-04 - 11s/epoch - 130us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.8714e-04 - val_loss: 1.7386e-04 - 11s/epoch - 131us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.8624e-04 - val_loss: 1.7147e-04 - 11s/epoch - 130us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.8598e-04 - val_loss: 1.7323e-04 - 11s/epoch - 130us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001732250712102389
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 07:18:21.476160: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:14376 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05446172133064781
cosine 0.053809129421469926
MAE: 0.002891451304660681
RMSE: 0.014244885060832484
r2: 0.8415695674150194
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 32, 90, 0.0008, 0.2, 188, 0.00018598134882579824, 0.0001732250712102389, 0.05446172133064781, 0.053809129421469926, 0.002891451304660681, 0.014244885060832484, 0.8415695674150194, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0012000000000000001 32 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 1886)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 07:18:30.071903: W tensorflow/c/c_api.cc:291] Operation '{name:'training_24/Adam/beta_1/Assign' id:16151 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_24/Adam/beta_1, training_24/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 07:18:41.939851: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:15697 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0030 - val_loss: 0.0869 - 14s/epoch - 168us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0011 - val_loss: 8.5126e-04 - 11s/epoch - 131us/sample
Epoch 3/90
84077/84077 - 11s - loss: 6.9653e-04 - val_loss: 4.9969e-04 - 11s/epoch - 131us/sample
Epoch 4/90
84077/84077 - 11s - loss: 5.6277e-04 - val_loss: 4.3384e-04 - 11s/epoch - 131us/sample
Epoch 5/90
84077/84077 - 11s - loss: 4.4752e-04 - val_loss: 3.8906e-04 - 11s/epoch - 131us/sample
Epoch 6/90
84077/84077 - 11s - loss: 3.8992e-04 - val_loss: 3.4954e-04 - 11s/epoch - 131us/sample
Epoch 7/90
84077/84077 - 11s - loss: 3.6301e-04 - val_loss: 3.2266e-04 - 11s/epoch - 130us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.4616e-04 - val_loss: 3.0960e-04 - 11s/epoch - 129us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.3562e-04 - val_loss: 3.0129e-04 - 11s/epoch - 130us/sample
Epoch 10/90
84077/84077 - 11s - loss: 3.2695e-04 - val_loss: 2.9812e-04 - 11s/epoch - 129us/sample
Epoch 11/90
84077/84077 - 11s - loss: 3.2058e-04 - val_loss: 2.9104e-04 - 11s/epoch - 130us/sample
Epoch 12/90
84077/84077 - 11s - loss: 3.1553e-04 - val_loss: 2.8586e-04 - 11s/epoch - 132us/sample
Epoch 13/90
84077/84077 - 11s - loss: 3.1118e-04 - val_loss: 2.8310e-04 - 11s/epoch - 131us/sample
Epoch 14/90
84077/84077 - 11s - loss: 3.0834e-04 - val_loss: 2.7902e-04 - 11s/epoch - 131us/sample
Epoch 15/90
84077/84077 - 11s - loss: 3.0550e-04 - val_loss: 2.7855e-04 - 11s/epoch - 131us/sample
Epoch 16/90
84077/84077 - 11s - loss: 3.0227e-04 - val_loss: 2.7497e-04 - 11s/epoch - 131us/sample
Epoch 17/90
84077/84077 - 11s - loss: 3.0006e-04 - val_loss: 2.7436e-04 - 11s/epoch - 132us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.9849e-04 - val_loss: 2.7248e-04 - 11s/epoch - 131us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.9736e-04 - val_loss: 2.7218e-04 - 11s/epoch - 131us/sample
Epoch 20/90
84077/84077 - 11s - loss: 2.9641e-04 - val_loss: 2.7255e-04 - 11s/epoch - 131us/sample
Epoch 21/90
84077/84077 - 11s - loss: 2.9516e-04 - val_loss: 2.7019e-04 - 11s/epoch - 131us/sample
Epoch 22/90
84077/84077 - 11s - loss: 2.9359e-04 - val_loss: 2.6784e-04 - 11s/epoch - 132us/sample
Epoch 23/90
84077/84077 - 11s - loss: 2.9151e-04 - val_loss: 2.6597e-04 - 11s/epoch - 131us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.9001e-04 - val_loss: 2.6631e-04 - 11s/epoch - 132us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.8927e-04 - val_loss: 2.6321e-04 - 11s/epoch - 131us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.8840e-04 - val_loss: 2.6280e-04 - 11s/epoch - 132us/sample
Epoch 27/90
84077/84077 - 11s - loss: 2.8780e-04 - val_loss: 2.6229e-04 - 11s/epoch - 131us/sample
Epoch 28/90
84077/84077 - 11s - loss: 2.8765e-04 - val_loss: 2.6252e-04 - 11s/epoch - 131us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.8630e-04 - val_loss: 2.6253e-04 - 11s/epoch - 131us/sample
Epoch 30/90
84077/84077 - 11s - loss: 2.8585e-04 - val_loss: 2.6093e-04 - 11s/epoch - 131us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.8518e-04 - val_loss: 2.5927e-04 - 11s/epoch - 132us/sample
Epoch 32/90
84077/84077 - 11s - loss: 2.8478e-04 - val_loss: 2.6041e-04 - 11s/epoch - 131us/sample
Epoch 33/90
84077/84077 - 11s - loss: 2.8316e-04 - val_loss: 2.5876e-04 - 11s/epoch - 131us/sample
Epoch 34/90
84077/84077 - 11s - loss: 2.8251e-04 - val_loss: 2.5570e-04 - 11s/epoch - 131us/sample
Epoch 35/90
84077/84077 - 11s - loss: 2.8176e-04 - val_loss: 2.5664e-04 - 11s/epoch - 131us/sample
Epoch 36/90
84077/84077 - 11s - loss: 2.8119e-04 - val_loss: 2.5588e-04 - 11s/epoch - 133us/sample
Epoch 37/90
84077/84077 - 11s - loss: 2.8057e-04 - val_loss: 2.5590e-04 - 11s/epoch - 132us/sample
Epoch 38/90
84077/84077 - 11s - loss: 2.7990e-04 - val_loss: 2.5785e-04 - 11s/epoch - 131us/sample
Epoch 39/90
84077/84077 - 11s - loss: 2.7951e-04 - val_loss: 2.5596e-04 - 11s/epoch - 131us/sample
Epoch 40/90
84077/84077 - 11s - loss: 2.7888e-04 - val_loss: 2.5429e-04 - 11s/epoch - 132us/sample
Epoch 41/90
84077/84077 - 11s - loss: 2.7856e-04 - val_loss: 2.5525e-04 - 11s/epoch - 132us/sample
Epoch 42/90
84077/84077 - 11s - loss: 2.7784e-04 - val_loss: 2.5404e-04 - 11s/epoch - 131us/sample
Epoch 43/90
84077/84077 - 11s - loss: 2.7776e-04 - val_loss: 2.5478e-04 - 11s/epoch - 131us/sample
Epoch 44/90
84077/84077 - 11s - loss: 2.7718e-04 - val_loss: 2.5297e-04 - 11s/epoch - 131us/sample
Epoch 45/90
84077/84077 - 11s - loss: 2.7720e-04 - val_loss: 2.5398e-04 - 11s/epoch - 131us/sample
Epoch 46/90
84077/84077 - 11s - loss: 2.7654e-04 - val_loss: 2.5350e-04 - 11s/epoch - 132us/sample
Epoch 47/90
84077/84077 - 11s - loss: 2.7629e-04 - val_loss: 2.5581e-04 - 11s/epoch - 131us/sample
Epoch 48/90
84077/84077 - 11s - loss: 2.7544e-04 - val_loss: 2.5258e-04 - 11s/epoch - 131us/sample
Epoch 49/90
84077/84077 - 11s - loss: 2.7524e-04 - val_loss: 2.5356e-04 - 11s/epoch - 131us/sample
Epoch 50/90
84077/84077 - 11s - loss: 2.7518e-04 - val_loss: 2.5280e-04 - 11s/epoch - 131us/sample
Epoch 51/90
84077/84077 - 11s - loss: 2.7458e-04 - val_loss: 2.5166e-04 - 11s/epoch - 132us/sample
Epoch 52/90
84077/84077 - 11s - loss: 2.7466e-04 - val_loss: 2.5224e-04 - 11s/epoch - 132us/sample
Epoch 53/90
84077/84077 - 11s - loss: 2.7444e-04 - val_loss: 2.5214e-04 - 11s/epoch - 131us/sample
Epoch 54/90
84077/84077 - 11s - loss: 2.7425e-04 - val_loss: 2.5288e-04 - 11s/epoch - 129us/sample
Epoch 55/90
84077/84077 - 11s - loss: 2.7365e-04 - val_loss: 2.5133e-04 - 11s/epoch - 129us/sample
Epoch 56/90
84077/84077 - 11s - loss: 2.7349e-04 - val_loss: 2.5114e-04 - 11s/epoch - 129us/sample
Epoch 57/90
84077/84077 - 11s - loss: 2.7332e-04 - val_loss: 2.5113e-04 - 11s/epoch - 128us/sample
Epoch 58/90
84077/84077 - 11s - loss: 2.7308e-04 - val_loss: 2.5048e-04 - 11s/epoch - 131us/sample
Epoch 59/90
84077/84077 - 11s - loss: 2.7259e-04 - val_loss: 2.5182e-04 - 11s/epoch - 130us/sample
Epoch 60/90
84077/84077 - 11s - loss: 2.7210e-04 - val_loss: 2.5049e-04 - 11s/epoch - 132us/sample
Epoch 61/90
84077/84077 - 11s - loss: 2.7233e-04 - val_loss: 2.5059e-04 - 11s/epoch - 132us/sample
Epoch 62/90
84077/84077 - 11s - loss: 2.7186e-04 - val_loss: 2.4997e-04 - 11s/epoch - 131us/sample
Epoch 63/90
84077/84077 - 11s - loss: 2.7129e-04 - val_loss: 2.4975e-04 - 11s/epoch - 131us/sample
Epoch 64/90
84077/84077 - 11s - loss: 2.7095e-04 - val_loss: 2.4838e-04 - 11s/epoch - 131us/sample
Epoch 65/90
84077/84077 - 11s - loss: 2.7089e-04 - val_loss: 2.4922e-04 - 11s/epoch - 131us/sample
Epoch 66/90
84077/84077 - 11s - loss: 2.7060e-04 - val_loss: 2.4899e-04 - 11s/epoch - 132us/sample
Epoch 67/90
84077/84077 - 11s - loss: 2.7061e-04 - val_loss: 2.4992e-04 - 11s/epoch - 131us/sample
Epoch 68/90
84077/84077 - 11s - loss: 2.7037e-04 - val_loss: 2.5020e-04 - 11s/epoch - 131us/sample
Epoch 69/90
84077/84077 - 11s - loss: 2.7035e-04 - val_loss: 2.4986e-04 - 11s/epoch - 131us/sample
Epoch 70/90
84077/84077 - 11s - loss: 2.7019e-04 - val_loss: 2.4856e-04 - 11s/epoch - 132us/sample
Epoch 71/90
84077/84077 - 11s - loss: 2.6961e-04 - val_loss: 2.4746e-04 - 11s/epoch - 131us/sample
Epoch 72/90
84077/84077 - 11s - loss: 2.6962e-04 - val_loss: 2.4828e-04 - 11s/epoch - 130us/sample
Epoch 73/90
84077/84077 - 11s - loss: 2.6971e-04 - val_loss: 2.4782e-04 - 11s/epoch - 129us/sample
Epoch 74/90
84077/84077 - 11s - loss: 2.6923e-04 - val_loss: 2.4763e-04 - 11s/epoch - 130us/sample
Epoch 75/90
84077/84077 - 11s - loss: 2.6947e-04 - val_loss: 2.4868e-04 - 11s/epoch - 130us/sample
Epoch 76/90
84077/84077 - 11s - loss: 2.6931e-04 - val_loss: 2.4689e-04 - 11s/epoch - 131us/sample
Epoch 77/90
84077/84077 - 11s - loss: 2.6907e-04 - val_loss: 2.4741e-04 - 11s/epoch - 131us/sample
Epoch 78/90
84077/84077 - 11s - loss: 2.6880e-04 - val_loss: 2.4775e-04 - 11s/epoch - 132us/sample
Epoch 79/90
84077/84077 - 11s - loss: 2.6890e-04 - val_loss: 2.4791e-04 - 11s/epoch - 131us/sample
Epoch 80/90
84077/84077 - 11s - loss: 2.6838e-04 - val_loss: 2.4847e-04 - 11s/epoch - 131us/sample
Epoch 81/90
84077/84077 - 11s - loss: 2.6866e-04 - val_loss: 2.4725e-04 - 11s/epoch - 131us/sample
Epoch 82/90
84077/84077 - 11s - loss: 2.6864e-04 - val_loss: 2.4673e-04 - 11s/epoch - 132us/sample
Epoch 83/90
84077/84077 - 11s - loss: 2.6843e-04 - val_loss: 2.4757e-04 - 11s/epoch - 131us/sample
Epoch 84/90
84077/84077 - 11s - loss: 2.6825e-04 - val_loss: 2.4735e-04 - 11s/epoch - 130us/sample
Epoch 85/90
84077/84077 - 11s - loss: 2.6852e-04 - val_loss: 2.4760e-04 - 11s/epoch - 131us/sample
Epoch 86/90
84077/84077 - 11s - loss: 2.6796e-04 - val_loss: 2.4699e-04 - 11s/epoch - 132us/sample
Epoch 87/90
84077/84077 - 11s - loss: 2.6801e-04 - val_loss: 2.4650e-04 - 11s/epoch - 131us/sample
Epoch 88/90
84077/84077 - 11s - loss: 2.6795e-04 - val_loss: 2.4804e-04 - 11s/epoch - 131us/sample
Epoch 89/90
84077/84077 - 11s - loss: 2.6754e-04 - val_loss: 2.4804e-04 - 11s/epoch - 130us/sample
Epoch 90/90
84077/84077 - 11s - loss: 2.6796e-04 - val_loss: 2.4682e-04 - 11s/epoch - 130us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002468159046489446
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 07:35:02.979852: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:15661 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10901260566892693
cosine 0.10756160264733779
MAE: 0.003651341685307399
RMSE: 0.0197405151414205
r2: 0.6955960321373713
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 32, 90, 0.0012000000000000001, 0.2, 188, 0.0002679607663580719, 0.0002468159046489446, 0.10901260566892693, 0.10756160264733779, 0.003651341685307399, 0.0197405151414205, 0.6955960321373713, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0008 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 1886)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 07:35:11.830612: W tensorflow/c/c_api.cc:291] Operation '{name:'training_26/Adam/batch_normalization_39/beta/m/Assign' id:17449 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/batch_normalization_39/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/batch_normalization_39/beta/m, training_26/Adam/batch_normalization_39/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 07:35:23.868147: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:16975 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0060 - val_loss: 0.0021 - 14s/epoch - 171us/sample
Epoch 2/90
84077/84077 - 11s - loss: 1.8731 - val_loss: 0.0022 - 11s/epoch - 131us/sample
Epoch 3/90
84077/84077 - 11s - loss: 0.0045 - val_loss: 0.0022 - 11s/epoch - 131us/sample
Epoch 4/90
84077/84077 - 11s - loss: 0.0025 - val_loss: 0.0010 - 11s/epoch - 131us/sample
Epoch 5/90
84077/84077 - 11s - loss: 0.0013 - val_loss: 0.0013 - 11s/epoch - 131us/sample
Epoch 6/90
84077/84077 - 11s - loss: 0.0101 - val_loss: 0.0020 - 11s/epoch - 131us/sample
Epoch 7/90
84077/84077 - 11s - loss: 0.0015 - val_loss: 0.0010 - 11s/epoch - 132us/sample
Epoch 8/90
84077/84077 - 11s - loss: 0.0018 - val_loss: 0.0010 - 11s/epoch - 131us/sample
Epoch 9/90
84077/84077 - 11s - loss: 0.0011 - val_loss: 8.3685e-04 - 11s/epoch - 131us/sample
Epoch 10/90
84077/84077 - 11s - loss: 8.7848e-04 - val_loss: 8.0110e-04 - 11s/epoch - 131us/sample
Epoch 11/90
84077/84077 - 11s - loss: 9.1878e-04 - val_loss: 7.5642e-04 - 11s/epoch - 131us/sample
Epoch 12/90
84077/84077 - 11s - loss: 7.9594e-04 - val_loss: 7.2145e-04 - 11s/epoch - 132us/sample
Epoch 13/90
84077/84077 - 11s - loss: 7.7076e-04 - val_loss: 7.0308e-04 - 11s/epoch - 130us/sample
Epoch 14/90
84077/84077 - 11s - loss: 7.5858e-04 - val_loss: 6.7666e-04 - 11s/epoch - 129us/sample
Epoch 15/90
84077/84077 - 11s - loss: 7.4832e-04 - val_loss: 6.6426e-04 - 11s/epoch - 128us/sample
Epoch 16/90
84077/84077 - 11s - loss: 6.9607e-04 - val_loss: 6.3646e-04 - 11s/epoch - 128us/sample
Epoch 17/90
84077/84077 - 11s - loss: 6.7472e-04 - val_loss: 6.1908e-04 - 11s/epoch - 130us/sample
Epoch 18/90
84077/84077 - 11s - loss: 6.5949e-04 - val_loss: 6.0671e-04 - 11s/epoch - 130us/sample
Epoch 19/90
84077/84077 - 11s - loss: 6.3816e-04 - val_loss: 5.8518e-04 - 11s/epoch - 131us/sample
Epoch 20/90
84077/84077 - 11s - loss: 6.2323e-04 - val_loss: 5.7584e-04 - 11s/epoch - 131us/sample
Epoch 21/90
84077/84077 - 11s - loss: 6.0779e-04 - val_loss: 5.7241e-04 - 11s/epoch - 132us/sample
Epoch 22/90
84077/84077 - 11s - loss: 5.9571e-04 - val_loss: 5.4980e-04 - 11s/epoch - 131us/sample
Epoch 23/90
84077/84077 - 11s - loss: 5.8392e-04 - val_loss: 5.3923e-04 - 11s/epoch - 131us/sample
Epoch 24/90
84077/84077 - 11s - loss: 5.7418e-04 - val_loss: 5.2819e-04 - 11s/epoch - 131us/sample
Epoch 25/90
84077/84077 - 11s - loss: 5.6163e-04 - val_loss: 5.1787e-04 - 11s/epoch - 132us/sample
Epoch 26/90
84077/84077 - 11s - loss: 5.4761e-04 - val_loss: 4.9806e-04 - 11s/epoch - 131us/sample
Epoch 27/90
84077/84077 - 11s - loss: 5.3234e-04 - val_loss: 4.8049e-04 - 11s/epoch - 131us/sample
Epoch 28/90
84077/84077 - 11s - loss: 5.1645e-04 - val_loss: 4.6975e-04 - 11s/epoch - 131us/sample
Epoch 29/90
84077/84077 - 11s - loss: 5.0564e-04 - val_loss: 4.6114e-04 - 11s/epoch - 131us/sample
Epoch 30/90
84077/84077 - 11s - loss: 4.9802e-04 - val_loss: 4.5390e-04 - 11s/epoch - 132us/sample
Epoch 31/90
84077/84077 - 11s - loss: 4.9220e-04 - val_loss: 4.5188e-04 - 11s/epoch - 130us/sample
Epoch 32/90
84077/84077 - 11s - loss: 4.8546e-04 - val_loss: 4.4874e-04 - 11s/epoch - 129us/sample
Epoch 33/90
84077/84077 - 11s - loss: 4.8132e-04 - val_loss: 4.4265e-04 - 11s/epoch - 129us/sample
Epoch 34/90
84077/84077 - 11s - loss: 4.7636e-04 - val_loss: 4.3865e-04 - 11s/epoch - 131us/sample
Epoch 35/90
84077/84077 - 11s - loss: 4.7079e-04 - val_loss: 4.3204e-04 - 11s/epoch - 130us/sample
Epoch 36/90
84077/84077 - 11s - loss: 4.6525e-04 - val_loss: 4.2993e-04 - 11s/epoch - 131us/sample
Epoch 37/90
84077/84077 - 11s - loss: 4.6041e-04 - val_loss: 4.2467e-04 - 11s/epoch - 131us/sample
Epoch 38/90
84077/84077 - 11s - loss: 4.5525e-04 - val_loss: 4.2198e-04 - 11s/epoch - 132us/sample
Epoch 39/90
84077/84077 - 11s - loss: 4.5069e-04 - val_loss: 4.1605e-04 - 11s/epoch - 132us/sample
Epoch 40/90
84077/84077 - 11s - loss: 4.4575e-04 - val_loss: 4.1335e-04 - 11s/epoch - 131us/sample
Epoch 41/90
84077/84077 - 11s - loss: 4.4120e-04 - val_loss: 4.0787e-04 - 11s/epoch - 131us/sample
Epoch 42/90
84077/84077 - 11s - loss: 4.3810e-04 - val_loss: 4.0683e-04 - 11s/epoch - 131us/sample
Epoch 43/90
84077/84077 - 11s - loss: 4.3497e-04 - val_loss: 4.0436e-04 - 11s/epoch - 132us/sample
Epoch 44/90
84077/84077 - 11s - loss: 4.3445e-04 - val_loss: 4.0047e-04 - 11s/epoch - 131us/sample
Epoch 45/90
84077/84077 - 11s - loss: 4.3165e-04 - val_loss: 4.0131e-04 - 11s/epoch - 131us/sample
Epoch 46/90
84077/84077 - 11s - loss: 4.3080e-04 - val_loss: 4.0006e-04 - 11s/epoch - 131us/sample
Epoch 47/90
84077/84077 - 11s - loss: 4.2907e-04 - val_loss: 3.9825e-04 - 11s/epoch - 132us/sample
Epoch 48/90
84077/84077 - 11s - loss: 4.2800e-04 - val_loss: 3.9664e-04 - 11s/epoch - 130us/sample
Epoch 49/90
84077/84077 - 11s - loss: 4.2558e-04 - val_loss: 3.9494e-04 - 11s/epoch - 129us/sample
Epoch 50/90
84077/84077 - 11s - loss: 4.2417e-04 - val_loss: 3.9118e-04 - 11s/epoch - 128us/sample
Epoch 51/90
84077/84077 - 11s - loss: 4.2182e-04 - val_loss: 3.8813e-04 - 11s/epoch - 129us/sample
Epoch 52/90
84077/84077 - 11s - loss: 4.2001e-04 - val_loss: 3.9079e-04 - 11s/epoch - 130us/sample
Epoch 53/90
84077/84077 - 11s - loss: 4.1889e-04 - val_loss: 3.8722e-04 - 11s/epoch - 130us/sample
Epoch 54/90
84077/84077 - 11s - loss: 4.1690e-04 - val_loss: 3.8387e-04 - 11s/epoch - 131us/sample
Epoch 55/90
84077/84077 - 11s - loss: 4.1636e-04 - val_loss: 3.8412e-04 - 11s/epoch - 131us/sample
Epoch 56/90
84077/84077 - 11s - loss: 4.1541e-04 - val_loss: 3.8456e-04 - 11s/epoch - 132us/sample
Epoch 57/90
84077/84077 - 11s - loss: 4.1411e-04 - val_loss: 3.8202e-04 - 11s/epoch - 132us/sample
Epoch 58/90
84077/84077 - 11s - loss: 4.1248e-04 - val_loss: 3.8167e-04 - 11s/epoch - 131us/sample
Epoch 59/90
84077/84077 - 11s - loss: 4.1174e-04 - val_loss: 3.7867e-04 - 11s/epoch - 131us/sample
Epoch 60/90
84077/84077 - 11s - loss: 4.0970e-04 - val_loss: 3.7667e-04 - 11s/epoch - 132us/sample
Epoch 61/90
84077/84077 - 11s - loss: 4.0875e-04 - val_loss: 3.7659e-04 - 11s/epoch - 132us/sample
Epoch 62/90
84077/84077 - 11s - loss: 4.0733e-04 - val_loss: 3.7615e-04 - 11s/epoch - 131us/sample
Epoch 63/90
84077/84077 - 11s - loss: 4.0548e-04 - val_loss: 3.7336e-04 - 11s/epoch - 131us/sample
Epoch 64/90
84077/84077 - 11s - loss: 4.0478e-04 - val_loss: 3.7289e-04 - 11s/epoch - 131us/sample
Epoch 65/90
84077/84077 - 11s - loss: 4.0320e-04 - val_loss: 3.7056e-04 - 11s/epoch - 132us/sample
Epoch 66/90
84077/84077 - 11s - loss: 4.0242e-04 - val_loss: 3.7185e-04 - 11s/epoch - 130us/sample
Epoch 67/90
84077/84077 - 11s - loss: 4.0040e-04 - val_loss: 3.6978e-04 - 11s/epoch - 130us/sample
Epoch 68/90
84077/84077 - 11s - loss: 3.9986e-04 - val_loss: 3.6819e-04 - 11s/epoch - 128us/sample
Epoch 69/90
84077/84077 - 11s - loss: 3.9911e-04 - val_loss: 3.6883e-04 - 11s/epoch - 128us/sample
Epoch 70/90
84077/84077 - 11s - loss: 3.9809e-04 - val_loss: 3.6890e-04 - 11s/epoch - 129us/sample
Epoch 71/90
84077/84077 - 11s - loss: 3.9742e-04 - val_loss: 3.6649e-04 - 11s/epoch - 130us/sample
Epoch 72/90
84077/84077 - 11s - loss: 3.9639e-04 - val_loss: 3.6537e-04 - 11s/epoch - 130us/sample
Epoch 73/90
84077/84077 - 11s - loss: 3.9622e-04 - val_loss: 3.6792e-04 - 11s/epoch - 131us/sample
Epoch 74/90
84077/84077 - 11s - loss: 3.9493e-04 - val_loss: 3.6521e-04 - 11s/epoch - 132us/sample
Epoch 75/90
84077/84077 - 11s - loss: 3.9352e-04 - val_loss: 3.6367e-04 - 11s/epoch - 131us/sample
Epoch 76/90
84077/84077 - 11s - loss: 3.9209e-04 - val_loss: 3.6171e-04 - 11s/epoch - 132us/sample
Epoch 77/90
84077/84077 - 11s - loss: 3.9117e-04 - val_loss: 3.6200e-04 - 11s/epoch - 131us/sample
Epoch 78/90
84077/84077 - 11s - loss: 3.9053e-04 - val_loss: 3.6051e-04 - 11s/epoch - 131us/sample
Epoch 79/90
84077/84077 - 11s - loss: 3.9050e-04 - val_loss: 3.6320e-04 - 11s/epoch - 132us/sample
Epoch 80/90
84077/84077 - 11s - loss: 3.8871e-04 - val_loss: 3.6290e-04 - 11s/epoch - 131us/sample
Epoch 81/90
84077/84077 - 11s - loss: 3.8881e-04 - val_loss: 3.6007e-04 - 11s/epoch - 131us/sample
Epoch 82/90
84077/84077 - 11s - loss: 3.8832e-04 - val_loss: 3.5914e-04 - 11s/epoch - 131us/sample
Epoch 83/90
84077/84077 - 11s - loss: 3.8752e-04 - val_loss: 3.5901e-04 - 11s/epoch - 132us/sample
Epoch 84/90
84077/84077 - 11s - loss: 3.8891e-04 - val_loss: 3.6049e-04 - 11s/epoch - 133us/sample
Epoch 85/90
84077/84077 - 11s - loss: 3.8651e-04 - val_loss: 3.5805e-04 - 11s/epoch - 132us/sample
Epoch 86/90
84077/84077 - 11s - loss: 3.8510e-04 - val_loss: 3.5845e-04 - 11s/epoch - 130us/sample
Epoch 87/90
84077/84077 - 11s - loss: 3.8378e-04 - val_loss: 3.5612e-04 - 11s/epoch - 130us/sample
Epoch 88/90
84077/84077 - 11s - loss: 3.8334e-04 - val_loss: 3.5766e-04 - 11s/epoch - 132us/sample
Epoch 89/90
84077/84077 - 11s - loss: 3.8230e-04 - val_loss: 3.5511e-04 - 11s/epoch - 130us/sample
Epoch 90/90
84077/84077 - 11s - loss: 3.8203e-04 - val_loss: 3.5811e-04 - 11s/epoch - 131us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00035811399820066705
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 07:51:43.978951: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:16946 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.07469303887570251
cosine 0.07375164696204298
MAE: 0.0032528696984771427
RMSE: 0.016371552746536906
r2: 0.7906537545294814
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 90, 0.0008, 0.2, 188, 0.00038202741337835895, 0.00035811399820066705, 0.07469303887570251, 0.07375164696204298, 0.0032528696984771427, 0.016371552746536906, 0.7906537545294814, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 32 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 1980)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 07:51:53.164230: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_42/beta/Assign' id:17822 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_42/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_42/beta, batch_normalization_42/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 07:52:05.521485: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:18237 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 6.2610 - val_loss: 0.0014 - 15s/epoch - 178us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0804 - val_loss: 6.2414e-04 - 11s/epoch - 134us/sample
Epoch 3/90
84077/84077 - 11s - loss: 7.0211e-04 - val_loss: 8.1766e-04 - 11s/epoch - 133us/sample
Epoch 4/90
84077/84077 - 11s - loss: 5.6026e-04 - val_loss: 5.0479e-04 - 11s/epoch - 133us/sample
Epoch 5/90
84077/84077 - 11s - loss: 4.6418e-04 - val_loss: 4.1292e-04 - 11s/epoch - 133us/sample
Epoch 6/90
84077/84077 - 11s - loss: 4.2397e-04 - val_loss: 3.7010e-04 - 11s/epoch - 133us/sample
Epoch 7/90
84077/84077 - 11s - loss: 3.8673e-04 - val_loss: 3.3314e-04 - 11s/epoch - 133us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.6590e-04 - val_loss: 3.2891e-04 - 11s/epoch - 133us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.4291e-04 - val_loss: 3.0150e-04 - 11s/epoch - 133us/sample
Epoch 10/90
84077/84077 - 11s - loss: 3.2705e-04 - val_loss: 2.9139e-04 - 11s/epoch - 135us/sample
Epoch 11/90
84077/84077 - 11s - loss: 3.1832e-04 - val_loss: 2.8355e-04 - 11s/epoch - 135us/sample
Epoch 12/90
84077/84077 - 11s - loss: 3.0823e-04 - val_loss: 2.7639e-04 - 11s/epoch - 134us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.9842e-04 - val_loss: 2.6664e-04 - 11s/epoch - 135us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.9142e-04 - val_loss: 2.6102e-04 - 11s/epoch - 132us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.8530e-04 - val_loss: 2.5965e-04 - 11s/epoch - 133us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.8237e-04 - val_loss: 2.5190e-04 - 11s/epoch - 133us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.7526e-04 - val_loss: 2.5120e-04 - 11s/epoch - 132us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.7275e-04 - val_loss: 2.4557e-04 - 11s/epoch - 133us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.6938e-04 - val_loss: 2.4347e-04 - 11s/epoch - 132us/sample
Epoch 20/90
84077/84077 - 11s - loss: 2.6688e-04 - val_loss: 2.4041e-04 - 11s/epoch - 132us/sample
Epoch 21/90
84077/84077 - 11s - loss: 2.6372e-04 - val_loss: 2.3672e-04 - 11s/epoch - 134us/sample
Epoch 22/90
84077/84077 - 11s - loss: 2.6051e-04 - val_loss: 2.3702e-04 - 11s/epoch - 132us/sample
Epoch 23/90
84077/84077 - 11s - loss: 2.5836e-04 - val_loss: 2.3247e-04 - 11s/epoch - 132us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.5494e-04 - val_loss: 2.3210e-04 - 11s/epoch - 132us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.5294e-04 - val_loss: 2.2780e-04 - 11s/epoch - 133us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.5047e-04 - val_loss: 2.2623e-04 - 11s/epoch - 133us/sample
Epoch 27/90
84077/84077 - 11s - loss: 2.4722e-04 - val_loss: 2.2246e-04 - 11s/epoch - 131us/sample
Epoch 28/90
84077/84077 - 11s - loss: 2.4532e-04 - val_loss: 2.1993e-04 - 11s/epoch - 131us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.4318e-04 - val_loss: 2.1808e-04 - 11s/epoch - 131us/sample
Epoch 30/90
84077/84077 - 11s - loss: 2.3993e-04 - val_loss: 2.1530e-04 - 11s/epoch - 131us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.3804e-04 - val_loss: 2.1338e-04 - 11s/epoch - 131us/sample
Epoch 32/90
84077/84077 - 11s - loss: 2.3493e-04 - val_loss: 2.1240e-04 - 11s/epoch - 132us/sample
Epoch 33/90
84077/84077 - 11s - loss: 2.3274e-04 - val_loss: 2.0784e-04 - 11s/epoch - 132us/sample
Epoch 34/90
84077/84077 - 11s - loss: 2.3114e-04 - val_loss: 2.0625e-04 - 11s/epoch - 133us/sample
Epoch 35/90
84077/84077 - 11s - loss: 2.2934e-04 - val_loss: 2.0566e-04 - 11s/epoch - 133us/sample
Epoch 36/90
84077/84077 - 11s - loss: 2.2681e-04 - val_loss: 2.0635e-04 - 11s/epoch - 132us/sample
Epoch 37/90
84077/84077 - 11s - loss: 2.2582e-04 - val_loss: 2.0397e-04 - 11s/epoch - 132us/sample
Epoch 38/90
84077/84077 - 11s - loss: 2.2413e-04 - val_loss: 2.0086e-04 - 11s/epoch - 132us/sample
Epoch 39/90
84077/84077 - 11s - loss: 2.2266e-04 - val_loss: 2.0204e-04 - 11s/epoch - 132us/sample
Epoch 40/90
84077/84077 - 11s - loss: 2.2091e-04 - val_loss: 1.9845e-04 - 11s/epoch - 133us/sample
Epoch 41/90
84077/84077 - 11s - loss: 2.1949e-04 - val_loss: 1.9713e-04 - 11s/epoch - 132us/sample
Epoch 42/90
84077/84077 - 11s - loss: 2.1798e-04 - val_loss: 1.9558e-04 - 11s/epoch - 132us/sample
Epoch 43/90
84077/84077 - 11s - loss: 2.1664e-04 - val_loss: 1.9412e-04 - 11s/epoch - 132us/sample
Epoch 44/90
84077/84077 - 11s - loss: 2.1535e-04 - val_loss: 1.9173e-04 - 11s/epoch - 133us/sample
Epoch 45/90
84077/84077 - 11s - loss: 2.1422e-04 - val_loss: 1.9491e-04 - 11s/epoch - 132us/sample
Epoch 46/90
84077/84077 - 11s - loss: 2.1259e-04 - val_loss: 1.9052e-04 - 11s/epoch - 134us/sample
Epoch 47/90
84077/84077 - 11s - loss: 2.1181e-04 - val_loss: 1.9108e-04 - 11s/epoch - 132us/sample
Epoch 48/90
84077/84077 - 11s - loss: 2.1076e-04 - val_loss: 1.9138e-04 - 11s/epoch - 132us/sample
Epoch 49/90
84077/84077 - 11s - loss: 2.1009e-04 - val_loss: 1.8622e-04 - 11s/epoch - 133us/sample
Epoch 50/90
84077/84077 - 11s - loss: 2.0933e-04 - val_loss: 1.8862e-04 - 11s/epoch - 133us/sample
Epoch 51/90
84077/84077 - 11s - loss: 2.0795e-04 - val_loss: 1.8612e-04 - 11s/epoch - 132us/sample
Epoch 52/90
84077/84077 - 11s - loss: 2.0645e-04 - val_loss: 1.8637e-04 - 11s/epoch - 132us/sample
Epoch 53/90
84077/84077 - 11s - loss: 2.0562e-04 - val_loss: 1.8411e-04 - 11s/epoch - 132us/sample
Epoch 54/90
84077/84077 - 11s - loss: 2.0427e-04 - val_loss: 1.8418e-04 - 11s/epoch - 133us/sample
Epoch 55/90
84077/84077 - 11s - loss: 2.0351e-04 - val_loss: 1.8402e-04 - 11s/epoch - 133us/sample
Epoch 56/90
84077/84077 - 11s - loss: 2.0246e-04 - val_loss: 1.8279e-04 - 11s/epoch - 132us/sample
Epoch 57/90
84077/84077 - 11s - loss: 2.0267e-04 - val_loss: 1.8131e-04 - 11s/epoch - 132us/sample
Epoch 58/90
84077/84077 - 11s - loss: 2.0168e-04 - val_loss: 1.8105e-04 - 11s/epoch - 132us/sample
Epoch 59/90
84077/84077 - 11s - loss: 2.0086e-04 - val_loss: 1.7939e-04 - 11s/epoch - 133us/sample
Epoch 60/90
84077/84077 - 11s - loss: 2.0012e-04 - val_loss: 1.8001e-04 - 11s/epoch - 132us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.9873e-04 - val_loss: 1.7982e-04 - 11s/epoch - 132us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.9872e-04 - val_loss: 1.8003e-04 - 11s/epoch - 132us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.9701e-04 - val_loss: 1.7743e-04 - 11s/epoch - 133us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.9669e-04 - val_loss: 1.7807e-04 - 11s/epoch - 133us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.9639e-04 - val_loss: 1.7815e-04 - 11s/epoch - 132us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.9564e-04 - val_loss: 1.7646e-04 - 11s/epoch - 134us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.9534e-04 - val_loss: 1.7589e-04 - 11s/epoch - 133us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.9498e-04 - val_loss: 1.7482e-04 - 11s/epoch - 133us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.9361e-04 - val_loss: 1.7489e-04 - 11s/epoch - 133us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.9309e-04 - val_loss: 1.7369e-04 - 11s/epoch - 133us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.9297e-04 - val_loss: 1.7311e-04 - 11s/epoch - 132us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.9237e-04 - val_loss: 1.7168e-04 - 11s/epoch - 132us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.9130e-04 - val_loss: 1.7243e-04 - 11s/epoch - 134us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.9132e-04 - val_loss: 1.7353e-04 - 11s/epoch - 132us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.9145e-04 - val_loss: 1.7326e-04 - 11s/epoch - 132us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.9064e-04 - val_loss: 1.7418e-04 - 11s/epoch - 132us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.9053e-04 - val_loss: 1.7096e-04 - 11s/epoch - 132us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.8973e-04 - val_loss: 1.7980e-04 - 11s/epoch - 134us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.8920e-04 - val_loss: 1.7180e-04 - 11s/epoch - 133us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.9029e-04 - val_loss: 1.7000e-04 - 11s/epoch - 133us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.8949e-04 - val_loss: 1.6959e-04 - 11s/epoch - 132us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.8842e-04 - val_loss: 1.6993e-04 - 11s/epoch - 132us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.8866e-04 - val_loss: 1.6982e-04 - 11s/epoch - 133us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.8807e-04 - val_loss: 1.6845e-04 - 11s/epoch - 133us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.8714e-04 - val_loss: 1.6822e-04 - 11s/epoch - 131us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.8718e-04 - val_loss: 1.6885e-04 - 11s/epoch - 132us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.8697e-04 - val_loss: 1.7018e-04 - 11s/epoch - 132us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.8676e-04 - val_loss: 1.6869e-04 - 11s/epoch - 132us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.8671e-04 - val_loss: 1.6995e-04 - 11s/epoch - 133us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.8626e-04 - val_loss: 1.7038e-04 - 11s/epoch - 133us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017038303864123892
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 08:08:39.349046: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:18201 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05197916092660465
cosine 0.0513440633913377
MAE: 0.0029114620623086613
RMSE: 0.013755955086252266
r2: 0.852205174250595
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 32, 90, 0.001, 0.2, 188, 0.00018626109371585006, 0.00017038303864123892, 0.05197916092660465, 0.0513440633913377, 0.0029114620623086613, 0.013755955086252266, 0.852205174250595, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 1886)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 08:08:48.711933: W tensorflow/c/c_api.cc:291] Operation '{name:'training_30/Adam/batch_normalization_46/beta/m/Assign' id:20035 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/batch_normalization_46/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_30/Adam/batch_normalization_46/beta/m, training_30/Adam/batch_normalization_46/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 08:08:57.242355: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:19515 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 11s - loss: 0.0076 - val_loss: 0.0015 - 11s/epoch - 131us/sample
Epoch 2/90
84077/84077 - 7s - loss: 0.0015 - val_loss: 0.0013 - 7s/epoch - 87us/sample
Epoch 3/90
84077/84077 - 7s - loss: 0.0021 - val_loss: 0.0014 - 7s/epoch - 87us/sample
Epoch 4/90
84077/84077 - 7s - loss: 0.0026 - val_loss: 0.0014 - 7s/epoch - 87us/sample
Epoch 5/90
84077/84077 - 7s - loss: 0.0011 - val_loss: 0.0011 - 7s/epoch - 87us/sample
Epoch 6/90
84077/84077 - 7s - loss: 8.8545e-04 - val_loss: 7.6728e-04 - 7s/epoch - 87us/sample
Epoch 7/90
84077/84077 - 7s - loss: 7.9208e-04 - val_loss: 6.5758e-04 - 7s/epoch - 88us/sample
Epoch 8/90
84077/84077 - 7s - loss: 6.6169e-04 - val_loss: 5.4982e-04 - 7s/epoch - 87us/sample
Epoch 9/90
84077/84077 - 7s - loss: 6.0356e-04 - val_loss: 4.7716e-04 - 7s/epoch - 87us/sample
Epoch 10/90
84077/84077 - 7s - loss: 5.7753e-04 - val_loss: 4.4409e-04 - 7s/epoch - 87us/sample
Epoch 11/90
84077/84077 - 7s - loss: 4.7479e-04 - val_loss: 3.8509e-04 - 7s/epoch - 87us/sample
Epoch 12/90
84077/84077 - 7s - loss: 4.1981e-04 - val_loss: 3.5028e-04 - 7s/epoch - 87us/sample
Epoch 13/90
84077/84077 - 7s - loss: 3.8122e-04 - val_loss: 3.1900e-04 - 7s/epoch - 87us/sample
Epoch 14/90
84077/84077 - 7s - loss: 3.5577e-04 - val_loss: 2.9555e-04 - 7s/epoch - 88us/sample
Epoch 15/90
84077/84077 - 7s - loss: 3.3555e-04 - val_loss: 2.9045e-04 - 7s/epoch - 88us/sample
Epoch 16/90
84077/84077 - 7s - loss: 3.1774e-04 - val_loss: 2.6909e-04 - 7s/epoch - 87us/sample
Epoch 17/90
84077/84077 - 7s - loss: 3.0463e-04 - val_loss: 2.6783e-04 - 7s/epoch - 87us/sample
Epoch 18/90
84077/84077 - 7s - loss: 2.9393e-04 - val_loss: 2.5737e-04 - 7s/epoch - 88us/sample
Epoch 19/90
84077/84077 - 7s - loss: 2.8494e-04 - val_loss: 2.4844e-04 - 7s/epoch - 87us/sample
Epoch 20/90
84077/84077 - 7s - loss: 2.7679e-04 - val_loss: 2.4562e-04 - 7s/epoch - 87us/sample
Epoch 21/90
84077/84077 - 7s - loss: 2.6961e-04 - val_loss: 2.3746e-04 - 7s/epoch - 87us/sample
Epoch 22/90
84077/84077 - 7s - loss: 2.6188e-04 - val_loss: 2.3601e-04 - 7s/epoch - 89us/sample
Epoch 23/90
84077/84077 - 7s - loss: 2.5733e-04 - val_loss: 2.2861e-04 - 7s/epoch - 88us/sample
Epoch 24/90
84077/84077 - 7s - loss: 2.5338e-04 - val_loss: 2.2756e-04 - 7s/epoch - 88us/sample
Epoch 25/90
84077/84077 - 7s - loss: 2.4808e-04 - val_loss: 2.2329e-04 - 7s/epoch - 87us/sample
Epoch 26/90
84077/84077 - 7s - loss: 2.4608e-04 - val_loss: 2.2259e-04 - 7s/epoch - 88us/sample
Epoch 27/90
84077/84077 - 7s - loss: 2.4203e-04 - val_loss: 2.1988e-04 - 7s/epoch - 87us/sample
Epoch 28/90
84077/84077 - 7s - loss: 2.3857e-04 - val_loss: 2.1671e-04 - 7s/epoch - 87us/sample
Epoch 29/90
84077/84077 - 7s - loss: 2.3794e-04 - val_loss: 2.1674e-04 - 7s/epoch - 88us/sample
Epoch 30/90
84077/84077 - 7s - loss: 2.3383e-04 - val_loss: 2.1299e-04 - 7s/epoch - 87us/sample
Epoch 31/90
84077/84077 - 7s - loss: 2.3059e-04 - val_loss: 2.1205e-04 - 7s/epoch - 87us/sample
Epoch 32/90
84077/84077 - 7s - loss: 2.2905e-04 - val_loss: 2.1251e-04 - 7s/epoch - 87us/sample
Epoch 33/90
84077/84077 - 7s - loss: 2.2730e-04 - val_loss: 2.1036e-04 - 7s/epoch - 87us/sample
Epoch 34/90
84077/84077 - 7s - loss: 2.2493e-04 - val_loss: 2.0774e-04 - 7s/epoch - 87us/sample
Epoch 35/90
84077/84077 - 7s - loss: 2.2365e-04 - val_loss: 2.0665e-04 - 7s/epoch - 89us/sample
Epoch 36/90
84077/84077 - 7s - loss: 2.2137e-04 - val_loss: 2.0620e-04 - 7s/epoch - 88us/sample
Epoch 37/90
84077/84077 - 7s - loss: 2.2004e-04 - val_loss: 2.0459e-04 - 7s/epoch - 88us/sample
Epoch 38/90
84077/84077 - 7s - loss: 2.1924e-04 - val_loss: 2.0338e-04 - 7s/epoch - 87us/sample
Epoch 39/90
84077/84077 - 7s - loss: 2.1676e-04 - val_loss: 2.0026e-04 - 7s/epoch - 88us/sample
Epoch 40/90
84077/84077 - 7s - loss: 2.1968e-04 - val_loss: 2.0212e-04 - 7s/epoch - 87us/sample
Epoch 41/90
84077/84077 - 7s - loss: 2.1561e-04 - val_loss: 1.9997e-04 - 7s/epoch - 87us/sample
Epoch 42/90
84077/84077 - 7s - loss: 2.1357e-04 - val_loss: 1.9877e-04 - 7s/epoch - 88us/sample
Epoch 43/90
84077/84077 - 7s - loss: 2.1252e-04 - val_loss: 2.0038e-04 - 7s/epoch - 87us/sample
Epoch 44/90
84077/84077 - 7s - loss: 2.1144e-04 - val_loss: 2.0457e-04 - 7s/epoch - 87us/sample
Epoch 45/90
84077/84077 - 7s - loss: 2.0971e-04 - val_loss: 1.9709e-04 - 7s/epoch - 88us/sample
Epoch 46/90
84077/84077 - 7s - loss: 2.0968e-04 - val_loss: 1.9481e-04 - 7s/epoch - 88us/sample
Epoch 47/90
84077/84077 - 7s - loss: 2.0860e-04 - val_loss: 1.9432e-04 - 7s/epoch - 87us/sample
Epoch 48/90
84077/84077 - 7s - loss: 2.0720e-04 - val_loss: 1.9505e-04 - 7s/epoch - 88us/sample
Epoch 49/90
84077/84077 - 7s - loss: 2.0607e-04 - val_loss: 1.9490e-04 - 7s/epoch - 88us/sample
Epoch 50/90
84077/84077 - 7s - loss: 2.0656e-04 - val_loss: 1.9353e-04 - 7s/epoch - 87us/sample
Epoch 51/90
84077/84077 - 7s - loss: 2.0402e-04 - val_loss: 1.9363e-04 - 7s/epoch - 87us/sample
Epoch 52/90
84077/84077 - 7s - loss: 2.0498e-04 - val_loss: 1.9255e-04 - 7s/epoch - 87us/sample
Epoch 53/90
84077/84077 - 7s - loss: 2.0321e-04 - val_loss: 1.9090e-04 - 7s/epoch - 87us/sample
Epoch 54/90
84077/84077 - 7s - loss: 2.0409e-04 - val_loss: 1.9432e-04 - 7s/epoch - 87us/sample
Epoch 55/90
84077/84077 - 7s - loss: 2.0236e-04 - val_loss: 1.9144e-04 - 7s/epoch - 88us/sample
Epoch 56/90
84077/84077 - 7s - loss: 2.0207e-04 - val_loss: 1.8994e-04 - 7s/epoch - 88us/sample
Epoch 57/90
84077/84077 - 7s - loss: 2.0116e-04 - val_loss: 1.8901e-04 - 7s/epoch - 87us/sample
Epoch 58/90
84077/84077 - 7s - loss: 2.0094e-04 - val_loss: 1.9127e-04 - 7s/epoch - 87us/sample
Epoch 59/90
84077/84077 - 7s - loss: 1.9895e-04 - val_loss: 1.8761e-04 - 7s/epoch - 87us/sample
Epoch 60/90
84077/84077 - 7s - loss: 1.9865e-04 - val_loss: 1.8686e-04 - 7s/epoch - 87us/sample
Epoch 61/90
84077/84077 - 7s - loss: 1.9850e-04 - val_loss: 1.8436e-04 - 7s/epoch - 87us/sample
Epoch 62/90
84077/84077 - 7s - loss: 1.9732e-04 - val_loss: 1.8592e-04 - 7s/epoch - 87us/sample
Epoch 63/90
84077/84077 - 7s - loss: 1.9728e-04 - val_loss: 1.8729e-04 - 7s/epoch - 88us/sample
Epoch 64/90
84077/84077 - 7s - loss: 1.9627e-04 - val_loss: 1.8509e-04 - 7s/epoch - 88us/sample
Epoch 65/90
84077/84077 - 7s - loss: 1.9638e-04 - val_loss: 1.8401e-04 - 7s/epoch - 87us/sample
Epoch 66/90
84077/84077 - 7s - loss: 1.9587e-04 - val_loss: 1.8357e-04 - 7s/epoch - 87us/sample
Epoch 67/90
84077/84077 - 7s - loss: 1.9489e-04 - val_loss: 1.8314e-04 - 7s/epoch - 88us/sample
Epoch 68/90
84077/84077 - 7s - loss: 1.9444e-04 - val_loss: 1.8276e-04 - 7s/epoch - 87us/sample
Epoch 69/90
84077/84077 - 7s - loss: 1.9454e-04 - val_loss: 1.8196e-04 - 7s/epoch - 87us/sample
Epoch 70/90
84077/84077 - 7s - loss: 1.9327e-04 - val_loss: 1.8314e-04 - 7s/epoch - 88us/sample
Epoch 71/90
84077/84077 - 7s - loss: 1.9317e-04 - val_loss: 1.8085e-04 - 7s/epoch - 87us/sample
Epoch 72/90
84077/84077 - 7s - loss: 1.9326e-04 - val_loss: 1.8424e-04 - 7s/epoch - 87us/sample
Epoch 73/90
84077/84077 - 7s - loss: 1.9279e-04 - val_loss: 1.8338e-04 - 7s/epoch - 87us/sample
Epoch 74/90
84077/84077 - 7s - loss: 1.9148e-04 - val_loss: 1.8058e-04 - 7s/epoch - 87us/sample
Epoch 75/90
84077/84077 - 7s - loss: 1.9131e-04 - val_loss: 1.8375e-04 - 7s/epoch - 88us/sample
Epoch 76/90
84077/84077 - 7s - loss: 1.9273e-04 - val_loss: 1.8257e-04 - 7s/epoch - 87us/sample
Epoch 77/90
84077/84077 - 7s - loss: 1.9117e-04 - val_loss: 1.7799e-04 - 7s/epoch - 88us/sample
Epoch 78/90
84077/84077 - 7s - loss: 1.9097e-04 - val_loss: 1.7761e-04 - 7s/epoch - 88us/sample
Epoch 79/90
84077/84077 - 7s - loss: 1.8958e-04 - val_loss: 1.7889e-04 - 7s/epoch - 87us/sample
Epoch 80/90
84077/84077 - 7s - loss: 1.8970e-04 - val_loss: 1.8143e-04 - 7s/epoch - 87us/sample
Epoch 81/90
84077/84077 - 7s - loss: 1.8996e-04 - val_loss: 1.7699e-04 - 7s/epoch - 87us/sample
Epoch 82/90
84077/84077 - 7s - loss: 1.8930e-04 - val_loss: 1.8130e-04 - 7s/epoch - 87us/sample
Epoch 83/90
84077/84077 - 7s - loss: 1.9037e-04 - val_loss: 1.7795e-04 - 7s/epoch - 88us/sample
Epoch 84/90
84077/84077 - 7s - loss: 1.8816e-04 - val_loss: 1.7537e-04 - 7s/epoch - 88us/sample
Epoch 85/90
84077/84077 - 7s - loss: 1.8793e-04 - val_loss: 1.7515e-04 - 7s/epoch - 87us/sample
Epoch 86/90
84077/84077 - 7s - loss: 1.9241e-04 - val_loss: 1.8430e-04 - 7s/epoch - 88us/sample
Epoch 87/90
84077/84077 - 7s - loss: 1.8902e-04 - val_loss: 1.7512e-04 - 7s/epoch - 87us/sample
Epoch 88/90
84077/84077 - 7s - loss: 1.8774e-04 - val_loss: 1.7816e-04 - 7s/epoch - 87us/sample
Epoch 89/90
84077/84077 - 7s - loss: 1.8715e-04 - val_loss: 1.7822e-04 - 7s/epoch - 87us/sample
Epoch 90/90
84077/84077 - 7s - loss: 1.8721e-04 - val_loss: 1.7568e-04 - 7s/epoch - 87us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017567706146489633
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 08:19:52.563788: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:19486 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02125064535723798
cosine 0.02100439796580689
MAE: 0.0019515840055625545
RMSE: 0.00850673163449846
r2: 0.9437134021011517
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 90, 0.001, 0.2, 188, 0.00018721140285742185, 0.00017567706146489633, 0.02125064535723798, 0.02100439796580689, 0.0019515840055625545, 0.00850673163449846, 0.9437134021011517, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 1791)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 08:20:02.336709: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/beta_1/Assign' id:21204 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/beta_1, training_32/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 08:20:14.802696: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:20770 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0064 - val_loss: 0.0018 - 15s/epoch - 182us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0019 - val_loss: 0.0012 - 11s/epoch - 133us/sample
Epoch 3/90
84077/84077 - 11s - loss: 0.0011 - val_loss: 8.9672e-04 - 11s/epoch - 133us/sample
Epoch 4/90
84077/84077 - 11s - loss: 0.0011 - val_loss: 7.6833e-04 - 11s/epoch - 133us/sample
Epoch 5/90
84077/84077 - 11s - loss: 7.8693e-04 - val_loss: 5.9091e-04 - 11s/epoch - 133us/sample
Epoch 6/90
84077/84077 - 11s - loss: 6.3242e-04 - val_loss: 5.0250e-04 - 11s/epoch - 134us/sample
Epoch 7/90
84077/84077 - 11s - loss: 5.3698e-04 - val_loss: 4.3891e-04 - 11s/epoch - 133us/sample
Epoch 8/90
84077/84077 - 11s - loss: 4.6920e-04 - val_loss: 3.9445e-04 - 11s/epoch - 133us/sample
Epoch 9/90
84077/84077 - 11s - loss: 4.2283e-04 - val_loss: 3.6261e-04 - 11s/epoch - 133us/sample
Epoch 10/90
84077/84077 - 11s - loss: 3.9092e-04 - val_loss: 3.3465e-04 - 11s/epoch - 132us/sample
Epoch 11/90
84077/84077 - 11s - loss: 3.6478e-04 - val_loss: 3.1852e-04 - 11s/epoch - 133us/sample
Epoch 12/90
84077/84077 - 11s - loss: 3.4677e-04 - val_loss: 3.0778e-04 - 11s/epoch - 133us/sample
Epoch 13/90
84077/84077 - 11s - loss: 3.3034e-04 - val_loss: 2.9393e-04 - 11s/epoch - 133us/sample
Epoch 14/90
84077/84077 - 11s - loss: 3.1762e-04 - val_loss: 2.8452e-04 - 11s/epoch - 133us/sample
Epoch 15/90
84077/84077 - 11s - loss: 3.0872e-04 - val_loss: 2.8045e-04 - 11s/epoch - 133us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.9924e-04 - val_loss: 2.6923e-04 - 11s/epoch - 134us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.9262e-04 - val_loss: 2.6836e-04 - 11s/epoch - 134us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.8658e-04 - val_loss: 2.6391e-04 - 11s/epoch - 133us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.8239e-04 - val_loss: 2.5709e-04 - 11s/epoch - 133us/sample
Epoch 20/90
84077/84077 - 11s - loss: 2.7766e-04 - val_loss: 2.5657e-04 - 11s/epoch - 134us/sample
Epoch 21/90
84077/84077 - 11s - loss: 2.7315e-04 - val_loss: 2.5388e-04 - 11s/epoch - 134us/sample
Epoch 22/90
84077/84077 - 11s - loss: 2.7057e-04 - val_loss: 2.5429e-04 - 11s/epoch - 133us/sample
Epoch 23/90
84077/84077 - 11s - loss: 2.6639e-04 - val_loss: 2.4475e-04 - 11s/epoch - 133us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.6333e-04 - val_loss: 2.4394e-04 - 11s/epoch - 134us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.6156e-04 - val_loss: 2.3942e-04 - 11s/epoch - 133us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.5904e-04 - val_loss: 2.4502e-04 - 11s/epoch - 133us/sample
Epoch 27/90
84077/84077 - 11s - loss: 2.5609e-04 - val_loss: 2.3791e-04 - 11s/epoch - 133us/sample
Epoch 28/90
84077/84077 - 11s - loss: 2.5458e-04 - val_loss: 2.3581e-04 - 11s/epoch - 133us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.5209e-04 - val_loss: 2.3104e-04 - 11s/epoch - 134us/sample
Epoch 30/90
84077/84077 - 11s - loss: 2.5114e-04 - val_loss: 2.3816e-04 - 11s/epoch - 133us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.4792e-04 - val_loss: 2.3373e-04 - 11s/epoch - 133us/sample
Epoch 32/90
84077/84077 - 11s - loss: 2.4750e-04 - val_loss: 2.3074e-04 - 11s/epoch - 133us/sample
Epoch 33/90
84077/84077 - 11s - loss: 2.4686e-04 - val_loss: 2.3236e-04 - 11s/epoch - 134us/sample
Epoch 34/90
84077/84077 - 11s - loss: 2.4431e-04 - val_loss: 2.2744e-04 - 11s/epoch - 134us/sample
Epoch 35/90
84077/84077 - 11s - loss: 2.4371e-04 - val_loss: 2.3197e-04 - 11s/epoch - 132us/sample
Epoch 36/90
84077/84077 - 11s - loss: 2.4201e-04 - val_loss: 2.2532e-04 - 11s/epoch - 132us/sample
Epoch 37/90
84077/84077 - 11s - loss: 2.4008e-04 - val_loss: 2.2812e-04 - 11s/epoch - 135us/sample
Epoch 38/90
84077/84077 - 11s - loss: 2.3838e-04 - val_loss: 2.2688e-04 - 11s/epoch - 134us/sample
Epoch 39/90
84077/84077 - 11s - loss: 2.3902e-04 - val_loss: 2.2440e-04 - 11s/epoch - 133us/sample
Epoch 40/90
84077/84077 - 11s - loss: 2.3867e-04 - val_loss: 2.2725e-04 - 11s/epoch - 133us/sample
Epoch 41/90
84077/84077 - 11s - loss: 2.3552e-04 - val_loss: 2.2208e-04 - 11s/epoch - 133us/sample
Epoch 42/90
84077/84077 - 11s - loss: 2.3576e-04 - val_loss: 2.1916e-04 - 11s/epoch - 134us/sample
Epoch 43/90
84077/84077 - 11s - loss: 2.3500e-04 - val_loss: 2.2317e-04 - 11s/epoch - 133us/sample
Epoch 44/90
84077/84077 - 11s - loss: 2.3418e-04 - val_loss: 2.2331e-04 - 11s/epoch - 133us/sample
Epoch 45/90
84077/84077 - 11s - loss: 2.3343e-04 - val_loss: 2.1831e-04 - 11s/epoch - 132us/sample
Epoch 46/90
84077/84077 - 11s - loss: 2.3200e-04 - val_loss: 2.1497e-04 - 11s/epoch - 134us/sample
Epoch 47/90
84077/84077 - 11s - loss: 2.3200e-04 - val_loss: 2.2019e-04 - 11s/epoch - 133us/sample
Epoch 48/90
84077/84077 - 11s - loss: 2.3234e-04 - val_loss: 2.1932e-04 - 11s/epoch - 133us/sample
Epoch 49/90
84077/84077 - 11s - loss: 2.3128e-04 - val_loss: 2.2188e-04 - 11s/epoch - 133us/sample
Epoch 50/90
84077/84077 - 11s - loss: 2.3011e-04 - val_loss: 2.1766e-04 - 11s/epoch - 133us/sample
Epoch 51/90
84077/84077 - 11s - loss: 2.2928e-04 - val_loss: 2.1793e-04 - 11s/epoch - 134us/sample
Epoch 52/90
84077/84077 - 11s - loss: 2.2907e-04 - val_loss: 2.1699e-04 - 11s/epoch - 133us/sample
Epoch 53/90
84077/84077 - 11s - loss: 2.2851e-04 - val_loss: 2.1492e-04 - 11s/epoch - 132us/sample
Epoch 54/90
84077/84077 - 11s - loss: 2.2773e-04 - val_loss: 2.1835e-04 - 11s/epoch - 132us/sample
Epoch 55/90
84077/84077 - 11s - loss: 2.2695e-04 - val_loss: 2.1484e-04 - 11s/epoch - 135us/sample
Epoch 56/90
84077/84077 - 11s - loss: 2.2622e-04 - val_loss: 2.1572e-04 - 11s/epoch - 134us/sample
Epoch 57/90
84077/84077 - 11s - loss: 2.2660e-04 - val_loss: 2.1158e-04 - 11s/epoch - 134us/sample
Epoch 58/90
84077/84077 - 11s - loss: 2.2538e-04 - val_loss: 2.1233e-04 - 11s/epoch - 133us/sample
Epoch 59/90
84077/84077 - 11s - loss: 2.2408e-04 - val_loss: 2.1193e-04 - 11s/epoch - 134us/sample
Epoch 60/90
84077/84077 - 11s - loss: 2.2491e-04 - val_loss: 2.1497e-04 - 11s/epoch - 134us/sample
Epoch 61/90
84077/84077 - 11s - loss: 2.2357e-04 - val_loss: 2.1626e-04 - 11s/epoch - 133us/sample
Epoch 62/90
84077/84077 - 11s - loss: 2.2325e-04 - val_loss: 2.1277e-04 - 11s/epoch - 132us/sample
Epoch 63/90
84077/84077 - 11s - loss: 2.2279e-04 - val_loss: 2.1066e-04 - 11s/epoch - 133us/sample
Epoch 64/90
84077/84077 - 11s - loss: 2.2283e-04 - val_loss: 2.0923e-04 - 11s/epoch - 133us/sample
Epoch 65/90
84077/84077 - 11s - loss: 2.2283e-04 - val_loss: 2.0989e-04 - 11s/epoch - 134us/sample
Epoch 66/90
84077/84077 - 11s - loss: 2.2316e-04 - val_loss: 2.1101e-04 - 11s/epoch - 133us/sample
Epoch 67/90
84077/84077 - 11s - loss: 2.2077e-04 - val_loss: 2.1220e-04 - 11s/epoch - 133us/sample
Epoch 68/90
84077/84077 - 11s - loss: 2.2093e-04 - val_loss: 2.1017e-04 - 11s/epoch - 133us/sample
Epoch 69/90
84077/84077 - 11s - loss: 2.1999e-04 - val_loss: 2.0704e-04 - 11s/epoch - 135us/sample
Epoch 70/90
84077/84077 - 11s - loss: 2.2099e-04 - val_loss: 2.1086e-04 - 11s/epoch - 134us/sample
Epoch 71/90
84077/84077 - 11s - loss: 2.2091e-04 - val_loss: 2.1398e-04 - 11s/epoch - 133us/sample
Epoch 72/90
84077/84077 - 11s - loss: 2.2042e-04 - val_loss: 2.1280e-04 - 11s/epoch - 133us/sample
Epoch 73/90
84077/84077 - 11s - loss: 2.1852e-04 - val_loss: 2.0818e-04 - 11s/epoch - 135us/sample
Epoch 74/90
84077/84077 - 11s - loss: 2.2061e-04 - val_loss: 2.0661e-04 - 11s/epoch - 134us/sample
Epoch 75/90
84077/84077 - 11s - loss: 2.1935e-04 - val_loss: 2.0623e-04 - 11s/epoch - 133us/sample
Epoch 76/90
84077/84077 - 11s - loss: 2.1886e-04 - val_loss: 2.0718e-04 - 11s/epoch - 133us/sample
Epoch 77/90
84077/84077 - 11s - loss: 2.1796e-04 - val_loss: 2.0884e-04 - 11s/epoch - 134us/sample
Epoch 78/90
84077/84077 - 11s - loss: 2.1744e-04 - val_loss: 2.0530e-04 - 11s/epoch - 133us/sample
Epoch 79/90
84077/84077 - 11s - loss: 2.1829e-04 - val_loss: 2.0801e-04 - 11s/epoch - 132us/sample
Epoch 80/90
84077/84077 - 11s - loss: 2.1710e-04 - val_loss: 2.0478e-04 - 11s/epoch - 133us/sample
Epoch 81/90
84077/84077 - 11s - loss: 2.1690e-04 - val_loss: 2.0888e-04 - 11s/epoch - 133us/sample
Epoch 82/90
84077/84077 - 11s - loss: 2.1643e-04 - val_loss: 2.0693e-04 - 11s/epoch - 134us/sample
Epoch 83/90
84077/84077 - 11s - loss: 2.1683e-04 - val_loss: 2.0481e-04 - 11s/epoch - 132us/sample
Epoch 84/90
84077/84077 - 11s - loss: 2.1674e-04 - val_loss: 2.0890e-04 - 11s/epoch - 134us/sample
Epoch 85/90
84077/84077 - 11s - loss: 2.1633e-04 - val_loss: 2.0601e-04 - 11s/epoch - 133us/sample
Epoch 86/90
84077/84077 - 11s - loss: 2.1674e-04 - val_loss: 2.0502e-04 - 11s/epoch - 133us/sample
Epoch 87/90
84077/84077 - 11s - loss: 2.1597e-04 - val_loss: 2.0395e-04 - 11s/epoch - 133us/sample
Epoch 88/90
84077/84077 - 11s - loss: 2.1573e-04 - val_loss: 2.0545e-04 - 11s/epoch - 134us/sample
Epoch 89/90
84077/84077 - 11s - loss: 2.1507e-04 - val_loss: 2.1081e-04 - 11s/epoch - 134us/sample
Epoch 90/90
84077/84077 - 11s - loss: 2.1495e-04 - val_loss: 2.0153e-04 - 11s/epoch - 133us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00020153424008167723
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 08:36:53.988665: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:20741 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.025672522877924698
cosine 0.025362091715842413
MAE: 0.0020674838013001835
RMSE: 0.009806156635671873
r2: 0.9249759484445905
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 32, 90, 0.001, 0.2, 188, 0.00021494976691056118, 0.00020153424008167723, 0.025672522877924698, 0.025362091715842413, 0.0020674838013001835, 0.009806156635671873, 0.9249759484445905, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 512.1418577388646
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 512.1418577388646.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706]
[1.9 90 0.001 8 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 1791)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 08:37:04.427355: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_52/gamma/Assign' id:21755 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_52/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_52/gamma, batch_normalization_52/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 08:37:38.986140: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:22028 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 39s - loss: 0.0055 - val_loss: 0.0012 - 39s/epoch - 462us/sample
Epoch 2/90
84077/84077 - 34s - loss: 0.0012 - val_loss: 0.0011 - 34s/epoch - 406us/sample
Epoch 3/90
84077/84077 - 34s - loss: 0.0010 - val_loss: 0.0065 - 34s/epoch - 409us/sample
Epoch 4/90
84077/84077 - 34s - loss: 9.1443e-04 - val_loss: 0.0249 - 34s/epoch - 407us/sample
Epoch 5/90
84077/84077 - 34s - loss: 8.5959e-04 - val_loss: 0.0591 - 34s/epoch - 407us/sample
Epoch 6/90
84077/84077 - 34s - loss: 8.3126e-04 - val_loss: 0.0568 - 34s/epoch - 408us/sample
Epoch 7/90
84077/84077 - 34s - loss: 8.0972e-04 - val_loss: 0.0880 - 34s/epoch - 409us/sample
Epoch 8/90
84077/84077 - 34s - loss: 7.9738e-04 - val_loss: 0.0906 - 34s/epoch - 409us/sample
Epoch 9/90
84077/84077 - 34s - loss: 7.8599e-04 - val_loss: 0.1219 - 34s/epoch - 407us/sample
Epoch 10/90
84077/84077 - 34s - loss: 7.8057e-04 - val_loss: 0.1360 - 34s/epoch - 409us/sample
Epoch 11/90
84077/84077 - 34s - loss: 7.7227e-04 - val_loss: 0.1721 - 34s/epoch - 408us/sample
Epoch 12/90
84077/84077 - 34s - loss: 7.6667e-04 - val_loss: 0.1738 - 34s/epoch - 410us/sample
Epoch 13/90
84077/84077 - 34s - loss: 7.6342e-04 - val_loss: 0.1616 - 34s/epoch - 407us/sample
Epoch 14/90
84077/84077 - 34s - loss: 7.6076e-04 - val_loss: 0.2018 - 34s/epoch - 410us/sample
Epoch 15/90
84077/84077 - 34s - loss: 7.5562e-04 - val_loss: 0.1913 - 34s/epoch - 404us/sample
Epoch 16/90
84077/84077 - 34s - loss: 7.5102e-04 - val_loss: 0.1666 - 34s/epoch - 408us/sample
Epoch 17/90
84077/84077 - 34s - loss: 7.5058e-04 - val_loss: 0.1706 - 34s/epoch - 408us/sample
Epoch 18/90
84077/84077 - 34s - loss: 7.4751e-04 - val_loss: 0.2219 - 34s/epoch - 409us/sample
Epoch 19/90
84077/84077 - 34s - loss: 7.4579e-04 - val_loss: 0.2355 - 34s/epoch - 407us/sample
Epoch 20/90
84077/84077 - 34s - loss: 7.4314e-04 - val_loss: 0.2125 - 34s/epoch - 408us/sample
Epoch 21/90
84077/84077 - 34s - loss: 7.4085e-04 - val_loss: 0.2146 - 34s/epoch - 406us/sample
Epoch 22/90
84077/84077 - 34s - loss: 7.3948e-04 - val_loss: 0.2165 - 34s/epoch - 406us/sample
Epoch 23/90
84077/84077 - 34s - loss: 7.3787e-04 - val_loss: 0.2487 - 34s/epoch - 409us/sample
Epoch 24/90
84077/84077 - 34s - loss: 7.3758e-04 - val_loss: 0.2497 - 34s/epoch - 408us/sample
Epoch 25/90
84077/84077 - 34s - loss: 7.3332e-04 - val_loss: 0.2321 - 34s/epoch - 407us/sample
Epoch 26/90
84077/84077 - 34s - loss: 7.3163e-04 - val_loss: 0.2440 - 34s/epoch - 408us/sample
Epoch 27/90
84077/84077 - 34s - loss: 7.3031e-04 - val_loss: 0.2114 - 34s/epoch - 409us/sample
Epoch 28/90
84077/84077 - 34s - loss: 7.2876e-04 - val_loss: 0.1835 - 34s/epoch - 408us/sample
Epoch 29/90
84077/84077 - 34s - loss: 7.2582e-04 - val_loss: 0.2196 - 34s/epoch - 409us/sample
Epoch 30/90
84077/84077 - 34s - loss: 7.2633e-04 - val_loss: 0.2017 - 34s/epoch - 407us/sample
Epoch 31/90
84077/84077 - 34s - loss: 7.2368e-04 - val_loss: 0.2622 - 34s/epoch - 407us/sample
Epoch 32/90
84077/84077 - 34s - loss: 7.2281e-04 - val_loss: 0.2172 - 34s/epoch - 407us/sample
Epoch 33/90
84077/84077 - 34s - loss: 7.2165e-04 - val_loss: 0.1963 - 34s/epoch - 409us/sample
Epoch 34/90
84077/84077 - 34s - loss: 7.1989e-04 - val_loss: 0.2123 - 34s/epoch - 409us/sample
Epoch 35/90
84077/84077 - 34s - loss: 7.1893e-04 - val_loss: 0.1804 - 34s/epoch - 408us/sample
Epoch 36/90
84077/84077 - 34s - loss: 7.1687e-04 - val_loss: 0.2129 - 34s/epoch - 408us/sample
Epoch 37/90
84077/84077 - 34s - loss: 7.1738e-04 - val_loss: 0.1753 - 34s/epoch - 408us/sample
Epoch 38/90
84077/84077 - 34s - loss: 7.1554e-04 - val_loss: 0.2091 - 34s/epoch - 407us/sample
Epoch 39/90
84077/84077 - 34s - loss: 7.1421e-04 - val_loss: 0.2009 - 34s/epoch - 409us/sample
Epoch 40/90
84077/84077 - 34s - loss: 7.1309e-04 - val_loss: 0.1731 - 34s/epoch - 410us/sample
Epoch 41/90
84077/84077 - 34s - loss: 7.1039e-04 - val_loss: 0.1833 - 34s/epoch - 409us/sample
Epoch 42/90
84077/84077 - 34s - loss: 7.1031e-04 - val_loss: 0.1756 - 34s/epoch - 407us/sample
Epoch 43/90
84077/84077 - 34s - loss: 7.0855e-04 - val_loss: 0.2010 - 34s/epoch - 407us/sample
Epoch 44/90
84077/84077 - 34s - loss: 7.0605e-04 - val_loss: 0.1709 - 34s/epoch - 406us/sample
Epoch 45/90
84077/84077 - 34s - loss: 7.0577e-04 - val_loss: 0.1768 - 34s/epoch - 410us/sample
Epoch 46/90
84077/84077 - 34s - loss: 7.0573e-04 - val_loss: 0.1927 - 34s/epoch - 408us/sample
Epoch 47/90
84077/84077 - 34s - loss: 7.0404e-04 - val_loss: 0.1633 - 34s/epoch - 408us/sample
Epoch 48/90
84077/84077 - 34s - loss: 7.0365e-04 - val_loss: 0.1539 - 34s/epoch - 406us/sample
Epoch 49/90
84077/84077 - 34s - loss: 7.0293e-04 - val_loss: 0.1733 - 34s/epoch - 408us/sample
Epoch 50/90
84077/84077 - 34s - loss: 7.0116e-04 - val_loss: 0.1346 - 34s/epoch - 407us/sample
Epoch 51/90
84077/84077 - 34s - loss: 7.0127e-04 - val_loss: 0.1303 - 34s/epoch - 409us/sample
Epoch 52/90
84077/84077 - 34s - loss: 6.9931e-04 - val_loss: 0.1266 - 34s/epoch - 407us/sample
Epoch 53/90
84077/84077 - 34s - loss: 6.9909e-04 - val_loss: 0.1300 - 34s/epoch - 409us/sample
Epoch 54/90
84077/84077 - 34s - loss: 6.9964e-04 - val_loss: 0.1685 - 34s/epoch - 405us/sample
Epoch 55/90
84077/84077 - 34s - loss: 6.9868e-04 - val_loss: 0.1398 - 34s/epoch - 408us/sample
Epoch 56/90
84077/84077 - 34s - loss: 6.9763e-04 - val_loss: 0.1449 - 34s/epoch - 409us/sample
Epoch 57/90
84077/84077 - 34s - loss: 6.9763e-04 - val_loss: 0.1372 - 34s/epoch - 408us/sample
Epoch 58/90
84077/84077 - 34s - loss: 6.9818e-04 - val_loss: 0.1194 - 34s/epoch - 409us/sample
Epoch 59/90
84077/84077 - 34s - loss: 6.9716e-04 - val_loss: 0.1463 - 34s/epoch - 407us/sample
Epoch 60/90
84077/84077 - 34s - loss: 6.9554e-04 - val_loss: 0.1090 - 34s/epoch - 409us/sample
Epoch 61/90
84077/84077 - 34s - loss: 6.9431e-04 - val_loss: 0.1171 - 34s/epoch - 408us/sample
Epoch 62/90
84077/84077 - 34s - loss: 6.9483e-04 - val_loss: 0.1517 - 34s/epoch - 410us/sample
Epoch 63/90
84077/84077 - 34s - loss: 6.9370e-04 - val_loss: 0.1244 - 34s/epoch - 408us/sample
Epoch 64/90
84077/84077 - 34s - loss: 6.9319e-04 - val_loss: 0.1249 - 34s/epoch - 408us/sample
Epoch 65/90
84077/84077 - 34s - loss: 6.9277e-04 - val_loss: 0.1152 - 34s/epoch - 407us/sample
Epoch 66/90
84077/84077 - 34s - loss: 6.9207e-04 - val_loss: 0.1293 - 34s/epoch - 408us/sample
Epoch 67/90
84077/84077 - 34s - loss: 6.9133e-04 - val_loss: 0.1153 - 34s/epoch - 407us/sample
Epoch 68/90
84077/84077 - 34s - loss: 6.9034e-04 - val_loss: 0.1253 - 34s/epoch - 409us/sample
Epoch 69/90
84077/84077 - 34s - loss: 6.9119e-04 - val_loss: 0.1095 - 34s/epoch - 408us/sample
Epoch 70/90
84077/84077 - 34s - loss: 6.8905e-04 - val_loss: 0.1195 - 34s/epoch - 408us/sample
Epoch 71/90
84077/84077 - 34s - loss: 6.9019e-04 - val_loss: 0.1118 - 34s/epoch - 408us/sample
Epoch 72/90
84077/84077 - 34s - loss: 6.8813e-04 - val_loss: 0.1114 - 34s/epoch - 406us/sample
Epoch 73/90
84077/84077 - 34s - loss: 6.8675e-04 - val_loss: 0.1075 - 34s/epoch - 409us/sample
Epoch 74/90
84077/84077 - 34s - loss: 6.8740e-04 - val_loss: 0.1150 - 34s/epoch - 407us/sample
Epoch 75/90
84077/84077 - 34s - loss: 6.8753e-04 - val_loss: 0.1101 - 34s/epoch - 410us/sample
Epoch 76/90
84077/84077 - 34s - loss: 6.8772e-04 - val_loss: 0.1115 - 34s/epoch - 407us/sample
Epoch 77/90
84077/84077 - 34s - loss: 6.8633e-04 - val_loss: 0.1037 - 34s/epoch - 408us/sample
Epoch 78/90
84077/84077 - 34s - loss: 6.8539e-04 - val_loss: 0.1071 - 34s/epoch - 409us/sample
Epoch 79/90
84077/84077 - 34s - loss: 6.8558e-04 - val_loss: 0.1127 - 34s/epoch - 408us/sample
Epoch 80/90
84077/84077 - 34s - loss: 6.8492e-04 - val_loss: 0.1027 - 34s/epoch - 407us/sample
Epoch 81/90
84077/84077 - 34s - loss: 6.8460e-04 - val_loss: 0.0973 - 34s/epoch - 409us/sample
Epoch 82/90
84077/84077 - 34s - loss: 6.8357e-04 - val_loss: 0.1054 - 34s/epoch - 406us/sample
Epoch 83/90
84077/84077 - 34s - loss: 6.8458e-04 - val_loss: 0.0939 - 34s/epoch - 408us/sample
Epoch 84/90
84077/84077 - 34s - loss: 6.8338e-04 - val_loss: 0.1103 - 34s/epoch - 409us/sample
Epoch 85/90
84077/84077 - 34s - loss: 6.8331e-04 - val_loss: 0.0805 - 34s/epoch - 408us/sample
Epoch 86/90
84077/84077 - 34s - loss: 6.8249e-04 - val_loss: 0.1001 - 34s/epoch - 410us/sample
Epoch 87/90
84077/84077 - 34s - loss: 6.8078e-04 - val_loss: 0.0760 - 34s/epoch - 407us/sample
Epoch 88/90
84077/84077 - 34s - loss: 6.8043e-04 - val_loss: 0.0918 - 34s/epoch - 408us/sample
Epoch 89/90
84077/84077 - 34s - loss: 6.7961e-04 - val_loss: 0.1113 - 34s/epoch - 404us/sample
Epoch 90/90
84077/84077 - 34s - loss: 6.7935e-04 - val_loss: 0.0814 - 34s/epoch - 410us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.08144552682230972
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 09:28:34.470864: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:21999 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.1962905513105367
cosine 0.194351856422405
MAE: 0.02513352614844197
RMSE: 0.2853047916562267
r2: -62.58651873388007
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 8, 90, 0.001, 0.2, 188, 0.0006793536835210003, 0.08144552682230972, 0.1962905513105367, 0.194351856422405, 0.02513352614844197, 0.2853047916562267, -62.58651873388007, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.001 32 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 2074)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 09:28:44.800922: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_54/moving_variance/Assign' id:22889 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_54/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_54/moving_variance, batch_normalization_54/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 09:28:57.747181: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_18/mul' id:23290 op device:{requested: '', assigned: ''} def:{{{node loss_18/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_18/mul/x, loss_18/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.0029 - val_loss: 0.0024 - 16s/epoch - 191us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 137us/sample
Epoch 3/90
84077/84077 - 12s - loss: 6.3338e-04 - val_loss: 4.9004e-04 - 12s/epoch - 138us/sample
Epoch 4/90
84077/84077 - 12s - loss: 5.0905e-04 - val_loss: 4.2772e-04 - 12s/epoch - 137us/sample
Epoch 5/90
84077/84077 - 12s - loss: 5.1486e-04 - val_loss: 4.0035e-04 - 12s/epoch - 137us/sample
Epoch 6/90
84077/84077 - 12s - loss: 4.0892e-04 - val_loss: 3.4705e-04 - 12s/epoch - 137us/sample
Epoch 7/90
84077/84077 - 11s - loss: 3.7081e-04 - val_loss: 3.2136e-04 - 11s/epoch - 137us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.4685e-04 - val_loss: 2.9161e-04 - 11s/epoch - 136us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.2036e-04 - val_loss: 2.7901e-04 - 11s/epoch - 136us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.0795e-04 - val_loss: 2.6623e-04 - 12s/epoch - 137us/sample
Epoch 11/90
84077/84077 - 12s - loss: 2.9579e-04 - val_loss: 2.5987e-04 - 12s/epoch - 138us/sample
Epoch 12/90
84077/84077 - 12s - loss: 2.8652e-04 - val_loss: 2.5374e-04 - 12s/epoch - 137us/sample
Epoch 13/90
84077/84077 - 12s - loss: 2.7949e-04 - val_loss: 2.4782e-04 - 12s/epoch - 137us/sample
Epoch 14/90
84077/84077 - 12s - loss: 2.7342e-04 - val_loss: 2.3780e-04 - 12s/epoch - 137us/sample
Epoch 15/90
84077/84077 - 12s - loss: 2.6789e-04 - val_loss: 2.3607e-04 - 12s/epoch - 137us/sample
Epoch 16/90
84077/84077 - 12s - loss: 2.6379e-04 - val_loss: 2.3351e-04 - 12s/epoch - 138us/sample
Epoch 17/90
84077/84077 - 12s - loss: 2.5870e-04 - val_loss: 2.3059e-04 - 12s/epoch - 138us/sample
Epoch 18/90
84077/84077 - 12s - loss: 2.5482e-04 - val_loss: 2.2610e-04 - 12s/epoch - 137us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.5214e-04 - val_loss: 2.2303e-04 - 11s/epoch - 136us/sample
Epoch 20/90
84077/84077 - 12s - loss: 2.4864e-04 - val_loss: 2.1793e-04 - 12s/epoch - 138us/sample
Epoch 21/90
84077/84077 - 12s - loss: 2.4529e-04 - val_loss: 2.1710e-04 - 12s/epoch - 138us/sample
Epoch 22/90
84077/84077 - 12s - loss: 2.4196e-04 - val_loss: 2.1401e-04 - 12s/epoch - 137us/sample
Epoch 23/90
84077/84077 - 12s - loss: 2.4007e-04 - val_loss: 2.1429e-04 - 12s/epoch - 137us/sample
Epoch 24/90
84077/84077 - 12s - loss: 2.3721e-04 - val_loss: 2.1286e-04 - 12s/epoch - 137us/sample
Epoch 25/90
84077/84077 - 12s - loss: 2.3509e-04 - val_loss: 2.0807e-04 - 12s/epoch - 138us/sample
Epoch 26/90
84077/84077 - 12s - loss: 2.3271e-04 - val_loss: 2.1839e-04 - 12s/epoch - 138us/sample
Epoch 27/90
84077/84077 - 12s - loss: 2.3143e-04 - val_loss: 2.0738e-04 - 12s/epoch - 137us/sample
Epoch 28/90
84077/84077 - 12s - loss: 2.2775e-04 - val_loss: 2.0714e-04 - 12s/epoch - 137us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.2788e-04 - val_loss: 2.0508e-04 - 11s/epoch - 137us/sample
Epoch 30/90
84077/84077 - 12s - loss: 2.2424e-04 - val_loss: 2.0199e-04 - 12s/epoch - 138us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.2255e-04 - val_loss: 1.9922e-04 - 11s/epoch - 136us/sample
Epoch 32/90
84077/84077 - 12s - loss: 2.2228e-04 - val_loss: 1.9831e-04 - 12s/epoch - 137us/sample
Epoch 33/90
84077/84077 - 12s - loss: 2.1971e-04 - val_loss: 1.9922e-04 - 12s/epoch - 137us/sample
Epoch 34/90
84077/84077 - 12s - loss: 2.1900e-04 - val_loss: 1.9599e-04 - 12s/epoch - 138us/sample
Epoch 35/90
84077/84077 - 12s - loss: 2.1709e-04 - val_loss: 1.9831e-04 - 12s/epoch - 139us/sample
Epoch 36/90
84077/84077 - 12s - loss: 2.1499e-04 - val_loss: 1.9370e-04 - 12s/epoch - 137us/sample
Epoch 37/90
84077/84077 - 12s - loss: 2.1355e-04 - val_loss: 1.9255e-04 - 12s/epoch - 137us/sample
Epoch 38/90
84077/84077 - 12s - loss: 2.1254e-04 - val_loss: 1.9323e-04 - 12s/epoch - 137us/sample
Epoch 39/90
84077/84077 - 12s - loss: 2.1057e-04 - val_loss: 1.8968e-04 - 12s/epoch - 137us/sample
Epoch 40/90
84077/84077 - 12s - loss: 2.0996e-04 - val_loss: 1.8962e-04 - 12s/epoch - 138us/sample
Epoch 41/90
84077/84077 - 12s - loss: 2.0761e-04 - val_loss: 1.8753e-04 - 12s/epoch - 138us/sample
Epoch 42/90
84077/84077 - 11s - loss: 2.0682e-04 - val_loss: 1.8520e-04 - 11s/epoch - 137us/sample
Epoch 43/90
84077/84077 - 12s - loss: 2.0680e-04 - val_loss: 1.8347e-04 - 12s/epoch - 138us/sample
Epoch 44/90
84077/84077 - 12s - loss: 2.0420e-04 - val_loss: 1.8390e-04 - 12s/epoch - 137us/sample
Epoch 45/90
84077/84077 - 12s - loss: 2.0297e-04 - val_loss: 1.8132e-04 - 12s/epoch - 139us/sample
Epoch 46/90
84077/84077 - 12s - loss: 2.0169e-04 - val_loss: 1.7898e-04 - 12s/epoch - 137us/sample
Epoch 47/90
84077/84077 - 12s - loss: 1.9969e-04 - val_loss: 1.8150e-04 - 12s/epoch - 138us/sample
Epoch 48/90
84077/84077 - 11s - loss: 2.0001e-04 - val_loss: 1.7704e-04 - 11s/epoch - 137us/sample
Epoch 49/90
84077/84077 - 11s - loss: 1.9747e-04 - val_loss: 1.7702e-04 - 11s/epoch - 137us/sample
Epoch 50/90
84077/84077 - 12s - loss: 1.9720e-04 - val_loss: 1.7379e-04 - 12s/epoch - 138us/sample
Epoch 51/90
84077/84077 - 12s - loss: 1.9512e-04 - val_loss: 1.8771e-04 - 12s/epoch - 137us/sample
Epoch 52/90
84077/84077 - 11s - loss: 1.9454e-04 - val_loss: 1.7463e-04 - 11s/epoch - 136us/sample
Epoch 53/90
84077/84077 - 12s - loss: 1.9258e-04 - val_loss: 1.7298e-04 - 12s/epoch - 137us/sample
Epoch 54/90
84077/84077 - 12s - loss: 1.9115e-04 - val_loss: 1.7413e-04 - 12s/epoch - 137us/sample
Epoch 55/90
84077/84077 - 12s - loss: 1.9035e-04 - val_loss: 1.7565e-04 - 12s/epoch - 138us/sample
Epoch 56/90
84077/84077 - 12s - loss: 1.8948e-04 - val_loss: 1.7910e-04 - 12s/epoch - 137us/sample
Epoch 57/90
84077/84077 - 12s - loss: 1.8900e-04 - val_loss: 1.7322e-04 - 12s/epoch - 137us/sample
Epoch 58/90
84077/84077 - 11s - loss: 1.8719e-04 - val_loss: 1.7282e-04 - 11s/epoch - 137us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.8845e-04 - val_loss: 1.7101e-04 - 11s/epoch - 136us/sample
Epoch 60/90
84077/84077 - 12s - loss: 1.8527e-04 - val_loss: 1.7682e-04 - 12s/epoch - 137us/sample
Epoch 61/90
84077/84077 - 12s - loss: 1.8647e-04 - val_loss: 1.7329e-04 - 12s/epoch - 138us/sample
Epoch 62/90
84077/84077 - 12s - loss: 1.8415e-04 - val_loss: 1.7533e-04 - 12s/epoch - 138us/sample
Epoch 63/90
84077/84077 - 12s - loss: 1.8388e-04 - val_loss: 1.7189e-04 - 12s/epoch - 137us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.8414e-04 - val_loss: 1.7257e-04 - 11s/epoch - 137us/sample
Epoch 65/90
84077/84077 - 12s - loss: 1.8192e-04 - val_loss: 1.7539e-04 - 12s/epoch - 138us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.8132e-04 - val_loss: 1.7470e-04 - 11s/epoch - 137us/sample
Epoch 67/90
84077/84077 - 12s - loss: 1.8009e-04 - val_loss: 1.7539e-04 - 12s/epoch - 137us/sample
Epoch 68/90
84077/84077 - 12s - loss: 1.8007e-04 - val_loss: 1.7406e-04 - 12s/epoch - 137us/sample
Epoch 69/90
84077/84077 - 12s - loss: 1.7929e-04 - val_loss: 1.6911e-04 - 12s/epoch - 138us/sample
Epoch 70/90
84077/84077 - 12s - loss: 1.7830e-04 - val_loss: 1.6815e-04 - 12s/epoch - 137us/sample
Epoch 71/90
84077/84077 - 12s - loss: 1.7831e-04 - val_loss: 1.7129e-04 - 12s/epoch - 137us/sample
Epoch 72/90
84077/84077 - 12s - loss: 1.7835e-04 - val_loss: 1.6522e-04 - 12s/epoch - 137us/sample
Epoch 73/90
84077/84077 - 12s - loss: 1.7776e-04 - val_loss: 1.6652e-04 - 12s/epoch - 137us/sample
Epoch 74/90
84077/84077 - 12s - loss: 1.7728e-04 - val_loss: 1.6465e-04 - 12s/epoch - 138us/sample
Epoch 75/90
84077/84077 - 12s - loss: 1.7620e-04 - val_loss: 1.6478e-04 - 12s/epoch - 137us/sample
Epoch 76/90
84077/84077 - 12s - loss: 1.7635e-04 - val_loss: 1.6550e-04 - 12s/epoch - 137us/sample
Epoch 77/90
84077/84077 - 12s - loss: 1.7494e-04 - val_loss: 1.5903e-04 - 12s/epoch - 137us/sample
Epoch 78/90
84077/84077 - 12s - loss: 1.7597e-04 - val_loss: 1.6241e-04 - 12s/epoch - 137us/sample
Epoch 79/90
84077/84077 - 12s - loss: 1.7589e-04 - val_loss: 1.6847e-04 - 12s/epoch - 138us/sample
Epoch 80/90
84077/84077 - 12s - loss: 1.7472e-04 - val_loss: 1.5864e-04 - 12s/epoch - 137us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.7411e-04 - val_loss: 1.5975e-04 - 11s/epoch - 136us/sample
Epoch 82/90
84077/84077 - 12s - loss: 1.7351e-04 - val_loss: 1.5757e-04 - 12s/epoch - 137us/sample
Epoch 83/90
84077/84077 - 12s - loss: 1.7321e-04 - val_loss: 1.6037e-04 - 12s/epoch - 138us/sample
Epoch 84/90
84077/84077 - 12s - loss: 1.7254e-04 - val_loss: 1.5823e-04 - 12s/epoch - 137us/sample
Epoch 85/90
84077/84077 - 12s - loss: 1.7161e-04 - val_loss: 1.5997e-04 - 12s/epoch - 138us/sample
Epoch 86/90
84077/84077 - 12s - loss: 1.7114e-04 - val_loss: 1.5794e-04 - 12s/epoch - 137us/sample
Epoch 87/90
84077/84077 - 12s - loss: 1.7216e-04 - val_loss: 1.5844e-04 - 12s/epoch - 137us/sample
Epoch 88/90
84077/84077 - 12s - loss: 1.7127e-04 - val_loss: 1.5521e-04 - 12s/epoch - 139us/sample
Epoch 89/90
84077/84077 - 12s - loss: 1.7045e-04 - val_loss: 1.5479e-04 - 12s/epoch - 137us/sample
Epoch 90/90
84077/84077 - 12s - loss: 1.6911e-04 - val_loss: 1.5550e-04 - 12s/epoch - 138us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.000155500129617767
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 09:46:06.915206: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_18/outputlayer/BiasAdd' id:23254 op device:{requested: '', assigned: ''} def:{{{node decoder_model_18/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_18/outputlayer/MatMul, decoder_model_18/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.043856487140523585
cosine 0.04330970086512843
MAE: 0.0026245975966571354
RMSE: 0.012650556376147657
r2: 0.8750277928946498
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 32, 90, 0.001, 0.2, 188, 0.00016911226302851652, 0.000155500129617767, 0.043856487140523585, 0.04330970086512843, 0.0026245975966571354, 0.012650556376147657, 0.8750277928946498, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0008 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 1980)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 09:46:17.799428: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_19/bias/Assign' id:24289 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_19/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_19/bias, dense_dec1_19/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 09:46:30.785700: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_19/mul' id:24571 op device:{requested: '', assigned: ''} def:{{{node loss_19/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_19/mul/x, loss_19/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.0071 - val_loss: 0.0018 - 16s/epoch - 194us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.2239 - val_loss: 0.0241 - 12s/epoch - 137us/sample
Epoch 3/85
84077/84077 - 11s - loss: 0.0138 - val_loss: 0.0040 - 11s/epoch - 137us/sample
Epoch 4/85
84077/84077 - 11s - loss: 0.0014 - val_loss: 0.0011 - 11s/epoch - 136us/sample
Epoch 5/85
84077/84077 - 11s - loss: 0.0014 - val_loss: 0.0011 - 11s/epoch - 137us/sample
Epoch 6/85
84077/84077 - 11s - loss: 0.0011 - val_loss: 7.9332e-04 - 11s/epoch - 136us/sample
Epoch 7/85
84077/84077 - 11s - loss: 9.7440e-04 - val_loss: 6.9077e-04 - 11s/epoch - 136us/sample
Epoch 8/85
84077/84077 - 11s - loss: 7.0875e-04 - val_loss: 6.0120e-04 - 11s/epoch - 136us/sample
Epoch 9/85
84077/84077 - 12s - loss: 6.8526e-04 - val_loss: 5.6112e-04 - 12s/epoch - 138us/sample
Epoch 10/85
84077/84077 - 12s - loss: 6.0298e-04 - val_loss: 7.7940e-04 - 12s/epoch - 137us/sample
Epoch 11/85
84077/84077 - 11s - loss: 5.4733e-04 - val_loss: 4.9730e-04 - 11s/epoch - 136us/sample
Epoch 12/85
84077/84077 - 12s - loss: 5.2157e-04 - val_loss: 4.6849e-04 - 12s/epoch - 137us/sample
Epoch 13/85
84077/84077 - 12s - loss: 4.9317e-04 - val_loss: 4.5303e-04 - 12s/epoch - 137us/sample
Epoch 14/85
84077/84077 - 12s - loss: 4.7357e-04 - val_loss: 4.3522e-04 - 12s/epoch - 137us/sample
Epoch 15/85
84077/84077 - 12s - loss: 4.5436e-04 - val_loss: 4.3748e-04 - 12s/epoch - 137us/sample
Epoch 16/85
84077/84077 - 11s - loss: 4.4454e-04 - val_loss: 4.2000e-04 - 11s/epoch - 137us/sample
Epoch 17/85
84077/84077 - 12s - loss: 4.3444e-04 - val_loss: 4.1803e-04 - 12s/epoch - 137us/sample
Epoch 18/85
84077/84077 - 12s - loss: 4.2814e-04 - val_loss: 4.1236e-04 - 12s/epoch - 138us/sample
Epoch 19/85
84077/84077 - 12s - loss: 4.2262e-04 - val_loss: 4.0316e-04 - 12s/epoch - 137us/sample
Epoch 20/85
84077/84077 - 11s - loss: 4.1903e-04 - val_loss: 4.0101e-04 - 11s/epoch - 137us/sample
Epoch 21/85
84077/84077 - 12s - loss: 4.1569e-04 - val_loss: 4.0268e-04 - 12s/epoch - 138us/sample
Epoch 22/85
84077/84077 - 12s - loss: 4.1199e-04 - val_loss: 4.0925e-04 - 12s/epoch - 138us/sample
Epoch 23/85
84077/84077 - 11s - loss: 4.0939e-04 - val_loss: 3.9245e-04 - 11s/epoch - 137us/sample
Epoch 24/85
84077/84077 - 12s - loss: 4.0751e-04 - val_loss: 3.9503e-04 - 12s/epoch - 137us/sample
Epoch 25/85
84077/84077 - 11s - loss: 4.0396e-04 - val_loss: 3.9345e-04 - 11s/epoch - 136us/sample
Epoch 26/85
84077/84077 - 12s - loss: 4.0151e-04 - val_loss: 3.8953e-04 - 12s/epoch - 137us/sample
Epoch 27/85
84077/84077 - 12s - loss: 3.9812e-04 - val_loss: 3.8699e-04 - 12s/epoch - 137us/sample
Epoch 28/85
84077/84077 - 11s - loss: 3.9581e-04 - val_loss: 3.8886e-04 - 11s/epoch - 136us/sample
Epoch 29/85
84077/84077 - 11s - loss: 3.9369e-04 - val_loss: 3.8834e-04 - 11s/epoch - 136us/sample
Epoch 30/85
84077/84077 - 12s - loss: 3.9195e-04 - val_loss: 3.8202e-04 - 12s/epoch - 138us/sample
Epoch 31/85
84077/84077 - 12s - loss: 3.8996e-04 - val_loss: 3.8007e-04 - 12s/epoch - 137us/sample
Epoch 32/85
84077/84077 - 11s - loss: 3.8891e-04 - val_loss: 3.7842e-04 - 11s/epoch - 137us/sample
Epoch 33/85
84077/84077 - 12s - loss: 3.8762e-04 - val_loss: 3.7995e-04 - 12s/epoch - 137us/sample
Epoch 34/85
84077/84077 - 12s - loss: 3.8617e-04 - val_loss: 3.7746e-04 - 12s/epoch - 138us/sample
Epoch 35/85
84077/84077 - 11s - loss: 3.8479e-04 - val_loss: 3.7899e-04 - 11s/epoch - 137us/sample
Epoch 36/85
84077/84077 - 12s - loss: 3.8341e-04 - val_loss: 3.7244e-04 - 12s/epoch - 137us/sample
Epoch 37/85
84077/84077 - 11s - loss: 3.8254e-04 - val_loss: 3.6961e-04 - 11s/epoch - 136us/sample
Epoch 38/85
84077/84077 - 11s - loss: 3.8171e-04 - val_loss: 3.7173e-04 - 11s/epoch - 137us/sample
Epoch 39/85
84077/84077 - 12s - loss: 3.8102e-04 - val_loss: 3.7212e-04 - 12s/epoch - 138us/sample
Epoch 40/85
84077/84077 - 12s - loss: 3.7896e-04 - val_loss: 3.6884e-04 - 12s/epoch - 137us/sample
Epoch 41/85
84077/84077 - 12s - loss: 3.7909e-04 - val_loss: 3.6737e-04 - 12s/epoch - 137us/sample
Epoch 42/85
84077/84077 - 11s - loss: 3.7822e-04 - val_loss: 3.6637e-04 - 11s/epoch - 136us/sample
Epoch 43/85
84077/84077 - 11s - loss: 3.7712e-04 - val_loss: 3.7132e-04 - 11s/epoch - 137us/sample
Epoch 44/85
84077/84077 - 12s - loss: 3.7640e-04 - val_loss: 3.6893e-04 - 12s/epoch - 137us/sample
Epoch 45/85
84077/84077 - 12s - loss: 3.7563e-04 - val_loss: 3.6658e-04 - 12s/epoch - 137us/sample
Epoch 46/85
84077/84077 - 11s - loss: 3.7465e-04 - val_loss: 3.6350e-04 - 11s/epoch - 137us/sample
Epoch 47/85
84077/84077 - 12s - loss: 3.7384e-04 - val_loss: 3.6145e-04 - 12s/epoch - 137us/sample
Epoch 48/85
84077/84077 - 12s - loss: 3.7396e-04 - val_loss: 3.6580e-04 - 12s/epoch - 137us/sample
Epoch 49/85
84077/84077 - 12s - loss: 3.7282e-04 - val_loss: 3.6256e-04 - 12s/epoch - 138us/sample
Epoch 50/85
84077/84077 - 11s - loss: 3.7196e-04 - val_loss: 3.6096e-04 - 11s/epoch - 137us/sample
Epoch 51/85
84077/84077 - 11s - loss: 3.7156e-04 - val_loss: 3.6192e-04 - 11s/epoch - 137us/sample
Epoch 52/85
84077/84077 - 11s - loss: 3.7098e-04 - val_loss: 3.6177e-04 - 11s/epoch - 135us/sample
Epoch 53/85
84077/84077 - 11s - loss: 3.6991e-04 - val_loss: 3.5544e-04 - 11s/epoch - 137us/sample
Epoch 54/85
84077/84077 - 12s - loss: 3.7005e-04 - val_loss: 3.5282e-04 - 12s/epoch - 138us/sample
Epoch 55/85
84077/84077 - 12s - loss: 3.6881e-04 - val_loss: 3.5873e-04 - 12s/epoch - 137us/sample
Epoch 56/85
84077/84077 - 12s - loss: 3.6911e-04 - val_loss: 3.5982e-04 - 12s/epoch - 137us/sample
Epoch 57/85
84077/84077 - 11s - loss: 3.6799e-04 - val_loss: 3.5966e-04 - 11s/epoch - 137us/sample
Epoch 58/85
84077/84077 - 12s - loss: 3.6756e-04 - val_loss: 3.5882e-04 - 12s/epoch - 137us/sample
Epoch 59/85
84077/84077 - 12s - loss: 3.6651e-04 - val_loss: 3.5738e-04 - 12s/epoch - 138us/sample
Epoch 60/85
84077/84077 - 12s - loss: 3.6739e-04 - val_loss: 3.5753e-04 - 12s/epoch - 137us/sample
Epoch 61/85
84077/84077 - 12s - loss: 3.6611e-04 - val_loss: 3.5495e-04 - 12s/epoch - 137us/sample
Epoch 62/85
84077/84077 - 12s - loss: 3.6579e-04 - val_loss: 3.5516e-04 - 12s/epoch - 137us/sample
Epoch 63/85
84077/84077 - 12s - loss: 3.6526e-04 - val_loss: 3.5686e-04 - 12s/epoch - 138us/sample
Epoch 64/85
84077/84077 - 12s - loss: 3.6495e-04 - val_loss: 3.5411e-04 - 12s/epoch - 138us/sample
Epoch 65/85
84077/84077 - 12s - loss: 3.6392e-04 - val_loss: 3.5518e-04 - 12s/epoch - 137us/sample
Epoch 66/85
84077/84077 - 11s - loss: 3.6365e-04 - val_loss: 3.5404e-04 - 11s/epoch - 137us/sample
Epoch 67/85
84077/84077 - 11s - loss: 3.6354e-04 - val_loss: 3.5652e-04 - 11s/epoch - 137us/sample
Epoch 68/85
84077/84077 - 11s - loss: 3.6315e-04 - val_loss: 3.5355e-04 - 11s/epoch - 136us/sample
Epoch 69/85
84077/84077 - 12s - loss: 3.6316e-04 - val_loss: 3.5508e-04 - 12s/epoch - 138us/sample
Epoch 70/85
84077/84077 - 11s - loss: 3.6234e-04 - val_loss: 3.5486e-04 - 11s/epoch - 137us/sample
Epoch 71/85
84077/84077 - 11s - loss: 3.6248e-04 - val_loss: 3.5209e-04 - 11s/epoch - 137us/sample
Epoch 72/85
84077/84077 - 11s - loss: 3.6195e-04 - val_loss: 3.5366e-04 - 11s/epoch - 136us/sample
Epoch 73/85
84077/84077 - 12s - loss: 3.6158e-04 - val_loss: 3.5381e-04 - 12s/epoch - 137us/sample
Epoch 74/85
84077/84077 - 12s - loss: 3.6147e-04 - val_loss: 3.4875e-04 - 12s/epoch - 138us/sample
Epoch 75/85
84077/84077 - 12s - loss: 3.6094e-04 - val_loss: 3.5052e-04 - 12s/epoch - 137us/sample
Epoch 76/85
84077/84077 - 12s - loss: 3.6056e-04 - val_loss: 3.4807e-04 - 12s/epoch - 137us/sample
Epoch 77/85
84077/84077 - 11s - loss: 3.6080e-04 - val_loss: 3.5161e-04 - 11s/epoch - 137us/sample
Epoch 78/85
84077/84077 - 11s - loss: 3.6048e-04 - val_loss: 3.4755e-04 - 11s/epoch - 137us/sample
Epoch 79/85
84077/84077 - 12s - loss: 3.5977e-04 - val_loss: 3.4620e-04 - 12s/epoch - 138us/sample
Epoch 80/85
84077/84077 - 12s - loss: 3.5883e-04 - val_loss: 3.4930e-04 - 12s/epoch - 137us/sample
Epoch 81/85
84077/84077 - 12s - loss: 3.5860e-04 - val_loss: 3.4801e-04 - 12s/epoch - 137us/sample
Epoch 82/85
84077/84077 - 11s - loss: 3.5836e-04 - val_loss: 3.4886e-04 - 11s/epoch - 136us/sample
Epoch 83/85
84077/84077 - 11s - loss: 3.5794e-04 - val_loss: 3.4539e-04 - 11s/epoch - 137us/sample
Epoch 84/85
84077/84077 - 12s - loss: 3.5794e-04 - val_loss: 3.4481e-04 - 12s/epoch - 137us/sample
Epoch 85/85
84077/84077 - 12s - loss: 3.5745e-04 - val_loss: 3.4403e-04 - 12s/epoch - 138us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00034402588563329204
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 10:02:40.209860: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:24542 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.06342264787597747
cosine 0.0626430378036182
MAE: 0.003035808926689651
RMSE: 0.015822179878215856
r2: 0.8044793894463872
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 85, 0.0008, 0.2, 188, 0.0003574498768641027, 0.00034402588563329204, 0.06342264787597747, 0.0626430378036182, 0.003035808926689651, 0.015822179878215856, 0.8044793894463872, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 32 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 1886)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 10:02:51.403484: W tensorflow/c/c_api.cc:291] Operation '{name:'training_40/Adam/bottleneck_zlog_20/bias/v/Assign' id:26438 op device:{requested: '', assigned: ''} def:{{{node training_40/Adam/bottleneck_zlog_20/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_40/Adam/bottleneck_zlog_20/bias/v, training_40/Adam/bottleneck_zlog_20/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 10:03:04.471392: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:25826 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0278 - val_loss: 0.0023 - 17s/epoch - 197us/sample
Epoch 2/85
84077/84077 - 11s - loss: 0.0016 - val_loss: 0.0014 - 11s/epoch - 136us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0012 - val_loss: 0.0010 - 12s/epoch - 137us/sample
Epoch 4/85
84077/84077 - 11s - loss: 0.0011 - val_loss: 7.4934e-04 - 11s/epoch - 137us/sample
Epoch 5/85
84077/84077 - 12s - loss: 8.0990e-04 - val_loss: 8.4775e-04 - 12s/epoch - 139us/sample
Epoch 6/85
84077/84077 - 11s - loss: 6.8030e-04 - val_loss: 5.6123e-04 - 11s/epoch - 137us/sample
Epoch 7/85
84077/84077 - 11s - loss: 5.5998e-04 - val_loss: 4.9177e-04 - 11s/epoch - 137us/sample
Epoch 8/85
84077/84077 - 12s - loss: 4.9995e-04 - val_loss: 4.5508e-04 - 12s/epoch - 137us/sample
Epoch 9/85
84077/84077 - 12s - loss: 4.5325e-04 - val_loss: 4.1284e-04 - 12s/epoch - 138us/sample
Epoch 10/85
84077/84077 - 12s - loss: 4.1691e-04 - val_loss: 3.8399e-04 - 12s/epoch - 137us/sample
Epoch 11/85
84077/84077 - 12s - loss: 3.9029e-04 - val_loss: 3.7556e-04 - 12s/epoch - 139us/sample
Epoch 12/85
84077/84077 - 12s - loss: 3.6918e-04 - val_loss: 3.7063e-04 - 12s/epoch - 138us/sample
Epoch 13/85
84077/84077 - 12s - loss: 3.5464e-04 - val_loss: 3.5914e-04 - 12s/epoch - 138us/sample
Epoch 14/85
84077/84077 - 11s - loss: 3.4350e-04 - val_loss: 3.4795e-04 - 11s/epoch - 137us/sample
Epoch 15/85
84077/84077 - 11s - loss: 3.3501e-04 - val_loss: 3.4583e-04 - 11s/epoch - 137us/sample
Epoch 16/85
84077/84077 - 12s - loss: 3.2826e-04 - val_loss: 3.4591e-04 - 12s/epoch - 138us/sample
Epoch 17/85
84077/84077 - 12s - loss: 3.2106e-04 - val_loss: 3.3346e-04 - 12s/epoch - 138us/sample
Epoch 18/85
84077/84077 - 12s - loss: 3.1471e-04 - val_loss: 3.3344e-04 - 12s/epoch - 137us/sample
Epoch 19/85
84077/84077 - 12s - loss: 3.0869e-04 - val_loss: 3.1706e-04 - 12s/epoch - 137us/sample
Epoch 20/85
84077/84077 - 12s - loss: 3.0491e-04 - val_loss: 3.3041e-04 - 12s/epoch - 138us/sample
Epoch 21/85
84077/84077 - 12s - loss: 3.0031e-04 - val_loss: 3.2089e-04 - 12s/epoch - 138us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.9726e-04 - val_loss: 3.0612e-04 - 12s/epoch - 138us/sample
Epoch 23/85
84077/84077 - 12s - loss: 2.9310e-04 - val_loss: 3.1508e-04 - 12s/epoch - 137us/sample
Epoch 24/85
84077/84077 - 11s - loss: 2.9066e-04 - val_loss: 3.1168e-04 - 11s/epoch - 137us/sample
Epoch 25/85
84077/84077 - 12s - loss: 2.8900e-04 - val_loss: 3.0727e-04 - 12s/epoch - 138us/sample
Epoch 26/85
84077/84077 - 12s - loss: 2.8641e-04 - val_loss: 3.0944e-04 - 12s/epoch - 139us/sample
Epoch 27/85
84077/84077 - 12s - loss: 2.8428e-04 - val_loss: 3.0999e-04 - 12s/epoch - 138us/sample
Epoch 28/85
84077/84077 - 12s - loss: 2.8325e-04 - val_loss: 3.0690e-04 - 12s/epoch - 137us/sample
Epoch 29/85
84077/84077 - 12s - loss: 2.8094e-04 - val_loss: 2.9639e-04 - 12s/epoch - 137us/sample
Epoch 30/85
84077/84077 - 12s - loss: 2.7934e-04 - val_loss: 3.2053e-04 - 12s/epoch - 138us/sample
Epoch 31/85
84077/84077 - 11s - loss: 2.7805e-04 - val_loss: 3.0781e-04 - 11s/epoch - 137us/sample
Epoch 32/85
84077/84077 - 12s - loss: 2.7635e-04 - val_loss: 3.0002e-04 - 12s/epoch - 137us/sample
Epoch 33/85
84077/84077 - 12s - loss: 2.7533e-04 - val_loss: 2.9628e-04 - 12s/epoch - 137us/sample
Epoch 34/85
84077/84077 - 11s - loss: 2.7424e-04 - val_loss: 3.0292e-04 - 11s/epoch - 136us/sample
Epoch 35/85
84077/84077 - 12s - loss: 2.7297e-04 - val_loss: 3.0097e-04 - 12s/epoch - 137us/sample
Epoch 36/85
84077/84077 - 12s - loss: 2.7256e-04 - val_loss: 2.9023e-04 - 12s/epoch - 137us/sample
Epoch 37/85
84077/84077 - 12s - loss: 2.7123e-04 - val_loss: 2.9321e-04 - 12s/epoch - 137us/sample
Epoch 38/85
84077/84077 - 12s - loss: 2.7003e-04 - val_loss: 2.8917e-04 - 12s/epoch - 137us/sample
Epoch 39/85
84077/84077 - 11s - loss: 2.7038e-04 - val_loss: 2.9721e-04 - 11s/epoch - 136us/sample
Epoch 40/85
84077/84077 - 12s - loss: 2.6896e-04 - val_loss: 2.9130e-04 - 12s/epoch - 137us/sample
Epoch 41/85
84077/84077 - 12s - loss: 2.6859e-04 - val_loss: 2.9079e-04 - 12s/epoch - 138us/sample
Epoch 42/85
84077/84077 - 12s - loss: 2.6724e-04 - val_loss: 2.8828e-04 - 12s/epoch - 138us/sample
Epoch 43/85
84077/84077 - 12s - loss: 2.6771e-04 - val_loss: 2.8715e-04 - 12s/epoch - 138us/sample
Epoch 44/85
84077/84077 - 12s - loss: 2.6645e-04 - val_loss: 2.9049e-04 - 12s/epoch - 137us/sample
Epoch 45/85
84077/84077 - 12s - loss: 2.6576e-04 - val_loss: 2.8900e-04 - 12s/epoch - 138us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.6495e-04 - val_loss: 2.7933e-04 - 12s/epoch - 138us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.6429e-04 - val_loss: 2.8673e-04 - 12s/epoch - 137us/sample
Epoch 48/85
84077/84077 - 12s - loss: 2.6288e-04 - val_loss: 2.8588e-04 - 12s/epoch - 137us/sample
Epoch 49/85
84077/84077 - 12s - loss: 2.6267e-04 - val_loss: 2.8473e-04 - 12s/epoch - 138us/sample
Epoch 50/85
84077/84077 - 12s - loss: 2.6236e-04 - val_loss: 2.8381e-04 - 12s/epoch - 137us/sample
Epoch 51/85
84077/84077 - 12s - loss: 2.6120e-04 - val_loss: 2.8401e-04 - 12s/epoch - 137us/sample
Epoch 52/85
84077/84077 - 11s - loss: 2.6037e-04 - val_loss: 2.8479e-04 - 11s/epoch - 135us/sample
Epoch 53/85
84077/84077 - 12s - loss: 2.6092e-04 - val_loss: 2.8453e-04 - 12s/epoch - 138us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.6028e-04 - val_loss: 2.8694e-04 - 12s/epoch - 137us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.5979e-04 - val_loss: 2.8114e-04 - 12s/epoch - 137us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.5868e-04 - val_loss: 2.8039e-04 - 12s/epoch - 137us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.5886e-04 - val_loss: 2.8370e-04 - 12s/epoch - 138us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.5836e-04 - val_loss: 2.7843e-04 - 12s/epoch - 137us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.5760e-04 - val_loss: 2.7187e-04 - 12s/epoch - 138us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.5761e-04 - val_loss: 2.7841e-04 - 12s/epoch - 138us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.5778e-04 - val_loss: 2.7521e-04 - 12s/epoch - 138us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.5657e-04 - val_loss: 2.7725e-04 - 12s/epoch - 138us/sample
Epoch 63/85
84077/84077 - 12s - loss: 2.5642e-04 - val_loss: 2.7067e-04 - 12s/epoch - 138us/sample
Epoch 64/85
84077/84077 - 12s - loss: 2.5559e-04 - val_loss: 2.7692e-04 - 12s/epoch - 137us/sample
Epoch 65/85
84077/84077 - 12s - loss: 2.5526e-04 - val_loss: 2.8130e-04 - 12s/epoch - 137us/sample
Epoch 66/85
84077/84077 - 12s - loss: 2.5461e-04 - val_loss: 2.8075e-04 - 12s/epoch - 138us/sample
Epoch 67/85
84077/84077 - 12s - loss: 2.5430e-04 - val_loss: 2.7685e-04 - 12s/epoch - 138us/sample
Epoch 68/85
84077/84077 - 12s - loss: 2.5391e-04 - val_loss: 2.7669e-04 - 12s/epoch - 137us/sample
Epoch 69/85
84077/84077 - 11s - loss: 2.5356e-04 - val_loss: 2.7819e-04 - 11s/epoch - 135us/sample
Epoch 70/85
84077/84077 - 11s - loss: 2.5383e-04 - val_loss: 2.6453e-04 - 11s/epoch - 135us/sample
Epoch 71/85
84077/84077 - 12s - loss: 2.5346e-04 - val_loss: 2.6850e-04 - 12s/epoch - 138us/sample
Epoch 72/85
84077/84077 - 12s - loss: 2.5346e-04 - val_loss: 2.7596e-04 - 12s/epoch - 138us/sample
Epoch 73/85
84077/84077 - 12s - loss: 2.5339e-04 - val_loss: 2.7246e-04 - 12s/epoch - 138us/sample
Epoch 74/85
84077/84077 - 12s - loss: 2.5219e-04 - val_loss: 2.6959e-04 - 12s/epoch - 138us/sample
Epoch 75/85
84077/84077 - 12s - loss: 2.5162e-04 - val_loss: 2.6943e-04 - 12s/epoch - 137us/sample
Epoch 76/85
84077/84077 - 12s - loss: 2.5136e-04 - val_loss: 2.6965e-04 - 12s/epoch - 137us/sample
Epoch 77/85
84077/84077 - 12s - loss: 2.5167e-04 - val_loss: 2.6361e-04 - 12s/epoch - 138us/sample
Epoch 78/85
84077/84077 - 12s - loss: 2.5088e-04 - val_loss: 2.6897e-04 - 12s/epoch - 139us/sample
Epoch 79/85
84077/84077 - 12s - loss: 2.5055e-04 - val_loss: 2.7031e-04 - 12s/epoch - 138us/sample
Epoch 80/85
84077/84077 - 12s - loss: 2.5024e-04 - val_loss: 2.6442e-04 - 12s/epoch - 138us/sample
Epoch 81/85
84077/84077 - 12s - loss: 2.4968e-04 - val_loss: 2.6868e-04 - 12s/epoch - 137us/sample
Epoch 82/85
84077/84077 - 12s - loss: 2.4928e-04 - val_loss: 2.6541e-04 - 12s/epoch - 138us/sample
Epoch 83/85
84077/84077 - 12s - loss: 2.4931e-04 - val_loss: 2.6867e-04 - 12s/epoch - 138us/sample
Epoch 84/85
84077/84077 - 12s - loss: 2.4890e-04 - val_loss: 2.6625e-04 - 12s/epoch - 138us/sample
Epoch 85/85
84077/84077 - 11s - loss: 2.4862e-04 - val_loss: 2.6438e-04 - 11s/epoch - 136us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00026437795299646277
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 10:19:16.671066: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_20/outputlayer/BiasAdd' id:25797 op device:{requested: '', assigned: ''} def:{{{node decoder_model_20/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_20/outputlayer/MatMul, decoder_model_20/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03388045798848203
cosine 0.03346414385998423
MAE: 0.002479053967673343
RMSE: 0.012826933263784703
r2: 0.8715455668423285
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 85, 0.001, 0.2, 188, 0.0002486198764870203, 0.00026437795299646277, 0.03388045798848203, 0.03346414385998423, 0.002479053967673343, 0.012826933263784703, 0.8715455668423285, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_63 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_63 (ReLU)                (None, 1980)         0           ['batch_normalization_63[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 10:19:28.027658: W tensorflow/c/c_api.cc:291] Operation '{name:'training_42/Adam/batch_normalization_65/gamma/m/Assign' id:27622 op device:{requested: '', assigned: ''} def:{{{node training_42/Adam/batch_normalization_65/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_42/Adam/batch_normalization_65/gamma/m, training_42/Adam/batch_normalization_65/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 10:19:41.440302: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_21/mul' id:27081 op device:{requested: '', assigned: ''} def:{{{node loss_21/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_21/mul/x, loss_21/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0229 - val_loss: 0.0021 - 17s/epoch - 203us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0022 - val_loss: 0.0028 - 12s/epoch - 140us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0013 - val_loss: 9.4492e-04 - 12s/epoch - 140us/sample
Epoch 4/85
84077/84077 - 12s - loss: 0.0013 - val_loss: 7.1405e-04 - 12s/epoch - 140us/sample
Epoch 5/85
84077/84077 - 12s - loss: 7.0787e-04 - val_loss: 5.5945e-04 - 12s/epoch - 140us/sample
Epoch 6/85
84077/84077 - 12s - loss: 5.8476e-04 - val_loss: 4.6598e-04 - 12s/epoch - 141us/sample
Epoch 7/85
84077/84077 - 12s - loss: 5.0676e-04 - val_loss: 4.2510e-04 - 12s/epoch - 141us/sample
Epoch 8/85
84077/84077 - 12s - loss: 4.5702e-04 - val_loss: 3.8439e-04 - 12s/epoch - 142us/sample
Epoch 9/85
84077/84077 - 12s - loss: 4.1945e-04 - val_loss: 3.5301e-04 - 12s/epoch - 141us/sample
Epoch 10/85
84077/84077 - 12s - loss: 3.8696e-04 - val_loss: 3.3618e-04 - 12s/epoch - 140us/sample
Epoch 11/85
84077/84077 - 12s - loss: 3.7068e-04 - val_loss: 3.1056e-04 - 12s/epoch - 141us/sample
Epoch 12/85
84077/84077 - 12s - loss: 3.5384e-04 - val_loss: 3.0363e-04 - 12s/epoch - 140us/sample
Epoch 13/85
84077/84077 - 12s - loss: 3.3724e-04 - val_loss: 2.9069e-04 - 12s/epoch - 141us/sample
Epoch 14/85
84077/84077 - 12s - loss: 3.3474e-04 - val_loss: 2.8583e-04 - 12s/epoch - 140us/sample
Epoch 15/85
84077/84077 - 12s - loss: 3.2055e-04 - val_loss: 2.7990e-04 - 12s/epoch - 140us/sample
Epoch 16/85
84077/84077 - 12s - loss: 3.1110e-04 - val_loss: 2.7516e-04 - 12s/epoch - 140us/sample
Epoch 17/85
84077/84077 - 12s - loss: 3.0760e-04 - val_loss: 2.6799e-04 - 12s/epoch - 140us/sample
Epoch 18/85
84077/84077 - 12s - loss: 3.0347e-04 - val_loss: 2.6169e-04 - 12s/epoch - 142us/sample
Epoch 19/85
84077/84077 - 12s - loss: 2.9832e-04 - val_loss: 2.6415e-04 - 12s/epoch - 140us/sample
Epoch 20/85
84077/84077 - 12s - loss: 2.8930e-04 - val_loss: 2.5632e-04 - 12s/epoch - 141us/sample
Epoch 21/85
84077/84077 - 12s - loss: 2.8697e-04 - val_loss: 2.5555e-04 - 12s/epoch - 140us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.8235e-04 - val_loss: 2.5136e-04 - 12s/epoch - 141us/sample
Epoch 23/85
84077/84077 - 12s - loss: 2.8486e-04 - val_loss: 2.6348e-04 - 12s/epoch - 141us/sample
Epoch 24/85
84077/84077 - 12s - loss: 2.8633e-04 - val_loss: 2.5716e-04 - 12s/epoch - 141us/sample
Epoch 25/85
84077/84077 - 12s - loss: 2.7753e-04 - val_loss: 2.4723e-04 - 12s/epoch - 141us/sample
Epoch 26/85
84077/84077 - 12s - loss: 2.7215e-04 - val_loss: 2.4274e-04 - 12s/epoch - 141us/sample
Epoch 27/85
84077/84077 - 12s - loss: 2.6784e-04 - val_loss: 2.3782e-04 - 12s/epoch - 141us/sample
Epoch 28/85
84077/84077 - 12s - loss: 2.7075e-04 - val_loss: 2.3994e-04 - 12s/epoch - 140us/sample
Epoch 29/85
84077/84077 - 12s - loss: 2.6332e-04 - val_loss: 2.3668e-04 - 12s/epoch - 140us/sample
Epoch 30/85
84077/84077 - 12s - loss: 2.6345e-04 - val_loss: 2.3611e-04 - 12s/epoch - 139us/sample
Epoch 31/85
84077/84077 - 12s - loss: 2.6044e-04 - val_loss: 2.3335e-04 - 12s/epoch - 141us/sample
Epoch 32/85
84077/84077 - 12s - loss: 2.5925e-04 - val_loss: 2.3401e-04 - 12s/epoch - 141us/sample
Epoch 33/85
84077/84077 - 12s - loss: 2.5764e-04 - val_loss: 2.2886e-04 - 12s/epoch - 140us/sample
Epoch 34/85
84077/84077 - 12s - loss: 2.5531e-04 - val_loss: 2.2742e-04 - 12s/epoch - 139us/sample
Epoch 35/85
84077/84077 - 12s - loss: 2.5594e-04 - val_loss: 2.2989e-04 - 12s/epoch - 140us/sample
Epoch 36/85
84077/84077 - 12s - loss: 2.5339e-04 - val_loss: 2.5780e-04 - 12s/epoch - 142us/sample
Epoch 37/85
84077/84077 - 12s - loss: 2.5066e-04 - val_loss: 2.2847e-04 - 12s/epoch - 141us/sample
Epoch 38/85
84077/84077 - 12s - loss: 2.5107e-04 - val_loss: 2.3390e-04 - 12s/epoch - 141us/sample
Epoch 39/85
84077/84077 - 12s - loss: 2.5706e-04 - val_loss: 2.2359e-04 - 12s/epoch - 140us/sample
Epoch 40/85
84077/84077 - 12s - loss: 2.4903e-04 - val_loss: 2.2375e-04 - 12s/epoch - 141us/sample
Epoch 41/85
84077/84077 - 12s - loss: 2.4635e-04 - val_loss: 2.2168e-04 - 12s/epoch - 141us/sample
Epoch 42/85
84077/84077 - 12s - loss: 2.4446e-04 - val_loss: 2.2048e-04 - 12s/epoch - 140us/sample
Epoch 43/85
84077/84077 - 12s - loss: 2.4643e-04 - val_loss: 2.4531e-04 - 12s/epoch - 140us/sample
Epoch 44/85
84077/84077 - 12s - loss: 2.4323e-04 - val_loss: 2.2067e-04 - 12s/epoch - 139us/sample
Epoch 45/85
84077/84077 - 12s - loss: 2.4315e-04 - val_loss: 2.2230e-04 - 12s/epoch - 141us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.4051e-04 - val_loss: 2.1700e-04 - 12s/epoch - 140us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.4237e-04 - val_loss: 2.1852e-04 - 12s/epoch - 140us/sample
Epoch 48/85
84077/84077 - 12s - loss: 2.3940e-04 - val_loss: 2.1584e-04 - 12s/epoch - 139us/sample
Epoch 49/85
84077/84077 - 12s - loss: 2.3849e-04 - val_loss: 2.1469e-04 - 12s/epoch - 141us/sample
Epoch 50/85
84077/84077 - 12s - loss: 2.3707e-04 - val_loss: 2.1410e-04 - 12s/epoch - 141us/sample
Epoch 51/85
84077/84077 - 12s - loss: 2.3721e-04 - val_loss: 2.1721e-04 - 12s/epoch - 140us/sample
Epoch 52/85
84077/84077 - 12s - loss: 2.3794e-04 - val_loss: 2.2118e-04 - 12s/epoch - 140us/sample
Epoch 53/85
84077/84077 - 12s - loss: 2.5325e-04 - val_loss: 2.2016e-04 - 12s/epoch - 141us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.4028e-04 - val_loss: 2.2031e-04 - 12s/epoch - 141us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.3716e-04 - val_loss: 2.1689e-04 - 12s/epoch - 141us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.3467e-04 - val_loss: 2.1467e-04 - 12s/epoch - 140us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.3993e-04 - val_loss: 2.1201e-04 - 12s/epoch - 140us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.3384e-04 - val_loss: 2.1276e-04 - 12s/epoch - 141us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.3610e-04 - val_loss: 2.1177e-04 - 12s/epoch - 140us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.3200e-04 - val_loss: 2.0929e-04 - 12s/epoch - 141us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.3157e-04 - val_loss: 2.0987e-04 - 12s/epoch - 139us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.3113e-04 - val_loss: 2.0879e-04 - 12s/epoch - 140us/sample
Epoch 63/85
84077/84077 - 12s - loss: 2.3451e-04 - val_loss: 2.1837e-04 - 12s/epoch - 139us/sample
Epoch 64/85
84077/84077 - 12s - loss: 2.2912e-04 - val_loss: 2.1061e-04 - 12s/epoch - 141us/sample
Epoch 65/85
84077/84077 - 12s - loss: 2.2802e-04 - val_loss: 2.0790e-04 - 12s/epoch - 140us/sample
Epoch 66/85
84077/84077 - 12s - loss: 2.2917e-04 - val_loss: 2.0847e-04 - 12s/epoch - 140us/sample
Epoch 67/85
84077/84077 - 12s - loss: 2.2837e-04 - val_loss: 2.1108e-04 - 12s/epoch - 139us/sample
Epoch 68/85
84077/84077 - 12s - loss: 2.2966e-04 - val_loss: 2.0454e-04 - 12s/epoch - 140us/sample
Epoch 69/85
84077/84077 - 12s - loss: 2.3000e-04 - val_loss: 2.0708e-04 - 12s/epoch - 140us/sample
Epoch 70/85
84077/84077 - 12s - loss: 2.3330e-04 - val_loss: 2.0731e-04 - 12s/epoch - 140us/sample
Epoch 71/85
84077/84077 - 12s - loss: 2.2572e-04 - val_loss: 2.0606e-04 - 12s/epoch - 139us/sample
Epoch 72/85
84077/84077 - 12s - loss: 2.2488e-04 - val_loss: 2.0611e-04 - 12s/epoch - 140us/sample
Epoch 73/85
84077/84077 - 12s - loss: 2.4245e-04 - val_loss: 2.0965e-04 - 12s/epoch - 140us/sample
Epoch 74/85
84077/84077 - 12s - loss: 2.2885e-04 - val_loss: 2.1927e-04 - 12s/epoch - 140us/sample
Epoch 75/85
84077/84077 - 12s - loss: 2.2832e-04 - val_loss: 2.2348e-04 - 12s/epoch - 140us/sample
Epoch 76/85
84077/84077 - 12s - loss: 2.2744e-04 - val_loss: 2.1176e-04 - 12s/epoch - 140us/sample
Epoch 77/85
84077/84077 - 12s - loss: 2.2327e-04 - val_loss: 2.0636e-04 - 12s/epoch - 141us/sample
Epoch 78/85
84077/84077 - 12s - loss: 2.2275e-04 - val_loss: 2.0652e-04 - 12s/epoch - 141us/sample
Epoch 79/85
84077/84077 - 12s - loss: 2.2504e-04 - val_loss: 2.0703e-04 - 12s/epoch - 141us/sample
Epoch 80/85
84077/84077 - 12s - loss: 2.2366e-04 - val_loss: 2.0621e-04 - 12s/epoch - 140us/sample
Epoch 81/85
84077/84077 - 12s - loss: 2.2241e-04 - val_loss: 2.0498e-04 - 12s/epoch - 140us/sample
Epoch 82/85
84077/84077 - 12s - loss: 2.2502e-04 - val_loss: 2.0507e-04 - 12s/epoch - 139us/sample
Epoch 83/85
84077/84077 - 12s - loss: 2.2238e-04 - val_loss: 2.0486e-04 - 12s/epoch - 140us/sample
Epoch 84/85
84077/84077 - 12s - loss: 2.2058e-04 - val_loss: 2.0556e-04 - 12s/epoch - 141us/sample
Epoch 85/85
84077/84077 - 12s - loss: 2.2544e-04 - val_loss: 2.0232e-04 - 12s/epoch - 140us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002023182690644637
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 10:36:14.799917: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_21/outputlayer/BiasAdd' id:27052 op device:{requested: '', assigned: ''} def:{{{node decoder_model_21/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_21/outputlayer/MatMul, decoder_model_21/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.025209844019739577
cosine 0.02490657739136039
MAE: 0.0022992428480605896
RMSE: 0.009691495618328997
r2: 0.9267725406086063
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 85, 0.001, 0.2, 188, 0.0002254358124551368, 0.0002023182690644637, 0.025209844019739577, 0.02490657739136039, 0.0022992428480605896, 0.009691495618328997, 0.9267725406086063, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 32 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_66 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_66 (ReLU)                (None, 1886)         0           ['batch_normalization_66[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 10:36:26.578570: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_66/moving_variance/Assign' id:27942 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_66/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_66/moving_variance, batch_normalization_66/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 10:36:40.195223: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_22/mul' id:28343 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0042 - val_loss: 0.0552 - 17s/epoch - 208us/sample
Epoch 2/90
84077/84077 - 12s - loss: 0.0026 - val_loss: 8.7052e-04 - 12s/epoch - 142us/sample
Epoch 3/90
84077/84077 - 12s - loss: 7.4933e-04 - val_loss: 7.0495e-04 - 12s/epoch - 141us/sample
Epoch 4/90
84077/84077 - 12s - loss: 9.8578e-04 - val_loss: 8.3743e-04 - 12s/epoch - 143us/sample
Epoch 5/90
84077/84077 - 12s - loss: 5.8846e-04 - val_loss: 4.5317e-04 - 12s/epoch - 141us/sample
Epoch 6/90
84077/84077 - 12s - loss: 5.6644e-04 - val_loss: 4.2843e-04 - 12s/epoch - 140us/sample
Epoch 7/90
84077/84077 - 12s - loss: 4.3715e-04 - val_loss: 4.1308e-04 - 12s/epoch - 140us/sample
Epoch 8/90
84077/84077 - 12s - loss: 4.3772e-04 - val_loss: 3.4437e-04 - 12s/epoch - 141us/sample
Epoch 9/90
84077/84077 - 12s - loss: 3.6501e-04 - val_loss: 3.1703e-04 - 12s/epoch - 141us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.4359e-04 - val_loss: 3.0362e-04 - 12s/epoch - 144us/sample
Epoch 11/90
84077/84077 - 12s - loss: 3.2629e-04 - val_loss: 2.9551e-04 - 12s/epoch - 144us/sample
Epoch 12/90
84077/84077 - 12s - loss: 3.1555e-04 - val_loss: 2.9034e-04 - 12s/epoch - 142us/sample
Epoch 13/90
84077/84077 - 12s - loss: 3.0911e-04 - val_loss: 2.8212e-04 - 12s/epoch - 143us/sample
Epoch 14/90
84077/84077 - 12s - loss: 3.0133e-04 - val_loss: 2.7843e-04 - 12s/epoch - 142us/sample
Epoch 15/90
84077/84077 - 12s - loss: 2.9637e-04 - val_loss: 2.7369e-04 - 12s/epoch - 142us/sample
Epoch 16/90
84077/84077 - 12s - loss: 2.9242e-04 - val_loss: 2.7138e-04 - 12s/epoch - 142us/sample
Epoch 17/90
84077/84077 - 12s - loss: 2.8857e-04 - val_loss: 2.6416e-04 - 12s/epoch - 142us/sample
Epoch 18/90
84077/84077 - 12s - loss: 2.8496e-04 - val_loss: 2.6565e-04 - 12s/epoch - 143us/sample
Epoch 19/90
84077/84077 - 12s - loss: 2.8247e-04 - val_loss: 2.6378e-04 - 12s/epoch - 142us/sample
Epoch 20/90
84077/84077 - 12s - loss: 2.7999e-04 - val_loss: 2.6323e-04 - 12s/epoch - 142us/sample
Epoch 21/90
84077/84077 - 12s - loss: 2.7759e-04 - val_loss: 2.5776e-04 - 12s/epoch - 143us/sample
Epoch 22/90
84077/84077 - 12s - loss: 2.7623e-04 - val_loss: 2.5599e-04 - 12s/epoch - 142us/sample
Epoch 23/90
84077/84077 - 12s - loss: 2.7385e-04 - val_loss: 2.5896e-04 - 12s/epoch - 142us/sample
Epoch 24/90
84077/84077 - 12s - loss: 2.7268e-04 - val_loss: 2.5565e-04 - 12s/epoch - 142us/sample
Epoch 25/90
84077/84077 - 12s - loss: 2.7036e-04 - val_loss: 2.5369e-04 - 12s/epoch - 143us/sample
Epoch 26/90
84077/84077 - 12s - loss: 2.6957e-04 - val_loss: 2.5196e-04 - 12s/epoch - 143us/sample
Epoch 27/90
84077/84077 - 12s - loss: 2.6821e-04 - val_loss: 2.5095e-04 - 12s/epoch - 142us/sample
Epoch 28/90
84077/84077 - 12s - loss: 2.6712e-04 - val_loss: 2.5231e-04 - 12s/epoch - 142us/sample
Epoch 29/90
84077/84077 - 12s - loss: 2.6588e-04 - val_loss: 2.5116e-04 - 12s/epoch - 142us/sample
Epoch 30/90
84077/84077 - 12s - loss: 2.6507e-04 - val_loss: 2.5006e-04 - 12s/epoch - 143us/sample
Epoch 31/90
84077/84077 - 12s - loss: 2.6432e-04 - val_loss: 2.4838e-04 - 12s/epoch - 144us/sample
Epoch 32/90
84077/84077 - 12s - loss: 2.6340e-04 - val_loss: 2.4558e-04 - 12s/epoch - 143us/sample
Epoch 33/90
84077/84077 - 12s - loss: 2.6204e-04 - val_loss: 2.4665e-04 - 12s/epoch - 141us/sample
Epoch 34/90
84077/84077 - 12s - loss: 2.6110e-04 - val_loss: 2.4572e-04 - 12s/epoch - 142us/sample
Epoch 35/90
84077/84077 - 12s - loss: 2.6090e-04 - val_loss: 2.4657e-04 - 12s/epoch - 144us/sample
Epoch 36/90
84077/84077 - 12s - loss: 2.5999e-04 - val_loss: 2.4526e-04 - 12s/epoch - 142us/sample
Epoch 37/90
84077/84077 - 12s - loss: 2.5897e-04 - val_loss: 2.4234e-04 - 12s/epoch - 141us/sample
Epoch 38/90
84077/84077 - 12s - loss: 2.5823e-04 - val_loss: 2.4238e-04 - 12s/epoch - 141us/sample
Epoch 39/90
84077/84077 - 12s - loss: 2.5724e-04 - val_loss: 2.4232e-04 - 12s/epoch - 142us/sample
Epoch 40/90
84077/84077 - 12s - loss: 2.5629e-04 - val_loss: 2.4093e-04 - 12s/epoch - 143us/sample
Epoch 41/90
84077/84077 - 12s - loss: 2.5542e-04 - val_loss: 2.3955e-04 - 12s/epoch - 142us/sample
Epoch 42/90
84077/84077 - 12s - loss: 2.5443e-04 - val_loss: 2.3883e-04 - 12s/epoch - 141us/sample
Epoch 43/90
84077/84077 - 12s - loss: 2.5326e-04 - val_loss: 2.3741e-04 - 12s/epoch - 141us/sample
Epoch 44/90
84077/84077 - 12s - loss: 2.5141e-04 - val_loss: 2.3799e-04 - 12s/epoch - 142us/sample
Epoch 45/90
84077/84077 - 12s - loss: 2.5055e-04 - val_loss: 2.3880e-04 - 12s/epoch - 143us/sample
Epoch 46/90
84077/84077 - 12s - loss: 2.4961e-04 - val_loss: 2.3533e-04 - 12s/epoch - 144us/sample
Epoch 47/90
84077/84077 - 12s - loss: 2.4909e-04 - val_loss: 2.3646e-04 - 12s/epoch - 142us/sample
Epoch 48/90
84077/84077 - 12s - loss: 2.4863e-04 - val_loss: 2.3666e-04 - 12s/epoch - 141us/sample
Epoch 49/90
84077/84077 - 12s - loss: 2.4786e-04 - val_loss: 2.3314e-04 - 12s/epoch - 146us/sample
Epoch 50/90
84077/84077 - 12s - loss: 2.4704e-04 - val_loss: 2.3429e-04 - 12s/epoch - 146us/sample
Epoch 51/90
84077/84077 - 12s - loss: 2.4693e-04 - val_loss: 2.3401e-04 - 12s/epoch - 143us/sample
Epoch 52/90
84077/84077 - 12s - loss: 2.4639e-04 - val_loss: 2.3285e-04 - 12s/epoch - 142us/sample
Epoch 53/90
84077/84077 - 12s - loss: 2.4583e-04 - val_loss: 2.3489e-04 - 12s/epoch - 142us/sample
Epoch 54/90
84077/84077 - 12s - loss: 2.4575e-04 - val_loss: 2.3272e-04 - 12s/epoch - 143us/sample
Epoch 55/90
84077/84077 - 12s - loss: 2.4547e-04 - val_loss: 2.3258e-04 - 12s/epoch - 142us/sample
Epoch 56/90
84077/84077 - 12s - loss: 2.4483e-04 - val_loss: 2.3196e-04 - 12s/epoch - 142us/sample
Epoch 57/90
84077/84077 - 12s - loss: 2.4500e-04 - val_loss: 2.2940e-04 - 12s/epoch - 142us/sample
Epoch 58/90
84077/84077 - 12s - loss: 2.4437e-04 - val_loss: 2.3068e-04 - 12s/epoch - 141us/sample
Epoch 59/90
84077/84077 - 12s - loss: 2.4458e-04 - val_loss: 2.3172e-04 - 12s/epoch - 142us/sample
Epoch 60/90
84077/84077 - 12s - loss: 2.4372e-04 - val_loss: 2.3260e-04 - 12s/epoch - 143us/sample
Epoch 61/90
84077/84077 - 12s - loss: 2.4334e-04 - val_loss: 2.2868e-04 - 12s/epoch - 142us/sample
Epoch 62/90
84077/84077 - 12s - loss: 2.4320e-04 - val_loss: 2.3031e-04 - 12s/epoch - 142us/sample
Epoch 63/90
84077/84077 - 12s - loss: 2.4289e-04 - val_loss: 2.2944e-04 - 12s/epoch - 142us/sample
Epoch 64/90
84077/84077 - 12s - loss: 2.4267e-04 - val_loss: 2.2914e-04 - 12s/epoch - 142us/sample
Epoch 65/90
84077/84077 - 12s - loss: 2.4241e-04 - val_loss: 2.2894e-04 - 12s/epoch - 142us/sample
Epoch 66/90
84077/84077 - 12s - loss: 2.4230e-04 - val_loss: 2.2720e-04 - 12s/epoch - 141us/sample
Epoch 67/90
84077/84077 - 12s - loss: 2.4210e-04 - val_loss: 2.2903e-04 - 12s/epoch - 142us/sample
Epoch 68/90
84077/84077 - 12s - loss: 2.4135e-04 - val_loss: 2.2849e-04 - 12s/epoch - 142us/sample
Epoch 69/90
84077/84077 - 12s - loss: 2.4179e-04 - val_loss: 2.2764e-04 - 12s/epoch - 142us/sample
Epoch 70/90
84077/84077 - 12s - loss: 2.4154e-04 - val_loss: 2.2769e-04 - 12s/epoch - 142us/sample
Epoch 71/90
84077/84077 - 12s - loss: 2.4063e-04 - val_loss: 2.2803e-04 - 12s/epoch - 143us/sample
Epoch 72/90
84077/84077 - 12s - loss: 2.4100e-04 - val_loss: 2.2914e-04 - 12s/epoch - 142us/sample
Epoch 73/90
84077/84077 - 12s - loss: 2.4062e-04 - val_loss: 2.2611e-04 - 12s/epoch - 143us/sample
Epoch 74/90
84077/84077 - 12s - loss: 2.4030e-04 - val_loss: 2.2832e-04 - 12s/epoch - 142us/sample
Epoch 75/90
84077/84077 - 12s - loss: 2.4015e-04 - val_loss: 2.2742e-04 - 12s/epoch - 142us/sample
Epoch 76/90
84077/84077 - 12s - loss: 2.3974e-04 - val_loss: 2.2596e-04 - 12s/epoch - 142us/sample
Epoch 77/90
84077/84077 - 12s - loss: 2.3945e-04 - val_loss: 2.2719e-04 - 12s/epoch - 140us/sample
Epoch 78/90
84077/84077 - 12s - loss: 2.3916e-04 - val_loss: 2.2599e-04 - 12s/epoch - 141us/sample
Epoch 79/90
84077/84077 - 12s - loss: 2.3926e-04 - val_loss: 2.2499e-04 - 12s/epoch - 141us/sample
Epoch 80/90
84077/84077 - 12s - loss: 2.3860e-04 - val_loss: 2.2563e-04 - 12s/epoch - 141us/sample
Epoch 81/90
84077/84077 - 12s - loss: 2.3842e-04 - val_loss: 2.2611e-04 - 12s/epoch - 142us/sample
Epoch 82/90
84077/84077 - 12s - loss: 2.3843e-04 - val_loss: 2.2546e-04 - 12s/epoch - 141us/sample
Epoch 83/90
84077/84077 - 12s - loss: 2.3848e-04 - val_loss: 2.2426e-04 - 12s/epoch - 140us/sample
Epoch 84/90
84077/84077 - 12s - loss: 2.3799e-04 - val_loss: 2.2419e-04 - 12s/epoch - 140us/sample
Epoch 85/90
84077/84077 - 12s - loss: 2.3761e-04 - val_loss: 2.2410e-04 - 12s/epoch - 143us/sample
Epoch 86/90
84077/84077 - 12s - loss: 2.3749e-04 - val_loss: 2.2499e-04 - 12s/epoch - 142us/sample
Epoch 87/90
84077/84077 - 12s - loss: 2.3729e-04 - val_loss: 2.2538e-04 - 12s/epoch - 140us/sample
Epoch 88/90
84077/84077 - 12s - loss: 2.3724e-04 - val_loss: 2.2592e-04 - 12s/epoch - 140us/sample
Epoch 89/90
84077/84077 - 12s - loss: 2.3667e-04 - val_loss: 2.2457e-04 - 12s/epoch - 141us/sample
Epoch 90/90
84077/84077 - 12s - loss: 2.3663e-04 - val_loss: 2.2426e-04 - 12s/epoch - 141us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00022426129331197861
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 10:54:25.483164: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_22/outputlayer/BiasAdd' id:28307 op device:{requested: '', assigned: ''} def:{{{node decoder_model_22/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_22/outputlayer/MatMul, decoder_model_22/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.08980568485664187
cosine 0.08866021040521549
MAE: 0.003344834597625637
RMSE: 0.01830724351068015
r2: 0.7381943608231774
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 32, 90, 0.001, 0.2, 188, 0.00023662828778659266, 0.00022426129331197861, 0.08980568485664187, 0.08866021040521549, 0.003344834597625637, 0.01830724351068015, 0.7381943608231774, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0008 32 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_69 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_69 (ReLU)                (None, 1980)         0           ['batch_normalization_69[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 10:54:37.456596: W tensorflow/c/c_api.cc:291] Operation '{name:'training_46/Adam/batch_normalization_69/gamma/m/Assign' id:30118 op device:{requested: '', assigned: ''} def:{{{node training_46/Adam/batch_normalization_69/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_46/Adam/batch_normalization_69/gamma/m, training_46/Adam/batch_normalization_69/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 10:54:51.035075: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_23/mul' id:29628 op device:{requested: '', assigned: ''} def:{{{node loss_23/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_23/mul/x, loss_23/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 1.4776 - val_loss: 0.0013 - 17s/epoch - 208us/sample
Epoch 2/90
84077/84077 - 12s - loss: 0.0805 - val_loss: 6.9451e-04 - 12s/epoch - 141us/sample
Epoch 3/90
84077/84077 - 12s - loss: 6.4469e-04 - val_loss: 4.9002e-04 - 12s/epoch - 141us/sample
Epoch 4/90
84077/84077 - 12s - loss: 7.4470e-04 - val_loss: 4.6722e-04 - 12s/epoch - 141us/sample
Epoch 5/90
84077/84077 - 12s - loss: 4.4109e-04 - val_loss: 3.7590e-04 - 12s/epoch - 140us/sample
Epoch 6/90
84077/84077 - 12s - loss: 3.9995e-04 - val_loss: 3.6728e-04 - 12s/epoch - 141us/sample
Epoch 7/90
84077/84077 - 12s - loss: 3.5739e-04 - val_loss: 3.0803e-04 - 12s/epoch - 141us/sample
Epoch 8/90
84077/84077 - 12s - loss: 3.4193e-04 - val_loss: 3.0209e-04 - 12s/epoch - 141us/sample
Epoch 9/90
84077/84077 - 12s - loss: 3.1424e-04 - val_loss: 3.6341e-04 - 12s/epoch - 141us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.0057e-04 - val_loss: 2.6751e-04 - 12s/epoch - 141us/sample
Epoch 11/90
84077/84077 - 12s - loss: 2.8852e-04 - val_loss: 2.6110e-04 - 12s/epoch - 142us/sample
Epoch 12/90
84077/84077 - 12s - loss: 2.7710e-04 - val_loss: 2.5567e-04 - 12s/epoch - 141us/sample
Epoch 13/90
84077/84077 - 12s - loss: 2.6783e-04 - val_loss: 2.4728e-04 - 12s/epoch - 141us/sample
Epoch 14/90
84077/84077 - 12s - loss: 2.6099e-04 - val_loss: 2.4138e-04 - 12s/epoch - 140us/sample
Epoch 15/90
84077/84077 - 12s - loss: 2.5380e-04 - val_loss: 2.3225e-04 - 12s/epoch - 142us/sample
Epoch 16/90
84077/84077 - 12s - loss: 2.4812e-04 - val_loss: 2.3396e-04 - 12s/epoch - 141us/sample
Epoch 17/90
84077/84077 - 12s - loss: 2.4428e-04 - val_loss: 2.3447e-04 - 12s/epoch - 141us/sample
Epoch 18/90
84077/84077 - 12s - loss: 2.4098e-04 - val_loss: 2.2747e-04 - 12s/epoch - 142us/sample
Epoch 19/90
84077/84077 - 12s - loss: 2.3846e-04 - val_loss: 2.2562e-04 - 12s/epoch - 141us/sample
Epoch 20/90
84077/84077 - 12s - loss: 2.3488e-04 - val_loss: 2.2070e-04 - 12s/epoch - 141us/sample
Epoch 21/90
84077/84077 - 12s - loss: 2.3215e-04 - val_loss: 2.1983e-04 - 12s/epoch - 141us/sample
Epoch 22/90
84077/84077 - 12s - loss: 2.2989e-04 - val_loss: 2.2082e-04 - 12s/epoch - 140us/sample
Epoch 23/90
84077/84077 - 12s - loss: 2.2824e-04 - val_loss: 2.1570e-04 - 12s/epoch - 141us/sample
Epoch 24/90
84077/84077 - 12s - loss: 2.2628e-04 - val_loss: 2.1317e-04 - 12s/epoch - 140us/sample
Epoch 25/90
84077/84077 - 12s - loss: 2.2517e-04 - val_loss: 2.1574e-04 - 12s/epoch - 142us/sample
Epoch 26/90
84077/84077 - 12s - loss: 2.2335e-04 - val_loss: 2.1379e-04 - 12s/epoch - 141us/sample
Epoch 27/90
84077/84077 - 12s - loss: 2.2213e-04 - val_loss: 2.1359e-04 - 12s/epoch - 141us/sample
Epoch 28/90
84077/84077 - 12s - loss: 2.2070e-04 - val_loss: 2.1144e-04 - 12s/epoch - 141us/sample
Epoch 29/90
84077/84077 - 12s - loss: 2.1961e-04 - val_loss: 2.1081e-04 - 12s/epoch - 140us/sample
Epoch 30/90
84077/84077 - 12s - loss: 2.1857e-04 - val_loss: 2.0813e-04 - 12s/epoch - 140us/sample
Epoch 31/90
84077/84077 - 12s - loss: 2.1720e-04 - val_loss: 2.0489e-04 - 12s/epoch - 142us/sample
Epoch 32/90
84077/84077 - 12s - loss: 2.1659e-04 - val_loss: 2.0527e-04 - 12s/epoch - 141us/sample
Epoch 33/90
84077/84077 - 12s - loss: 2.1533e-04 - val_loss: 2.0387e-04 - 12s/epoch - 140us/sample
Epoch 34/90
84077/84077 - 12s - loss: 2.1419e-04 - val_loss: 2.0390e-04 - 12s/epoch - 141us/sample
Epoch 35/90
84077/84077 - 12s - loss: 2.1304e-04 - val_loss: 2.0180e-04 - 12s/epoch - 141us/sample
Epoch 36/90
84077/84077 - 12s - loss: 2.1183e-04 - val_loss: 2.0303e-04 - 12s/epoch - 142us/sample
Epoch 37/90
84077/84077 - 12s - loss: 2.1195e-04 - val_loss: 2.0151e-04 - 12s/epoch - 141us/sample
Epoch 38/90
84077/84077 - 12s - loss: 2.1111e-04 - val_loss: 2.0268e-04 - 12s/epoch - 141us/sample
Epoch 39/90
84077/84077 - 12s - loss: 2.1075e-04 - val_loss: 1.9708e-04 - 12s/epoch - 140us/sample
Epoch 40/90
84077/84077 - 12s - loss: 2.0972e-04 - val_loss: 2.0307e-04 - 12s/epoch - 140us/sample
Epoch 41/90
84077/84077 - 12s - loss: 2.0945e-04 - val_loss: 1.9819e-04 - 12s/epoch - 141us/sample
Epoch 42/90
84077/84077 - 12s - loss: 2.0870e-04 - val_loss: 1.9662e-04 - 12s/epoch - 141us/sample
Epoch 43/90
84077/84077 - 12s - loss: 2.0813e-04 - val_loss: 1.9957e-04 - 12s/epoch - 142us/sample
Epoch 44/90
84077/84077 - 12s - loss: 2.0780e-04 - val_loss: 1.9916e-04 - 12s/epoch - 141us/sample
Epoch 45/90
84077/84077 - 12s - loss: 2.0778e-04 - val_loss: 1.9868e-04 - 12s/epoch - 141us/sample
Epoch 46/90
84077/84077 - 12s - loss: 2.0757e-04 - val_loss: 1.9713e-04 - 12s/epoch - 141us/sample
Epoch 47/90
84077/84077 - 12s - loss: 2.0631e-04 - val_loss: 1.9468e-04 - 12s/epoch - 141us/sample
Epoch 48/90
84077/84077 - 12s - loss: 2.0548e-04 - val_loss: 1.9802e-04 - 12s/epoch - 142us/sample
Epoch 49/90
84077/84077 - 12s - loss: 2.0513e-04 - val_loss: 1.9290e-04 - 12s/epoch - 141us/sample
Epoch 50/90
84077/84077 - 12s - loss: 2.0454e-04 - val_loss: 1.9345e-04 - 12s/epoch - 140us/sample
Epoch 51/90
84077/84077 - 12s - loss: 2.0382e-04 - val_loss: 1.9471e-04 - 12s/epoch - 141us/sample
Epoch 52/90
84077/84077 - 12s - loss: 2.0304e-04 - val_loss: 1.9391e-04 - 12s/epoch - 141us/sample
Epoch 53/90
84077/84077 - 12s - loss: 2.0234e-04 - val_loss: 1.9467e-04 - 12s/epoch - 141us/sample
Epoch 54/90
84077/84077 - 12s - loss: 2.0225e-04 - val_loss: 1.9233e-04 - 12s/epoch - 141us/sample
Epoch 55/90
84077/84077 - 12s - loss: 2.0162e-04 - val_loss: 1.9209e-04 - 12s/epoch - 141us/sample
Epoch 56/90
84077/84077 - 12s - loss: 2.0051e-04 - val_loss: 1.9478e-04 - 12s/epoch - 141us/sample
Epoch 57/90
84077/84077 - 12s - loss: 2.0009e-04 - val_loss: 1.8759e-04 - 12s/epoch - 142us/sample
Epoch 58/90
84077/84077 - 12s - loss: 1.9937e-04 - val_loss: 1.8940e-04 - 12s/epoch - 141us/sample
Epoch 59/90
84077/84077 - 12s - loss: 1.9854e-04 - val_loss: 1.8726e-04 - 12s/epoch - 141us/sample
Epoch 60/90
84077/84077 - 12s - loss: 1.9840e-04 - val_loss: 1.8962e-04 - 12s/epoch - 141us/sample
Epoch 61/90
84077/84077 - 12s - loss: 1.9805e-04 - val_loss: 1.8656e-04 - 12s/epoch - 141us/sample
Epoch 62/90
84077/84077 - 12s - loss: 1.9784e-04 - val_loss: 1.8721e-04 - 12s/epoch - 141us/sample
Epoch 63/90
84077/84077 - 12s - loss: 1.9774e-04 - val_loss: 1.8952e-04 - 12s/epoch - 141us/sample
Epoch 64/90
84077/84077 - 12s - loss: 1.9713e-04 - val_loss: 1.8736e-04 - 12s/epoch - 141us/sample
Epoch 65/90
84077/84077 - 12s - loss: 1.9714e-04 - val_loss: 1.8928e-04 - 12s/epoch - 141us/sample
Epoch 66/90
84077/84077 - 12s - loss: 1.9680e-04 - val_loss: 1.8621e-04 - 12s/epoch - 141us/sample
Epoch 67/90
84077/84077 - 12s - loss: 1.9641e-04 - val_loss: 1.8595e-04 - 12s/epoch - 140us/sample
Epoch 68/90
84077/84077 - 12s - loss: 1.9646e-04 - val_loss: 1.8484e-04 - 12s/epoch - 142us/sample
Epoch 69/90
84077/84077 - 12s - loss: 1.9599e-04 - val_loss: 1.8706e-04 - 12s/epoch - 141us/sample
Epoch 70/90
84077/84077 - 12s - loss: 1.9613e-04 - val_loss: 1.8536e-04 - 12s/epoch - 140us/sample
Epoch 71/90
84077/84077 - 12s - loss: 1.9567e-04 - val_loss: 1.8532e-04 - 12s/epoch - 141us/sample
Epoch 72/90
84077/84077 - 12s - loss: 1.9572e-04 - val_loss: 1.8499e-04 - 12s/epoch - 141us/sample
Epoch 73/90
84077/84077 - 12s - loss: 1.9514e-04 - val_loss: 1.8612e-04 - 12s/epoch - 142us/sample
Epoch 74/90
84077/84077 - 12s - loss: 1.9504e-04 - val_loss: 1.8477e-04 - 12s/epoch - 141us/sample
Epoch 75/90
84077/84077 - 12s - loss: 1.9472e-04 - val_loss: 1.8565e-04 - 12s/epoch - 141us/sample
Epoch 76/90
84077/84077 - 12s - loss: 1.9445e-04 - val_loss: 1.8391e-04 - 12s/epoch - 141us/sample
Epoch 77/90
84077/84077 - 12s - loss: 1.9416e-04 - val_loss: 1.8401e-04 - 12s/epoch - 141us/sample
Epoch 78/90
84077/84077 - 12s - loss: 1.9432e-04 - val_loss: 1.8363e-04 - 12s/epoch - 142us/sample
Epoch 79/90
84077/84077 - 12s - loss: 1.9384e-04 - val_loss: 1.8363e-04 - 12s/epoch - 141us/sample
Epoch 80/90
84077/84077 - 12s - loss: 1.9370e-04 - val_loss: 1.8492e-04 - 12s/epoch - 141us/sample
Epoch 81/90
84077/84077 - 12s - loss: 1.9349e-04 - val_loss: 1.8305e-04 - 12s/epoch - 141us/sample
Epoch 82/90
84077/84077 - 12s - loss: 1.9322e-04 - val_loss: 1.8389e-04 - 12s/epoch - 141us/sample
Epoch 83/90
84077/84077 - 12s - loss: 1.9317e-04 - val_loss: 1.8387e-04 - 12s/epoch - 142us/sample
Epoch 84/90
84077/84077 - 12s - loss: 1.9294e-04 - val_loss: 1.8258e-04 - 12s/epoch - 141us/sample
Epoch 85/90
84077/84077 - 12s - loss: 1.9271e-04 - val_loss: 1.8213e-04 - 12s/epoch - 140us/sample
Epoch 86/90
84077/84077 - 12s - loss: 1.9233e-04 - val_loss: 1.8132e-04 - 12s/epoch - 140us/sample
Epoch 87/90
84077/84077 - 12s - loss: 1.9228e-04 - val_loss: 1.8192e-04 - 12s/epoch - 141us/sample
Epoch 88/90
84077/84077 - 12s - loss: 1.9203e-04 - val_loss: 1.8288e-04 - 12s/epoch - 142us/sample
Epoch 89/90
84077/84077 - 12s - loss: 1.9217e-04 - val_loss: 1.8151e-04 - 12s/epoch - 141us/sample
Epoch 90/90
84077/84077 - 12s - loss: 1.9206e-04 - val_loss: 1.8222e-04 - 12s/epoch - 141us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018221971845597043
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 11:12:28.278957: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_23/outputlayer/BiasAdd' id:29592 op device:{requested: '', assigned: ''} def:{{{node decoder_model_23/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_23/outputlayer/MatMul, decoder_model_23/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.06147633069586382
cosine 0.06072670023225462
MAE: 0.0028992869857962945
RMSE: 0.015128749343946904
r2: 0.8212240785363796
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 32, 90, 0.0008, 0.2, 188, 0.000192063316505525, 0.00018221971845597043, 0.06147633069586382, 0.06072670023225462, 0.0028992869857962945, 0.015128749343946904, 0.8212240785363796, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 2
Fitness    = 512.1418577388646
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 512.1418577388646.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646]
[1.7999999999999998 85 0.0008 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_72 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_72 (ReLU)                (None, 1697)         0           ['batch_normalization_72[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 11:12:40.470951: W tensorflow/c/c_api.cc:291] Operation '{name:'training_48/Adam/bottleneck_zlog_24/bias/m/Assign' id:31407 op device:{requested: '', assigned: ''} def:{{{node training_48/Adam/bottleneck_zlog_24/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_48/Adam/bottleneck_zlog_24/bias/m, training_48/Adam/bottleneck_zlog_24/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 11:12:54.220344: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_24/mul' id:30906 op device:{requested: '', assigned: ''} def:{{{node loss_24/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_24/mul/x, loss_24/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0389 - val_loss: 0.0021 - 18s/epoch - 212us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0097 - val_loss: 0.0016 - 12s/epoch - 140us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 140us/sample
Epoch 4/85
84077/84077 - 12s - loss: 0.0010 - val_loss: 7.7282e-04 - 12s/epoch - 140us/sample
Epoch 5/85
84077/84077 - 12s - loss: 7.3861e-04 - val_loss: 5.9476e-04 - 12s/epoch - 140us/sample
Epoch 6/85
84077/84077 - 12s - loss: 6.0141e-04 - val_loss: 5.1523e-04 - 12s/epoch - 140us/sample
Epoch 7/85
84077/84077 - 12s - loss: 5.1696e-04 - val_loss: 4.4540e-04 - 12s/epoch - 140us/sample
Epoch 8/85
84077/84077 - 12s - loss: 4.5145e-04 - val_loss: 3.7555e-04 - 12s/epoch - 140us/sample
Epoch 9/85
84077/84077 - 12s - loss: 4.0116e-04 - val_loss: 3.4103e-04 - 12s/epoch - 141us/sample
Epoch 10/85
84077/84077 - 12s - loss: 3.6747e-04 - val_loss: 3.1819e-04 - 12s/epoch - 140us/sample
Epoch 11/85
84077/84077 - 12s - loss: 3.4192e-04 - val_loss: 2.9627e-04 - 12s/epoch - 140us/sample
Epoch 12/85
84077/84077 - 12s - loss: 3.2479e-04 - val_loss: 2.9277e-04 - 12s/epoch - 140us/sample
Epoch 13/85
84077/84077 - 12s - loss: 3.1105e-04 - val_loss: 2.7920e-04 - 12s/epoch - 141us/sample
Epoch 14/85
84077/84077 - 12s - loss: 3.0091e-04 - val_loss: 2.7364e-04 - 12s/epoch - 140us/sample
Epoch 15/85
84077/84077 - 12s - loss: 2.9250e-04 - val_loss: 2.6719e-04 - 12s/epoch - 141us/sample
Epoch 16/85
84077/84077 - 12s - loss: 2.8599e-04 - val_loss: 2.5865e-04 - 12s/epoch - 140us/sample
Epoch 17/85
84077/84077 - 12s - loss: 2.8082e-04 - val_loss: 2.5464e-04 - 12s/epoch - 140us/sample
Epoch 18/85
84077/84077 - 12s - loss: 2.7534e-04 - val_loss: 2.5069e-04 - 12s/epoch - 140us/sample
Epoch 19/85
84077/84077 - 12s - loss: 2.7012e-04 - val_loss: 2.5607e-04 - 12s/epoch - 141us/sample
Epoch 20/85
84077/84077 - 12s - loss: 2.6638e-04 - val_loss: 2.5253e-04 - 12s/epoch - 140us/sample
Epoch 21/85
84077/84077 - 12s - loss: 2.6499e-04 - val_loss: 2.5060e-04 - 12s/epoch - 140us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.5979e-04 - val_loss: 2.4940e-04 - 12s/epoch - 141us/sample
Epoch 23/85
84077/84077 - 12s - loss: 2.5722e-04 - val_loss: 2.4182e-04 - 12s/epoch - 140us/sample
Epoch 24/85
84077/84077 - 12s - loss: 2.5511e-04 - val_loss: 2.3918e-04 - 12s/epoch - 141us/sample
Epoch 25/85
84077/84077 - 12s - loss: 2.5215e-04 - val_loss: 2.3683e-04 - 12s/epoch - 140us/sample
Epoch 26/85
84077/84077 - 12s - loss: 2.4971e-04 - val_loss: 2.3737e-04 - 12s/epoch - 140us/sample
Epoch 27/85
84077/84077 - 12s - loss: 2.4833e-04 - val_loss: 2.3615e-04 - 12s/epoch - 140us/sample
Epoch 28/85
84077/84077 - 12s - loss: 2.4571e-04 - val_loss: 2.3284e-04 - 12s/epoch - 140us/sample
Epoch 29/85
84077/84077 - 12s - loss: 2.4290e-04 - val_loss: 2.3266e-04 - 12s/epoch - 141us/sample
Epoch 30/85
84077/84077 - 12s - loss: 2.4230e-04 - val_loss: 2.3039e-04 - 12s/epoch - 140us/sample
Epoch 31/85
84077/84077 - 12s - loss: 2.4059e-04 - val_loss: 2.3362e-04 - 12s/epoch - 140us/sample
Epoch 32/85
84077/84077 - 12s - loss: 2.3907e-04 - val_loss: 2.2562e-04 - 12s/epoch - 140us/sample
Epoch 33/85
84077/84077 - 12s - loss: 2.3732e-04 - val_loss: 2.2504e-04 - 12s/epoch - 141us/sample
Epoch 34/85
84077/84077 - 12s - loss: 2.3570e-04 - val_loss: 2.2780e-04 - 12s/epoch - 141us/sample
Epoch 35/85
84077/84077 - 12s - loss: 2.3447e-04 - val_loss: 2.3243e-04 - 12s/epoch - 141us/sample
Epoch 36/85
84077/84077 - 12s - loss: 2.3424e-04 - val_loss: 2.2845e-04 - 12s/epoch - 141us/sample
Epoch 37/85
84077/84077 - 12s - loss: 2.3218e-04 - val_loss: 2.2199e-04 - 12s/epoch - 140us/sample
Epoch 38/85
84077/84077 - 12s - loss: 2.3286e-04 - val_loss: 2.2146e-04 - 12s/epoch - 141us/sample
Epoch 39/85
84077/84077 - 12s - loss: 2.3201e-04 - val_loss: 2.2599e-04 - 12s/epoch - 141us/sample
Epoch 40/85
84077/84077 - 12s - loss: 2.2953e-04 - val_loss: 2.2308e-04 - 12s/epoch - 140us/sample
Epoch 41/85
84077/84077 - 12s - loss: 2.2937e-04 - val_loss: 2.2172e-04 - 12s/epoch - 140us/sample
Epoch 42/85
84077/84077 - 12s - loss: 2.2958e-04 - val_loss: 2.2075e-04 - 12s/epoch - 140us/sample
Epoch 43/85
84077/84077 - 12s - loss: 2.2724e-04 - val_loss: 2.1594e-04 - 12s/epoch - 141us/sample
Epoch 44/85
84077/84077 - 12s - loss: 2.2805e-04 - val_loss: 2.2181e-04 - 12s/epoch - 140us/sample
Epoch 45/85
84077/84077 - 12s - loss: 2.2611e-04 - val_loss: 2.1403e-04 - 12s/epoch - 140us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.2481e-04 - val_loss: 2.1744e-04 - 12s/epoch - 140us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.2481e-04 - val_loss: 2.1471e-04 - 12s/epoch - 140us/sample
Epoch 48/85
84077/84077 - 12s - loss: 2.2366e-04 - val_loss: 2.1525e-04 - 12s/epoch - 140us/sample
Epoch 49/85
84077/84077 - 12s - loss: 2.2333e-04 - val_loss: 2.1232e-04 - 12s/epoch - 142us/sample
Epoch 50/85
84077/84077 - 12s - loss: 2.2281e-04 - val_loss: 2.1608e-04 - 12s/epoch - 140us/sample
Epoch 51/85
84077/84077 - 12s - loss: 2.2171e-04 - val_loss: 2.1069e-04 - 12s/epoch - 140us/sample
Epoch 52/85
84077/84077 - 12s - loss: 2.2099e-04 - val_loss: 2.1149e-04 - 12s/epoch - 139us/sample
Epoch 53/85
84077/84077 - 12s - loss: 2.2103e-04 - val_loss: 2.1558e-04 - 12s/epoch - 140us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.2067e-04 - val_loss: 2.1034e-04 - 12s/epoch - 141us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.1931e-04 - val_loss: 2.0969e-04 - 12s/epoch - 141us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.1904e-04 - val_loss: 2.0963e-04 - 12s/epoch - 140us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.1875e-04 - val_loss: 2.1256e-04 - 12s/epoch - 140us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.1688e-04 - val_loss: 2.0824e-04 - 12s/epoch - 140us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.1820e-04 - val_loss: 2.0679e-04 - 12s/epoch - 140us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.1665e-04 - val_loss: 2.0977e-04 - 12s/epoch - 142us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.1670e-04 - val_loss: 2.0792e-04 - 12s/epoch - 140us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.1578e-04 - val_loss: 2.1178e-04 - 12s/epoch - 140us/sample
Epoch 63/85
84077/84077 - 12s - loss: 2.1591e-04 - val_loss: 2.0469e-04 - 12s/epoch - 140us/sample
Epoch 64/85
84077/84077 - 12s - loss: 2.1516e-04 - val_loss: 2.0584e-04 - 12s/epoch - 140us/sample
Epoch 65/85
84077/84077 - 12s - loss: 2.1480e-04 - val_loss: 2.0547e-04 - 12s/epoch - 142us/sample
Epoch 66/85
84077/84077 - 12s - loss: 2.1436e-04 - val_loss: 2.0608e-04 - 12s/epoch - 141us/sample
Epoch 67/85
84077/84077 - 12s - loss: 2.1441e-04 - val_loss: 2.0565e-04 - 12s/epoch - 140us/sample
Epoch 68/85
84077/84077 - 12s - loss: 2.1249e-04 - val_loss: 2.0528e-04 - 12s/epoch - 140us/sample
Epoch 69/85
84077/84077 - 12s - loss: 2.1463e-04 - val_loss: 2.0558e-04 - 12s/epoch - 140us/sample
Epoch 70/85
84077/84077 - 12s - loss: 2.1264e-04 - val_loss: 2.0247e-04 - 12s/epoch - 141us/sample
Epoch 71/85
84077/84077 - 12s - loss: 2.1282e-04 - val_loss: 2.0541e-04 - 12s/epoch - 140us/sample
Epoch 72/85
84077/84077 - 12s - loss: 2.1231e-04 - val_loss: 2.0242e-04 - 12s/epoch - 140us/sample
Epoch 73/85
84077/84077 - 12s - loss: 2.1190e-04 - val_loss: 2.0391e-04 - 12s/epoch - 140us/sample
Epoch 74/85
84077/84077 - 12s - loss: 2.1247e-04 - val_loss: 2.0275e-04 - 12s/epoch - 140us/sample
Epoch 75/85
84077/84077 - 12s - loss: 2.1052e-04 - val_loss: 2.0196e-04 - 12s/epoch - 141us/sample
Epoch 76/85
84077/84077 - 12s - loss: 2.1076e-04 - val_loss: 2.0567e-04 - 12s/epoch - 141us/sample
Epoch 77/85
84077/84077 - 12s - loss: 2.1123e-04 - val_loss: 1.9953e-04 - 12s/epoch - 140us/sample
Epoch 78/85
84077/84077 - 12s - loss: 2.1060e-04 - val_loss: 2.0309e-04 - 12s/epoch - 140us/sample
Epoch 79/85
84077/84077 - 12s - loss: 2.0968e-04 - val_loss: 2.0474e-04 - 12s/epoch - 140us/sample
Epoch 80/85
84077/84077 - 12s - loss: 2.0938e-04 - val_loss: 2.0174e-04 - 12s/epoch - 139us/sample
Epoch 81/85
84077/84077 - 12s - loss: 2.0965e-04 - val_loss: 1.9777e-04 - 12s/epoch - 138us/sample
Epoch 82/85
84077/84077 - 12s - loss: 2.0909e-04 - val_loss: 2.0246e-04 - 12s/epoch - 139us/sample
Epoch 83/85
84077/84077 - 12s - loss: 2.0859e-04 - val_loss: 2.0168e-04 - 12s/epoch - 140us/sample
Epoch 84/85
84077/84077 - 12s - loss: 2.1077e-04 - val_loss: 2.0260e-04 - 12s/epoch - 140us/sample
Epoch 85/85
84077/84077 - 12s - loss: 2.0880e-04 - val_loss: 1.9720e-04 - 12s/epoch - 142us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00019720434986548972
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 11:29:27.529953: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_24/outputlayer/BiasAdd' id:30877 op device:{requested: '', assigned: ''} def:{{{node decoder_model_24/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_24/outputlayer/MatMul, decoder_model_24/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023318774951882292
cosine 0.023037047357586386
MAE: 0.0020681779818467073
RMSE: 0.009459929457874456
r2: 0.9302703134537283
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 85, 0.0008, 0.2, 188, 0.00020879902098060608, 0.00019720434986548972, 0.023318774951882292, 0.023037047357586386, 0.0020681779818467073, 0.009459929457874456, 0.9302703134537283, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0006000000000000001 32 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_75 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_75 (ReLU)                (None, 1980)         0           ['batch_normalization_75[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 11:29:40.472967: W tensorflow/c/c_api.cc:291] Operation '{name:'training_50/Adam/batch_normalization_76/beta/m/Assign' id:32714 op device:{requested: '', assigned: ''} def:{{{node training_50/Adam/batch_normalization_76/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_50/Adam/batch_normalization_76/beta/m, training_50/Adam/batch_normalization_76/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 11:29:54.375536: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_25/mul' id:32171 op device:{requested: '', assigned: ''} def:{{{node loss_25/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul/x, loss_25/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0050 - val_loss: 0.0012 - 18s/epoch - 216us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0012 - val_loss: 0.0013 - 12s/epoch - 143us/sample
Epoch 3/85
84077/84077 - 12s - loss: 6.3261e-04 - val_loss: 5.0978e-04 - 12s/epoch - 143us/sample
Epoch 4/85
84077/84077 - 12s - loss: 4.9049e-04 - val_loss: 4.1144e-04 - 12s/epoch - 144us/sample
Epoch 5/85
84077/84077 - 12s - loss: 4.3467e-04 - val_loss: 3.7384e-04 - 12s/epoch - 143us/sample
Epoch 6/85
84077/84077 - 12s - loss: 4.0107e-04 - val_loss: 3.3517e-04 - 12s/epoch - 143us/sample
Epoch 7/85
84077/84077 - 12s - loss: 3.5689e-04 - val_loss: 3.3727e-04 - 12s/epoch - 143us/sample
Epoch 8/85
84077/84077 - 12s - loss: 3.3839e-04 - val_loss: 3.2230e-04 - 12s/epoch - 143us/sample
Epoch 9/85
84077/84077 - 12s - loss: 3.1987e-04 - val_loss: 2.8026e-04 - 12s/epoch - 143us/sample
Epoch 10/85
84077/84077 - 12s - loss: 3.0901e-04 - val_loss: 2.7214e-04 - 12s/epoch - 143us/sample
Epoch 11/85
84077/84077 - 12s - loss: 3.0061e-04 - val_loss: 2.6919e-04 - 12s/epoch - 144us/sample
Epoch 12/85
84077/84077 - 12s - loss: 2.9499e-04 - val_loss: 2.6285e-04 - 12s/epoch - 143us/sample
Epoch 13/85
84077/84077 - 12s - loss: 2.8783e-04 - val_loss: 2.5647e-04 - 12s/epoch - 143us/sample
Epoch 14/85
84077/84077 - 12s - loss: 2.8097e-04 - val_loss: 2.5122e-04 - 12s/epoch - 143us/sample
Epoch 15/85
84077/84077 - 12s - loss: 2.7554e-04 - val_loss: 2.4801e-04 - 12s/epoch - 143us/sample
Epoch 16/85
84077/84077 - 12s - loss: 2.7048e-04 - val_loss: 2.4334e-04 - 12s/epoch - 144us/sample
Epoch 17/85
84077/84077 - 12s - loss: 2.6497e-04 - val_loss: 2.3924e-04 - 12s/epoch - 143us/sample
Epoch 18/85
84077/84077 - 12s - loss: 2.6117e-04 - val_loss: 2.3336e-04 - 12s/epoch - 143us/sample
Epoch 19/85
84077/84077 - 12s - loss: 2.5734e-04 - val_loss: 2.3197e-04 - 12s/epoch - 143us/sample
Epoch 20/85
84077/84077 - 12s - loss: 2.5462e-04 - val_loss: 2.2940e-04 - 12s/epoch - 142us/sample
Epoch 21/85
84077/84077 - 12s - loss: 2.5173e-04 - val_loss: 2.2569e-04 - 12s/epoch - 144us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.4871e-04 - val_loss: 2.2625e-04 - 12s/epoch - 143us/sample
Epoch 23/85
84077/84077 - 12s - loss: 2.4561e-04 - val_loss: 2.2261e-04 - 12s/epoch - 143us/sample
Epoch 24/85
84077/84077 - 12s - loss: 2.4411e-04 - val_loss: 2.2199e-04 - 12s/epoch - 142us/sample
Epoch 25/85
84077/84077 - 12s - loss: 2.4203e-04 - val_loss: 2.1916e-04 - 12s/epoch - 144us/sample
Epoch 26/85
84077/84077 - 12s - loss: 2.4036e-04 - val_loss: 2.1989e-04 - 12s/epoch - 144us/sample
Epoch 27/85
84077/84077 - 12s - loss: 2.3819e-04 - val_loss: 2.1683e-04 - 12s/epoch - 142us/sample
Epoch 28/85
84077/84077 - 12s - loss: 2.3689e-04 - val_loss: 2.1645e-04 - 12s/epoch - 143us/sample
Epoch 29/85
84077/84077 - 12s - loss: 2.3496e-04 - val_loss: 2.1199e-04 - 12s/epoch - 143us/sample
Epoch 30/85
84077/84077 - 12s - loss: 2.3339e-04 - val_loss: 2.1131e-04 - 12s/epoch - 144us/sample
Epoch 31/85
84077/84077 - 12s - loss: 2.3197e-04 - val_loss: 2.1031e-04 - 12s/epoch - 143us/sample
Epoch 32/85
84077/84077 - 12s - loss: 2.3082e-04 - val_loss: 2.1034e-04 - 12s/epoch - 143us/sample
Epoch 33/85
84077/84077 - 12s - loss: 2.2876e-04 - val_loss: 2.0828e-04 - 12s/epoch - 142us/sample
Epoch 34/85
84077/84077 - 12s - loss: 2.2772e-04 - val_loss: 2.0748e-04 - 12s/epoch - 143us/sample
Epoch 35/85
84077/84077 - 12s - loss: 2.2614e-04 - val_loss: 2.0721e-04 - 12s/epoch - 145us/sample
Epoch 36/85
84077/84077 - 12s - loss: 2.2513e-04 - val_loss: 2.0494e-04 - 12s/epoch - 143us/sample
Epoch 37/85
84077/84077 - 12s - loss: 2.2369e-04 - val_loss: 2.0508e-04 - 12s/epoch - 142us/sample
Epoch 38/85
84077/84077 - 12s - loss: 2.2221e-04 - val_loss: 2.0319e-04 - 12s/epoch - 142us/sample
Epoch 39/85
84077/84077 - 12s - loss: 2.2033e-04 - val_loss: 2.0196e-04 - 12s/epoch - 143us/sample
Epoch 40/85
84077/84077 - 12s - loss: 2.1876e-04 - val_loss: 2.0058e-04 - 12s/epoch - 144us/sample
Epoch 41/85
84077/84077 - 12s - loss: 2.1787e-04 - val_loss: 2.0010e-04 - 12s/epoch - 144us/sample
Epoch 42/85
84077/84077 - 12s - loss: 2.1706e-04 - val_loss: 1.9739e-04 - 12s/epoch - 143us/sample
Epoch 43/85
84077/84077 - 12s - loss: 2.1614e-04 - val_loss: 1.9737e-04 - 12s/epoch - 143us/sample
Epoch 44/85
84077/84077 - 12s - loss: 2.1518e-04 - val_loss: 1.9612e-04 - 12s/epoch - 143us/sample
Epoch 45/85
84077/84077 - 12s - loss: 2.1466e-04 - val_loss: 1.9688e-04 - 12s/epoch - 144us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.1369e-04 - val_loss: 1.9702e-04 - 12s/epoch - 143us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.1215e-04 - val_loss: 1.9477e-04 - 12s/epoch - 143us/sample
Epoch 48/85
84077/84077 - 12s - loss: 2.1118e-04 - val_loss: 1.9449e-04 - 12s/epoch - 143us/sample
Epoch 49/85
84077/84077 - 12s - loss: 2.1079e-04 - val_loss: 1.9192e-04 - 12s/epoch - 143us/sample
Epoch 50/85
84077/84077 - 12s - loss: 2.1038e-04 - val_loss: 1.9264e-04 - 12s/epoch - 143us/sample
Epoch 51/85
84077/84077 - 12s - loss: 2.0916e-04 - val_loss: 1.9154e-04 - 12s/epoch - 143us/sample
Epoch 52/85
84077/84077 - 12s - loss: 2.0809e-04 - val_loss: 1.8975e-04 - 12s/epoch - 143us/sample
Epoch 53/85
84077/84077 - 12s - loss: 2.0765e-04 - val_loss: 1.9089e-04 - 12s/epoch - 142us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.0635e-04 - val_loss: 1.8848e-04 - 12s/epoch - 143us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.0584e-04 - val_loss: 1.8773e-04 - 12s/epoch - 142us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.0546e-04 - val_loss: 1.8813e-04 - 12s/epoch - 141us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.0413e-04 - val_loss: 1.8575e-04 - 12s/epoch - 142us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.0364e-04 - val_loss: 1.8628e-04 - 12s/epoch - 142us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.0259e-04 - val_loss: 1.8760e-04 - 12s/epoch - 143us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.0270e-04 - val_loss: 1.8557e-04 - 12s/epoch - 144us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.0086e-04 - val_loss: 1.8262e-04 - 12s/epoch - 143us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.0115e-04 - val_loss: 1.8228e-04 - 12s/epoch - 143us/sample
Epoch 63/85
84077/84077 - 12s - loss: 1.9947e-04 - val_loss: 1.8281e-04 - 12s/epoch - 143us/sample
Epoch 64/85
84077/84077 - 12s - loss: 1.9939e-04 - val_loss: 1.8588e-04 - 12s/epoch - 143us/sample
Epoch 65/85
84077/84077 - 12s - loss: 1.9939e-04 - val_loss: 1.8180e-04 - 12s/epoch - 144us/sample
Epoch 66/85
84077/84077 - 12s - loss: 1.9776e-04 - val_loss: 1.8119e-04 - 12s/epoch - 143us/sample
Epoch 67/85
84077/84077 - 12s - loss: 1.9726e-04 - val_loss: 1.7977e-04 - 12s/epoch - 143us/sample
Epoch 68/85
84077/84077 - 12s - loss: 1.9687e-04 - val_loss: 1.7979e-04 - 12s/epoch - 143us/sample
Epoch 69/85
84077/84077 - 12s - loss: 1.9576e-04 - val_loss: 1.7955e-04 - 12s/epoch - 145us/sample
Epoch 70/85
84077/84077 - 12s - loss: 1.9625e-04 - val_loss: 1.7978e-04 - 12s/epoch - 143us/sample
Epoch 71/85
84077/84077 - 12s - loss: 1.9514e-04 - val_loss: 1.7915e-04 - 12s/epoch - 143us/sample
Epoch 72/85
84077/84077 - 12s - loss: 1.9401e-04 - val_loss: 1.7710e-04 - 12s/epoch - 142us/sample
Epoch 73/85
84077/84077 - 12s - loss: 1.9374e-04 - val_loss: 1.7633e-04 - 12s/epoch - 142us/sample
Epoch 74/85
84077/84077 - 12s - loss: 1.9310e-04 - val_loss: 1.7559e-04 - 12s/epoch - 142us/sample
Epoch 75/85
84077/84077 - 12s - loss: 1.9215e-04 - val_loss: 1.7504e-04 - 12s/epoch - 143us/sample
Epoch 76/85
84077/84077 - 12s - loss: 1.9238e-04 - val_loss: 1.7378e-04 - 12s/epoch - 143us/sample
Epoch 77/85
84077/84077 - 12s - loss: 1.9197e-04 - val_loss: 1.7424e-04 - 12s/epoch - 143us/sample
Epoch 78/85
84077/84077 - 12s - loss: 1.9107e-04 - val_loss: 1.7338e-04 - 12s/epoch - 143us/sample
Epoch 79/85
84077/84077 - 12s - loss: 1.9042e-04 - val_loss: 1.7516e-04 - 12s/epoch - 143us/sample
Epoch 80/85
84077/84077 - 12s - loss: 1.8952e-04 - val_loss: 1.7237e-04 - 12s/epoch - 144us/sample
Epoch 81/85
84077/84077 - 12s - loss: 1.8915e-04 - val_loss: 1.7268e-04 - 12s/epoch - 143us/sample
Epoch 82/85
84077/84077 - 12s - loss: 1.8898e-04 - val_loss: 1.7170e-04 - 12s/epoch - 143us/sample
Epoch 83/85
84077/84077 - 12s - loss: 1.8745e-04 - val_loss: 1.6998e-04 - 12s/epoch - 142us/sample
Epoch 84/85
84077/84077 - 12s - loss: 1.8787e-04 - val_loss: 1.7190e-04 - 12s/epoch - 142us/sample
Epoch 85/85
84077/84077 - 12s - loss: 1.8641e-04 - val_loss: 1.6985e-04 - 12s/epoch - 144us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00016985271466156025
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 11:46:47.064019: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_25/outputlayer/BiasAdd' id:32135 op device:{requested: '', assigned: ''} def:{{{node decoder_model_25/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_25/outputlayer/MatMul, decoder_model_25/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05623934321359844
cosine 0.055548735216241193
MAE: 0.0027431561839975087
RMSE: 0.014107264710836023
r2: 0.8445503384156255
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 32, 85, 0.0006000000000000001, 0.2, 188, 0.00018640909894282925, 0.00016985271466156025, 0.05623934321359844, 0.055548735216241193, 0.0027431561839975087, 0.014107264710836023, 0.8445503384156255, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.001 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_78 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_78 (ReLU)                (None, 1697)         0           ['batch_normalization_78[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 11:47:00.172932: W tensorflow/c/c_api.cc:291] Operation '{name:'training_52/Adam/bottleneck_zlog_26/kernel/m/Assign' id:33942 op device:{requested: '', assigned: ''} def:{{{node training_52/Adam/bottleneck_zlog_26/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_52/Adam/bottleneck_zlog_26/kernel/m, training_52/Adam/bottleneck_zlog_26/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 11:47:14.215723: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_26/mul' id:33449 op device:{requested: '', assigned: ''} def:{{{node loss_26/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_26/mul/x, loss_26/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0062 - val_loss: 0.0029 - 19s/epoch - 221us/sample
Epoch 2/90
84077/84077 - 12s - loss: 0.0023 - val_loss: 0.0017 - 12s/epoch - 142us/sample
Epoch 3/90
84077/84077 - 12s - loss: 0.0012 - val_loss: 8.5494e-04 - 12s/epoch - 141us/sample
Epoch 4/90
84077/84077 - 12s - loss: 8.3374e-04 - val_loss: 6.4822e-04 - 12s/epoch - 142us/sample
Epoch 5/90
84077/84077 - 12s - loss: 7.7628e-04 - val_loss: 5.6459e-04 - 12s/epoch - 143us/sample
Epoch 6/90
84077/84077 - 12s - loss: 6.6209e-04 - val_loss: 0.0013 - 12s/epoch - 142us/sample
Epoch 7/90
84077/84077 - 12s - loss: 6.0403e-04 - val_loss: 4.6238e-04 - 12s/epoch - 142us/sample
Epoch 8/90
84077/84077 - 12s - loss: 4.8262e-04 - val_loss: 4.1992e-04 - 12s/epoch - 142us/sample
Epoch 9/90
84077/84077 - 12s - loss: 4.4130e-04 - val_loss: 3.8900e-04 - 12s/epoch - 142us/sample
Epoch 10/90
84077/84077 - 12s - loss: 4.0641e-04 - val_loss: 3.5242e-04 - 12s/epoch - 142us/sample
Epoch 11/90
84077/84077 - 12s - loss: 3.7863e-04 - val_loss: 3.4234e-04 - 12s/epoch - 142us/sample
Epoch 12/90
84077/84077 - 12s - loss: 3.5629e-04 - val_loss: 3.3156e-04 - 12s/epoch - 143us/sample
Epoch 13/90
84077/84077 - 12s - loss: 3.4519e-04 - val_loss: 3.1913e-04 - 12s/epoch - 143us/sample
Epoch 14/90
84077/84077 - 12s - loss: 3.2778e-04 - val_loss: 3.0747e-04 - 12s/epoch - 142us/sample
Epoch 15/90
84077/84077 - 12s - loss: 3.1773e-04 - val_loss: 3.0042e-04 - 12s/epoch - 141us/sample
Epoch 16/90
84077/84077 - 12s - loss: 3.0895e-04 - val_loss: 2.8982e-04 - 12s/epoch - 141us/sample
Epoch 17/90
84077/84077 - 12s - loss: 3.0111e-04 - val_loss: 2.8433e-04 - 12s/epoch - 144us/sample
Epoch 18/90
84077/84077 - 12s - loss: 2.9596e-04 - val_loss: 2.7863e-04 - 12s/epoch - 143us/sample
Epoch 19/90
84077/84077 - 12s - loss: 2.9072e-04 - val_loss: 2.7804e-04 - 12s/epoch - 143us/sample
Epoch 20/90
84077/84077 - 12s - loss: 2.9089e-04 - val_loss: 2.7387e-04 - 12s/epoch - 142us/sample
Epoch 21/90
84077/84077 - 12s - loss: 2.8274e-04 - val_loss: 2.6892e-04 - 12s/epoch - 142us/sample
Epoch 22/90
84077/84077 - 12s - loss: 2.7842e-04 - val_loss: 2.7064e-04 - 12s/epoch - 142us/sample
Epoch 23/90
84077/84077 - 12s - loss: 2.7520e-04 - val_loss: 2.6613e-04 - 12s/epoch - 143us/sample
Epoch 24/90
84077/84077 - 12s - loss: 2.7089e-04 - val_loss: 2.6040e-04 - 12s/epoch - 142us/sample
Epoch 25/90
84077/84077 - 12s - loss: 2.6908e-04 - val_loss: 2.6066e-04 - 12s/epoch - 142us/sample
Epoch 26/90
84077/84077 - 12s - loss: 2.6596e-04 - val_loss: 2.5658e-04 - 12s/epoch - 142us/sample
Epoch 27/90
84077/84077 - 12s - loss: 2.6347e-04 - val_loss: 2.5612e-04 - 12s/epoch - 142us/sample
Epoch 28/90
84077/84077 - 12s - loss: 2.6516e-04 - val_loss: 2.5414e-04 - 12s/epoch - 143us/sample
Epoch 29/90
84077/84077 - 12s - loss: 2.5940e-04 - val_loss: 2.5528e-04 - 12s/epoch - 143us/sample
Epoch 30/90
84077/84077 - 12s - loss: 2.5817e-04 - val_loss: 2.5474e-04 - 12s/epoch - 143us/sample
Epoch 31/90
84077/84077 - 12s - loss: 2.5549e-04 - val_loss: 2.5407e-04 - 12s/epoch - 142us/sample
Epoch 32/90
84077/84077 - 12s - loss: 2.5557e-04 - val_loss: 2.4957e-04 - 12s/epoch - 142us/sample
Epoch 33/90
84077/84077 - 12s - loss: 2.5374e-04 - val_loss: 2.4652e-04 - 12s/epoch - 143us/sample
Epoch 34/90
84077/84077 - 12s - loss: 2.5087e-04 - val_loss: 2.5091e-04 - 12s/epoch - 142us/sample
Epoch 35/90
84077/84077 - 12s - loss: 2.5169e-04 - val_loss: 2.4557e-04 - 12s/epoch - 142us/sample
Epoch 36/90
84077/84077 - 12s - loss: 2.4888e-04 - val_loss: 2.4561e-04 - 12s/epoch - 142us/sample
Epoch 37/90
84077/84077 - 12s - loss: 2.4787e-04 - val_loss: 2.4508e-04 - 12s/epoch - 142us/sample
Epoch 38/90
84077/84077 - 12s - loss: 2.4750e-04 - val_loss: 2.4235e-04 - 12s/epoch - 142us/sample
Epoch 39/90
84077/84077 - 12s - loss: 2.4525e-04 - val_loss: 2.3971e-04 - 12s/epoch - 142us/sample
Epoch 40/90
84077/84077 - 12s - loss: 2.4426e-04 - val_loss: 2.4059e-04 - 12s/epoch - 142us/sample
Epoch 41/90
84077/84077 - 12s - loss: 2.4747e-04 - val_loss: 2.4361e-04 - 12s/epoch - 141us/sample
Epoch 42/90
84077/84077 - 12s - loss: 2.4284e-04 - val_loss: 2.4298e-04 - 12s/epoch - 142us/sample
Epoch 43/90
84077/84077 - 12s - loss: 2.4147e-04 - val_loss: 2.3675e-04 - 12s/epoch - 143us/sample
Epoch 44/90
84077/84077 - 12s - loss: 2.4026e-04 - val_loss: 2.3624e-04 - 12s/epoch - 142us/sample
Epoch 45/90
84077/84077 - 12s - loss: 2.4247e-04 - val_loss: 2.3182e-04 - 12s/epoch - 141us/sample
Epoch 46/90
84077/84077 - 12s - loss: 2.4026e-04 - val_loss: 2.3063e-04 - 12s/epoch - 142us/sample
Epoch 47/90
84077/84077 - 12s - loss: 2.3751e-04 - val_loss: 2.3331e-04 - 12s/epoch - 143us/sample
Epoch 48/90
84077/84077 - 12s - loss: 2.3675e-04 - val_loss: 2.3435e-04 - 12s/epoch - 144us/sample
Epoch 49/90
84077/84077 - 12s - loss: 2.3700e-04 - val_loss: 2.2959e-04 - 12s/epoch - 143us/sample
Epoch 50/90
84077/84077 - 12s - loss: 2.3560e-04 - val_loss: 2.3066e-04 - 12s/epoch - 142us/sample
Epoch 51/90
84077/84077 - 12s - loss: 2.3500e-04 - val_loss: 2.3059e-04 - 12s/epoch - 142us/sample
Epoch 52/90
84077/84077 - 12s - loss: 2.3409e-04 - val_loss: 2.3194e-04 - 12s/epoch - 143us/sample
Epoch 53/90
84077/84077 - 12s - loss: 2.3308e-04 - val_loss: 2.2824e-04 - 12s/epoch - 146us/sample
Epoch 54/90
84077/84077 - 12s - loss: 2.3370e-04 - val_loss: 2.3145e-04 - 12s/epoch - 145us/sample
Epoch 55/90
84077/84077 - 12s - loss: 2.3127e-04 - val_loss: 2.2971e-04 - 12s/epoch - 143us/sample
Epoch 56/90
84077/84077 - 12s - loss: 2.3026e-04 - val_loss: 2.2821e-04 - 12s/epoch - 144us/sample
Epoch 57/90
84077/84077 - 12s - loss: 2.2984e-04 - val_loss: 2.2810e-04 - 12s/epoch - 144us/sample
Epoch 58/90
84077/84077 - 12s - loss: 2.3252e-04 - val_loss: 2.2595e-04 - 12s/epoch - 145us/sample
Epoch 59/90
84077/84077 - 12s - loss: 2.3280e-04 - val_loss: 2.2583e-04 - 12s/epoch - 145us/sample
Epoch 60/90
84077/84077 - 12s - loss: 2.3453e-04 - val_loss: 2.2410e-04 - 12s/epoch - 143us/sample
Epoch 61/90
84077/84077 - 12s - loss: 2.2877e-04 - val_loss: 2.2386e-04 - 12s/epoch - 143us/sample
Epoch 62/90
84077/84077 - 12s - loss: 2.2906e-04 - val_loss: 2.2377e-04 - 12s/epoch - 143us/sample
Epoch 63/90
84077/84077 - 12s - loss: 2.2789e-04 - val_loss: 2.1979e-04 - 12s/epoch - 144us/sample
Epoch 64/90
84077/84077 - 12s - loss: 2.2815e-04 - val_loss: 2.2174e-04 - 12s/epoch - 144us/sample
Epoch 65/90
84077/84077 - 12s - loss: 2.2633e-04 - val_loss: 2.2587e-04 - 12s/epoch - 144us/sample
Epoch 66/90
84077/84077 - 12s - loss: 2.2512e-04 - val_loss: 2.2277e-04 - 12s/epoch - 143us/sample
Epoch 67/90
84077/84077 - 12s - loss: 2.2520e-04 - val_loss: 2.2168e-04 - 12s/epoch - 144us/sample
Epoch 68/90
84077/84077 - 12s - loss: 2.2449e-04 - val_loss: 2.2037e-04 - 12s/epoch - 144us/sample
Epoch 69/90
84077/84077 - 12s - loss: 2.2413e-04 - val_loss: 2.1968e-04 - 12s/epoch - 144us/sample
Epoch 70/90
84077/84077 - 12s - loss: 2.2494e-04 - val_loss: 2.2052e-04 - 12s/epoch - 143us/sample
Epoch 71/90
84077/84077 - 12s - loss: 2.2468e-04 - val_loss: 2.1878e-04 - 12s/epoch - 143us/sample
Epoch 72/90
84077/84077 - 12s - loss: 2.2201e-04 - val_loss: 2.2043e-04 - 12s/epoch - 143us/sample
Epoch 73/90
84077/84077 - 12s - loss: 2.2205e-04 - val_loss: 2.2124e-04 - 12s/epoch - 145us/sample
Epoch 74/90
84077/84077 - 12s - loss: 2.3108e-04 - val_loss: 2.5428e-04 - 12s/epoch - 144us/sample
Epoch 75/90
84077/84077 - 12s - loss: 2.2740e-04 - val_loss: 2.1864e-04 - 12s/epoch - 144us/sample
Epoch 76/90
84077/84077 - 12s - loss: 2.2138e-04 - val_loss: 2.1884e-04 - 12s/epoch - 144us/sample
Epoch 77/90
84077/84077 - 12s - loss: 2.2025e-04 - val_loss: 2.1842e-04 - 12s/epoch - 143us/sample
Epoch 78/90
84077/84077 - 12s - loss: 2.2313e-04 - val_loss: 2.1673e-04 - 12s/epoch - 145us/sample
Epoch 79/90
84077/84077 - 12s - loss: 2.1994e-04 - val_loss: 2.1668e-04 - 12s/epoch - 143us/sample
Epoch 80/90
84077/84077 - 12s - loss: 2.2015e-04 - val_loss: 2.1577e-04 - 12s/epoch - 143us/sample
Epoch 81/90
84077/84077 - 12s - loss: 2.2026e-04 - val_loss: 2.1332e-04 - 12s/epoch - 143us/sample
Epoch 82/90
84077/84077 - 12s - loss: 2.1950e-04 - val_loss: 2.1558e-04 - 12s/epoch - 144us/sample
Epoch 83/90
84077/84077 - 12s - loss: 2.2555e-04 - val_loss: 2.2123e-04 - 12s/epoch - 144us/sample
Epoch 84/90
84077/84077 - 12s - loss: 2.1974e-04 - val_loss: 2.1264e-04 - 12s/epoch - 143us/sample
Epoch 85/90
84077/84077 - 12s - loss: 2.1835e-04 - val_loss: 2.1169e-04 - 12s/epoch - 143us/sample
Epoch 86/90
84077/84077 - 12s - loss: 2.1917e-04 - val_loss: 2.1610e-04 - 12s/epoch - 143us/sample
Epoch 87/90
84077/84077 - 12s - loss: 2.1800e-04 - val_loss: 2.1500e-04 - 12s/epoch - 145us/sample
Epoch 88/90
84077/84077 - 12s - loss: 2.1683e-04 - val_loss: 2.1438e-04 - 12s/epoch - 144us/sample
Epoch 89/90
84077/84077 - 12s - loss: 2.1690e-04 - val_loss: 2.1563e-04 - 12s/epoch - 144us/sample
Epoch 90/90
84077/84077 - 12s - loss: 2.1659e-04 - val_loss: 2.1219e-04 - 12s/epoch - 143us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00021218727033303083
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 12:05:06.449537: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_26/outputlayer/BiasAdd' id:33420 op device:{requested: '', assigned: ''} def:{{{node decoder_model_26/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_26/outputlayer/MatMul, decoder_model_26/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.026291326487343974
cosine 0.02599152167485278
MAE: 0.0021017939898032496
RMSE: 0.010313758702220661
r2: 0.9170039766114004
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 90, 0.001, 0.2, 188, 0.00021659343154575316, 0.00021218727033303083, 0.026291326487343974, 0.02599152167485278, 0.0021017939898032496, 0.010313758702220661, 0.9170039766114004, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 80 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_81 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_81 (ReLU)                (None, 1980)         0           ['batch_normalization_81[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-11 12:05:19.835308: W tensorflow/c/c_api.cc:291] Operation '{name:'training_54/Adam/dense_dec1_27/bias/v/Assign' id:35328 op device:{requested: '', assigned: ''} def:{{{node training_54/Adam/dense_dec1_27/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_54/Adam/dense_dec1_27/bias/v, training_54/Adam/dense_dec1_27/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 12:05:34.114249: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_27/mul' id:34704 op device:{requested: '', assigned: ''} def:{{{node loss_27/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_27/mul/x, loss_27/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0057 - val_loss: 0.0015 - 19s/epoch - 224us/sample
Epoch 2/80
84077/84077 - 12s - loss: 0.0028 - val_loss: 0.0015 - 12s/epoch - 144us/sample
Epoch 3/80
84077/84077 - 12s - loss: 0.0014 - val_loss: 9.9089e-04 - 12s/epoch - 144us/sample
Epoch 4/80
84077/84077 - 12s - loss: 9.3330e-04 - val_loss: 7.0718e-04 - 12s/epoch - 145us/sample
Epoch 5/80
84077/84077 - 12s - loss: 7.2495e-04 - val_loss: 5.7325e-04 - 12s/epoch - 145us/sample
Epoch 6/80
84077/84077 - 12s - loss: 6.1016e-04 - val_loss: 4.9415e-04 - 12s/epoch - 146us/sample
Epoch 7/80
84077/84077 - 12s - loss: 5.2965e-04 - val_loss: 4.5101e-04 - 12s/epoch - 145us/sample
Epoch 8/80
84077/84077 - 12s - loss: 4.8434e-04 - val_loss: 4.3136e-04 - 12s/epoch - 145us/sample
Epoch 9/80
84077/84077 - 12s - loss: 4.6273e-04 - val_loss: 3.9135e-04 - 12s/epoch - 144us/sample
Epoch 10/80
84077/84077 - 12s - loss: 4.3079e-04 - val_loss: 3.6953e-04 - 12s/epoch - 147us/sample
Epoch 11/80
84077/84077 - 12s - loss: 4.1786e-04 - val_loss: 3.6724e-04 - 12s/epoch - 145us/sample
Epoch 12/80
84077/84077 - 12s - loss: 4.0431e-04 - val_loss: 3.5227e-04 - 12s/epoch - 144us/sample
Epoch 13/80
84077/84077 - 12s - loss: 3.9257e-04 - val_loss: 3.3972e-04 - 12s/epoch - 144us/sample
Epoch 14/80
84077/84077 - 12s - loss: 3.8188e-04 - val_loss: 3.3288e-04 - 12s/epoch - 144us/sample
Epoch 15/80
84077/84077 - 12s - loss: 3.6952e-04 - val_loss: 3.2229e-04 - 12s/epoch - 146us/sample
Epoch 16/80
84077/84077 - 12s - loss: 3.6237e-04 - val_loss: 3.1747e-04 - 12s/epoch - 145us/sample
Epoch 17/80
84077/84077 - 12s - loss: 3.5191e-04 - val_loss: 3.0807e-04 - 12s/epoch - 144us/sample
Epoch 18/80
84077/84077 - 12s - loss: 3.5673e-04 - val_loss: 3.0176e-04 - 12s/epoch - 145us/sample
Epoch 19/80
84077/84077 - 12s - loss: 3.3889e-04 - val_loss: 2.9300e-04 - 12s/epoch - 145us/sample
Epoch 20/80
84077/84077 - 12s - loss: 3.2976e-04 - val_loss: 2.9524e-04 - 12s/epoch - 145us/sample
Epoch 21/80
84077/84077 - 12s - loss: 3.2520e-04 - val_loss: 2.8916e-04 - 12s/epoch - 145us/sample
Epoch 22/80
84077/84077 - 12s - loss: 3.2000e-04 - val_loss: 2.8442e-04 - 12s/epoch - 144us/sample
Epoch 23/80
84077/84077 - 12s - loss: 3.1343e-04 - val_loss: 2.7766e-04 - 12s/epoch - 145us/sample
Epoch 24/80
84077/84077 - 12s - loss: 3.1134e-04 - val_loss: 2.7779e-04 - 12s/epoch - 145us/sample
Epoch 25/80
84077/84077 - 12s - loss: 3.0413e-04 - val_loss: 2.7319e-04 - 12s/epoch - 146us/sample
Epoch 26/80
84077/84077 - 12s - loss: 3.0228e-04 - val_loss: 2.7392e-04 - 12s/epoch - 146us/sample
Epoch 27/80
84077/84077 - 12s - loss: 3.0087e-04 - val_loss: 2.6344e-04 - 12s/epoch - 145us/sample
Epoch 28/80
84077/84077 - 12s - loss: 2.9761e-04 - val_loss: 2.6686e-04 - 12s/epoch - 145us/sample
Epoch 29/80
84077/84077 - 12s - loss: 2.9189e-04 - val_loss: 2.5958e-04 - 12s/epoch - 146us/sample
Epoch 30/80
84077/84077 - 12s - loss: 2.9116e-04 - val_loss: 2.5847e-04 - 12s/epoch - 145us/sample
Epoch 31/80
84077/84077 - 12s - loss: 2.8736e-04 - val_loss: 2.6423e-04 - 12s/epoch - 145us/sample
Epoch 32/80
84077/84077 - 12s - loss: 2.9380e-04 - val_loss: 2.5893e-04 - 12s/epoch - 144us/sample
Epoch 33/80
84077/84077 - 12s - loss: 2.8372e-04 - val_loss: 2.5193e-04 - 12s/epoch - 144us/sample
Epoch 34/80
84077/84077 - 12s - loss: 2.7915e-04 - val_loss: 2.4759e-04 - 12s/epoch - 146us/sample
Epoch 35/80
84077/84077 - 12s - loss: 2.7721e-04 - val_loss: 2.4981e-04 - 12s/epoch - 145us/sample
Epoch 36/80
84077/84077 - 12s - loss: 2.7474e-04 - val_loss: 2.4770e-04 - 12s/epoch - 145us/sample
Epoch 37/80
84077/84077 - 12s - loss: 2.9046e-04 - val_loss: 2.4667e-04 - 12s/epoch - 144us/sample
Epoch 38/80
84077/84077 - 12s - loss: 2.7298e-04 - val_loss: 2.4425e-04 - 12s/epoch - 144us/sample
Epoch 39/80
84077/84077 - 12s - loss: 2.7297e-04 - val_loss: 2.4518e-04 - 12s/epoch - 145us/sample
Epoch 40/80
84077/84077 - 12s - loss: 2.7495e-04 - val_loss: 2.4527e-04 - 12s/epoch - 145us/sample
Epoch 41/80
84077/84077 - 12s - loss: 2.6907e-04 - val_loss: 2.4261e-04 - 12s/epoch - 143us/sample
Epoch 42/80
84077/84077 - 12s - loss: 2.7379e-04 - val_loss: 2.4204e-04 - 12s/epoch - 143us/sample
Epoch 43/80
84077/84077 - 12s - loss: 2.6504e-04 - val_loss: 2.4253e-04 - 12s/epoch - 142us/sample
Epoch 44/80
84077/84077 - 12s - loss: 2.6461e-04 - val_loss: 2.3947e-04 - 12s/epoch - 145us/sample
Epoch 45/80
84077/84077 - 12s - loss: 2.6244e-04 - val_loss: 2.4194e-04 - 12s/epoch - 144us/sample
Epoch 46/80
84077/84077 - 12s - loss: 2.6151e-04 - val_loss: 2.3585e-04 - 12s/epoch - 145us/sample
Epoch 47/80
84077/84077 - 12s - loss: 2.6890e-04 - val_loss: 2.3480e-04 - 12s/epoch - 145us/sample
Epoch 48/80
84077/84077 - 12s - loss: 2.5740e-04 - val_loss: 2.3449e-04 - 12s/epoch - 144us/sample
Epoch 49/80
84077/84077 - 12s - loss: 2.5787e-04 - val_loss: 2.3680e-04 - 12s/epoch - 144us/sample
Epoch 50/80
84077/84077 - 12s - loss: 2.5704e-04 - val_loss: 2.3232e-04 - 12s/epoch - 143us/sample
Epoch 51/80
84077/84077 - 12s - loss: 2.6197e-04 - val_loss: 2.3257e-04 - 12s/epoch - 144us/sample
Epoch 52/80
84077/84077 - 12s - loss: 2.5696e-04 - val_loss: 2.4291e-04 - 12s/epoch - 144us/sample
Epoch 53/80
84077/84077 - 12s - loss: 2.5515e-04 - val_loss: 2.2745e-04 - 12s/epoch - 145us/sample
Epoch 54/80
84077/84077 - 12s - loss: 2.5965e-04 - val_loss: 2.3107e-04 - 12s/epoch - 144us/sample
Epoch 55/80
84077/84077 - 12s - loss: 2.5222e-04 - val_loss: 2.2566e-04 - 12s/epoch - 143us/sample
Epoch 56/80
84077/84077 - 12s - loss: 2.5864e-04 - val_loss: 2.3353e-04 - 12s/epoch - 145us/sample
Epoch 57/80
84077/84077 - 12s - loss: 2.5728e-04 - val_loss: 2.2844e-04 - 12s/epoch - 144us/sample
Epoch 58/80
84077/84077 - 12s - loss: 2.4957e-04 - val_loss: 2.2558e-04 - 12s/epoch - 145us/sample
Epoch 59/80
84077/84077 - 12s - loss: 2.4903e-04 - val_loss: 2.2515e-04 - 12s/epoch - 143us/sample
Epoch 60/80
84077/84077 - 12s - loss: 2.4938e-04 - val_loss: 2.2656e-04 - 12s/epoch - 143us/sample
Epoch 61/80
84077/84077 - 12s - loss: 2.5361e-04 - val_loss: 2.2652e-04 - 12s/epoch - 144us/sample
Epoch 62/80
84077/84077 - 12s - loss: 2.4641e-04 - val_loss: 2.2390e-04 - 12s/epoch - 144us/sample
Epoch 63/80
84077/84077 - 12s - loss: 2.4684e-04 - val_loss: 2.2723e-04 - 12s/epoch - 144us/sample
Epoch 64/80
84077/84077 - 12s - loss: 2.4531e-04 - val_loss: 2.2509e-04 - 12s/epoch - 144us/sample
Epoch 65/80
84077/84077 - 12s - loss: 2.4474e-04 - val_loss: 2.2296e-04 - 12s/epoch - 144us/sample
Epoch 66/80
84077/84077 - 12s - loss: 2.4482e-04 - val_loss: 2.2398e-04 - 12s/epoch - 143us/sample
Epoch 67/80
84077/84077 - 12s - loss: 2.4392e-04 - val_loss: 2.2490e-04 - 12s/epoch - 144us/sample
Epoch 68/80
84077/84077 - 12s - loss: 2.4305e-04 - val_loss: 2.2165e-04 - 12s/epoch - 145us/sample
Epoch 69/80
84077/84077 - 12s - loss: 2.4685e-04 - val_loss: 2.2077e-04 - 12s/epoch - 144us/sample
Epoch 70/80
84077/84077 - 12s - loss: 2.4192e-04 - val_loss: 2.1921e-04 - 12s/epoch - 144us/sample
Epoch 71/80
84077/84077 - 12s - loss: 2.4178e-04 - val_loss: 2.1754e-04 - 12s/epoch - 143us/sample
Epoch 72/80
84077/84077 - 12s - loss: 2.4471e-04 - val_loss: 2.2336e-04 - 12s/epoch - 144us/sample
Epoch 73/80
84077/84077 - 12s - loss: 2.3829e-04 - val_loss: 2.1721e-04 - 12s/epoch - 145us/sample
Epoch 74/80
84077/84077 - 12s - loss: 2.4242e-04 - val_loss: 2.2000e-04 - 12s/epoch - 142us/sample
Epoch 75/80
84077/84077 - 12s - loss: 2.4357e-04 - val_loss: 2.5664e-04 - 12s/epoch - 142us/sample
Epoch 76/80
84077/84077 - 12s - loss: 2.5349e-04 - val_loss: 2.2088e-04 - 12s/epoch - 141us/sample
Epoch 77/80
84077/84077 - 12s - loss: 2.4227e-04 - val_loss: 2.1968e-04 - 12s/epoch - 144us/sample
Epoch 78/80
84077/84077 - 12s - loss: 2.3907e-04 - val_loss: 2.1873e-04 - 12s/epoch - 146us/sample
Epoch 79/80
84077/84077 - 12s - loss: 2.3630e-04 - val_loss: 2.1947e-04 - 12s/epoch - 144us/sample
Epoch 80/80
84077/84077 - 12s - loss: 2.3967e-04 - val_loss: 2.1528e-04 - 12s/epoch - 143us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00021527518684729763
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 12:21:35.534363: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_27/outputlayer/BiasAdd' id:34675 op device:{requested: '', assigned: ''} def:{{{node decoder_model_27/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_27/outputlayer/MatMul, decoder_model_27/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.028974961758851694
cosine 0.028633885348135463
MAE: 0.002670439353340784
RMSE: 0.010334162522659049
r2: 0.9166580467308479
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 80, 0.001, 0.2, 188, 0.00023967414023011815, 0.00021527518684729763, 0.028974961758851694, 0.028633885348135463, 0.002670439353340784, 0.010334162522659049, 0.9166580467308479, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0008 32 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_84 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_84 (ReLU)                (None, 1980)         0           ['batch_normalization_84[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 12:21:49.237948: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_28/kernel/Assign' id:35526 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_28/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_28/kernel, dense_enc0_28/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 12:22:04.130798: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_28/mul' id:35978 op device:{requested: '', assigned: ''} def:{{{node loss_28/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_28/mul/x, loss_28/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0685 - val_loss: 0.0675 - 20s/epoch - 235us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.0669 - val_loss: 0.0674 - 13s/epoch - 150us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0669 - val_loss: 0.0674 - 13s/epoch - 149us/sample
Epoch 4/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0674 - 13s/epoch - 149us/sample
Epoch 5/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 6/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 7/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 8/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 9/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 10/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 11/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 12/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 13/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 14/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 15/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 16/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 17/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 18/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 19/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 20/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 21/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 22/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 23/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 24/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 25/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 26/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 27/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 28/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 29/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 30/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 31/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 32/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 33/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 34/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 35/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 36/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 37/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 152us/sample
Epoch 38/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 39/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 40/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 41/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 42/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 152us/sample
Epoch 43/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 44/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 45/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 46/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 47/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 48/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 49/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 50/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 51/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 52/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 53/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 54/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 55/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 56/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 57/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 58/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 59/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 60/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 61/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 62/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 63/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 64/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 65/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 66/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 67/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 68/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 69/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 70/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 71/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 72/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 73/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 74/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 75/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 76/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 77/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 78/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 79/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 80/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 81/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 149us/sample
Epoch 82/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 151us/sample
Epoch 83/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 84/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
Epoch 85/85
84077/84077 - 13s - loss: 0.0668 - val_loss: 0.0673 - 13s/epoch - 150us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573504136043
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 12:39:46.585767: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_28/outputlayer/BiasAdd' id:35930 op device:{requested: '', assigned: ''} def:{{{node decoder_model_28/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_28/outputlayer/MatMul, decoder_model_28/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1936650208521797
cosine 1.1530326993256024
MAE: 5.593186414463599
RMSE: 5.809174701695642
r2: -24733.367228591153
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'binary_crossentropy', 32, 85, 0.0008, 0.2, 188, 0.06683691492447787, 0.06734573504136043, 1.1936650208521797, 1.1530326993256024, 5.593186414463599, 5.809174701695642, -24733.367228591153, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_87 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_87 (ReLU)                (None, 1791)         0           ['batch_normalization_87[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 12:40:00.600947: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_29/kernel/Assign' id:37175 op device:{requested: '', assigned: ''} def:{{{node outputlayer_29/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_29/kernel, outputlayer_29/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 12:40:15.404848: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_29/mul' id:37296 op device:{requested: '', assigned: ''} def:{{{node loss_29/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_29/mul/x, loss_29/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0073 - val_loss: 0.0017 - 20s/epoch - 234us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0017 - val_loss: 0.1744 - 12s/epoch - 147us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0015 - val_loss: 0.0011 - 12s/epoch - 147us/sample
Epoch 4/85
84077/84077 - 12s - loss: 9.7532e-04 - val_loss: 8.5532e-04 - 12s/epoch - 146us/sample
Epoch 5/85
84077/84077 - 12s - loss: 8.4681e-04 - val_loss: 6.4964e-04 - 12s/epoch - 145us/sample
Epoch 6/85
84077/84077 - 12s - loss: 6.4130e-04 - val_loss: 5.4301e-04 - 12s/epoch - 146us/sample
Epoch 7/85
84077/84077 - 12s - loss: 5.5273e-04 - val_loss: 4.6027e-04 - 12s/epoch - 147us/sample
Epoch 8/85
84077/84077 - 12s - loss: 4.7882e-04 - val_loss: 4.1005e-04 - 12s/epoch - 148us/sample
Epoch 9/85
84077/84077 - 12s - loss: 4.3054e-04 - val_loss: 3.8101e-04 - 12s/epoch - 147us/sample
Epoch 10/85
84077/84077 - 12s - loss: 3.9900e-04 - val_loss: 3.5225e-04 - 12s/epoch - 147us/sample
Epoch 11/85
84077/84077 - 12s - loss: 3.7074e-04 - val_loss: 3.3751e-04 - 12s/epoch - 148us/sample
Epoch 12/85
84077/84077 - 13s - loss: 3.4909e-04 - val_loss: 3.2235e-04 - 13s/epoch - 149us/sample
Epoch 13/85
84077/84077 - 12s - loss: 3.3478e-04 - val_loss: 3.1794e-04 - 12s/epoch - 148us/sample
Epoch 14/85
84077/84077 - 12s - loss: 3.2308e-04 - val_loss: 2.9861e-04 - 12s/epoch - 147us/sample
Epoch 15/85
84077/84077 - 12s - loss: 3.1511e-04 - val_loss: 2.9879e-04 - 12s/epoch - 148us/sample
Epoch 16/85
84077/84077 - 12s - loss: 3.0342e-04 - val_loss: 2.8454e-04 - 12s/epoch - 148us/sample
Epoch 17/85
84077/84077 - 12s - loss: 3.0559e-04 - val_loss: 2.8440e-04 - 12s/epoch - 148us/sample
Epoch 18/85
84077/84077 - 12s - loss: 2.9319e-04 - val_loss: 2.8306e-04 - 12s/epoch - 147us/sample
Epoch 19/85
84077/84077 - 12s - loss: 2.8660e-04 - val_loss: 2.7261e-04 - 12s/epoch - 148us/sample
Epoch 20/85
84077/84077 - 12s - loss: 2.8359e-04 - val_loss: 2.7457e-04 - 12s/epoch - 147us/sample
Epoch 21/85
84077/84077 - 12s - loss: 2.7813e-04 - val_loss: 2.6888e-04 - 12s/epoch - 147us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.7260e-04 - val_loss: 2.6313e-04 - 12s/epoch - 148us/sample
Epoch 23/85
84077/84077 - 12s - loss: 2.6993e-04 - val_loss: 2.5743e-04 - 12s/epoch - 147us/sample
Epoch 24/85
84077/84077 - 12s - loss: 2.6818e-04 - val_loss: 2.5973e-04 - 12s/epoch - 148us/sample
Epoch 25/85
84077/84077 - 12s - loss: 2.6739e-04 - val_loss: 2.5326e-04 - 12s/epoch - 147us/sample
Epoch 26/85
84077/84077 - 12s - loss: 2.5950e-04 - val_loss: 2.5241e-04 - 12s/epoch - 148us/sample
Epoch 27/85
84077/84077 - 12s - loss: 2.6202e-04 - val_loss: 2.5311e-04 - 12s/epoch - 148us/sample
Epoch 28/85
84077/84077 - 13s - loss: 2.6511e-04 - val_loss: 2.5031e-04 - 13s/epoch - 149us/sample
Epoch 29/85
84077/84077 - 12s - loss: 2.5492e-04 - val_loss: 2.4599e-04 - 12s/epoch - 148us/sample
Epoch 30/85
84077/84077 - 12s - loss: 2.5076e-04 - val_loss: 2.3976e-04 - 12s/epoch - 147us/sample
Epoch 31/85
84077/84077 - 12s - loss: 2.4943e-04 - val_loss: 2.3715e-04 - 12s/epoch - 147us/sample
Epoch 32/85
84077/84077 - 13s - loss: 2.5113e-04 - val_loss: 2.3761e-04 - 13s/epoch - 149us/sample
Epoch 33/85
84077/84077 - 12s - loss: 2.4656e-04 - val_loss: 2.3611e-04 - 12s/epoch - 148us/sample
Epoch 34/85
84077/84077 - 12s - loss: 2.4337e-04 - val_loss: 2.3287e-04 - 12s/epoch - 147us/sample
Epoch 35/85
84077/84077 - 12s - loss: 2.4420e-04 - val_loss: 2.3342e-04 - 12s/epoch - 147us/sample
Epoch 36/85
84077/84077 - 12s - loss: 2.4305e-04 - val_loss: 2.3415e-04 - 12s/epoch - 147us/sample
Epoch 37/85
84077/84077 - 13s - loss: 2.4083e-04 - val_loss: 2.3219e-04 - 13s/epoch - 149us/sample
Epoch 38/85
84077/84077 - 12s - loss: 2.4046e-04 - val_loss: 2.3089e-04 - 12s/epoch - 148us/sample
Epoch 39/85
84077/84077 - 12s - loss: 2.3978e-04 - val_loss: 2.2923e-04 - 12s/epoch - 148us/sample
Epoch 40/85
84077/84077 - 12s - loss: 2.3699e-04 - val_loss: 2.2668e-04 - 12s/epoch - 148us/sample
Epoch 41/85
84077/84077 - 12s - loss: 2.3956e-04 - val_loss: 2.3297e-04 - 12s/epoch - 147us/sample
Epoch 42/85
84077/84077 - 12s - loss: 2.3603e-04 - val_loss: 2.2466e-04 - 12s/epoch - 147us/sample
Epoch 43/85
84077/84077 - 12s - loss: 2.3309e-04 - val_loss: 2.2160e-04 - 12s/epoch - 149us/sample
Epoch 44/85
84077/84077 - 12s - loss: 2.3339e-04 - val_loss: 2.2615e-04 - 12s/epoch - 148us/sample
Epoch 45/85
84077/84077 - 12s - loss: 2.3370e-04 - val_loss: 2.2175e-04 - 12s/epoch - 148us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.3497e-04 - val_loss: 2.2283e-04 - 12s/epoch - 147us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.3060e-04 - val_loss: 2.1955e-04 - 12s/epoch - 148us/sample
Epoch 48/85
84077/84077 - 13s - loss: 2.3174e-04 - val_loss: 2.1983e-04 - 13s/epoch - 149us/sample
Epoch 49/85
84077/84077 - 13s - loss: 2.2854e-04 - val_loss: 2.1470e-04 - 13s/epoch - 149us/sample
Epoch 50/85
84077/84077 - 12s - loss: 2.3159e-04 - val_loss: 2.1705e-04 - 12s/epoch - 148us/sample
Epoch 51/85
84077/84077 - 12s - loss: 2.2823e-04 - val_loss: 2.1470e-04 - 12s/epoch - 148us/sample
Epoch 52/85
84077/84077 - 12s - loss: 2.2595e-04 - val_loss: 2.1515e-04 - 12s/epoch - 147us/sample
Epoch 53/85
84077/84077 - 13s - loss: 2.2575e-04 - val_loss: 2.1591e-04 - 13s/epoch - 149us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.2525e-04 - val_loss: 2.1582e-04 - 12s/epoch - 148us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.2489e-04 - val_loss: 2.1537e-04 - 12s/epoch - 148us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.2409e-04 - val_loss: 2.1690e-04 - 12s/epoch - 147us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.2684e-04 - val_loss: 2.1438e-04 - 12s/epoch - 147us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.2538e-04 - val_loss: 2.1501e-04 - 12s/epoch - 148us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.2950e-04 - val_loss: 2.1589e-04 - 12s/epoch - 147us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.2148e-04 - val_loss: 2.1446e-04 - 12s/epoch - 147us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.2101e-04 - val_loss: 2.1069e-04 - 12s/epoch - 147us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.2314e-04 - val_loss: 2.1312e-04 - 12s/epoch - 145us/sample
Epoch 63/85
84077/84077 - 12s - loss: 2.1847e-04 - val_loss: 2.1005e-04 - 12s/epoch - 145us/sample
Epoch 64/85
84077/84077 - 12s - loss: 2.2028e-04 - val_loss: 2.1059e-04 - 12s/epoch - 145us/sample
Epoch 65/85
84077/84077 - 12s - loss: 2.1870e-04 - val_loss: 2.1359e-04 - 12s/epoch - 147us/sample
Epoch 66/85
84077/84077 - 12s - loss: 2.2437e-04 - val_loss: 2.0978e-04 - 12s/epoch - 148us/sample
Epoch 67/85
84077/84077 - 12s - loss: 2.1796e-04 - val_loss: 2.0945e-04 - 12s/epoch - 148us/sample
Epoch 68/85
84077/84077 - 12s - loss: 2.1675e-04 - val_loss: 2.1009e-04 - 12s/epoch - 148us/sample
Epoch 69/85
84077/84077 - 12s - loss: 2.1876e-04 - val_loss: 2.0763e-04 - 12s/epoch - 148us/sample
Epoch 70/85
84077/84077 - 12s - loss: 2.1582e-04 - val_loss: 2.0659e-04 - 12s/epoch - 147us/sample
Epoch 71/85
84077/84077 - 12s - loss: 2.1673e-04 - val_loss: 2.0301e-04 - 12s/epoch - 148us/sample
Epoch 72/85
84077/84077 - 12s - loss: 2.1584e-04 - val_loss: 2.0202e-04 - 12s/epoch - 147us/sample
Epoch 73/85
84077/84077 - 12s - loss: 2.1628e-04 - val_loss: 2.0374e-04 - 12s/epoch - 148us/sample
Epoch 74/85
84077/84077 - 12s - loss: 2.1723e-04 - val_loss: 2.0167e-04 - 12s/epoch - 148us/sample
Epoch 75/85
84077/84077 - 12s - loss: 2.1489e-04 - val_loss: 2.0112e-04 - 12s/epoch - 147us/sample
Epoch 76/85
84077/84077 - 12s - loss: 2.1539e-04 - val_loss: 2.0050e-04 - 12s/epoch - 147us/sample
Epoch 77/85
84077/84077 - 12s - loss: 2.1562e-04 - val_loss: 2.0591e-04 - 12s/epoch - 148us/sample
Epoch 78/85
84077/84077 - 12s - loss: 2.1468e-04 - val_loss: 1.9971e-04 - 12s/epoch - 148us/sample
Epoch 79/85
84077/84077 - 12s - loss: 2.1495e-04 - val_loss: 2.0389e-04 - 12s/epoch - 148us/sample
Epoch 80/85
84077/84077 - 12s - loss: 2.1264e-04 - val_loss: 2.0537e-04 - 12s/epoch - 147us/sample
Epoch 81/85
84077/84077 - 12s - loss: 2.1208e-04 - val_loss: 1.9957e-04 - 12s/epoch - 147us/sample
Epoch 82/85
84077/84077 - 12s - loss: 2.1302e-04 - val_loss: 2.0025e-04 - 12s/epoch - 147us/sample
Epoch 83/85
84077/84077 - 12s - loss: 2.1177e-04 - val_loss: 2.0474e-04 - 12s/epoch - 147us/sample
Epoch 84/85
84077/84077 - 12s - loss: 2.1149e-04 - val_loss: 2.0019e-04 - 12s/epoch - 148us/sample
Epoch 85/85
84077/84077 - 13s - loss: 2.1148e-04 - val_loss: 2.0242e-04 - 13s/epoch - 149us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002024181466751511
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 12:57:40.394018: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_29/outputlayer/BiasAdd' id:37267 op device:{requested: '', assigned: ''} def:{{{node decoder_model_29/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_29/outputlayer/MatMul, decoder_model_29/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.026042726458698175
cosine 0.025735085553812136
MAE: 0.0020426778628237657
RMSE: 0.009878389907935332
r2: 0.9239057576480796
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 32, 85, 0.001, 0.2, 188, 0.000211475265709484, 0.0002024181466751511, 0.026042726458698175, 0.025735085553812136, 0.0020426778628237657, 0.009878389907935332, 0.9239057576480796, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 3
Fitness    = 512.1418577388646
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 512.1418577388646.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646]
[2.1 85 0.0012000000000000001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_90 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_90 (ReLU)                (None, 1980)         0           ['batch_normalization_90[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 12:57:54.565418: W tensorflow/c/c_api.cc:291] Operation '{name:'training_60/Adam/batch_normalization_92/beta/m/Assign' id:39099 op device:{requested: '', assigned: ''} def:{{{node training_60/Adam/batch_normalization_92/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_60/Adam/batch_normalization_92/beta/m, training_60/Adam/batch_normalization_92/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 12:58:05.087601: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_30/mul' id:38551 op device:{requested: '', assigned: ''} def:{{{node loss_30/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_30/mul/x, loss_30/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0080 - val_loss: 0.0019 - 15s/epoch - 181us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0017 - val_loss: 0.0025 - 8s/epoch - 95us/sample
Epoch 3/85
84077/84077 - 8s - loss: 1.0237 - val_loss: 0.0018 - 8s/epoch - 95us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0013 - val_loss: 0.0011 - 8s/epoch - 95us/sample
Epoch 5/85
84077/84077 - 8s - loss: 0.6653 - val_loss: 0.0025 - 8s/epoch - 96us/sample
Epoch 6/85
84077/84077 - 8s - loss: 0.0020 - val_loss: 0.0012 - 8s/epoch - 95us/sample
Epoch 7/85
84077/84077 - 8s - loss: 0.0046 - val_loss: 0.0028 - 8s/epoch - 95us/sample
Epoch 8/85
84077/84077 - 8s - loss: 0.0023 - val_loss: 0.0014 - 8s/epoch - 95us/sample
Epoch 9/85
84077/84077 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 94us/sample
Epoch 10/85
84077/84077 - 8s - loss: 0.0011 - val_loss: 9.3179e-04 - 8s/epoch - 95us/sample
Epoch 11/85
84077/84077 - 8s - loss: 9.1178e-04 - val_loss: 8.1263e-04 - 8s/epoch - 95us/sample
Epoch 12/85
84077/84077 - 8s - loss: 0.0011 - val_loss: 7.7037e-04 - 8s/epoch - 95us/sample
Epoch 13/85
84077/84077 - 8s - loss: 9.5924e-04 - val_loss: 7.9199e-04 - 8s/epoch - 95us/sample
Epoch 14/85
84077/84077 - 8s - loss: 0.0138 - val_loss: 0.0027 - 8s/epoch - 95us/sample
Epoch 15/85
84077/84077 - 8s - loss: 0.0016 - val_loss: 8.6993e-04 - 8s/epoch - 95us/sample
Epoch 16/85
84077/84077 - 8s - loss: 0.0017 - val_loss: 8.6180e-04 - 8s/epoch - 95us/sample
Epoch 17/85
84077/84077 - 8s - loss: 8.9732e-04 - val_loss: 7.2821e-04 - 8s/epoch - 97us/sample
Epoch 18/85
84077/84077 - 8s - loss: 7.6930e-04 - val_loss: 6.7377e-04 - 8s/epoch - 95us/sample
Epoch 19/85
84077/84077 - 8s - loss: 0.0021 - val_loss: 7.9489e-04 - 8s/epoch - 95us/sample
Epoch 20/85
84077/84077 - 8s - loss: 0.0020 - val_loss: 0.0012 - 8s/epoch - 95us/sample
Epoch 21/85
84077/84077 - 8s - loss: 9.0864e-04 - val_loss: 7.0787e-04 - 8s/epoch - 94us/sample
Epoch 22/85
84077/84077 - 8s - loss: 7.3367e-04 - val_loss: 6.4977e-04 - 8s/epoch - 94us/sample
Epoch 23/85
84077/84077 - 8s - loss: 6.8922e-04 - val_loss: 6.2483e-04 - 8s/epoch - 95us/sample
Epoch 24/85
84077/84077 - 8s - loss: 6.8478e-04 - val_loss: 6.2691e-04 - 8s/epoch - 96us/sample
Epoch 25/85
84077/84077 - 8s - loss: 6.5326e-04 - val_loss: 6.0045e-04 - 8s/epoch - 95us/sample
Epoch 26/85
84077/84077 - 8s - loss: 6.3552e-04 - val_loss: 5.8779e-04 - 8s/epoch - 95us/sample
Epoch 27/85
84077/84077 - 8s - loss: 6.1927e-04 - val_loss: 5.7514e-04 - 8s/epoch - 94us/sample
Epoch 28/85
84077/84077 - 8s - loss: 6.3176e-04 - val_loss: 5.6222e-04 - 8s/epoch - 95us/sample
Epoch 29/85
84077/84077 - 8s - loss: 5.9643e-04 - val_loss: 5.5107e-04 - 8s/epoch - 95us/sample
Epoch 30/85
84077/84077 - 8s - loss: 5.8159e-04 - val_loss: 5.4185e-04 - 8s/epoch - 95us/sample
Epoch 31/85
84077/84077 - 8s - loss: 5.9034e-04 - val_loss: 5.3527e-04 - 8s/epoch - 95us/sample
Epoch 32/85
84077/84077 - 8s - loss: 5.5790e-04 - val_loss: 5.1627e-04 - 8s/epoch - 96us/sample
Epoch 33/85
84077/84077 - 8s - loss: 5.5179e-04 - val_loss: 5.0314e-04 - 8s/epoch - 95us/sample
Epoch 34/85
84077/84077 - 8s - loss: 5.3531e-04 - val_loss: 4.9733e-04 - 8s/epoch - 95us/sample
Epoch 35/85
84077/84077 - 8s - loss: 5.2342e-04 - val_loss: 4.8665e-04 - 8s/epoch - 94us/sample
Epoch 36/85
84077/84077 - 8s - loss: 5.1281e-04 - val_loss: 4.7834e-04 - 8s/epoch - 95us/sample
Epoch 37/85
84077/84077 - 8s - loss: 5.0682e-04 - val_loss: 4.6853e-04 - 8s/epoch - 94us/sample
Epoch 38/85
84077/84077 - 8s - loss: 5.0298e-04 - val_loss: 4.7276e-04 - 8s/epoch - 94us/sample
Epoch 39/85
84077/84077 - 8s - loss: 4.9421e-04 - val_loss: 4.6528e-04 - 8s/epoch - 95us/sample
Epoch 40/85
84077/84077 - 8s - loss: 4.8725e-04 - val_loss: 4.5638e-04 - 8s/epoch - 95us/sample
Epoch 41/85
84077/84077 - 8s - loss: 4.8006e-04 - val_loss: 4.5215e-04 - 8s/epoch - 95us/sample
Epoch 42/85
84077/84077 - 8s - loss: 4.7495e-04 - val_loss: 4.4449e-04 - 8s/epoch - 95us/sample
Epoch 43/85
84077/84077 - 8s - loss: 4.6905e-04 - val_loss: 4.3778e-04 - 8s/epoch - 95us/sample
Epoch 44/85
84077/84077 - 8s - loss: 4.6079e-04 - val_loss: 4.3445e-04 - 8s/epoch - 96us/sample
Epoch 45/85
84077/84077 - 8s - loss: 4.5513e-04 - val_loss: 4.2779e-04 - 8s/epoch - 95us/sample
Epoch 46/85
84077/84077 - 8s - loss: 4.5049e-04 - val_loss: 4.2733e-04 - 8s/epoch - 95us/sample
Epoch 47/85
84077/84077 - 8s - loss: 4.4457e-04 - val_loss: 4.2023e-04 - 8s/epoch - 96us/sample
Epoch 48/85
84077/84077 - 8s - loss: 4.4232e-04 - val_loss: 4.2082e-04 - 8s/epoch - 95us/sample
Epoch 49/85
84077/84077 - 8s - loss: 4.3854e-04 - val_loss: 4.1529e-04 - 8s/epoch - 95us/sample
Epoch 50/85
84077/84077 - 8s - loss: 4.3704e-04 - val_loss: 4.1116e-04 - 8s/epoch - 95us/sample
Epoch 51/85
84077/84077 - 8s - loss: 4.3305e-04 - val_loss: 4.1107e-04 - 8s/epoch - 95us/sample
Epoch 52/85
84077/84077 - 8s - loss: 4.3025e-04 - val_loss: 4.0657e-04 - 8s/epoch - 95us/sample
Epoch 53/85
84077/84077 - 8s - loss: 4.2663e-04 - val_loss: 4.0422e-04 - 8s/epoch - 95us/sample
Epoch 54/85
84077/84077 - 8s - loss: 4.2463e-04 - val_loss: 4.0234e-04 - 8s/epoch - 95us/sample
Epoch 55/85
84077/84077 - 8s - loss: 4.2259e-04 - val_loss: 4.0300e-04 - 8s/epoch - 96us/sample
Epoch 56/85
84077/84077 - 8s - loss: 4.2070e-04 - val_loss: 4.0051e-04 - 8s/epoch - 96us/sample
Epoch 57/85
84077/84077 - 8s - loss: 4.1943e-04 - val_loss: 3.9965e-04 - 8s/epoch - 94us/sample
Epoch 58/85
84077/84077 - 8s - loss: 4.1815e-04 - val_loss: 3.9711e-04 - 8s/epoch - 94us/sample
Epoch 59/85
84077/84077 - 8s - loss: 4.1566e-04 - val_loss: 3.9550e-04 - 8s/epoch - 94us/sample
Epoch 60/85
84077/84077 - 8s - loss: 4.1398e-04 - val_loss: 3.9423e-04 - 8s/epoch - 94us/sample
Epoch 61/85
84077/84077 - 8s - loss: 4.1332e-04 - val_loss: 3.9463e-04 - 8s/epoch - 94us/sample
Epoch 62/85
84077/84077 - 8s - loss: 4.1141e-04 - val_loss: 3.9327e-04 - 8s/epoch - 95us/sample
Epoch 63/85
84077/84077 - 8s - loss: 4.1076e-04 - val_loss: 3.9226e-04 - 8s/epoch - 95us/sample
Epoch 64/85
84077/84077 - 8s - loss: 4.1066e-04 - val_loss: 3.9209e-04 - 8s/epoch - 95us/sample
Epoch 65/85
84077/84077 - 8s - loss: 4.0913e-04 - val_loss: 3.9167e-04 - 8s/epoch - 94us/sample
Epoch 66/85
84077/84077 - 8s - loss: 4.0808e-04 - val_loss: 3.9086e-04 - 8s/epoch - 93us/sample
Epoch 67/85
84077/84077 - 8s - loss: 4.0711e-04 - val_loss: 3.9010e-04 - 8s/epoch - 93us/sample
Epoch 68/85
84077/84077 - 8s - loss: 4.0597e-04 - val_loss: 3.8742e-04 - 8s/epoch - 93us/sample
Epoch 69/85
84077/84077 - 8s - loss: 4.0602e-04 - val_loss: 3.8727e-04 - 8s/epoch - 94us/sample
Epoch 70/85
84077/84077 - 8s - loss: 4.0398e-04 - val_loss: 3.8747e-04 - 8s/epoch - 93us/sample
Epoch 71/85
84077/84077 - 8s - loss: 4.0354e-04 - val_loss: 3.8565e-04 - 8s/epoch - 95us/sample
Epoch 72/85
84077/84077 - 8s - loss: 4.0237e-04 - val_loss: 3.8725e-04 - 8s/epoch - 94us/sample
Epoch 73/85
84077/84077 - 8s - loss: 4.0199e-04 - val_loss: 3.8522e-04 - 8s/epoch - 94us/sample
Epoch 74/85
84077/84077 - 8s - loss: 4.0084e-04 - val_loss: 3.8194e-04 - 8s/epoch - 94us/sample
Epoch 75/85
84077/84077 - 8s - loss: 3.9824e-04 - val_loss: 3.8058e-04 - 8s/epoch - 94us/sample
Epoch 76/85
84077/84077 - 8s - loss: 3.9703e-04 - val_loss: 3.8161e-04 - 8s/epoch - 94us/sample
Epoch 77/85
84077/84077 - 8s - loss: 3.9539e-04 - val_loss: 3.7769e-04 - 8s/epoch - 96us/sample
Epoch 78/85
84077/84077 - 8s - loss: 3.9463e-04 - val_loss: 3.7779e-04 - 8s/epoch - 94us/sample
Epoch 79/85
84077/84077 - 8s - loss: 3.9320e-04 - val_loss: 3.7587e-04 - 8s/epoch - 94us/sample
Epoch 80/85
84077/84077 - 8s - loss: 3.9199e-04 - val_loss: 3.7762e-04 - 8s/epoch - 94us/sample
Epoch 81/85
84077/84077 - 8s - loss: 3.9184e-04 - val_loss: 3.7527e-04 - 8s/epoch - 94us/sample
Epoch 82/85
84077/84077 - 8s - loss: 3.9082e-04 - val_loss: 3.7506e-04 - 8s/epoch - 94us/sample
Epoch 83/85
84077/84077 - 8s - loss: 3.8979e-04 - val_loss: 3.7243e-04 - 8s/epoch - 94us/sample
Epoch 84/85
84077/84077 - 8s - loss: 3.8794e-04 - val_loss: 3.7060e-04 - 8s/epoch - 94us/sample
Epoch 85/85
84077/84077 - 8s - loss: 3.8801e-04 - val_loss: 3.7172e-04 - 8s/epoch - 95us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00037171840523669684
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 13:09:16.737070: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_30/outputlayer/BiasAdd' id:38522 op device:{requested: '', assigned: ''} def:{{{node decoder_model_30/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_30/outputlayer/MatMul, decoder_model_30/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0800581898000977
cosine 0.07906684675660443
MAE: 0.0032278905367634433
RMSE: 0.016954852875072382
r2: 0.7754441435446678
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.2, 188, 0.00038801393627478664, 0.00037171840523669684, 0.0800581898000977, 0.07906684675660443, 0.0032278905367634433, 0.016954852875072382, 0.7754441435446678, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 90 0.0012000000000000001 32 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_93 (BatchN  (None, 1603)        6412        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_93 (ReLU)                (None, 1603)         0           ['batch_normalization_93[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 13:09:31.098565: W tensorflow/c/c_api.cc:291] Operation '{name:'training_62/Adam/dense_dec1_31/bias/m/Assign' id:40319 op device:{requested: '', assigned: ''} def:{{{node training_62/Adam/dense_dec1_31/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_62/Adam/dense_dec1_31/bias/m, training_62/Adam/dense_dec1_31/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 13:09:45.920908: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_31/mul' id:39806 op device:{requested: '', assigned: ''} def:{{{node loss_31/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_31/mul/x, loss_31/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0093 - val_loss: 0.0014 - 20s/epoch - 237us/sample
Epoch 2/90
84077/84077 - 12s - loss: 0.0016 - val_loss: 0.0011 - 12s/epoch - 147us/sample
Epoch 3/90
84077/84077 - 12s - loss: 0.0012 - val_loss: 9.3168e-04 - 12s/epoch - 145us/sample
Epoch 4/90
84077/84077 - 12s - loss: 7.9104e-04 - val_loss: 6.3347e-04 - 12s/epoch - 145us/sample
Epoch 5/90
84077/84077 - 12s - loss: 8.0257e-04 - val_loss: 7.1414e-04 - 12s/epoch - 145us/sample
Epoch 6/90
84077/84077 - 12s - loss: 7.7074e-04 - val_loss: 5.5624e-04 - 12s/epoch - 147us/sample
Epoch 7/90
84077/84077 - 12s - loss: 5.3079e-04 - val_loss: 4.4817e-04 - 12s/epoch - 147us/sample
Epoch 8/90
84077/84077 - 12s - loss: 4.7365e-04 - val_loss: 4.1438e-04 - 12s/epoch - 146us/sample
Epoch 9/90
84077/84077 - 12s - loss: 4.3287e-04 - val_loss: 3.6927e-04 - 12s/epoch - 147us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.9961e-04 - val_loss: 3.5624e-04 - 12s/epoch - 147us/sample
Epoch 11/90
84077/84077 - 12s - loss: 3.7382e-04 - val_loss: 3.3031e-04 - 12s/epoch - 148us/sample
Epoch 12/90
84077/84077 - 12s - loss: 3.5702e-04 - val_loss: 3.4207e-04 - 12s/epoch - 147us/sample
Epoch 13/90
84077/84077 - 12s - loss: 3.4279e-04 - val_loss: 3.1425e-04 - 12s/epoch - 147us/sample
Epoch 14/90
84077/84077 - 12s - loss: 3.3100e-04 - val_loss: 3.1154e-04 - 12s/epoch - 146us/sample
Epoch 15/90
84077/84077 - 12s - loss: 3.2110e-04 - val_loss: 2.9888e-04 - 12s/epoch - 146us/sample
Epoch 16/90
84077/84077 - 12s - loss: 3.1381e-04 - val_loss: 2.8966e-04 - 12s/epoch - 147us/sample
Epoch 17/90
84077/84077 - 12s - loss: 3.1095e-04 - val_loss: 2.9294e-04 - 12s/epoch - 148us/sample
Epoch 18/90
84077/84077 - 12s - loss: 3.0186e-04 - val_loss: 2.7909e-04 - 12s/epoch - 147us/sample
Epoch 19/90
84077/84077 - 12s - loss: 2.9705e-04 - val_loss: 2.7816e-04 - 12s/epoch - 145us/sample
Epoch 20/90
84077/84077 - 12s - loss: 2.9205e-04 - val_loss: 2.7703e-04 - 12s/epoch - 144us/sample
Epoch 21/90
84077/84077 - 12s - loss: 2.8698e-04 - val_loss: 2.7358e-04 - 12s/epoch - 144us/sample
Epoch 22/90
84077/84077 - 12s - loss: 2.8767e-04 - val_loss: 2.7468e-04 - 12s/epoch - 145us/sample
Epoch 23/90
84077/84077 - 12s - loss: 2.8378e-04 - val_loss: 2.7334e-04 - 12s/epoch - 148us/sample
Epoch 24/90
84077/84077 - 12s - loss: 2.7818e-04 - val_loss: 2.5976e-04 - 12s/epoch - 147us/sample
Epoch 25/90
84077/84077 - 12s - loss: 2.7503e-04 - val_loss: 2.6167e-04 - 12s/epoch - 147us/sample
Epoch 26/90
84077/84077 - 12s - loss: 2.7094e-04 - val_loss: 2.5592e-04 - 12s/epoch - 146us/sample
Epoch 27/90
84077/84077 - 12s - loss: 2.7183e-04 - val_loss: 2.6468e-04 - 12s/epoch - 147us/sample
Epoch 28/90
84077/84077 - 12s - loss: 2.6877e-04 - val_loss: 2.5434e-04 - 12s/epoch - 148us/sample
Epoch 29/90
84077/84077 - 12s - loss: 2.6571e-04 - val_loss: 2.4955e-04 - 12s/epoch - 147us/sample
Epoch 30/90
84077/84077 - 12s - loss: 2.6458e-04 - val_loss: 2.5622e-04 - 12s/epoch - 146us/sample
Epoch 31/90
84077/84077 - 12s - loss: 2.6186e-04 - val_loss: 2.5051e-04 - 12s/epoch - 146us/sample
Epoch 32/90
84077/84077 - 12s - loss: 2.5934e-04 - val_loss: 2.4453e-04 - 12s/epoch - 146us/sample
Epoch 33/90
84077/84077 - 12s - loss: 2.5757e-04 - val_loss: 2.4818e-04 - 12s/epoch - 148us/sample
Epoch 34/90
84077/84077 - 12s - loss: 2.5520e-04 - val_loss: 2.4061e-04 - 12s/epoch - 148us/sample
Epoch 35/90
84077/84077 - 12s - loss: 2.5484e-04 - val_loss: 2.4877e-04 - 12s/epoch - 148us/sample
Epoch 36/90
84077/84077 - 12s - loss: 2.5250e-04 - val_loss: 2.4058e-04 - 12s/epoch - 147us/sample
Epoch 37/90
84077/84077 - 12s - loss: 2.5496e-04 - val_loss: 2.5097e-04 - 12s/epoch - 147us/sample
Epoch 38/90
84077/84077 - 12s - loss: 2.5195e-04 - val_loss: 2.3996e-04 - 12s/epoch - 148us/sample
Epoch 39/90
84077/84077 - 12s - loss: 2.4830e-04 - val_loss: 2.3708e-04 - 12s/epoch - 147us/sample
Epoch 40/90
84077/84077 - 12s - loss: 2.4850e-04 - val_loss: 2.3820e-04 - 12s/epoch - 148us/sample
Epoch 41/90
84077/84077 - 12s - loss: 2.4812e-04 - val_loss: 2.3463e-04 - 12s/epoch - 148us/sample
Epoch 42/90
84077/84077 - 12s - loss: 2.4780e-04 - val_loss: 2.3092e-04 - 12s/epoch - 147us/sample
Epoch 43/90
84077/84077 - 12s - loss: 2.4385e-04 - val_loss: 2.3569e-04 - 12s/epoch - 147us/sample
Epoch 44/90
84077/84077 - 12s - loss: 2.4617e-04 - val_loss: 2.3369e-04 - 12s/epoch - 148us/sample
Epoch 45/90
84077/84077 - 12s - loss: 2.4344e-04 - val_loss: 2.2789e-04 - 12s/epoch - 147us/sample
Epoch 46/90
84077/84077 - 12s - loss: 2.4404e-04 - val_loss: 2.3337e-04 - 12s/epoch - 147us/sample
Epoch 47/90
84077/84077 - 12s - loss: 2.4101e-04 - val_loss: 2.3086e-04 - 12s/epoch - 146us/sample
Epoch 48/90
84077/84077 - 12s - loss: 2.4034e-04 - val_loss: 2.3024e-04 - 12s/epoch - 147us/sample
Epoch 49/90
84077/84077 - 12s - loss: 2.3938e-04 - val_loss: 2.3016e-04 - 12s/epoch - 147us/sample
Epoch 50/90
84077/84077 - 12s - loss: 2.3905e-04 - val_loss: 2.2614e-04 - 12s/epoch - 148us/sample
Epoch 51/90
84077/84077 - 12s - loss: 2.3725e-04 - val_loss: 2.2332e-04 - 12s/epoch - 146us/sample
Epoch 52/90
84077/84077 - 12s - loss: 2.4270e-04 - val_loss: 2.3146e-04 - 12s/epoch - 146us/sample
Epoch 53/90
84077/84077 - 12s - loss: 2.3609e-04 - val_loss: 2.2300e-04 - 12s/epoch - 146us/sample
Epoch 54/90
84077/84077 - 12s - loss: 2.3620e-04 - val_loss: 2.2900e-04 - 12s/epoch - 147us/sample
Epoch 55/90
84077/84077 - 12s - loss: 2.3650e-04 - val_loss: 2.2435e-04 - 12s/epoch - 148us/sample
Epoch 56/90
84077/84077 - 12s - loss: 2.3468e-04 - val_loss: 2.2594e-04 - 12s/epoch - 147us/sample
Epoch 57/90
84077/84077 - 12s - loss: 2.3349e-04 - val_loss: 2.2410e-04 - 12s/epoch - 146us/sample
Epoch 58/90
84077/84077 - 12s - loss: 2.3242e-04 - val_loss: 2.2564e-04 - 12s/epoch - 147us/sample
Epoch 59/90
84077/84077 - 12s - loss: 2.3401e-04 - val_loss: 2.2482e-04 - 12s/epoch - 147us/sample
Epoch 60/90
84077/84077 - 12s - loss: 2.3522e-04 - val_loss: 2.2066e-04 - 12s/epoch - 148us/sample
Epoch 61/90
84077/84077 - 12s - loss: 2.3176e-04 - val_loss: 2.2049e-04 - 12s/epoch - 147us/sample
Epoch 62/90
84077/84077 - 12s - loss: 2.3121e-04 - val_loss: 2.1890e-04 - 12s/epoch - 146us/sample
Epoch 63/90
84077/84077 - 12s - loss: 2.3106e-04 - val_loss: 2.2024e-04 - 12s/epoch - 146us/sample
Epoch 64/90
84077/84077 - 12s - loss: 2.3196e-04 - val_loss: 2.2285e-04 - 12s/epoch - 146us/sample
Epoch 65/90
84077/84077 - 12s - loss: 2.2894e-04 - val_loss: 2.1884e-04 - 12s/epoch - 147us/sample
Epoch 66/90
84077/84077 - 12s - loss: 2.2817e-04 - val_loss: 2.1924e-04 - 12s/epoch - 147us/sample
Epoch 67/90
84077/84077 - 12s - loss: 2.2854e-04 - val_loss: 2.2034e-04 - 12s/epoch - 146us/sample
Epoch 68/90
84077/84077 - 12s - loss: 2.2736e-04 - val_loss: 2.2217e-04 - 12s/epoch - 146us/sample
Epoch 69/90
84077/84077 - 12s - loss: 2.3069e-04 - val_loss: 2.2237e-04 - 12s/epoch - 146us/sample
Epoch 70/90
84077/84077 - 12s - loss: 2.3168e-04 - val_loss: 2.1612e-04 - 12s/epoch - 147us/sample
Epoch 71/90
84077/84077 - 12s - loss: 2.2823e-04 - val_loss: 2.1614e-04 - 12s/epoch - 147us/sample
Epoch 72/90
84077/84077 - 12s - loss: 2.2614e-04 - val_loss: 2.1648e-04 - 12s/epoch - 147us/sample
Epoch 73/90
84077/84077 - 12s - loss: 2.2647e-04 - val_loss: 2.1690e-04 - 12s/epoch - 147us/sample
Epoch 74/90
84077/84077 - 12s - loss: 2.2582e-04 - val_loss: 2.1748e-04 - 12s/epoch - 146us/sample
Epoch 75/90
84077/84077 - 12s - loss: 2.2633e-04 - val_loss: 2.1655e-04 - 12s/epoch - 147us/sample
Epoch 76/90
84077/84077 - 12s - loss: 2.2658e-04 - val_loss: 2.1532e-04 - 12s/epoch - 147us/sample
Epoch 77/90
84077/84077 - 12s - loss: 2.2260e-04 - val_loss: 2.1573e-04 - 12s/epoch - 147us/sample
Epoch 78/90
84077/84077 - 12s - loss: 2.2454e-04 - val_loss: 2.1342e-04 - 12s/epoch - 146us/sample
Epoch 79/90
84077/84077 - 12s - loss: 2.2413e-04 - val_loss: 2.1386e-04 - 12s/epoch - 146us/sample
Epoch 80/90
84077/84077 - 12s - loss: 2.2606e-04 - val_loss: 2.1387e-04 - 12s/epoch - 147us/sample
Epoch 81/90
84077/84077 - 12s - loss: 2.2233e-04 - val_loss: 2.1251e-04 - 12s/epoch - 147us/sample
Epoch 82/90
84077/84077 - 12s - loss: 2.2370e-04 - val_loss: 2.1429e-04 - 12s/epoch - 147us/sample
Epoch 83/90
84077/84077 - 12s - loss: 2.2355e-04 - val_loss: 2.1820e-04 - 12s/epoch - 146us/sample
Epoch 84/90
84077/84077 - 12s - loss: 2.2094e-04 - val_loss: 2.1162e-04 - 12s/epoch - 147us/sample
Epoch 85/90
84077/84077 - 12s - loss: 2.2100e-04 - val_loss: 2.2044e-04 - 12s/epoch - 146us/sample
Epoch 86/90
84077/84077 - 12s - loss: 2.2266e-04 - val_loss: 2.1619e-04 - 12s/epoch - 147us/sample
Epoch 87/90
84077/84077 - 12s - loss: 2.2031e-04 - val_loss: 2.1456e-04 - 12s/epoch - 147us/sample
Epoch 88/90
84077/84077 - 12s - loss: 2.1963e-04 - val_loss: 2.1690e-04 - 12s/epoch - 147us/sample
Epoch 89/90
84077/84077 - 12s - loss: 2.1991e-04 - val_loss: 2.1123e-04 - 12s/epoch - 146us/sample
Epoch 90/90
84077/84077 - 12s - loss: 2.2001e-04 - val_loss: 2.2232e-04 - 12s/epoch - 146us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00022231568590892103
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 13:28:06.616261: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_31/outputlayer/BiasAdd' id:39777 op device:{requested: '', assigned: ''} def:{{{node decoder_model_31/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_31/outputlayer/MatMul, decoder_model_31/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.025055168460676153
cosine 0.02475874361481993
MAE: 0.0022316874574916613
RMSE: 0.010288015552614
r2: 0.9174464182615416
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 32, 90, 0.0012000000000000001, 0.2, 188, 0.00022001350279059502, 0.00022231568590892103, 0.025055168460676153, 0.02475874361481993, 0.0022316874574916613, 0.010288015552614, 0.9174464182615416, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0004000000000000001 32 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_96 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_96 (ReLU)                (None, 2074)         0           ['batch_normalization_96[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 13:28:21.592291: W tensorflow/c/c_api.cc:291] Operation '{name:'training_64/Adam/outputlayer_32/bias/v/Assign' id:41768 op device:{requested: '', assigned: ''} def:{{{node training_64/Adam/outputlayer_32/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_64/Adam/outputlayer_32/bias/v, training_64/Adam/outputlayer_32/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 13:28:36.856188: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_32/mul' id:41071 op device:{requested: '', assigned: ''} def:{{{node loss_32/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_32/mul/x, loss_32/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0029 - val_loss: 8.5623e-04 - 21s/epoch - 244us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.0021 - val_loss: 5.7507e-04 - 13s/epoch - 150us/sample
Epoch 3/85
84077/84077 - 13s - loss: 7.3970e-04 - val_loss: 5.0493e-04 - 13s/epoch - 150us/sample
Epoch 4/85
84077/84077 - 13s - loss: 6.2613e-04 - val_loss: 4.6098e-04 - 13s/epoch - 149us/sample
Epoch 5/85
84077/84077 - 13s - loss: 8.3301e-04 - val_loss: 5.7059e-04 - 13s/epoch - 149us/sample
Epoch 6/85
84077/84077 - 13s - loss: 6.3026e-04 - val_loss: 5.8817e-04 - 13s/epoch - 151us/sample
Epoch 7/85
84077/84077 - 13s - loss: 5.1902e-04 - val_loss: 3.8649e-04 - 13s/epoch - 150us/sample
Epoch 8/85
84077/84077 - 13s - loss: 4.0939e-04 - val_loss: 3.7072e-04 - 13s/epoch - 149us/sample
Epoch 9/85
84077/84077 - 13s - loss: 3.9333e-04 - val_loss: 3.4829e-04 - 13s/epoch - 149us/sample
Epoch 10/85
84077/84077 - 13s - loss: 3.7373e-04 - val_loss: 3.4062e-04 - 13s/epoch - 149us/sample
Epoch 11/85
84077/84077 - 13s - loss: 3.5541e-04 - val_loss: 3.4741e-04 - 13s/epoch - 151us/sample
Epoch 12/85
84077/84077 - 13s - loss: 3.4545e-04 - val_loss: 3.1165e-04 - 13s/epoch - 150us/sample
Epoch 13/85
84077/84077 - 13s - loss: 3.3768e-04 - val_loss: 3.0786e-04 - 13s/epoch - 150us/sample
Epoch 14/85
84077/84077 - 13s - loss: 3.3057e-04 - val_loss: 3.0266e-04 - 13s/epoch - 150us/sample
Epoch 15/85
84077/84077 - 13s - loss: 3.2621e-04 - val_loss: 2.9932e-04 - 13s/epoch - 150us/sample
Epoch 16/85
84077/84077 - 13s - loss: 3.2134e-04 - val_loss: 2.9711e-04 - 13s/epoch - 150us/sample
Epoch 17/85
84077/84077 - 13s - loss: 3.1824e-04 - val_loss: 2.9510e-04 - 13s/epoch - 150us/sample
Epoch 18/85
84077/84077 - 13s - loss: 3.1547e-04 - val_loss: 2.9212e-04 - 13s/epoch - 150us/sample
Epoch 19/85
84077/84077 - 13s - loss: 3.1247e-04 - val_loss: 2.8871e-04 - 13s/epoch - 149us/sample
Epoch 20/85
84077/84077 - 13s - loss: 3.0980e-04 - val_loss: 2.8568e-04 - 13s/epoch - 149us/sample
Epoch 21/85
84077/84077 - 13s - loss: 3.0689e-04 - val_loss: 2.8474e-04 - 13s/epoch - 149us/sample
Epoch 22/85
84077/84077 - 13s - loss: 3.0587e-04 - val_loss: 2.8306e-04 - 13s/epoch - 150us/sample
Epoch 23/85
84077/84077 - 13s - loss: 3.0395e-04 - val_loss: 2.8420e-04 - 13s/epoch - 151us/sample
Epoch 24/85
84077/84077 - 13s - loss: 3.0250e-04 - val_loss: 2.7987e-04 - 13s/epoch - 150us/sample
Epoch 25/85
84077/84077 - 13s - loss: 3.0129e-04 - val_loss: 2.7889e-04 - 13s/epoch - 150us/sample
Epoch 26/85
84077/84077 - 13s - loss: 3.0045e-04 - val_loss: 2.7933e-04 - 13s/epoch - 150us/sample
Epoch 27/85
84077/84077 - 13s - loss: 2.9911e-04 - val_loss: 2.7729e-04 - 13s/epoch - 150us/sample
Epoch 28/85
84077/84077 - 13s - loss: 2.9805e-04 - val_loss: 2.7640e-04 - 13s/epoch - 150us/sample
Epoch 29/85
84077/84077 - 13s - loss: 2.9748e-04 - val_loss: 2.7696e-04 - 13s/epoch - 151us/sample
Epoch 30/85
84077/84077 - 13s - loss: 2.9607e-04 - val_loss: 2.7505e-04 - 13s/epoch - 150us/sample
Epoch 31/85
84077/84077 - 13s - loss: 2.9458e-04 - val_loss: 2.7222e-04 - 13s/epoch - 149us/sample
Epoch 32/85
84077/84077 - 13s - loss: 2.9277e-04 - val_loss: 2.7207e-04 - 13s/epoch - 149us/sample
Epoch 33/85
84077/84077 - 13s - loss: 2.9187e-04 - val_loss: 2.7128e-04 - 13s/epoch - 149us/sample
Epoch 34/85
84077/84077 - 13s - loss: 2.9027e-04 - val_loss: 2.7009e-04 - 13s/epoch - 151us/sample
Epoch 35/85
84077/84077 - 13s - loss: 2.8951e-04 - val_loss: 2.6857e-04 - 13s/epoch - 150us/sample
Epoch 36/85
84077/84077 - 13s - loss: 2.8915e-04 - val_loss: 2.6874e-04 - 13s/epoch - 149us/sample
Epoch 37/85
84077/84077 - 13s - loss: 2.8873e-04 - val_loss: 2.6741e-04 - 13s/epoch - 149us/sample
Epoch 38/85
84077/84077 - 13s - loss: 2.8810e-04 - val_loss: 2.6793e-04 - 13s/epoch - 149us/sample
Epoch 39/85
84077/84077 - 13s - loss: 2.8710e-04 - val_loss: 2.6702e-04 - 13s/epoch - 151us/sample
Epoch 40/85
84077/84077 - 13s - loss: 2.8696e-04 - val_loss: 2.6607e-04 - 13s/epoch - 150us/sample
Epoch 41/85
84077/84077 - 13s - loss: 2.8639e-04 - val_loss: 2.6594e-04 - 13s/epoch - 150us/sample
Epoch 42/85
84077/84077 - 13s - loss: 2.8584e-04 - val_loss: 2.6580e-04 - 13s/epoch - 149us/sample
Epoch 43/85
84077/84077 - 13s - loss: 2.8550e-04 - val_loss: 2.6505e-04 - 13s/epoch - 149us/sample
Epoch 44/85
84077/84077 - 13s - loss: 2.8511e-04 - val_loss: 2.6571e-04 - 13s/epoch - 149us/sample
Epoch 45/85
84077/84077 - 13s - loss: 2.8471e-04 - val_loss: 2.6494e-04 - 13s/epoch - 149us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.8451e-04 - val_loss: 2.6385e-04 - 12s/epoch - 148us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.8407e-04 - val_loss: 2.6372e-04 - 12s/epoch - 147us/sample
Epoch 48/85
84077/84077 - 12s - loss: 2.8372e-04 - val_loss: 2.6350e-04 - 12s/epoch - 147us/sample
Epoch 49/85
84077/84077 - 12s - loss: 2.8391e-04 - val_loss: 2.6418e-04 - 12s/epoch - 148us/sample
Epoch 50/85
84077/84077 - 13s - loss: 2.8325e-04 - val_loss: 2.6413e-04 - 13s/epoch - 150us/sample
Epoch 51/85
84077/84077 - 13s - loss: 2.8264e-04 - val_loss: 2.6323e-04 - 13s/epoch - 149us/sample
Epoch 52/85
84077/84077 - 12s - loss: 2.8185e-04 - val_loss: 2.6209e-04 - 12s/epoch - 148us/sample
Epoch 53/85
84077/84077 - 12s - loss: 2.8154e-04 - val_loss: 2.6285e-04 - 12s/epoch - 147us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.8126e-04 - val_loss: 2.6156e-04 - 12s/epoch - 147us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.8080e-04 - val_loss: 2.6140e-04 - 12s/epoch - 147us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.8087e-04 - val_loss: 2.6176e-04 - 12s/epoch - 148us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.8018e-04 - val_loss: 2.6055e-04 - 12s/epoch - 148us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.8033e-04 - val_loss: 2.6049e-04 - 12s/epoch - 147us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.8000e-04 - val_loss: 2.6106e-04 - 12s/epoch - 147us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.7989e-04 - val_loss: 2.6027e-04 - 12s/epoch - 147us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.7976e-04 - val_loss: 2.6117e-04 - 12s/epoch - 147us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.7928e-04 - val_loss: 2.5979e-04 - 12s/epoch - 149us/sample
Epoch 63/85
84077/84077 - 12s - loss: 2.7967e-04 - val_loss: 2.5959e-04 - 12s/epoch - 147us/sample
Epoch 64/85
84077/84077 - 12s - loss: 2.7954e-04 - val_loss: 2.5931e-04 - 12s/epoch - 147us/sample
Epoch 65/85
84077/84077 - 12s - loss: 2.7906e-04 - val_loss: 2.5869e-04 - 12s/epoch - 147us/sample
Epoch 66/85
84077/84077 - 12s - loss: 2.7896e-04 - val_loss: 2.6045e-04 - 12s/epoch - 147us/sample
Epoch 67/85
84077/84077 - 12s - loss: 2.7869e-04 - val_loss: 2.5968e-04 - 12s/epoch - 148us/sample
Epoch 68/85
84077/84077 - 12s - loss: 2.7880e-04 - val_loss: 2.5962e-04 - 12s/epoch - 147us/sample
Epoch 69/85
84077/84077 - 12s - loss: 2.7834e-04 - val_loss: 2.5900e-04 - 12s/epoch - 147us/sample
Epoch 70/85
84077/84077 - 12s - loss: 2.7833e-04 - val_loss: 2.6085e-04 - 12s/epoch - 147us/sample
Epoch 71/85
84077/84077 - 12s - loss: 2.7806e-04 - val_loss: 2.5883e-04 - 12s/epoch - 147us/sample
Epoch 72/85
84077/84077 - 12s - loss: 2.7790e-04 - val_loss: 2.5836e-04 - 12s/epoch - 148us/sample
Epoch 73/85
84077/84077 - 12s - loss: 2.7798e-04 - val_loss: 2.5748e-04 - 12s/epoch - 148us/sample
Epoch 74/85
84077/84077 - 12s - loss: 2.7797e-04 - val_loss: 2.5935e-04 - 12s/epoch - 147us/sample
Epoch 75/85
84077/84077 - 12s - loss: 2.7786e-04 - val_loss: 2.5892e-04 - 12s/epoch - 147us/sample
Epoch 76/85
84077/84077 - 12s - loss: 2.7788e-04 - val_loss: 2.5907e-04 - 12s/epoch - 147us/sample
Epoch 77/85
84077/84077 - 12s - loss: 2.7729e-04 - val_loss: 2.5902e-04 - 12s/epoch - 148us/sample
Epoch 78/85
84077/84077 - 12s - loss: 2.7760e-04 - val_loss: 2.5979e-04 - 12s/epoch - 147us/sample
Epoch 79/85
84077/84077 - 12s - loss: 2.7745e-04 - val_loss: 2.5799e-04 - 12s/epoch - 147us/sample
Epoch 80/85
84077/84077 - 12s - loss: 2.7736e-04 - val_loss: 2.5808e-04 - 12s/epoch - 147us/sample
Epoch 81/85
84077/84077 - 12s - loss: 2.7662e-04 - val_loss: 2.5840e-04 - 12s/epoch - 147us/sample
Epoch 82/85
84077/84077 - 12s - loss: 2.7701e-04 - val_loss: 2.5836e-04 - 12s/epoch - 147us/sample
Epoch 83/85
84077/84077 - 12s - loss: 2.7700e-04 - val_loss: 2.5912e-04 - 12s/epoch - 148us/sample
Epoch 84/85
84077/84077 - 12s - loss: 2.7687e-04 - val_loss: 2.5969e-04 - 12s/epoch - 147us/sample
Epoch 85/85
84077/84077 - 12s - loss: 2.7671e-04 - val_loss: 2.5751e-04 - 12s/epoch - 147us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00025750717463560386
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 13:46:09.784018: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_32/outputlayer/BiasAdd' id:41035 op device:{requested: '', assigned: ''} def:{{{node decoder_model_32/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_32/outputlayer/MatMul, decoder_model_32/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.11908593568186711
cosine 0.11749906063368443
MAE: 0.003599423315877935
RMSE: 0.020371052698865834
r2: 0.6758252289687305
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 32, 85, 0.0004000000000000001, 0.2, 188, 0.00027671069813307956, 0.00025750717463560386, 0.11908593568186711, 0.11749906063368443, 0.003599423315877935, 0.020371052698865834, 0.6758252289687305, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 85 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_99 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_99 (ReLU)                (None, 1697)         0           ['batch_normalization_99[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 13:46:24.767797: W tensorflow/c/c_api.cc:291] Operation '{name:'training_66/Adam/dense_dec0_33/bias/m/Assign' id:42883 op device:{requested: '', assigned: ''} def:{{{node training_66/Adam/dense_dec0_33/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_66/Adam/dense_dec0_33/bias/m, training_66/Adam/dense_dec0_33/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 13:46:39.833551: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_33/mul' id:42349 op device:{requested: '', assigned: ''} def:{{{node loss_33/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_33/mul/x, loss_33/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0193 - val_loss: 0.0019 - 20s/epoch - 243us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0019 - val_loss: 0.0013 - 12s/epoch - 145us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0013 - val_loss: 9.8198e-04 - 12s/epoch - 145us/sample
Epoch 4/85
84077/84077 - 12s - loss: 9.2821e-04 - val_loss: 7.4928e-04 - 12s/epoch - 145us/sample
Epoch 5/85
84077/84077 - 12s - loss: 7.6307e-04 - val_loss: 5.8885e-04 - 12s/epoch - 145us/sample
Epoch 6/85
84077/84077 - 12s - loss: 6.2173e-04 - val_loss: 5.0215e-04 - 12s/epoch - 145us/sample
Epoch 7/85
84077/84077 - 12s - loss: 5.3265e-04 - val_loss: 4.3317e-04 - 12s/epoch - 145us/sample
Epoch 8/85
84077/84077 - 12s - loss: 4.6012e-04 - val_loss: 3.7758e-04 - 12s/epoch - 147us/sample
Epoch 9/85
84077/84077 - 12s - loss: 4.0565e-04 - val_loss: 3.4153e-04 - 12s/epoch - 146us/sample
Epoch 10/85
84077/84077 - 12s - loss: 3.7136e-04 - val_loss: 3.2318e-04 - 12s/epoch - 145us/sample
Epoch 11/85
84077/84077 - 12s - loss: 3.4670e-04 - val_loss: 3.0475e-04 - 12s/epoch - 145us/sample
Epoch 12/85
84077/84077 - 12s - loss: 3.2839e-04 - val_loss: 2.8863e-04 - 12s/epoch - 145us/sample
Epoch 13/85
84077/84077 - 12s - loss: 3.1289e-04 - val_loss: 2.8067e-04 - 12s/epoch - 146us/sample
Epoch 14/85
84077/84077 - 12s - loss: 3.0135e-04 - val_loss: 2.7130e-04 - 12s/epoch - 147us/sample
Epoch 15/85
84077/84077 - 12s - loss: 2.8985e-04 - val_loss: 2.6368e-04 - 12s/epoch - 146us/sample
Epoch 16/85
84077/84077 - 12s - loss: 2.8347e-04 - val_loss: 2.6754e-04 - 12s/epoch - 145us/sample
Epoch 17/85
84077/84077 - 12s - loss: 2.7645e-04 - val_loss: 2.5691e-04 - 12s/epoch - 145us/sample
Epoch 18/85
84077/84077 - 12s - loss: 2.7100e-04 - val_loss: 2.5470e-04 - 12s/epoch - 145us/sample
Epoch 19/85
84077/84077 - 12s - loss: 2.6585e-04 - val_loss: 2.4305e-04 - 12s/epoch - 146us/sample
Epoch 20/85
84077/84077 - 12s - loss: 2.6038e-04 - val_loss: 2.4598e-04 - 12s/epoch - 146us/sample
Epoch 21/85
84077/84077 - 12s - loss: 2.5620e-04 - val_loss: 2.4009e-04 - 12s/epoch - 146us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.5336e-04 - val_loss: 2.3564e-04 - 12s/epoch - 145us/sample
Epoch 23/85
84077/84077 - 12s - loss: 2.5110e-04 - val_loss: 2.3600e-04 - 12s/epoch - 145us/sample
Epoch 24/85
84077/84077 - 12s - loss: 2.5065e-04 - val_loss: 2.4066e-04 - 12s/epoch - 145us/sample
Epoch 25/85
84077/84077 - 12s - loss: 2.4443e-04 - val_loss: 2.3120e-04 - 12s/epoch - 146us/sample
Epoch 26/85
84077/84077 - 12s - loss: 2.4235e-04 - val_loss: 2.2713e-04 - 12s/epoch - 146us/sample
Epoch 27/85
84077/84077 - 12s - loss: 2.4117e-04 - val_loss: 2.3289e-04 - 12s/epoch - 145us/sample
Epoch 28/85
84077/84077 - 12s - loss: 2.3859e-04 - val_loss: 2.2581e-04 - 12s/epoch - 145us/sample
Epoch 29/85
84077/84077 - 12s - loss: 2.3752e-04 - val_loss: 2.2582e-04 - 12s/epoch - 145us/sample
Epoch 30/85
84077/84077 - 12s - loss: 2.3661e-04 - val_loss: 2.2697e-04 - 12s/epoch - 146us/sample
Epoch 31/85
84077/84077 - 12s - loss: 2.3297e-04 - val_loss: 2.2206e-04 - 12s/epoch - 146us/sample
Epoch 32/85
84077/84077 - 12s - loss: 2.3259e-04 - val_loss: 2.2218e-04 - 12s/epoch - 145us/sample
Epoch 33/85
84077/84077 - 12s - loss: 2.3272e-04 - val_loss: 2.2413e-04 - 12s/epoch - 146us/sample
Epoch 34/85
84077/84077 - 12s - loss: 2.3991e-04 - val_loss: 2.2003e-04 - 12s/epoch - 145us/sample
Epoch 35/85
84077/84077 - 12s - loss: 2.2879e-04 - val_loss: 2.2061e-04 - 12s/epoch - 146us/sample
Epoch 36/85
84077/84077 - 12s - loss: 2.3392e-04 - val_loss: 2.2600e-04 - 12s/epoch - 146us/sample
Epoch 37/85
84077/84077 - 12s - loss: 2.2808e-04 - val_loss: 2.1936e-04 - 12s/epoch - 146us/sample
Epoch 38/85
84077/84077 - 12s - loss: 2.2689e-04 - val_loss: 2.1702e-04 - 12s/epoch - 145us/sample
Epoch 39/85
84077/84077 - 12s - loss: 2.2641e-04 - val_loss: 2.1583e-04 - 12s/epoch - 145us/sample
Epoch 40/85
84077/84077 - 12s - loss: 2.2550e-04 - val_loss: 2.1556e-04 - 12s/epoch - 145us/sample
Epoch 41/85
84077/84077 - 12s - loss: 2.2565e-04 - val_loss: 2.1799e-04 - 12s/epoch - 146us/sample
Epoch 42/85
84077/84077 - 12s - loss: 2.2219e-04 - val_loss: 2.1450e-04 - 12s/epoch - 146us/sample
Epoch 43/85
84077/84077 - 12s - loss: 2.2290e-04 - val_loss: 2.1513e-04 - 12s/epoch - 146us/sample
Epoch 44/85
84077/84077 - 12s - loss: 2.2214e-04 - val_loss: 2.1345e-04 - 12s/epoch - 145us/sample
Epoch 45/85
84077/84077 - 12s - loss: 2.2024e-04 - val_loss: 2.1132e-04 - 12s/epoch - 146us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.2043e-04 - val_loss: 2.1561e-04 - 12s/epoch - 145us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.2316e-04 - val_loss: 2.1020e-04 - 12s/epoch - 146us/sample
Epoch 48/85
84077/84077 - 12s - loss: 2.1755e-04 - val_loss: 2.1047e-04 - 12s/epoch - 146us/sample
Epoch 49/85
84077/84077 - 12s - loss: 2.1971e-04 - val_loss: 2.1426e-04 - 12s/epoch - 146us/sample
Epoch 50/85
84077/84077 - 12s - loss: 2.1765e-04 - val_loss: 2.1547e-04 - 12s/epoch - 145us/sample
Epoch 51/85
84077/84077 - 12s - loss: 2.2126e-04 - val_loss: 2.0824e-04 - 12s/epoch - 145us/sample
Epoch 52/85
84077/84077 - 12s - loss: 2.1856e-04 - val_loss: 2.0903e-04 - 12s/epoch - 145us/sample
Epoch 53/85
84077/84077 - 12s - loss: 2.1576e-04 - val_loss: 2.0410e-04 - 12s/epoch - 146us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.1551e-04 - val_loss: 2.0918e-04 - 12s/epoch - 146us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.2202e-04 - val_loss: 2.1629e-04 - 12s/epoch - 146us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.1569e-04 - val_loss: 2.0516e-04 - 12s/epoch - 145us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.1417e-04 - val_loss: 2.0938e-04 - 12s/epoch - 145us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.1274e-04 - val_loss: 2.1170e-04 - 12s/epoch - 145us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.1381e-04 - val_loss: 2.0785e-04 - 12s/epoch - 145us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.1477e-04 - val_loss: 2.0528e-04 - 12s/epoch - 146us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.1230e-04 - val_loss: 2.0434e-04 - 12s/epoch - 147us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.1120e-04 - val_loss: 2.0776e-04 - 12s/epoch - 145us/sample
Epoch 63/85
84077/84077 - 12s - loss: 2.1078e-04 - val_loss: 2.0304e-04 - 12s/epoch - 145us/sample
Epoch 64/85
84077/84077 - 12s - loss: 2.1032e-04 - val_loss: 2.0375e-04 - 12s/epoch - 145us/sample
Epoch 65/85
84077/84077 - 12s - loss: 2.1265e-04 - val_loss: 2.0772e-04 - 12s/epoch - 146us/sample
Epoch 66/85
84077/84077 - 12s - loss: 2.1162e-04 - val_loss: 2.0175e-04 - 12s/epoch - 146us/sample
Epoch 67/85
84077/84077 - 12s - loss: 2.0962e-04 - val_loss: 2.0155e-04 - 12s/epoch - 145us/sample
Epoch 68/85
84077/84077 - 12s - loss: 2.0853e-04 - val_loss: 2.0058e-04 - 12s/epoch - 145us/sample
Epoch 69/85
84077/84077 - 12s - loss: 2.0914e-04 - val_loss: 2.0074e-04 - 12s/epoch - 145us/sample
Epoch 70/85
84077/84077 - 12s - loss: 2.1122e-04 - val_loss: 2.0437e-04 - 12s/epoch - 145us/sample
Epoch 71/85
84077/84077 - 12s - loss: 2.0895e-04 - val_loss: 3.1291e-04 - 12s/epoch - 147us/sample
Epoch 72/85
84077/84077 - 12s - loss: 2.1157e-04 - val_loss: 1.9642e-04 - 12s/epoch - 146us/sample
Epoch 73/85
84077/84077 - 12s - loss: 2.0709e-04 - val_loss: 1.9645e-04 - 12s/epoch - 145us/sample
Epoch 74/85
84077/84077 - 12s - loss: 2.0886e-04 - val_loss: 1.9820e-04 - 12s/epoch - 145us/sample
Epoch 75/85
84077/84077 - 12s - loss: 2.0973e-04 - val_loss: 1.9834e-04 - 12s/epoch - 145us/sample
Epoch 76/85
84077/84077 - 12s - loss: 2.0693e-04 - val_loss: 1.9865e-04 - 12s/epoch - 145us/sample
Epoch 77/85
84077/84077 - 12s - loss: 2.0699e-04 - val_loss: 2.0000e-04 - 12s/epoch - 146us/sample
Epoch 78/85
84077/84077 - 12s - loss: 2.0642e-04 - val_loss: 1.9486e-04 - 12s/epoch - 145us/sample
Epoch 79/85
84077/84077 - 12s - loss: 2.0485e-04 - val_loss: 1.9544e-04 - 12s/epoch - 145us/sample
Epoch 80/85
84077/84077 - 12s - loss: 2.0611e-04 - val_loss: 1.9574e-04 - 12s/epoch - 145us/sample
Epoch 81/85
84077/84077 - 12s - loss: 2.0532e-04 - val_loss: 1.9765e-04 - 12s/epoch - 145us/sample
Epoch 82/85
84077/84077 - 12s - loss: 2.0595e-04 - val_loss: 1.9472e-04 - 12s/epoch - 146us/sample
Epoch 83/85
84077/84077 - 12s - loss: 2.0477e-04 - val_loss: 1.9196e-04 - 12s/epoch - 146us/sample
Epoch 84/85
84077/84077 - 12s - loss: 2.0588e-04 - val_loss: 1.9365e-04 - 12s/epoch - 145us/sample
Epoch 85/85
84077/84077 - 12s - loss: 2.0405e-04 - val_loss: 1.9521e-04 - 12s/epoch - 145us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001952090589963143
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 14:03:50.578904: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_33/outputlayer/BiasAdd' id:42320 op device:{requested: '', assigned: ''} def:{{{node decoder_model_33/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_33/outputlayer/MatMul, decoder_model_33/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02493519959771992
cosine 0.024660800795831016
MAE: 0.002092091199312654
RMSE: 0.00941499705754838
r2: 0.9309683127092538
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 85, 0.001, 0.2, 188, 0.00020405418462609843, 0.0001952090589963143, 0.02493519959771992, 0.024660800795831016, 0.002092091199312654, 0.00941499705754838, 0.9309683127092538, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 90 0.001 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_102 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_102 (ReLU)               (None, 1603)         0           ['batch_normalization_102[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 14:04:05.841641: W tensorflow/c/c_api.cc:291] Operation '{name:'training_68/Adam/bottleneck_zlog_34/kernel/m/Assign' id:44097 op device:{requested: '', assigned: ''} def:{{{node training_68/Adam/bottleneck_zlog_34/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_68/Adam/bottleneck_zlog_34/kernel/m, training_68/Adam/bottleneck_zlog_34/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 14:04:21.091062: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_34/mul' id:43604 op device:{requested: '', assigned: ''} def:{{{node loss_34/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_34/mul/x, loss_34/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0097 - val_loss: 0.0022 - 21s/epoch - 249us/sample
Epoch 2/90
84077/84077 - 12s - loss: 0.0024 - val_loss: 0.0022 - 12s/epoch - 146us/sample
Epoch 3/90
84077/84077 - 12s - loss: 0.0012 - val_loss: 9.4076e-04 - 12s/epoch - 146us/sample
Epoch 4/90
84077/84077 - 12s - loss: 9.5565e-04 - val_loss: 8.1437e-04 - 12s/epoch - 146us/sample
Epoch 5/90
84077/84077 - 12s - loss: 0.0012 - val_loss: 0.0027 - 12s/epoch - 146us/sample
Epoch 6/90
84077/84077 - 12s - loss: 9.2675e-04 - val_loss: 6.7961e-04 - 12s/epoch - 146us/sample
Epoch 7/90
84077/84077 - 12s - loss: 7.3794e-04 - val_loss: 5.9594e-04 - 12s/epoch - 146us/sample
Epoch 8/90
84077/84077 - 12s - loss: 6.2557e-04 - val_loss: 5.3082e-04 - 12s/epoch - 147us/sample
Epoch 9/90
84077/84077 - 12s - loss: 5.5721e-04 - val_loss: 5.8620e-04 - 12s/epoch - 147us/sample
Epoch 10/90
84077/84077 - 12s - loss: 5.1231e-04 - val_loss: 5.0810e-04 - 12s/epoch - 147us/sample
Epoch 11/90
84077/84077 - 12s - loss: 4.9711e-04 - val_loss: 4.4503e-04 - 12s/epoch - 147us/sample
Epoch 12/90
84077/84077 - 12s - loss: 4.5203e-04 - val_loss: 4.1145e-04 - 12s/epoch - 146us/sample
Epoch 13/90
84077/84077 - 12s - loss: 4.3255e-04 - val_loss: 3.9737e-04 - 12s/epoch - 147us/sample
Epoch 14/90
84077/84077 - 12s - loss: 4.3838e-04 - val_loss: 3.8359e-04 - 12s/epoch - 148us/sample
Epoch 15/90
84077/84077 - 12s - loss: 4.0336e-04 - val_loss: 3.7909e-04 - 12s/epoch - 147us/sample
Epoch 16/90
84077/84077 - 12s - loss: 3.9814e-04 - val_loss: 3.7990e-04 - 12s/epoch - 147us/sample
Epoch 17/90
84077/84077 - 12s - loss: 3.8391e-04 - val_loss: 3.8539e-04 - 12s/epoch - 146us/sample
Epoch 18/90
84077/84077 - 12s - loss: 3.7138e-04 - val_loss: 3.6899e-04 - 12s/epoch - 146us/sample
Epoch 19/90
84077/84077 - 12s - loss: 3.6217e-04 - val_loss: 3.5269e-04 - 12s/epoch - 147us/sample
Epoch 20/90
84077/84077 - 12s - loss: 3.5269e-04 - val_loss: 3.4096e-04 - 12s/epoch - 147us/sample
Epoch 21/90
84077/84077 - 12s - loss: 3.4773e-04 - val_loss: 3.3551e-04 - 12s/epoch - 147us/sample
Epoch 22/90
84077/84077 - 12s - loss: 3.4009e-04 - val_loss: 3.3017e-04 - 12s/epoch - 146us/sample
Epoch 23/90
84077/84077 - 12s - loss: 3.3491e-04 - val_loss: 3.3991e-04 - 12s/epoch - 146us/sample
Epoch 24/90
84077/84077 - 12s - loss: 3.2989e-04 - val_loss: 3.2137e-04 - 12s/epoch - 147us/sample
Epoch 25/90
84077/84077 - 12s - loss: 3.2532e-04 - val_loss: 3.1529e-04 - 12s/epoch - 147us/sample
Epoch 26/90
84077/84077 - 12s - loss: 3.2153e-04 - val_loss: 3.0729e-04 - 12s/epoch - 146us/sample
Epoch 27/90
84077/84077 - 12s - loss: 3.1767e-04 - val_loss: 3.0328e-04 - 12s/epoch - 146us/sample
Epoch 28/90
84077/84077 - 12s - loss: 3.1419e-04 - val_loss: 3.0358e-04 - 12s/epoch - 146us/sample
Epoch 29/90
84077/84077 - 12s - loss: 3.1499e-04 - val_loss: 2.9745e-04 - 12s/epoch - 148us/sample
Epoch 30/90
84077/84077 - 12s - loss: 3.0609e-04 - val_loss: 3.2337e-04 - 12s/epoch - 147us/sample
Epoch 31/90
84077/84077 - 12s - loss: 3.0708e-04 - val_loss: 2.9264e-04 - 12s/epoch - 147us/sample
Epoch 32/90
84077/84077 - 12s - loss: 3.0376e-04 - val_loss: 2.9338e-04 - 12s/epoch - 146us/sample
Epoch 33/90
84077/84077 - 12s - loss: 2.9971e-04 - val_loss: 2.9268e-04 - 12s/epoch - 146us/sample
Epoch 34/90
84077/84077 - 12s - loss: 2.9947e-04 - val_loss: 2.8890e-04 - 12s/epoch - 147us/sample
Epoch 35/90
84077/84077 - 12s - loss: 2.9421e-04 - val_loss: 2.8526e-04 - 12s/epoch - 147us/sample
Epoch 36/90
84077/84077 - 12s - loss: 2.9436e-04 - val_loss: 2.7596e-04 - 12s/epoch - 147us/sample
Epoch 37/90
84077/84077 - 12s - loss: 2.9599e-04 - val_loss: 2.8315e-04 - 12s/epoch - 146us/sample
Epoch 38/90
84077/84077 - 12s - loss: 2.8982e-04 - val_loss: 2.8156e-04 - 12s/epoch - 146us/sample
Epoch 39/90
84077/84077 - 12s - loss: 2.8837e-04 - val_loss: 2.7901e-04 - 12s/epoch - 147us/sample
Epoch 40/90
84077/84077 - 12s - loss: 2.8535e-04 - val_loss: 2.7921e-04 - 12s/epoch - 148us/sample
Epoch 41/90
84077/84077 - 12s - loss: 2.8710e-04 - val_loss: 2.7355e-04 - 12s/epoch - 147us/sample
Epoch 42/90
84077/84077 - 12s - loss: 2.8937e-04 - val_loss: 2.8410e-04 - 12s/epoch - 146us/sample
Epoch 43/90
84077/84077 - 12s - loss: 2.8656e-04 - val_loss: 2.7010e-04 - 12s/epoch - 146us/sample
Epoch 44/90
84077/84077 - 12s - loss: 2.8318e-04 - val_loss: 2.7143e-04 - 12s/epoch - 146us/sample
Epoch 45/90
84077/84077 - 12s - loss: 2.8053e-04 - val_loss: 2.6996e-04 - 12s/epoch - 147us/sample
Epoch 46/90
84077/84077 - 12s - loss: 2.7814e-04 - val_loss: 2.7524e-04 - 12s/epoch - 147us/sample
Epoch 47/90
84077/84077 - 12s - loss: 2.7784e-04 - val_loss: 2.6316e-04 - 12s/epoch - 147us/sample
Epoch 48/90
84077/84077 - 12s - loss: 2.7656e-04 - val_loss: 2.7210e-04 - 12s/epoch - 146us/sample
Epoch 49/90
84077/84077 - 12s - loss: 2.7600e-04 - val_loss: 2.6816e-04 - 12s/epoch - 146us/sample
Epoch 50/90
84077/84077 - 12s - loss: 2.7345e-04 - val_loss: 2.6877e-04 - 12s/epoch - 147us/sample
Epoch 51/90
84077/84077 - 12s - loss: 2.7461e-04 - val_loss: 2.6466e-04 - 12s/epoch - 148us/sample
Epoch 52/90
84077/84077 - 12s - loss: 2.7191e-04 - val_loss: 2.6506e-04 - 12s/epoch - 147us/sample
Epoch 53/90
84077/84077 - 12s - loss: 2.7505e-04 - val_loss: 2.6645e-04 - 12s/epoch - 146us/sample
Epoch 54/90
84077/84077 - 12s - loss: 2.7063e-04 - val_loss: 2.6107e-04 - 12s/epoch - 147us/sample
Epoch 55/90
84077/84077 - 12s - loss: 2.7131e-04 - val_loss: 2.6117e-04 - 12s/epoch - 146us/sample
Epoch 56/90
84077/84077 - 12s - loss: 2.6910e-04 - val_loss: 2.6072e-04 - 12s/epoch - 148us/sample
Epoch 57/90
84077/84077 - 12s - loss: 2.7018e-04 - val_loss: 2.5831e-04 - 12s/epoch - 147us/sample
Epoch 58/90
84077/84077 - 12s - loss: 2.6774e-04 - val_loss: 2.5722e-04 - 12s/epoch - 147us/sample
Epoch 59/90
84077/84077 - 12s - loss: 2.7234e-04 - val_loss: 2.6091e-04 - 12s/epoch - 146us/sample
Epoch 60/90
84077/84077 - 12s - loss: 2.6678e-04 - val_loss: 2.5775e-04 - 12s/epoch - 146us/sample
Epoch 61/90
84077/84077 - 12s - loss: 2.6617e-04 - val_loss: 2.5592e-04 - 12s/epoch - 147us/sample
Epoch 62/90
84077/84077 - 12s - loss: 2.6655e-04 - val_loss: 2.5688e-04 - 12s/epoch - 148us/sample
Epoch 63/90
84077/84077 - 12s - loss: 2.6583e-04 - val_loss: 2.5149e-04 - 12s/epoch - 147us/sample
Epoch 64/90
84077/84077 - 12s - loss: 2.6618e-04 - val_loss: 2.5325e-04 - 12s/epoch - 146us/sample
Epoch 65/90
84077/84077 - 12s - loss: 2.6817e-04 - val_loss: 2.5183e-04 - 12s/epoch - 147us/sample
Epoch 66/90
84077/84077 - 12s - loss: 2.6342e-04 - val_loss: 2.5143e-04 - 12s/epoch - 147us/sample
Epoch 67/90
84077/84077 - 12s - loss: 2.6531e-04 - val_loss: 2.5636e-04 - 12s/epoch - 147us/sample
Epoch 68/90
84077/84077 - 12s - loss: 2.6257e-04 - val_loss: 2.5026e-04 - 12s/epoch - 147us/sample
Epoch 69/90
84077/84077 - 12s - loss: 2.6405e-04 - val_loss: 2.5627e-04 - 12s/epoch - 147us/sample
Epoch 70/90
84077/84077 - 12s - loss: 2.6168e-04 - val_loss: 2.5419e-04 - 12s/epoch - 146us/sample
Epoch 71/90
84077/84077 - 12s - loss: 2.6209e-04 - val_loss: 2.5445e-04 - 12s/epoch - 146us/sample
Epoch 72/90
84077/84077 - 12s - loss: 2.6220e-04 - val_loss: 2.5032e-04 - 12s/epoch - 146us/sample
Epoch 73/90
84077/84077 - 12s - loss: 2.6253e-04 - val_loss: 2.5017e-04 - 12s/epoch - 148us/sample
Epoch 74/90
84077/84077 - 12s - loss: 2.5828e-04 - val_loss: 2.4940e-04 - 12s/epoch - 147us/sample
Epoch 75/90
84077/84077 - 12s - loss: 2.6057e-04 - val_loss: 2.4816e-04 - 12s/epoch - 146us/sample
Epoch 76/90
84077/84077 - 12s - loss: 2.5820e-04 - val_loss: 2.4643e-04 - 12s/epoch - 147us/sample
Epoch 77/90
84077/84077 - 12s - loss: 2.6079e-04 - val_loss: 2.4703e-04 - 12s/epoch - 147us/sample
Epoch 78/90
84077/84077 - 12s - loss: 2.5738e-04 - val_loss: 2.5074e-04 - 12s/epoch - 148us/sample
Epoch 79/90
84077/84077 - 12s - loss: 2.5626e-04 - val_loss: 2.4419e-04 - 12s/epoch - 147us/sample
Epoch 80/90
84077/84077 - 12s - loss: 2.6156e-04 - val_loss: 2.4798e-04 - 12s/epoch - 147us/sample
Epoch 81/90
84077/84077 - 12s - loss: 2.5785e-04 - val_loss: 2.4755e-04 - 12s/epoch - 146us/sample
Epoch 82/90
84077/84077 - 12s - loss: 2.5500e-04 - val_loss: 2.4461e-04 - 12s/epoch - 146us/sample
Epoch 83/90
84077/84077 - 12s - loss: 2.5629e-04 - val_loss: 2.4547e-04 - 12s/epoch - 146us/sample
Epoch 84/90
84077/84077 - 12s - loss: 2.5550e-04 - val_loss: 2.4734e-04 - 12s/epoch - 148us/sample
Epoch 85/90
84077/84077 - 12s - loss: 2.5635e-04 - val_loss: 2.4371e-04 - 12s/epoch - 147us/sample
Epoch 86/90
84077/84077 - 12s - loss: 2.5431e-04 - val_loss: 2.4438e-04 - 12s/epoch - 147us/sample
Epoch 87/90
84077/84077 - 12s - loss: 2.5310e-04 - val_loss: 2.4884e-04 - 12s/epoch - 146us/sample
Epoch 88/90
84077/84077 - 12s - loss: 2.5357e-04 - val_loss: 2.4006e-04 - 12s/epoch - 146us/sample
Epoch 89/90
84077/84077 - 12s - loss: 2.5757e-04 - val_loss: 2.4772e-04 - 12s/epoch - 146us/sample
Epoch 90/90
84077/84077 - 12s - loss: 2.5984e-04 - val_loss: 2.4270e-04 - 12s/epoch - 148us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002426994922784225
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 14:22:42.183985: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_34/outputlayer/BiasAdd' id:43575 op device:{requested: '', assigned: ''} def:{{{node decoder_model_34/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_34/outputlayer/MatMul, decoder_model_34/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.032900702355886415
cosine 0.032514782971600986
MAE: 0.0024264378762232797
RMSE: 0.01169891400397595
r2: 0.8932249731502918
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 32, 90, 0.001, 0.2, 188, 0.0002598440965765543, 0.0002426994922784225, 0.032900702355886415, 0.032514782971600986, 0.0024264378762232797, 0.01169891400397595, 0.8932249731502918, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 80 0.001 8 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_105 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_105 (ReLU)               (None, 1980)         0           ['batch_normalization_105[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-11 14:22:57.768428: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_35/bias/Assign' id:44519 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_35/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_35/bias, bottleneck_zmean_35/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 14:23:39.184025: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_35/mul' id:44866 op device:{requested: '', assigned: ''} def:{{{node loss_35/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_35/mul/x, loss_35/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 49s - loss: 0.0019 - val_loss: 7.6524e-04 - 49s/epoch - 583us/sample
Epoch 2/80
84077/84077 - 40s - loss: 6.0762e-04 - val_loss: 7.6504e-04 - 40s/epoch - 474us/sample
Epoch 3/80
84077/84077 - 40s - loss: 5.1470e-04 - val_loss: 0.0030 - 40s/epoch - 475us/sample
Epoch 4/80
84077/84077 - 40s - loss: 4.9179e-04 - val_loss: 0.0066 - 40s/epoch - 474us/sample
Epoch 5/80
84077/84077 - 40s - loss: 4.7592e-04 - val_loss: 0.0100 - 40s/epoch - 476us/sample
Epoch 6/80
84077/84077 - 40s - loss: 4.6774e-04 - val_loss: 0.0113 - 40s/epoch - 474us/sample
Epoch 7/80
84077/84077 - 40s - loss: 4.6401e-04 - val_loss: 0.0161 - 40s/epoch - 475us/sample
Epoch 8/80
84077/84077 - 40s - loss: 4.6212e-04 - val_loss: 0.0146 - 40s/epoch - 475us/sample
Epoch 9/80
84077/84077 - 40s - loss: 4.5824e-04 - val_loss: 0.0153 - 40s/epoch - 473us/sample
Epoch 10/80
84077/84077 - 40s - loss: 4.5619e-04 - val_loss: 0.0157 - 40s/epoch - 475us/sample
Epoch 11/80
84077/84077 - 40s - loss: 4.5437e-04 - val_loss: 0.0182 - 40s/epoch - 474us/sample
Epoch 12/80
84077/84077 - 40s - loss: 4.5295e-04 - val_loss: 0.0158 - 40s/epoch - 474us/sample
Epoch 13/80
84077/84077 - 40s - loss: 4.4876e-04 - val_loss: 0.0184 - 40s/epoch - 476us/sample
Epoch 14/80
84077/84077 - 40s - loss: 4.4247e-04 - val_loss: 0.0166 - 40s/epoch - 473us/sample
Epoch 15/80
84077/84077 - 40s - loss: 4.3948e-04 - val_loss: 0.0183 - 40s/epoch - 475us/sample
Epoch 16/80
84077/84077 - 40s - loss: 4.3724e-04 - val_loss: 0.0166 - 40s/epoch - 475us/sample
Epoch 17/80
84077/84077 - 40s - loss: 4.3527e-04 - val_loss: 0.0138 - 40s/epoch - 474us/sample
Epoch 18/80
84077/84077 - 40s - loss: 4.3384e-04 - val_loss: 0.0170 - 40s/epoch - 476us/sample
Epoch 19/80
84077/84077 - 40s - loss: 4.3214e-04 - val_loss: 0.0153 - 40s/epoch - 473us/sample
Epoch 20/80
84077/84077 - 40s - loss: 4.3106e-04 - val_loss: 0.0164 - 40s/epoch - 475us/sample
Epoch 21/80
84077/84077 - 40s - loss: 4.3031e-04 - val_loss: 0.0184 - 40s/epoch - 475us/sample
Epoch 22/80
84077/84077 - 40s - loss: 4.2833e-04 - val_loss: 0.0134 - 40s/epoch - 472us/sample
Epoch 23/80
84077/84077 - 40s - loss: 4.2742e-04 - val_loss: 0.0154 - 40s/epoch - 475us/sample
Epoch 24/80
84077/84077 - 40s - loss: 4.2687e-04 - val_loss: 0.0148 - 40s/epoch - 475us/sample
Epoch 25/80
84077/84077 - 40s - loss: 4.2637e-04 - val_loss: 0.0143 - 40s/epoch - 475us/sample
Epoch 26/80
84077/84077 - 40s - loss: 4.2564e-04 - val_loss: 0.0159 - 40s/epoch - 475us/sample
Epoch 27/80
84077/84077 - 40s - loss: 4.2503e-04 - val_loss: 0.0166 - 40s/epoch - 473us/sample
Epoch 28/80
84077/84077 - 40s - loss: 4.2472e-04 - val_loss: 0.0179 - 40s/epoch - 476us/sample
Epoch 29/80
84077/84077 - 40s - loss: 4.2308e-04 - val_loss: 0.0148 - 40s/epoch - 476us/sample
Epoch 30/80
84077/84077 - 41s - loss: 4.2197e-04 - val_loss: 0.0195 - 41s/epoch - 482us/sample
Epoch 31/80
84077/84077 - 40s - loss: 4.2137e-04 - val_loss: 0.0146 - 40s/epoch - 481us/sample
Epoch 32/80
84077/84077 - 41s - loss: 4.2127e-04 - val_loss: 0.0162 - 41s/epoch - 482us/sample
Epoch 33/80
84077/84077 - 40s - loss: 4.2066e-04 - val_loss: 0.0155 - 40s/epoch - 481us/sample
Epoch 34/80
84077/84077 - 40s - loss: 4.2068e-04 - val_loss: 0.0151 - 40s/epoch - 481us/sample
Epoch 35/80
84077/84077 - 41s - loss: 4.1970e-04 - val_loss: 0.0150 - 41s/epoch - 482us/sample
Epoch 36/80
84077/84077 - 40s - loss: 4.1992e-04 - val_loss: 0.0141 - 40s/epoch - 480us/sample
Epoch 37/80
84077/84077 - 40s - loss: 4.1899e-04 - val_loss: 0.0160 - 40s/epoch - 481us/sample
Epoch 38/80
84077/84077 - 40s - loss: 4.1865e-04 - val_loss: 0.0150 - 40s/epoch - 479us/sample
Epoch 39/80
84077/84077 - 41s - loss: 4.1836e-04 - val_loss: 0.0175 - 41s/epoch - 482us/sample
Epoch 40/80
84077/84077 - 40s - loss: 4.1827e-04 - val_loss: 0.0129 - 40s/epoch - 480us/sample
Epoch 41/80
84077/84077 - 40s - loss: 4.1818e-04 - val_loss: 0.0160 - 40s/epoch - 482us/sample
Epoch 42/80
84077/84077 - 40s - loss: 4.1767e-04 - val_loss: 0.0125 - 40s/epoch - 480us/sample
Epoch 43/80
84077/84077 - 41s - loss: 4.1734e-04 - val_loss: 0.0116 - 41s/epoch - 483us/sample
Epoch 44/80
84077/84077 - 40s - loss: 4.1755e-04 - val_loss: 0.0148 - 40s/epoch - 481us/sample
Epoch 45/80
84077/84077 - 40s - loss: 4.1695e-04 - val_loss: 0.0137 - 40s/epoch - 479us/sample
Epoch 46/80
84077/84077 - 41s - loss: 4.1651e-04 - val_loss: 0.0157 - 41s/epoch - 483us/sample
Epoch 47/80
84077/84077 - 40s - loss: 4.1652e-04 - val_loss: 0.0128 - 40s/epoch - 480us/sample
Epoch 48/80
84077/84077 - 41s - loss: 4.1559e-04 - val_loss: 0.0139 - 41s/epoch - 482us/sample
Epoch 49/80
84077/84077 - 40s - loss: 4.1617e-04 - val_loss: 0.0122 - 40s/epoch - 479us/sample
Epoch 50/80
84077/84077 - 41s - loss: 4.1574e-04 - val_loss: 0.0126 - 41s/epoch - 483us/sample
Epoch 51/80
84077/84077 - 40s - loss: 4.1540e-04 - val_loss: 0.0124 - 40s/epoch - 480us/sample
Epoch 52/80
84077/84077 - 40s - loss: 4.1485e-04 - val_loss: 0.0132 - 40s/epoch - 481us/sample
Epoch 53/80
84077/84077 - 40s - loss: 4.1442e-04 - val_loss: 0.0119 - 40s/epoch - 481us/sample
Epoch 54/80
84077/84077 - 41s - loss: 4.1448e-04 - val_loss: 0.0120 - 41s/epoch - 482us/sample
Epoch 55/80
84077/84077 - 40s - loss: 4.1436e-04 - val_loss: 0.0106 - 40s/epoch - 479us/sample
Epoch 56/80
84077/84077 - 41s - loss: 4.1433e-04 - val_loss: 0.0121 - 41s/epoch - 482us/sample
Epoch 57/80
84077/84077 - 41s - loss: 4.1417e-04 - val_loss: 0.0110 - 41s/epoch - 482us/sample
Epoch 58/80
84077/84077 - 40s - loss: 4.1413e-04 - val_loss: 0.0105 - 40s/epoch - 479us/sample
Epoch 59/80
84077/84077 - 40s - loss: 4.1389e-04 - val_loss: 0.0124 - 40s/epoch - 476us/sample
Epoch 60/80
84077/84077 - 40s - loss: 4.1301e-04 - val_loss: 0.0101 - 40s/epoch - 476us/sample
Epoch 61/80
84077/84077 - 40s - loss: 4.1317e-04 - val_loss: 0.0110 - 40s/epoch - 476us/sample
Epoch 62/80
84077/84077 - 40s - loss: 4.1320e-04 - val_loss: 0.0128 - 40s/epoch - 474us/sample
Epoch 63/80
84077/84077 - 40s - loss: 4.1325e-04 - val_loss: 0.0099 - 40s/epoch - 475us/sample
Epoch 64/80
84077/84077 - 40s - loss: 4.1314e-04 - val_loss: 0.0109 - 40s/epoch - 475us/sample
Epoch 65/80
84077/84077 - 40s - loss: 4.1312e-04 - val_loss: 0.0100 - 40s/epoch - 474us/sample
Epoch 66/80
84077/84077 - 40s - loss: 4.1290e-04 - val_loss: 0.0110 - 40s/epoch - 476us/sample
Epoch 67/80
84077/84077 - 40s - loss: 4.1299e-04 - val_loss: 0.0109 - 40s/epoch - 474us/sample
Epoch 68/80
84077/84077 - 40s - loss: 4.1291e-04 - val_loss: 0.0097 - 40s/epoch - 475us/sample
Epoch 69/80
84077/84077 - 40s - loss: 4.1258e-04 - val_loss: 0.0106 - 40s/epoch - 475us/sample
Epoch 70/80
84077/84077 - 40s - loss: 4.1255e-04 - val_loss: 0.0099 - 40s/epoch - 473us/sample
Epoch 71/80
84077/84077 - 40s - loss: 4.1254e-04 - val_loss: 0.0105 - 40s/epoch - 475us/sample
Epoch 72/80
84077/84077 - 40s - loss: 4.1232e-04 - val_loss: 0.0106 - 40s/epoch - 475us/sample
Epoch 73/80
84077/84077 - 40s - loss: 4.1190e-04 - val_loss: 0.0087 - 40s/epoch - 474us/sample
Epoch 74/80
84077/84077 - 40s - loss: 4.1182e-04 - val_loss: 0.0105 - 40s/epoch - 475us/sample
Epoch 75/80
84077/84077 - 40s - loss: 4.1195e-04 - val_loss: 0.0112 - 40s/epoch - 474us/sample
Epoch 76/80
84077/84077 - 40s - loss: 4.1170e-04 - val_loss: 0.0100 - 40s/epoch - 475us/sample
Epoch 77/80
84077/84077 - 40s - loss: 4.1181e-04 - val_loss: 0.0098 - 40s/epoch - 475us/sample
Epoch 78/80
84077/84077 - 40s - loss: 4.1124e-04 - val_loss: 0.0083 - 40s/epoch - 473us/sample
Epoch 79/80
84077/84077 - 40s - loss: 4.1171e-04 - val_loss: 0.0090 - 40s/epoch - 475us/sample
Epoch 80/80
84077/84077 - 40s - loss: 4.1153e-04 - val_loss: 0.0099 - 40s/epoch - 473us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.009864791158685535
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 15:16:32.354653: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_35/outputlayer/BiasAdd' id:44830 op device:{requested: '', assigned: ''} def:{{{node decoder_model_35/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_35/outputlayer/MatMul, decoder_model_35/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.25832809790747147
cosine 0.2549677407363232
MAE: 0.02308724201449101
RMSE: 0.32859366143163765
r2: -83.34884371528766
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 8, 80, 0.001, 0.2, 188, 0.00041153094454991014, 0.009864791158685535, 0.25832809790747147, 0.2549677407363232, 0.02308724201449101, 0.32859366143163765, -83.34884371528766, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 4
Fitness    = 512.1418577388646
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 512.1418577388646.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646, 512.1418577388646]
[1.7999999999999998 80 0.0012000000000000001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_108 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_108 (ReLU)               (None, 1697)         0           ['batch_normalization_108[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-11 15:16:48.439510: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_110/moving_mean/Assign' id:45967 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_110/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_110/moving_mean, batch_normalization_110/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 15:17:03.759508: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_36/mul' id:46144 op device:{requested: '', assigned: ''} def:{{{node loss_36/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_36/mul/x, loss_36/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0066 - val_loss: 0.0019 - 21s/epoch - 251us/sample
Epoch 2/80
84077/84077 - 13s - loss: 0.0020 - val_loss: 0.0011 - 13s/epoch - 149us/sample
Epoch 3/80
84077/84077 - 13s - loss: 0.0013 - val_loss: 0.0019 - 13s/epoch - 150us/sample
Epoch 4/80
84077/84077 - 13s - loss: 8.9326e-04 - val_loss: 7.2064e-04 - 13s/epoch - 149us/sample
Epoch 5/80
84077/84077 - 12s - loss: 7.3557e-04 - val_loss: 7.0081e-04 - 12s/epoch - 148us/sample
Epoch 6/80
84077/84077 - 12s - loss: 6.1873e-04 - val_loss: 5.1927e-04 - 12s/epoch - 148us/sample
Epoch 7/80
84077/84077 - 12s - loss: 5.4189e-04 - val_loss: 4.7848e-04 - 12s/epoch - 148us/sample
Epoch 8/80
84077/84077 - 13s - loss: 4.8998e-04 - val_loss: 4.6590e-04 - 13s/epoch - 149us/sample
Epoch 9/80
84077/84077 - 12s - loss: 4.6568e-04 - val_loss: 4.2687e-04 - 12s/epoch - 149us/sample
Epoch 10/80
84077/84077 - 13s - loss: 4.2560e-04 - val_loss: 3.9602e-04 - 13s/epoch - 150us/sample
Epoch 11/80
84077/84077 - 12s - loss: 4.0156e-04 - val_loss: 3.7923e-04 - 12s/epoch - 149us/sample
Epoch 12/80
84077/84077 - 13s - loss: 3.8461e-04 - val_loss: 3.6509e-04 - 13s/epoch - 149us/sample
Epoch 13/80
84077/84077 - 12s - loss: 3.7028e-04 - val_loss: 3.5013e-04 - 12s/epoch - 149us/sample
Epoch 14/80
84077/84077 - 12s - loss: 3.5522e-04 - val_loss: 3.4984e-04 - 12s/epoch - 148us/sample
Epoch 15/80
84077/84077 - 12s - loss: 3.4487e-04 - val_loss: 3.3994e-04 - 12s/epoch - 149us/sample
Epoch 16/80
84077/84077 - 13s - loss: 3.3603e-04 - val_loss: 3.3206e-04 - 13s/epoch - 150us/sample
Epoch 17/80
84077/84077 - 12s - loss: 3.2845e-04 - val_loss: 3.3009e-04 - 12s/epoch - 148us/sample
Epoch 18/80
84077/84077 - 13s - loss: 3.2099e-04 - val_loss: 3.1746e-04 - 13s/epoch - 149us/sample
Epoch 19/80
84077/84077 - 12s - loss: 3.1581e-04 - val_loss: 3.1623e-04 - 12s/epoch - 148us/sample
Epoch 20/80
84077/84077 - 13s - loss: 3.0971e-04 - val_loss: 3.2492e-04 - 13s/epoch - 149us/sample
Epoch 21/80
84077/84077 - 13s - loss: 3.0460e-04 - val_loss: 3.1939e-04 - 13s/epoch - 150us/sample
Epoch 22/80
84077/84077 - 13s - loss: 3.0383e-04 - val_loss: 3.1036e-04 - 13s/epoch - 149us/sample
Epoch 23/80
84077/84077 - 12s - loss: 2.9958e-04 - val_loss: 3.1588e-04 - 12s/epoch - 149us/sample
Epoch 24/80
84077/84077 - 12s - loss: 2.9469e-04 - val_loss: 2.9901e-04 - 12s/epoch - 149us/sample
Epoch 25/80
84077/84077 - 12s - loss: 2.9139e-04 - val_loss: 2.9172e-04 - 12s/epoch - 148us/sample
Epoch 26/80
84077/84077 - 12s - loss: 2.8768e-04 - val_loss: 2.8552e-04 - 12s/epoch - 149us/sample
Epoch 27/80
84077/84077 - 13s - loss: 2.8572e-04 - val_loss: 2.8421e-04 - 13s/epoch - 149us/sample
Epoch 28/80
84077/84077 - 13s - loss: 2.8278e-04 - val_loss: 2.7896e-04 - 13s/epoch - 149us/sample
Epoch 29/80
84077/84077 - 12s - loss: 2.8037e-04 - val_loss: 2.8463e-04 - 12s/epoch - 149us/sample
Epoch 30/80
84077/84077 - 13s - loss: 2.7817e-04 - val_loss: 2.8030e-04 - 13s/epoch - 149us/sample
Epoch 31/80
84077/84077 - 12s - loss: 2.7718e-04 - val_loss: 2.9245e-04 - 12s/epoch - 148us/sample
Epoch 32/80
84077/84077 - 13s - loss: 2.7417e-04 - val_loss: 2.8291e-04 - 13s/epoch - 149us/sample
Epoch 33/80
84077/84077 - 13s - loss: 2.7263e-04 - val_loss: 2.7845e-04 - 13s/epoch - 150us/sample
Epoch 34/80
84077/84077 - 13s - loss: 2.7200e-04 - val_loss: 2.6750e-04 - 13s/epoch - 149us/sample
Epoch 35/80
84077/84077 - 12s - loss: 2.7118e-04 - val_loss: 2.7423e-04 - 12s/epoch - 149us/sample
Epoch 36/80
84077/84077 - 12s - loss: 2.6992e-04 - val_loss: 2.7081e-04 - 12s/epoch - 149us/sample
Epoch 37/80
84077/84077 - 12s - loss: 2.6716e-04 - val_loss: 2.7469e-04 - 12s/epoch - 148us/sample
Epoch 38/80
84077/84077 - 13s - loss: 2.6637e-04 - val_loss: 2.6732e-04 - 13s/epoch - 149us/sample
Epoch 39/80
84077/84077 - 13s - loss: 2.6451e-04 - val_loss: 2.6753e-04 - 13s/epoch - 150us/sample
Epoch 40/80
84077/84077 - 13s - loss: 2.6460e-04 - val_loss: 2.6736e-04 - 13s/epoch - 149us/sample
Epoch 41/80
84077/84077 - 12s - loss: 2.6251e-04 - val_loss: 2.6842e-04 - 12s/epoch - 148us/sample
Epoch 42/80
84077/84077 - 13s - loss: 2.6132e-04 - val_loss: 2.6364e-04 - 13s/epoch - 149us/sample
Epoch 43/80
84077/84077 - 13s - loss: 2.5947e-04 - val_loss: 2.5793e-04 - 13s/epoch - 149us/sample
Epoch 44/80
84077/84077 - 13s - loss: 2.6119e-04 - val_loss: 2.6387e-04 - 13s/epoch - 150us/sample
Epoch 45/80
84077/84077 - 13s - loss: 2.5816e-04 - val_loss: 2.6059e-04 - 13s/epoch - 149us/sample
Epoch 46/80
84077/84077 - 13s - loss: 2.5729e-04 - val_loss: 2.5942e-04 - 13s/epoch - 149us/sample
Epoch 47/80
84077/84077 - 12s - loss: 2.5665e-04 - val_loss: 2.5564e-04 - 12s/epoch - 149us/sample
Epoch 48/80
84077/84077 - 13s - loss: 2.5527e-04 - val_loss: 2.5857e-04 - 13s/epoch - 149us/sample
Epoch 49/80
84077/84077 - 12s - loss: 2.5351e-04 - val_loss: 2.5563e-04 - 12s/epoch - 148us/sample
Epoch 50/80
84077/84077 - 13s - loss: 2.5331e-04 - val_loss: 2.5798e-04 - 13s/epoch - 150us/sample
Epoch 51/80
84077/84077 - 12s - loss: 2.5257e-04 - val_loss: 2.4782e-04 - 12s/epoch - 149us/sample
Epoch 52/80
84077/84077 - 12s - loss: 2.5145e-04 - val_loss: 2.5931e-04 - 12s/epoch - 148us/sample
Epoch 53/80
84077/84077 - 12s - loss: 2.5194e-04 - val_loss: 2.5635e-04 - 12s/epoch - 148us/sample
Epoch 54/80
84077/84077 - 12s - loss: 2.5149e-04 - val_loss: 2.5460e-04 - 12s/epoch - 149us/sample
Epoch 55/80
84077/84077 - 12s - loss: 2.5030e-04 - val_loss: 2.5750e-04 - 12s/epoch - 149us/sample
Epoch 56/80
84077/84077 - 13s - loss: 2.5064e-04 - val_loss: 2.5059e-04 - 13s/epoch - 150us/sample
Epoch 57/80
84077/84077 - 13s - loss: 2.5039e-04 - val_loss: 2.5169e-04 - 13s/epoch - 149us/sample
Epoch 58/80
84077/84077 - 13s - loss: 2.4658e-04 - val_loss: 2.4955e-04 - 13s/epoch - 149us/sample
Epoch 59/80
84077/84077 - 12s - loss: 2.4635e-04 - val_loss: 2.4764e-04 - 12s/epoch - 148us/sample
Epoch 60/80
84077/84077 - 13s - loss: 2.4766e-04 - val_loss: 2.6034e-04 - 13s/epoch - 149us/sample
Epoch 61/80
84077/84077 - 13s - loss: 2.4987e-04 - val_loss: 2.4903e-04 - 13s/epoch - 149us/sample
Epoch 62/80
84077/84077 - 13s - loss: 2.4737e-04 - val_loss: 2.4810e-04 - 13s/epoch - 150us/sample
Epoch 63/80
84077/84077 - 13s - loss: 2.4569e-04 - val_loss: 2.4544e-04 - 13s/epoch - 149us/sample
Epoch 64/80
84077/84077 - 13s - loss: 2.4428e-04 - val_loss: 2.4694e-04 - 13s/epoch - 149us/sample
Epoch 65/80
84077/84077 - 12s - loss: 2.4443e-04 - val_loss: 2.4842e-04 - 12s/epoch - 148us/sample
Epoch 66/80
84077/84077 - 13s - loss: 2.4583e-04 - val_loss: 2.5264e-04 - 13s/epoch - 149us/sample
Epoch 67/80
84077/84077 - 13s - loss: 2.4332e-04 - val_loss: 2.4436e-04 - 13s/epoch - 149us/sample
Epoch 68/80
84077/84077 - 13s - loss: 2.4207e-04 - val_loss: 2.4497e-04 - 13s/epoch - 149us/sample
Epoch 69/80
84077/84077 - 12s - loss: 2.4130e-04 - val_loss: 2.4411e-04 - 12s/epoch - 149us/sample
Epoch 70/80
84077/84077 - 12s - loss: 2.4033e-04 - val_loss: 2.4651e-04 - 12s/epoch - 149us/sample
Epoch 71/80
84077/84077 - 12s - loss: 2.4053e-04 - val_loss: 2.4246e-04 - 12s/epoch - 148us/sample
Epoch 72/80
84077/84077 - 13s - loss: 2.4079e-04 - val_loss: 2.4567e-04 - 13s/epoch - 149us/sample
Epoch 73/80
84077/84077 - 13s - loss: 2.3991e-04 - val_loss: 2.4106e-04 - 13s/epoch - 150us/sample
Epoch 74/80
84077/84077 - 13s - loss: 2.3973e-04 - val_loss: 2.4058e-04 - 13s/epoch - 149us/sample
Epoch 75/80
84077/84077 - 12s - loss: 2.3941e-04 - val_loss: 2.5037e-04 - 12s/epoch - 148us/sample
Epoch 76/80
84077/84077 - 12s - loss: 2.3912e-04 - val_loss: 2.3941e-04 - 12s/epoch - 149us/sample
Epoch 77/80
84077/84077 - 12s - loss: 2.3840e-04 - val_loss: 2.4442e-04 - 12s/epoch - 149us/sample
Epoch 78/80
84077/84077 - 13s - loss: 2.3785e-04 - val_loss: 2.3779e-04 - 13s/epoch - 150us/sample
Epoch 79/80
84077/84077 - 13s - loss: 2.3722e-04 - val_loss: 2.3978e-04 - 13s/epoch - 149us/sample
Epoch 80/80
84077/84077 - 13s - loss: 2.3896e-04 - val_loss: 2.3925e-04 - 13s/epoch - 149us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00023924976028983
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 15:33:36.032820: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_36/outputlayer/BiasAdd' id:46115 op device:{requested: '', assigned: ''} def:{{{node decoder_model_36/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_36/outputlayer/MatMul, decoder_model_36/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.029529733036464684
cosine 0.02917789618949032
MAE: 0.0022774228656345405
RMSE: 0.011528035107814213
r2: 0.8962581645125628
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 80, 0.0012000000000000001, 0.2, 188, 0.0002389608811349377, 0.00023924976028983, 0.029529733036464684, 0.02917789618949032, 0.0022774228656345405, 0.011528035107814213, 0.8962581645125628, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.001 16 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_111 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_111 (ReLU)               (None, 1697)         0           ['batch_normalization_111[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 15:33:52.485589: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_113/moving_mean/Assign' id:47222 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_113/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_113/moving_mean, batch_normalization_113/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 15:34:16.579780: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_37/mul' id:47399 op device:{requested: '', assigned: ''} def:{{{node loss_37/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_37/mul/x, loss_37/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 31s - loss: 0.0052 - val_loss: 4.2538 - 31s/epoch - 366us/sample
Epoch 2/90
84077/84077 - 22s - loss: 0.0012 - val_loss: 9.7630e-04 - 22s/epoch - 257us/sample
Epoch 3/90
84077/84077 - 22s - loss: 8.5446e-04 - val_loss: 6.8383e-04 - 22s/epoch - 257us/sample
Epoch 4/90
84077/84077 - 22s - loss: 6.9078e-04 - val_loss: 6.3987e-04 - 22s/epoch - 256us/sample
Epoch 5/90
84077/84077 - 22s - loss: 6.2139e-04 - val_loss: 5.9654e-04 - 22s/epoch - 258us/sample
Epoch 6/90
84077/84077 - 22s - loss: 5.8309e-04 - val_loss: 5.9207e-04 - 22s/epoch - 257us/sample
Epoch 7/90
84077/84077 - 22s - loss: 5.5697e-04 - val_loss: 5.5776e-04 - 22s/epoch - 257us/sample
Epoch 8/90
84077/84077 - 22s - loss: 5.2811e-04 - val_loss: 5.2370e-04 - 22s/epoch - 258us/sample
Epoch 9/90
84077/84077 - 22s - loss: 5.0524e-04 - val_loss: 5.2196e-04 - 22s/epoch - 256us/sample
Epoch 10/90
84077/84077 - 22s - loss: 4.9037e-04 - val_loss: 5.0616e-04 - 22s/epoch - 256us/sample
Epoch 11/90
84077/84077 - 22s - loss: 4.7507e-04 - val_loss: 4.9432e-04 - 22s/epoch - 257us/sample
Epoch 12/90
84077/84077 - 22s - loss: 4.5900e-04 - val_loss: 5.1215e-04 - 22s/epoch - 258us/sample
Epoch 13/90
84077/84077 - 22s - loss: 4.4614e-04 - val_loss: 4.6664e-04 - 22s/epoch - 256us/sample
Epoch 14/90
84077/84077 - 22s - loss: 4.3943e-04 - val_loss: 4.5456e-04 - 22s/epoch - 256us/sample
Epoch 15/90
84077/84077 - 22s - loss: 4.2181e-04 - val_loss: 4.4291e-04 - 22s/epoch - 258us/sample
Epoch 16/90
84077/84077 - 22s - loss: 4.1657e-04 - val_loss: 4.3465e-04 - 22s/epoch - 256us/sample
Epoch 17/90
84077/84077 - 22s - loss: 4.0414e-04 - val_loss: 4.3239e-04 - 22s/epoch - 257us/sample
Epoch 18/90
84077/84077 - 22s - loss: 3.9536e-04 - val_loss: 4.0559e-04 - 22s/epoch - 256us/sample
Epoch 19/90
84077/84077 - 22s - loss: 3.8640e-04 - val_loss: 3.7851e-04 - 22s/epoch - 258us/sample
Epoch 20/90
84077/84077 - 22s - loss: 3.7696e-04 - val_loss: 3.6391e-04 - 22s/epoch - 256us/sample
Epoch 21/90
84077/84077 - 22s - loss: 3.7431e-04 - val_loss: 3.5988e-04 - 22s/epoch - 256us/sample
Epoch 22/90
84077/84077 - 22s - loss: 3.6342e-04 - val_loss: 3.5470e-04 - 22s/epoch - 258us/sample
Epoch 23/90
84077/84077 - 22s - loss: 3.5682e-04 - val_loss: 3.4748e-04 - 22s/epoch - 257us/sample
Epoch 24/90
84077/84077 - 22s - loss: 3.5323e-04 - val_loss: 3.4506e-04 - 22s/epoch - 257us/sample
Epoch 25/90
84077/84077 - 22s - loss: 3.5092e-04 - val_loss: 3.4230e-04 - 22s/epoch - 256us/sample
Epoch 26/90
84077/84077 - 22s - loss: 3.4528e-04 - val_loss: 3.5351e-04 - 22s/epoch - 258us/sample
Epoch 27/90
84077/84077 - 22s - loss: 3.4331e-04 - val_loss: 3.3080e-04 - 22s/epoch - 256us/sample
Epoch 28/90
84077/84077 - 22s - loss: 3.3688e-04 - val_loss: 3.2932e-04 - 22s/epoch - 257us/sample
Epoch 29/90
84077/84077 - 22s - loss: 3.3536e-04 - val_loss: 3.2917e-04 - 22s/epoch - 258us/sample
Epoch 30/90
84077/84077 - 22s - loss: 3.3143e-04 - val_loss: 3.2487e-04 - 22s/epoch - 257us/sample
Epoch 31/90
84077/84077 - 22s - loss: 3.2811e-04 - val_loss: 3.2164e-04 - 22s/epoch - 257us/sample
Epoch 32/90
84077/84077 - 22s - loss: 3.2492e-04 - val_loss: 3.1006e-04 - 22s/epoch - 256us/sample
Epoch 33/90
84077/84077 - 22s - loss: 3.2626e-04 - val_loss: 3.0892e-04 - 22s/epoch - 258us/sample
Epoch 34/90
84077/84077 - 22s - loss: 3.2285e-04 - val_loss: 3.1037e-04 - 22s/epoch - 257us/sample
Epoch 35/90
84077/84077 - 22s - loss: 3.1654e-04 - val_loss: 3.0398e-04 - 22s/epoch - 257us/sample
Epoch 36/90
84077/84077 - 22s - loss: 3.1641e-04 - val_loss: 3.1830e-04 - 22s/epoch - 258us/sample
Epoch 37/90
84077/84077 - 22s - loss: 3.1426e-04 - val_loss: 2.9863e-04 - 22s/epoch - 257us/sample
Epoch 38/90
84077/84077 - 21s - loss: 3.1114e-04 - val_loss: 3.0568e-04 - 21s/epoch - 256us/sample
Epoch 39/90
84077/84077 - 22s - loss: 3.1499e-04 - val_loss: 2.9772e-04 - 22s/epoch - 257us/sample
Epoch 40/90
84077/84077 - 22s - loss: 3.1065e-04 - val_loss: 2.9955e-04 - 22s/epoch - 258us/sample
Epoch 41/90
84077/84077 - 22s - loss: 3.0946e-04 - val_loss: 2.9031e-04 - 22s/epoch - 256us/sample
Epoch 42/90
84077/84077 - 22s - loss: 3.0568e-04 - val_loss: 2.8846e-04 - 22s/epoch - 256us/sample
Epoch 43/90
84077/84077 - 22s - loss: 3.0118e-04 - val_loss: 2.8369e-04 - 22s/epoch - 259us/sample
Epoch 44/90
84077/84077 - 22s - loss: 3.0117e-04 - val_loss: 2.9119e-04 - 22s/epoch - 257us/sample
Epoch 45/90
84077/84077 - 22s - loss: 3.0297e-04 - val_loss: 2.8870e-04 - 22s/epoch - 256us/sample
Epoch 46/90
84077/84077 - 22s - loss: 3.0086e-04 - val_loss: 2.8440e-04 - 22s/epoch - 258us/sample
Epoch 47/90
84077/84077 - 22s - loss: 3.0093e-04 - val_loss: 2.8351e-04 - 22s/epoch - 257us/sample
Epoch 48/90
84077/84077 - 22s - loss: 2.9537e-04 - val_loss: 2.8784e-04 - 22s/epoch - 257us/sample
Epoch 49/90
84077/84077 - 22s - loss: 2.9463e-04 - val_loss: 2.8541e-04 - 22s/epoch - 257us/sample
Epoch 50/90
84077/84077 - 22s - loss: 2.9581e-04 - val_loss: 2.7968e-04 - 22s/epoch - 258us/sample
Epoch 51/90
84077/84077 - 22s - loss: 2.9321e-04 - val_loss: 2.8626e-04 - 22s/epoch - 256us/sample
Epoch 52/90
84077/84077 - 22s - loss: 2.9667e-04 - val_loss: 2.7927e-04 - 22s/epoch - 256us/sample
Epoch 53/90
84077/84077 - 22s - loss: 2.9426e-04 - val_loss: 2.7994e-04 - 22s/epoch - 257us/sample
Epoch 54/90
84077/84077 - 22s - loss: 2.8973e-04 - val_loss: 2.8120e-04 - 22s/epoch - 257us/sample
Epoch 55/90
84077/84077 - 22s - loss: 2.9230e-04 - val_loss: 2.7618e-04 - 22s/epoch - 257us/sample
Epoch 56/90
84077/84077 - 22s - loss: 2.9418e-04 - val_loss: 2.7652e-04 - 22s/epoch - 257us/sample
Epoch 57/90
84077/84077 - 22s - loss: 2.9124e-04 - val_loss: 2.7545e-04 - 22s/epoch - 257us/sample
Epoch 58/90
84077/84077 - 22s - loss: 2.9156e-04 - val_loss: 2.7674e-04 - 22s/epoch - 257us/sample
Epoch 59/90
84077/84077 - 22s - loss: 2.8594e-04 - val_loss: 2.6864e-04 - 22s/epoch - 256us/sample
Epoch 60/90
84077/84077 - 22s - loss: 2.8606e-04 - val_loss: 2.7103e-04 - 22s/epoch - 258us/sample
Epoch 61/90
84077/84077 - 22s - loss: 2.8623e-04 - val_loss: 2.6917e-04 - 22s/epoch - 256us/sample
Epoch 62/90
84077/84077 - 22s - loss: 2.8879e-04 - val_loss: 2.7261e-04 - 22s/epoch - 256us/sample
Epoch 63/90
84077/84077 - 22s - loss: 2.9071e-04 - val_loss: 2.7123e-04 - 22s/epoch - 258us/sample
Epoch 64/90
84077/84077 - 22s - loss: 2.8967e-04 - val_loss: 2.7757e-04 - 22s/epoch - 256us/sample
Epoch 65/90
84077/84077 - 22s - loss: 2.8689e-04 - val_loss: 2.7271e-04 - 22s/epoch - 256us/sample
Epoch 66/90
84077/84077 - 21s - loss: 2.8459e-04 - val_loss: 2.6858e-04 - 21s/epoch - 256us/sample
Epoch 67/90
84077/84077 - 22s - loss: 2.8254e-04 - val_loss: 2.6892e-04 - 22s/epoch - 258us/sample
Epoch 68/90
84077/84077 - 22s - loss: 2.8383e-04 - val_loss: 2.6985e-04 - 22s/epoch - 257us/sample
Epoch 69/90
84077/84077 - 22s - loss: 2.8516e-04 - val_loss: 2.7080e-04 - 22s/epoch - 256us/sample
Epoch 70/90
84077/84077 - 22s - loss: 2.8194e-04 - val_loss: 2.7398e-04 - 22s/epoch - 258us/sample
Epoch 71/90
84077/84077 - 22s - loss: 2.8175e-04 - val_loss: 2.6625e-04 - 22s/epoch - 257us/sample
Epoch 72/90
84077/84077 - 22s - loss: 2.7707e-04 - val_loss: 2.6786e-04 - 22s/epoch - 257us/sample
Epoch 73/90
84077/84077 - 22s - loss: 2.7807e-04 - val_loss: 2.5859e-04 - 22s/epoch - 258us/sample
Epoch 74/90
84077/84077 - 22s - loss: 2.7566e-04 - val_loss: 2.6297e-04 - 22s/epoch - 257us/sample
Epoch 75/90
84077/84077 - 22s - loss: 2.7566e-04 - val_loss: 2.5976e-04 - 22s/epoch - 256us/sample
Epoch 76/90
84077/84077 - 22s - loss: 2.7641e-04 - val_loss: 2.6212e-04 - 22s/epoch - 257us/sample
Epoch 77/90
84077/84077 - 22s - loss: 2.8045e-04 - val_loss: 2.6143e-04 - 22s/epoch - 258us/sample
Epoch 78/90
84077/84077 - 22s - loss: 2.7334e-04 - val_loss: 2.5679e-04 - 22s/epoch - 257us/sample
Epoch 79/90
84077/84077 - 22s - loss: 2.7578e-04 - val_loss: 2.5921e-04 - 22s/epoch - 256us/sample
Epoch 80/90
84077/84077 - 22s - loss: 2.7384e-04 - val_loss: 2.6764e-04 - 22s/epoch - 258us/sample
Epoch 81/90
84077/84077 - 22s - loss: 2.7744e-04 - val_loss: 2.6073e-04 - 22s/epoch - 256us/sample
Epoch 82/90
84077/84077 - 22s - loss: 2.7390e-04 - val_loss: 2.5891e-04 - 22s/epoch - 256us/sample
Epoch 83/90
84077/84077 - 22s - loss: 2.7554e-04 - val_loss: 2.5730e-04 - 22s/epoch - 258us/sample
Epoch 84/90
84077/84077 - 22s - loss: 2.7517e-04 - val_loss: 2.5940e-04 - 22s/epoch - 257us/sample
Epoch 85/90
84077/84077 - 22s - loss: 2.7861e-04 - val_loss: 2.6468e-04 - 22s/epoch - 257us/sample
Epoch 86/90
84077/84077 - 22s - loss: 2.7089e-04 - val_loss: 2.5737e-04 - 22s/epoch - 259us/sample
Epoch 87/90
84077/84077 - 22s - loss: 2.7193e-04 - val_loss: 2.6004e-04 - 22s/epoch - 257us/sample
Epoch 88/90
84077/84077 - 22s - loss: 2.7035e-04 - val_loss: 2.6166e-04 - 22s/epoch - 256us/sample
Epoch 89/90
84077/84077 - 22s - loss: 2.7199e-04 - val_loss: 2.5582e-04 - 22s/epoch - 257us/sample
Epoch 90/90
84077/84077 - 22s - loss: 2.7166e-04 - val_loss: 2.6212e-04 - 22s/epoch - 257us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002621155487913503
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 16:06:23.419950: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_37/outputlayer/BiasAdd' id:47370 op device:{requested: '', assigned: ''} def:{{{node decoder_model_37/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_37/outputlayer/MatMul, decoder_model_37/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.037389691806428885
cosine 0.03693282726538193
MAE: 0.002533794313757914
RMSE: 0.012161637726371558
r2: 0.8845203906932778
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 16, 90, 0.001, 0.2, 188, 0.0002716592360566594, 0.0002621155487913503, 0.037389691806428885, 0.03693282726538193, 0.002533794313757914, 0.012161637726371558, 0.8845203906932778, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0012000000000000001 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_114 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_114 (ReLU)               (None, 1791)         0           ['batch_normalization_114[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 16:06:40.799809: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_116/gamma/Assign' id:48463 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_116/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_116/gamma, batch_normalization_116/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 16:06:57.120571: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_38/mul' id:48654 op device:{requested: '', assigned: ''} def:{{{node loss_38/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_38/mul/x, loss_38/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0063 - val_loss: 0.0018 - 23s/epoch - 269us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.0022 - val_loss: 0.0011 - 13s/epoch - 154us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0019 - val_loss: 9.2083e-04 - 13s/epoch - 154us/sample
Epoch 4/85
84077/84077 - 13s - loss: 0.0013 - val_loss: 0.0016 - 13s/epoch - 154us/sample
Epoch 5/85
84077/84077 - 13s - loss: 9.6267e-04 - val_loss: 6.9235e-04 - 13s/epoch - 154us/sample
Epoch 6/85
84077/84077 - 13s - loss: 0.0011 - val_loss: 7.0901e-04 - 13s/epoch - 155us/sample
Epoch 7/85
84077/84077 - 13s - loss: 7.6659e-04 - val_loss: 6.2302e-04 - 13s/epoch - 156us/sample
Epoch 8/85
84077/84077 - 13s - loss: 6.5055e-04 - val_loss: 5.2574e-04 - 13s/epoch - 155us/sample
Epoch 9/85
84077/84077 - 13s - loss: 5.4667e-04 - val_loss: 4.8320e-04 - 13s/epoch - 154us/sample
Epoch 10/85
84077/84077 - 13s - loss: 5.3727e-04 - val_loss: 4.8111e-04 - 13s/epoch - 154us/sample
Epoch 11/85
84077/84077 - 13s - loss: 4.9788e-04 - val_loss: 4.3322e-04 - 13s/epoch - 155us/sample
Epoch 12/85
84077/84077 - 13s - loss: 4.5195e-04 - val_loss: 4.2197e-04 - 13s/epoch - 156us/sample
Epoch 13/85
84077/84077 - 13s - loss: 4.3247e-04 - val_loss: 4.0440e-04 - 13s/epoch - 154us/sample
Epoch 14/85
84077/84077 - 13s - loss: 4.1705e-04 - val_loss: 4.0011e-04 - 13s/epoch - 154us/sample
Epoch 15/85
84077/84077 - 13s - loss: 4.0525e-04 - val_loss: 3.8846e-04 - 13s/epoch - 154us/sample
Epoch 16/85
84077/84077 - 13s - loss: 3.9510e-04 - val_loss: 3.7637e-04 - 13s/epoch - 154us/sample
Epoch 17/85
84077/84077 - 13s - loss: 3.8569e-04 - val_loss: 3.6839e-04 - 13s/epoch - 155us/sample
Epoch 18/85
84077/84077 - 13s - loss: 3.7844e-04 - val_loss: 3.6475e-04 - 13s/epoch - 155us/sample
Epoch 19/85
84077/84077 - 13s - loss: 3.7277e-04 - val_loss: 3.6357e-04 - 13s/epoch - 155us/sample
Epoch 20/85
84077/84077 - 13s - loss: 3.6462e-04 - val_loss: 3.5289e-04 - 13s/epoch - 154us/sample
Epoch 21/85
84077/84077 - 13s - loss: 3.5733e-04 - val_loss: 3.5235e-04 - 13s/epoch - 154us/sample
Epoch 22/85
84077/84077 - 13s - loss: 3.5291e-04 - val_loss: 3.4631e-04 - 13s/epoch - 155us/sample
Epoch 23/85
84077/84077 - 13s - loss: 3.4826e-04 - val_loss: 3.4268e-04 - 13s/epoch - 156us/sample
Epoch 24/85
84077/84077 - 13s - loss: 3.4407e-04 - val_loss: 3.3794e-04 - 13s/epoch - 155us/sample
Epoch 25/85
84077/84077 - 13s - loss: 3.3793e-04 - val_loss: 3.3643e-04 - 13s/epoch - 155us/sample
Epoch 26/85
84077/84077 - 13s - loss: 3.3425e-04 - val_loss: 3.3442e-04 - 13s/epoch - 154us/sample
Epoch 27/85
84077/84077 - 13s - loss: 3.3121e-04 - val_loss: 3.3067e-04 - 13s/epoch - 154us/sample
Epoch 28/85
84077/84077 - 13s - loss: 3.2802e-04 - val_loss: 3.2442e-04 - 13s/epoch - 155us/sample
Epoch 29/85
84077/84077 - 13s - loss: 3.2481e-04 - val_loss: 3.2651e-04 - 13s/epoch - 155us/sample
Epoch 30/85
84077/84077 - 13s - loss: 3.2334e-04 - val_loss: 3.2216e-04 - 13s/epoch - 154us/sample
Epoch 31/85
84077/84077 - 13s - loss: 3.2061e-04 - val_loss: 3.1940e-04 - 13s/epoch - 154us/sample
Epoch 32/85
84077/84077 - 13s - loss: 3.1788e-04 - val_loss: 3.1686e-04 - 13s/epoch - 155us/sample
Epoch 33/85
84077/84077 - 13s - loss: 3.1564e-04 - val_loss: 3.1347e-04 - 13s/epoch - 156us/sample
Epoch 34/85
84077/84077 - 13s - loss: 3.1422e-04 - val_loss: 3.1437e-04 - 13s/epoch - 155us/sample
Epoch 35/85
84077/84077 - 13s - loss: 3.1222e-04 - val_loss: 3.1700e-04 - 13s/epoch - 155us/sample
Epoch 36/85
84077/84077 - 13s - loss: 3.1018e-04 - val_loss: 3.1208e-04 - 13s/epoch - 154us/sample
Epoch 37/85
84077/84077 - 13s - loss: 3.0775e-04 - val_loss: 3.0549e-04 - 13s/epoch - 154us/sample
Epoch 38/85
84077/84077 - 13s - loss: 3.0599e-04 - val_loss: 3.1280e-04 - 13s/epoch - 155us/sample
Epoch 39/85
84077/84077 - 13s - loss: 3.0468e-04 - val_loss: 3.0624e-04 - 13s/epoch - 155us/sample
Epoch 40/85
84077/84077 - 13s - loss: 3.0294e-04 - val_loss: 3.0209e-04 - 13s/epoch - 154us/sample
Epoch 41/85
84077/84077 - 13s - loss: 3.0200e-04 - val_loss: 3.0345e-04 - 13s/epoch - 154us/sample
Epoch 42/85
84077/84077 - 13s - loss: 3.0049e-04 - val_loss: 3.0458e-04 - 13s/epoch - 154us/sample
Epoch 43/85
84077/84077 - 13s - loss: 2.9956e-04 - val_loss: 3.0574e-04 - 13s/epoch - 156us/sample
Epoch 44/85
84077/84077 - 13s - loss: 2.9818e-04 - val_loss: 3.0437e-04 - 13s/epoch - 154us/sample
Epoch 45/85
84077/84077 - 13s - loss: 2.9684e-04 - val_loss: 3.0591e-04 - 13s/epoch - 153us/sample
Epoch 46/85
84077/84077 - 13s - loss: 2.9445e-04 - val_loss: 2.9944e-04 - 13s/epoch - 153us/sample
Epoch 47/85
84077/84077 - 13s - loss: 2.9404e-04 - val_loss: 2.9205e-04 - 13s/epoch - 153us/sample
Epoch 48/85
84077/84077 - 13s - loss: 2.9296e-04 - val_loss: 2.9898e-04 - 13s/epoch - 153us/sample
Epoch 49/85
84077/84077 - 13s - loss: 2.9088e-04 - val_loss: 2.9579e-04 - 13s/epoch - 153us/sample
Epoch 50/85
84077/84077 - 13s - loss: 2.9201e-04 - val_loss: 2.9522e-04 - 13s/epoch - 152us/sample
Epoch 51/85
84077/84077 - 13s - loss: 2.9068e-04 - val_loss: 2.9641e-04 - 13s/epoch - 153us/sample
Epoch 52/85
84077/84077 - 13s - loss: 2.8915e-04 - val_loss: 2.9541e-04 - 13s/epoch - 152us/sample
Epoch 53/85
84077/84077 - 13s - loss: 2.8874e-04 - val_loss: 2.9378e-04 - 13s/epoch - 156us/sample
Epoch 54/85
84077/84077 - 13s - loss: 2.8779e-04 - val_loss: 2.9646e-04 - 13s/epoch - 155us/sample
Epoch 55/85
84077/84077 - 13s - loss: 2.8850e-04 - val_loss: 2.9297e-04 - 13s/epoch - 152us/sample
Epoch 56/85
84077/84077 - 13s - loss: 2.8643e-04 - val_loss: 2.9253e-04 - 13s/epoch - 152us/sample
Epoch 57/85
84077/84077 - 13s - loss: 2.8590e-04 - val_loss: 2.8845e-04 - 13s/epoch - 152us/sample
Epoch 58/85
84077/84077 - 13s - loss: 2.8447e-04 - val_loss: 2.8999e-04 - 13s/epoch - 153us/sample
Epoch 59/85
84077/84077 - 13s - loss: 2.8400e-04 - val_loss: 2.8559e-04 - 13s/epoch - 153us/sample
Epoch 60/85
84077/84077 - 13s - loss: 2.8355e-04 - val_loss: 2.8549e-04 - 13s/epoch - 152us/sample
Epoch 61/85
84077/84077 - 13s - loss: 2.8323e-04 - val_loss: 2.8776e-04 - 13s/epoch - 152us/sample
Epoch 62/85
84077/84077 - 13s - loss: 2.8230e-04 - val_loss: 2.8632e-04 - 13s/epoch - 152us/sample
Epoch 63/85
84077/84077 - 13s - loss: 2.8227e-04 - val_loss: 2.8510e-04 - 13s/epoch - 152us/sample
Epoch 64/85
84077/84077 - 13s - loss: 2.8159e-04 - val_loss: 2.8391e-04 - 13s/epoch - 153us/sample
Epoch 65/85
84077/84077 - 13s - loss: 2.8112e-04 - val_loss: 2.8057e-04 - 13s/epoch - 153us/sample
Epoch 66/85
84077/84077 - 13s - loss: 2.8076e-04 - val_loss: 2.8708e-04 - 13s/epoch - 152us/sample
Epoch 67/85
84077/84077 - 13s - loss: 2.8001e-04 - val_loss: 2.8345e-04 - 13s/epoch - 152us/sample
Epoch 68/85
84077/84077 - 13s - loss: 2.7953e-04 - val_loss: 2.8472e-04 - 13s/epoch - 152us/sample
Epoch 69/85
84077/84077 - 13s - loss: 2.7912e-04 - val_loss: 2.8157e-04 - 13s/epoch - 152us/sample
Epoch 70/85
84077/84077 - 13s - loss: 2.7869e-04 - val_loss: 2.8055e-04 - 13s/epoch - 154us/sample
Epoch 71/85
84077/84077 - 13s - loss: 2.7820e-04 - val_loss: 2.8209e-04 - 13s/epoch - 152us/sample
Epoch 72/85
84077/84077 - 13s - loss: 2.7816e-04 - val_loss: 2.7993e-04 - 13s/epoch - 152us/sample
Epoch 73/85
84077/84077 - 13s - loss: 2.7729e-04 - val_loss: 2.7888e-04 - 13s/epoch - 152us/sample
Epoch 74/85
84077/84077 - 13s - loss: 2.7743e-04 - val_loss: 2.8214e-04 - 13s/epoch - 152us/sample
Epoch 75/85
84077/84077 - 13s - loss: 2.7737e-04 - val_loss: 2.8137e-04 - 13s/epoch - 152us/sample
Epoch 76/85
84077/84077 - 13s - loss: 2.7685e-04 - val_loss: 2.8027e-04 - 13s/epoch - 153us/sample
Epoch 77/85
84077/84077 - 13s - loss: 2.7638e-04 - val_loss: 2.8345e-04 - 13s/epoch - 152us/sample
Epoch 78/85
84077/84077 - 13s - loss: 2.7639e-04 - val_loss: 2.7421e-04 - 13s/epoch - 152us/sample
Epoch 79/85
84077/84077 - 13s - loss: 2.7593e-04 - val_loss: 2.8025e-04 - 13s/epoch - 152us/sample
Epoch 80/85
84077/84077 - 13s - loss: 2.7547e-04 - val_loss: 2.7900e-04 - 13s/epoch - 152us/sample
Epoch 81/85
84077/84077 - 13s - loss: 2.7566e-04 - val_loss: 2.8248e-04 - 13s/epoch - 153us/sample
Epoch 82/85
84077/84077 - 13s - loss: 2.7526e-04 - val_loss: 2.8137e-04 - 13s/epoch - 153us/sample
Epoch 83/85
84077/84077 - 13s - loss: 2.7502e-04 - val_loss: 2.7807e-04 - 13s/epoch - 152us/sample
Epoch 84/85
84077/84077 - 13s - loss: 2.7459e-04 - val_loss: 2.7909e-04 - 13s/epoch - 152us/sample
Epoch 85/85
84077/84077 - 13s - loss: 2.7488e-04 - val_loss: 2.7694e-04 - 13s/epoch - 152us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00027693694836764714
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 16:25:05.646260: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_38/outputlayer/BiasAdd' id:48625 op device:{requested: '', assigned: ''} def:{{{node decoder_model_38/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_38/outputlayer/MatMul, decoder_model_38/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04078239865533768
cosine 0.04026065149750665
MAE: 0.002588606557949397
RMSE: 0.013438438842824005
r2: 0.8589727745954892
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 32, 85, 0.0012000000000000001, 0.2, 188, 0.0002748789007934208, 0.00027693694836764714, 0.04078239865533768, 0.04026065149750665, 0.002588606557949397, 0.013438438842824005, 0.8589727745954892, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.001 32 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_117 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_117 (ReLU)               (None, 1603)         0           ['batch_normalization_117[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 16:25:22.492803: W tensorflow/c/c_api.cc:291] Operation '{name:'training_78/Adam/batch_normalization_118/beta/v/Assign' id:50543 op device:{requested: '', assigned: ''} def:{{{node training_78/Adam/batch_normalization_118/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_78/Adam/batch_normalization_118/beta/v, training_78/Adam/batch_normalization_118/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 16:25:38.352973: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_39/mul' id:49909 op device:{requested: '', assigned: ''} def:{{{node loss_39/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_39/mul/x, loss_39/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0073 - val_loss: 0.0018 - 22s/epoch - 263us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.0015 - val_loss: 0.0013 - 13s/epoch - 151us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0014 - val_loss: 8.5764e-04 - 13s/epoch - 151us/sample
Epoch 4/85
84077/84077 - 13s - loss: 8.8087e-04 - val_loss: 7.0388e-04 - 13s/epoch - 151us/sample
Epoch 5/85
84077/84077 - 13s - loss: 0.0014 - val_loss: 8.7503e-04 - 13s/epoch - 151us/sample
Epoch 6/85
84077/84077 - 13s - loss: 8.5493e-04 - val_loss: 5.8580e-04 - 13s/epoch - 152us/sample
Epoch 7/85
84077/84077 - 13s - loss: 7.3644e-04 - val_loss: 5.3705e-04 - 13s/epoch - 152us/sample
Epoch 8/85
84077/84077 - 13s - loss: 5.9252e-04 - val_loss: 4.9177e-04 - 13s/epoch - 151us/sample
Epoch 9/85
84077/84077 - 13s - loss: 5.4994e-04 - val_loss: 4.3516e-04 - 13s/epoch - 151us/sample
Epoch 10/85
84077/84077 - 13s - loss: 5.0071e-04 - val_loss: 4.4961e-04 - 13s/epoch - 151us/sample
Epoch 11/85
84077/84077 - 13s - loss: 4.4681e-04 - val_loss: 4.0089e-04 - 13s/epoch - 152us/sample
Epoch 12/85
84077/84077 - 13s - loss: 4.1702e-04 - val_loss: 3.6974e-04 - 13s/epoch - 153us/sample
Epoch 13/85
84077/84077 - 13s - loss: 3.9757e-04 - val_loss: 3.5325e-04 - 13s/epoch - 152us/sample
Epoch 14/85
84077/84077 - 13s - loss: 3.7922e-04 - val_loss: 3.5054e-04 - 13s/epoch - 151us/sample
Epoch 15/85
84077/84077 - 13s - loss: 3.6577e-04 - val_loss: 3.4028e-04 - 13s/epoch - 151us/sample
Epoch 16/85
84077/84077 - 13s - loss: 3.5603e-04 - val_loss: 3.2983e-04 - 13s/epoch - 151us/sample
Epoch 17/85
84077/84077 - 13s - loss: 3.4699e-04 - val_loss: 3.1958e-04 - 13s/epoch - 153us/sample
Epoch 18/85
84077/84077 - 13s - loss: 3.3883e-04 - val_loss: 3.1514e-04 - 13s/epoch - 151us/sample
Epoch 19/85
84077/84077 - 13s - loss: 3.3258e-04 - val_loss: 3.1462e-04 - 13s/epoch - 151us/sample
Epoch 20/85
84077/84077 - 13s - loss: 3.2663e-04 - val_loss: 3.1015e-04 - 13s/epoch - 151us/sample
Epoch 21/85
84077/84077 - 13s - loss: 3.2195e-04 - val_loss: 3.1076e-04 - 13s/epoch - 151us/sample
Epoch 22/85
84077/84077 - 13s - loss: 3.1685e-04 - val_loss: 3.0317e-04 - 13s/epoch - 152us/sample
Epoch 23/85
84077/84077 - 13s - loss: 3.1258e-04 - val_loss: 3.0674e-04 - 13s/epoch - 152us/sample
Epoch 24/85
84077/84077 - 13s - loss: 3.0855e-04 - val_loss: 3.1097e-04 - 13s/epoch - 152us/sample
Epoch 25/85
84077/84077 - 13s - loss: 3.0516e-04 - val_loss: 2.9504e-04 - 13s/epoch - 151us/sample
Epoch 26/85
84077/84077 - 13s - loss: 3.0212e-04 - val_loss: 2.9743e-04 - 13s/epoch - 151us/sample
Epoch 27/85
84077/84077 - 13s - loss: 2.9845e-04 - val_loss: 2.9659e-04 - 13s/epoch - 151us/sample
Epoch 28/85
84077/84077 - 13s - loss: 2.9588e-04 - val_loss: 2.9386e-04 - 13s/epoch - 152us/sample
Epoch 29/85
84077/84077 - 13s - loss: 2.9324e-04 - val_loss: 2.9174e-04 - 13s/epoch - 152us/sample
Epoch 30/85
84077/84077 - 13s - loss: 2.9107e-04 - val_loss: 2.8591e-04 - 13s/epoch - 151us/sample
Epoch 31/85
84077/84077 - 13s - loss: 2.8951e-04 - val_loss: 2.9343e-04 - 13s/epoch - 151us/sample
Epoch 32/85
84077/84077 - 13s - loss: 2.8717e-04 - val_loss: 2.8988e-04 - 13s/epoch - 151us/sample
Epoch 33/85
84077/84077 - 13s - loss: 2.8492e-04 - val_loss: 2.8308e-04 - 13s/epoch - 151us/sample
Epoch 34/85
84077/84077 - 13s - loss: 2.8338e-04 - val_loss: 2.7809e-04 - 13s/epoch - 152us/sample
Epoch 35/85
84077/84077 - 13s - loss: 2.8201e-04 - val_loss: 2.7735e-04 - 13s/epoch - 153us/sample
Epoch 36/85
84077/84077 - 13s - loss: 2.8139e-04 - val_loss: 2.7502e-04 - 13s/epoch - 151us/sample
Epoch 37/85
84077/84077 - 13s - loss: 2.8007e-04 - val_loss: 2.7555e-04 - 13s/epoch - 151us/sample
Epoch 38/85
84077/84077 - 13s - loss: 2.7916e-04 - val_loss: 2.7247e-04 - 13s/epoch - 151us/sample
Epoch 39/85
84077/84077 - 13s - loss: 2.7745e-04 - val_loss: 2.7165e-04 - 13s/epoch - 151us/sample
Epoch 40/85
84077/84077 - 13s - loss: 2.7583e-04 - val_loss: 2.7272e-04 - 13s/epoch - 152us/sample
Epoch 41/85
84077/84077 - 13s - loss: 2.7471e-04 - val_loss: 2.7017e-04 - 13s/epoch - 152us/sample
Epoch 42/85
84077/84077 - 13s - loss: 2.7353e-04 - val_loss: 2.7180e-04 - 13s/epoch - 151us/sample
Epoch 43/85
84077/84077 - 13s - loss: 2.7293e-04 - val_loss: 2.6373e-04 - 13s/epoch - 151us/sample
Epoch 44/85
84077/84077 - 13s - loss: 2.7083e-04 - val_loss: 2.6405e-04 - 13s/epoch - 151us/sample
Epoch 45/85
84077/84077 - 13s - loss: 2.7060e-04 - val_loss: 2.6651e-04 - 13s/epoch - 153us/sample
Epoch 46/85
84077/84077 - 13s - loss: 2.6960e-04 - val_loss: 2.7084e-04 - 13s/epoch - 152us/sample
Epoch 47/85
84077/84077 - 13s - loss: 2.6858e-04 - val_loss: 2.6904e-04 - 13s/epoch - 151us/sample
Epoch 48/85
84077/84077 - 13s - loss: 2.6731e-04 - val_loss: 2.6842e-04 - 13s/epoch - 151us/sample
Epoch 49/85
84077/84077 - 13s - loss: 2.6688e-04 - val_loss: 2.6266e-04 - 13s/epoch - 151us/sample
Epoch 50/85
84077/84077 - 13s - loss: 2.6580e-04 - val_loss: 2.6251e-04 - 13s/epoch - 151us/sample
Epoch 51/85
84077/84077 - 13s - loss: 2.6493e-04 - val_loss: 2.6183e-04 - 13s/epoch - 153us/sample
Epoch 52/85
84077/84077 - 13s - loss: 2.6406e-04 - val_loss: 2.6217e-04 - 13s/epoch - 152us/sample
Epoch 53/85
84077/84077 - 13s - loss: 2.6377e-04 - val_loss: 2.5945e-04 - 13s/epoch - 151us/sample
Epoch 54/85
84077/84077 - 13s - loss: 2.6375e-04 - val_loss: 2.6660e-04 - 13s/epoch - 151us/sample
Epoch 55/85
84077/84077 - 13s - loss: 2.6227e-04 - val_loss: 2.6119e-04 - 13s/epoch - 151us/sample
Epoch 56/85
84077/84077 - 13s - loss: 2.6189e-04 - val_loss: 2.5410e-04 - 13s/epoch - 152us/sample
Epoch 57/85
84077/84077 - 13s - loss: 2.6112e-04 - val_loss: 2.5693e-04 - 13s/epoch - 151us/sample
Epoch 58/85
84077/84077 - 13s - loss: 2.6055e-04 - val_loss: 2.5479e-04 - 13s/epoch - 151us/sample
Epoch 59/85
84077/84077 - 13s - loss: 2.5999e-04 - val_loss: 2.5929e-04 - 13s/epoch - 151us/sample
Epoch 60/85
84077/84077 - 13s - loss: 2.5983e-04 - val_loss: 2.5958e-04 - 13s/epoch - 151us/sample
Epoch 61/85
84077/84077 - 13s - loss: 2.5916e-04 - val_loss: 2.5165e-04 - 13s/epoch - 151us/sample
Epoch 62/85
84077/84077 - 13s - loss: 2.5832e-04 - val_loss: 2.6031e-04 - 13s/epoch - 152us/sample
Epoch 63/85
84077/84077 - 13s - loss: 2.5939e-04 - val_loss: 2.5874e-04 - 13s/epoch - 151us/sample
Epoch 64/85
84077/84077 - 13s - loss: 2.5817e-04 - val_loss: 2.5382e-04 - 13s/epoch - 151us/sample
Epoch 65/85
84077/84077 - 13s - loss: 2.5711e-04 - val_loss: 2.5834e-04 - 13s/epoch - 151us/sample
Epoch 66/85
84077/84077 - 13s - loss: 2.5676e-04 - val_loss: 2.5818e-04 - 13s/epoch - 151us/sample
Epoch 67/85
84077/84077 - 13s - loss: 2.5653e-04 - val_loss: 2.5300e-04 - 13s/epoch - 152us/sample
Epoch 68/85
84077/84077 - 13s - loss: 2.5580e-04 - val_loss: 2.5247e-04 - 13s/epoch - 152us/sample
Epoch 69/85
84077/84077 - 13s - loss: 2.5594e-04 - val_loss: 2.5461e-04 - 13s/epoch - 152us/sample
Epoch 70/85
84077/84077 - 13s - loss: 2.5495e-04 - val_loss: 2.5286e-04 - 13s/epoch - 151us/sample
Epoch 71/85
84077/84077 - 13s - loss: 2.5597e-04 - val_loss: 2.5152e-04 - 13s/epoch - 152us/sample
Epoch 72/85
84077/84077 - 13s - loss: 2.5452e-04 - val_loss: 2.5161e-04 - 13s/epoch - 151us/sample
Epoch 73/85
84077/84077 - 13s - loss: 2.5462e-04 - val_loss: 2.5267e-04 - 13s/epoch - 153us/sample
Epoch 74/85
84077/84077 - 13s - loss: 2.5421e-04 - val_loss: 2.5108e-04 - 13s/epoch - 152us/sample
Epoch 75/85
84077/84077 - 13s - loss: 2.5348e-04 - val_loss: 2.5096e-04 - 13s/epoch - 151us/sample
Epoch 76/85
84077/84077 - 13s - loss: 2.5371e-04 - val_loss: 2.5800e-04 - 13s/epoch - 151us/sample
Epoch 77/85
84077/84077 - 13s - loss: 2.5322e-04 - val_loss: 2.5263e-04 - 13s/epoch - 151us/sample
Epoch 78/85
84077/84077 - 13s - loss: 2.5295e-04 - val_loss: 2.4875e-04 - 13s/epoch - 153us/sample
Epoch 79/85
84077/84077 - 13s - loss: 2.5265e-04 - val_loss: 2.5048e-04 - 13s/epoch - 151us/sample
Epoch 80/85
84077/84077 - 13s - loss: 2.5232e-04 - val_loss: 2.4997e-04 - 13s/epoch - 151us/sample
Epoch 81/85
84077/84077 - 13s - loss: 2.5205e-04 - val_loss: 2.5042e-04 - 13s/epoch - 151us/sample
Epoch 82/85
84077/84077 - 13s - loss: 2.5164e-04 - val_loss: 2.4861e-04 - 13s/epoch - 151us/sample
Epoch 83/85
84077/84077 - 13s - loss: 2.5120e-04 - val_loss: 2.4714e-04 - 13s/epoch - 152us/sample
Epoch 84/85
84077/84077 - 13s - loss: 2.5181e-04 - val_loss: 2.4906e-04 - 13s/epoch - 152us/sample
Epoch 85/85
84077/84077 - 13s - loss: 2.5078e-04 - val_loss: 2.5159e-04 - 13s/epoch - 151us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002515879667287697
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 16:43:31.632728: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_39/outputlayer/BiasAdd' id:49880 op device:{requested: '', assigned: ''} def:{{{node decoder_model_39/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_39/outputlayer/MatMul, decoder_model_39/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.032160315984287695
cosine 0.03175312281198639
MAE: 0.0024857286050312447
RMSE: 0.012172574822459968
r2: 0.8843272675057197
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 32, 85, 0.001, 0.2, 188, 0.00025077696134806477, 0.0002515879667287697, 0.032160315984287695, 0.03175312281198639, 0.0024857286050312447, 0.012172574822459968, 0.8843272675057197, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 90 0.0012000000000000001 32 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_120 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_120 (ReLU)               (None, 1697)         0           ['batch_normalization_120[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 16:43:48.746301: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_120/moving_variance/Assign' id:50770 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_120/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_120/moving_variance, batch_normalization_120/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 16:44:05.037541: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_40/mul' id:51164 op device:{requested: '', assigned: ''} def:{{{node loss_40/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_40/mul/x, loss_40/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0059 - val_loss: 2.0763 - 23s/epoch - 273us/sample
Epoch 2/90
84077/84077 - 13s - loss: 0.0018 - val_loss: 0.0010 - 13s/epoch - 152us/sample
Epoch 3/90
84077/84077 - 13s - loss: 0.0013 - val_loss: 8.5111e-04 - 13s/epoch - 152us/sample
Epoch 4/90
84077/84077 - 13s - loss: 8.4387e-04 - val_loss: 6.7307e-04 - 13s/epoch - 152us/sample
Epoch 5/90
84077/84077 - 13s - loss: 7.1064e-04 - val_loss: 5.4775e-04 - 13s/epoch - 152us/sample
Epoch 6/90
84077/84077 - 13s - loss: 5.6668e-04 - val_loss: 4.6307e-04 - 13s/epoch - 152us/sample
Epoch 7/90
84077/84077 - 13s - loss: 4.8421e-04 - val_loss: 4.0963e-04 - 13s/epoch - 153us/sample
Epoch 8/90
84077/84077 - 13s - loss: 4.3214e-04 - val_loss: 3.6935e-04 - 13s/epoch - 153us/sample
Epoch 9/90
84077/84077 - 13s - loss: 4.0069e-04 - val_loss: 3.5153e-04 - 13s/epoch - 152us/sample
Epoch 10/90
84077/84077 - 13s - loss: 3.8153e-04 - val_loss: 3.3210e-04 - 13s/epoch - 152us/sample
Epoch 11/90
84077/84077 - 13s - loss: 3.6276e-04 - val_loss: 3.2479e-04 - 13s/epoch - 152us/sample
Epoch 12/90
84077/84077 - 13s - loss: 3.4962e-04 - val_loss: 3.0997e-04 - 13s/epoch - 152us/sample
Epoch 13/90
84077/84077 - 13s - loss: 3.3877e-04 - val_loss: 3.0698e-04 - 13s/epoch - 153us/sample
Epoch 14/90
84077/84077 - 13s - loss: 3.2879e-04 - val_loss: 2.9693e-04 - 13s/epoch - 152us/sample
Epoch 15/90
84077/84077 - 13s - loss: 3.2055e-04 - val_loss: 2.8783e-04 - 13s/epoch - 152us/sample
Epoch 16/90
84077/84077 - 13s - loss: 3.1286e-04 - val_loss: 2.7819e-04 - 13s/epoch - 152us/sample
Epoch 17/90
84077/84077 - 13s - loss: 3.0852e-04 - val_loss: 2.7971e-04 - 13s/epoch - 152us/sample
Epoch 18/90
84077/84077 - 13s - loss: 3.0032e-04 - val_loss: 2.7075e-04 - 13s/epoch - 153us/sample
Epoch 19/90
84077/84077 - 13s - loss: 2.9585e-04 - val_loss: 2.6770e-04 - 13s/epoch - 152us/sample
Epoch 20/90
84077/84077 - 13s - loss: 2.9023e-04 - val_loss: 2.6258e-04 - 13s/epoch - 152us/sample
Epoch 21/90
84077/84077 - 13s - loss: 2.8644e-04 - val_loss: 2.6231e-04 - 13s/epoch - 152us/sample
Epoch 22/90
84077/84077 - 13s - loss: 2.8143e-04 - val_loss: 2.5612e-04 - 13s/epoch - 152us/sample
Epoch 23/90
84077/84077 - 13s - loss: 2.7854e-04 - val_loss: 2.6086e-04 - 13s/epoch - 153us/sample
Epoch 24/90
84077/84077 - 13s - loss: 2.7531e-04 - val_loss: 2.5353e-04 - 13s/epoch - 152us/sample
Epoch 25/90
84077/84077 - 13s - loss: 2.7378e-04 - val_loss: 2.4881e-04 - 13s/epoch - 152us/sample
Epoch 26/90
84077/84077 - 13s - loss: 2.7008e-04 - val_loss: 2.4746e-04 - 13s/epoch - 152us/sample
Epoch 27/90
84077/84077 - 13s - loss: 2.6625e-04 - val_loss: 2.4866e-04 - 13s/epoch - 152us/sample
Epoch 28/90
84077/84077 - 13s - loss: 2.6427e-04 - val_loss: 2.4205e-04 - 13s/epoch - 152us/sample
Epoch 29/90
84077/84077 - 13s - loss: 2.6197e-04 - val_loss: 2.4599e-04 - 13s/epoch - 154us/sample
Epoch 30/90
84077/84077 - 13s - loss: 2.6198e-04 - val_loss: 2.4167e-04 - 13s/epoch - 152us/sample
Epoch 31/90
84077/84077 - 13s - loss: 2.5835e-04 - val_loss: 2.4055e-04 - 13s/epoch - 152us/sample
Epoch 32/90
84077/84077 - 13s - loss: 2.5642e-04 - val_loss: 2.3745e-04 - 13s/epoch - 152us/sample
Epoch 33/90
84077/84077 - 13s - loss: 2.5594e-04 - val_loss: 2.3822e-04 - 13s/epoch - 152us/sample
Epoch 34/90
84077/84077 - 13s - loss: 2.5597e-04 - val_loss: 2.3471e-04 - 13s/epoch - 153us/sample
Epoch 35/90
84077/84077 - 13s - loss: 2.5312e-04 - val_loss: 2.3705e-04 - 13s/epoch - 152us/sample
Epoch 36/90
84077/84077 - 13s - loss: 2.5026e-04 - val_loss: 2.3115e-04 - 13s/epoch - 152us/sample
Epoch 37/90
84077/84077 - 13s - loss: 2.4973e-04 - val_loss: 2.3025e-04 - 13s/epoch - 152us/sample
Epoch 38/90
84077/84077 - 13s - loss: 2.4802e-04 - val_loss: 2.3213e-04 - 13s/epoch - 152us/sample
Epoch 39/90
84077/84077 - 13s - loss: 2.4722e-04 - val_loss: 2.3306e-04 - 13s/epoch - 152us/sample
Epoch 40/90
84077/84077 - 13s - loss: 2.4628e-04 - val_loss: 2.2848e-04 - 13s/epoch - 153us/sample
Epoch 41/90
84077/84077 - 13s - loss: 2.4440e-04 - val_loss: 2.3391e-04 - 13s/epoch - 152us/sample
Epoch 42/90
84077/84077 - 13s - loss: 2.4348e-04 - val_loss: 2.2704e-04 - 13s/epoch - 152us/sample
Epoch 43/90
84077/84077 - 13s - loss: 2.4291e-04 - val_loss: 2.3164e-04 - 13s/epoch - 152us/sample
Epoch 44/90
84077/84077 - 13s - loss: 2.4251e-04 - val_loss: 2.2735e-04 - 13s/epoch - 152us/sample
Epoch 45/90
84077/84077 - 13s - loss: 2.4080e-04 - val_loss: 2.2549e-04 - 13s/epoch - 152us/sample
Epoch 46/90
84077/84077 - 13s - loss: 2.4038e-04 - val_loss: 2.2617e-04 - 13s/epoch - 153us/sample
Epoch 47/90
84077/84077 - 13s - loss: 2.4077e-04 - val_loss: 2.2554e-04 - 13s/epoch - 152us/sample
Epoch 48/90
84077/84077 - 13s - loss: 2.3764e-04 - val_loss: 2.2835e-04 - 13s/epoch - 152us/sample
Epoch 49/90
84077/84077 - 13s - loss: 2.3801e-04 - val_loss: 2.2482e-04 - 13s/epoch - 152us/sample
Epoch 50/90
84077/84077 - 13s - loss: 2.3727e-04 - val_loss: 2.2070e-04 - 13s/epoch - 152us/sample
Epoch 51/90
84077/84077 - 13s - loss: 2.3729e-04 - val_loss: 2.2112e-04 - 13s/epoch - 153us/sample
Epoch 52/90
84077/84077 - 13s - loss: 2.3568e-04 - val_loss: 2.2224e-04 - 13s/epoch - 152us/sample
Epoch 53/90
84077/84077 - 13s - loss: 2.3445e-04 - val_loss: 2.1894e-04 - 13s/epoch - 153us/sample
Epoch 54/90
84077/84077 - 13s - loss: 2.3486e-04 - val_loss: 2.2127e-04 - 13s/epoch - 152us/sample
Epoch 55/90
84077/84077 - 13s - loss: 2.3378e-04 - val_loss: 2.2306e-04 - 13s/epoch - 152us/sample
Epoch 56/90
84077/84077 - 13s - loss: 2.3343e-04 - val_loss: 2.2038e-04 - 13s/epoch - 152us/sample
Epoch 57/90
84077/84077 - 13s - loss: 2.3293e-04 - val_loss: 2.2294e-04 - 13s/epoch - 153us/sample
Epoch 58/90
84077/84077 - 13s - loss: 2.3258e-04 - val_loss: 2.1800e-04 - 13s/epoch - 153us/sample
Epoch 59/90
84077/84077 - 13s - loss: 2.3055e-04 - val_loss: 2.2056e-04 - 13s/epoch - 152us/sample
Epoch 60/90
84077/84077 - 13s - loss: 2.3180e-04 - val_loss: 2.1590e-04 - 13s/epoch - 153us/sample
Epoch 61/90
84077/84077 - 13s - loss: 2.3064e-04 - val_loss: 2.1766e-04 - 13s/epoch - 152us/sample
Epoch 62/90
84077/84077 - 13s - loss: 2.3271e-04 - val_loss: 2.2401e-04 - 13s/epoch - 152us/sample
Epoch 63/90
84077/84077 - 13s - loss: 2.3029e-04 - val_loss: 2.2096e-04 - 13s/epoch - 153us/sample
Epoch 64/90
84077/84077 - 13s - loss: 2.2947e-04 - val_loss: 2.1501e-04 - 13s/epoch - 152us/sample
Epoch 65/90
84077/84077 - 13s - loss: 2.2907e-04 - val_loss: 2.2335e-04 - 13s/epoch - 152us/sample
Epoch 66/90
84077/84077 - 13s - loss: 2.2799e-04 - val_loss: 2.1641e-04 - 13s/epoch - 152us/sample
Epoch 67/90
84077/84077 - 13s - loss: 2.2748e-04 - val_loss: 2.1318e-04 - 13s/epoch - 152us/sample
Epoch 68/90
84077/84077 - 13s - loss: 2.2709e-04 - val_loss: 2.1322e-04 - 13s/epoch - 152us/sample
Epoch 69/90
84077/84077 - 13s - loss: 2.2760e-04 - val_loss: 2.1841e-04 - 13s/epoch - 154us/sample
Epoch 70/90
84077/84077 - 13s - loss: 2.3219e-04 - val_loss: 2.1255e-04 - 13s/epoch - 153us/sample
Epoch 71/90
84077/84077 - 13s - loss: 2.2626e-04 - val_loss: 2.1506e-04 - 13s/epoch - 152us/sample
Epoch 72/90
84077/84077 - 13s - loss: 2.2608e-04 - val_loss: 2.1598e-04 - 13s/epoch - 152us/sample
Epoch 73/90
84077/84077 - 13s - loss: 2.2564e-04 - val_loss: 2.1705e-04 - 13s/epoch - 152us/sample
Epoch 74/90
84077/84077 - 13s - loss: 2.2594e-04 - val_loss: 2.1276e-04 - 13s/epoch - 152us/sample
Epoch 75/90
84077/84077 - 13s - loss: 2.2527e-04 - val_loss: 2.1319e-04 - 13s/epoch - 153us/sample
Epoch 76/90
84077/84077 - 13s - loss: 2.2596e-04 - val_loss: 2.1208e-04 - 13s/epoch - 153us/sample
Epoch 77/90
84077/84077 - 13s - loss: 2.2521e-04 - val_loss: 2.1615e-04 - 13s/epoch - 152us/sample
Epoch 78/90
84077/84077 - 13s - loss: 2.2383e-04 - val_loss: 2.1007e-04 - 13s/epoch - 153us/sample
Epoch 79/90
84077/84077 - 13s - loss: 2.2304e-04 - val_loss: 2.0931e-04 - 13s/epoch - 152us/sample
Epoch 80/90
84077/84077 - 13s - loss: 2.2250e-04 - val_loss: 2.1061e-04 - 13s/epoch - 153us/sample
Epoch 81/90
84077/84077 - 13s - loss: 2.2254e-04 - val_loss: 2.0924e-04 - 13s/epoch - 154us/sample
Epoch 82/90
84077/84077 - 13s - loss: 2.2305e-04 - val_loss: 2.0936e-04 - 13s/epoch - 153us/sample
Epoch 83/90
84077/84077 - 13s - loss: 2.2250e-04 - val_loss: 2.0906e-04 - 13s/epoch - 152us/sample
Epoch 84/90
84077/84077 - 13s - loss: 2.2191e-04 - val_loss: 2.1279e-04 - 13s/epoch - 152us/sample
Epoch 85/90
84077/84077 - 13s - loss: 2.2208e-04 - val_loss: 2.1080e-04 - 13s/epoch - 152us/sample
Epoch 86/90
84077/84077 - 13s - loss: 2.2088e-04 - val_loss: 2.0806e-04 - 13s/epoch - 153us/sample
Epoch 87/90
84077/84077 - 13s - loss: 2.2065e-04 - val_loss: 2.2143e-04 - 13s/epoch - 153us/sample
Epoch 88/90
84077/84077 - 13s - loss: 2.2142e-04 - val_loss: 2.0963e-04 - 13s/epoch - 152us/sample
Epoch 89/90
84077/84077 - 13s - loss: 2.1970e-04 - val_loss: 2.0956e-04 - 13s/epoch - 152us/sample
Epoch 90/90
84077/84077 - 13s - loss: 2.2080e-04 - val_loss: 2.0755e-04 - 13s/epoch - 152us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.000207554628916205
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 17:03:08.736482: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_40/outputlayer/BiasAdd' id:51135 op device:{requested: '', assigned: ''} def:{{{node decoder_model_40/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_40/outputlayer/MatMul, decoder_model_40/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.025248270087872324
cosine 0.024939962101844952
MAE: 0.002237136530418382
RMSE: 0.009871677917148252
r2: 0.9239682330283465
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 90, 0.0012000000000000001, 0.2, 188, 0.00022079845503189652, 0.000207554628916205, 0.025248270087872324, 0.024939962101844952, 0.002237136530418382, 0.009871677917148252, 0.9239682330283465, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.001 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_123 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_123 (ReLU)               (None, 1603)         0           ['batch_normalization_123[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 17:03:26.739076: W tensorflow/c/c_api.cc:291] Operation '{name:'training_82/Adam/batch_normalization_123/beta/m/Assign' id:52893 op device:{requested: '', assigned: ''} def:{{{node training_82/Adam/batch_normalization_123/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_82/Adam/batch_normalization_123/beta/m, training_82/Adam/batch_normalization_123/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 17:03:38.528843: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_41/mul' id:52419 op device:{requested: '', assigned: ''} def:{{{node loss_41/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_41/mul/x, loss_41/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0109 - val_loss: 0.0015 - 18s/epoch - 216us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0014 - val_loss: 0.0014 - 8s/epoch - 96us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0019 - val_loss: 0.0011 - 8s/epoch - 96us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0011 - val_loss: 8.7901e-04 - 8s/epoch - 96us/sample
Epoch 5/85
84077/84077 - 8s - loss: 0.0099 - val_loss: 9.1364e-04 - 8s/epoch - 96us/sample
Epoch 6/85
84077/84077 - 8s - loss: 8.2184e-04 - val_loss: 6.8608e-04 - 8s/epoch - 96us/sample
Epoch 7/85
84077/84077 - 8s - loss: 7.0901e-04 - val_loss: 5.9574e-04 - 8s/epoch - 96us/sample
Epoch 8/85
84077/84077 - 8s - loss: 6.1932e-04 - val_loss: 5.2631e-04 - 8s/epoch - 96us/sample
Epoch 9/85
84077/84077 - 8s - loss: 5.4618e-04 - val_loss: 4.4952e-04 - 8s/epoch - 97us/sample
Epoch 10/85
84077/84077 - 8s - loss: 4.6723e-04 - val_loss: 3.8938e-04 - 8s/epoch - 96us/sample
Epoch 11/85
84077/84077 - 8s - loss: 4.2838e-04 - val_loss: 3.4933e-04 - 8s/epoch - 96us/sample
Epoch 12/85
84077/84077 - 8s - loss: 3.8736e-04 - val_loss: 3.3400e-04 - 8s/epoch - 96us/sample
Epoch 13/85
84077/84077 - 8s - loss: 3.6312e-04 - val_loss: 3.0833e-04 - 8s/epoch - 96us/sample
Epoch 14/85
84077/84077 - 8s - loss: 3.3875e-04 - val_loss: 2.8750e-04 - 8s/epoch - 96us/sample
Epoch 15/85
84077/84077 - 8s - loss: 3.2838e-04 - val_loss: 3.0491e-04 - 8s/epoch - 96us/sample
Epoch 16/85
84077/84077 - 8s - loss: 3.0950e-04 - val_loss: 2.7457e-04 - 8s/epoch - 97us/sample
Epoch 17/85
84077/84077 - 8s - loss: 2.9706e-04 - val_loss: 2.6998e-04 - 8s/epoch - 96us/sample
Epoch 18/85
84077/84077 - 8s - loss: 2.8758e-04 - val_loss: 2.5119e-04 - 8s/epoch - 96us/sample
Epoch 19/85
84077/84077 - 8s - loss: 2.7747e-04 - val_loss: 2.4884e-04 - 8s/epoch - 95us/sample
Epoch 20/85
84077/84077 - 8s - loss: 2.7171e-04 - val_loss: 2.4119e-04 - 8s/epoch - 96us/sample
Epoch 21/85
84077/84077 - 8s - loss: 2.6491e-04 - val_loss: 2.3554e-04 - 8s/epoch - 96us/sample
Epoch 22/85
84077/84077 - 8s - loss: 2.5773e-04 - val_loss: 2.3167e-04 - 8s/epoch - 96us/sample
Epoch 23/85
84077/84077 - 8s - loss: 2.5405e-04 - val_loss: 2.2946e-04 - 8s/epoch - 96us/sample
Epoch 24/85
84077/84077 - 8s - loss: 2.4745e-04 - val_loss: 2.2756e-04 - 8s/epoch - 96us/sample
Epoch 25/85
84077/84077 - 8s - loss: 2.4479e-04 - val_loss: 2.2131e-04 - 8s/epoch - 97us/sample
Epoch 26/85
84077/84077 - 8s - loss: 2.4064e-04 - val_loss: 2.2056e-04 - 8s/epoch - 97us/sample
Epoch 27/85
84077/84077 - 8s - loss: 2.3774e-04 - val_loss: 2.1412e-04 - 8s/epoch - 96us/sample
Epoch 28/85
84077/84077 - 8s - loss: 2.3425e-04 - val_loss: 2.1360e-04 - 8s/epoch - 96us/sample
Epoch 29/85
84077/84077 - 8s - loss: 2.3185e-04 - val_loss: 2.1006e-04 - 8s/epoch - 96us/sample
Epoch 30/85
84077/84077 - 8s - loss: 2.2916e-04 - val_loss: 2.0983e-04 - 8s/epoch - 96us/sample
Epoch 31/85
84077/84077 - 8s - loss: 2.2610e-04 - val_loss: 2.0706e-04 - 8s/epoch - 96us/sample
Epoch 32/85
84077/84077 - 8s - loss: 2.2746e-04 - val_loss: 2.0760e-04 - 8s/epoch - 96us/sample
Epoch 33/85
84077/84077 - 8s - loss: 2.2293e-04 - val_loss: 2.0385e-04 - 8s/epoch - 97us/sample
Epoch 34/85
84077/84077 - 8s - loss: 2.2071e-04 - val_loss: 2.0564e-04 - 8s/epoch - 96us/sample
Epoch 35/85
84077/84077 - 8s - loss: 2.2034e-04 - val_loss: 2.0230e-04 - 8s/epoch - 96us/sample
Epoch 36/85
84077/84077 - 8s - loss: 2.1856e-04 - val_loss: 1.9998e-04 - 8s/epoch - 96us/sample
Epoch 37/85
84077/84077 - 8s - loss: 2.1619e-04 - val_loss: 1.9806e-04 - 8s/epoch - 96us/sample
Epoch 38/85
84077/84077 - 8s - loss: 2.1384e-04 - val_loss: 2.0006e-04 - 8s/epoch - 96us/sample
Epoch 39/85
84077/84077 - 8s - loss: 2.1519e-04 - val_loss: 1.9937e-04 - 8s/epoch - 96us/sample
Epoch 40/85
84077/84077 - 8s - loss: 2.1278e-04 - val_loss: 1.9488e-04 - 8s/epoch - 96us/sample
Epoch 41/85
84077/84077 - 8s - loss: 2.1057e-04 - val_loss: 1.9424e-04 - 8s/epoch - 97us/sample
Epoch 42/85
84077/84077 - 8s - loss: 2.0964e-04 - val_loss: 1.9359e-04 - 8s/epoch - 96us/sample
Epoch 43/85
84077/84077 - 8s - loss: 2.0960e-04 - val_loss: 1.9393e-04 - 8s/epoch - 96us/sample
Epoch 44/85
84077/84077 - 8s - loss: 2.0744e-04 - val_loss: 1.8955e-04 - 8s/epoch - 96us/sample
Epoch 45/85
84077/84077 - 8s - loss: 2.0643e-04 - val_loss: 1.8814e-04 - 8s/epoch - 96us/sample
Epoch 46/85
84077/84077 - 8s - loss: 2.0538e-04 - val_loss: 1.8744e-04 - 8s/epoch - 96us/sample
Epoch 47/85
84077/84077 - 8s - loss: 2.0538e-04 - val_loss: 1.8748e-04 - 8s/epoch - 96us/sample
Epoch 48/85
84077/84077 - 8s - loss: 2.0329e-04 - val_loss: 1.8630e-04 - 8s/epoch - 96us/sample
Epoch 49/85
84077/84077 - 8s - loss: 2.0321e-04 - val_loss: 1.8492e-04 - 8s/epoch - 97us/sample
Epoch 50/85
84077/84077 - 8s - loss: 2.0388e-04 - val_loss: 2.1959e-04 - 8s/epoch - 96us/sample
Epoch 51/85
84077/84077 - 8s - loss: 2.0348e-04 - val_loss: 1.8565e-04 - 8s/epoch - 96us/sample
Epoch 52/85
84077/84077 - 8s - loss: 2.0064e-04 - val_loss: 1.8190e-04 - 8s/epoch - 96us/sample
Epoch 53/85
84077/84077 - 8s - loss: 2.0007e-04 - val_loss: 1.8215e-04 - 8s/epoch - 96us/sample
Epoch 54/85
84077/84077 - 8s - loss: 2.0013e-04 - val_loss: 1.8228e-04 - 8s/epoch - 96us/sample
Epoch 55/85
84077/84077 - 8s - loss: 2.0007e-04 - val_loss: 1.8036e-04 - 8s/epoch - 96us/sample
Epoch 56/85
84077/84077 - 8s - loss: 1.9922e-04 - val_loss: 1.8330e-04 - 8s/epoch - 96us/sample
Epoch 57/85
84077/84077 - 8s - loss: 1.9809e-04 - val_loss: 1.8044e-04 - 8s/epoch - 97us/sample
Epoch 58/85
84077/84077 - 8s - loss: 1.9735e-04 - val_loss: 1.8123e-04 - 8s/epoch - 97us/sample
Epoch 59/85
84077/84077 - 8s - loss: 1.9707e-04 - val_loss: 1.7834e-04 - 8s/epoch - 96us/sample
Epoch 60/85
84077/84077 - 8s - loss: 1.9648e-04 - val_loss: 1.8078e-04 - 8s/epoch - 96us/sample
Epoch 61/85
84077/84077 - 8s - loss: 1.9704e-04 - val_loss: 1.8977e-04 - 8s/epoch - 96us/sample
Epoch 62/85
84077/84077 - 8s - loss: 1.9554e-04 - val_loss: 1.7947e-04 - 8s/epoch - 96us/sample
Epoch 63/85
84077/84077 - 8s - loss: 1.9589e-04 - val_loss: 1.7925e-04 - 8s/epoch - 96us/sample
Epoch 64/85
84077/84077 - 8s - loss: 1.9410e-04 - val_loss: 1.8274e-04 - 8s/epoch - 96us/sample
Epoch 65/85
84077/84077 - 8s - loss: 1.9399e-04 - val_loss: 1.7515e-04 - 8s/epoch - 96us/sample
Epoch 66/85
84077/84077 - 8s - loss: 1.9343e-04 - val_loss: 1.7509e-04 - 8s/epoch - 97us/sample
Epoch 67/85
84077/84077 - 8s - loss: 1.9313e-04 - val_loss: 1.7657e-04 - 8s/epoch - 96us/sample
Epoch 68/85
84077/84077 - 8s - loss: 1.9279e-04 - val_loss: 1.7663e-04 - 8s/epoch - 96us/sample
Epoch 69/85
84077/84077 - 8s - loss: 1.9139e-04 - val_loss: 1.7590e-04 - 8s/epoch - 96us/sample
Epoch 70/85
84077/84077 - 8s - loss: 1.9128e-04 - val_loss: 1.7418e-04 - 8s/epoch - 96us/sample
Epoch 71/85
84077/84077 - 8s - loss: 1.9143e-04 - val_loss: 1.7483e-04 - 8s/epoch - 96us/sample
Epoch 72/85
84077/84077 - 8s - loss: 1.9006e-04 - val_loss: 1.7749e-04 - 8s/epoch - 96us/sample
Epoch 73/85
84077/84077 - 8s - loss: 1.9083e-04 - val_loss: 1.7496e-04 - 8s/epoch - 96us/sample
Epoch 74/85
84077/84077 - 8s - loss: 1.8973e-04 - val_loss: 1.7460e-04 - 8s/epoch - 97us/sample
Epoch 75/85
84077/84077 - 8s - loss: 1.8908e-04 - val_loss: 1.7601e-04 - 8s/epoch - 96us/sample
Epoch 76/85
84077/84077 - 8s - loss: 1.9192e-04 - val_loss: 1.7255e-04 - 8s/epoch - 96us/sample
Epoch 77/85
84077/84077 - 8s - loss: 1.8845e-04 - val_loss: 1.7379e-04 - 8s/epoch - 96us/sample
Epoch 78/85
84077/84077 - 8s - loss: 1.9008e-04 - val_loss: 1.7684e-04 - 8s/epoch - 96us/sample
Epoch 79/85
84077/84077 - 8s - loss: 1.8941e-04 - val_loss: 1.7565e-04 - 8s/epoch - 96us/sample
Epoch 80/85
84077/84077 - 8s - loss: 1.8943e-04 - val_loss: 1.7280e-04 - 8s/epoch - 96us/sample
Epoch 81/85
84077/84077 - 8s - loss: 1.8766e-04 - val_loss: 1.7201e-04 - 8s/epoch - 96us/sample
Epoch 82/85
84077/84077 - 8s - loss: 1.8724e-04 - val_loss: 1.7413e-04 - 8s/epoch - 97us/sample
Epoch 83/85
84077/84077 - 8s - loss: 1.8660e-04 - val_loss: 1.7307e-04 - 8s/epoch - 96us/sample
Epoch 84/85
84077/84077 - 8s - loss: 1.9101e-04 - val_loss: 1.7251e-04 - 8s/epoch - 96us/sample
Epoch 85/85
84077/84077 - 8s - loss: 1.8602e-04 - val_loss: 1.6999e-04 - 8s/epoch - 96us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00016998990549511723
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 17:14:59.762282: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_41/outputlayer/BiasAdd' id:52390 op device:{requested: '', assigned: ''} def:{{{node decoder_model_41/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_41/outputlayer/MatMul, decoder_model_41/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.020477067623400146
cosine 0.020229488831824332
MAE: 0.002274046993057673
RMSE: 0.008180139914152475
r2: 0.9480036704110387
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 64, 85, 0.001, 0.2, 188, 0.0001860214399642577, 0.00016998990549511723, 0.020477067623400146, 0.020229488831824332, 0.002274046993057673, 0.008180139914152475, 0.9480036704110387, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 80 0.001 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_126 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_126 (ReLU)               (None, 1603)         0           ['batch_normalization_126[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-11 17:15:17.991970: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_126/beta/Assign' id:53266 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_126/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_126/beta, batch_normalization_126/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 17:15:34.547136: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_42/mul' id:53674 op device:{requested: '', assigned: ''} def:{{{node loss_42/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_42/mul/x, loss_42/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0060 - val_loss: 0.0041 - 24s/epoch - 280us/sample
Epoch 2/80
84077/84077 - 13s - loss: 0.0016 - val_loss: 887.1161 - 13s/epoch - 153us/sample
Epoch 3/80
84077/84077 - 13s - loss: 0.0022 - val_loss: 0.0011 - 13s/epoch - 153us/sample
Epoch 4/80
84077/84077 - 13s - loss: 0.0019 - val_loss: 0.0013 - 13s/epoch - 153us/sample
Epoch 5/80
84077/84077 - 13s - loss: 0.0047 - val_loss: 0.0019 - 13s/epoch - 153us/sample
Epoch 6/80
84077/84077 - 13s - loss: 0.0013 - val_loss: 0.0010 - 13s/epoch - 153us/sample
Epoch 7/80
84077/84077 - 13s - loss: 0.0016 - val_loss: 0.0018 - 13s/epoch - 153us/sample
Epoch 8/80
84077/84077 - 13s - loss: 0.0013 - val_loss: 8.7928e-04 - 13s/epoch - 154us/sample
Epoch 9/80
84077/84077 - 13s - loss: 0.0012 - val_loss: 8.1969e-04 - 13s/epoch - 153us/sample
Epoch 10/80
84077/84077 - 13s - loss: 8.5541e-04 - val_loss: 7.7793e-04 - 13s/epoch - 153us/sample
Epoch 11/80
84077/84077 - 13s - loss: 8.1380e-04 - val_loss: 7.4729e-04 - 13s/epoch - 153us/sample
Epoch 12/80
84077/84077 - 13s - loss: 8.3078e-04 - val_loss: 7.3050e-04 - 13s/epoch - 153us/sample
Epoch 13/80
84077/84077 - 13s - loss: 7.8090e-04 - val_loss: 7.0744e-04 - 13s/epoch - 155us/sample
Epoch 14/80
84077/84077 - 13s - loss: 7.5317e-04 - val_loss: 7.2527e-04 - 13s/epoch - 154us/sample
Epoch 15/80
84077/84077 - 13s - loss: 7.3016e-04 - val_loss: 6.8573e-04 - 13s/epoch - 153us/sample
Epoch 16/80
84077/84077 - 13s - loss: 7.1614e-04 - val_loss: 6.7934e-04 - 13s/epoch - 153us/sample
Epoch 17/80
84077/84077 - 13s - loss: 6.9292e-04 - val_loss: 6.4070e-04 - 13s/epoch - 153us/sample
Epoch 18/80
84077/84077 - 13s - loss: 6.6683e-04 - val_loss: 6.1016e-04 - 13s/epoch - 153us/sample
Epoch 19/80
84077/84077 - 13s - loss: 6.3830e-04 - val_loss: 5.8798e-04 - 13s/epoch - 155us/sample
Epoch 20/80
84077/84077 - 13s - loss: 6.1870e-04 - val_loss: 5.6603e-04 - 13s/epoch - 158us/sample
Epoch 21/80
84077/84077 - 13s - loss: 6.0070e-04 - val_loss: 5.4955e-04 - 13s/epoch - 156us/sample
Epoch 22/80
84077/84077 - 13s - loss: 5.8258e-04 - val_loss: 5.3053e-04 - 13s/epoch - 155us/sample
Epoch 23/80
84077/84077 - 13s - loss: 5.6668e-04 - val_loss: 5.2042e-04 - 13s/epoch - 156us/sample
Epoch 24/80
84077/84077 - 13s - loss: 5.5623e-04 - val_loss: 5.0985e-04 - 13s/epoch - 157us/sample
Epoch 25/80
84077/84077 - 13s - loss: 5.4644e-04 - val_loss: 5.0245e-04 - 13s/epoch - 156us/sample
Epoch 26/80
84077/84077 - 13s - loss: 5.3929e-04 - val_loss: 4.9420e-04 - 13s/epoch - 156us/sample
Epoch 27/80
84077/84077 - 13s - loss: 5.3397e-04 - val_loss: 4.9013e-04 - 13s/epoch - 156us/sample
Epoch 28/80
84077/84077 - 13s - loss: 5.2864e-04 - val_loss: 4.8783e-04 - 13s/epoch - 156us/sample
Epoch 29/80
84077/84077 - 13s - loss: 5.2332e-04 - val_loss: 4.8204e-04 - 13s/epoch - 156us/sample
Epoch 30/80
84077/84077 - 13s - loss: 5.1905e-04 - val_loss: 4.8147e-04 - 13s/epoch - 157us/sample
Epoch 31/80
84077/84077 - 13s - loss: 5.1530e-04 - val_loss: 4.7841e-04 - 13s/epoch - 156us/sample
Epoch 32/80
84077/84077 - 13s - loss: 5.1101e-04 - val_loss: 4.7333e-04 - 13s/epoch - 155us/sample
Epoch 33/80
84077/84077 - 13s - loss: 5.0830e-04 - val_loss: 4.7096e-04 - 13s/epoch - 156us/sample
Epoch 34/80
84077/84077 - 13s - loss: 5.0455e-04 - val_loss: 4.7092e-04 - 13s/epoch - 156us/sample
Epoch 35/80
84077/84077 - 13s - loss: 5.0165e-04 - val_loss: 4.6696e-04 - 13s/epoch - 157us/sample
Epoch 36/80
84077/84077 - 13s - loss: 4.9876e-04 - val_loss: 4.6440e-04 - 13s/epoch - 156us/sample
Epoch 37/80
84077/84077 - 13s - loss: 4.9575e-04 - val_loss: 4.6089e-04 - 13s/epoch - 156us/sample
Epoch 38/80
84077/84077 - 13s - loss: 4.9370e-04 - val_loss: 4.6132e-04 - 13s/epoch - 156us/sample
Epoch 39/80
84077/84077 - 13s - loss: 4.9051e-04 - val_loss: 4.5956e-04 - 13s/epoch - 156us/sample
Epoch 40/80
84077/84077 - 13s - loss: 4.8862e-04 - val_loss: 4.6053e-04 - 13s/epoch - 156us/sample
Epoch 41/80
84077/84077 - 13s - loss: 4.8714e-04 - val_loss: 4.5640e-04 - 13s/epoch - 158us/sample
Epoch 42/80
84077/84077 - 13s - loss: 4.8603e-04 - val_loss: 4.5587e-04 - 13s/epoch - 155us/sample
Epoch 43/80
84077/84077 - 13s - loss: 4.8353e-04 - val_loss: 4.5435e-04 - 13s/epoch - 155us/sample
Epoch 44/80
84077/84077 - 13s - loss: 4.8273e-04 - val_loss: 4.5276e-04 - 13s/epoch - 156us/sample
Epoch 45/80
84077/84077 - 13s - loss: 4.8116e-04 - val_loss: 4.5167e-04 - 13s/epoch - 156us/sample
Epoch 46/80
84077/84077 - 13s - loss: 4.8052e-04 - val_loss: 4.5145e-04 - 13s/epoch - 156us/sample
Epoch 47/80
84077/84077 - 13s - loss: 4.7993e-04 - val_loss: 4.5275e-04 - 13s/epoch - 156us/sample
Epoch 48/80
84077/84077 - 13s - loss: 4.7932e-04 - val_loss: 4.4944e-04 - 13s/epoch - 156us/sample
Epoch 49/80
84077/84077 - 13s - loss: 4.7794e-04 - val_loss: 4.5090e-04 - 13s/epoch - 156us/sample
Epoch 50/80
84077/84077 - 13s - loss: 4.7773e-04 - val_loss: 4.4848e-04 - 13s/epoch - 156us/sample
Epoch 51/80
84077/84077 - 13s - loss: 4.7712e-04 - val_loss: 4.4779e-04 - 13s/epoch - 156us/sample
Epoch 52/80
84077/84077 - 13s - loss: 4.7653e-04 - val_loss: 4.4822e-04 - 13s/epoch - 158us/sample
Epoch 53/80
84077/84077 - 13s - loss: 4.7675e-04 - val_loss: 4.4751e-04 - 13s/epoch - 156us/sample
Epoch 54/80
84077/84077 - 13s - loss: 4.7620e-04 - val_loss: 4.4639e-04 - 13s/epoch - 156us/sample
Epoch 55/80
84077/84077 - 13s - loss: 4.7476e-04 - val_loss: 4.4639e-04 - 13s/epoch - 156us/sample
Epoch 56/80
84077/84077 - 13s - loss: 4.7388e-04 - val_loss: 4.4877e-04 - 13s/epoch - 156us/sample
Epoch 57/80
84077/84077 - 13s - loss: 4.7295e-04 - val_loss: 4.4397e-04 - 13s/epoch - 157us/sample
Epoch 58/80
84077/84077 - 13s - loss: 4.7205e-04 - val_loss: 4.4620e-04 - 13s/epoch - 156us/sample
Epoch 59/80
84077/84077 - 13s - loss: 4.7123e-04 - val_loss: 4.4591e-04 - 13s/epoch - 156us/sample
Epoch 60/80
84077/84077 - 13s - loss: 4.7172e-04 - val_loss: 4.4472e-04 - 13s/epoch - 156us/sample
Epoch 61/80
84077/84077 - 13s - loss: 4.7044e-04 - val_loss: 4.4276e-04 - 13s/epoch - 156us/sample
Epoch 62/80
84077/84077 - 13s - loss: 4.7081e-04 - val_loss: 4.4609e-04 - 13s/epoch - 157us/sample
Epoch 63/80
84077/84077 - 13s - loss: 4.6983e-04 - val_loss: 4.4625e-04 - 13s/epoch - 156us/sample
Epoch 64/80
84077/84077 - 13s - loss: 4.6992e-04 - val_loss: 4.4301e-04 - 13s/epoch - 156us/sample
Epoch 65/80
84077/84077 - 13s - loss: 4.6932e-04 - val_loss: 4.4285e-04 - 13s/epoch - 156us/sample
Epoch 66/80
84077/84077 - 13s - loss: 4.6910e-04 - val_loss: 4.4246e-04 - 13s/epoch - 156us/sample
Epoch 67/80
84077/84077 - 13s - loss: 4.6813e-04 - val_loss: 4.4431e-04 - 13s/epoch - 156us/sample
Epoch 68/80
84077/84077 - 13s - loss: 4.6760e-04 - val_loss: 4.4126e-04 - 13s/epoch - 158us/sample
Epoch 69/80
84077/84077 - 13s - loss: 4.6743e-04 - val_loss: 4.4202e-04 - 13s/epoch - 156us/sample
Epoch 70/80
84077/84077 - 13s - loss: 4.6839e-04 - val_loss: 4.5029e-04 - 13s/epoch - 156us/sample
Epoch 71/80
84077/84077 - 13s - loss: 4.6766e-04 - val_loss: 4.4304e-04 - 13s/epoch - 156us/sample
Epoch 72/80
84077/84077 - 13s - loss: 4.6731e-04 - val_loss: 4.4150e-04 - 13s/epoch - 156us/sample
Epoch 73/80
84077/84077 - 13s - loss: 4.6763e-04 - val_loss: 4.4366e-04 - 13s/epoch - 157us/sample
Epoch 74/80
84077/84077 - 13s - loss: 4.6714e-04 - val_loss: 4.4041e-04 - 13s/epoch - 157us/sample
Epoch 75/80
84077/84077 - 13s - loss: 4.6617e-04 - val_loss: 4.3976e-04 - 13s/epoch - 156us/sample
Epoch 76/80
84077/84077 - 13s - loss: 4.6565e-04 - val_loss: 4.4030e-04 - 13s/epoch - 156us/sample
Epoch 77/80
84077/84077 - 13s - loss: 4.6547e-04 - val_loss: 4.4181e-04 - 13s/epoch - 156us/sample
Epoch 78/80
84077/84077 - 13s - loss: 4.6491e-04 - val_loss: 4.3965e-04 - 13s/epoch - 156us/sample
Epoch 79/80
84077/84077 - 13s - loss: 4.6258e-04 - val_loss: 4.3681e-04 - 13s/epoch - 157us/sample
Epoch 80/80
84077/84077 - 13s - loss: 4.6046e-04 - val_loss: 4.3970e-04 - 13s/epoch - 156us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00043970169943644303
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 17:32:51.753229: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_42/outputlayer/BiasAdd' id:53645 op device:{requested: '', assigned: ''} def:{{{node decoder_model_42/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_42/outputlayer/MatMul, decoder_model_42/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10477710790250448
cosine 0.10342273973475176
MAE: 0.0036222326404727755
RMSE: 0.018921709630565428
r2: 0.7203289226368319
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 32, 80, 0.001, 0.2, 188, 0.0004604575857043958, 0.00043970169943644303, 0.10477710790250448, 0.10342273973475176, 0.0036222326404727755, 0.018921709630565428, 0.7203289226368319, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 85 0.0012000000000000001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_129 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_129 (ReLU)               (None, 1697)         0           ['batch_normalization_129[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 17:33:10.401679: W tensorflow/c/c_api.cc:291] Operation '{name:'training_86/Adam/bottleneck_zlog_43/bias/m/Assign' id:55427 op device:{requested: '', assigned: ''} def:{{{node training_86/Adam/bottleneck_zlog_43/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_86/Adam/bottleneck_zlog_43/bias/m, training_86/Adam/bottleneck_zlog_43/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 17:33:27.418959: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_43/mul' id:54929 op device:{requested: '', assigned: ''} def:{{{node loss_43/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_43/mul/x, loss_43/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0061 - val_loss: 0.0017 - 24s/epoch - 289us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.0022 - val_loss: 0.0021 - 13s/epoch - 158us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0013 - val_loss: 0.0015 - 13s/epoch - 156us/sample
Epoch 4/85
84077/84077 - 13s - loss: 0.0010 - val_loss: 7.9804e-04 - 13s/epoch - 157us/sample
Epoch 5/85
84077/84077 - 13s - loss: 8.3505e-04 - val_loss: 6.8936e-04 - 13s/epoch - 157us/sample
Epoch 6/85
84077/84077 - 13s - loss: 6.7023e-04 - val_loss: 6.8512e-04 - 13s/epoch - 156us/sample
Epoch 7/85
84077/84077 - 13s - loss: 5.8201e-04 - val_loss: 4.9371e-04 - 13s/epoch - 157us/sample
Epoch 8/85
84077/84077 - 13s - loss: 5.2992e-04 - val_loss: 4.7021e-04 - 13s/epoch - 157us/sample
Epoch 9/85
84077/84077 - 13s - loss: 4.8708e-04 - val_loss: 4.2651e-04 - 13s/epoch - 157us/sample
Epoch 10/85
84077/84077 - 13s - loss: 4.5679e-04 - val_loss: 4.1422e-04 - 13s/epoch - 156us/sample
Epoch 11/85
84077/84077 - 13s - loss: 4.3200e-04 - val_loss: 3.9014e-04 - 13s/epoch - 157us/sample
Epoch 12/85
84077/84077 - 13s - loss: 4.1316e-04 - val_loss: 3.6891e-04 - 13s/epoch - 156us/sample
Epoch 13/85
84077/84077 - 13s - loss: 3.9294e-04 - val_loss: 3.7004e-04 - 13s/epoch - 157us/sample
Epoch 14/85
84077/84077 - 13s - loss: 3.8480e-04 - val_loss: 3.5344e-04 - 13s/epoch - 158us/sample
Epoch 15/85
84077/84077 - 13s - loss: 3.7021e-04 - val_loss: 3.4419e-04 - 13s/epoch - 157us/sample
Epoch 16/85
84077/84077 - 13s - loss: 3.5787e-04 - val_loss: 3.2579e-04 - 13s/epoch - 157us/sample
Epoch 17/85
84077/84077 - 13s - loss: 3.4877e-04 - val_loss: 3.2235e-04 - 13s/epoch - 156us/sample
Epoch 18/85
84077/84077 - 13s - loss: 3.4180e-04 - val_loss: 3.1780e-04 - 13s/epoch - 156us/sample
Epoch 19/85
84077/84077 - 13s - loss: 3.3280e-04 - val_loss: 3.0862e-04 - 13s/epoch - 157us/sample
Epoch 20/85
84077/84077 - 13s - loss: 3.2528e-04 - val_loss: 2.9800e-04 - 13s/epoch - 158us/sample
Epoch 21/85
84077/84077 - 13s - loss: 3.2076e-04 - val_loss: 3.0143e-04 - 13s/epoch - 157us/sample
Epoch 22/85
84077/84077 - 13s - loss: 3.1479e-04 - val_loss: 2.9243e-04 - 13s/epoch - 157us/sample
Epoch 23/85
84077/84077 - 13s - loss: 3.1029e-04 - val_loss: 2.9896e-04 - 13s/epoch - 156us/sample
Epoch 24/85
84077/84077 - 13s - loss: 3.0379e-04 - val_loss: 2.8442e-04 - 13s/epoch - 156us/sample
Epoch 25/85
84077/84077 - 13s - loss: 3.0163e-04 - val_loss: 2.8602e-04 - 13s/epoch - 158us/sample
Epoch 26/85
84077/84077 - 13s - loss: 2.9621e-04 - val_loss: 2.8650e-04 - 13s/epoch - 157us/sample
Epoch 27/85
84077/84077 - 13s - loss: 2.9387e-04 - val_loss: 2.8346e-04 - 13s/epoch - 155us/sample
Epoch 28/85
84077/84077 - 13s - loss: 2.9363e-04 - val_loss: 2.7478e-04 - 13s/epoch - 154us/sample
Epoch 29/85
84077/84077 - 13s - loss: 2.8809e-04 - val_loss: 2.7638e-04 - 13s/epoch - 154us/sample
Epoch 30/85
84077/84077 - 13s - loss: 2.8871e-04 - val_loss: 2.7691e-04 - 13s/epoch - 155us/sample
Epoch 31/85
84077/84077 - 13s - loss: 2.8189e-04 - val_loss: 2.7312e-04 - 13s/epoch - 156us/sample
Epoch 32/85
84077/84077 - 13s - loss: 2.8050e-04 - val_loss: 2.6445e-04 - 13s/epoch - 156us/sample
Epoch 33/85
84077/84077 - 13s - loss: 2.7796e-04 - val_loss: 2.9072e-04 - 13s/epoch - 156us/sample
Epoch 34/85
84077/84077 - 13s - loss: 2.7566e-04 - val_loss: 2.6240e-04 - 13s/epoch - 155us/sample
Epoch 35/85
84077/84077 - 13s - loss: 2.7463e-04 - val_loss: 2.6762e-04 - 13s/epoch - 154us/sample
Epoch 36/85
84077/84077 - 13s - loss: 2.7035e-04 - val_loss: 2.6079e-04 - 13s/epoch - 156us/sample
Epoch 37/85
84077/84077 - 13s - loss: 2.7226e-04 - val_loss: 2.5768e-04 - 13s/epoch - 155us/sample
Epoch 38/85
84077/84077 - 13s - loss: 2.6900e-04 - val_loss: 2.5388e-04 - 13s/epoch - 154us/sample
Epoch 39/85
84077/84077 - 13s - loss: 2.6745e-04 - val_loss: 2.6001e-04 - 13s/epoch - 154us/sample
Epoch 40/85
84077/84077 - 13s - loss: 2.6714e-04 - val_loss: 2.5135e-04 - 13s/epoch - 154us/sample
Epoch 41/85
84077/84077 - 13s - loss: 2.6394e-04 - val_loss: 2.4874e-04 - 13s/epoch - 154us/sample
Epoch 42/85
84077/84077 - 13s - loss: 2.6336e-04 - val_loss: 2.4867e-04 - 13s/epoch - 156us/sample
Epoch 43/85
84077/84077 - 13s - loss: 2.6146e-04 - val_loss: 2.5457e-04 - 13s/epoch - 154us/sample
Epoch 44/85
84077/84077 - 13s - loss: 2.6274e-04 - val_loss: 2.4562e-04 - 13s/epoch - 154us/sample
Epoch 45/85
84077/84077 - 13s - loss: 2.6099e-04 - val_loss: 2.4523e-04 - 13s/epoch - 154us/sample
Epoch 46/85
84077/84077 - 13s - loss: 2.5937e-04 - val_loss: 2.4907e-04 - 13s/epoch - 154us/sample
Epoch 47/85
84077/84077 - 13s - loss: 2.5995e-04 - val_loss: 2.5135e-04 - 13s/epoch - 154us/sample
Epoch 48/85
84077/84077 - 13s - loss: 2.5668e-04 - val_loss: 2.4541e-04 - 13s/epoch - 155us/sample
Epoch 49/85
84077/84077 - 13s - loss: 2.5395e-04 - val_loss: 2.4172e-04 - 13s/epoch - 154us/sample
Epoch 50/85
84077/84077 - 13s - loss: 2.5387e-04 - val_loss: 2.3897e-04 - 13s/epoch - 154us/sample
Epoch 51/85
84077/84077 - 13s - loss: 2.5225e-04 - val_loss: 2.3833e-04 - 13s/epoch - 154us/sample
Epoch 52/85
84077/84077 - 13s - loss: 2.5059e-04 - val_loss: 2.3784e-04 - 13s/epoch - 154us/sample
Epoch 53/85
84077/84077 - 13s - loss: 2.5085e-04 - val_loss: 2.3891e-04 - 13s/epoch - 154us/sample
Epoch 54/85
84077/84077 - 13s - loss: 2.4824e-04 - val_loss: 2.3588e-04 - 13s/epoch - 156us/sample
Epoch 55/85
84077/84077 - 13s - loss: 2.4886e-04 - val_loss: 2.3756e-04 - 13s/epoch - 154us/sample
Epoch 56/85
84077/84077 - 13s - loss: 2.5006e-04 - val_loss: 2.3270e-04 - 13s/epoch - 154us/sample
Epoch 57/85
84077/84077 - 13s - loss: 2.4653e-04 - val_loss: 2.3454e-04 - 13s/epoch - 153us/sample
Epoch 58/85
84077/84077 - 13s - loss: 2.4636e-04 - val_loss: 2.3256e-04 - 13s/epoch - 154us/sample
Epoch 59/85
84077/84077 - 13s - loss: 2.4666e-04 - val_loss: 2.3618e-04 - 13s/epoch - 155us/sample
Epoch 60/85
84077/84077 - 13s - loss: 2.4374e-04 - val_loss: 2.2892e-04 - 13s/epoch - 154us/sample
Epoch 61/85
84077/84077 - 13s - loss: 2.4392e-04 - val_loss: 2.3332e-04 - 13s/epoch - 154us/sample
Epoch 62/85
84077/84077 - 13s - loss: 2.4398e-04 - val_loss: 2.3045e-04 - 13s/epoch - 154us/sample
Epoch 63/85
84077/84077 - 13s - loss: 2.4639e-04 - val_loss: 2.3201e-04 - 13s/epoch - 153us/sample
Epoch 64/85
84077/84077 - 13s - loss: 2.4574e-04 - val_loss: 2.3513e-04 - 13s/epoch - 154us/sample
Epoch 65/85
84077/84077 - 13s - loss: 2.4301e-04 - val_loss: 2.2731e-04 - 13s/epoch - 155us/sample
Epoch 66/85
84077/84077 - 13s - loss: 2.4082e-04 - val_loss: 2.2851e-04 - 13s/epoch - 154us/sample
Epoch 67/85
84077/84077 - 13s - loss: 2.3923e-04 - val_loss: 2.2824e-04 - 13s/epoch - 153us/sample
Epoch 68/85
84077/84077 - 13s - loss: 2.3977e-04 - val_loss: 2.2768e-04 - 13s/epoch - 154us/sample
Epoch 69/85
84077/84077 - 13s - loss: 2.4031e-04 - val_loss: 2.2522e-04 - 13s/epoch - 153us/sample
Epoch 70/85
84077/84077 - 13s - loss: 2.4010e-04 - val_loss: 2.3193e-04 - 13s/epoch - 153us/sample
Epoch 71/85
84077/84077 - 13s - loss: 2.3956e-04 - val_loss: 2.2671e-04 - 13s/epoch - 156us/sample
Epoch 72/85
84077/84077 - 13s - loss: 2.4037e-04 - val_loss: 2.2925e-04 - 13s/epoch - 154us/sample
Epoch 73/85
84077/84077 - 13s - loss: 2.4106e-04 - val_loss: 2.2563e-04 - 13s/epoch - 154us/sample
Epoch 74/85
84077/84077 - 13s - loss: 2.3757e-04 - val_loss: 2.2212e-04 - 13s/epoch - 153us/sample
Epoch 75/85
84077/84077 - 13s - loss: 2.3569e-04 - val_loss: 2.2152e-04 - 13s/epoch - 154us/sample
Epoch 76/85
84077/84077 - 13s - loss: 2.4131e-04 - val_loss: 2.2019e-04 - 13s/epoch - 154us/sample
Epoch 77/85
84077/84077 - 13s - loss: 2.3579e-04 - val_loss: 2.2502e-04 - 13s/epoch - 155us/sample
Epoch 78/85
84077/84077 - 13s - loss: 2.3581e-04 - val_loss: 2.2187e-04 - 13s/epoch - 154us/sample
Epoch 79/85
84077/84077 - 13s - loss: 2.3675e-04 - val_loss: 2.2860e-04 - 13s/epoch - 154us/sample
Epoch 80/85
84077/84077 - 13s - loss: 2.3498e-04 - val_loss: 2.2042e-04 - 13s/epoch - 154us/sample
Epoch 81/85
84077/84077 - 13s - loss: 2.3367e-04 - val_loss: 2.2047e-04 - 13s/epoch - 154us/sample
Epoch 82/85
84077/84077 - 13s - loss: 2.3358e-04 - val_loss: 2.2381e-04 - 13s/epoch - 154us/sample
Epoch 83/85
84077/84077 - 13s - loss: 2.3169e-04 - val_loss: 2.1853e-04 - 13s/epoch - 155us/sample
Epoch 84/85
84077/84077 - 13s - loss: 2.3357e-04 - val_loss: 2.1795e-04 - 13s/epoch - 154us/sample
Epoch 85/85
84077/84077 - 13s - loss: 2.3155e-04 - val_loss: 2.1881e-04 - 13s/epoch - 154us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002188087563417775
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 17:51:46.258824: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_43/outputlayer/BiasAdd' id:54900 op device:{requested: '', assigned: ''} def:{{{node decoder_model_43/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_43/outputlayer/MatMul, decoder_model_43/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02763344972327731
cosine 0.027302647682264716
MAE: 0.0021750978270234605
RMSE: 0.010479189283833183
r2: 0.9143006265772599
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 32, 85, 0.0012000000000000001, 0.2, 188, 0.00023155488259115297, 0.0002188087563417775, 0.02763344972327731, 0.027302647682264716, 0.0021750978270234605, 0.010479189283833183, 0.9143006265772599, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 5
Fitness    = 512.1418577388646
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 512.1418577388646.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646]
[1.6999999999999997 85 0.001 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_132 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_132 (ReLU)               (None, 1603)         0           ['batch_normalization_132[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 17:52:05.194469: W tensorflow/c/c_api.cc:291] Operation '{name:'training_88/Adam/dense_dec0_44/kernel/v/Assign' id:56855 op device:{requested: '', assigned: ''} def:{{{node training_88/Adam/dense_dec0_44/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_88/Adam/dense_dec0_44/kernel/v, training_88/Adam/dense_dec0_44/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 17:52:17.689370: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_44/mul' id:56191 op device:{requested: '', assigned: ''} def:{{{node loss_44/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_44/mul/x, loss_44/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0065 - val_loss: 9.2179e-04 - 19s/epoch - 232us/sample
Epoch 2/85
84077/84077 - 8s - loss: 8.0596e-04 - val_loss: 8.2089e-04 - 8s/epoch - 99us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0018 - val_loss: 0.0012 - 8s/epoch - 99us/sample
Epoch 4/85
84077/84077 - 8s - loss: 8.3493e-04 - val_loss: 7.2897e-04 - 8s/epoch - 99us/sample
Epoch 5/85
84077/84077 - 8s - loss: 6.2709e-04 - val_loss: 6.0284e-04 - 8s/epoch - 99us/sample
Epoch 6/85
84077/84077 - 8s - loss: 5.4507e-04 - val_loss: 4.7751e-04 - 8s/epoch - 99us/sample
Epoch 7/85
84077/84077 - 8s - loss: 4.6381e-04 - val_loss: 4.4239e-04 - 8s/epoch - 99us/sample
Epoch 8/85
84077/84077 - 8s - loss: 4.0007e-04 - val_loss: 3.5174e-04 - 8s/epoch - 98us/sample
Epoch 9/85
84077/84077 - 8s - loss: 3.5215e-04 - val_loss: 3.1866e-04 - 8s/epoch - 100us/sample
Epoch 10/85
84077/84077 - 8s - loss: 3.1979e-04 - val_loss: 3.0259e-04 - 8s/epoch - 99us/sample
Epoch 11/85
84077/84077 - 8s - loss: 2.9230e-04 - val_loss: 2.6084e-04 - 8s/epoch - 99us/sample
Epoch 12/85
84077/84077 - 8s - loss: 2.6716e-04 - val_loss: 2.3493e-04 - 8s/epoch - 98us/sample
Epoch 13/85
84077/84077 - 8s - loss: 2.4647e-04 - val_loss: 2.1944e-04 - 8s/epoch - 99us/sample
Epoch 14/85
84077/84077 - 8s - loss: 2.3029e-04 - val_loss: 2.0664e-04 - 8s/epoch - 98us/sample
Epoch 15/85
84077/84077 - 8s - loss: 2.1739e-04 - val_loss: 1.8964e-04 - 8s/epoch - 99us/sample
Epoch 16/85
84077/84077 - 8s - loss: 2.0605e-04 - val_loss: 1.8252e-04 - 8s/epoch - 99us/sample
Epoch 17/85
84077/84077 - 8s - loss: 1.9650e-04 - val_loss: 1.7367e-04 - 8s/epoch - 100us/sample
Epoch 18/85
84077/84077 - 8s - loss: 1.8962e-04 - val_loss: 1.6908e-04 - 8s/epoch - 99us/sample
Epoch 19/85
84077/84077 - 8s - loss: 1.8356e-04 - val_loss: 1.6463e-04 - 8s/epoch - 99us/sample
Epoch 20/85
84077/84077 - 8s - loss: 1.7938e-04 - val_loss: 1.6075e-04 - 8s/epoch - 99us/sample
Epoch 21/85
84077/84077 - 8s - loss: 1.7499e-04 - val_loss: 1.5859e-04 - 8s/epoch - 99us/sample
Epoch 22/85
84077/84077 - 8s - loss: 1.7216e-04 - val_loss: 1.5633e-04 - 8s/epoch - 99us/sample
Epoch 23/85
84077/84077 - 8s - loss: 1.6893e-04 - val_loss: 1.5408e-04 - 8s/epoch - 99us/sample
Epoch 24/85
84077/84077 - 8s - loss: 1.6610e-04 - val_loss: 1.5251e-04 - 8s/epoch - 98us/sample
Epoch 25/85
84077/84077 - 8s - loss: 1.6415e-04 - val_loss: 1.5009e-04 - 8s/epoch - 100us/sample
Epoch 26/85
84077/84077 - 8s - loss: 1.6209e-04 - val_loss: 1.4883e-04 - 8s/epoch - 99us/sample
Epoch 27/85
84077/84077 - 8s - loss: 1.5987e-04 - val_loss: 1.4818e-04 - 8s/epoch - 99us/sample
Epoch 28/85
84077/84077 - 8s - loss: 1.5907e-04 - val_loss: 1.4770e-04 - 8s/epoch - 99us/sample
Epoch 29/85
84077/84077 - 8s - loss: 1.5781e-04 - val_loss: 1.4652e-04 - 8s/epoch - 99us/sample
Epoch 30/85
84077/84077 - 8s - loss: 1.5595e-04 - val_loss: 1.4303e-04 - 8s/epoch - 99us/sample
Epoch 31/85
84077/84077 - 8s - loss: 1.5450e-04 - val_loss: 1.4557e-04 - 8s/epoch - 99us/sample
Epoch 32/85
84077/84077 - 8s - loss: 1.5351e-04 - val_loss: 1.4378e-04 - 8s/epoch - 99us/sample
Epoch 33/85
84077/84077 - 8s - loss: 1.5257e-04 - val_loss: 1.4285e-04 - 8s/epoch - 100us/sample
Epoch 34/85
84077/84077 - 8s - loss: 1.5173e-04 - val_loss: 1.4031e-04 - 8s/epoch - 100us/sample
Epoch 35/85
84077/84077 - 8s - loss: 1.5085e-04 - val_loss: 1.4047e-04 - 8s/epoch - 99us/sample
Epoch 36/85
84077/84077 - 8s - loss: 1.4971e-04 - val_loss: 1.4124e-04 - 8s/epoch - 99us/sample
Epoch 37/85
84077/84077 - 8s - loss: 1.4909e-04 - val_loss: 1.3941e-04 - 8s/epoch - 99us/sample
Epoch 38/85
84077/84077 - 8s - loss: 1.4777e-04 - val_loss: 1.3922e-04 - 8s/epoch - 99us/sample
Epoch 39/85
84077/84077 - 8s - loss: 1.4732e-04 - val_loss: 1.3673e-04 - 8s/epoch - 99us/sample
Epoch 40/85
84077/84077 - 8s - loss: 1.4667e-04 - val_loss: 1.3840e-04 - 8s/epoch - 99us/sample
Epoch 41/85
84077/84077 - 8s - loss: 1.4571e-04 - val_loss: 1.3824e-04 - 8s/epoch - 99us/sample
Epoch 42/85
84077/84077 - 8s - loss: 1.4540e-04 - val_loss: 1.3714e-04 - 8s/epoch - 100us/sample
Epoch 43/85
84077/84077 - 8s - loss: 1.4485e-04 - val_loss: 1.3530e-04 - 8s/epoch - 99us/sample
Epoch 44/85
84077/84077 - 8s - loss: 1.4426e-04 - val_loss: 1.3587e-04 - 8s/epoch - 99us/sample
Epoch 45/85
84077/84077 - 8s - loss: 1.4351e-04 - val_loss: 1.3580e-04 - 8s/epoch - 99us/sample
Epoch 46/85
84077/84077 - 8s - loss: 1.4321e-04 - val_loss: 1.3712e-04 - 8s/epoch - 99us/sample
Epoch 47/85
84077/84077 - 8s - loss: 1.4271e-04 - val_loss: 1.3512e-04 - 8s/epoch - 99us/sample
Epoch 48/85
84077/84077 - 8s - loss: 1.4230e-04 - val_loss: 1.3452e-04 - 8s/epoch - 99us/sample
Epoch 49/85
84077/84077 - 8s - loss: 1.4157e-04 - val_loss: 1.3366e-04 - 8s/epoch - 99us/sample
Epoch 50/85
84077/84077 - 8s - loss: 1.4143e-04 - val_loss: 1.3463e-04 - 8s/epoch - 100us/sample
Epoch 51/85
84077/84077 - 8s - loss: 1.4122e-04 - val_loss: 1.3354e-04 - 8s/epoch - 99us/sample
Epoch 52/85
84077/84077 - 8s - loss: 1.4084e-04 - val_loss: 1.3327e-04 - 8s/epoch - 99us/sample
Epoch 53/85
84077/84077 - 8s - loss: 1.4012e-04 - val_loss: 1.3271e-04 - 8s/epoch - 99us/sample
Epoch 54/85
84077/84077 - 8s - loss: 1.4007e-04 - val_loss: 1.3253e-04 - 8s/epoch - 99us/sample
Epoch 55/85
84077/84077 - 8s - loss: 1.3913e-04 - val_loss: 1.3223e-04 - 8s/epoch - 99us/sample
Epoch 56/85
84077/84077 - 8s - loss: 1.3928e-04 - val_loss: 1.3202e-04 - 8s/epoch - 99us/sample
Epoch 57/85
84077/84077 - 8s - loss: 1.3893e-04 - val_loss: 1.3252e-04 - 8s/epoch - 99us/sample
Epoch 58/85
84077/84077 - 8s - loss: 1.3820e-04 - val_loss: 1.3148e-04 - 8s/epoch - 100us/sample
Epoch 59/85
84077/84077 - 8s - loss: 1.3808e-04 - val_loss: 1.3097e-04 - 8s/epoch - 99us/sample
Epoch 60/85
84077/84077 - 8s - loss: 1.3832e-04 - val_loss: 1.3213e-04 - 8s/epoch - 98us/sample
Epoch 61/85
84077/84077 - 8s - loss: 1.3759e-04 - val_loss: 1.3218e-04 - 8s/epoch - 99us/sample
Epoch 62/85
84077/84077 - 8s - loss: 1.3715e-04 - val_loss: 1.3040e-04 - 8s/epoch - 99us/sample
Epoch 63/85
84077/84077 - 8s - loss: 1.3683e-04 - val_loss: 1.3192e-04 - 8s/epoch - 99us/sample
Epoch 64/85
84077/84077 - 8s - loss: 1.3640e-04 - val_loss: 1.2937e-04 - 8s/epoch - 99us/sample
Epoch 65/85
84077/84077 - 8s - loss: 1.3636e-04 - val_loss: 1.3021e-04 - 8s/epoch - 99us/sample
Epoch 66/85
84077/84077 - 8s - loss: 1.3627e-04 - val_loss: 1.3050e-04 - 8s/epoch - 100us/sample
Epoch 67/85
84077/84077 - 8s - loss: 1.3588e-04 - val_loss: 1.2964e-04 - 8s/epoch - 100us/sample
Epoch 68/85
84077/84077 - 8s - loss: 1.3598e-04 - val_loss: 1.2988e-04 - 8s/epoch - 99us/sample
Epoch 69/85
84077/84077 - 8s - loss: 1.3555e-04 - val_loss: 1.2981e-04 - 8s/epoch - 99us/sample
Epoch 70/85
84077/84077 - 8s - loss: 1.3493e-04 - val_loss: 1.2902e-04 - 8s/epoch - 99us/sample
Epoch 71/85
84077/84077 - 8s - loss: 1.3532e-04 - val_loss: 1.2957e-04 - 8s/epoch - 99us/sample
Epoch 72/85
84077/84077 - 8s - loss: 1.3477e-04 - val_loss: 1.2873e-04 - 8s/epoch - 99us/sample
Epoch 73/85
84077/84077 - 8s - loss: 1.3474e-04 - val_loss: 1.2883e-04 - 8s/epoch - 99us/sample
Epoch 74/85
84077/84077 - 8s - loss: 1.3462e-04 - val_loss: 1.2794e-04 - 8s/epoch - 99us/sample
Epoch 75/85
84077/84077 - 8s - loss: 1.3394e-04 - val_loss: 1.3010e-04 - 8s/epoch - 100us/sample
Epoch 76/85
84077/84077 - 8s - loss: 1.3392e-04 - val_loss: 1.2917e-04 - 8s/epoch - 99us/sample
Epoch 77/85
84077/84077 - 8s - loss: 1.3335e-04 - val_loss: 1.2763e-04 - 8s/epoch - 99us/sample
Epoch 78/85
84077/84077 - 8s - loss: 1.3376e-04 - val_loss: 1.2823e-04 - 8s/epoch - 99us/sample
Epoch 79/85
84077/84077 - 8s - loss: 1.3344e-04 - val_loss: 1.2773e-04 - 8s/epoch - 99us/sample
Epoch 80/85
84077/84077 - 8s - loss: 1.3306e-04 - val_loss: 1.2739e-04 - 8s/epoch - 99us/sample
Epoch 81/85
84077/84077 - 8s - loss: 1.3301e-04 - val_loss: 1.2812e-04 - 8s/epoch - 99us/sample
Epoch 82/85
84077/84077 - 8s - loss: 1.3276e-04 - val_loss: 1.2745e-04 - 8s/epoch - 99us/sample
Epoch 83/85
84077/84077 - 8s - loss: 1.3266e-04 - val_loss: 1.2720e-04 - 8s/epoch - 100us/sample
Epoch 84/85
84077/84077 - 8s - loss: 1.3252e-04 - val_loss: 1.2717e-04 - 8s/epoch - 99us/sample
Epoch 85/85
84077/84077 - 8s - loss: 1.3222e-04 - val_loss: 1.2596e-04 - 8s/epoch - 99us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012596103603333244
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 18:04:00.481233: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_44/outputlayer/BiasAdd' id:56155 op device:{requested: '', assigned: ''} def:{{{node decoder_model_44/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_44/outputlayer/MatMul, decoder_model_44/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030953201399277534
cosine 0.03057922868435803
MAE: 0.002209804434924891
RMSE: 0.010059171673407152
r2: 0.9212134396452564
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'logcosh', 64, 85, 0.001, 0.2, 188, 0.00013222476927181662, 0.00012596103603333244, 0.030953201399277534, 0.03057922868435803, 0.002209804434924891, 0.010059171673407152, 0.9212134396452564, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.0012000000000000001 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_135 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_135 (ReLU)               (None, 1603)         0           ['batch_normalization_135[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 18:04:19.558122: W tensorflow/c/c_api.cc:291] Operation '{name:'training_90/Adam/beta_1/Assign' id:57900 op device:{requested: '', assigned: ''} def:{{{node training_90/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_90/Adam/beta_1, training_90/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 18:04:31.967868: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_45/mul' id:57469 op device:{requested: '', assigned: ''} def:{{{node loss_45/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_45/mul/x, loss_45/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0074 - val_loss: 0.0017 - 20s/epoch - 232us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 100us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0011 - val_loss: 9.1926e-04 - 8s/epoch - 98us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0018 - 8s/epoch - 99us/sample
Epoch 5/85
84077/84077 - 8s - loss: 8.3640e-04 - val_loss: 6.7529e-04 - 8s/epoch - 99us/sample
Epoch 6/85
84077/84077 - 8s - loss: 6.7730e-04 - val_loss: 5.4985e-04 - 8s/epoch - 98us/sample
Epoch 7/85
84077/84077 - 8s - loss: 5.8575e-04 - val_loss: 5.1445e-04 - 8s/epoch - 99us/sample
Epoch 8/85
84077/84077 - 8s - loss: 4.9909e-04 - val_loss: 4.0366e-04 - 8s/epoch - 99us/sample
Epoch 9/85
84077/84077 - 8s - loss: 4.3426e-04 - val_loss: 3.5703e-04 - 8s/epoch - 98us/sample
Epoch 10/85
84077/84077 - 8s - loss: 3.9797e-04 - val_loss: 3.3543e-04 - 8s/epoch - 98us/sample
Epoch 11/85
84077/84077 - 8s - loss: 3.5481e-04 - val_loss: 3.0610e-04 - 8s/epoch - 100us/sample
Epoch 12/85
84077/84077 - 8s - loss: 4.5063e-04 - val_loss: 3.1238e-04 - 8s/epoch - 99us/sample
Epoch 13/85
84077/84077 - 8s - loss: 3.3381e-04 - val_loss: 2.8602e-04 - 8s/epoch - 99us/sample
Epoch 14/85
84077/84077 - 8s - loss: 3.1208e-04 - val_loss: 2.7533e-04 - 8s/epoch - 98us/sample
Epoch 15/85
84077/84077 - 8s - loss: 2.9797e-04 - val_loss: 2.6233e-04 - 8s/epoch - 98us/sample
Epoch 16/85
84077/84077 - 8s - loss: 2.8642e-04 - val_loss: 2.5190e-04 - 8s/epoch - 98us/sample
Epoch 17/85
84077/84077 - 8s - loss: 2.8143e-04 - val_loss: 2.4432e-04 - 8s/epoch - 99us/sample
Epoch 18/85
84077/84077 - 8s - loss: 2.6988e-04 - val_loss: 2.4072e-04 - 8s/epoch - 98us/sample
Epoch 19/85
84077/84077 - 8s - loss: 2.6208e-04 - val_loss: 2.3794e-04 - 8s/epoch - 99us/sample
Epoch 20/85
84077/84077 - 8s - loss: 2.6015e-04 - val_loss: 2.3517e-04 - 8s/epoch - 100us/sample
Epoch 21/85
84077/84077 - 8s - loss: 2.5263e-04 - val_loss: 2.2575e-04 - 8s/epoch - 99us/sample
Epoch 22/85
84077/84077 - 8s - loss: 2.4831e-04 - val_loss: 2.2451e-04 - 8s/epoch - 99us/sample
Epoch 23/85
84077/84077 - 8s - loss: 2.4444e-04 - val_loss: 2.2128e-04 - 8s/epoch - 98us/sample
Epoch 24/85
84077/84077 - 8s - loss: 2.3962e-04 - val_loss: 2.1668e-04 - 8s/epoch - 98us/sample
Epoch 25/85
84077/84077 - 8s - loss: 2.3789e-04 - val_loss: 2.1903e-04 - 8s/epoch - 99us/sample
Epoch 26/85
84077/84077 - 8s - loss: 2.3547e-04 - val_loss: 2.1285e-04 - 8s/epoch - 99us/sample
Epoch 27/85
84077/84077 - 8s - loss: 2.3481e-04 - val_loss: 2.1244e-04 - 8s/epoch - 98us/sample
Epoch 28/85
84077/84077 - 8s - loss: 2.3122e-04 - val_loss: 2.0933e-04 - 8s/epoch - 99us/sample
Epoch 29/85
84077/84077 - 8s - loss: 2.2628e-04 - val_loss: 2.1033e-04 - 8s/epoch - 99us/sample
Epoch 30/85
84077/84077 - 8s - loss: 2.2905e-04 - val_loss: 2.0477e-04 - 8s/epoch - 99us/sample
Epoch 31/85
84077/84077 - 8s - loss: 2.2356e-04 - val_loss: 2.0233e-04 - 8s/epoch - 98us/sample
Epoch 32/85
84077/84077 - 8s - loss: 2.2211e-04 - val_loss: 2.0253e-04 - 8s/epoch - 98us/sample
Epoch 33/85
84077/84077 - 8s - loss: 2.2009e-04 - val_loss: 2.0339e-04 - 8s/epoch - 98us/sample
Epoch 34/85
84077/84077 - 8s - loss: 2.2020e-04 - val_loss: 2.0000e-04 - 8s/epoch - 98us/sample
Epoch 35/85
84077/84077 - 8s - loss: 2.1834e-04 - val_loss: 1.9761e-04 - 8s/epoch - 98us/sample
Epoch 36/85
84077/84077 - 8s - loss: 2.1612e-04 - val_loss: 1.9702e-04 - 8s/epoch - 99us/sample
Epoch 37/85
84077/84077 - 8s - loss: 2.1527e-04 - val_loss: 2.0004e-04 - 8s/epoch - 99us/sample
Epoch 38/85
84077/84077 - 8s - loss: 2.1336e-04 - val_loss: 1.9711e-04 - 8s/epoch - 99us/sample
Epoch 39/85
84077/84077 - 8s - loss: 2.1469e-04 - val_loss: 1.9410e-04 - 8s/epoch - 98us/sample
Epoch 40/85
84077/84077 - 8s - loss: 2.1022e-04 - val_loss: 1.9254e-04 - 8s/epoch - 99us/sample
Epoch 41/85
84077/84077 - 8s - loss: 2.0937e-04 - val_loss: 1.8958e-04 - 8s/epoch - 99us/sample
Epoch 42/85
84077/84077 - 8s - loss: 2.0852e-04 - val_loss: 1.8867e-04 - 8s/epoch - 99us/sample
Epoch 43/85
84077/84077 - 8s - loss: 2.0757e-04 - val_loss: 1.8866e-04 - 8s/epoch - 98us/sample
Epoch 44/85
84077/84077 - 8s - loss: 2.0725e-04 - val_loss: 1.8651e-04 - 8s/epoch - 98us/sample
Epoch 45/85
84077/84077 - 8s - loss: 2.0604e-04 - val_loss: 1.8741e-04 - 8s/epoch - 99us/sample
Epoch 46/85
84077/84077 - 8s - loss: 2.0601e-04 - val_loss: 1.8703e-04 - 8s/epoch - 99us/sample
Epoch 47/85
84077/84077 - 8s - loss: 2.0540e-04 - val_loss: 1.8571e-04 - 8s/epoch - 99us/sample
Epoch 48/85
84077/84077 - 8s - loss: 2.0343e-04 - val_loss: 1.9020e-04 - 8s/epoch - 99us/sample
Epoch 49/85
84077/84077 - 8s - loss: 2.0281e-04 - val_loss: 1.8544e-04 - 8s/epoch - 98us/sample
Epoch 50/85
84077/84077 - 8s - loss: 2.0163e-04 - val_loss: 1.8352e-04 - 8s/epoch - 99us/sample
Epoch 51/85
84077/84077 - 8s - loss: 2.0130e-04 - val_loss: 1.8552e-04 - 8s/epoch - 99us/sample
Epoch 52/85
84077/84077 - 8s - loss: 2.0112e-04 - val_loss: 1.8438e-04 - 8s/epoch - 99us/sample
Epoch 53/85
84077/84077 - 8s - loss: 2.0067e-04 - val_loss: 1.8308e-04 - 8s/epoch - 99us/sample
Epoch 54/85
84077/84077 - 8s - loss: 1.9926e-04 - val_loss: 1.8574e-04 - 8s/epoch - 99us/sample
Epoch 55/85
84077/84077 - 8s - loss: 1.9939e-04 - val_loss: 1.8130e-04 - 8s/epoch - 99us/sample
Epoch 56/85
84077/84077 - 8s - loss: 1.9821e-04 - val_loss: 1.8185e-04 - 8s/epoch - 99us/sample
Epoch 57/85
84077/84077 - 8s - loss: 1.9737e-04 - val_loss: 1.8131e-04 - 8s/epoch - 99us/sample
Epoch 58/85
84077/84077 - 8s - loss: 1.9762e-04 - val_loss: 1.8033e-04 - 8s/epoch - 99us/sample
Epoch 59/85
84077/84077 - 8s - loss: 1.9574e-04 - val_loss: 1.7865e-04 - 8s/epoch - 99us/sample
Epoch 60/85
84077/84077 - 8s - loss: 2.0010e-04 - val_loss: 1.7968e-04 - 8s/epoch - 99us/sample
Epoch 61/85
84077/84077 - 8s - loss: 1.9516e-04 - val_loss: 1.7862e-04 - 8s/epoch - 98us/sample
Epoch 62/85
84077/84077 - 8s - loss: 1.9456e-04 - val_loss: 1.7713e-04 - 8s/epoch - 100us/sample
Epoch 63/85
84077/84077 - 8s - loss: 1.9338e-04 - val_loss: 1.7866e-04 - 8s/epoch - 99us/sample
Epoch 64/85
84077/84077 - 8s - loss: 1.9340e-04 - val_loss: 1.7639e-04 - 8s/epoch - 99us/sample
Epoch 65/85
84077/84077 - 8s - loss: 1.9392e-04 - val_loss: 1.7684e-04 - 8s/epoch - 98us/sample
Epoch 66/85
84077/84077 - 8s - loss: 1.9325e-04 - val_loss: 1.7729e-04 - 8s/epoch - 98us/sample
Epoch 67/85
84077/84077 - 8s - loss: 1.9231e-04 - val_loss: 1.7595e-04 - 8s/epoch - 99us/sample
Epoch 68/85
84077/84077 - 8s - loss: 1.9149e-04 - val_loss: 1.7570e-04 - 8s/epoch - 99us/sample
Epoch 69/85
84077/84077 - 8s - loss: 1.9075e-04 - val_loss: 1.7564e-04 - 8s/epoch - 98us/sample
Epoch 70/85
84077/84077 - 8s - loss: 1.9099e-04 - val_loss: 1.7486e-04 - 8s/epoch - 99us/sample
Epoch 71/85
84077/84077 - 8s - loss: 1.8976e-04 - val_loss: 1.7634e-04 - 8s/epoch - 99us/sample
Epoch 72/85
84077/84077 - 8s - loss: 1.8980e-04 - val_loss: 1.7556e-04 - 8s/epoch - 99us/sample
Epoch 73/85
84077/84077 - 8s - loss: 1.8973e-04 - val_loss: 1.7465e-04 - 8s/epoch - 99us/sample
Epoch 74/85
84077/84077 - 8s - loss: 1.8904e-04 - val_loss: 1.7626e-04 - 8s/epoch - 99us/sample
Epoch 75/85
84077/84077 - 8s - loss: 1.8844e-04 - val_loss: 1.7430e-04 - 8s/epoch - 98us/sample
Epoch 76/85
84077/84077 - 8s - loss: 1.8981e-04 - val_loss: 1.7466e-04 - 8s/epoch - 98us/sample
Epoch 77/85
84077/84077 - 8s - loss: 1.8845e-04 - val_loss: 1.7568e-04 - 8s/epoch - 98us/sample
Epoch 78/85
84077/84077 - 8s - loss: 1.8785e-04 - val_loss: 1.7532e-04 - 8s/epoch - 98us/sample
Epoch 79/85
84077/84077 - 8s - loss: 1.8802e-04 - val_loss: 1.7250e-04 - 8s/epoch - 100us/sample
Epoch 80/85
84077/84077 - 8s - loss: 1.9932e-04 - val_loss: 1.7704e-04 - 8s/epoch - 99us/sample
Epoch 81/85
84077/84077 - 8s - loss: 1.8672e-04 - val_loss: 1.7274e-04 - 8s/epoch - 99us/sample
Epoch 82/85
84077/84077 - 8s - loss: 1.8587e-04 - val_loss: 1.7151e-04 - 8s/epoch - 98us/sample
Epoch 83/85
84077/84077 - 8s - loss: 1.8567e-04 - val_loss: 1.7219e-04 - 8s/epoch - 98us/sample
Epoch 84/85
84077/84077 - 8s - loss: 1.8592e-04 - val_loss: 1.7204e-04 - 8s/epoch - 99us/sample
Epoch 85/85
84077/84077 - 8s - loss: 1.8553e-04 - val_loss: 1.7228e-04 - 8s/epoch - 98us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00017227775996254434
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 18:16:12.977007: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_45/outputlayer/BiasAdd' id:57440 op device:{requested: '', assigned: ''} def:{{{node decoder_model_45/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_45/outputlayer/MatMul, decoder_model_45/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02038235935157158
cosine 0.02013364430159822
MAE: 0.0021580861067268427
RMSE: 0.00819692381608422
r2: 0.9478368069754465
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.2, 188, 0.00018553044329770615, 0.00017227775996254434, 0.02038235935157158, 0.02013364430159822, 0.0021580861067268427, 0.00819692381608422, 0.9478368069754465, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0014000000000000002 32 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_138 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_138 (ReLU)               (None, 1886)         0           ['batch_normalization_138[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 18:16:32.461026: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_139/gamma/Assign' id:58451 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_139/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_139/gamma, batch_normalization_139/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 18:16:49.786917: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_46/mul' id:58724 op device:{requested: '', assigned: ''} def:{{{node loss_46/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_46/mul/x, loss_46/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 25s - loss: 0.0131 - val_loss: 0.0022 - 25s/epoch - 295us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.0339 - val_loss: 0.0013 - 13s/epoch - 159us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0059 - val_loss: 0.0378 - 13s/epoch - 159us/sample
Epoch 4/85
84077/84077 - 13s - loss: 0.0131 - val_loss: 0.0055 - 13s/epoch - 159us/sample
Epoch 5/85
84077/84077 - 13s - loss: 0.0026 - val_loss: 0.0016 - 13s/epoch - 160us/sample
Epoch 6/85
84077/84077 - 13s - loss: 0.0016 - val_loss: 0.0012 - 13s/epoch - 159us/sample
Epoch 7/85
84077/84077 - 13s - loss: 0.0012 - val_loss: 0.0010 - 13s/epoch - 159us/sample
Epoch 8/85
84077/84077 - 13s - loss: 0.0062 - val_loss: 0.0011 - 13s/epoch - 159us/sample
Epoch 9/85
84077/84077 - 13s - loss: 0.0011 - val_loss: 9.7710e-04 - 13s/epoch - 159us/sample
Epoch 10/85
84077/84077 - 13s - loss: 0.0011 - val_loss: 9.1507e-04 - 13s/epoch - 160us/sample
Epoch 11/85
84077/84077 - 13s - loss: 9.9382e-04 - val_loss: 8.8624e-04 - 13s/epoch - 160us/sample
Epoch 12/85
84077/84077 - 13s - loss: 9.3861e-04 - val_loss: 8.3019e-04 - 13s/epoch - 159us/sample
Epoch 13/85
84077/84077 - 13s - loss: 8.2673e-04 - val_loss: 7.4576e-04 - 13s/epoch - 159us/sample
Epoch 14/85
84077/84077 - 13s - loss: 7.6354e-04 - val_loss: 7.0015e-04 - 13s/epoch - 159us/sample
Epoch 15/85
84077/84077 - 13s - loss: 7.3776e-04 - val_loss: 6.6183e-04 - 13s/epoch - 159us/sample
Epoch 16/85
84077/84077 - 13s - loss: 6.8698e-04 - val_loss: 6.1623e-04 - 13s/epoch - 160us/sample
Epoch 17/85
84077/84077 - 13s - loss: 6.5557e-04 - val_loss: 5.9715e-04 - 13s/epoch - 159us/sample
Epoch 18/85
84077/84077 - 13s - loss: 6.3043e-04 - val_loss: 5.7727e-04 - 13s/epoch - 159us/sample
Epoch 19/85
84077/84077 - 13s - loss: 6.1142e-04 - val_loss: 5.6111e-04 - 13s/epoch - 159us/sample
Epoch 20/85
84077/84077 - 13s - loss: 5.9985e-04 - val_loss: 5.5069e-04 - 13s/epoch - 159us/sample
Epoch 21/85
84077/84077 - 13s - loss: 5.8576e-04 - val_loss: 5.3658e-04 - 13s/epoch - 160us/sample
Epoch 22/85
84077/84077 - 13s - loss: 5.7166e-04 - val_loss: 5.2476e-04 - 13s/epoch - 160us/sample
Epoch 23/85
84077/84077 - 13s - loss: 5.5679e-04 - val_loss: 5.0739e-04 - 13s/epoch - 159us/sample
Epoch 24/85
84077/84077 - 13s - loss: 5.4339e-04 - val_loss: 4.9809e-04 - 13s/epoch - 159us/sample
Epoch 25/85
84077/84077 - 13s - loss: 5.3269e-04 - val_loss: 4.8956e-04 - 13s/epoch - 159us/sample
Epoch 26/85
84077/84077 - 13s - loss: 5.2247e-04 - val_loss: 4.7552e-04 - 13s/epoch - 159us/sample
Epoch 27/85
84077/84077 - 13s - loss: 5.0932e-04 - val_loss: 4.6567e-04 - 13s/epoch - 160us/sample
Epoch 28/85
84077/84077 - 13s - loss: 5.0185e-04 - val_loss: 4.6709e-04 - 13s/epoch - 159us/sample
Epoch 29/85
84077/84077 - 13s - loss: 4.9680e-04 - val_loss: 4.5911e-04 - 13s/epoch - 159us/sample
Epoch 30/85
84077/84077 - 13s - loss: 4.9072e-04 - val_loss: 4.5263e-04 - 13s/epoch - 159us/sample
Epoch 31/85
84077/84077 - 13s - loss: 4.8602e-04 - val_loss: 4.5022e-04 - 13s/epoch - 159us/sample
Epoch 32/85
84077/84077 - 13s - loss: 4.8057e-04 - val_loss: 4.4924e-04 - 13s/epoch - 159us/sample
Epoch 33/85
84077/84077 - 13s - loss: 4.7685e-04 - val_loss: 4.4152e-04 - 13s/epoch - 160us/sample
Epoch 34/85
84077/84077 - 13s - loss: 4.6951e-04 - val_loss: 4.3285e-04 - 13s/epoch - 159us/sample
Epoch 35/85
84077/84077 - 13s - loss: 4.6419e-04 - val_loss: 4.2651e-04 - 13s/epoch - 159us/sample
Epoch 36/85
84077/84077 - 13s - loss: 4.6043e-04 - val_loss: 4.2445e-04 - 13s/epoch - 159us/sample
Epoch 37/85
84077/84077 - 13s - loss: 4.5600e-04 - val_loss: 4.2275e-04 - 13s/epoch - 159us/sample
Epoch 38/85
84077/84077 - 13s - loss: 4.5137e-04 - val_loss: 4.1828e-04 - 13s/epoch - 160us/sample
Epoch 39/85
84077/84077 - 13s - loss: 4.4582e-04 - val_loss: 4.1449e-04 - 13s/epoch - 159us/sample
Epoch 40/85
84077/84077 - 13s - loss: 4.4248e-04 - val_loss: 4.0946e-04 - 13s/epoch - 159us/sample
Epoch 41/85
84077/84077 - 13s - loss: 4.3854e-04 - val_loss: 4.0690e-04 - 13s/epoch - 159us/sample
Epoch 42/85
84077/84077 - 13s - loss: 4.3313e-04 - val_loss: 3.9942e-04 - 13s/epoch - 159us/sample
Epoch 43/85
84077/84077 - 13s - loss: 4.2907e-04 - val_loss: 3.9592e-04 - 13s/epoch - 160us/sample
Epoch 44/85
84077/84077 - 13s - loss: 4.2381e-04 - val_loss: 3.9055e-04 - 13s/epoch - 160us/sample
Epoch 45/85
84077/84077 - 13s - loss: 4.1950e-04 - val_loss: 3.8879e-04 - 13s/epoch - 159us/sample
Epoch 46/85
84077/84077 - 13s - loss: 4.1827e-04 - val_loss: 4.0041e-04 - 13s/epoch - 159us/sample
Epoch 47/85
84077/84077 - 13s - loss: 4.1072e-04 - val_loss: 3.8435e-04 - 13s/epoch - 159us/sample
Epoch 48/85
84077/84077 - 13s - loss: 4.0672e-04 - val_loss: 3.7795e-04 - 13s/epoch - 159us/sample
Epoch 49/85
84077/84077 - 13s - loss: 4.0681e-04 - val_loss: 3.7599e-04 - 13s/epoch - 160us/sample
Epoch 50/85
84077/84077 - 13s - loss: 4.0370e-04 - val_loss: 3.7659e-04 - 13s/epoch - 160us/sample
Epoch 51/85
84077/84077 - 13s - loss: 4.0028e-04 - val_loss: 3.7065e-04 - 13s/epoch - 159us/sample
Epoch 52/85
84077/84077 - 13s - loss: 3.9835e-04 - val_loss: 3.6811e-04 - 13s/epoch - 159us/sample
Epoch 53/85
84077/84077 - 13s - loss: 3.9656e-04 - val_loss: 3.6790e-04 - 13s/epoch - 159us/sample
Epoch 54/85
84077/84077 - 13s - loss: 3.9488e-04 - val_loss: 3.6681e-04 - 13s/epoch - 160us/sample
Epoch 55/85
84077/84077 - 13s - loss: 3.9295e-04 - val_loss: 3.6732e-04 - 13s/epoch - 160us/sample
Epoch 56/85
84077/84077 - 13s - loss: 3.9320e-04 - val_loss: 3.6489e-04 - 13s/epoch - 159us/sample
Epoch 57/85
84077/84077 - 13s - loss: 3.8856e-04 - val_loss: 3.6106e-04 - 13s/epoch - 159us/sample
Epoch 58/85
84077/84077 - 13s - loss: 3.8752e-04 - val_loss: 3.6047e-04 - 13s/epoch - 159us/sample
Epoch 59/85
84077/84077 - 13s - loss: 3.8488e-04 - val_loss: 3.5836e-04 - 13s/epoch - 159us/sample
Epoch 60/85
84077/84077 - 14s - loss: 3.8243e-04 - val_loss: 3.5774e-04 - 14s/epoch - 161us/sample
Epoch 61/85
84077/84077 - 13s - loss: 3.8020e-04 - val_loss: 3.5117e-04 - 13s/epoch - 160us/sample
Epoch 62/85
84077/84077 - 13s - loss: 3.7922e-04 - val_loss: 3.5037e-04 - 13s/epoch - 159us/sample
Epoch 63/85
84077/84077 - 13s - loss: 3.7651e-04 - val_loss: 3.5461e-04 - 13s/epoch - 159us/sample
Epoch 64/85
84077/84077 - 13s - loss: 3.7368e-04 - val_loss: 3.4651e-04 - 13s/epoch - 159us/sample
Epoch 65/85
84077/84077 - 13s - loss: 3.7130e-04 - val_loss: 3.4458e-04 - 13s/epoch - 160us/sample
Epoch 66/85
84077/84077 - 13s - loss: 3.6858e-04 - val_loss: 3.4367e-04 - 13s/epoch - 160us/sample
Epoch 67/85
84077/84077 - 13s - loss: 3.6482e-04 - val_loss: 3.3839e-04 - 13s/epoch - 159us/sample
Epoch 68/85
84077/84077 - 13s - loss: 3.6343e-04 - val_loss: 3.3817e-04 - 13s/epoch - 159us/sample
Epoch 69/85
84077/84077 - 13s - loss: 3.6092e-04 - val_loss: 3.3282e-04 - 13s/epoch - 159us/sample
Epoch 70/85
84077/84077 - 13s - loss: 3.5436e-04 - val_loss: 3.3013e-04 - 13s/epoch - 159us/sample
Epoch 71/85
84077/84077 - 13s - loss: 3.5024e-04 - val_loss: 3.2413e-04 - 13s/epoch - 160us/sample
Epoch 72/85
84077/84077 - 13s - loss: 3.4839e-04 - val_loss: 3.2198e-04 - 13s/epoch - 159us/sample
Epoch 73/85
84077/84077 - 13s - loss: 3.4549e-04 - val_loss: 3.2033e-04 - 13s/epoch - 159us/sample
Epoch 74/85
84077/84077 - 13s - loss: 3.4369e-04 - val_loss: 3.2021e-04 - 13s/epoch - 159us/sample
Epoch 75/85
84077/84077 - 13s - loss: 3.3988e-04 - val_loss: 3.1736e-04 - 13s/epoch - 159us/sample
Epoch 76/85
84077/84077 - 13s - loss: 3.3860e-04 - val_loss: 3.1694e-04 - 13s/epoch - 159us/sample
Epoch 77/85
84077/84077 - 14s - loss: 3.3670e-04 - val_loss: 3.1337e-04 - 14s/epoch - 161us/sample
Epoch 78/85
84077/84077 - 13s - loss: 3.3503e-04 - val_loss: 3.1103e-04 - 13s/epoch - 159us/sample
Epoch 79/85
84077/84077 - 13s - loss: 3.3443e-04 - val_loss: 3.0830e-04 - 13s/epoch - 159us/sample
Epoch 80/85
84077/84077 - 13s - loss: 3.3356e-04 - val_loss: 3.0899e-04 - 13s/epoch - 159us/sample
Epoch 81/85
84077/84077 - 13s - loss: 3.3008e-04 - val_loss: 3.0888e-04 - 13s/epoch - 159us/sample
Epoch 82/85
84077/84077 - 14s - loss: 3.3287e-04 - val_loss: 3.0536e-04 - 14s/epoch - 161us/sample
Epoch 83/85
84077/84077 - 13s - loss: 3.2737e-04 - val_loss: 3.0344e-04 - 13s/epoch - 160us/sample
Epoch 84/85
84077/84077 - 13s - loss: 3.2924e-04 - val_loss: 3.0250e-04 - 13s/epoch - 159us/sample
Epoch 85/85
84077/84077 - 13s - loss: 3.2652e-04 - val_loss: 2.9948e-04 - 13s/epoch - 159us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.000299483622960448
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 18:35:39.084294: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_46/outputlayer/BiasAdd' id:58695 op device:{requested: '', assigned: ''} def:{{{node decoder_model_46/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_46/outputlayer/MatMul, decoder_model_46/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.057335882144192975
cosine 0.05662162582359444
MAE: 0.0029525771921965283
RMSE: 0.014264184187184129
r2: 0.8411078146492477
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 85, 0.0014000000000000002, 0.2, 188, 0.0003265211178001615, 0.000299483622960448, 0.057335882144192975, 0.05662162582359444, 0.0029525771921965283, 0.014264184187184129, 0.8411078146492477, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.0008 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_141 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_141 (ReLU)               (None, 1603)         0           ['batch_normalization_141[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_141[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_141[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 18:35:58.901721: W tensorflow/c/c_api.cc:291] Operation '{name:'training_94/Adam/dense_enc0_47/kernel/v/Assign' id:60576 op device:{requested: '', assigned: ''} def:{{{node training_94/Adam/dense_enc0_47/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_94/Adam/dense_enc0_47/kernel/v, training_94/Adam/dense_enc0_47/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 18:36:11.833806: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_47/mul' id:59986 op device:{requested: '', assigned: ''} def:{{{node loss_47/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_47/mul/x, loss_47/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 33.2939 - val_loss: 9.2664e-04 - 20s/epoch - 242us/sample
Epoch 2/85
84077/84077 - 8s - loss: 8.6249e-04 - val_loss: 7.9510e-04 - 8s/epoch - 100us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0195 - val_loss: 0.0016 - 8s/epoch - 100us/sample
Epoch 4/85
84077/84077 - 8s - loss: 9.0957e-04 - val_loss: 6.3599e-04 - 8s/epoch - 100us/sample
Epoch 5/85
84077/84077 - 8s - loss: 6.6143e-04 - val_loss: 6.4003e-04 - 8s/epoch - 100us/sample
Epoch 6/85
84077/84077 - 8s - loss: 5.5149e-04 - val_loss: 4.9120e-04 - 8s/epoch - 100us/sample
Epoch 7/85
84077/84077 - 8s - loss: 4.7933e-04 - val_loss: 4.4258e-04 - 8s/epoch - 100us/sample
Epoch 8/85
84077/84077 - 8s - loss: 4.2086e-04 - val_loss: 3.3567e-04 - 8s/epoch - 100us/sample
Epoch 9/85
84077/84077 - 9s - loss: 3.5319e-04 - val_loss: 3.0562e-04 - 9s/epoch - 101us/sample
Epoch 10/85
84077/84077 - 8s - loss: 3.2568e-04 - val_loss: 2.9847e-04 - 8s/epoch - 101us/sample
Epoch 11/85
84077/84077 - 8s - loss: 3.0238e-04 - val_loss: 2.7446e-04 - 8s/epoch - 100us/sample
Epoch 12/85
84077/84077 - 8s - loss: 2.8466e-04 - val_loss: 2.5318e-04 - 8s/epoch - 100us/sample
Epoch 13/85
84077/84077 - 8s - loss: 2.6711e-04 - val_loss: 2.4089e-04 - 8s/epoch - 100us/sample
Epoch 14/85
84077/84077 - 8s - loss: 2.5442e-04 - val_loss: 2.2385e-04 - 8s/epoch - 100us/sample
Epoch 15/85
84077/84077 - 8s - loss: 2.4247e-04 - val_loss: 2.1307e-04 - 8s/epoch - 100us/sample
Epoch 16/85
84077/84077 - 8s - loss: 2.2894e-04 - val_loss: 2.0072e-04 - 8s/epoch - 100us/sample
Epoch 17/85
84077/84077 - 8s - loss: 2.2071e-04 - val_loss: 1.9675e-04 - 8s/epoch - 101us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.1186e-04 - val_loss: 1.8864e-04 - 9s/epoch - 101us/sample
Epoch 19/85
84077/84077 - 8s - loss: 2.0534e-04 - val_loss: 1.8347e-04 - 8s/epoch - 100us/sample
Epoch 20/85
84077/84077 - 8s - loss: 1.9781e-04 - val_loss: 1.8329e-04 - 8s/epoch - 100us/sample
Epoch 21/85
84077/84077 - 8s - loss: 1.9309e-04 - val_loss: 1.7402e-04 - 8s/epoch - 100us/sample
Epoch 22/85
84077/84077 - 8s - loss: 1.9103e-04 - val_loss: 1.7358e-04 - 8s/epoch - 100us/sample
Epoch 23/85
84077/84077 - 8s - loss: 1.8624e-04 - val_loss: 1.6716e-04 - 8s/epoch - 100us/sample
Epoch 24/85
84077/84077 - 8s - loss: 1.8269e-04 - val_loss: 1.6811e-04 - 8s/epoch - 100us/sample
Epoch 25/85
84077/84077 - 8s - loss: 1.7956e-04 - val_loss: 1.6408e-04 - 8s/epoch - 100us/sample
Epoch 26/85
84077/84077 - 8s - loss: 1.7650e-04 - val_loss: 1.6109e-04 - 8s/epoch - 101us/sample
Epoch 27/85
84077/84077 - 8s - loss: 1.7394e-04 - val_loss: 1.5636e-04 - 8s/epoch - 100us/sample
Epoch 28/85
84077/84077 - 8s - loss: 1.7075e-04 - val_loss: 1.5560e-04 - 8s/epoch - 100us/sample
Epoch 29/85
84077/84077 - 8s - loss: 1.6884e-04 - val_loss: 1.5446e-04 - 8s/epoch - 99us/sample
Epoch 30/85
84077/84077 - 8s - loss: 1.6687e-04 - val_loss: 1.5260e-04 - 8s/epoch - 100us/sample
Epoch 31/85
84077/84077 - 8s - loss: 1.6360e-04 - val_loss: 1.4936e-04 - 8s/epoch - 100us/sample
Epoch 32/85
84077/84077 - 8s - loss: 1.6303e-04 - val_loss: 1.5007e-04 - 8s/epoch - 99us/sample
Epoch 33/85
84077/84077 - 8s - loss: 1.6104e-04 - val_loss: 1.4700e-04 - 8s/epoch - 99us/sample
Epoch 34/85
84077/84077 - 8s - loss: 1.5970e-04 - val_loss: 1.4593e-04 - 8s/epoch - 99us/sample
Epoch 35/85
84077/84077 - 8s - loss: 1.5748e-04 - val_loss: 1.4470e-04 - 8s/epoch - 99us/sample
Epoch 36/85
84077/84077 - 8s - loss: 1.5685e-04 - val_loss: 1.4389e-04 - 8s/epoch - 98us/sample
Epoch 37/85
84077/84077 - 8s - loss: 1.5622e-04 - val_loss: 1.4426e-04 - 8s/epoch - 98us/sample
Epoch 38/85
84077/84077 - 8s - loss: 1.5500e-04 - val_loss: 1.4223e-04 - 8s/epoch - 98us/sample
Epoch 39/85
84077/84077 - 8s - loss: 1.5444e-04 - val_loss: 1.4275e-04 - 8s/epoch - 98us/sample
Epoch 40/85
84077/84077 - 8s - loss: 1.5347e-04 - val_loss: 1.4215e-04 - 8s/epoch - 98us/sample
Epoch 41/85
84077/84077 - 8s - loss: 1.5246e-04 - val_loss: 1.4064e-04 - 8s/epoch - 98us/sample
Epoch 42/85
84077/84077 - 8s - loss: 1.5162e-04 - val_loss: 1.4082e-04 - 8s/epoch - 99us/sample
Epoch 43/85
84077/84077 - 8s - loss: 1.5151e-04 - val_loss: 1.4054e-04 - 8s/epoch - 99us/sample
Epoch 44/85
84077/84077 - 8s - loss: 1.5018e-04 - val_loss: 1.3867e-04 - 8s/epoch - 98us/sample
Epoch 45/85
84077/84077 - 8s - loss: 1.4933e-04 - val_loss: 1.3835e-04 - 8s/epoch - 98us/sample
Epoch 46/85
84077/84077 - 8s - loss: 1.4862e-04 - val_loss: 1.3764e-04 - 8s/epoch - 98us/sample
Epoch 47/85
84077/84077 - 8s - loss: 1.4811e-04 - val_loss: 1.3719e-04 - 8s/epoch - 98us/sample
Epoch 48/85
84077/84077 - 8s - loss: 1.4775e-04 - val_loss: 1.3679e-04 - 8s/epoch - 98us/sample
Epoch 49/85
84077/84077 - 8s - loss: 1.4688e-04 - val_loss: 1.3713e-04 - 8s/epoch - 98us/sample
Epoch 50/85
84077/84077 - 8s - loss: 1.4658e-04 - val_loss: 1.3711e-04 - 8s/epoch - 99us/sample
Epoch 51/85
84077/84077 - 8s - loss: 1.4570e-04 - val_loss: 1.3613e-04 - 8s/epoch - 99us/sample
Epoch 52/85
84077/84077 - 8s - loss: 1.4563e-04 - val_loss: 1.3643e-04 - 8s/epoch - 99us/sample
Epoch 53/85
84077/84077 - 8s - loss: 1.4497e-04 - val_loss: 1.3449e-04 - 8s/epoch - 98us/sample
Epoch 54/85
84077/84077 - 8s - loss: 1.4467e-04 - val_loss: 1.3383e-04 - 8s/epoch - 98us/sample
Epoch 55/85
84077/84077 - 8s - loss: 1.4422e-04 - val_loss: 1.3508e-04 - 8s/epoch - 98us/sample
Epoch 56/85
84077/84077 - 8s - loss: 1.4371e-04 - val_loss: 1.3441e-04 - 8s/epoch - 99us/sample
Epoch 57/85
84077/84077 - 8s - loss: 1.4300e-04 - val_loss: 1.3310e-04 - 8s/epoch - 98us/sample
Epoch 58/85
84077/84077 - 8s - loss: 1.4294e-04 - val_loss: 1.3290e-04 - 8s/epoch - 98us/sample
Epoch 59/85
84077/84077 - 8s - loss: 1.4266e-04 - val_loss: 1.3337e-04 - 8s/epoch - 99us/sample
Epoch 60/85
84077/84077 - 8s - loss: 1.4207e-04 - val_loss: 1.3307e-04 - 8s/epoch - 98us/sample
Epoch 61/85
84077/84077 - 8s - loss: 1.4193e-04 - val_loss: 1.3340e-04 - 8s/epoch - 98us/sample
Epoch 62/85
84077/84077 - 8s - loss: 1.4154e-04 - val_loss: 1.3463e-04 - 8s/epoch - 98us/sample
Epoch 63/85
84077/84077 - 8s - loss: 1.4125e-04 - val_loss: 1.3262e-04 - 8s/epoch - 98us/sample
Epoch 64/85
84077/84077 - 8s - loss: 1.4094e-04 - val_loss: 1.3213e-04 - 8s/epoch - 98us/sample
Epoch 65/85
84077/84077 - 8s - loss: 1.4060e-04 - val_loss: 1.3403e-04 - 8s/epoch - 98us/sample
Epoch 66/85
84077/84077 - 8s - loss: 1.4029e-04 - val_loss: 1.3189e-04 - 8s/epoch - 99us/sample
Epoch 67/85
84077/84077 - 8s - loss: 1.4008e-04 - val_loss: 1.3171e-04 - 8s/epoch - 99us/sample
Epoch 68/85
84077/84077 - 8s - loss: 1.3938e-04 - val_loss: 1.3057e-04 - 8s/epoch - 99us/sample
Epoch 69/85
84077/84077 - 8s - loss: 1.3951e-04 - val_loss: 1.3219e-04 - 8s/epoch - 98us/sample
Epoch 70/85
84077/84077 - 8s - loss: 1.3915e-04 - val_loss: 1.3101e-04 - 8s/epoch - 98us/sample
Epoch 71/85
84077/84077 - 8s - loss: 1.3853e-04 - val_loss: 1.3165e-04 - 8s/epoch - 98us/sample
Epoch 72/85
84077/84077 - 8s - loss: 1.3852e-04 - val_loss: 1.3082e-04 - 8s/epoch - 98us/sample
Epoch 73/85
84077/84077 - 8s - loss: 1.3829e-04 - val_loss: 1.3018e-04 - 8s/epoch - 99us/sample
Epoch 74/85
84077/84077 - 8s - loss: 1.3792e-04 - val_loss: 1.3024e-04 - 8s/epoch - 99us/sample
Epoch 75/85
84077/84077 - 8s - loss: 1.3799e-04 - val_loss: 1.2971e-04 - 8s/epoch - 99us/sample
Epoch 76/85
84077/84077 - 8s - loss: 1.3750e-04 - val_loss: 1.2986e-04 - 8s/epoch - 99us/sample
Epoch 77/85
84077/84077 - 8s - loss: 1.3695e-04 - val_loss: 1.2872e-04 - 8s/epoch - 99us/sample
Epoch 78/85
84077/84077 - 8s - loss: 1.3725e-04 - val_loss: 1.2912e-04 - 8s/epoch - 98us/sample
Epoch 79/85
84077/84077 - 8s - loss: 1.3685e-04 - val_loss: 1.2943e-04 - 8s/epoch - 98us/sample
Epoch 80/85
84077/84077 - 8s - loss: 1.3686e-04 - val_loss: 1.2916e-04 - 8s/epoch - 98us/sample
Epoch 81/85
84077/84077 - 8s - loss: 1.3628e-04 - val_loss: 1.3018e-04 - 8s/epoch - 98us/sample
Epoch 82/85
84077/84077 - 8s - loss: 1.3660e-04 - val_loss: 1.2869e-04 - 8s/epoch - 98us/sample
Epoch 83/85
84077/84077 - 8s - loss: 1.3598e-04 - val_loss: 1.3059e-04 - 8s/epoch - 98us/sample
Epoch 84/85
84077/84077 - 8s - loss: 1.3579e-04 - val_loss: 1.2779e-04 - 8s/epoch - 100us/sample
Epoch 85/85
84077/84077 - 8s - loss: 1.3555e-04 - val_loss: 1.2862e-04 - 8s/epoch - 99us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001286218657270319
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 18:47:55.393466: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_47/outputlayer/BiasAdd' id:59950 op device:{requested: '', assigned: ''} def:{{{node decoder_model_47/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_47/outputlayer/MatMul, decoder_model_47/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031262812827354894
cosine 0.030880368243156618
MAE: 0.002301963078184148
RMSE: 0.010274187637857545
r2: 0.9176847660452147
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'logcosh', 64, 85, 0.0008, 0.2, 188, 0.00013554800294180325, 0.0001286218657270319, 0.031262812827354894, 0.030880368243156618, 0.002301963078184148, 0.010274187637857545, 0.9176847660452147, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 85 0.001 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_144 (Batch  (None, 1697)        6788        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_144 (ReLU)               (None, 1697)         0           ['batch_normalization_144[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          319224      ['re_lu_144[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          319224      ['re_lu_144[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1965019     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,212,223
Trainable params: 4,205,059
Non-trainable params: 7,164
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 18:48:15.028872: W tensorflow/c/c_api.cc:291] Operation '{name:'training_96/Adam/dense_dec1_48/kernel/m/Assign' id:61769 op device:{requested: '', assigned: ''} def:{{{node training_96/Adam/dense_dec1_48/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_96/Adam/dense_dec1_48/kernel/m, training_96/Adam/dense_dec1_48/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 18:48:27.894730: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_48/mul' id:61264 op device:{requested: '', assigned: ''} def:{{{node loss_48/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_48/mul/x, loss_48/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0098 - val_loss: 0.0016 - 20s/epoch - 243us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0014 - val_loss: 0.0017 - 8s/epoch - 100us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0011 - 8s/epoch - 99us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0024 - val_loss: 0.0053 - 8s/epoch - 98us/sample
Epoch 5/85
84077/84077 - 8s - loss: 0.0015 - val_loss: 9.1917e-04 - 8s/epoch - 99us/sample
Epoch 6/85
84077/84077 - 8s - loss: 8.6514e-04 - val_loss: 7.3590e-04 - 8s/epoch - 98us/sample
Epoch 7/85
84077/84077 - 8s - loss: 7.1902e-04 - val_loss: 6.0924e-04 - 8s/epoch - 98us/sample
Epoch 8/85
84077/84077 - 8s - loss: 6.3543e-04 - val_loss: 5.3516e-04 - 8s/epoch - 99us/sample
Epoch 9/85
84077/84077 - 8s - loss: 5.5637e-04 - val_loss: 4.7052e-04 - 8s/epoch - 98us/sample
Epoch 10/85
84077/84077 - 8s - loss: 4.9195e-04 - val_loss: 4.1108e-04 - 8s/epoch - 99us/sample
Epoch 11/85
84077/84077 - 8s - loss: 4.3987e-04 - val_loss: 3.5920e-04 - 8s/epoch - 98us/sample
Epoch 12/85
84077/84077 - 8s - loss: 3.9682e-04 - val_loss: 3.3990e-04 - 8s/epoch - 100us/sample
Epoch 13/85
84077/84077 - 8s - loss: 3.6168e-04 - val_loss: 3.0243e-04 - 8s/epoch - 99us/sample
Epoch 14/85
84077/84077 - 8s - loss: 3.3120e-04 - val_loss: 2.8124e-04 - 8s/epoch - 98us/sample
Epoch 15/85
84077/84077 - 8s - loss: 3.1078e-04 - val_loss: 2.8044e-04 - 8s/epoch - 99us/sample
Epoch 16/85
84077/84077 - 8s - loss: 2.9295e-04 - val_loss: 2.5188e-04 - 8s/epoch - 98us/sample
Epoch 17/85
84077/84077 - 8s - loss: 2.7824e-04 - val_loss: 2.4667e-04 - 8s/epoch - 98us/sample
Epoch 18/85
84077/84077 - 8s - loss: 2.6985e-04 - val_loss: 2.4061e-04 - 8s/epoch - 99us/sample
Epoch 19/85
84077/84077 - 8s - loss: 2.6087e-04 - val_loss: 2.3179e-04 - 8s/epoch - 98us/sample
Epoch 20/85
84077/84077 - 8s - loss: 2.5496e-04 - val_loss: 2.2830e-04 - 8s/epoch - 99us/sample
Epoch 21/85
84077/84077 - 8s - loss: 2.5018e-04 - val_loss: 2.2532e-04 - 8s/epoch - 99us/sample
Epoch 22/85
84077/84077 - 8s - loss: 2.4548e-04 - val_loss: 2.2378e-04 - 8s/epoch - 99us/sample
Epoch 23/85
84077/84077 - 8s - loss: 2.4222e-04 - val_loss: 2.1767e-04 - 8s/epoch - 99us/sample
Epoch 24/85
84077/84077 - 8s - loss: 2.3879e-04 - val_loss: 2.1715e-04 - 8s/epoch - 98us/sample
Epoch 25/85
84077/84077 - 8s - loss: 2.3441e-04 - val_loss: 2.1075e-04 - 8s/epoch - 99us/sample
Epoch 26/85
84077/84077 - 8s - loss: 2.3264e-04 - val_loss: 2.1269e-04 - 8s/epoch - 98us/sample
Epoch 27/85
84077/84077 - 8s - loss: 2.3065e-04 - val_loss: 2.1274e-04 - 8s/epoch - 98us/sample
Epoch 28/85
84077/84077 - 8s - loss: 2.2845e-04 - val_loss: 2.0927e-04 - 8s/epoch - 99us/sample
Epoch 29/85
84077/84077 - 8s - loss: 2.2525e-04 - val_loss: 2.0916e-04 - 8s/epoch - 100us/sample
Epoch 30/85
84077/84077 - 8s - loss: 2.2411e-04 - val_loss: 2.0415e-04 - 8s/epoch - 99us/sample
Epoch 31/85
84077/84077 - 8s - loss: 2.2420e-04 - val_loss: 2.0254e-04 - 8s/epoch - 98us/sample
Epoch 32/85
84077/84077 - 8s - loss: 2.2097e-04 - val_loss: 2.0181e-04 - 8s/epoch - 99us/sample
Epoch 33/85
84077/84077 - 8s - loss: 2.1871e-04 - val_loss: 2.0185e-04 - 8s/epoch - 98us/sample
Epoch 34/85
84077/84077 - 8s - loss: 2.1712e-04 - val_loss: 1.9959e-04 - 8s/epoch - 98us/sample
Epoch 35/85
84077/84077 - 8s - loss: 2.2820e-04 - val_loss: 1.9923e-04 - 8s/epoch - 98us/sample
Epoch 36/85
84077/84077 - 8s - loss: 2.1507e-04 - val_loss: 2.0014e-04 - 8s/epoch - 98us/sample
Epoch 37/85
84077/84077 - 8s - loss: 2.1452e-04 - val_loss: 1.9665e-04 - 8s/epoch - 99us/sample
Epoch 38/85
84077/84077 - 8s - loss: 2.1339e-04 - val_loss: 1.9309e-04 - 8s/epoch - 99us/sample
Epoch 39/85
84077/84077 - 8s - loss: 2.1193e-04 - val_loss: 1.9488e-04 - 8s/epoch - 98us/sample
Epoch 40/85
84077/84077 - 8s - loss: 2.1064e-04 - val_loss: 1.9352e-04 - 8s/epoch - 98us/sample
Epoch 41/85
84077/84077 - 8s - loss: 2.1001e-04 - val_loss: 1.9742e-04 - 8s/epoch - 98us/sample
Epoch 42/85
84077/84077 - 8s - loss: 2.0936e-04 - val_loss: 1.9210e-04 - 8s/epoch - 98us/sample
Epoch 43/85
84077/84077 - 8s - loss: 2.0787e-04 - val_loss: 1.9315e-04 - 8s/epoch - 98us/sample
Epoch 44/85
84077/84077 - 8s - loss: 2.0621e-04 - val_loss: 1.9093e-04 - 8s/epoch - 98us/sample
Epoch 45/85
84077/84077 - 8s - loss: 2.0640e-04 - val_loss: 1.9120e-04 - 8s/epoch - 99us/sample
Epoch 46/85
84077/84077 - 8s - loss: 2.0573e-04 - val_loss: 1.9203e-04 - 8s/epoch - 99us/sample
Epoch 47/85
84077/84077 - 8s - loss: 2.0486e-04 - val_loss: 1.9030e-04 - 8s/epoch - 99us/sample
Epoch 48/85
84077/84077 - 8s - loss: 2.0368e-04 - val_loss: 1.9148e-04 - 8s/epoch - 99us/sample
Epoch 49/85
84077/84077 - 8s - loss: 2.0324e-04 - val_loss: 1.8632e-04 - 8s/epoch - 98us/sample
Epoch 50/85
84077/84077 - 8s - loss: 2.0200e-04 - val_loss: 1.8981e-04 - 8s/epoch - 99us/sample
Epoch 51/85
84077/84077 - 8s - loss: 2.0161e-04 - val_loss: 1.8845e-04 - 8s/epoch - 98us/sample
Epoch 52/85
84077/84077 - 8s - loss: 2.0586e-04 - val_loss: 1.9016e-04 - 8s/epoch - 98us/sample
Epoch 53/85
84077/84077 - 8s - loss: 2.0146e-04 - val_loss: 1.8728e-04 - 8s/epoch - 98us/sample
Epoch 54/85
84077/84077 - 8s - loss: 2.0059e-04 - val_loss: 1.8649e-04 - 8s/epoch - 100us/sample
Epoch 55/85
84077/84077 - 8s - loss: 1.9925e-04 - val_loss: 1.8458e-04 - 8s/epoch - 99us/sample
Epoch 56/85
84077/84077 - 8s - loss: 1.9880e-04 - val_loss: 1.8946e-04 - 8s/epoch - 98us/sample
Epoch 57/85
84077/84077 - 8s - loss: 1.9902e-04 - val_loss: 1.8806e-04 - 8s/epoch - 98us/sample
Epoch 58/85
84077/84077 - 8s - loss: 1.9781e-04 - val_loss: 1.8589e-04 - 8s/epoch - 98us/sample
Epoch 59/85
84077/84077 - 8s - loss: 1.9836e-04 - val_loss: 1.8382e-04 - 8s/epoch - 98us/sample
Epoch 60/85
84077/84077 - 8s - loss: 2.1991e-04 - val_loss: 1.8511e-04 - 8s/epoch - 99us/sample
Epoch 61/85
84077/84077 - 8s - loss: 1.9702e-04 - val_loss: 1.8433e-04 - 8s/epoch - 98us/sample
Epoch 62/85
84077/84077 - 8s - loss: 1.9616e-04 - val_loss: 1.8348e-04 - 8s/epoch - 100us/sample
Epoch 63/85
84077/84077 - 8s - loss: 1.9617e-04 - val_loss: 1.8140e-04 - 8s/epoch - 99us/sample
Epoch 64/85
84077/84077 - 8s - loss: 1.9543e-04 - val_loss: 1.8446e-04 - 8s/epoch - 99us/sample
Epoch 65/85
84077/84077 - 8s - loss: 1.9499e-04 - val_loss: 1.8318e-04 - 8s/epoch - 98us/sample
Epoch 66/85
84077/84077 - 8s - loss: 1.9493e-04 - val_loss: 1.8193e-04 - 8s/epoch - 98us/sample
Epoch 67/85
84077/84077 - 8s - loss: 1.9380e-04 - val_loss: 1.8080e-04 - 8s/epoch - 98us/sample
Epoch 68/85
84077/84077 - 8s - loss: 1.9369e-04 - val_loss: 1.8271e-04 - 8s/epoch - 98us/sample
Epoch 69/85
84077/84077 - 8s - loss: 1.9282e-04 - val_loss: 1.8135e-04 - 8s/epoch - 98us/sample
Epoch 70/85
84077/84077 - 8s - loss: 1.9582e-04 - val_loss: 1.8125e-04 - 8s/epoch - 98us/sample
Epoch 71/85
84077/84077 - 8s - loss: 1.9304e-04 - val_loss: 1.7975e-04 - 8s/epoch - 99us/sample
Epoch 72/85
84077/84077 - 8s - loss: 1.9227e-04 - val_loss: 1.7897e-04 - 8s/epoch - 99us/sample
Epoch 73/85
84077/84077 - 8s - loss: 1.9205e-04 - val_loss: 1.8048e-04 - 8s/epoch - 99us/sample
Epoch 74/85
84077/84077 - 8s - loss: 1.9123e-04 - val_loss: 1.8177e-04 - 8s/epoch - 98us/sample
Epoch 75/85
84077/84077 - 8s - loss: 1.9285e-04 - val_loss: 1.9138e-04 - 8s/epoch - 99us/sample
Epoch 76/85
84077/84077 - 8s - loss: 1.9067e-04 - val_loss: 1.7903e-04 - 8s/epoch - 98us/sample
Epoch 77/85
84077/84077 - 8s - loss: 1.9264e-04 - val_loss: 1.7931e-04 - 8s/epoch - 98us/sample
Epoch 78/85
84077/84077 - 8s - loss: 1.9045e-04 - val_loss: 1.7609e-04 - 8s/epoch - 98us/sample
Epoch 79/85
84077/84077 - 8s - loss: 1.8947e-04 - val_loss: 1.7921e-04 - 8s/epoch - 100us/sample
Epoch 80/85
84077/84077 - 8s - loss: 1.9126e-04 - val_loss: 1.7985e-04 - 8s/epoch - 99us/sample
Epoch 81/85
84077/84077 - 8s - loss: 1.8955e-04 - val_loss: 1.7649e-04 - 8s/epoch - 98us/sample
Epoch 82/85
84077/84077 - 8s - loss: 1.8969e-04 - val_loss: 1.8295e-04 - 8s/epoch - 98us/sample
Epoch 83/85
84077/84077 - 8s - loss: 1.8903e-04 - val_loss: 1.7745e-04 - 8s/epoch - 99us/sample
Epoch 84/85
84077/84077 - 8s - loss: 1.8890e-04 - val_loss: 1.7905e-04 - 8s/epoch - 98us/sample
Epoch 85/85
84077/84077 - 8s - loss: 1.8795e-04 - val_loss: 1.8089e-04 - 8s/epoch - 99us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018088912550626442
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 19:00:08.219509: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_48/outputlayer/BiasAdd' id:61235 op device:{requested: '', assigned: ''} def:{{{node decoder_model_48/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_48/outputlayer/MatMul, decoder_model_48/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.021889568728652498
cosine 0.021620448791660536
MAE: 0.002453122630976472
RMSE: 0.008625921377210063
r2: 0.9422409739607038
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'mse', 64, 85, 0.001, 0.2, 188, 0.00018795003032388356, 0.00018088912550626442, 0.021889568728652498, 0.021620448791660536, 0.002453122630976472, 0.008625921377210063, 0.9422409739607038, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 8 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_147 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_147 (ReLU)               (None, 1886)         0           ['batch_normalization_147[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_147[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_147[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 19:00:28.524794: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_49/bias/Assign' id:62317 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_49/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_49/bias, dense_dec0_49/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 19:01:13.971755: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_49/mul' id:62519 op device:{requested: '', assigned: ''} def:{{{node loss_49/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_49/mul/x, loss_49/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 55s - loss: 0.0045 - val_loss: 0.0014 - 55s/epoch - 658us/sample
Epoch 2/85
84077/84077 - 43s - loss: 0.0019 - val_loss: 0.0012 - 43s/epoch - 511us/sample
Epoch 3/85
84077/84077 - 43s - loss: 0.0016 - val_loss: 0.0011 - 43s/epoch - 508us/sample
Epoch 4/85
84077/84077 - 43s - loss: 0.0011 - val_loss: 0.0010 - 43s/epoch - 511us/sample
Epoch 5/85
84077/84077 - 43s - loss: 0.0010 - val_loss: 0.0011 - 43s/epoch - 510us/sample
Epoch 6/85
84077/84077 - 43s - loss: 9.8520e-04 - val_loss: 0.0019 - 43s/epoch - 511us/sample
Epoch 7/85
84077/84077 - 43s - loss: 9.4264e-04 - val_loss: 0.0090 - 43s/epoch - 511us/sample
Epoch 8/85
84077/84077 - 43s - loss: 9.1301e-04 - val_loss: 0.0148 - 43s/epoch - 509us/sample
Epoch 9/85
84077/84077 - 43s - loss: 8.8101e-04 - val_loss: 0.0307 - 43s/epoch - 511us/sample
Epoch 10/85
84077/84077 - 43s - loss: 8.5642e-04 - val_loss: 0.0418 - 43s/epoch - 509us/sample
Epoch 11/85
84077/84077 - 43s - loss: 8.3932e-04 - val_loss: 0.0847 - 43s/epoch - 508us/sample
Epoch 12/85
84077/84077 - 43s - loss: 8.2755e-04 - val_loss: 0.0787 - 43s/epoch - 511us/sample
Epoch 13/85
84077/84077 - 43s - loss: 8.1753e-04 - val_loss: 0.0829 - 43s/epoch - 508us/sample
Epoch 14/85
84077/84077 - 43s - loss: 8.0727e-04 - val_loss: 0.0669 - 43s/epoch - 512us/sample
Epoch 15/85
84077/84077 - 43s - loss: 7.9961e-04 - val_loss: 0.0936 - 43s/epoch - 508us/sample
Epoch 16/85
84077/84077 - 43s - loss: 7.9577e-04 - val_loss: 0.1293 - 43s/epoch - 511us/sample
Epoch 17/85
84077/84077 - 43s - loss: 7.9128e-04 - val_loss: 0.0955 - 43s/epoch - 509us/sample
Epoch 18/85
84077/84077 - 43s - loss: 7.8687e-04 - val_loss: 0.0872 - 43s/epoch - 511us/sample
Epoch 19/85
84077/84077 - 43s - loss: 7.8171e-04 - val_loss: 0.1175 - 43s/epoch - 508us/sample
Epoch 20/85
84077/84077 - 43s - loss: 7.7722e-04 - val_loss: 0.1020 - 43s/epoch - 509us/sample
Epoch 21/85
84077/84077 - 43s - loss: 7.7292e-04 - val_loss: 0.1048 - 43s/epoch - 511us/sample
Epoch 22/85
84077/84077 - 43s - loss: 7.7056e-04 - val_loss: 0.1162 - 43s/epoch - 509us/sample
Epoch 23/85
84077/84077 - 43s - loss: 7.6717e-04 - val_loss: 0.1364 - 43s/epoch - 510us/sample
Epoch 24/85
84077/84077 - 43s - loss: 7.6222e-04 - val_loss: 0.1369 - 43s/epoch - 515us/sample
Epoch 25/85
84077/84077 - 43s - loss: 7.5901e-04 - val_loss: 0.1231 - 43s/epoch - 511us/sample
Epoch 26/85
84077/84077 - 43s - loss: 7.5511e-04 - val_loss: 0.1467 - 43s/epoch - 512us/sample
Epoch 27/85
84077/84077 - 43s - loss: 7.5206e-04 - val_loss: 0.1599 - 43s/epoch - 513us/sample
Epoch 28/85
84077/84077 - 43s - loss: 7.4826e-04 - val_loss: 0.1614 - 43s/epoch - 512us/sample
Epoch 29/85
84077/84077 - 44s - loss: 7.4500e-04 - val_loss: 0.1918 - 44s/epoch - 522us/sample
Epoch 30/85
84077/84077 - 44s - loss: 7.4153e-04 - val_loss: 0.1819 - 44s/epoch - 521us/sample
Epoch 31/85
84077/84077 - 44s - loss: 7.3872e-04 - val_loss: 0.1985 - 44s/epoch - 523us/sample
Epoch 32/85
84077/84077 - 43s - loss: 7.3676e-04 - val_loss: 0.1995 - 43s/epoch - 510us/sample
Epoch 33/85
84077/84077 - 43s - loss: 7.3311e-04 - val_loss: 0.2200 - 43s/epoch - 511us/sample
Epoch 34/85
84077/84077 - 43s - loss: 7.3092e-04 - val_loss: 0.2173 - 43s/epoch - 511us/sample
Epoch 35/85
84077/84077 - 43s - loss: 7.2697e-04 - val_loss: 0.2122 - 43s/epoch - 516us/sample
Epoch 36/85
84077/84077 - 44s - loss: 7.2650e-04 - val_loss: 0.2122 - 44s/epoch - 518us/sample
Epoch 37/85
84077/84077 - 43s - loss: 7.2294e-04 - val_loss: 0.1834 - 43s/epoch - 509us/sample
Epoch 38/85
84077/84077 - 43s - loss: 7.2147e-04 - val_loss: 0.2025 - 43s/epoch - 514us/sample
Epoch 39/85
84077/84077 - 43s - loss: 7.2052e-04 - val_loss: 0.2007 - 43s/epoch - 513us/sample
Epoch 40/85
84077/84077 - 43s - loss: 7.1788e-04 - val_loss: 0.2009 - 43s/epoch - 511us/sample
Epoch 41/85
84077/84077 - 43s - loss: 7.1684e-04 - val_loss: 0.1935 - 43s/epoch - 511us/sample
Epoch 42/85
84077/84077 - 44s - loss: 7.1619e-04 - val_loss: 0.2213 - 44s/epoch - 520us/sample
Epoch 43/85
84077/84077 - 43s - loss: 7.1524e-04 - val_loss: 0.2263 - 43s/epoch - 516us/sample
Epoch 44/85
84077/84077 - 43s - loss: 7.1423e-04 - val_loss: 0.2089 - 43s/epoch - 508us/sample
Epoch 45/85
84077/84077 - 43s - loss: 7.1186e-04 - val_loss: 0.2690 - 43s/epoch - 509us/sample
Epoch 46/85
84077/84077 - 43s - loss: 7.1086e-04 - val_loss: 0.2532 - 43s/epoch - 510us/sample
Epoch 47/85
84077/84077 - 43s - loss: 7.0921e-04 - val_loss: 0.2356 - 43s/epoch - 509us/sample
Epoch 48/85
84077/84077 - 43s - loss: 7.0843e-04 - val_loss: 0.2059 - 43s/epoch - 511us/sample
Epoch 49/85
84077/84077 - 43s - loss: 7.0851e-04 - val_loss: 0.2200 - 43s/epoch - 508us/sample
Epoch 50/85
84077/84077 - 43s - loss: 7.0789e-04 - val_loss: 0.2399 - 43s/epoch - 509us/sample
Epoch 51/85
84077/84077 - 43s - loss: 7.0567e-04 - val_loss: 0.2390 - 43s/epoch - 510us/sample
Epoch 52/85
84077/84077 - 43s - loss: 7.0524e-04 - val_loss: 0.2693 - 43s/epoch - 509us/sample
Epoch 53/85
84077/84077 - 43s - loss: 7.0333e-04 - val_loss: 0.2648 - 43s/epoch - 511us/sample
Epoch 54/85
84077/84077 - 43s - loss: 7.0324e-04 - val_loss: 0.2531 - 43s/epoch - 508us/sample
Epoch 55/85
84077/84077 - 43s - loss: 7.0207e-04 - val_loss: 0.3112 - 43s/epoch - 510us/sample
Epoch 56/85
84077/84077 - 43s - loss: 7.0029e-04 - val_loss: 0.2423 - 43s/epoch - 508us/sample
Epoch 57/85
84077/84077 - 43s - loss: 7.0048e-04 - val_loss: 0.2261 - 43s/epoch - 510us/sample
Epoch 58/85
84077/84077 - 43s - loss: 6.9900e-04 - val_loss: 0.2734 - 43s/epoch - 511us/sample
Epoch 59/85
84077/84077 - 43s - loss: 6.9907e-04 - val_loss: 0.2324 - 43s/epoch - 508us/sample
Epoch 60/85
84077/84077 - 43s - loss: 6.9728e-04 - val_loss: 0.2447 - 43s/epoch - 512us/sample
Epoch 61/85
84077/84077 - 43s - loss: 6.9750e-04 - val_loss: 0.2319 - 43s/epoch - 508us/sample
Epoch 62/85
84077/84077 - 43s - loss: 6.9562e-04 - val_loss: 0.2128 - 43s/epoch - 510us/sample
Epoch 63/85
84077/84077 - 43s - loss: 6.9622e-04 - val_loss: 0.2272 - 43s/epoch - 509us/sample
Epoch 64/85
84077/84077 - 43s - loss: 6.9581e-04 - val_loss: 0.2052 - 43s/epoch - 509us/sample
Epoch 65/85
84077/84077 - 43s - loss: 6.9555e-04 - val_loss: 0.2143 - 43s/epoch - 511us/sample
Epoch 66/85
84077/84077 - 43s - loss: 6.9513e-04 - val_loss: 0.2179 - 43s/epoch - 507us/sample
Epoch 67/85
84077/84077 - 43s - loss: 6.9394e-04 - val_loss: 0.1935 - 43s/epoch - 510us/sample
Epoch 68/85
84077/84077 - 43s - loss: 6.9287e-04 - val_loss: 0.2321 - 43s/epoch - 509us/sample
Epoch 69/85
84077/84077 - 43s - loss: 6.9298e-04 - val_loss: 0.1910 - 43s/epoch - 508us/sample
Epoch 70/85
84077/84077 - 43s - loss: 6.9213e-04 - val_loss: 0.1835 - 43s/epoch - 511us/sample
Epoch 71/85
84077/84077 - 43s - loss: 6.9250e-04 - val_loss: 0.2043 - 43s/epoch - 508us/sample
Epoch 72/85
84077/84077 - 43s - loss: 6.9097e-04 - val_loss: 0.1954 - 43s/epoch - 510us/sample
Epoch 73/85
84077/84077 - 43s - loss: 6.9138e-04 - val_loss: 0.2280 - 43s/epoch - 510us/sample
Epoch 74/85
84077/84077 - 43s - loss: 6.9029e-04 - val_loss: 0.2566 - 43s/epoch - 508us/sample
Epoch 75/85
84077/84077 - 43s - loss: 6.9103e-04 - val_loss: 0.2035 - 43s/epoch - 511us/sample
Epoch 76/85
84077/84077 - 43s - loss: 6.8932e-04 - val_loss: 0.2196 - 43s/epoch - 508us/sample
Epoch 77/85
84077/84077 - 43s - loss: 6.8843e-04 - val_loss: 0.2791 - 43s/epoch - 509us/sample
Epoch 78/85
84077/84077 - 43s - loss: 6.8869e-04 - val_loss: 0.1957 - 43s/epoch - 510us/sample
Epoch 79/85
84077/84077 - 43s - loss: 6.8793e-04 - val_loss: 0.2641 - 43s/epoch - 509us/sample
Epoch 80/85
84077/84077 - 43s - loss: 6.8736e-04 - val_loss: 0.2764 - 43s/epoch - 509us/sample
Epoch 81/85
84077/84077 - 43s - loss: 6.8702e-04 - val_loss: 0.2017 - 43s/epoch - 509us/sample
Epoch 82/85
84077/84077 - 43s - loss: 6.8740e-04 - val_loss: 0.2122 - 43s/epoch - 510us/sample
Epoch 83/85
84077/84077 - 43s - loss: 6.8800e-04 - val_loss: 0.2485 - 43s/epoch - 510us/sample
Epoch 84/85
84077/84077 - 43s - loss: 6.8611e-04 - val_loss: 0.1870 - 43s/epoch - 510us/sample
Epoch 85/85
84077/84077 - 43s - loss: 6.8530e-04 - val_loss: 0.2246 - 43s/epoch - 511us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.224606983779992
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 20:01:27.301120: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_49/outputlayer/BiasAdd' id:62490 op device:{requested: '', assigned: ''} def:{{{node decoder_model_49/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_49/outputlayer/MatMul, decoder_model_49/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.24767266651989003
cosine 0.24571973511824696
MAE: 0.040040969303651884
RMSE: 0.4728131942495113
r2: -173.61879584771006
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 8, 85, 0.001, 0.2, 188, 0.0006853034751228141, 0.224606983779992, 0.24767266651989003, 0.24571973511824696, 0.040040969303651884, 0.4728131942495113, -173.61879584771006, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5999999999999996 90 0.0012000000000000001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1508)         1423552     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_150 (Batch  (None, 1508)        6032        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_150 (ReLU)               (None, 1508)         0           ['batch_normalization_150[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          283692      ['re_lu_150[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          283692      ['re_lu_150[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1750315     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,747,283
Trainable params: 3,740,875
Non-trainable params: 6,408
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 20:01:47.760720: W tensorflow/c/c_api.cc:291] Operation '{name:'training_100/Adam/dense_dec0_50/bias/m/Assign' id:64311 op device:{requested: '', assigned: ''} def:{{{node training_100/Adam/dense_dec0_50/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_100/Adam/dense_dec0_50/bias/m, training_100/Adam/dense_dec0_50/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 20:02:05.461513: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_50/mul' id:63774 op device:{requested: '', assigned: ''} def:{{{node loss_50/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_50/mul/x, loss_50/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 26s - loss: 0.0061 - val_loss: 0.0025 - 26s/epoch - 308us/sample
Epoch 2/90
84077/84077 - 13s - loss: 0.0028 - val_loss: 0.0016 - 13s/epoch - 160us/sample
Epoch 3/90
84077/84077 - 13s - loss: 0.0013 - val_loss: 0.0010 - 13s/epoch - 159us/sample
Epoch 4/90
84077/84077 - 13s - loss: 0.0018 - val_loss: 0.0045 - 13s/epoch - 159us/sample
Epoch 5/90
84077/84077 - 13s - loss: 0.0012 - val_loss: 7.9660e-04 - 13s/epoch - 159us/sample
Epoch 6/90
84077/84077 - 13s - loss: 8.3106e-04 - val_loss: 6.6954e-04 - 13s/epoch - 158us/sample
Epoch 7/90
84077/84077 - 13s - loss: 7.2594e-04 - val_loss: 5.8629e-04 - 13s/epoch - 159us/sample
Epoch 8/90
84077/84077 - 13s - loss: 6.2155e-04 - val_loss: 5.1819e-04 - 13s/epoch - 160us/sample
Epoch 9/90
84077/84077 - 13s - loss: 5.6583e-04 - val_loss: 4.7489e-04 - 13s/epoch - 159us/sample
Epoch 10/90
84077/84077 - 13s - loss: 5.1148e-04 - val_loss: 4.6235e-04 - 13s/epoch - 159us/sample
Epoch 11/90
84077/84077 - 13s - loss: 4.7880e-04 - val_loss: 4.2514e-04 - 13s/epoch - 158us/sample
Epoch 12/90
84077/84077 - 13s - loss: 4.5674e-04 - val_loss: 3.9856e-04 - 13s/epoch - 158us/sample
Epoch 13/90
84077/84077 - 13s - loss: 4.3822e-04 - val_loss: 3.8582e-04 - 13s/epoch - 159us/sample
Epoch 14/90
84077/84077 - 13s - loss: 4.2237e-04 - val_loss: 3.7512e-04 - 13s/epoch - 160us/sample
Epoch 15/90
84077/84077 - 13s - loss: 4.1191e-04 - val_loss: 3.8654e-04 - 13s/epoch - 159us/sample
Epoch 16/90
84077/84077 - 13s - loss: 4.0139e-04 - val_loss: 3.6288e-04 - 13s/epoch - 159us/sample
Epoch 17/90
84077/84077 - 13s - loss: 3.9168e-04 - val_loss: 3.5984e-04 - 13s/epoch - 158us/sample
Epoch 18/90
84077/84077 - 13s - loss: 3.8073e-04 - val_loss: 3.3499e-04 - 13s/epoch - 159us/sample
Epoch 19/90
84077/84077 - 13s - loss: 3.7202e-04 - val_loss: 3.3267e-04 - 13s/epoch - 161us/sample
Epoch 20/90
84077/84077 - 13s - loss: 3.6319e-04 - val_loss: 3.2166e-04 - 13s/epoch - 160us/sample
Epoch 21/90
84077/84077 - 13s - loss: 3.5823e-04 - val_loss: 3.1819e-04 - 13s/epoch - 159us/sample
Epoch 22/90
84077/84077 - 13s - loss: 3.5315e-04 - val_loss: 3.1650e-04 - 13s/epoch - 159us/sample
Epoch 23/90
84077/84077 - 13s - loss: 3.4638e-04 - val_loss: 3.0806e-04 - 13s/epoch - 159us/sample
Epoch 24/90
84077/84077 - 13s - loss: 3.4230e-04 - val_loss: 3.0305e-04 - 13s/epoch - 159us/sample
Epoch 25/90
84077/84077 - 13s - loss: 3.3794e-04 - val_loss: 3.0072e-04 - 13s/epoch - 160us/sample
Epoch 26/90
84077/84077 - 13s - loss: 3.3293e-04 - val_loss: 2.9796e-04 - 13s/epoch - 159us/sample
Epoch 27/90
84077/84077 - 13s - loss: 3.2842e-04 - val_loss: 2.9330e-04 - 13s/epoch - 159us/sample
Epoch 28/90
84077/84077 - 13s - loss: 3.2503e-04 - val_loss: 2.8768e-04 - 13s/epoch - 159us/sample
Epoch 29/90
84077/84077 - 13s - loss: 3.2106e-04 - val_loss: 2.9032e-04 - 13s/epoch - 159us/sample
Epoch 30/90
84077/84077 - 13s - loss: 3.1597e-04 - val_loss: 2.8499e-04 - 13s/epoch - 159us/sample
Epoch 31/90
84077/84077 - 13s - loss: 3.1316e-04 - val_loss: 2.7747e-04 - 13s/epoch - 161us/sample
Epoch 32/90
84077/84077 - 13s - loss: 3.1070e-04 - val_loss: 2.7588e-04 - 13s/epoch - 159us/sample
Epoch 33/90
84077/84077 - 13s - loss: 3.0805e-04 - val_loss: 2.7380e-04 - 13s/epoch - 158us/sample
Epoch 34/90
84077/84077 - 13s - loss: 3.0316e-04 - val_loss: 2.7042e-04 - 13s/epoch - 158us/sample
Epoch 35/90
84077/84077 - 13s - loss: 3.0193e-04 - val_loss: 2.6897e-04 - 13s/epoch - 158us/sample
Epoch 36/90
84077/84077 - 13s - loss: 2.9794e-04 - val_loss: 2.7252e-04 - 13s/epoch - 160us/sample
Epoch 37/90
84077/84077 - 13s - loss: 2.9743e-04 - val_loss: 2.6426e-04 - 13s/epoch - 159us/sample
Epoch 38/90
84077/84077 - 13s - loss: 2.9467e-04 - val_loss: 2.6270e-04 - 13s/epoch - 159us/sample
Epoch 39/90
84077/84077 - 13s - loss: 2.9183e-04 - val_loss: 2.6381e-04 - 13s/epoch - 158us/sample
Epoch 40/90
84077/84077 - 13s - loss: 2.9139e-04 - val_loss: 2.6610e-04 - 13s/epoch - 158us/sample
Epoch 41/90
84077/84077 - 13s - loss: 2.8982e-04 - val_loss: 2.6581e-04 - 13s/epoch - 158us/sample
Epoch 42/90
84077/84077 - 14s - loss: 2.8552e-04 - val_loss: 2.5772e-04 - 14s/epoch - 161us/sample
Epoch 43/90
84077/84077 - 13s - loss: 2.8579e-04 - val_loss: 2.5458e-04 - 13s/epoch - 159us/sample
Epoch 44/90
84077/84077 - 13s - loss: 2.8199e-04 - val_loss: 2.5460e-04 - 13s/epoch - 159us/sample
Epoch 45/90
84077/84077 - 13s - loss: 2.8117e-04 - val_loss: 2.5436e-04 - 13s/epoch - 159us/sample
Epoch 46/90
84077/84077 - 13s - loss: 2.8159e-04 - val_loss: 2.5833e-04 - 13s/epoch - 159us/sample
Epoch 47/90
84077/84077 - 13s - loss: 2.7639e-04 - val_loss: 2.4998e-04 - 13s/epoch - 160us/sample
Epoch 48/90
84077/84077 - 13s - loss: 2.7641e-04 - val_loss: 2.5095e-04 - 13s/epoch - 160us/sample
Epoch 49/90
84077/84077 - 13s - loss: 2.7412e-04 - val_loss: 2.4783e-04 - 13s/epoch - 159us/sample
Epoch 50/90
84077/84077 - 13s - loss: 2.7359e-04 - val_loss: 2.4834e-04 - 13s/epoch - 158us/sample
Epoch 51/90
84077/84077 - 13s - loss: 2.7341e-04 - val_loss: 2.4535e-04 - 13s/epoch - 158us/sample
Epoch 52/90
84077/84077 - 13s - loss: 2.7192e-04 - val_loss: 2.4386e-04 - 13s/epoch - 159us/sample
Epoch 53/90
84077/84077 - 14s - loss: 2.7006e-04 - val_loss: 2.4960e-04 - 14s/epoch - 161us/sample
Epoch 54/90
84077/84077 - 13s - loss: 2.6806e-04 - val_loss: 2.4440e-04 - 13s/epoch - 159us/sample
Epoch 55/90
84077/84077 - 13s - loss: 2.6732e-04 - val_loss: 2.3926e-04 - 13s/epoch - 159us/sample
Epoch 56/90
84077/84077 - 13s - loss: 2.6761e-04 - val_loss: 2.3906e-04 - 13s/epoch - 158us/sample
Epoch 57/90
84077/84077 - 13s - loss: 2.6577e-04 - val_loss: 2.4833e-04 - 13s/epoch - 158us/sample
Epoch 58/90
84077/84077 - 13s - loss: 2.6226e-04 - val_loss: 2.3838e-04 - 13s/epoch - 160us/sample
Epoch 59/90
84077/84077 - 13s - loss: 2.6467e-04 - val_loss: 2.4360e-04 - 13s/epoch - 160us/sample
Epoch 60/90
84077/84077 - 13s - loss: 2.6086e-04 - val_loss: 2.4524e-04 - 13s/epoch - 158us/sample
Epoch 61/90
84077/84077 - 13s - loss: 2.6057e-04 - val_loss: 2.3307e-04 - 13s/epoch - 158us/sample
Epoch 62/90
84077/84077 - 13s - loss: 2.5927e-04 - val_loss: 2.3526e-04 - 13s/epoch - 159us/sample
Epoch 63/90
84077/84077 - 13s - loss: 2.5776e-04 - val_loss: 2.3437e-04 - 13s/epoch - 159us/sample
Epoch 64/90
84077/84077 - 14s - loss: 2.5716e-04 - val_loss: 2.3702e-04 - 14s/epoch - 161us/sample
Epoch 65/90
84077/84077 - 13s - loss: 2.5515e-04 - val_loss: 2.3238e-04 - 13s/epoch - 159us/sample
Epoch 66/90
84077/84077 - 13s - loss: 2.5560e-04 - val_loss: 2.3559e-04 - 13s/epoch - 159us/sample
Epoch 67/90
84077/84077 - 13s - loss: 2.5552e-04 - val_loss: 2.3070e-04 - 13s/epoch - 158us/sample
Epoch 68/90
84077/84077 - 13s - loss: 2.5307e-04 - val_loss: 2.3100e-04 - 13s/epoch - 159us/sample
Epoch 69/90
84077/84077 - 13s - loss: 2.5091e-04 - val_loss: 2.2977e-04 - 13s/epoch - 160us/sample
Epoch 70/90
84077/84077 - 13s - loss: 2.5136e-04 - val_loss: 2.2730e-04 - 13s/epoch - 160us/sample
Epoch 71/90
84077/84077 - 13s - loss: 2.5078e-04 - val_loss: 2.3058e-04 - 13s/epoch - 159us/sample
Epoch 72/90
84077/84077 - 13s - loss: 2.5207e-04 - val_loss: 2.3162e-04 - 13s/epoch - 159us/sample
Epoch 73/90
84077/84077 - 13s - loss: 2.4846e-04 - val_loss: 2.2641e-04 - 13s/epoch - 158us/sample
Epoch 74/90
84077/84077 - 13s - loss: 2.5119e-04 - val_loss: 2.2839e-04 - 13s/epoch - 159us/sample
Epoch 75/90
84077/84077 - 13s - loss: 2.5332e-04 - val_loss: 2.2649e-04 - 13s/epoch - 160us/sample
Epoch 76/90
84077/84077 - 13s - loss: 2.4688e-04 - val_loss: 2.2918e-04 - 13s/epoch - 160us/sample
Epoch 77/90
84077/84077 - 13s - loss: 2.4856e-04 - val_loss: 2.2602e-04 - 13s/epoch - 158us/sample
Epoch 78/90
84077/84077 - 13s - loss: 2.4645e-04 - val_loss: 2.2498e-04 - 13s/epoch - 158us/sample
Epoch 79/90
84077/84077 - 13s - loss: 2.4434e-04 - val_loss: 2.2528e-04 - 13s/epoch - 159us/sample
Epoch 80/90
84077/84077 - 13s - loss: 2.4426e-04 - val_loss: 2.2719e-04 - 13s/epoch - 159us/sample
Epoch 81/90
84077/84077 - 13s - loss: 2.4484e-04 - val_loss: 2.2296e-04 - 13s/epoch - 160us/sample
Epoch 82/90
84077/84077 - 13s - loss: 2.4691e-04 - val_loss: 2.2592e-04 - 13s/epoch - 159us/sample
Epoch 83/90
84077/84077 - 13s - loss: 2.4367e-04 - val_loss: 2.2410e-04 - 13s/epoch - 159us/sample
Epoch 84/90
84077/84077 - 13s - loss: 2.4305e-04 - val_loss: 2.2461e-04 - 13s/epoch - 158us/sample
Epoch 85/90
84077/84077 - 13s - loss: 2.4259e-04 - val_loss: 2.2393e-04 - 13s/epoch - 159us/sample
Epoch 86/90
84077/84077 - 14s - loss: 2.4219e-04 - val_loss: 2.2540e-04 - 14s/epoch - 161us/sample
Epoch 87/90
84077/84077 - 13s - loss: 2.4226e-04 - val_loss: 2.2141e-04 - 13s/epoch - 159us/sample
Epoch 88/90
84077/84077 - 13s - loss: 2.4173e-04 - val_loss: 2.2562e-04 - 13s/epoch - 159us/sample
Epoch 89/90
84077/84077 - 13s - loss: 2.4108e-04 - val_loss: 2.2237e-04 - 13s/epoch - 159us/sample
Epoch 90/90
84077/84077 - 13s - loss: 2.3947e-04 - val_loss: 2.2417e-04 - 13s/epoch - 158us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002241714597098271
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 20:22:00.316514: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_50/outputlayer/BiasAdd' id:63745 op device:{requested: '', assigned: ''} def:{{{node decoder_model_50/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_50/outputlayer/MatMul, decoder_model_50/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030470646385925452
cosine 0.03010417292058947
MAE: 0.0023696263478935773
RMSE: 0.010758140532115462
r2: 0.9096514988500142
RMSE zero-vector: 0.04004287452915337
['1.5999999999999996custom_VAE', 'mse', 32, 90, 0.0012000000000000001, 0.2, 188, 0.00023946548436405924, 0.0002241714597098271, 0.030470646385925452, 0.03010417292058947, 0.0023696263478935773, 0.010758140532115462, 0.9096514988500142, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 6
Fitness    = 512.1418577388646
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 512.1418577388646.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646]
[2.0 85 0.001 64 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_153 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_153 (ReLU)               (None, 1886)         0           ['batch_normalization_153[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_153[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_153[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 20:22:21.930920: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_51/bias/Assign' id:64692 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_51/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_51/bias, bottleneck_zmean_51/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 20:22:35.093760: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_51/mul' id:65039 op device:{requested: '', assigned: ''} def:{{{node loss_51/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_51/mul/x, loss_51/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0042 - val_loss: 9.6771e-04 - 21s/epoch - 251us/sample
Epoch 2/85
84077/84077 - 8s - loss: 8.2734e-04 - val_loss: 8.0275e-04 - 8s/epoch - 101us/sample
Epoch 3/85
84077/84077 - 8s - loss: 9.1109e-04 - val_loss: 6.4581e-04 - 8s/epoch - 101us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0137 - val_loss: 0.0010 - 8s/epoch - 101us/sample
Epoch 5/85
84077/84077 - 8s - loss: 7.3971e-04 - val_loss: 5.7931e-04 - 8s/epoch - 101us/sample
Epoch 6/85
84077/84077 - 8s - loss: 6.6734e-04 - val_loss: 4.8279e-04 - 8s/epoch - 100us/sample
Epoch 7/85
84077/84077 - 9s - loss: 4.9749e-04 - val_loss: 4.4956e-04 - 9s/epoch - 101us/sample
Epoch 8/85
84077/84077 - 9s - loss: 4.1952e-04 - val_loss: 4.0833e-04 - 9s/epoch - 101us/sample
Epoch 9/85
84077/84077 - 8s - loss: 3.7200e-04 - val_loss: 3.2714e-04 - 8s/epoch - 101us/sample
Epoch 10/85
84077/84077 - 8s - loss: 3.6176e-04 - val_loss: 3.0255e-04 - 8s/epoch - 101us/sample
Epoch 11/85
84077/84077 - 8s - loss: 3.2868e-04 - val_loss: 2.8449e-04 - 8s/epoch - 101us/sample
Epoch 12/85
84077/84077 - 8s - loss: 2.9885e-04 - val_loss: 2.6816e-04 - 8s/epoch - 100us/sample
Epoch 13/85
84077/84077 - 8s - loss: 2.8563e-04 - val_loss: 2.5437e-04 - 8s/epoch - 101us/sample
Epoch 14/85
84077/84077 - 8s - loss: 2.6712e-04 - val_loss: 2.4531e-04 - 8s/epoch - 100us/sample
Epoch 15/85
84077/84077 - 8s - loss: 2.5651e-04 - val_loss: 2.3079e-04 - 8s/epoch - 101us/sample
Epoch 16/85
84077/84077 - 9s - loss: 2.4727e-04 - val_loss: 2.2230e-04 - 9s/epoch - 102us/sample
Epoch 17/85
84077/84077 - 8s - loss: 2.3949e-04 - val_loss: 2.1234e-04 - 8s/epoch - 101us/sample
Epoch 18/85
84077/84077 - 8s - loss: 2.3119e-04 - val_loss: 2.0592e-04 - 8s/epoch - 101us/sample
Epoch 19/85
84077/84077 - 8s - loss: 2.2481e-04 - val_loss: 1.9769e-04 - 8s/epoch - 101us/sample
Epoch 20/85
84077/84077 - 8s - loss: 2.1624e-04 - val_loss: 1.9520e-04 - 8s/epoch - 101us/sample
Epoch 21/85
84077/84077 - 8s - loss: 2.1457e-04 - val_loss: 1.9061e-04 - 8s/epoch - 101us/sample
Epoch 22/85
84077/84077 - 8s - loss: 2.0542e-04 - val_loss: 1.8272e-04 - 8s/epoch - 101us/sample
Epoch 23/85
84077/84077 - 8s - loss: 1.9931e-04 - val_loss: 1.7963e-04 - 8s/epoch - 101us/sample
Epoch 24/85
84077/84077 - 9s - loss: 1.9460e-04 - val_loss: 1.7685e-04 - 9s/epoch - 102us/sample
Epoch 25/85
84077/84077 - 9s - loss: 1.9084e-04 - val_loss: 1.7120e-04 - 9s/epoch - 101us/sample
Epoch 26/85
84077/84077 - 8s - loss: 1.8672e-04 - val_loss: 1.6921e-04 - 8s/epoch - 101us/sample
Epoch 27/85
84077/84077 - 8s - loss: 1.8413e-04 - val_loss: 1.6737e-04 - 8s/epoch - 101us/sample
Epoch 28/85
84077/84077 - 8s - loss: 1.8136e-04 - val_loss: 1.6507e-04 - 8s/epoch - 101us/sample
Epoch 29/85
84077/84077 - 8s - loss: 1.7929e-04 - val_loss: 1.6267e-04 - 8s/epoch - 101us/sample
Epoch 30/85
84077/84077 - 8s - loss: 1.7651e-04 - val_loss: 1.6271e-04 - 8s/epoch - 101us/sample
Epoch 31/85
84077/84077 - 8s - loss: 1.7422e-04 - val_loss: 1.6012e-04 - 8s/epoch - 101us/sample
Epoch 32/85
84077/84077 - 9s - loss: 1.7255e-04 - val_loss: 1.5770e-04 - 9s/epoch - 101us/sample
Epoch 33/85
84077/84077 - 9s - loss: 1.7108e-04 - val_loss: 1.5682e-04 - 9s/epoch - 101us/sample
Epoch 34/85
84077/84077 - 8s - loss: 1.6966e-04 - val_loss: 1.5536e-04 - 8s/epoch - 101us/sample
Epoch 35/85
84077/84077 - 8s - loss: 1.6767e-04 - val_loss: 1.5635e-04 - 8s/epoch - 101us/sample
Epoch 36/85
84077/84077 - 8s - loss: 1.6680e-04 - val_loss: 1.5431e-04 - 8s/epoch - 101us/sample
Epoch 37/85
84077/84077 - 8s - loss: 1.6551e-04 - val_loss: 1.5262e-04 - 8s/epoch - 101us/sample
Epoch 38/85
84077/84077 - 8s - loss: 1.6384e-04 - val_loss: 1.5130e-04 - 8s/epoch - 101us/sample
Epoch 39/85
84077/84077 - 8s - loss: 1.6270e-04 - val_loss: 1.5084e-04 - 8s/epoch - 101us/sample
Epoch 40/85
84077/84077 - 8s - loss: 1.6153e-04 - val_loss: 1.4808e-04 - 8s/epoch - 101us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.5991e-04 - val_loss: 1.5020e-04 - 9s/epoch - 102us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.5892e-04 - val_loss: 1.4722e-04 - 9s/epoch - 101us/sample
Epoch 43/85
84077/84077 - 8s - loss: 1.5786e-04 - val_loss: 1.4508e-04 - 8s/epoch - 101us/sample
Epoch 44/85
84077/84077 - 8s - loss: 1.5690e-04 - val_loss: 1.4479e-04 - 8s/epoch - 101us/sample
Epoch 45/85
84077/84077 - 8s - loss: 1.5619e-04 - val_loss: 1.4530e-04 - 8s/epoch - 101us/sample
Epoch 46/85
84077/84077 - 8s - loss: 1.5537e-04 - val_loss: 1.4414e-04 - 8s/epoch - 101us/sample
Epoch 47/85
84077/84077 - 8s - loss: 1.5486e-04 - val_loss: 1.4403e-04 - 8s/epoch - 101us/sample
Epoch 48/85
84077/84077 - 8s - loss: 1.5362e-04 - val_loss: 1.4271e-04 - 8s/epoch - 101us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.5280e-04 - val_loss: 1.4349e-04 - 9s/epoch - 102us/sample
Epoch 50/85
84077/84077 - 8s - loss: 1.5222e-04 - val_loss: 1.4357e-04 - 8s/epoch - 101us/sample
Epoch 51/85
84077/84077 - 8s - loss: 1.5148e-04 - val_loss: 1.4245e-04 - 8s/epoch - 101us/sample
Epoch 52/85
84077/84077 - 8s - loss: 1.5091e-04 - val_loss: 1.4054e-04 - 8s/epoch - 101us/sample
Epoch 53/85
84077/84077 - 8s - loss: 1.5043e-04 - val_loss: 1.4068e-04 - 8s/epoch - 101us/sample
Epoch 54/85
84077/84077 - 8s - loss: 1.5008e-04 - val_loss: 1.4101e-04 - 8s/epoch - 101us/sample
Epoch 55/85
84077/84077 - 8s - loss: 1.4893e-04 - val_loss: 1.3987e-04 - 8s/epoch - 101us/sample
Epoch 56/85
84077/84077 - 8s - loss: 1.4901e-04 - val_loss: 1.3832e-04 - 8s/epoch - 101us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.4832e-04 - val_loss: 1.4068e-04 - 9s/epoch - 102us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.4791e-04 - val_loss: 1.3871e-04 - 9s/epoch - 101us/sample
Epoch 59/85
84077/84077 - 8s - loss: 1.4742e-04 - val_loss: 1.3810e-04 - 8s/epoch - 101us/sample
Epoch 60/85
84077/84077 - 8s - loss: 1.4691e-04 - val_loss: 1.3838e-04 - 8s/epoch - 101us/sample
Epoch 61/85
84077/84077 - 8s - loss: 1.4701e-04 - val_loss: 1.3847e-04 - 8s/epoch - 101us/sample
Epoch 62/85
84077/84077 - 8s - loss: 1.4652e-04 - val_loss: 1.3840e-04 - 8s/epoch - 101us/sample
Epoch 63/85
84077/84077 - 8s - loss: 1.4586e-04 - val_loss: 1.3804e-04 - 8s/epoch - 101us/sample
Epoch 64/85
84077/84077 - 8s - loss: 1.4559e-04 - val_loss: 1.3864e-04 - 8s/epoch - 101us/sample
Epoch 65/85
84077/84077 - 8s - loss: 1.4515e-04 - val_loss: 1.3794e-04 - 8s/epoch - 101us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.4497e-04 - val_loss: 1.3768e-04 - 9s/epoch - 102us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.4459e-04 - val_loss: 1.3597e-04 - 9s/epoch - 101us/sample
Epoch 68/85
84077/84077 - 8s - loss: 1.4435e-04 - val_loss: 1.3648e-04 - 8s/epoch - 101us/sample
Epoch 69/85
84077/84077 - 8s - loss: 1.4386e-04 - val_loss: 1.3767e-04 - 8s/epoch - 101us/sample
Epoch 70/85
84077/84077 - 8s - loss: 1.4377e-04 - val_loss: 1.3559e-04 - 8s/epoch - 101us/sample
Epoch 71/85
84077/84077 - 8s - loss: 1.4356e-04 - val_loss: 1.3662e-04 - 8s/epoch - 101us/sample
Epoch 72/85
84077/84077 - 8s - loss: 1.4293e-04 - val_loss: 1.3713e-04 - 8s/epoch - 101us/sample
Epoch 73/85
84077/84077 - 8s - loss: 1.4325e-04 - val_loss: 1.3488e-04 - 8s/epoch - 101us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.4228e-04 - val_loss: 1.3455e-04 - 9s/epoch - 102us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.4211e-04 - val_loss: 1.3529e-04 - 9s/epoch - 101us/sample
Epoch 76/85
84077/84077 - 8s - loss: 1.4207e-04 - val_loss: 1.3438e-04 - 8s/epoch - 101us/sample
Epoch 77/85
84077/84077 - 8s - loss: 1.4144e-04 - val_loss: 1.3451e-04 - 8s/epoch - 101us/sample
Epoch 78/85
84077/84077 - 8s - loss: 1.4146e-04 - val_loss: 1.3410e-04 - 8s/epoch - 101us/sample
Epoch 79/85
84077/84077 - 8s - loss: 1.4117e-04 - val_loss: 1.3355e-04 - 8s/epoch - 101us/sample
Epoch 80/85
84077/84077 - 8s - loss: 1.4118e-04 - val_loss: 1.3341e-04 - 8s/epoch - 101us/sample
Epoch 81/85
84077/84077 - 8s - loss: 1.4105e-04 - val_loss: 1.3461e-04 - 8s/epoch - 101us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.4018e-04 - val_loss: 1.3346e-04 - 9s/epoch - 101us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.4016e-04 - val_loss: 1.3407e-04 - 9s/epoch - 101us/sample
Epoch 84/85
84077/84077 - 8s - loss: 1.4025e-04 - val_loss: 1.3293e-04 - 8s/epoch - 101us/sample
Epoch 85/85
84077/84077 - 8s - loss: 1.3986e-04 - val_loss: 1.3243e-04 - 8s/epoch - 101us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00013243039323294542
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 20:34:32.092263: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_51/outputlayer/BiasAdd' id:65003 op device:{requested: '', assigned: ''} def:{{{node decoder_model_51/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_51/outputlayer/MatMul, decoder_model_51/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.033700405674483486
cosine 0.033284058725369936
MAE: 0.0022313549432193577
RMSE: 0.010850699174883622
r2: 0.9081163302248866
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 85, 0.001, 0.2, 188, 0.00013985664976716504, 0.00013243039323294542, 0.033700405674483486, 0.033284058725369936, 0.0022313549432193577, 0.010850699174883622, 0.9081163302248866, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 85 0.0008 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_156 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_156 (ReLU)               (None, 1603)         0           ['batch_normalization_156[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_156[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_156[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 20:34:53.535819: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_158/beta/Assign' id:66133 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_158/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_158/beta, batch_normalization_158/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 20:35:06.808744: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_52/mul' id:66317 op device:{requested: '', assigned: ''} def:{{{node loss_52/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_52/mul/x, loss_52/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0071 - val_loss: 0.0016 - 21s/epoch - 255us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0015 - val_loss: 0.0020 - 8s/epoch - 99us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0050 - val_loss: 0.0013 - 8s/epoch - 99us/sample
Epoch 4/85
84077/84077 - 8s - loss: 6.3660 - val_loss: 0.0021 - 8s/epoch - 99us/sample
Epoch 5/85
84077/84077 - 8s - loss: 0.0016 - val_loss: 0.0012 - 8s/epoch - 99us/sample
Epoch 6/85
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0012 - 8s/epoch - 99us/sample
Epoch 7/85
84077/84077 - 8s - loss: 0.0011 - val_loss: 8.5317e-04 - 8s/epoch - 99us/sample
Epoch 8/85
84077/84077 - 8s - loss: 8.6410e-04 - val_loss: 0.0022 - 8s/epoch - 99us/sample
Epoch 9/85
84077/84077 - 8s - loss: 0.0012 - val_loss: 6.9138e-04 - 8s/epoch - 99us/sample
Epoch 10/85
84077/84077 - 8s - loss: 7.1325e-04 - val_loss: 5.9279e-04 - 8s/epoch - 100us/sample
Epoch 11/85
84077/84077 - 8s - loss: 6.7997e-04 - val_loss: 5.6018e-04 - 8s/epoch - 99us/sample
Epoch 12/85
84077/84077 - 8s - loss: 6.0100e-04 - val_loss: 5.0692e-04 - 8s/epoch - 99us/sample
Epoch 13/85
84077/84077 - 8s - loss: 5.3694e-04 - val_loss: 4.7995e-04 - 8s/epoch - 99us/sample
Epoch 14/85
84077/84077 - 8s - loss: 5.3516e-04 - val_loss: 4.6424e-04 - 8s/epoch - 99us/sample
Epoch 15/85
84077/84077 - 8s - loss: 5.1053e-04 - val_loss: 4.7955e-04 - 8s/epoch - 99us/sample
Epoch 16/85
84077/84077 - 8s - loss: 5.0224e-04 - val_loss: 4.2268e-04 - 8s/epoch - 99us/sample
Epoch 17/85
84077/84077 - 8s - loss: 4.5204e-04 - val_loss: 4.0633e-04 - 8s/epoch - 99us/sample
Epoch 18/85
84077/84077 - 8s - loss: 4.3683e-04 - val_loss: 4.0691e-04 - 8s/epoch - 100us/sample
Epoch 19/85
84077/84077 - 8s - loss: 4.2455e-04 - val_loss: 3.8170e-04 - 8s/epoch - 100us/sample
Epoch 20/85
84077/84077 - 8s - loss: 4.1659e-04 - val_loss: 3.9149e-04 - 8s/epoch - 99us/sample
Epoch 21/85
84077/84077 - 8s - loss: 4.0203e-04 - val_loss: 3.6644e-04 - 8s/epoch - 99us/sample
Epoch 22/85
84077/84077 - 8s - loss: 3.8785e-04 - val_loss: 3.5941e-04 - 8s/epoch - 99us/sample
Epoch 23/85
84077/84077 - 8s - loss: 3.8275e-04 - val_loss: 3.4315e-04 - 8s/epoch - 99us/sample
Epoch 24/85
84077/84077 - 8s - loss: 3.7209e-04 - val_loss: 3.4011e-04 - 8s/epoch - 99us/sample
Epoch 25/85
84077/84077 - 8s - loss: 3.6605e-04 - val_loss: 3.3539e-04 - 8s/epoch - 99us/sample
Epoch 26/85
84077/84077 - 8s - loss: 3.6042e-04 - val_loss: 3.2667e-04 - 8s/epoch - 99us/sample
Epoch 27/85
84077/84077 - 8s - loss: 3.5574e-04 - val_loss: 3.2613e-04 - 8s/epoch - 100us/sample
Epoch 28/85
84077/84077 - 8s - loss: 3.5013e-04 - val_loss: 3.1877e-04 - 8s/epoch - 99us/sample
Epoch 29/85
84077/84077 - 8s - loss: 3.4406e-04 - val_loss: 3.1433e-04 - 8s/epoch - 99us/sample
Epoch 30/85
84077/84077 - 8s - loss: 3.3945e-04 - val_loss: 3.1402e-04 - 8s/epoch - 99us/sample
Epoch 31/85
84077/84077 - 8s - loss: 3.3432e-04 - val_loss: 3.0943e-04 - 8s/epoch - 99us/sample
Epoch 32/85
84077/84077 - 8s - loss: 3.3201e-04 - val_loss: 3.0439e-04 - 8s/epoch - 99us/sample
Epoch 33/85
84077/84077 - 8s - loss: 3.2907e-04 - val_loss: 3.0462e-04 - 8s/epoch - 99us/sample
Epoch 34/85
84077/84077 - 8s - loss: 3.2528e-04 - val_loss: 3.0215e-04 - 8s/epoch - 99us/sample
Epoch 35/85
84077/84077 - 8s - loss: 3.2277e-04 - val_loss: 3.0151e-04 - 8s/epoch - 100us/sample
Epoch 36/85
84077/84077 - 8s - loss: 3.2078e-04 - val_loss: 2.9783e-04 - 8s/epoch - 100us/sample
Epoch 37/85
84077/84077 - 8s - loss: 3.1829e-04 - val_loss: 2.9644e-04 - 8s/epoch - 99us/sample
Epoch 38/85
84077/84077 - 8s - loss: 3.1761e-04 - val_loss: 2.9693e-04 - 8s/epoch - 99us/sample
Epoch 39/85
84077/84077 - 8s - loss: 3.1554e-04 - val_loss: 2.9291e-04 - 8s/epoch - 99us/sample
Epoch 40/85
84077/84077 - 8s - loss: 3.1362e-04 - val_loss: 2.9210e-04 - 8s/epoch - 99us/sample
Epoch 41/85
84077/84077 - 8s - loss: 3.1246e-04 - val_loss: 2.9315e-04 - 8s/epoch - 99us/sample
Epoch 42/85
84077/84077 - 8s - loss: 3.1090e-04 - val_loss: 2.8828e-04 - 8s/epoch - 99us/sample
Epoch 43/85
84077/84077 - 8s - loss: 3.0956e-04 - val_loss: 2.8931e-04 - 8s/epoch - 100us/sample
Epoch 44/85
84077/84077 - 8s - loss: 3.0843e-04 - val_loss: 2.8865e-04 - 8s/epoch - 100us/sample
Epoch 45/85
84077/84077 - 8s - loss: 3.0795e-04 - val_loss: 2.8668e-04 - 8s/epoch - 99us/sample
Epoch 46/85
84077/84077 - 8s - loss: 3.0637e-04 - val_loss: 2.8511e-04 - 8s/epoch - 99us/sample
Epoch 47/85
84077/84077 - 8s - loss: 3.0614e-04 - val_loss: 2.8617e-04 - 8s/epoch - 99us/sample
Epoch 48/85
84077/84077 - 8s - loss: 3.0447e-04 - val_loss: 2.8699e-04 - 8s/epoch - 99us/sample
Epoch 49/85
84077/84077 - 8s - loss: 3.0336e-04 - val_loss: 2.8572e-04 - 8s/epoch - 99us/sample
Epoch 50/85
84077/84077 - 8s - loss: 3.0363e-04 - val_loss: 2.8445e-04 - 8s/epoch - 99us/sample
Epoch 51/85
84077/84077 - 8s - loss: 3.0258e-04 - val_loss: 2.8313e-04 - 8s/epoch - 100us/sample
Epoch 52/85
84077/84077 - 8s - loss: 3.0160e-04 - val_loss: 2.8252e-04 - 8s/epoch - 100us/sample
Epoch 53/85
84077/84077 - 8s - loss: 3.0118e-04 - val_loss: 2.8625e-04 - 8s/epoch - 99us/sample
Epoch 54/85
84077/84077 - 8s - loss: 2.9970e-04 - val_loss: 2.8230e-04 - 8s/epoch - 99us/sample
Epoch 55/85
84077/84077 - 8s - loss: 2.9945e-04 - val_loss: 2.8247e-04 - 8s/epoch - 99us/sample
Epoch 56/85
84077/84077 - 8s - loss: 2.9911e-04 - val_loss: 2.8102e-04 - 8s/epoch - 99us/sample
Epoch 57/85
84077/84077 - 8s - loss: 2.9868e-04 - val_loss: 2.8231e-04 - 8s/epoch - 99us/sample
Epoch 58/85
84077/84077 - 8s - loss: 2.9737e-04 - val_loss: 2.8007e-04 - 8s/epoch - 99us/sample
Epoch 59/85
84077/84077 - 8s - loss: 2.9709e-04 - val_loss: 2.8000e-04 - 8s/epoch - 99us/sample
Epoch 60/85
84077/84077 - 8s - loss: 2.9685e-04 - val_loss: 2.7768e-04 - 8s/epoch - 100us/sample
Epoch 61/85
84077/84077 - 8s - loss: 2.9624e-04 - val_loss: 2.7897e-04 - 8s/epoch - 99us/sample
Epoch 62/85
84077/84077 - 8s - loss: 2.9568e-04 - val_loss: 2.7920e-04 - 8s/epoch - 99us/sample
Epoch 63/85
84077/84077 - 8s - loss: 2.9534e-04 - val_loss: 2.7675e-04 - 8s/epoch - 99us/sample
Epoch 64/85
84077/84077 - 8s - loss: 2.9403e-04 - val_loss: 2.7690e-04 - 8s/epoch - 99us/sample
Epoch 65/85
84077/84077 - 8s - loss: 2.9428e-04 - val_loss: 2.7736e-04 - 8s/epoch - 99us/sample
Epoch 66/85
84077/84077 - 8s - loss: 2.9408e-04 - val_loss: 2.7546e-04 - 8s/epoch - 99us/sample
Epoch 67/85
84077/84077 - 8s - loss: 2.9347e-04 - val_loss: 2.7686e-04 - 8s/epoch - 99us/sample
Epoch 68/85
84077/84077 - 8s - loss: 2.9244e-04 - val_loss: 2.7471e-04 - 8s/epoch - 100us/sample
Epoch 69/85
84077/84077 - 8s - loss: 2.9244e-04 - val_loss: 2.7455e-04 - 8s/epoch - 100us/sample
Epoch 70/85
84077/84077 - 8s - loss: 2.9247e-04 - val_loss: 2.7516e-04 - 8s/epoch - 99us/sample
Epoch 71/85
84077/84077 - 8s - loss: 2.9210e-04 - val_loss: 2.7382e-04 - 8s/epoch - 99us/sample
Epoch 72/85
84077/84077 - 8s - loss: 2.9128e-04 - val_loss: 2.7431e-04 - 8s/epoch - 99us/sample
Epoch 73/85
84077/84077 - 8s - loss: 2.9129e-04 - val_loss: 2.7426e-04 - 8s/epoch - 99us/sample
Epoch 74/85
84077/84077 - 8s - loss: 2.9045e-04 - val_loss: 2.7281e-04 - 8s/epoch - 99us/sample
Epoch 75/85
84077/84077 - 8s - loss: 2.9034e-04 - val_loss: 2.7321e-04 - 8s/epoch - 99us/sample
Epoch 76/85
84077/84077 - 8s - loss: 2.9024e-04 - val_loss: 2.8179e-04 - 8s/epoch - 100us/sample
Epoch 77/85
84077/84077 - 8s - loss: 2.8936e-04 - val_loss: 2.7183e-04 - 8s/epoch - 100us/sample
Epoch 78/85
84077/84077 - 8s - loss: 2.8925e-04 - val_loss: 2.7326e-04 - 8s/epoch - 99us/sample
Epoch 79/85
84077/84077 - 8s - loss: 2.8921e-04 - val_loss: 2.7247e-04 - 8s/epoch - 99us/sample
Epoch 80/85
84077/84077 - 8s - loss: 2.8884e-04 - val_loss: 2.7299e-04 - 8s/epoch - 99us/sample
Epoch 81/85
84077/84077 - 8s - loss: 2.8852e-04 - val_loss: 2.7203e-04 - 8s/epoch - 99us/sample
Epoch 82/85
84077/84077 - 8s - loss: 2.8849e-04 - val_loss: 2.7177e-04 - 8s/epoch - 99us/sample
Epoch 83/85
84077/84077 - 8s - loss: 2.8785e-04 - val_loss: 2.7241e-04 - 8s/epoch - 99us/sample
Epoch 84/85
84077/84077 - 8s - loss: 2.8678e-04 - val_loss: 2.6991e-04 - 8s/epoch - 99us/sample
Epoch 85/85
84077/84077 - 8s - loss: 2.8701e-04 - val_loss: 2.7225e-04 - 8s/epoch - 100us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0002722533162049407
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 20:46:51.520202: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_52/outputlayer/BiasAdd' id:66288 op device:{requested: '', assigned: ''} def:{{{node decoder_model_52/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_52/outputlayer/MatMul, decoder_model_52/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04667771787280614
cosine 0.046090503512227717
MAE: 0.0028140179120602136
RMSE: 0.013259455936376039
r2: 0.8627071377778334
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'mse', 64, 85, 0.0008, 0.2, 188, 0.00028701466085462177, 0.0002722533162049407, 0.04667771787280614, 0.046090503512227717, 0.0028140179120602136, 0.013259455936376039, 0.8627071377778334, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0012000000000000001 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_159 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_159 (ReLU)               (None, 1886)         0           ['batch_normalization_159[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_159[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_159[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 20:47:12.999130: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_53/kernel/Assign' id:67451 op device:{requested: '', assigned: ''} def:{{{node outputlayer_53/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_53/kernel, outputlayer_53/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 20:47:31.196352: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_53/mul' id:67572 op device:{requested: '', assigned: ''} def:{{{node loss_53/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_53/mul/x, loss_53/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0109 - val_loss: 0.0171 - 27s/epoch - 319us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0023 - val_loss: 0.0144 - 14s/epoch - 166us/sample
Epoch 3/85
84077/84077 - 14s - loss: 1.9715 - val_loss: 0.0025 - 14s/epoch - 164us/sample
Epoch 4/85
84077/84077 - 14s - loss: 0.0018 - val_loss: 0.0012 - 14s/epoch - 164us/sample
Epoch 5/85
84077/84077 - 14s - loss: 0.0013 - val_loss: 0.0014 - 14s/epoch - 163us/sample
Epoch 6/85
84077/84077 - 14s - loss: 0.0014 - val_loss: 0.0010 - 14s/epoch - 164us/sample
Epoch 7/85
84077/84077 - 14s - loss: 8.9832e-04 - val_loss: 0.0016 - 14s/epoch - 164us/sample
Epoch 8/85
84077/84077 - 14s - loss: 0.0030 - val_loss: 8.0735e-04 - 14s/epoch - 165us/sample
Epoch 9/85
84077/84077 - 14s - loss: 9.8034e-04 - val_loss: 6.6933e-04 - 14s/epoch - 165us/sample
Epoch 10/85
84077/84077 - 14s - loss: 7.2975e-04 - val_loss: 6.3739e-04 - 14s/epoch - 164us/sample
Epoch 11/85
84077/84077 - 14s - loss: 8.2117e-04 - val_loss: 6.8063e-04 - 14s/epoch - 164us/sample
Epoch 12/85
84077/84077 - 14s - loss: 0.0013 - val_loss: 6.0804e-04 - 14s/epoch - 164us/sample
Epoch 13/85
84077/84077 - 14s - loss: 6.4382e-04 - val_loss: 5.5269e-04 - 14s/epoch - 164us/sample
Epoch 14/85
84077/84077 - 14s - loss: 6.0968e-04 - val_loss: 5.3576e-04 - 14s/epoch - 164us/sample
Epoch 15/85
84077/84077 - 14s - loss: 5.7749e-04 - val_loss: 5.0932e-04 - 14s/epoch - 166us/sample
Epoch 16/85
84077/84077 - 14s - loss: 5.5685e-04 - val_loss: 4.8412e-04 - 14s/epoch - 164us/sample
Epoch 17/85
84077/84077 - 14s - loss: 5.3077e-04 - val_loss: 4.7877e-04 - 14s/epoch - 164us/sample
Epoch 18/85
84077/84077 - 14s - loss: 5.0901e-04 - val_loss: 4.6551e-04 - 14s/epoch - 164us/sample
Epoch 19/85
84077/84077 - 14s - loss: 4.9745e-04 - val_loss: 4.5457e-04 - 14s/epoch - 165us/sample
Epoch 20/85
84077/84077 - 14s - loss: 4.8572e-04 - val_loss: 4.4770e-04 - 14s/epoch - 165us/sample
Epoch 21/85
84077/84077 - 14s - loss: 4.7921e-04 - val_loss: 4.3687e-04 - 14s/epoch - 164us/sample
Epoch 22/85
84077/84077 - 14s - loss: 4.6976e-04 - val_loss: 4.3218e-04 - 14s/epoch - 163us/sample
Epoch 23/85
84077/84077 - 14s - loss: 4.6135e-04 - val_loss: 4.2612e-04 - 14s/epoch - 164us/sample
Epoch 24/85
84077/84077 - 14s - loss: 4.5526e-04 - val_loss: 4.2127e-04 - 14s/epoch - 164us/sample
Epoch 25/85
84077/84077 - 14s - loss: 4.5048e-04 - val_loss: 4.1891e-04 - 14s/epoch - 165us/sample
Epoch 26/85
84077/84077 - 14s - loss: 4.4623e-04 - val_loss: 4.1698e-04 - 14s/epoch - 166us/sample
Epoch 27/85
84077/84077 - 14s - loss: 4.4349e-04 - val_loss: 4.1363e-04 - 14s/epoch - 164us/sample
Epoch 28/85
84077/84077 - 14s - loss: 4.4009e-04 - val_loss: 4.1205e-04 - 14s/epoch - 164us/sample
Epoch 29/85
84077/84077 - 14s - loss: 4.3763e-04 - val_loss: 4.1030e-04 - 14s/epoch - 164us/sample
Epoch 30/85
84077/84077 - 14s - loss: 4.3559e-04 - val_loss: 4.0797e-04 - 14s/epoch - 164us/sample
Epoch 31/85
84077/84077 - 14s - loss: 4.3148e-04 - val_loss: 4.0683e-04 - 14s/epoch - 165us/sample
Epoch 32/85
84077/84077 - 14s - loss: 4.2924e-04 - val_loss: 4.0789e-04 - 14s/epoch - 166us/sample
Epoch 33/85
84077/84077 - 14s - loss: 4.2786e-04 - val_loss: 4.0669e-04 - 14s/epoch - 164us/sample
Epoch 34/85
84077/84077 - 14s - loss: 4.2595e-04 - val_loss: 4.0396e-04 - 14s/epoch - 164us/sample
Epoch 35/85
84077/84077 - 14s - loss: 4.2384e-04 - val_loss: 4.0551e-04 - 14s/epoch - 164us/sample
Epoch 36/85
84077/84077 - 14s - loss: 4.2310e-04 - val_loss: 4.0026e-04 - 14s/epoch - 164us/sample
Epoch 37/85
84077/84077 - 14s - loss: 4.2161e-04 - val_loss: 4.0428e-04 - 14s/epoch - 165us/sample
Epoch 38/85
84077/84077 - 14s - loss: 4.2017e-04 - val_loss: 3.9903e-04 - 14s/epoch - 164us/sample
Epoch 39/85
84077/84077 - 14s - loss: 4.1888e-04 - val_loss: 3.9850e-04 - 14s/epoch - 164us/sample
Epoch 40/85
84077/84077 - 14s - loss: 4.1744e-04 - val_loss: 3.9822e-04 - 14s/epoch - 164us/sample
Epoch 41/85
84077/84077 - 14s - loss: 4.1677e-04 - val_loss: 3.9826e-04 - 14s/epoch - 164us/sample
Epoch 42/85
84077/84077 - 14s - loss: 4.1591e-04 - val_loss: 3.9562e-04 - 14s/epoch - 165us/sample
Epoch 43/85
84077/84077 - 14s - loss: 4.1417e-04 - val_loss: 3.9287e-04 - 14s/epoch - 166us/sample
Epoch 44/85
84077/84077 - 14s - loss: 4.1305e-04 - val_loss: 3.9249e-04 - 14s/epoch - 164us/sample
Epoch 45/85
84077/84077 - 14s - loss: 4.1081e-04 - val_loss: 3.9165e-04 - 14s/epoch - 164us/sample
Epoch 46/85
84077/84077 - 14s - loss: 4.1030e-04 - val_loss: 3.8888e-04 - 14s/epoch - 164us/sample
Epoch 47/85
84077/84077 - 14s - loss: 4.0971e-04 - val_loss: 3.9157e-04 - 14s/epoch - 165us/sample
Epoch 48/85
84077/84077 - 14s - loss: 4.0817e-04 - val_loss: 3.8775e-04 - 14s/epoch - 165us/sample
Epoch 49/85
84077/84077 - 14s - loss: 4.0853e-04 - val_loss: 3.8848e-04 - 14s/epoch - 164us/sample
Epoch 50/85
84077/84077 - 14s - loss: 4.0640e-04 - val_loss: 3.8855e-04 - 14s/epoch - 164us/sample
Epoch 51/85
84077/84077 - 14s - loss: 4.0549e-04 - val_loss: 3.8746e-04 - 14s/epoch - 164us/sample
Epoch 52/85
84077/84077 - 14s - loss: 4.0492e-04 - val_loss: 3.8652e-04 - 14s/epoch - 164us/sample
Epoch 53/85
84077/84077 - 14s - loss: 4.0479e-04 - val_loss: 3.8324e-04 - 14s/epoch - 166us/sample
Epoch 54/85
84077/84077 - 14s - loss: 4.0412e-04 - val_loss: 3.8332e-04 - 14s/epoch - 164us/sample
Epoch 55/85
84077/84077 - 14s - loss: 4.0296e-04 - val_loss: 3.8357e-04 - 14s/epoch - 164us/sample
Epoch 56/85
84077/84077 - 14s - loss: 4.0310e-04 - val_loss: 3.8376e-04 - 14s/epoch - 164us/sample
Epoch 57/85
84077/84077 - 14s - loss: 4.0235e-04 - val_loss: 3.8499e-04 - 14s/epoch - 164us/sample
Epoch 58/85
84077/84077 - 14s - loss: 4.0229e-04 - val_loss: 3.8346e-04 - 14s/epoch - 164us/sample
Epoch 59/85
84077/84077 - 14s - loss: 4.0101e-04 - val_loss: 3.8366e-04 - 14s/epoch - 166us/sample
Epoch 60/85
84077/84077 - 14s - loss: 4.0093e-04 - val_loss: 3.8086e-04 - 14s/epoch - 164us/sample
Epoch 61/85
84077/84077 - 14s - loss: 4.0072e-04 - val_loss: 3.8277e-04 - 14s/epoch - 164us/sample
Epoch 62/85
84077/84077 - 14s - loss: 4.0061e-04 - val_loss: 3.8211e-04 - 14s/epoch - 164us/sample
Epoch 63/85
84077/84077 - 14s - loss: 3.9998e-04 - val_loss: 3.8106e-04 - 14s/epoch - 164us/sample
Epoch 64/85
84077/84077 - 14s - loss: 3.9936e-04 - val_loss: 3.8241e-04 - 14s/epoch - 165us/sample
Epoch 65/85
84077/84077 - 14s - loss: 3.9898e-04 - val_loss: 3.7977e-04 - 14s/epoch - 165us/sample
Epoch 66/85
84077/84077 - 14s - loss: 3.9848e-04 - val_loss: 3.8334e-04 - 14s/epoch - 164us/sample
Epoch 67/85
84077/84077 - 14s - loss: 3.9806e-04 - val_loss: 3.7854e-04 - 14s/epoch - 164us/sample
Epoch 68/85
84077/84077 - 14s - loss: 3.9751e-04 - val_loss: 3.7794e-04 - 14s/epoch - 164us/sample
Epoch 69/85
84077/84077 - 14s - loss: 3.9739e-04 - val_loss: 3.8170e-04 - 14s/epoch - 164us/sample
Epoch 70/85
84077/84077 - 14s - loss: 3.9697e-04 - val_loss: 3.7662e-04 - 14s/epoch - 165us/sample
Epoch 71/85
84077/84077 - 14s - loss: 3.9606e-04 - val_loss: 3.7698e-04 - 14s/epoch - 164us/sample
Epoch 72/85
84077/84077 - 14s - loss: 3.9554e-04 - val_loss: 3.7530e-04 - 14s/epoch - 163us/sample
Epoch 73/85
84077/84077 - 14s - loss: 3.9481e-04 - val_loss: 3.7523e-04 - 14s/epoch - 164us/sample
Epoch 74/85
84077/84077 - 14s - loss: 3.9520e-04 - val_loss: 3.7525e-04 - 14s/epoch - 164us/sample
Epoch 75/85
84077/84077 - 14s - loss: 3.9432e-04 - val_loss: 3.7685e-04 - 14s/epoch - 165us/sample
Epoch 76/85
84077/84077 - 14s - loss: 3.9396e-04 - val_loss: 3.7568e-04 - 14s/epoch - 166us/sample
Epoch 77/85
84077/84077 - 14s - loss: 3.9360e-04 - val_loss: 3.7669e-04 - 14s/epoch - 164us/sample
Epoch 78/85
84077/84077 - 14s - loss: 3.9349e-04 - val_loss: 3.7770e-04 - 14s/epoch - 164us/sample
Epoch 79/85
84077/84077 - 14s - loss: 3.9271e-04 - val_loss: 3.7386e-04 - 14s/epoch - 164us/sample
Epoch 80/85
84077/84077 - 14s - loss: 3.9309e-04 - val_loss: 3.7682e-04 - 14s/epoch - 164us/sample
Epoch 81/85
84077/84077 - 14s - loss: 3.9270e-04 - val_loss: 3.7715e-04 - 14s/epoch - 166us/sample
Epoch 82/85
84077/84077 - 14s - loss: 3.9182e-04 - val_loss: 3.7286e-04 - 14s/epoch - 165us/sample
Epoch 83/85
84077/84077 - 14s - loss: 3.9203e-04 - val_loss: 3.7669e-04 - 14s/epoch - 164us/sample
Epoch 84/85
84077/84077 - 14s - loss: 3.9188e-04 - val_loss: 3.7350e-04 - 14s/epoch - 164us/sample
Epoch 85/85
84077/84077 - 14s - loss: 3.9153e-04 - val_loss: 3.7248e-04 - 14s/epoch - 164us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00037247540166147344
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 21:06:56.860822: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_53/outputlayer/BiasAdd' id:67543 op device:{requested: '', assigned: ''} def:{{{node decoder_model_53/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_53/outputlayer/MatMul, decoder_model_53/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.07515064267709608
cosine 0.07422559447718365
MAE: 0.0032486564788371084
RMSE: 0.016726646569568827
r2: 0.7814588442759125
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 85, 0.0012000000000000001, 0.2, 188, 0.00039152775428807793, 0.00037247540166147344, 0.07515064267709608, 0.07422559447718365, 0.0032486564788371084, 0.016726646569568827, 0.7814588442759125, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0008 16 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_162 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_162 (ReLU)               (None, 1886)         0           ['batch_normalization_162[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_162[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_162[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 21:07:18.999312: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_54/bias/Assign' id:68545 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_54/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_54/bias, dense_dec1_54/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 21:07:47.433847: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_54/mul' id:68827 op device:{requested: '', assigned: ''} def:{{{node loss_54/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_54/mul/x, loss_54/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 38s - loss: 0.0108 - val_loss: 1722.4783 - 38s/epoch - 450us/sample
Epoch 2/90
84077/84077 - 24s - loss: 0.0033 - val_loss: 0.0062 - 24s/epoch - 289us/sample
Epoch 3/90
84077/84077 - 24s - loss: 0.0017 - val_loss: 0.0011 - 24s/epoch - 290us/sample
Epoch 4/90
84077/84077 - 24s - loss: 0.0016 - val_loss: 9.9317e-04 - 24s/epoch - 289us/sample
Epoch 5/90
84077/84077 - 24s - loss: 0.0010 - val_loss: 8.9921e-04 - 24s/epoch - 288us/sample
Epoch 6/90
84077/84077 - 24s - loss: 9.4903e-04 - val_loss: 8.5035e-04 - 24s/epoch - 289us/sample
Epoch 7/90
84077/84077 - 24s - loss: 8.9611e-04 - val_loss: 8.2164e-04 - 24s/epoch - 291us/sample
Epoch 8/90
84077/84077 - 24s - loss: 8.6663e-04 - val_loss: 7.7987e-04 - 24s/epoch - 288us/sample
Epoch 9/90
84077/84077 - 24s - loss: 8.2374e-04 - val_loss: 7.6135e-04 - 24s/epoch - 289us/sample
Epoch 10/90
84077/84077 - 24s - loss: 7.7390e-04 - val_loss: 7.9939e-04 - 24s/epoch - 291us/sample
Epoch 11/90
84077/84077 - 24s - loss: 7.4243e-04 - val_loss: 8.6778e-04 - 24s/epoch - 289us/sample
Epoch 12/90
84077/84077 - 24s - loss: 7.1020e-04 - val_loss: 9.0073e-04 - 24s/epoch - 289us/sample
Epoch 13/90
84077/84077 - 25s - loss: 6.7978e-04 - val_loss: 8.2554e-04 - 25s/epoch - 292us/sample
Epoch 14/90
84077/84077 - 24s - loss: 6.5387e-04 - val_loss: 8.3219e-04 - 24s/epoch - 288us/sample
Epoch 15/90
84077/84077 - 24s - loss: 6.2905e-04 - val_loss: 8.0961e-04 - 24s/epoch - 289us/sample
Epoch 16/90
84077/84077 - 24s - loss: 6.1097e-04 - val_loss: 7.7507e-04 - 24s/epoch - 291us/sample
Epoch 17/90
84077/84077 - 24s - loss: 5.9688e-04 - val_loss: 7.6875e-04 - 24s/epoch - 289us/sample
Epoch 18/90
84077/84077 - 24s - loss: 5.8754e-04 - val_loss: 7.0913e-04 - 24s/epoch - 289us/sample
Epoch 19/90
84077/84077 - 24s - loss: 5.8022e-04 - val_loss: 7.2262e-04 - 24s/epoch - 289us/sample
Epoch 20/90
84077/84077 - 24s - loss: 5.7382e-04 - val_loss: 6.9363e-04 - 24s/epoch - 291us/sample
Epoch 21/90
84077/84077 - 24s - loss: 5.6918e-04 - val_loss: 6.7433e-04 - 24s/epoch - 289us/sample
Epoch 22/90
84077/84077 - 24s - loss: 5.6309e-04 - val_loss: 6.8550e-04 - 24s/epoch - 289us/sample
Epoch 23/90
84077/84077 - 24s - loss: 5.5896e-04 - val_loss: 6.3627e-04 - 24s/epoch - 291us/sample
Epoch 24/90
84077/84077 - 24s - loss: 5.5437e-04 - val_loss: 6.1878e-04 - 24s/epoch - 289us/sample
Epoch 25/90
84077/84077 - 24s - loss: 5.4918e-04 - val_loss: 6.1937e-04 - 24s/epoch - 290us/sample
Epoch 26/90
84077/84077 - 24s - loss: 5.4577e-04 - val_loss: 5.8635e-04 - 24s/epoch - 291us/sample
Epoch 27/90
84077/84077 - 24s - loss: 5.4238e-04 - val_loss: 5.7766e-04 - 24s/epoch - 288us/sample
Epoch 28/90
84077/84077 - 24s - loss: 5.4064e-04 - val_loss: 6.0661e-04 - 24s/epoch - 289us/sample
Epoch 29/90
84077/84077 - 24s - loss: 5.3723e-04 - val_loss: 5.9355e-04 - 24s/epoch - 289us/sample
Epoch 30/90
84077/84077 - 25s - loss: 5.3408e-04 - val_loss: 5.7347e-04 - 25s/epoch - 292us/sample
Epoch 31/90
84077/84077 - 24s - loss: 5.3320e-04 - val_loss: 5.4011e-04 - 24s/epoch - 290us/sample
Epoch 32/90
84077/84077 - 24s - loss: 5.3119e-04 - val_loss: 5.4734e-04 - 24s/epoch - 289us/sample
Epoch 33/90
84077/84077 - 24s - loss: 5.2975e-04 - val_loss: 5.4623e-04 - 24s/epoch - 290us/sample
Epoch 34/90
84077/84077 - 24s - loss: 5.2733e-04 - val_loss: 5.4411e-04 - 24s/epoch - 289us/sample
Epoch 35/90
84077/84077 - 24s - loss: 5.2589e-04 - val_loss: 5.3958e-04 - 24s/epoch - 288us/sample
Epoch 36/90
84077/84077 - 24s - loss: 5.2479e-04 - val_loss: 5.3239e-04 - 24s/epoch - 289us/sample
Epoch 37/90
84077/84077 - 24s - loss: 5.2415e-04 - val_loss: 5.2391e-04 - 24s/epoch - 290us/sample
Epoch 38/90
84077/84077 - 24s - loss: 5.2129e-04 - val_loss: 5.2025e-04 - 24s/epoch - 289us/sample
Epoch 39/90
84077/84077 - 24s - loss: 5.2140e-04 - val_loss: 5.3404e-04 - 24s/epoch - 289us/sample
Epoch 40/90
84077/84077 - 24s - loss: 5.1977e-04 - val_loss: 5.1922e-04 - 24s/epoch - 291us/sample
Epoch 41/90
84077/84077 - 24s - loss: 5.1780e-04 - val_loss: 5.3194e-04 - 24s/epoch - 289us/sample
Epoch 42/90
84077/84077 - 24s - loss: 5.1659e-04 - val_loss: 5.1227e-04 - 24s/epoch - 288us/sample
Epoch 43/90
84077/84077 - 24s - loss: 5.1565e-04 - val_loss: 5.0781e-04 - 24s/epoch - 291us/sample
Epoch 44/90
84077/84077 - 24s - loss: 5.1481e-04 - val_loss: 5.0476e-04 - 24s/epoch - 289us/sample
Epoch 45/90
84077/84077 - 24s - loss: 5.1240e-04 - val_loss: 5.1516e-04 - 24s/epoch - 289us/sample
Epoch 46/90
84077/84077 - 24s - loss: 5.1290e-04 - val_loss: 5.0622e-04 - 24s/epoch - 290us/sample
Epoch 47/90
84077/84077 - 24s - loss: 5.1163e-04 - val_loss: 5.1863e-04 - 24s/epoch - 289us/sample
Epoch 48/90
84077/84077 - 24s - loss: 5.1048e-04 - val_loss: 5.1335e-04 - 24s/epoch - 289us/sample
Epoch 49/90
84077/84077 - 24s - loss: 5.1027e-04 - val_loss: 5.0135e-04 - 24s/epoch - 288us/sample
Epoch 50/90
84077/84077 - 24s - loss: 5.1007e-04 - val_loss: 5.0244e-04 - 24s/epoch - 290us/sample
Epoch 51/90
84077/84077 - 24s - loss: 5.0867e-04 - val_loss: 5.0087e-04 - 24s/epoch - 288us/sample
Epoch 52/90
84077/84077 - 24s - loss: 5.0744e-04 - val_loss: 4.9691e-04 - 24s/epoch - 288us/sample
Epoch 53/90
84077/84077 - 24s - loss: 5.0782e-04 - val_loss: 4.9808e-04 - 24s/epoch - 290us/sample
Epoch 54/90
84077/84077 - 24s - loss: 5.0809e-04 - val_loss: 4.9166e-04 - 24s/epoch - 289us/sample
Epoch 55/90
84077/84077 - 24s - loss: 5.0661e-04 - val_loss: 4.8863e-04 - 24s/epoch - 288us/sample
Epoch 56/90
84077/84077 - 24s - loss: 5.0657e-04 - val_loss: 4.9283e-04 - 24s/epoch - 288us/sample
Epoch 57/90
84077/84077 - 24s - loss: 5.0534e-04 - val_loss: 4.8571e-04 - 24s/epoch - 290us/sample
Epoch 58/90
84077/84077 - 24s - loss: 5.0479e-04 - val_loss: 4.8176e-04 - 24s/epoch - 288us/sample
Epoch 59/90
84077/84077 - 24s - loss: 5.0444e-04 - val_loss: 4.9370e-04 - 24s/epoch - 288us/sample
Epoch 60/90
84077/84077 - 24s - loss: 5.0429e-04 - val_loss: 4.8639e-04 - 24s/epoch - 291us/sample
Epoch 61/90
84077/84077 - 24s - loss: 5.0350e-04 - val_loss: 4.8552e-04 - 24s/epoch - 289us/sample
Epoch 62/90
84077/84077 - 24s - loss: 5.0344e-04 - val_loss: 4.8072e-04 - 24s/epoch - 288us/sample
Epoch 63/90
84077/84077 - 24s - loss: 5.0255e-04 - val_loss: 4.8659e-04 - 24s/epoch - 289us/sample
Epoch 64/90
84077/84077 - 24s - loss: 5.0250e-04 - val_loss: 4.8130e-04 - 24s/epoch - 289us/sample
Epoch 65/90
84077/84077 - 24s - loss: 5.0256e-04 - val_loss: 4.8682e-04 - 24s/epoch - 288us/sample
Epoch 66/90
84077/84077 - 24s - loss: 5.0140e-04 - val_loss: 4.8190e-04 - 24s/epoch - 289us/sample
Epoch 67/90
84077/84077 - 24s - loss: 5.0130e-04 - val_loss: 4.8285e-04 - 24s/epoch - 290us/sample
Epoch 68/90
84077/84077 - 24s - loss: 5.0135e-04 - val_loss: 4.8685e-04 - 24s/epoch - 288us/sample
Epoch 69/90
84077/84077 - 24s - loss: 5.0137e-04 - val_loss: 4.8026e-04 - 24s/epoch - 289us/sample
Epoch 70/90
84077/84077 - 24s - loss: 4.9958e-04 - val_loss: 4.7842e-04 - 24s/epoch - 289us/sample
Epoch 71/90
84077/84077 - 24s - loss: 4.9989e-04 - val_loss: 4.7722e-04 - 24s/epoch - 288us/sample
Epoch 72/90
84077/84077 - 24s - loss: 4.9955e-04 - val_loss: 4.7793e-04 - 24s/epoch - 288us/sample
Epoch 73/90
84077/84077 - 24s - loss: 5.0008e-04 - val_loss: 4.8013e-04 - 24s/epoch - 290us/sample
Epoch 74/90
84077/84077 - 24s - loss: 4.9980e-04 - val_loss: 4.7315e-04 - 24s/epoch - 288us/sample
Epoch 75/90
84077/84077 - 24s - loss: 4.9863e-04 - val_loss: 4.7595e-04 - 24s/epoch - 287us/sample
Epoch 76/90
84077/84077 - 24s - loss: 4.9757e-04 - val_loss: 4.7589e-04 - 24s/epoch - 289us/sample
Epoch 77/90
84077/84077 - 24s - loss: 4.9697e-04 - val_loss: 4.7488e-04 - 24s/epoch - 288us/sample
Epoch 78/90
84077/84077 - 24s - loss: 4.9637e-04 - val_loss: 4.7425e-04 - 24s/epoch - 287us/sample
Epoch 79/90
84077/84077 - 24s - loss: 4.9635e-04 - val_loss: 4.7557e-04 - 24s/epoch - 288us/sample
Epoch 80/90
84077/84077 - 24s - loss: 4.9682e-04 - val_loss: 4.7244e-04 - 24s/epoch - 289us/sample
Epoch 81/90
84077/84077 - 24s - loss: 4.9508e-04 - val_loss: 4.7112e-04 - 24s/epoch - 288us/sample
Epoch 82/90
84077/84077 - 24s - loss: 4.9507e-04 - val_loss: 4.7082e-04 - 24s/epoch - 287us/sample
Epoch 83/90
84077/84077 - 24s - loss: 4.9474e-04 - val_loss: 4.7519e-04 - 24s/epoch - 289us/sample
Epoch 84/90
84077/84077 - 24s - loss: 4.9593e-04 - val_loss: 4.7593e-04 - 24s/epoch - 287us/sample
Epoch 85/90
84077/84077 - 24s - loss: 4.9401e-04 - val_loss: 4.7629e-04 - 24s/epoch - 287us/sample
Epoch 86/90
84077/84077 - 24s - loss: 4.9306e-04 - val_loss: 4.7174e-04 - 24s/epoch - 288us/sample
Epoch 87/90
84077/84077 - 24s - loss: 4.9301e-04 - val_loss: 4.6809e-04 - 24s/epoch - 289us/sample
Epoch 88/90
84077/84077 - 24s - loss: 4.9310e-04 - val_loss: 4.7296e-04 - 24s/epoch - 287us/sample
Epoch 89/90
84077/84077 - 24s - loss: 4.9247e-04 - val_loss: 4.7612e-04 - 24s/epoch - 287us/sample
Epoch 90/90
84077/84077 - 24s - loss: 4.9275e-04 - val_loss: 4.7361e-04 - 24s/epoch - 288us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0004736116316909813
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 21:43:54.738492: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_54/outputlayer/BiasAdd' id:68798 op device:{requested: '', assigned: ''} def:{{{node decoder_model_54/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_54/outputlayer/MatMul, decoder_model_54/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10535753540794872
cosine 0.10399767053556502
MAE: 0.00390717560428143
RMSE: 0.019751218374450808
r2: 0.6953322516732916
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 16, 90, 0.0008, 0.2, 188, 0.0004927480767745919, 0.0004736116316909813, 0.10535753540794872, 0.10399767053556502, 0.00390717560428143, 0.019751218374450808, 0.6953322516732916, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_165 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_165 (ReLU)               (None, 1980)         0           ['batch_normalization_165[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_165[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_165[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 21:44:17.246976: W tensorflow/c/c_api.cc:291] Operation '{name:'training_110/Adam/batch_normalization_165/beta/v/Assign' id:70700 op device:{requested: '', assigned: ''} def:{{{node training_110/Adam/batch_normalization_165/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_110/Adam/batch_normalization_165/beta/v, training_110/Adam/batch_normalization_165/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 21:44:30.932839: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_55/mul' id:70089 op device:{requested: '', assigned: ''} def:{{{node loss_55/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_55/mul/x, loss_55/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0036 - val_loss: 8.7805e-04 - 22s/epoch - 266us/sample
Epoch 2/90
84077/84077 - 9s - loss: 9.8118e-04 - val_loss: 9.9594e-04 - 9s/epoch - 105us/sample
Epoch 3/90
84077/84077 - 9s - loss: 9.6308e-04 - val_loss: 0.0057 - 9s/epoch - 103us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 6.4071e-04 - 9s/epoch - 103us/sample
Epoch 5/90
84077/84077 - 9s - loss: 6.0043e-04 - val_loss: 4.7803e-04 - 9s/epoch - 103us/sample
Epoch 6/90
84077/84077 - 9s - loss: 5.1546e-04 - val_loss: 4.7436e-04 - 9s/epoch - 103us/sample
Epoch 7/90
84077/84077 - 9s - loss: 4.4197e-04 - val_loss: 4.2910e-04 - 9s/epoch - 103us/sample
Epoch 8/90
84077/84077 - 9s - loss: 3.6259e-04 - val_loss: 3.0457e-04 - 9s/epoch - 103us/sample
Epoch 9/90
84077/84077 - 9s - loss: 3.2409e-04 - val_loss: 2.7319e-04 - 9s/epoch - 103us/sample
Epoch 10/90
84077/84077 - 9s - loss: 3.0013e-04 - val_loss: 2.6013e-04 - 9s/epoch - 103us/sample
Epoch 11/90
84077/84077 - 9s - loss: 2.7370e-04 - val_loss: 2.2832e-04 - 9s/epoch - 103us/sample
Epoch 12/90
84077/84077 - 9s - loss: 2.5584e-04 - val_loss: 2.2602e-04 - 9s/epoch - 104us/sample
Epoch 13/90
84077/84077 - 9s - loss: 2.4220e-04 - val_loss: 2.0669e-04 - 9s/epoch - 103us/sample
Epoch 14/90
84077/84077 - 9s - loss: 2.2873e-04 - val_loss: 1.9661e-04 - 9s/epoch - 103us/sample
Epoch 15/90
84077/84077 - 9s - loss: 2.1973e-04 - val_loss: 1.8737e-04 - 9s/epoch - 103us/sample
Epoch 16/90
84077/84077 - 9s - loss: 2.1005e-04 - val_loss: 1.8193e-04 - 9s/epoch - 103us/sample
Epoch 17/90
84077/84077 - 9s - loss: 2.0282e-04 - val_loss: 1.7621e-04 - 9s/epoch - 103us/sample
Epoch 18/90
84077/84077 - 9s - loss: 1.9908e-04 - val_loss: 1.7006e-04 - 9s/epoch - 103us/sample
Epoch 19/90
84077/84077 - 9s - loss: 1.9267e-04 - val_loss: 1.6768e-04 - 9s/epoch - 103us/sample
Epoch 20/90
84077/84077 - 9s - loss: 1.8783e-04 - val_loss: 1.6303e-04 - 9s/epoch - 105us/sample
Epoch 21/90
84077/84077 - 9s - loss: 1.8443e-04 - val_loss: 1.6108e-04 - 9s/epoch - 104us/sample
Epoch 22/90
84077/84077 - 9s - loss: 1.8066e-04 - val_loss: 1.5843e-04 - 9s/epoch - 103us/sample
Epoch 23/90
84077/84077 - 9s - loss: 1.7720e-04 - val_loss: 1.5657e-04 - 9s/epoch - 104us/sample
Epoch 24/90
84077/84077 - 9s - loss: 1.7545e-04 - val_loss: 1.5491e-04 - 9s/epoch - 103us/sample
Epoch 25/90
84077/84077 - 9s - loss: 1.7386e-04 - val_loss: 1.5281e-04 - 9s/epoch - 103us/sample
Epoch 26/90
84077/84077 - 9s - loss: 1.7116e-04 - val_loss: 1.5095e-04 - 9s/epoch - 103us/sample
Epoch 27/90
84077/84077 - 9s - loss: 1.6880e-04 - val_loss: 1.4941e-04 - 9s/epoch - 103us/sample
Epoch 28/90
84077/84077 - 9s - loss: 1.6745e-04 - val_loss: 1.4889e-04 - 9s/epoch - 104us/sample
Epoch 29/90
84077/84077 - 9s - loss: 1.6560e-04 - val_loss: 1.4608e-04 - 9s/epoch - 104us/sample
Epoch 30/90
84077/84077 - 9s - loss: 1.6430e-04 - val_loss: 1.4567e-04 - 9s/epoch - 103us/sample
Epoch 31/90
84077/84077 - 9s - loss: 1.6329e-04 - val_loss: 1.4503e-04 - 9s/epoch - 103us/sample
Epoch 32/90
84077/84077 - 9s - loss: 1.6108e-04 - val_loss: 1.4463e-04 - 9s/epoch - 103us/sample
Epoch 33/90
84077/84077 - 9s - loss: 1.6026e-04 - val_loss: 1.4643e-04 - 9s/epoch - 103us/sample
Epoch 34/90
84077/84077 - 9s - loss: 1.5926e-04 - val_loss: 1.4319e-04 - 9s/epoch - 103us/sample
Epoch 35/90
84077/84077 - 9s - loss: 1.5851e-04 - val_loss: 1.4173e-04 - 9s/epoch - 103us/sample
Epoch 36/90
84077/84077 - 9s - loss: 1.5768e-04 - val_loss: 1.4148e-04 - 9s/epoch - 104us/sample
Epoch 37/90
84077/84077 - 9s - loss: 1.5633e-04 - val_loss: 1.4043e-04 - 9s/epoch - 104us/sample
Epoch 38/90
84077/84077 - 9s - loss: 1.5578e-04 - val_loss: 1.3983e-04 - 9s/epoch - 103us/sample
Epoch 39/90
84077/84077 - 9s - loss: 1.5464e-04 - val_loss: 1.4011e-04 - 9s/epoch - 103us/sample
Epoch 40/90
84077/84077 - 9s - loss: 1.5418e-04 - val_loss: 1.3994e-04 - 9s/epoch - 103us/sample
Epoch 41/90
84077/84077 - 9s - loss: 1.5351e-04 - val_loss: 1.3813e-04 - 9s/epoch - 103us/sample
Epoch 42/90
84077/84077 - 9s - loss: 1.5304e-04 - val_loss: 1.3841e-04 - 9s/epoch - 103us/sample
Epoch 43/90
84077/84077 - 9s - loss: 1.5220e-04 - val_loss: 1.3733e-04 - 9s/epoch - 103us/sample
Epoch 44/90
84077/84077 - 9s - loss: 1.5147e-04 - val_loss: 1.3673e-04 - 9s/epoch - 104us/sample
Epoch 45/90
84077/84077 - 9s - loss: 1.5080e-04 - val_loss: 1.3698e-04 - 9s/epoch - 104us/sample
Epoch 46/90
84077/84077 - 9s - loss: 1.5036e-04 - val_loss: 1.3656e-04 - 9s/epoch - 103us/sample
Epoch 47/90
84077/84077 - 9s - loss: 1.4940e-04 - val_loss: 1.3676e-04 - 9s/epoch - 103us/sample
Epoch 48/90
84077/84077 - 9s - loss: 1.4909e-04 - val_loss: 1.3526e-04 - 9s/epoch - 103us/sample
Epoch 49/90
84077/84077 - 9s - loss: 1.4866e-04 - val_loss: 1.3540e-04 - 9s/epoch - 103us/sample
Epoch 50/90
84077/84077 - 9s - loss: 1.4777e-04 - val_loss: 1.3520e-04 - 9s/epoch - 103us/sample
Epoch 51/90
84077/84077 - 9s - loss: 1.4761e-04 - val_loss: 1.3437e-04 - 9s/epoch - 103us/sample
Epoch 52/90
84077/84077 - 9s - loss: 1.4683e-04 - val_loss: 1.3369e-04 - 9s/epoch - 104us/sample
Epoch 53/90
84077/84077 - 9s - loss: 1.4669e-04 - val_loss: 1.3318e-04 - 9s/epoch - 104us/sample
Epoch 54/90
84077/84077 - 9s - loss: 1.4633e-04 - val_loss: 1.3346e-04 - 9s/epoch - 103us/sample
Epoch 55/90
84077/84077 - 9s - loss: 1.4580e-04 - val_loss: 1.3390e-04 - 9s/epoch - 103us/sample
Epoch 56/90
84077/84077 - 9s - loss: 1.4580e-04 - val_loss: 1.3240e-04 - 9s/epoch - 103us/sample
Epoch 57/90
84077/84077 - 9s - loss: 1.4495e-04 - val_loss: 1.3126e-04 - 9s/epoch - 103us/sample
Epoch 58/90
84077/84077 - 9s - loss: 1.4502e-04 - val_loss: 1.3282e-04 - 9s/epoch - 103us/sample
Epoch 59/90
84077/84077 - 9s - loss: 1.4380e-04 - val_loss: 1.3205e-04 - 9s/epoch - 103us/sample
Epoch 60/90
84077/84077 - 9s - loss: 1.4422e-04 - val_loss: 1.3325e-04 - 9s/epoch - 103us/sample
Epoch 61/90
84077/84077 - 9s - loss: 1.4353e-04 - val_loss: 1.3105e-04 - 9s/epoch - 105us/sample
Epoch 62/90
84077/84077 - 9s - loss: 1.4360e-04 - val_loss: 1.3278e-04 - 9s/epoch - 103us/sample
Epoch 63/90
84077/84077 - 9s - loss: 1.4292e-04 - val_loss: 1.3132e-04 - 9s/epoch - 103us/sample
Epoch 64/90
84077/84077 - 9s - loss: 1.4312e-04 - val_loss: 1.3054e-04 - 9s/epoch - 103us/sample
Epoch 65/90
84077/84077 - 9s - loss: 1.4206e-04 - val_loss: 1.3194e-04 - 9s/epoch - 103us/sample
Epoch 66/90
84077/84077 - 9s - loss: 1.4199e-04 - val_loss: 1.3037e-04 - 9s/epoch - 103us/sample
Epoch 67/90
84077/84077 - 9s - loss: 1.4154e-04 - val_loss: 1.3029e-04 - 9s/epoch - 103us/sample
Epoch 68/90
84077/84077 - 9s - loss: 1.4182e-04 - val_loss: 1.3148e-04 - 9s/epoch - 103us/sample
Epoch 69/90
84077/84077 - 9s - loss: 1.4139e-04 - val_loss: 1.3006e-04 - 9s/epoch - 104us/sample
Epoch 70/90
84077/84077 - 9s - loss: 1.4057e-04 - val_loss: 1.3176e-04 - 9s/epoch - 103us/sample
Epoch 71/90
84077/84077 - 9s - loss: 1.4093e-04 - val_loss: 1.2996e-04 - 9s/epoch - 103us/sample
Epoch 72/90
84077/84077 - 9s - loss: 1.4044e-04 - val_loss: 1.3069e-04 - 9s/epoch - 103us/sample
Epoch 73/90
84077/84077 - 9s - loss: 1.4017e-04 - val_loss: 1.2971e-04 - 9s/epoch - 103us/sample
Epoch 74/90
84077/84077 - 9s - loss: 1.4011e-04 - val_loss: 1.2993e-04 - 9s/epoch - 103us/sample
Epoch 75/90
84077/84077 - 9s - loss: 1.4007e-04 - val_loss: 1.2975e-04 - 9s/epoch - 103us/sample
Epoch 76/90
84077/84077 - 9s - loss: 1.3981e-04 - val_loss: 1.2847e-04 - 9s/epoch - 104us/sample
Epoch 77/90
84077/84077 - 9s - loss: 1.3932e-04 - val_loss: 1.2877e-04 - 9s/epoch - 105us/sample
Epoch 78/90
84077/84077 - 9s - loss: 1.3928e-04 - val_loss: 1.2943e-04 - 9s/epoch - 104us/sample
Epoch 79/90
84077/84077 - 9s - loss: 1.3880e-04 - val_loss: 1.2816e-04 - 9s/epoch - 103us/sample
Epoch 80/90
84077/84077 - 9s - loss: 1.3852e-04 - val_loss: 1.2934e-04 - 9s/epoch - 103us/sample
Epoch 81/90
84077/84077 - 9s - loss: 1.3816e-04 - val_loss: 1.2908e-04 - 9s/epoch - 103us/sample
Epoch 82/90
84077/84077 - 9s - loss: 1.3822e-04 - val_loss: 1.3039e-04 - 9s/epoch - 103us/sample
Epoch 83/90
84077/84077 - 9s - loss: 1.3804e-04 - val_loss: 1.2820e-04 - 9s/epoch - 103us/sample
Epoch 84/90
84077/84077 - 9s - loss: 1.3764e-04 - val_loss: 1.2952e-04 - 9s/epoch - 103us/sample
Epoch 85/90
84077/84077 - 9s - loss: 1.3804e-04 - val_loss: 1.2853e-04 - 9s/epoch - 105us/sample
Epoch 86/90
84077/84077 - 9s - loss: 1.3768e-04 - val_loss: 1.2835e-04 - 9s/epoch - 104us/sample
Epoch 87/90
84077/84077 - 9s - loss: 1.3725e-04 - val_loss: 1.2792e-04 - 9s/epoch - 103us/sample
Epoch 88/90
84077/84077 - 9s - loss: 1.3730e-04 - val_loss: 1.2683e-04 - 9s/epoch - 103us/sample
Epoch 89/90
84077/84077 - 9s - loss: 1.3674e-04 - val_loss: 1.2756e-04 - 9s/epoch - 103us/sample
Epoch 90/90
84077/84077 - 9s - loss: 1.3741e-04 - val_loss: 1.2709e-04 - 9s/epoch - 103us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001270868105908344
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 21:57:29.245039: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_55/outputlayer/BiasAdd' id:70053 op device:{requested: '', assigned: ''} def:{{{node decoder_model_55/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_55/outputlayer/MatMul, decoder_model_55/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03159808439991372
cosine 0.03121823152749205
MAE: 0.0026474939994289358
RMSE: 0.009957586115252529
r2: 0.9230025189069374
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 90, 0.001, 0.2, 188, 0.0001374052489339618, 0.0001270868105908344, 0.03159808439991372, 0.03121823152749205, 0.0026474939994289358, 0.009957586115252529, 0.9230025189069374, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0014000000000000002 16 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_168 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_168 (ReLU)               (None, 1791)         0           ['batch_normalization_168[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_168[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_168[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 21:57:51.880953: W tensorflow/c/c_api.cc:291] Operation '{name:'training_112/Adam/batch_normalization_169/gamma/m/Assign' id:71882 op device:{requested: '', assigned: ''} def:{{{node training_112/Adam/batch_normalization_169/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_112/Adam/batch_normalization_169/gamma/m, training_112/Adam/batch_normalization_169/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 21:58:20.152889: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_56/mul' id:71367 op device:{requested: '', assigned: ''} def:{{{node loss_56/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_56/mul/x, loss_56/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 38s - loss: 0.0052 - val_loss: 0.0012 - 38s/epoch - 452us/sample
Epoch 2/85
84077/84077 - 24s - loss: 0.0012 - val_loss: 0.0011 - 24s/epoch - 284us/sample
Epoch 3/85
84077/84077 - 24s - loss: 8.6989e-04 - val_loss: 7.2789e-04 - 24s/epoch - 285us/sample
Epoch 4/85
84077/84077 - 24s - loss: 6.7789e-04 - val_loss: 6.8249e-04 - 24s/epoch - 286us/sample
Epoch 5/85
84077/84077 - 24s - loss: 5.8796e-04 - val_loss: 7.1025e-04 - 24s/epoch - 284us/sample
Epoch 6/85
84077/84077 - 24s - loss: 5.3790e-04 - val_loss: 6.6243e-04 - 24s/epoch - 284us/sample
Epoch 7/85
84077/84077 - 24s - loss: 5.0591e-04 - val_loss: 6.5110e-04 - 24s/epoch - 286us/sample
Epoch 8/85
84077/84077 - 24s - loss: 4.8234e-04 - val_loss: 6.9496e-04 - 24s/epoch - 284us/sample
Epoch 9/85
84077/84077 - 24s - loss: 4.6522e-04 - val_loss: 6.6009e-04 - 24s/epoch - 284us/sample
Epoch 10/85
84077/84077 - 24s - loss: 4.5005e-04 - val_loss: 6.7658e-04 - 24s/epoch - 286us/sample
Epoch 11/85
84077/84077 - 24s - loss: 4.3827e-04 - val_loss: 6.6895e-04 - 24s/epoch - 284us/sample
Epoch 12/85
84077/84077 - 24s - loss: 4.2109e-04 - val_loss: 6.1531e-04 - 24s/epoch - 284us/sample
Epoch 13/85
84077/84077 - 24s - loss: 4.0986e-04 - val_loss: 6.2336e-04 - 24s/epoch - 286us/sample
Epoch 14/85
84077/84077 - 24s - loss: 3.9473e-04 - val_loss: 5.8307e-04 - 24s/epoch - 284us/sample
Epoch 15/85
84077/84077 - 24s - loss: 3.8716e-04 - val_loss: 5.6795e-04 - 24s/epoch - 284us/sample
Epoch 16/85
84077/84077 - 24s - loss: 3.7756e-04 - val_loss: 5.1039e-04 - 24s/epoch - 286us/sample
Epoch 17/85
84077/84077 - 24s - loss: 3.7209e-04 - val_loss: 4.9795e-04 - 24s/epoch - 285us/sample
Epoch 18/85
84077/84077 - 24s - loss: 3.6375e-04 - val_loss: 4.5965e-04 - 24s/epoch - 284us/sample
Epoch 19/85
84077/84077 - 24s - loss: 3.5751e-04 - val_loss: 4.4830e-04 - 24s/epoch - 284us/sample
Epoch 20/85
84077/84077 - 24s - loss: 3.5409e-04 - val_loss: 4.0960e-04 - 24s/epoch - 286us/sample
Epoch 21/85
84077/84077 - 24s - loss: 3.4632e-04 - val_loss: 4.1300e-04 - 24s/epoch - 285us/sample
Epoch 22/85
84077/84077 - 24s - loss: 3.4402e-04 - val_loss: 3.9326e-04 - 24s/epoch - 284us/sample
Epoch 23/85
84077/84077 - 24s - loss: 3.3840e-04 - val_loss: 3.6913e-04 - 24s/epoch - 286us/sample
Epoch 24/85
84077/84077 - 24s - loss: 3.3235e-04 - val_loss: 3.5222e-04 - 24s/epoch - 284us/sample
Epoch 25/85
84077/84077 - 24s - loss: 3.3180e-04 - val_loss: 3.5134e-04 - 24s/epoch - 285us/sample
Epoch 26/85
84077/84077 - 24s - loss: 3.2975e-04 - val_loss: 3.4800e-04 - 24s/epoch - 285us/sample
Epoch 27/85
84077/84077 - 24s - loss: 3.2565e-04 - val_loss: 3.3986e-04 - 24s/epoch - 284us/sample
Epoch 28/85
84077/84077 - 24s - loss: 3.2200e-04 - val_loss: 3.7372e-04 - 24s/epoch - 284us/sample
Epoch 29/85
84077/84077 - 24s - loss: 3.1734e-04 - val_loss: 3.3153e-04 - 24s/epoch - 285us/sample
Epoch 30/85
84077/84077 - 24s - loss: 3.1722e-04 - val_loss: 3.2406e-04 - 24s/epoch - 286us/sample
Epoch 31/85
84077/84077 - 24s - loss: 3.1666e-04 - val_loss: 3.2685e-04 - 24s/epoch - 284us/sample
Epoch 32/85
84077/84077 - 24s - loss: 3.1752e-04 - val_loss: 3.2825e-04 - 24s/epoch - 285us/sample
Epoch 33/85
84077/84077 - 24s - loss: 3.0951e-04 - val_loss: 3.0621e-04 - 24s/epoch - 287us/sample
Epoch 34/85
84077/84077 - 24s - loss: 3.0972e-04 - val_loss: 3.1764e-04 - 24s/epoch - 285us/sample
Epoch 35/85
84077/84077 - 24s - loss: 3.0656e-04 - val_loss: 2.9889e-04 - 24s/epoch - 285us/sample
Epoch 36/85
84077/84077 - 24s - loss: 3.0584e-04 - val_loss: 3.0744e-04 - 24s/epoch - 287us/sample
Epoch 37/85
84077/84077 - 24s - loss: 3.0557e-04 - val_loss: 3.0603e-04 - 24s/epoch - 284us/sample
Epoch 38/85
84077/84077 - 24s - loss: 3.0198e-04 - val_loss: 2.9946e-04 - 24s/epoch - 284us/sample
Epoch 39/85
84077/84077 - 24s - loss: 3.0184e-04 - val_loss: 3.0635e-04 - 24s/epoch - 286us/sample
Epoch 40/85
84077/84077 - 24s - loss: 2.9792e-04 - val_loss: 2.9156e-04 - 24s/epoch - 285us/sample
Epoch 41/85
84077/84077 - 24s - loss: 2.9889e-04 - val_loss: 3.0437e-04 - 24s/epoch - 285us/sample
Epoch 42/85
84077/84077 - 24s - loss: 2.9749e-04 - val_loss: 2.9890e-04 - 24s/epoch - 285us/sample
Epoch 43/85
84077/84077 - 24s - loss: 2.9653e-04 - val_loss: 2.9699e-04 - 24s/epoch - 286us/sample
Epoch 44/85
84077/84077 - 24s - loss: 2.9319e-04 - val_loss: 2.9594e-04 - 24s/epoch - 284us/sample
Epoch 45/85
84077/84077 - 24s - loss: 2.9313e-04 - val_loss: 2.9668e-04 - 24s/epoch - 284us/sample
Epoch 46/85
84077/84077 - 24s - loss: 2.9191e-04 - val_loss: 2.8559e-04 - 24s/epoch - 286us/sample
Epoch 47/85
84077/84077 - 24s - loss: 2.9121e-04 - val_loss: 2.9564e-04 - 24s/epoch - 285us/sample
Epoch 48/85
84077/84077 - 24s - loss: 2.8899e-04 - val_loss: 2.8528e-04 - 24s/epoch - 284us/sample
Epoch 49/85
84077/84077 - 24s - loss: 2.9307e-04 - val_loss: 3.0581e-04 - 24s/epoch - 286us/sample
Epoch 50/85
84077/84077 - 24s - loss: 2.9264e-04 - val_loss: 2.9342e-04 - 24s/epoch - 284us/sample
Epoch 51/85
84077/84077 - 24s - loss: 2.8684e-04 - val_loss: 2.8732e-04 - 24s/epoch - 284us/sample
Epoch 52/85
84077/84077 - 24s - loss: 2.8668e-04 - val_loss: 2.8610e-04 - 24s/epoch - 286us/sample
Epoch 53/85
84077/84077 - 24s - loss: 2.8853e-04 - val_loss: 2.9900e-04 - 24s/epoch - 284us/sample
Epoch 54/85
84077/84077 - 24s - loss: 2.8319e-04 - val_loss: 2.7333e-04 - 24s/epoch - 285us/sample
Epoch 55/85
84077/84077 - 24s - loss: 2.8566e-04 - val_loss: 2.8295e-04 - 24s/epoch - 284us/sample
Epoch 56/85
84077/84077 - 24s - loss: 2.8896e-04 - val_loss: 2.9273e-04 - 24s/epoch - 286us/sample
Epoch 57/85
84077/84077 - 24s - loss: 2.8337e-04 - val_loss: 2.8353e-04 - 24s/epoch - 284us/sample
Epoch 58/85
84077/84077 - 24s - loss: 2.8139e-04 - val_loss: 2.8333e-04 - 24s/epoch - 284us/sample
Epoch 59/85
84077/84077 - 24s - loss: 2.8081e-04 - val_loss: 2.7820e-04 - 24s/epoch - 286us/sample
Epoch 60/85
84077/84077 - 24s - loss: 2.8007e-04 - val_loss: 2.8172e-04 - 24s/epoch - 285us/sample
Epoch 61/85
84077/84077 - 24s - loss: 2.7949e-04 - val_loss: 2.7753e-04 - 24s/epoch - 285us/sample
Epoch 62/85
84077/84077 - 24s - loss: 2.7640e-04 - val_loss: 2.7525e-04 - 24s/epoch - 284us/sample
Epoch 63/85
84077/84077 - 24s - loss: 2.7604e-04 - val_loss: 2.7913e-04 - 24s/epoch - 286us/sample
Epoch 64/85
84077/84077 - 24s - loss: 2.7843e-04 - val_loss: 2.7178e-04 - 24s/epoch - 283us/sample
Epoch 65/85
84077/84077 - 24s - loss: 2.7581e-04 - val_loss: 2.7081e-04 - 24s/epoch - 284us/sample
Epoch 66/85
84077/84077 - 24s - loss: 2.7650e-04 - val_loss: 2.8058e-04 - 24s/epoch - 286us/sample
Epoch 67/85
84077/84077 - 24s - loss: 2.7350e-04 - val_loss: 2.6927e-04 - 24s/epoch - 284us/sample
Epoch 68/85
84077/84077 - 24s - loss: 2.7738e-04 - val_loss: 2.8390e-04 - 24s/epoch - 284us/sample
Epoch 69/85
84077/84077 - 24s - loss: 2.7456e-04 - val_loss: 2.6901e-04 - 24s/epoch - 284us/sample
Epoch 70/85
84077/84077 - 24s - loss: 2.7275e-04 - val_loss: 2.6477e-04 - 24s/epoch - 285us/sample
Epoch 71/85
84077/84077 - 24s - loss: 2.7834e-04 - val_loss: 2.6798e-04 - 24s/epoch - 284us/sample
Epoch 72/85
84077/84077 - 24s - loss: 2.7359e-04 - val_loss: 2.6392e-04 - 24s/epoch - 284us/sample
Epoch 73/85
84077/84077 - 24s - loss: 2.7125e-04 - val_loss: 2.7219e-04 - 24s/epoch - 286us/sample
Epoch 74/85
84077/84077 - 24s - loss: 2.6997e-04 - val_loss: 2.6869e-04 - 24s/epoch - 284us/sample
Epoch 75/85
84077/84077 - 24s - loss: 2.7049e-04 - val_loss: 2.6748e-04 - 24s/epoch - 284us/sample
Epoch 76/85
84077/84077 - 24s - loss: 2.7073e-04 - val_loss: 2.6014e-04 - 24s/epoch - 285us/sample
Epoch 77/85
84077/84077 - 24s - loss: 2.6980e-04 - val_loss: 2.7379e-04 - 24s/epoch - 284us/sample
Epoch 78/85
84077/84077 - 24s - loss: 2.7245e-04 - val_loss: 2.7923e-04 - 24s/epoch - 284us/sample
Epoch 79/85
84077/84077 - 24s - loss: 2.6785e-04 - val_loss: 2.6409e-04 - 24s/epoch - 283us/sample
Epoch 80/85
84077/84077 - 24s - loss: 2.7112e-04 - val_loss: 2.7219e-04 - 24s/epoch - 286us/sample
Epoch 81/85
84077/84077 - 24s - loss: 2.6707e-04 - val_loss: 2.6526e-04 - 24s/epoch - 284us/sample
Epoch 82/85
84077/84077 - 24s - loss: 2.6510e-04 - val_loss: 2.5734e-04 - 24s/epoch - 284us/sample
Epoch 83/85
84077/84077 - 24s - loss: 2.6687e-04 - val_loss: 2.7072e-04 - 24s/epoch - 285us/sample
Epoch 84/85
84077/84077 - 24s - loss: 2.6555e-04 - val_loss: 2.6323e-04 - 24s/epoch - 284us/sample
Epoch 85/85
84077/84077 - 24s - loss: 2.6623e-04 - val_loss: 2.5964e-04 - 24s/epoch - 284us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00025963710618918267
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 22:31:56.794990: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_56/outputlayer/BiasAdd' id:71338 op device:{requested: '', assigned: ''} def:{{{node decoder_model_56/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_56/outputlayer/MatMul, decoder_model_56/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03626976529000012
cosine 0.035833558238449914
MAE: 0.0025122497594239797
RMSE: 0.012173456391203596
r2: 0.8842804262222164
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 16, 85, 0.0014000000000000002, 0.2, 188, 0.0002662287540265215, 0.00025963710618918267, 0.03626976529000012, 0.035833558238449914, 0.0025122497594239797, 0.012173456391203596, 0.8842804262222164, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 90 0.0012000000000000001 32 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_171 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_171 (ReLU)               (None, 1603)         0           ['batch_normalization_171[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_171[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_171[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-11 22:32:19.912358: W tensorflow/c/c_api.cc:291] Operation '{name:'training_114/Adam/beta_1/Assign' id:73135 op device:{requested: '', assigned: ''} def:{{{node training_114/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_114/Adam/beta_1, training_114/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 22:32:39.019755: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_57/mul' id:72641 op device:{requested: '', assigned: ''} def:{{{node loss_57/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_57/mul/x, loss_57/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 28s - loss: 0.0781 - val_loss: 0.0685 - 28s/epoch - 337us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0676 - val_loss: 0.0676 - 14s/epoch - 168us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0670 - val_loss: 0.0674 - 14s/epoch - 168us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0677 - val_loss: 0.0675 - 14s/epoch - 169us/sample
Epoch 5/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 169us/sample
Epoch 6/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 169us/sample
Epoch 7/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0674 - 14s/epoch - 168us/sample
Epoch 8/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 9/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 10/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 11/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 12/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 13/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 14/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 15/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 16/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 170us/sample
Epoch 17/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 18/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 19/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 20/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 21/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 22/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 23/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 24/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 25/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 26/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 27/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 28/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 29/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 30/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 31/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 32/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 33/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 34/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 35/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 36/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 37/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 38/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 39/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 40/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 41/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 42/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 43/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 44/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 45/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 46/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 47/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 48/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 49/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 50/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 51/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 52/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 53/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 54/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 55/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 56/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 57/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 58/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 59/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 60/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 61/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 62/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 63/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 64/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 65/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 66/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 67/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 68/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 69/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 70/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 71/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 72/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 73/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 74/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 75/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 76/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 77/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 78/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 79/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 80/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 81/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 82/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 83/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 84/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 85/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 86/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 87/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 88/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 89/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 90/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734572936210712
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 22:53:45.770692: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_57/outputlayer/BiasAdd' id:72593 op device:{requested: '', assigned: ''} def:{{{node decoder_model_57/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_57/outputlayer/MatMul, decoder_model_57/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2106443426389604
cosine 1.1881312802690842
MAE: 5.106436160824747
RMSE: 6.064829879405339
r2: -27260.95880635632
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'binary_crossentropy', 32, 90, 0.0012000000000000001, 0.2, 188, 0.06683691113064297, 0.06734572936210712, 1.2106443426389604, 1.1881312802690842, 5.106436160824747, 6.064829879405339, -27260.95880635632, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 80 0.0008 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_174 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_174 (ReLU)               (None, 1791)         0           ['batch_normalization_174[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_174[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_174[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-11 22:54:09.377612: W tensorflow/c/c_api.cc:291] Operation '{name:'training_116/Adam/dense_dec0_58/kernel/m/Assign' id:74516 op device:{requested: '', assigned: ''} def:{{{node training_116/Adam/dense_dec0_58/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_116/Adam/dense_dec0_58/kernel/m, training_116/Adam/dense_dec0_58/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 22:54:23.433241: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_58/mul' id:73966 op device:{requested: '', assigned: ''} def:{{{node loss_58/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_58/mul/x, loss_58/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0041 - val_loss: 9.4873e-04 - 23s/epoch - 275us/sample
Epoch 2/80
84077/84077 - 9s - loss: 8.5699e-04 - val_loss: 7.7442e-04 - 9s/epoch - 104us/sample
Epoch 3/80
84077/84077 - 9s - loss: 9.7282e-04 - val_loss: 0.0013 - 9s/epoch - 104us/sample
Epoch 4/80
84077/84077 - 9s - loss: 7.0025e-04 - val_loss: 5.9968e-04 - 9s/epoch - 104us/sample
Epoch 5/80
84077/84077 - 9s - loss: 6.0074e-04 - val_loss: 4.2515e-04 - 9s/epoch - 104us/sample
Epoch 6/80
84077/84077 - 9s - loss: 4.2848e-04 - val_loss: 3.6656e-04 - 9s/epoch - 105us/sample
Epoch 7/80
84077/84077 - 9s - loss: 3.7561e-04 - val_loss: 3.3768e-04 - 9s/epoch - 104us/sample
Epoch 8/80
84077/84077 - 9s - loss: 3.4585e-04 - val_loss: 2.8847e-04 - 9s/epoch - 104us/sample
Epoch 9/80
84077/84077 - 9s - loss: 3.3153e-04 - val_loss: 2.9844e-04 - 9s/epoch - 104us/sample
Epoch 10/80
84077/84077 - 9s - loss: 2.9493e-04 - val_loss: 2.4798e-04 - 9s/epoch - 104us/sample
Epoch 11/80
84077/84077 - 9s - loss: 2.8004e-04 - val_loss: 2.4420e-04 - 9s/epoch - 104us/sample
Epoch 12/80
84077/84077 - 9s - loss: 2.6014e-04 - val_loss: 2.2967e-04 - 9s/epoch - 104us/sample
Epoch 13/80
84077/84077 - 9s - loss: 2.4472e-04 - val_loss: 2.1537e-04 - 9s/epoch - 104us/sample
Epoch 14/80
84077/84077 - 9s - loss: 2.3495e-04 - val_loss: 2.0814e-04 - 9s/epoch - 104us/sample
Epoch 15/80
84077/84077 - 9s - loss: 2.2681e-04 - val_loss: 1.9599e-04 - 9s/epoch - 105us/sample
Epoch 16/80
84077/84077 - 9s - loss: 2.1568e-04 - val_loss: 1.8867e-04 - 9s/epoch - 105us/sample
Epoch 17/80
84077/84077 - 9s - loss: 2.0711e-04 - val_loss: 1.8242e-04 - 9s/epoch - 104us/sample
Epoch 18/80
84077/84077 - 9s - loss: 2.0053e-04 - val_loss: 1.7622e-04 - 9s/epoch - 104us/sample
Epoch 19/80
84077/84077 - 9s - loss: 1.9543e-04 - val_loss: 1.7527e-04 - 9s/epoch - 104us/sample
Epoch 20/80
84077/84077 - 9s - loss: 1.8994e-04 - val_loss: 1.6984e-04 - 9s/epoch - 104us/sample
Epoch 21/80
84077/84077 - 9s - loss: 1.8739e-04 - val_loss: 1.6455e-04 - 9s/epoch - 104us/sample
Epoch 22/80
84077/84077 - 9s - loss: 1.8165e-04 - val_loss: 1.6431e-04 - 9s/epoch - 104us/sample
Epoch 23/80
84077/84077 - 9s - loss: 1.7747e-04 - val_loss: 1.5878e-04 - 9s/epoch - 106us/sample
Epoch 24/80
84077/84077 - 9s - loss: 1.7501e-04 - val_loss: 1.5734e-04 - 9s/epoch - 105us/sample
Epoch 25/80
84077/84077 - 9s - loss: 1.7301e-04 - val_loss: 1.5434e-04 - 9s/epoch - 104us/sample
Epoch 26/80
84077/84077 - 9s - loss: 1.7060e-04 - val_loss: 1.5419e-04 - 9s/epoch - 104us/sample
Epoch 27/80
84077/84077 - 9s - loss: 1.6863e-04 - val_loss: 1.5184e-04 - 9s/epoch - 104us/sample
Epoch 28/80
84077/84077 - 9s - loss: 1.6668e-04 - val_loss: 1.5059e-04 - 9s/epoch - 104us/sample
Epoch 29/80
84077/84077 - 9s - loss: 1.6487e-04 - val_loss: 1.5015e-04 - 9s/epoch - 104us/sample
Epoch 30/80
84077/84077 - 9s - loss: 1.6432e-04 - val_loss: 1.4923e-04 - 9s/epoch - 104us/sample
Epoch 31/80
84077/84077 - 9s - loss: 1.6192e-04 - val_loss: 1.4689e-04 - 9s/epoch - 105us/sample
Epoch 32/80
84077/84077 - 9s - loss: 1.6123e-04 - val_loss: 1.4777e-04 - 9s/epoch - 105us/sample
Epoch 33/80
84077/84077 - 9s - loss: 1.5927e-04 - val_loss: 1.4542e-04 - 9s/epoch - 104us/sample
Epoch 34/80
84077/84077 - 9s - loss: 1.5792e-04 - val_loss: 1.4439e-04 - 9s/epoch - 104us/sample
Epoch 35/80
84077/84077 - 9s - loss: 1.5700e-04 - val_loss: 1.4345e-04 - 9s/epoch - 104us/sample
Epoch 36/80
84077/84077 - 9s - loss: 1.5627e-04 - val_loss: 1.4285e-04 - 9s/epoch - 104us/sample
Epoch 37/80
84077/84077 - 9s - loss: 1.5541e-04 - val_loss: 1.4128e-04 - 9s/epoch - 104us/sample
Epoch 38/80
84077/84077 - 9s - loss: 1.5447e-04 - val_loss: 1.4201e-04 - 9s/epoch - 104us/sample
Epoch 39/80
84077/84077 - 9s - loss: 1.5374e-04 - val_loss: 1.4175e-04 - 9s/epoch - 105us/sample
Epoch 40/80
84077/84077 - 9s - loss: 1.5360e-04 - val_loss: 1.4119e-04 - 9s/epoch - 106us/sample
Epoch 41/80
84077/84077 - 9s - loss: 1.5265e-04 - val_loss: 1.4347e-04 - 9s/epoch - 104us/sample
Epoch 42/80
84077/84077 - 9s - loss: 1.5177e-04 - val_loss: 1.3833e-04 - 9s/epoch - 104us/sample
Epoch 43/80
84077/84077 - 9s - loss: 1.5057e-04 - val_loss: 1.3908e-04 - 9s/epoch - 104us/sample
Epoch 44/80
84077/84077 - 9s - loss: 1.5015e-04 - val_loss: 1.3918e-04 - 9s/epoch - 104us/sample
Epoch 45/80
84077/84077 - 9s - loss: 1.4924e-04 - val_loss: 1.3832e-04 - 9s/epoch - 104us/sample
Epoch 46/80
84077/84077 - 9s - loss: 1.4864e-04 - val_loss: 1.3777e-04 - 9s/epoch - 104us/sample
Epoch 47/80
84077/84077 - 9s - loss: 1.4837e-04 - val_loss: 1.3632e-04 - 9s/epoch - 105us/sample
Epoch 48/80
84077/84077 - 9s - loss: 1.4804e-04 - val_loss: 1.3745e-04 - 9s/epoch - 105us/sample
Epoch 49/80
84077/84077 - 9s - loss: 1.4775e-04 - val_loss: 1.3581e-04 - 9s/epoch - 104us/sample
Epoch 50/80
84077/84077 - 9s - loss: 1.4705e-04 - val_loss: 1.3464e-04 - 9s/epoch - 104us/sample
Epoch 51/80
84077/84077 - 9s - loss: 1.4704e-04 - val_loss: 1.3431e-04 - 9s/epoch - 104us/sample
Epoch 52/80
84077/84077 - 9s - loss: 1.4607e-04 - val_loss: 1.3549e-04 - 9s/epoch - 104us/sample
Epoch 53/80
84077/84077 - 9s - loss: 1.4508e-04 - val_loss: 1.3384e-04 - 9s/epoch - 104us/sample
Epoch 54/80
84077/84077 - 9s - loss: 1.4496e-04 - val_loss: 1.3520e-04 - 9s/epoch - 104us/sample
Epoch 55/80
84077/84077 - 9s - loss: 1.4453e-04 - val_loss: 1.3298e-04 - 9s/epoch - 105us/sample
Epoch 56/80
84077/84077 - 9s - loss: 1.4436e-04 - val_loss: 1.3383e-04 - 9s/epoch - 105us/sample
Epoch 57/80
84077/84077 - 9s - loss: 1.4404e-04 - val_loss: 1.3293e-04 - 9s/epoch - 104us/sample
Epoch 58/80
84077/84077 - 9s - loss: 1.4393e-04 - val_loss: 1.3315e-04 - 9s/epoch - 104us/sample
Epoch 59/80
84077/84077 - 9s - loss: 1.4280e-04 - val_loss: 1.3221e-04 - 9s/epoch - 104us/sample
Epoch 60/80
84077/84077 - 9s - loss: 1.4308e-04 - val_loss: 1.3309e-04 - 9s/epoch - 104us/sample
Epoch 61/80
84077/84077 - 9s - loss: 1.4213e-04 - val_loss: 1.3214e-04 - 9s/epoch - 104us/sample
Epoch 62/80
84077/84077 - 9s - loss: 1.4214e-04 - val_loss: 1.3135e-04 - 9s/epoch - 104us/sample
Epoch 63/80
84077/84077 - 9s - loss: 1.4192e-04 - val_loss: 1.3195e-04 - 9s/epoch - 105us/sample
Epoch 64/80
84077/84077 - 9s - loss: 1.4155e-04 - val_loss: 1.3254e-04 - 9s/epoch - 105us/sample
Epoch 65/80
84077/84077 - 9s - loss: 1.4126e-04 - val_loss: 1.3200e-04 - 9s/epoch - 104us/sample
Epoch 66/80
84077/84077 - 9s - loss: 1.4153e-04 - val_loss: 1.3074e-04 - 9s/epoch - 104us/sample
Epoch 67/80
84077/84077 - 9s - loss: 1.4098e-04 - val_loss: 1.3303e-04 - 9s/epoch - 104us/sample
Epoch 68/80
84077/84077 - 9s - loss: 1.4026e-04 - val_loss: 1.3097e-04 - 9s/epoch - 104us/sample
Epoch 69/80
84077/84077 - 9s - loss: 1.4028e-04 - val_loss: 1.3152e-04 - 9s/epoch - 104us/sample
Epoch 70/80
84077/84077 - 9s - loss: 1.3966e-04 - val_loss: 1.3105e-04 - 9s/epoch - 104us/sample
Epoch 71/80
84077/84077 - 9s - loss: 1.3977e-04 - val_loss: 1.3034e-04 - 9s/epoch - 104us/sample
Epoch 72/80
84077/84077 - 9s - loss: 1.3929e-04 - val_loss: 1.2896e-04 - 9s/epoch - 106us/sample
Epoch 73/80
84077/84077 - 9s - loss: 1.3905e-04 - val_loss: 1.2999e-04 - 9s/epoch - 104us/sample
Epoch 74/80
84077/84077 - 9s - loss: 1.3879e-04 - val_loss: 1.2995e-04 - 9s/epoch - 104us/sample
Epoch 75/80
84077/84077 - 9s - loss: 1.3843e-04 - val_loss: 1.2968e-04 - 9s/epoch - 104us/sample
Epoch 76/80
84077/84077 - 9s - loss: 1.3849e-04 - val_loss: 1.2940e-04 - 9s/epoch - 104us/sample
Epoch 77/80
84077/84077 - 9s - loss: 1.3808e-04 - val_loss: 1.2837e-04 - 9s/epoch - 104us/sample
Epoch 78/80
84077/84077 - 9s - loss: 1.3798e-04 - val_loss: 1.2964e-04 - 9s/epoch - 104us/sample
Epoch 79/80
84077/84077 - 9s - loss: 1.3773e-04 - val_loss: 1.3042e-04 - 9s/epoch - 104us/sample
Epoch 80/80
84077/84077 - 9s - loss: 1.3767e-04 - val_loss: 1.2846e-04 - 9s/epoch - 105us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012846274671491313
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 23:06:01.394650: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_58/outputlayer/BiasAdd' id:73930 op device:{requested: '', assigned: ''} def:{{{node decoder_model_58/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_58/outputlayer/MatMul, decoder_model_58/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031548830251361314
cosine 0.031170539504178855
MAE: 0.002554125010413617
RMSE: 0.010143143070377962
r2: 0.9198338776424297
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 80, 0.0008, 0.2, 188, 0.00013767203007144724, 0.00012846274671491313, 0.031548830251361314, 0.031170539504178855, 0.002554125010413617, 0.010143143070377962, 0.9198338776424297, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 7
Fitness    = 512.1418577388646
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 512.1418577388646.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646]
[1.5999999999999996 80 0.0008 64 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1508)         1423552     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_177 (Batch  (None, 1508)        6032        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_177 (ReLU)               (None, 1508)         0           ['batch_normalization_177[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          283692      ['re_lu_177[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          283692      ['re_lu_177[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1750315     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,747,283
Trainable params: 3,740,875
Non-trainable params: 6,408
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-11 23:06:25.236869: W tensorflow/c/c_api.cc:291] Operation '{name:'training_118/Adam/batch_normalization_177/beta/v/Assign' id:75862 op device:{requested: '', assigned: ''} def:{{{node training_118/Adam/batch_normalization_177/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_118/Adam/batch_normalization_177/beta/v, training_118/Adam/batch_normalization_177/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 23:06:39.198009: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_59/mul' id:75251 op device:{requested: '', assigned: ''} def:{{{node loss_59/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_59/mul/x, loss_59/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0289 - val_loss: 0.0010 - 23s/epoch - 277us/sample
Epoch 2/80
84077/84077 - 9s - loss: 9.8681e-04 - val_loss: 0.0037 - 9s/epoch - 104us/sample
Epoch 3/80
84077/84077 - 9s - loss: 8.1668e-04 - val_loss: 6.4037e-04 - 9s/epoch - 105us/sample
Epoch 4/80
84077/84077 - 9s - loss: 6.6851e-04 - val_loss: 6.6596e-04 - 9s/epoch - 104us/sample
Epoch 5/80
84077/84077 - 9s - loss: 6.2579e-04 - val_loss: 4.4572e-04 - 9s/epoch - 104us/sample
Epoch 6/80
84077/84077 - 9s - loss: 4.5516e-04 - val_loss: 3.8947e-04 - 9s/epoch - 104us/sample
Epoch 7/80
84077/84077 - 9s - loss: 3.7443e-04 - val_loss: 3.2461e-04 - 9s/epoch - 104us/sample
Epoch 8/80
84077/84077 - 9s - loss: 3.4093e-04 - val_loss: 2.9712e-04 - 9s/epoch - 104us/sample
Epoch 9/80
84077/84077 - 9s - loss: 3.1571e-04 - val_loss: 2.7634e-04 - 9s/epoch - 104us/sample
Epoch 10/80
84077/84077 - 9s - loss: 2.9425e-04 - val_loss: 2.6494e-04 - 9s/epoch - 104us/sample
Epoch 11/80
84077/84077 - 9s - loss: 2.7335e-04 - val_loss: 2.4161e-04 - 9s/epoch - 104us/sample
Epoch 12/80
84077/84077 - 9s - loss: 2.5635e-04 - val_loss: 2.2047e-04 - 9s/epoch - 105us/sample
Epoch 13/80
84077/84077 - 9s - loss: 2.4039e-04 - val_loss: 2.0453e-04 - 9s/epoch - 104us/sample
Epoch 14/80
84077/84077 - 9s - loss: 2.2576e-04 - val_loss: 1.9468e-04 - 9s/epoch - 104us/sample
Epoch 15/80
84077/84077 - 9s - loss: 2.1581e-04 - val_loss: 1.8843e-04 - 9s/epoch - 104us/sample
Epoch 16/80
84077/84077 - 9s - loss: 2.0714e-04 - val_loss: 1.8152e-04 - 9s/epoch - 104us/sample
Epoch 17/80
84077/84077 - 9s - loss: 1.9756e-04 - val_loss: 1.7319e-04 - 9s/epoch - 104us/sample
Epoch 18/80
84077/84077 - 9s - loss: 1.9135e-04 - val_loss: 1.6994e-04 - 9s/epoch - 104us/sample
Epoch 19/80
84077/84077 - 9s - loss: 1.8635e-04 - val_loss: 1.6456e-04 - 9s/epoch - 104us/sample
Epoch 20/80
84077/84077 - 9s - loss: 1.8150e-04 - val_loss: 1.6235e-04 - 9s/epoch - 105us/sample
Epoch 21/80
84077/84077 - 9s - loss: 1.7829e-04 - val_loss: 1.5774e-04 - 9s/epoch - 105us/sample
Epoch 22/80
84077/84077 - 9s - loss: 1.7427e-04 - val_loss: 1.5387e-04 - 9s/epoch - 104us/sample
Epoch 23/80
84077/84077 - 9s - loss: 1.7082e-04 - val_loss: 1.5364e-04 - 9s/epoch - 104us/sample
Epoch 24/80
84077/84077 - 9s - loss: 1.6850e-04 - val_loss: 1.5166e-04 - 9s/epoch - 104us/sample
Epoch 25/80
84077/84077 - 9s - loss: 1.6658e-04 - val_loss: 1.5205e-04 - 9s/epoch - 104us/sample
Epoch 26/80
84077/84077 - 9s - loss: 1.6446e-04 - val_loss: 1.4850e-04 - 9s/epoch - 104us/sample
Epoch 27/80
84077/84077 - 9s - loss: 1.6246e-04 - val_loss: 1.4733e-04 - 9s/epoch - 104us/sample
Epoch 28/80
84077/84077 - 9s - loss: 1.6093e-04 - val_loss: 1.4700e-04 - 9s/epoch - 104us/sample
Epoch 29/80
84077/84077 - 9s - loss: 1.5990e-04 - val_loss: 1.4523e-04 - 9s/epoch - 105us/sample
Epoch 30/80
84077/84077 - 9s - loss: 1.5847e-04 - val_loss: 1.4352e-04 - 9s/epoch - 104us/sample
Epoch 31/80
84077/84077 - 9s - loss: 1.5686e-04 - val_loss: 1.4374e-04 - 9s/epoch - 107us/sample
Epoch 32/80
84077/84077 - 9s - loss: 1.5713e-04 - val_loss: 1.4183e-04 - 9s/epoch - 104us/sample
Epoch 33/80
84077/84077 - 9s - loss: 1.5500e-04 - val_loss: 1.4116e-04 - 9s/epoch - 104us/sample
Epoch 34/80
84077/84077 - 9s - loss: 1.5417e-04 - val_loss: 1.4046e-04 - 9s/epoch - 104us/sample
Epoch 35/80
84077/84077 - 9s - loss: 1.5279e-04 - val_loss: 1.4016e-04 - 9s/epoch - 104us/sample
Epoch 36/80
84077/84077 - 9s - loss: 1.5209e-04 - val_loss: 1.3866e-04 - 9s/epoch - 104us/sample
Epoch 37/80
84077/84077 - 9s - loss: 1.5135e-04 - val_loss: 1.3953e-04 - 9s/epoch - 105us/sample
Epoch 38/80
84077/84077 - 9s - loss: 1.5047e-04 - val_loss: 1.3857e-04 - 9s/epoch - 104us/sample
Epoch 39/80
84077/84077 - 9s - loss: 1.5063e-04 - val_loss: 1.3813e-04 - 9s/epoch - 104us/sample
Epoch 40/80
84077/84077 - 9s - loss: 1.4923e-04 - val_loss: 1.3835e-04 - 9s/epoch - 104us/sample
Epoch 41/80
84077/84077 - 9s - loss: 1.4882e-04 - val_loss: 1.3731e-04 - 9s/epoch - 104us/sample
Epoch 42/80
84077/84077 - 9s - loss: 1.4827e-04 - val_loss: 1.3605e-04 - 9s/epoch - 104us/sample
Epoch 43/80
84077/84077 - 9s - loss: 1.4755e-04 - val_loss: 1.3564e-04 - 9s/epoch - 104us/sample
Epoch 44/80
84077/84077 - 9s - loss: 1.4720e-04 - val_loss: 1.3700e-04 - 9s/epoch - 104us/sample
Epoch 45/80
84077/84077 - 9s - loss: 1.4597e-04 - val_loss: 1.3470e-04 - 9s/epoch - 105us/sample
Epoch 46/80
84077/84077 - 9s - loss: 1.4571e-04 - val_loss: 1.3449e-04 - 9s/epoch - 104us/sample
Epoch 47/80
84077/84077 - 9s - loss: 1.4543e-04 - val_loss: 1.3514e-04 - 9s/epoch - 104us/sample
Epoch 48/80
84077/84077 - 9s - loss: 1.4494e-04 - val_loss: 1.3491e-04 - 9s/epoch - 104us/sample
Epoch 49/80
84077/84077 - 9s - loss: 1.4439e-04 - val_loss: 1.3348e-04 - 9s/epoch - 104us/sample
Epoch 50/80
84077/84077 - 9s - loss: 1.4392e-04 - val_loss: 1.3275e-04 - 9s/epoch - 104us/sample
Epoch 51/80
84077/84077 - 9s - loss: 1.4402e-04 - val_loss: 1.3303e-04 - 9s/epoch - 104us/sample
Epoch 52/80
84077/84077 - 9s - loss: 1.4312e-04 - val_loss: 1.3343e-04 - 9s/epoch - 104us/sample
Epoch 53/80
84077/84077 - 9s - loss: 1.4283e-04 - val_loss: 1.3275e-04 - 9s/epoch - 105us/sample
Epoch 54/80
84077/84077 - 9s - loss: 1.4225e-04 - val_loss: 1.3322e-04 - 9s/epoch - 104us/sample
Epoch 55/80
84077/84077 - 9s - loss: 1.4193e-04 - val_loss: 1.3290e-04 - 9s/epoch - 104us/sample
Epoch 56/80
84077/84077 - 9s - loss: 1.4147e-04 - val_loss: 1.3102e-04 - 9s/epoch - 104us/sample
Epoch 57/80
84077/84077 - 9s - loss: 1.4097e-04 - val_loss: 1.3181e-04 - 9s/epoch - 104us/sample
Epoch 58/80
84077/84077 - 9s - loss: 1.4081e-04 - val_loss: 1.3082e-04 - 9s/epoch - 104us/sample
Epoch 59/80
84077/84077 - 9s - loss: 1.4067e-04 - val_loss: 1.3216e-04 - 9s/epoch - 104us/sample
Epoch 60/80
84077/84077 - 9s - loss: 1.4017e-04 - val_loss: 1.3098e-04 - 9s/epoch - 104us/sample
Epoch 61/80
84077/84077 - 9s - loss: 1.3989e-04 - val_loss: 1.3181e-04 - 9s/epoch - 105us/sample
Epoch 62/80
84077/84077 - 9s - loss: 1.3932e-04 - val_loss: 1.3122e-04 - 9s/epoch - 105us/sample
Epoch 63/80
84077/84077 - 9s - loss: 1.3976e-04 - val_loss: 1.3055e-04 - 9s/epoch - 104us/sample
Epoch 64/80
84077/84077 - 9s - loss: 1.3870e-04 - val_loss: 1.3024e-04 - 9s/epoch - 104us/sample
Epoch 65/80
84077/84077 - 9s - loss: 1.3913e-04 - val_loss: 1.2885e-04 - 9s/epoch - 104us/sample
Epoch 66/80
84077/84077 - 9s - loss: 1.3827e-04 - val_loss: 1.2928e-04 - 9s/epoch - 104us/sample
Epoch 67/80
84077/84077 - 9s - loss: 1.3860e-04 - val_loss: 1.3032e-04 - 9s/epoch - 104us/sample
Epoch 68/80
84077/84077 - 9s - loss: 1.3802e-04 - val_loss: 1.2897e-04 - 9s/epoch - 104us/sample
Epoch 69/80
84077/84077 - 9s - loss: 1.3781e-04 - val_loss: 1.2777e-04 - 9s/epoch - 105us/sample
Epoch 70/80
84077/84077 - 9s - loss: 1.3766e-04 - val_loss: 1.2737e-04 - 9s/epoch - 105us/sample
Epoch 71/80
84077/84077 - 9s - loss: 1.3720e-04 - val_loss: 1.2925e-04 - 9s/epoch - 104us/sample
Epoch 72/80
84077/84077 - 9s - loss: 1.3736e-04 - val_loss: 1.2871e-04 - 9s/epoch - 104us/sample
Epoch 73/80
84077/84077 - 9s - loss: 1.3681e-04 - val_loss: 1.2821e-04 - 9s/epoch - 104us/sample
Epoch 74/80
84077/84077 - 9s - loss: 1.3656e-04 - val_loss: 1.2807e-04 - 9s/epoch - 104us/sample
Epoch 75/80
84077/84077 - 9s - loss: 1.3621e-04 - val_loss: 1.2773e-04 - 9s/epoch - 104us/sample
Epoch 76/80
84077/84077 - 9s - loss: 1.3660e-04 - val_loss: 1.2831e-04 - 9s/epoch - 104us/sample
Epoch 77/80
84077/84077 - 9s - loss: 1.3592e-04 - val_loss: 1.2765e-04 - 9s/epoch - 104us/sample
Epoch 78/80
84077/84077 - 9s - loss: 1.3553e-04 - val_loss: 1.2753e-04 - 9s/epoch - 105us/sample
Epoch 79/80
84077/84077 - 9s - loss: 1.3567e-04 - val_loss: 1.2657e-04 - 9s/epoch - 105us/sample
Epoch 80/80
84077/84077 - 9s - loss: 1.3502e-04 - val_loss: 1.2852e-04 - 9s/epoch - 104us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012852292172560415
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 23:18:16.639350: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_59/outputlayer/BiasAdd' id:75215 op device:{requested: '', assigned: ''} def:{{{node decoder_model_59/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_59/outputlayer/MatMul, decoder_model_59/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.029681130705045963
cosine 0.029319882415759474
MAE: 0.002556495746224637
RMSE: 0.009849973718227227
r2: 0.9244294269974648
RMSE zero-vector: 0.04004287452915337
['1.5999999999999996custom_VAE', 'logcosh', 64, 80, 0.0008, 0.2, 188, 0.0001350218907025784, 0.00012852292172560415, 0.029681130705045963, 0.029319882415759474, 0.002556495746224637, 0.009849973718227227, 0.9244294269974648, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 64 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_180 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_180 (ReLU)               (None, 1886)         0           ['batch_normalization_180[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_180[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_180[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 23:18:40.922558: W tensorflow/c/c_api.cc:291] Operation '{name:'training_120/Adam/decay/Assign' id:77052 op device:{requested: '', assigned: ''} def:{{{node training_120/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_120/Adam/decay, training_120/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 23:18:55.890936: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_60/mul' id:76548 op device:{requested: '', assigned: ''} def:{{{node loss_60/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_60/mul/x, loss_60/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0719 - val_loss: 0.0748 - 24s/epoch - 291us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0679 - val_loss: 0.0677 - 9s/epoch - 107us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0675 - val_loss: 0.0676 - 9s/epoch - 107us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 7/85
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 8/85
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 9/85
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 10/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 108us/sample
Epoch 11/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 12/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 13/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 14/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 15/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 16/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 17/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 18/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 19/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 20/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 21/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 22/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 23/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 24/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 25/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 26/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 27/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 28/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 29/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 30/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 31/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 32/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 33/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 34/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 35/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 36/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 37/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 38/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 39/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 40/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 41/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 42/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 43/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 44/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 45/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 46/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 47/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 48/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 49/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 50/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 51/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 52/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 53/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 54/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 55/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 56/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 57/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 58/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 59/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 60/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 61/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 62/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 63/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 64/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 65/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 66/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 67/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 68/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 69/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 70/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 71/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 72/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 73/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 74/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 75/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 76/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 77/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 78/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 79/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 80/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 81/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 82/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 83/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 84/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 85/85
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0673457424337209
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 23:31:37.568443: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_60/outputlayer/BiasAdd' id:76500 op device:{requested: '', assigned: ''} def:{{{node decoder_model_60/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_60/outputlayer/MatMul, decoder_model_60/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1456653940401624
cosine 1.1384290065122253
MAE: 5.428168880381549
RMSE: 5.586349192270814
r2: -22709.73192397843
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 64, 85, 0.001, 0.2, 188, 0.06683692670129922, 0.0673457424337209, 1.1456653940401624, 1.1384290065122253, 5.428168880381549, 5.586349192270814, -22709.73192397843, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.001 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_183 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_183 (ReLU)               (None, 1980)         0           ['batch_normalization_183[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_183[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_183[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 23:32:05.698442: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_61/bias/Assign' id:77440 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_61/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_61/bias, dense_enc0_61/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 23:32:22.092391: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_61/mul' id:77873 op device:{requested: '', assigned: ''} def:{{{node loss_61/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_61/mul/x, loss_61/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 28s - loss: 0.0035 - val_loss: 9.3048e-04 - 28s/epoch - 334us/sample
Epoch 2/85
84077/84077 - 9s - loss: 8.6829e-04 - val_loss: 9.2125e-04 - 9s/epoch - 107us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 7.1231e-04 - 9s/epoch - 107us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0015 - val_loss: 8.8043e-04 - 9s/epoch - 107us/sample
Epoch 5/85
84077/84077 - 9s - loss: 6.8864e-04 - val_loss: 5.1100e-04 - 9s/epoch - 107us/sample
Epoch 6/85
84077/84077 - 9s - loss: 4.9168e-04 - val_loss: 4.4092e-04 - 9s/epoch - 108us/sample
Epoch 7/85
84077/84077 - 9s - loss: 4.3978e-04 - val_loss: 4.0663e-04 - 9s/epoch - 107us/sample
Epoch 8/85
84077/84077 - 9s - loss: 4.0216e-04 - val_loss: 3.4194e-04 - 9s/epoch - 107us/sample
Epoch 9/85
84077/84077 - 9s - loss: 3.5598e-04 - val_loss: 3.0772e-04 - 9s/epoch - 107us/sample
Epoch 10/85
84077/84077 - 9s - loss: 3.2858e-04 - val_loss: 2.7588e-04 - 9s/epoch - 107us/sample
Epoch 11/85
84077/84077 - 9s - loss: 3.0381e-04 - val_loss: 2.4983e-04 - 9s/epoch - 107us/sample
Epoch 12/85
84077/84077 - 9s - loss: 2.7419e-04 - val_loss: 2.4099e-04 - 9s/epoch - 107us/sample
Epoch 13/85
84077/84077 - 9s - loss: 2.5633e-04 - val_loss: 2.2947e-04 - 9s/epoch - 107us/sample
Epoch 14/85
84077/84077 - 9s - loss: 2.4282e-04 - val_loss: 2.0876e-04 - 9s/epoch - 108us/sample
Epoch 15/85
84077/84077 - 9s - loss: 2.2982e-04 - val_loss: 1.9880e-04 - 9s/epoch - 107us/sample
Epoch 16/85
84077/84077 - 9s - loss: 2.2011e-04 - val_loss: 1.9225e-04 - 9s/epoch - 107us/sample
Epoch 17/85
84077/84077 - 9s - loss: 2.1212e-04 - val_loss: 1.8685e-04 - 9s/epoch - 107us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.0467e-04 - val_loss: 1.7997e-04 - 9s/epoch - 107us/sample
Epoch 19/85
84077/84077 - 9s - loss: 1.9982e-04 - val_loss: 1.7586e-04 - 9s/epoch - 107us/sample
Epoch 20/85
84077/84077 - 9s - loss: 1.9318e-04 - val_loss: 1.7063e-04 - 9s/epoch - 107us/sample
Epoch 21/85
84077/84077 - 9s - loss: 1.8789e-04 - val_loss: 1.6531e-04 - 9s/epoch - 107us/sample
Epoch 22/85
84077/84077 - 9s - loss: 1.8368e-04 - val_loss: 1.6374e-04 - 9s/epoch - 109us/sample
Epoch 23/85
84077/84077 - 9s - loss: 1.8068e-04 - val_loss: 1.6134e-04 - 9s/epoch - 108us/sample
Epoch 24/85
84077/84077 - 9s - loss: 1.7674e-04 - val_loss: 1.5889e-04 - 9s/epoch - 107us/sample
Epoch 25/85
84077/84077 - 9s - loss: 1.7414e-04 - val_loss: 1.5814e-04 - 9s/epoch - 107us/sample
Epoch 26/85
84077/84077 - 9s - loss: 1.7227e-04 - val_loss: 1.5422e-04 - 9s/epoch - 107us/sample
Epoch 27/85
84077/84077 - 9s - loss: 1.7055e-04 - val_loss: 1.5462e-04 - 9s/epoch - 107us/sample
Epoch 28/85
84077/84077 - 9s - loss: 1.6798e-04 - val_loss: 1.5081e-04 - 9s/epoch - 107us/sample
Epoch 29/85
84077/84077 - 9s - loss: 1.6623e-04 - val_loss: 1.4826e-04 - 9s/epoch - 107us/sample
Epoch 30/85
84077/84077 - 9s - loss: 1.6545e-04 - val_loss: 1.4972e-04 - 9s/epoch - 109us/sample
Epoch 31/85
84077/84077 - 9s - loss: 1.6344e-04 - val_loss: 1.4735e-04 - 9s/epoch - 107us/sample
Epoch 32/85
84077/84077 - 9s - loss: 1.6251e-04 - val_loss: 1.4650e-04 - 9s/epoch - 107us/sample
Epoch 33/85
84077/84077 - 9s - loss: 1.6109e-04 - val_loss: 1.4646e-04 - 9s/epoch - 107us/sample
Epoch 34/85
84077/84077 - 9s - loss: 1.6016e-04 - val_loss: 1.4688e-04 - 9s/epoch - 107us/sample
Epoch 35/85
84077/84077 - 9s - loss: 1.5893e-04 - val_loss: 1.4384e-04 - 9s/epoch - 107us/sample
Epoch 36/85
84077/84077 - 9s - loss: 1.5852e-04 - val_loss: 1.4407e-04 - 9s/epoch - 107us/sample
Epoch 37/85
84077/84077 - 9s - loss: 1.5756e-04 - val_loss: 1.4365e-04 - 9s/epoch - 107us/sample
Epoch 38/85
84077/84077 - 9s - loss: 1.5663e-04 - val_loss: 1.4176e-04 - 9s/epoch - 108us/sample
Epoch 39/85
84077/84077 - 9s - loss: 1.5571e-04 - val_loss: 1.4193e-04 - 9s/epoch - 108us/sample
Epoch 40/85
84077/84077 - 9s - loss: 1.5547e-04 - val_loss: 1.4149e-04 - 9s/epoch - 107us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.5473e-04 - val_loss: 1.4143e-04 - 9s/epoch - 107us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.5388e-04 - val_loss: 1.4056e-04 - 9s/epoch - 107us/sample
Epoch 43/85
84077/84077 - 9s - loss: 1.5339e-04 - val_loss: 1.3955e-04 - 9s/epoch - 107us/sample
Epoch 44/85
84077/84077 - 9s - loss: 1.5320e-04 - val_loss: 1.4025e-04 - 9s/epoch - 107us/sample
Epoch 45/85
84077/84077 - 9s - loss: 1.5214e-04 - val_loss: 1.3953e-04 - 9s/epoch - 107us/sample
Epoch 46/85
84077/84077 - 9s - loss: 1.5159e-04 - val_loss: 1.3936e-04 - 9s/epoch - 108us/sample
Epoch 47/85
84077/84077 - 9s - loss: 1.5126e-04 - val_loss: 1.3881e-04 - 9s/epoch - 108us/sample
Epoch 48/85
84077/84077 - 9s - loss: 1.5033e-04 - val_loss: 1.3955e-04 - 9s/epoch - 107us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.5025e-04 - val_loss: 1.3871e-04 - 9s/epoch - 107us/sample
Epoch 50/85
84077/84077 - 9s - loss: 1.4934e-04 - val_loss: 1.3753e-04 - 9s/epoch - 107us/sample
Epoch 51/85
84077/84077 - 9s - loss: 1.4906e-04 - val_loss: 1.3691e-04 - 9s/epoch - 107us/sample
Epoch 52/85
84077/84077 - 9s - loss: 1.4873e-04 - val_loss: 1.3679e-04 - 9s/epoch - 107us/sample
Epoch 53/85
84077/84077 - 9s - loss: 1.4842e-04 - val_loss: 1.3656e-04 - 9s/epoch - 107us/sample
Epoch 54/85
84077/84077 - 9s - loss: 1.4823e-04 - val_loss: 1.3709e-04 - 9s/epoch - 109us/sample
Epoch 55/85
84077/84077 - 9s - loss: 1.4778e-04 - val_loss: 1.3516e-04 - 9s/epoch - 108us/sample
Epoch 56/85
84077/84077 - 9s - loss: 1.4715e-04 - val_loss: 1.3578e-04 - 9s/epoch - 107us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.4709e-04 - val_loss: 1.3553e-04 - 9s/epoch - 107us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.4658e-04 - val_loss: 1.3750e-04 - 9s/epoch - 107us/sample
Epoch 59/85
84077/84077 - 9s - loss: 1.4596e-04 - val_loss: 1.3466e-04 - 9s/epoch - 107us/sample
Epoch 60/85
84077/84077 - 9s - loss: 1.4595e-04 - val_loss: 1.3414e-04 - 9s/epoch - 107us/sample
Epoch 61/85
84077/84077 - 9s - loss: 1.4543e-04 - val_loss: 1.3475e-04 - 9s/epoch - 107us/sample
Epoch 62/85
84077/84077 - 9s - loss: 1.4555e-04 - val_loss: 1.3500e-04 - 9s/epoch - 109us/sample
Epoch 63/85
84077/84077 - 9s - loss: 1.4537e-04 - val_loss: 1.3455e-04 - 9s/epoch - 108us/sample
Epoch 64/85
84077/84077 - 9s - loss: 1.4444e-04 - val_loss: 1.3304e-04 - 9s/epoch - 107us/sample
Epoch 65/85
84077/84077 - 9s - loss: 1.4450e-04 - val_loss: 1.3410e-04 - 9s/epoch - 107us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.4393e-04 - val_loss: 1.3380e-04 - 9s/epoch - 107us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.4437e-04 - val_loss: 1.3393e-04 - 9s/epoch - 107us/sample
Epoch 68/85
84077/84077 - 9s - loss: 1.4373e-04 - val_loss: 1.3351e-04 - 9s/epoch - 107us/sample
Epoch 69/85
84077/84077 - 9s - loss: 1.4306e-04 - val_loss: 1.3390e-04 - 9s/epoch - 107us/sample
Epoch 70/85
84077/84077 - 9s - loss: 1.4343e-04 - val_loss: 1.3233e-04 - 9s/epoch - 108us/sample
Epoch 71/85
84077/84077 - 9s - loss: 1.4249e-04 - val_loss: 1.3259e-04 - 9s/epoch - 108us/sample
Epoch 72/85
84077/84077 - 9s - loss: 1.4230e-04 - val_loss: 1.3268e-04 - 9s/epoch - 107us/sample
Epoch 73/85
84077/84077 - 9s - loss: 1.4205e-04 - val_loss: 1.3200e-04 - 9s/epoch - 107us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.4193e-04 - val_loss: 1.3157e-04 - 9s/epoch - 107us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.4116e-04 - val_loss: 1.3145e-04 - 9s/epoch - 107us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.4119e-04 - val_loss: 1.3231e-04 - 9s/epoch - 107us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.4111e-04 - val_loss: 1.3146e-04 - 9s/epoch - 107us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.4077e-04 - val_loss: 1.3104e-04 - 9s/epoch - 108us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.4129e-04 - val_loss: 1.3262e-04 - 9s/epoch - 108us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.4010e-04 - val_loss: 1.3081e-04 - 9s/epoch - 107us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.4029e-04 - val_loss: 1.3184e-04 - 9s/epoch - 107us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.4015e-04 - val_loss: 1.2943e-04 - 9s/epoch - 107us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.3997e-04 - val_loss: 1.2932e-04 - 9s/epoch - 107us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.3973e-04 - val_loss: 1.3058e-04 - 9s/epoch - 107us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.3963e-04 - val_loss: 1.2996e-04 - 9s/epoch - 107us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012996316649062286
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 23:45:06.561917: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_61/outputlayer/BiasAdd' id:77837 op device:{requested: '', assigned: ''} def:{{{node decoder_model_61/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_61/outputlayer/MatMul, decoder_model_61/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03357948358211882
cosine 0.03317478714451507
MAE: 0.0024992170180888613
RMSE: 0.010485303265723047
r2: 0.9142493977216998
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 85, 0.001, 0.2, 188, 0.0001396331034202361, 0.00012996316649062286, 0.03357948358211882, 0.03317478714451507, 0.0024992170180888613, 0.010485303265723047, 0.9142493977216998, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0008 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_186 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_186 (ReLU)               (None, 2074)         0           ['batch_normalization_186[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_186[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_186[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 23:45:39.857127: W tensorflow/c/c_api.cc:291] Operation '{name:'training_124/Adam/bottleneck_zlog_62/bias/v/Assign' id:79793 op device:{requested: '', assigned: ''} def:{{{node training_124/Adam/bottleneck_zlog_62/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_124/Adam/bottleneck_zlog_62/bias/v, training_124/Adam/bottleneck_zlog_62/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 23:45:56.551052: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_62/mul' id:79158 op device:{requested: '', assigned: ''} def:{{{node loss_62/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_62/mul/x, loss_62/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 29s - loss: 0.0288 - val_loss: 9.3809e-04 - 29s/epoch - 342us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0110 - val_loss: 0.0093 - 9s/epoch - 109us/sample
Epoch 3/85
84077/84077 - 9s - loss: 8.9986e-04 - val_loss: 0.0013 - 9s/epoch - 108us/sample
Epoch 4/85
84077/84077 - 9s - loss: 7.2277e-04 - val_loss: 5.9710e-04 - 9s/epoch - 108us/sample
Epoch 5/85
84077/84077 - 9s - loss: 6.6845e-04 - val_loss: 5.5277e-04 - 9s/epoch - 108us/sample
Epoch 6/85
84077/84077 - 9s - loss: 4.6674e-04 - val_loss: 3.9556e-04 - 9s/epoch - 108us/sample
Epoch 7/85
84077/84077 - 9s - loss: 4.4017e-04 - val_loss: 3.5619e-04 - 9s/epoch - 108us/sample
Epoch 8/85
84077/84077 - 9s - loss: 4.1025e-04 - val_loss: 3.4138e-04 - 9s/epoch - 108us/sample
Epoch 9/85
84077/84077 - 9s - loss: 3.5361e-04 - val_loss: 3.0436e-04 - 9s/epoch - 108us/sample
Epoch 10/85
84077/84077 - 9s - loss: 3.1221e-04 - val_loss: 2.7085e-04 - 9s/epoch - 108us/sample
Epoch 11/85
84077/84077 - 9s - loss: 2.9129e-04 - val_loss: 2.5667e-04 - 9s/epoch - 109us/sample
Epoch 12/85
84077/84077 - 9s - loss: 2.7584e-04 - val_loss: 2.3734e-04 - 9s/epoch - 108us/sample
Epoch 13/85
84077/84077 - 9s - loss: 2.5530e-04 - val_loss: 2.2159e-04 - 9s/epoch - 108us/sample
Epoch 14/85
84077/84077 - 9s - loss: 2.4141e-04 - val_loss: 2.0978e-04 - 9s/epoch - 108us/sample
Epoch 15/85
84077/84077 - 9s - loss: 2.2847e-04 - val_loss: 2.0262e-04 - 9s/epoch - 108us/sample
Epoch 16/85
84077/84077 - 9s - loss: 2.1700e-04 - val_loss: 1.9134e-04 - 9s/epoch - 108us/sample
Epoch 17/85
84077/84077 - 9s - loss: 2.0781e-04 - val_loss: 1.8378e-04 - 9s/epoch - 108us/sample
Epoch 18/85
84077/84077 - 9s - loss: 1.9871e-04 - val_loss: 1.7790e-04 - 9s/epoch - 108us/sample
Epoch 19/85
84077/84077 - 9s - loss: 1.9174e-04 - val_loss: 1.6856e-04 - 9s/epoch - 109us/sample
Epoch 20/85
84077/84077 - 9s - loss: 1.8583e-04 - val_loss: 1.6562e-04 - 9s/epoch - 108us/sample
Epoch 21/85
84077/84077 - 9s - loss: 1.8152e-04 - val_loss: 1.6274e-04 - 9s/epoch - 108us/sample
Epoch 22/85
84077/84077 - 9s - loss: 1.7681e-04 - val_loss: 1.5954e-04 - 9s/epoch - 108us/sample
Epoch 23/85
84077/84077 - 9s - loss: 1.7335e-04 - val_loss: 1.5524e-04 - 9s/epoch - 108us/sample
Epoch 24/85
84077/84077 - 9s - loss: 1.7021e-04 - val_loss: 1.5421e-04 - 9s/epoch - 108us/sample
Epoch 25/85
84077/84077 - 9s - loss: 1.6777e-04 - val_loss: 1.5183e-04 - 9s/epoch - 108us/sample
Epoch 26/85
84077/84077 - 9s - loss: 1.6504e-04 - val_loss: 1.4935e-04 - 9s/epoch - 108us/sample
Epoch 27/85
84077/84077 - 9s - loss: 1.6271e-04 - val_loss: 1.4672e-04 - 9s/epoch - 109us/sample
Epoch 28/85
84077/84077 - 9s - loss: 1.6112e-04 - val_loss: 1.4604e-04 - 9s/epoch - 109us/sample
Epoch 29/85
84077/84077 - 9s - loss: 1.5933e-04 - val_loss: 1.4414e-04 - 9s/epoch - 108us/sample
Epoch 30/85
84077/84077 - 9s - loss: 1.5781e-04 - val_loss: 1.4393e-04 - 9s/epoch - 108us/sample
Epoch 31/85
84077/84077 - 9s - loss: 1.5706e-04 - val_loss: 1.4360e-04 - 9s/epoch - 108us/sample
Epoch 32/85
84077/84077 - 9s - loss: 1.5531e-04 - val_loss: 1.4129e-04 - 9s/epoch - 108us/sample
Epoch 33/85
84077/84077 - 9s - loss: 1.5417e-04 - val_loss: 1.4172e-04 - 9s/epoch - 108us/sample
Epoch 34/85
84077/84077 - 9s - loss: 1.5430e-04 - val_loss: 1.3954e-04 - 9s/epoch - 108us/sample
Epoch 35/85
84077/84077 - 9s - loss: 1.5252e-04 - val_loss: 1.3998e-04 - 9s/epoch - 109us/sample
Epoch 36/85
84077/84077 - 9s - loss: 1.5140e-04 - val_loss: 1.3944e-04 - 9s/epoch - 108us/sample
Epoch 37/85
84077/84077 - 9s - loss: 1.5045e-04 - val_loss: 1.3784e-04 - 9s/epoch - 108us/sample
Epoch 38/85
84077/84077 - 9s - loss: 1.4974e-04 - val_loss: 1.3828e-04 - 9s/epoch - 108us/sample
Epoch 39/85
84077/84077 - 9s - loss: 1.4889e-04 - val_loss: 1.3680e-04 - 9s/epoch - 108us/sample
Epoch 40/85
84077/84077 - 9s - loss: 1.4855e-04 - val_loss: 1.3663e-04 - 9s/epoch - 108us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.4750e-04 - val_loss: 1.3566e-04 - 9s/epoch - 108us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.4699e-04 - val_loss: 1.3543e-04 - 9s/epoch - 108us/sample
Epoch 43/85
84077/84077 - 9s - loss: 1.4650e-04 - val_loss: 1.3480e-04 - 9s/epoch - 109us/sample
Epoch 44/85
84077/84077 - 9s - loss: 1.4581e-04 - val_loss: 1.3406e-04 - 9s/epoch - 108us/sample
Epoch 45/85
84077/84077 - 9s - loss: 1.4545e-04 - val_loss: 1.3403e-04 - 9s/epoch - 108us/sample
Epoch 46/85
84077/84077 - 9s - loss: 1.4472e-04 - val_loss: 1.3353e-04 - 9s/epoch - 108us/sample
Epoch 47/85
84077/84077 - 9s - loss: 1.4444e-04 - val_loss: 1.3275e-04 - 9s/epoch - 108us/sample
Epoch 48/85
84077/84077 - 9s - loss: 1.4401e-04 - val_loss: 1.3375e-04 - 9s/epoch - 108us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.4351e-04 - val_loss: 1.3317e-04 - 9s/epoch - 108us/sample
Epoch 50/85
84077/84077 - 9s - loss: 1.4291e-04 - val_loss: 1.3176e-04 - 9s/epoch - 108us/sample
Epoch 51/85
84077/84077 - 9s - loss: 1.4291e-04 - val_loss: 1.3188e-04 - 9s/epoch - 109us/sample
Epoch 52/85
84077/84077 - 9s - loss: 1.4238e-04 - val_loss: 1.3147e-04 - 9s/epoch - 108us/sample
Epoch 53/85
84077/84077 - 9s - loss: 1.4213e-04 - val_loss: 1.3108e-04 - 9s/epoch - 108us/sample
Epoch 54/85
84077/84077 - 9s - loss: 1.4133e-04 - val_loss: 1.3132e-04 - 9s/epoch - 108us/sample
Epoch 55/85
84077/84077 - 9s - loss: 1.4120e-04 - val_loss: 1.3283e-04 - 9s/epoch - 108us/sample
Epoch 56/85
84077/84077 - 9s - loss: 1.4111e-04 - val_loss: 1.3146e-04 - 9s/epoch - 108us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.4029e-04 - val_loss: 1.3028e-04 - 9s/epoch - 108us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.4010e-04 - val_loss: 1.3094e-04 - 9s/epoch - 108us/sample
Epoch 59/85
84077/84077 - 9s - loss: 1.4023e-04 - val_loss: 1.3080e-04 - 9s/epoch - 109us/sample
Epoch 60/85
84077/84077 - 9s - loss: 1.3946e-04 - val_loss: 1.3020e-04 - 9s/epoch - 108us/sample
Epoch 61/85
84077/84077 - 9s - loss: 1.3920e-04 - val_loss: 1.2949e-04 - 9s/epoch - 108us/sample
Epoch 62/85
84077/84077 - 9s - loss: 1.3907e-04 - val_loss: 1.2890e-04 - 9s/epoch - 108us/sample
Epoch 63/85
84077/84077 - 9s - loss: 1.3879e-04 - val_loss: 1.2887e-04 - 9s/epoch - 108us/sample
Epoch 64/85
84077/84077 - 9s - loss: 1.3843e-04 - val_loss: 1.2842e-04 - 9s/epoch - 108us/sample
Epoch 65/85
84077/84077 - 9s - loss: 1.3805e-04 - val_loss: 1.2980e-04 - 9s/epoch - 108us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.3807e-04 - val_loss: 1.2879e-04 - 9s/epoch - 108us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.3769e-04 - val_loss: 1.2815e-04 - 9s/epoch - 110us/sample
Epoch 68/85
84077/84077 - 9s - loss: 1.3750e-04 - val_loss: 1.2790e-04 - 9s/epoch - 108us/sample
Epoch 69/85
84077/84077 - 9s - loss: 1.3727e-04 - val_loss: 1.2868e-04 - 9s/epoch - 108us/sample
Epoch 70/85
84077/84077 - 9s - loss: 1.3692e-04 - val_loss: 1.2847e-04 - 9s/epoch - 108us/sample
Epoch 71/85
84077/84077 - 9s - loss: 1.3676e-04 - val_loss: 1.2852e-04 - 9s/epoch - 108us/sample
Epoch 72/85
84077/84077 - 9s - loss: 1.3636e-04 - val_loss: 1.2788e-04 - 9s/epoch - 108us/sample
Epoch 73/85
84077/84077 - 9s - loss: 1.3614e-04 - val_loss: 1.2730e-04 - 9s/epoch - 108us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.3606e-04 - val_loss: 1.2732e-04 - 9s/epoch - 108us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.3566e-04 - val_loss: 1.2791e-04 - 9s/epoch - 109us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.3629e-04 - val_loss: 1.2673e-04 - 9s/epoch - 108us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.3559e-04 - val_loss: 1.2671e-04 - 9s/epoch - 108us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.3514e-04 - val_loss: 1.2722e-04 - 9s/epoch - 108us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.3502e-04 - val_loss: 1.2602e-04 - 9s/epoch - 108us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.3494e-04 - val_loss: 1.2625e-04 - 9s/epoch - 108us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.3482e-04 - val_loss: 1.2689e-04 - 9s/epoch - 108us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.3435e-04 - val_loss: 1.2610e-04 - 9s/epoch - 108us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.3426e-04 - val_loss: 1.2738e-04 - 9s/epoch - 109us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.3432e-04 - val_loss: 1.2546e-04 - 9s/epoch - 108us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.3432e-04 - val_loss: 1.2654e-04 - 9s/epoch - 108us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012653699785977298
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-11 23:58:46.933740: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_62/outputlayer/BiasAdd' id:79122 op device:{requested: '', assigned: ''} def:{{{node decoder_model_62/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_62/outputlayer/MatMul, decoder_model_62/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.030988065639899545
cosine 0.030608281532264768
MAE: 0.002297737303647249
RMSE: 0.010001363337016197
r2: 0.9221599580181647
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 64, 85, 0.0008, 0.2, 188, 0.00013432303095468185, 0.00012653699785977298, 0.030988065639899545, 0.030608281532264768, 0.002297737303647249, 0.010001363337016197, 0.9221599580181647, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0008 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_189 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_189 (ReLU)               (None, 1886)         0           ['batch_normalization_189[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_189[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_189[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-11 23:59:19.563539: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_63/kernel/Assign' id:80091 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_63/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_63/kernel, bottleneck_zmean_63/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-11 23:59:36.340383: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_63/mul' id:80443 op device:{requested: '', assigned: ''} def:{{{node loss_63/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_63/mul/x, loss_63/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 29s - loss: 0.0042 - val_loss: 9.0191e-04 - 29s/epoch - 342us/sample
Epoch 2/85
84077/84077 - 9s - loss: 8.8159e-04 - val_loss: 0.0011 - 9s/epoch - 108us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0051 - val_loss: 7.0120e-04 - 9s/epoch - 108us/sample
Epoch 4/85
84077/84077 - 9s - loss: 7.1436e-04 - val_loss: 6.6416e-04 - 9s/epoch - 108us/sample
Epoch 5/85
84077/84077 - 9s - loss: 7.8416e-04 - val_loss: 0.0017 - 9s/epoch - 108us/sample
Epoch 6/85
84077/84077 - 9s - loss: 6.9628e-04 - val_loss: 4.2610e-04 - 9s/epoch - 109us/sample
Epoch 7/85
84077/84077 - 9s - loss: 4.1831e-04 - val_loss: 3.7304e-04 - 9s/epoch - 110us/sample
Epoch 8/85
84077/84077 - 9s - loss: 3.7875e-04 - val_loss: 3.3738e-04 - 9s/epoch - 109us/sample
Epoch 9/85
84077/84077 - 9s - loss: 3.5666e-04 - val_loss: 3.0175e-04 - 9s/epoch - 108us/sample
Epoch 10/85
84077/84077 - 9s - loss: 3.2506e-04 - val_loss: 2.8146e-04 - 9s/epoch - 108us/sample
Epoch 11/85
84077/84077 - 9s - loss: 3.0345e-04 - val_loss: 2.6791e-04 - 9s/epoch - 108us/sample
Epoch 12/85
84077/84077 - 9s - loss: 2.7995e-04 - val_loss: 2.4700e-04 - 9s/epoch - 109us/sample
Epoch 13/85
84077/84077 - 9s - loss: 2.6491e-04 - val_loss: 2.3122e-04 - 9s/epoch - 108us/sample
Epoch 14/85
84077/84077 - 9s - loss: 2.5124e-04 - val_loss: 2.4192e-04 - 9s/epoch - 108us/sample
Epoch 15/85
84077/84077 - 9s - loss: 2.4091e-04 - val_loss: 2.0941e-04 - 9s/epoch - 110us/sample
Epoch 16/85
84077/84077 - 9s - loss: 2.3001e-04 - val_loss: 2.0537e-04 - 9s/epoch - 109us/sample
Epoch 17/85
84077/84077 - 9s - loss: 2.2412e-04 - val_loss: 1.9454e-04 - 9s/epoch - 108us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.1383e-04 - val_loss: 1.8686e-04 - 9s/epoch - 108us/sample
Epoch 19/85
84077/84077 - 9s - loss: 2.0829e-04 - val_loss: 1.8159e-04 - 9s/epoch - 108us/sample
Epoch 20/85
84077/84077 - 9s - loss: 2.0257e-04 - val_loss: 1.7720e-04 - 9s/epoch - 109us/sample
Epoch 21/85
84077/84077 - 9s - loss: 1.9690e-04 - val_loss: 1.7386e-04 - 9s/epoch - 108us/sample
Epoch 22/85
84077/84077 - 9s - loss: 1.9292e-04 - val_loss: 1.7084e-04 - 9s/epoch - 109us/sample
Epoch 23/85
84077/84077 - 9s - loss: 1.8825e-04 - val_loss: 1.6601e-04 - 9s/epoch - 110us/sample
Epoch 24/85
84077/84077 - 9s - loss: 1.8513e-04 - val_loss: 1.6313e-04 - 9s/epoch - 109us/sample
Epoch 25/85
84077/84077 - 9s - loss: 1.8084e-04 - val_loss: 1.5838e-04 - 9s/epoch - 108us/sample
Epoch 26/85
84077/84077 - 9s - loss: 1.7715e-04 - val_loss: 1.5602e-04 - 9s/epoch - 108us/sample
Epoch 27/85
84077/84077 - 9s - loss: 1.7448e-04 - val_loss: 1.5456e-04 - 9s/epoch - 108us/sample
Epoch 28/85
84077/84077 - 9s - loss: 1.7237e-04 - val_loss: 1.5214e-04 - 9s/epoch - 108us/sample
Epoch 29/85
84077/84077 - 9s - loss: 1.7026e-04 - val_loss: 1.5136e-04 - 9s/epoch - 108us/sample
Epoch 30/85
84077/84077 - 9s - loss: 1.6824e-04 - val_loss: 1.4889e-04 - 9s/epoch - 109us/sample
Epoch 31/85
84077/84077 - 9s - loss: 1.6654e-04 - val_loss: 1.4839e-04 - 9s/epoch - 110us/sample
Epoch 32/85
84077/84077 - 9s - loss: 1.6469e-04 - val_loss: 1.4970e-04 - 9s/epoch - 109us/sample
Epoch 33/85
84077/84077 - 9s - loss: 1.6359e-04 - val_loss: 1.4604e-04 - 9s/epoch - 108us/sample
Epoch 34/85
84077/84077 - 9s - loss: 1.6251e-04 - val_loss: 1.4442e-04 - 9s/epoch - 109us/sample
Epoch 35/85
84077/84077 - 9s - loss: 1.6139e-04 - val_loss: 1.4440e-04 - 9s/epoch - 108us/sample
Epoch 36/85
84077/84077 - 9s - loss: 1.6076e-04 - val_loss: 1.4316e-04 - 9s/epoch - 109us/sample
Epoch 37/85
84077/84077 - 9s - loss: 1.6036e-04 - val_loss: 1.4251e-04 - 9s/epoch - 108us/sample
Epoch 38/85
84077/84077 - 9s - loss: 1.5887e-04 - val_loss: 1.4142e-04 - 9s/epoch - 108us/sample
Epoch 39/85
84077/84077 - 9s - loss: 1.5760e-04 - val_loss: 1.4092e-04 - 9s/epoch - 110us/sample
Epoch 40/85
84077/84077 - 9s - loss: 1.5684e-04 - val_loss: 1.4045e-04 - 9s/epoch - 109us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.5610e-04 - val_loss: 1.4044e-04 - 9s/epoch - 108us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.5571e-04 - val_loss: 1.3964e-04 - 9s/epoch - 108us/sample
Epoch 43/85
84077/84077 - 9s - loss: 1.5495e-04 - val_loss: 1.3808e-04 - 9s/epoch - 108us/sample
Epoch 44/85
84077/84077 - 9s - loss: 1.5431e-04 - val_loss: 1.3796e-04 - 9s/epoch - 109us/sample
Epoch 45/85
84077/84077 - 9s - loss: 1.5321e-04 - val_loss: 1.3759e-04 - 9s/epoch - 108us/sample
Epoch 46/85
84077/84077 - 9s - loss: 1.5337e-04 - val_loss: 1.3831e-04 - 9s/epoch - 109us/sample
Epoch 47/85
84077/84077 - 9s - loss: 1.5279e-04 - val_loss: 1.3803e-04 - 9s/epoch - 110us/sample
Epoch 48/85
84077/84077 - 9s - loss: 1.5231e-04 - val_loss: 1.3629e-04 - 9s/epoch - 109us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.5160e-04 - val_loss: 1.3595e-04 - 9s/epoch - 108us/sample
Epoch 50/85
84077/84077 - 9s - loss: 1.5088e-04 - val_loss: 1.3630e-04 - 9s/epoch - 108us/sample
Epoch 51/85
84077/84077 - 9s - loss: 1.5079e-04 - val_loss: 1.3628e-04 - 9s/epoch - 108us/sample
Epoch 52/85
84077/84077 - 9s - loss: 1.4996e-04 - val_loss: 1.3517e-04 - 9s/epoch - 109us/sample
Epoch 53/85
84077/84077 - 9s - loss: 1.4978e-04 - val_loss: 1.3570e-04 - 9s/epoch - 108us/sample
Epoch 54/85
84077/84077 - 9s - loss: 1.4975e-04 - val_loss: 1.3415e-04 - 9s/epoch - 109us/sample
Epoch 55/85
84077/84077 - 9s - loss: 1.4895e-04 - val_loss: 1.3468e-04 - 9s/epoch - 110us/sample
Epoch 56/85
84077/84077 - 9s - loss: 1.4897e-04 - val_loss: 1.3363e-04 - 9s/epoch - 108us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.4828e-04 - val_loss: 1.3289e-04 - 9s/epoch - 108us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.4760e-04 - val_loss: 1.3480e-04 - 9s/epoch - 108us/sample
Epoch 59/85
84077/84077 - 9s - loss: 1.4769e-04 - val_loss: 1.3343e-04 - 9s/epoch - 109us/sample
Epoch 60/85
84077/84077 - 9s - loss: 1.4717e-04 - val_loss: 1.3381e-04 - 9s/epoch - 108us/sample
Epoch 61/85
84077/84077 - 9s - loss: 1.4690e-04 - val_loss: 1.3301e-04 - 9s/epoch - 108us/sample
Epoch 62/85
84077/84077 - 9s - loss: 1.4623e-04 - val_loss: 1.3257e-04 - 9s/epoch - 109us/sample
Epoch 63/85
84077/84077 - 9s - loss: 1.4595e-04 - val_loss: 1.3197e-04 - 9s/epoch - 110us/sample
Epoch 64/85
84077/84077 - 9s - loss: 1.4587e-04 - val_loss: 1.3318e-04 - 9s/epoch - 108us/sample
Epoch 65/85
84077/84077 - 9s - loss: 1.4538e-04 - val_loss: 1.3187e-04 - 9s/epoch - 108us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.4525e-04 - val_loss: 1.3108e-04 - 9s/epoch - 108us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.4482e-04 - val_loss: 1.3217e-04 - 9s/epoch - 108us/sample
Epoch 68/85
84077/84077 - 9s - loss: 1.4439e-04 - val_loss: 1.3186e-04 - 9s/epoch - 108us/sample
Epoch 69/85
84077/84077 - 9s - loss: 1.4444e-04 - val_loss: 1.3119e-04 - 9s/epoch - 109us/sample
Epoch 70/85
84077/84077 - 9s - loss: 1.4365e-04 - val_loss: 1.3133e-04 - 9s/epoch - 109us/sample
Epoch 71/85
84077/84077 - 9s - loss: 1.4401e-04 - val_loss: 1.3184e-04 - 9s/epoch - 109us/sample
Epoch 72/85
84077/84077 - 9s - loss: 1.4387e-04 - val_loss: 1.3020e-04 - 9s/epoch - 108us/sample
Epoch 73/85
84077/84077 - 9s - loss: 1.4332e-04 - val_loss: 1.2998e-04 - 9s/epoch - 108us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.4325e-04 - val_loss: 1.3093e-04 - 9s/epoch - 108us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.4319e-04 - val_loss: 1.2974e-04 - 9s/epoch - 108us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.4255e-04 - val_loss: 1.2957e-04 - 9s/epoch - 108us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.4256e-04 - val_loss: 1.2882e-04 - 9s/epoch - 108us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.4270e-04 - val_loss: 1.3144e-04 - 9s/epoch - 109us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.4200e-04 - val_loss: 1.2828e-04 - 9s/epoch - 109us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.4185e-04 - val_loss: 1.3004e-04 - 9s/epoch - 108us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.4156e-04 - val_loss: 1.2918e-04 - 9s/epoch - 108us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.4164e-04 - val_loss: 1.2851e-04 - 9s/epoch - 108us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.4091e-04 - val_loss: 1.2939e-04 - 9s/epoch - 108us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.4163e-04 - val_loss: 1.2836e-04 - 9s/epoch - 108us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.4124e-04 - val_loss: 1.2885e-04 - 9s/epoch - 108us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001288538614876504
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 00:12:29.484060: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_63/outputlayer/BiasAdd' id:80407 op device:{requested: '', assigned: ''} def:{{{node decoder_model_63/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_63/outputlayer/MatMul, decoder_model_63/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03375015752308857
cosine 0.033340709252364056
MAE: 0.0025450311260487362
RMSE: 0.010411979657092476
r2: 0.9154547572455062
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 85, 0.0008, 0.2, 188, 0.0001412435627810691, 0.0001288538614876504, 0.03375015752308857, 0.033340709252364056, 0.0025450311260487362, 0.010411979657092476, 0.9154547572455062, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0012000000000000001 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_192 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_192 (ReLU)               (None, 1886)         0           ['batch_normalization_192[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_192[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_192[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 00:13:02.761024: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_64/kernel/Assign' id:81600 op device:{requested: '', assigned: ''} def:{{{node outputlayer_64/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_64/kernel, outputlayer_64/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 00:13:19.350555: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_64/mul' id:81721 op device:{requested: '', assigned: ''} def:{{{node loss_64/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_64/mul/x, loss_64/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 29s - loss: 0.3508 - val_loss: 0.0019 - 29s/epoch - 345us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0020 - val_loss: 0.0016 - 9s/epoch - 110us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 109us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0145 - val_loss: 0.0012 - 9s/epoch - 109us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 109us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0013 - val_loss: 8.7453e-04 - 9s/epoch - 109us/sample
Epoch 7/85
84077/84077 - 9s - loss: 8.5232e-04 - val_loss: 7.2387e-04 - 9s/epoch - 109us/sample
Epoch 8/85
84077/84077 - 9s - loss: 7.8054e-04 - val_loss: 6.2979e-04 - 9s/epoch - 109us/sample
Epoch 9/85
84077/84077 - 9s - loss: 0.0011 - val_loss: 7.7790e-04 - 9s/epoch - 108us/sample
Epoch 10/85
84077/84077 - 9s - loss: 6.3876e-04 - val_loss: 5.1907e-04 - 9s/epoch - 109us/sample
Epoch 11/85
84077/84077 - 9s - loss: 5.5843e-04 - val_loss: 6.4132e-04 - 9s/epoch - 110us/sample
Epoch 12/85
84077/84077 - 9s - loss: 5.0325e-04 - val_loss: 4.1471e-04 - 9s/epoch - 109us/sample
Epoch 13/85
84077/84077 - 9s - loss: 4.8056e-04 - val_loss: 3.8230e-04 - 9s/epoch - 109us/sample
Epoch 14/85
84077/84077 - 9s - loss: 4.1721e-04 - val_loss: 3.6595e-04 - 9s/epoch - 109us/sample
Epoch 15/85
84077/84077 - 9s - loss: 3.9360e-04 - val_loss: 3.3164e-04 - 9s/epoch - 109us/sample
Epoch 16/85
84077/84077 - 9s - loss: 3.6797e-04 - val_loss: 3.3934e-04 - 9s/epoch - 109us/sample
Epoch 17/85
84077/84077 - 9s - loss: 3.4728e-04 - val_loss: 2.9704e-04 - 9s/epoch - 109us/sample
Epoch 18/85
84077/84077 - 9s - loss: 3.2968e-04 - val_loss: 2.9125e-04 - 9s/epoch - 109us/sample
Epoch 19/85
84077/84077 - 9s - loss: 3.1240e-04 - val_loss: 2.7405e-04 - 9s/epoch - 110us/sample
Epoch 20/85
84077/84077 - 9s - loss: 3.0364e-04 - val_loss: 2.7119e-04 - 9s/epoch - 109us/sample
Epoch 21/85
84077/84077 - 9s - loss: 2.9120e-04 - val_loss: 2.6314e-04 - 9s/epoch - 109us/sample
Epoch 22/85
84077/84077 - 9s - loss: 2.8016e-04 - val_loss: 2.5555e-04 - 9s/epoch - 109us/sample
Epoch 23/85
84077/84077 - 9s - loss: 2.7324e-04 - val_loss: 2.4814e-04 - 9s/epoch - 109us/sample
Epoch 24/85
84077/84077 - 9s - loss: 2.6570e-04 - val_loss: 2.4054e-04 - 9s/epoch - 109us/sample
Epoch 25/85
84077/84077 - 9s - loss: 2.5870e-04 - val_loss: 2.3718e-04 - 9s/epoch - 108us/sample
Epoch 26/85
84077/84077 - 9s - loss: 2.5448e-04 - val_loss: 2.3439e-04 - 9s/epoch - 110us/sample
Epoch 27/85
84077/84077 - 9s - loss: 2.4893e-04 - val_loss: 2.2940e-04 - 9s/epoch - 109us/sample
Epoch 28/85
84077/84077 - 9s - loss: 2.4523e-04 - val_loss: 2.2547e-04 - 9s/epoch - 109us/sample
Epoch 29/85
84077/84077 - 9s - loss: 2.4095e-04 - val_loss: 2.2582e-04 - 9s/epoch - 109us/sample
Epoch 30/85
84077/84077 - 9s - loss: 2.3743e-04 - val_loss: 2.2262e-04 - 9s/epoch - 109us/sample
Epoch 31/85
84077/84077 - 9s - loss: 2.3446e-04 - val_loss: 2.1647e-04 - 9s/epoch - 108us/sample
Epoch 32/85
84077/84077 - 9s - loss: 2.3148e-04 - val_loss: 2.1588e-04 - 9s/epoch - 108us/sample
Epoch 33/85
84077/84077 - 9s - loss: 2.2940e-04 - val_loss: 2.1553e-04 - 9s/epoch - 108us/sample
Epoch 34/85
84077/84077 - 9s - loss: 2.2607e-04 - val_loss: 2.1257e-04 - 9s/epoch - 110us/sample
Epoch 35/85
84077/84077 - 9s - loss: 2.2336e-04 - val_loss: 2.1099e-04 - 9s/epoch - 109us/sample
Epoch 36/85
84077/84077 - 9s - loss: 2.2258e-04 - val_loss: 2.0953e-04 - 9s/epoch - 109us/sample
Epoch 37/85
84077/84077 - 9s - loss: 2.2209e-04 - val_loss: 2.1013e-04 - 9s/epoch - 109us/sample
Epoch 38/85
84077/84077 - 9s - loss: 2.1825e-04 - val_loss: 2.0508e-04 - 9s/epoch - 109us/sample
Epoch 39/85
84077/84077 - 9s - loss: 2.1636e-04 - val_loss: 2.0506e-04 - 9s/epoch - 109us/sample
Epoch 40/85
84077/84077 - 9s - loss: 2.1533e-04 - val_loss: 2.0566e-04 - 9s/epoch - 108us/sample
Epoch 41/85
84077/84077 - 9s - loss: 2.1341e-04 - val_loss: 2.0653e-04 - 9s/epoch - 110us/sample
Epoch 42/85
84077/84077 - 9s - loss: 2.1305e-04 - val_loss: 2.0684e-04 - 9s/epoch - 110us/sample
Epoch 43/85
84077/84077 - 9s - loss: 2.1104e-04 - val_loss: 2.0236e-04 - 9s/epoch - 109us/sample
Epoch 44/85
84077/84077 - 9s - loss: 2.1044e-04 - val_loss: 2.0320e-04 - 9s/epoch - 109us/sample
Epoch 45/85
84077/84077 - 9s - loss: 2.0914e-04 - val_loss: 2.0139e-04 - 9s/epoch - 109us/sample
Epoch 46/85
84077/84077 - 9s - loss: 2.0765e-04 - val_loss: 1.9926e-04 - 9s/epoch - 109us/sample
Epoch 47/85
84077/84077 - 9s - loss: 2.1089e-04 - val_loss: 1.9853e-04 - 9s/epoch - 108us/sample
Epoch 48/85
84077/84077 - 9s - loss: 2.0600e-04 - val_loss: 1.9853e-04 - 9s/epoch - 109us/sample
Epoch 49/85
84077/84077 - 9s - loss: 2.0524e-04 - val_loss: 1.9822e-04 - 9s/epoch - 110us/sample
Epoch 50/85
84077/84077 - 9s - loss: 2.0605e-04 - val_loss: 1.9984e-04 - 9s/epoch - 110us/sample
Epoch 51/85
84077/84077 - 9s - loss: 2.0338e-04 - val_loss: 1.9654e-04 - 9s/epoch - 109us/sample
Epoch 52/85
84077/84077 - 9s - loss: 2.0332e-04 - val_loss: 1.9724e-04 - 9s/epoch - 109us/sample
Epoch 53/85
84077/84077 - 9s - loss: 2.0173e-04 - val_loss: 1.9572e-04 - 9s/epoch - 109us/sample
Epoch 54/85
84077/84077 - 9s - loss: 2.0148e-04 - val_loss: 1.9303e-04 - 9s/epoch - 111us/sample
Epoch 55/85
84077/84077 - 10s - loss: 2.0054e-04 - val_loss: 1.9306e-04 - 10s/epoch - 122us/sample
Epoch 56/85
84077/84077 - 10s - loss: 2.0003e-04 - val_loss: 1.9591e-04 - 10s/epoch - 122us/sample
Epoch 57/85
84077/84077 - 10s - loss: 1.9929e-04 - val_loss: 1.9345e-04 - 10s/epoch - 125us/sample
Epoch 58/85
84077/84077 - 10s - loss: 1.9836e-04 - val_loss: 1.9239e-04 - 10s/epoch - 121us/sample
Epoch 59/85
84077/84077 - 10s - loss: 1.9807e-04 - val_loss: 1.9503e-04 - 10s/epoch - 118us/sample
Epoch 60/85
84077/84077 - 10s - loss: 1.9714e-04 - val_loss: 1.9070e-04 - 10s/epoch - 118us/sample
Epoch 61/85
84077/84077 - 10s - loss: 1.9717e-04 - val_loss: 1.9197e-04 - 10s/epoch - 118us/sample
Epoch 62/85
84077/84077 - 10s - loss: 1.9770e-04 - val_loss: 1.9144e-04 - 10s/epoch - 117us/sample
Epoch 63/85
84077/84077 - 10s - loss: 1.9583e-04 - val_loss: 1.9014e-04 - 10s/epoch - 117us/sample
Epoch 64/85
84077/84077 - 10s - loss: 1.9520e-04 - val_loss: 1.8908e-04 - 10s/epoch - 119us/sample
Epoch 65/85
84077/84077 - 10s - loss: 1.9513e-04 - val_loss: 1.8836e-04 - 10s/epoch - 118us/sample
Epoch 66/85
84077/84077 - 10s - loss: 1.9442e-04 - val_loss: 1.9043e-04 - 10s/epoch - 118us/sample
Epoch 67/85
84077/84077 - 10s - loss: 1.9423e-04 - val_loss: 1.8859e-04 - 10s/epoch - 117us/sample
Epoch 68/85
84077/84077 - 10s - loss: 1.9370e-04 - val_loss: 1.9033e-04 - 10s/epoch - 118us/sample
Epoch 69/85
84077/84077 - 10s - loss: 1.9310e-04 - val_loss: 1.8858e-04 - 10s/epoch - 117us/sample
Epoch 70/85
84077/84077 - 10s - loss: 1.9269e-04 - val_loss: 1.8874e-04 - 10s/epoch - 117us/sample
Epoch 71/85
84077/84077 - 10s - loss: 1.9198e-04 - val_loss: 1.8664e-04 - 10s/epoch - 119us/sample
Epoch 72/85
84077/84077 - 10s - loss: 1.9167e-04 - val_loss: 1.8763e-04 - 10s/epoch - 118us/sample
Epoch 73/85
84077/84077 - 10s - loss: 1.9122e-04 - val_loss: 1.8705e-04 - 10s/epoch - 118us/sample
Epoch 74/85
84077/84077 - 10s - loss: 1.9126e-04 - val_loss: 1.8885e-04 - 10s/epoch - 118us/sample
Epoch 75/85
84077/84077 - 10s - loss: 1.9071e-04 - val_loss: 1.8524e-04 - 10s/epoch - 118us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.9005e-04 - val_loss: 1.8729e-04 - 9s/epoch - 110us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.8998e-04 - val_loss: 1.8613e-04 - 9s/epoch - 109us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.8922e-04 - val_loss: 1.8460e-04 - 9s/epoch - 109us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.8865e-04 - val_loss: 1.8395e-04 - 9s/epoch - 110us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.8914e-04 - val_loss: 1.8482e-04 - 9s/epoch - 109us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.8836e-04 - val_loss: 1.8390e-04 - 9s/epoch - 109us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.8903e-04 - val_loss: 1.8555e-04 - 9s/epoch - 109us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.8793e-04 - val_loss: 1.8440e-04 - 9s/epoch - 110us/sample
Epoch 84/85
84077/84077 - 10s - loss: 1.8791e-04 - val_loss: 1.8204e-04 - 10s/epoch - 117us/sample
Epoch 85/85
84077/84077 - 10s - loss: 1.8721e-04 - val_loss: 1.8559e-04 - 10s/epoch - 120us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00018558508131867507
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 00:26:34.952433: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_64/outputlayer/BiasAdd' id:81692 op device:{requested: '', assigned: ''} def:{{{node decoder_model_64/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_64/outputlayer/MatMul, decoder_model_64/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.021743550552863092
cosine 0.021483880324513403
MAE: 0.001940178781954585
RMSE: 0.009081011501220936
r2: 0.9357856368289621
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.2, 188, 0.0001872075851674723, 0.00018558508131867507, 0.021743550552863092, 0.021483880324513403, 0.001940178781954585, 0.009081011501220936, 0.9357856368289621, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_195 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_195 (ReLU)               (None, 1886)         0           ['batch_normalization_195[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_195[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_195[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 00:27:10.456370: W tensorflow/c/c_api.cc:291] Operation '{name:'training_130/Adam/outputlayer_65/bias/m/Assign' id:83566 op device:{requested: '', assigned: ''} def:{{{node training_130/Adam/outputlayer_65/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_130/Adam/outputlayer_65/bias/m, training_130/Adam/outputlayer_65/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 00:27:30.524324: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_65/mul' id:82983 op device:{requested: '', assigned: ''} def:{{{node loss_65/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_65/mul/x, loss_65/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 34s - loss: 0.0154 - val_loss: 9.6039e-04 - 34s/epoch - 400us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0020 - val_loss: 9.0004e-04 - 11s/epoch - 137us/sample
Epoch 3/90
84077/84077 - 11s - loss: 0.0012 - val_loss: 7.1476e-04 - 11s/epoch - 129us/sample
Epoch 4/90
84077/84077 - 11s - loss: 6.7440e-04 - val_loss: 0.0030 - 11s/epoch - 129us/sample
Epoch 5/90
84077/84077 - 11s - loss: 6.3575e-04 - val_loss: 6.3358e-04 - 11s/epoch - 129us/sample
Epoch 6/90
84077/84077 - 11s - loss: 5.5577e-04 - val_loss: 4.3954e-04 - 11s/epoch - 130us/sample
Epoch 7/90
84077/84077 - 11s - loss: 4.5495e-04 - val_loss: 4.1674e-04 - 11s/epoch - 128us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.8746e-04 - val_loss: 3.3972e-04 - 11s/epoch - 131us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.5732e-04 - val_loss: 3.0362e-04 - 11s/epoch - 130us/sample
Epoch 10/90
84077/84077 - 11s - loss: 3.2069e-04 - val_loss: 2.8239e-04 - 11s/epoch - 130us/sample
Epoch 11/90
84077/84077 - 11s - loss: 2.9929e-04 - val_loss: 2.5976e-04 - 11s/epoch - 130us/sample
Epoch 12/90
84077/84077 - 11s - loss: 2.7794e-04 - val_loss: 2.4571e-04 - 11s/epoch - 129us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.5834e-04 - val_loss: 2.2815e-04 - 11s/epoch - 130us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.4628e-04 - val_loss: 2.2417e-04 - 11s/epoch - 130us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.3377e-04 - val_loss: 2.1141e-04 - 11s/epoch - 131us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.2436e-04 - val_loss: 2.0302e-04 - 11s/epoch - 130us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.1306e-04 - val_loss: 1.8992e-04 - 11s/epoch - 129us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.0654e-04 - val_loss: 1.8179e-04 - 11s/epoch - 129us/sample
Epoch 19/90
84077/84077 - 11s - loss: 1.9856e-04 - val_loss: 1.7866e-04 - 11s/epoch - 130us/sample
Epoch 20/90
84077/84077 - 11s - loss: 1.9250e-04 - val_loss: 1.7299e-04 - 11s/epoch - 130us/sample
Epoch 21/90
84077/84077 - 11s - loss: 1.8853e-04 - val_loss: 1.7053e-04 - 11s/epoch - 132us/sample
Epoch 22/90
84077/84077 - 11s - loss: 1.8317e-04 - val_loss: 1.6124e-04 - 11s/epoch - 130us/sample
Epoch 23/90
84077/84077 - 11s - loss: 1.7883e-04 - val_loss: 1.5776e-04 - 11s/epoch - 130us/sample
Epoch 24/90
84077/84077 - 11s - loss: 1.7531e-04 - val_loss: 1.5780e-04 - 11s/epoch - 129us/sample
Epoch 25/90
84077/84077 - 11s - loss: 1.7206e-04 - val_loss: 1.5478e-04 - 11s/epoch - 130us/sample
Epoch 26/90
84077/84077 - 11s - loss: 1.6953e-04 - val_loss: 1.5355e-04 - 11s/epoch - 130us/sample
Epoch 27/90
84077/84077 - 11s - loss: 1.6694e-04 - val_loss: 1.5259e-04 - 11s/epoch - 130us/sample
Epoch 28/90
84077/84077 - 11s - loss: 1.6575e-04 - val_loss: 1.4945e-04 - 11s/epoch - 131us/sample
Epoch 29/90
84077/84077 - 11s - loss: 1.6310e-04 - val_loss: 1.4926e-04 - 11s/epoch - 129us/sample
Epoch 30/90
84077/84077 - 11s - loss: 1.6260e-04 - val_loss: 1.4703e-04 - 11s/epoch - 130us/sample
Epoch 31/90
84077/84077 - 11s - loss: 1.6023e-04 - val_loss: 1.4876e-04 - 11s/epoch - 129us/sample
Epoch 32/90
84077/84077 - 11s - loss: 1.5951e-04 - val_loss: 1.4554e-04 - 11s/epoch - 129us/sample
Epoch 33/90
84077/84077 - 11s - loss: 1.5784e-04 - val_loss: 1.4472e-04 - 11s/epoch - 130us/sample
Epoch 34/90
84077/84077 - 11s - loss: 1.5750e-04 - val_loss: 1.4381e-04 - 11s/epoch - 131us/sample
Epoch 35/90
84077/84077 - 11s - loss: 1.5568e-04 - val_loss: 1.4251e-04 - 11s/epoch - 130us/sample
Epoch 36/90
84077/84077 - 11s - loss: 1.5451e-04 - val_loss: 1.4206e-04 - 11s/epoch - 130us/sample
Epoch 37/90
84077/84077 - 11s - loss: 1.5332e-04 - val_loss: 1.4211e-04 - 11s/epoch - 129us/sample
Epoch 38/90
84077/84077 - 11s - loss: 1.5241e-04 - val_loss: 1.4013e-04 - 11s/epoch - 129us/sample
Epoch 39/90
84077/84077 - 11s - loss: 1.5157e-04 - val_loss: 1.3894e-04 - 11s/epoch - 129us/sample
Epoch 40/90
84077/84077 - 11s - loss: 1.5108e-04 - val_loss: 1.4048e-04 - 11s/epoch - 130us/sample
Epoch 41/90
84077/84077 - 11s - loss: 1.5077e-04 - val_loss: 1.3889e-04 - 11s/epoch - 131us/sample
Epoch 42/90
84077/84077 - 11s - loss: 1.4966e-04 - val_loss: 1.3890e-04 - 11s/epoch - 130us/sample
Epoch 43/90
84077/84077 - 11s - loss: 1.4854e-04 - val_loss: 1.3717e-04 - 11s/epoch - 129us/sample
Epoch 44/90
84077/84077 - 11s - loss: 1.4887e-04 - val_loss: 1.3942e-04 - 11s/epoch - 129us/sample
Epoch 45/90
84077/84077 - 11s - loss: 1.4790e-04 - val_loss: 1.3765e-04 - 11s/epoch - 129us/sample
Epoch 46/90
84077/84077 - 11s - loss: 1.4728e-04 - val_loss: 1.3802e-04 - 11s/epoch - 130us/sample
Epoch 47/90
84077/84077 - 11s - loss: 1.4650e-04 - val_loss: 1.3697e-04 - 11s/epoch - 132us/sample
Epoch 48/90
84077/84077 - 11s - loss: 1.4596e-04 - val_loss: 1.3676e-04 - 11s/epoch - 131us/sample
Epoch 49/90
84077/84077 - 11s - loss: 1.4601e-04 - val_loss: 1.3563e-04 - 11s/epoch - 129us/sample
Epoch 50/90
84077/84077 - 11s - loss: 1.4515e-04 - val_loss: 1.3670e-04 - 11s/epoch - 129us/sample
Epoch 51/90
84077/84077 - 11s - loss: 1.4424e-04 - val_loss: 1.3586e-04 - 11s/epoch - 129us/sample
Epoch 52/90
84077/84077 - 11s - loss: 1.4391e-04 - val_loss: 1.3494e-04 - 11s/epoch - 130us/sample
Epoch 53/90
84077/84077 - 11s - loss: 1.4353e-04 - val_loss: 1.3395e-04 - 11s/epoch - 131us/sample
Epoch 54/90
84077/84077 - 11s - loss: 1.4296e-04 - val_loss: 1.3539e-04 - 11s/epoch - 130us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.4332e-04 - val_loss: 1.3238e-04 - 11s/epoch - 129us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.4275e-04 - val_loss: 1.3414e-04 - 11s/epoch - 130us/sample
Epoch 57/90
84077/84077 - 11s - loss: 1.4219e-04 - val_loss: 1.3414e-04 - 11s/epoch - 129us/sample
Epoch 58/90
84077/84077 - 11s - loss: 1.4185e-04 - val_loss: 1.3283e-04 - 11s/epoch - 130us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.4131e-04 - val_loss: 1.3254e-04 - 11s/epoch - 129us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.4146e-04 - val_loss: 1.3227e-04 - 11s/epoch - 131us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.4194e-04 - val_loss: 1.3277e-04 - 11s/epoch - 129us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.4027e-04 - val_loss: 1.3303e-04 - 11s/epoch - 129us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.4037e-04 - val_loss: 1.3155e-04 - 11s/epoch - 129us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.3992e-04 - val_loss: 1.3130e-04 - 11s/epoch - 130us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.3955e-04 - val_loss: 1.3165e-04 - 11s/epoch - 129us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.3960e-04 - val_loss: 1.3140e-04 - 11s/epoch - 131us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.3872e-04 - val_loss: 1.3094e-04 - 11s/epoch - 130us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.3837e-04 - val_loss: 1.3116e-04 - 11s/epoch - 129us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.3827e-04 - val_loss: 1.3049e-04 - 11s/epoch - 129us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.3850e-04 - val_loss: 1.3002e-04 - 11s/epoch - 129us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.3794e-04 - val_loss: 1.2995e-04 - 11s/epoch - 129us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.3757e-04 - val_loss: 1.2912e-04 - 11s/epoch - 130us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.3759e-04 - val_loss: 1.2909e-04 - 11s/epoch - 131us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.3793e-04 - val_loss: 1.2939e-04 - 11s/epoch - 130us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.3697e-04 - val_loss: 1.3131e-04 - 11s/epoch - 129us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.3713e-04 - val_loss: 1.2927e-04 - 11s/epoch - 129us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.3648e-04 - val_loss: 1.2908e-04 - 11s/epoch - 130us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.3686e-04 - val_loss: 1.2927e-04 - 11s/epoch - 130us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.3617e-04 - val_loss: 1.2906e-04 - 11s/epoch - 131us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.3619e-04 - val_loss: 1.2791e-04 - 11s/epoch - 130us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.3634e-04 - val_loss: 1.2861e-04 - 11s/epoch - 129us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.3586e-04 - val_loss: 1.2902e-04 - 11s/epoch - 129us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.3548e-04 - val_loss: 1.3101e-04 - 11s/epoch - 129us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.3547e-04 - val_loss: 1.2947e-04 - 11s/epoch - 130us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.3552e-04 - val_loss: 1.2903e-04 - 11s/epoch - 130us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.3504e-04 - val_loss: 1.2886e-04 - 11s/epoch - 132us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.3503e-04 - val_loss: 1.2721e-04 - 11s/epoch - 129us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.3485e-04 - val_loss: 1.2764e-04 - 11s/epoch - 130us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.3452e-04 - val_loss: 1.2926e-04 - 11s/epoch - 129us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.3461e-04 - val_loss: 1.2674e-04 - 11s/epoch - 130us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012673991395433862
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 00:43:49.147453: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_65/outputlayer/BiasAdd' id:82947 op device:{requested: '', assigned: ''} def:{{{node decoder_model_65/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_65/outputlayer/MatMul, decoder_model_65/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03055458069678976
cosine 0.030188742935079546
MAE: 0.002182895259590516
RMSE: 0.010175889129502725
r2: 0.9192799524095406
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 90, 0.001, 0.2, 188, 0.00013461296129680888, 0.00012673991395433862, 0.03055458069678976, 0.030188742935079546, 0.002182895259590516, 0.010175889129502725, 0.9192799524095406, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.001 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_198 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_198 (ReLU)               (None, 1980)         0           ['batch_normalization_198[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_198[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_198[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
File geneticVAE_OFM93k_custom_VAE2.1_cr0.2_bs64_ep85_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-12 00:44:24.939027: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_185_1/gamma/Assign' id:84379 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_185_1/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_185_1/gamma, batch_normalization_185_1/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-12 00:44:40.092443: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_184_1/beta/m/Assign' id:84941 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_184_1/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_184_1/beta/m, batch_normalization_184_1/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 00:44:55.044292: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_67/outputlayer/BiasAdd' id:84686 op device:{requested: '', assigned: ''} def:{{{node decoder_model_67/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_67/outputlayer/MatMul, decoder_model_67/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.033309966452762595
cosine 0.03290433535586571
MAE: 0.002491045683860212
RMSE: 0.010481660683630835
r2: 0.9143049988084953
RMSE zero-vector: 0.04004287452915337
No handles with labels found to put in legend.
['2.1custom_VAE', 'logcosh', 64, 85, 0.001, 0.2, 188, '--', '--', 0.033309966452762595, 0.03290433535586571, 0.002491045683860212, 0.010481660683630835, 0.9143049988084953, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6999999999999997 80 0.001 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1603)         1513232     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_201 (Batch  (None, 1603)        6412        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_201 (ReLU)               (None, 1603)         0           ['batch_normalization_201[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          301552      ['re_lu_201[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          301552      ['re_lu_201[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1858235     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,980,983
Trainable params: 3,974,195
Non-trainable params: 6,788
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 00:45:28.579033: W tensorflow/c/c_api.cc:291] Operation '{name:'training_132/Adam/outputlayer_68/bias/v/Assign' id:86401 op device:{requested: '', assigned: ''} def:{{{node training_132/Adam/outputlayer_68/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_132/Adam/outputlayer_68/bias/v, training_132/Adam/outputlayer_68/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 00:45:49.439232: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_68/mul' id:85704 op device:{requested: '', assigned: ''} def:{{{node loss_68/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_68/mul/x, loss_68/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 35s - loss: 0.0041 - val_loss: 8.6958e-04 - 35s/epoch - 412us/sample
Epoch 2/80
84077/84077 - 11s - loss: 8.1428e-04 - val_loss: 0.0014 - 11s/epoch - 135us/sample
Epoch 3/80
84077/84077 - 11s - loss: 0.0015 - val_loss: 6.7366e-04 - 11s/epoch - 132us/sample
Epoch 4/80
84077/84077 - 11s - loss: 6.6285e-04 - val_loss: 5.5456e-04 - 11s/epoch - 132us/sample
Epoch 5/80
84077/84077 - 11s - loss: 5.9591e-04 - val_loss: 4.7429e-04 - 11s/epoch - 132us/sample
Epoch 6/80
84077/84077 - 11s - loss: 5.5694e-04 - val_loss: 4.7949e-04 - 11s/epoch - 132us/sample
Epoch 7/80
84077/84077 - 11s - loss: 4.4836e-04 - val_loss: 4.0186e-04 - 11s/epoch - 132us/sample
Epoch 8/80
84077/84077 - 11s - loss: 3.8814e-04 - val_loss: 3.4997e-04 - 11s/epoch - 134us/sample
Epoch 9/80
84077/84077 - 11s - loss: 3.3932e-04 - val_loss: 2.9271e-04 - 11s/epoch - 132us/sample
Epoch 10/80
84077/84077 - 11s - loss: 3.0902e-04 - val_loss: 2.7037e-04 - 11s/epoch - 131us/sample
Epoch 11/80
84077/84077 - 11s - loss: 2.8588e-04 - val_loss: 2.4599e-04 - 11s/epoch - 131us/sample
Epoch 12/80
84077/84077 - 11s - loss: 2.6239e-04 - val_loss: 2.2869e-04 - 11s/epoch - 132us/sample
Epoch 13/80
84077/84077 - 11s - loss: 2.4682e-04 - val_loss: 2.1563e-04 - 11s/epoch - 131us/sample
Epoch 14/80
84077/84077 - 11s - loss: 2.3201e-04 - val_loss: 1.9943e-04 - 11s/epoch - 135us/sample
Epoch 15/80
84077/84077 - 11s - loss: 2.1751e-04 - val_loss: 1.9016e-04 - 11s/epoch - 132us/sample
Epoch 16/80
84077/84077 - 11s - loss: 2.0747e-04 - val_loss: 1.8691e-04 - 11s/epoch - 132us/sample
Epoch 17/80
84077/84077 - 11s - loss: 1.9827e-04 - val_loss: 1.7492e-04 - 11s/epoch - 133us/sample
Epoch 18/80
84077/84077 - 11s - loss: 1.9119e-04 - val_loss: 1.7042e-04 - 11s/epoch - 132us/sample
Epoch 19/80
84077/84077 - 11s - loss: 1.8592e-04 - val_loss: 1.6469e-04 - 11s/epoch - 132us/sample
Epoch 20/80
84077/84077 - 11s - loss: 1.8115e-04 - val_loss: 1.6199e-04 - 11s/epoch - 133us/sample
Epoch 21/80
84077/84077 - 11s - loss: 1.7642e-04 - val_loss: 1.5817e-04 - 11s/epoch - 132us/sample
Epoch 22/80
84077/84077 - 11s - loss: 1.7367e-04 - val_loss: 1.5526e-04 - 11s/epoch - 132us/sample
Epoch 23/80
84077/84077 - 11s - loss: 1.7023e-04 - val_loss: 1.5315e-04 - 11s/epoch - 132us/sample
Epoch 24/80
84077/84077 - 11s - loss: 1.6790e-04 - val_loss: 1.5283e-04 - 11s/epoch - 132us/sample
Epoch 25/80
84077/84077 - 11s - loss: 1.6554e-04 - val_loss: 1.4976e-04 - 11s/epoch - 132us/sample
Epoch 26/80
84077/84077 - 11s - loss: 1.6306e-04 - val_loss: 1.4741e-04 - 11s/epoch - 133us/sample
Epoch 27/80
84077/84077 - 11s - loss: 1.6111e-04 - val_loss: 1.4621e-04 - 11s/epoch - 133us/sample
Epoch 28/80
84077/84077 - 11s - loss: 1.5910e-04 - val_loss: 1.4634e-04 - 11s/epoch - 132us/sample
Epoch 29/80
84077/84077 - 11s - loss: 1.5787e-04 - val_loss: 1.4722e-04 - 11s/epoch - 133us/sample
Epoch 30/80
84077/84077 - 11s - loss: 1.5724e-04 - val_loss: 1.4297e-04 - 11s/epoch - 132us/sample
Epoch 31/80
84077/84077 - 11s - loss: 1.5591e-04 - val_loss: 1.4288e-04 - 11s/epoch - 132us/sample
Epoch 32/80
84077/84077 - 11s - loss: 1.5451e-04 - val_loss: 1.4286e-04 - 11s/epoch - 133us/sample
Epoch 33/80
84077/84077 - 11s - loss: 1.5363e-04 - val_loss: 1.4290e-04 - 11s/epoch - 134us/sample
Epoch 34/80
84077/84077 - 11s - loss: 1.5240e-04 - val_loss: 1.4086e-04 - 11s/epoch - 132us/sample
Epoch 35/80
84077/84077 - 11s - loss: 1.5109e-04 - val_loss: 1.4096e-04 - 11s/epoch - 132us/sample
Epoch 36/80
84077/84077 - 11s - loss: 1.5091e-04 - val_loss: 1.4049e-04 - 11s/epoch - 132us/sample
Epoch 37/80
84077/84077 - 11s - loss: 1.5011e-04 - val_loss: 1.3985e-04 - 11s/epoch - 132us/sample
Epoch 38/80
84077/84077 - 11s - loss: 1.4929e-04 - val_loss: 1.3841e-04 - 11s/epoch - 132us/sample
Epoch 39/80
84077/84077 - 11s - loss: 1.4858e-04 - val_loss: 1.3971e-04 - 11s/epoch - 134us/sample
Epoch 40/80
84077/84077 - 11s - loss: 1.4762e-04 - val_loss: 1.3844e-04 - 11s/epoch - 132us/sample
Epoch 41/80
84077/84077 - 11s - loss: 1.4759e-04 - val_loss: 1.3828e-04 - 11s/epoch - 132us/sample
Epoch 42/80
84077/84077 - 11s - loss: 1.4665e-04 - val_loss: 1.3686e-04 - 11s/epoch - 132us/sample
Epoch 43/80
84077/84077 - 11s - loss: 1.4614e-04 - val_loss: 1.3689e-04 - 11s/epoch - 132us/sample
Epoch 44/80
84077/84077 - 11s - loss: 1.4554e-04 - val_loss: 1.3553e-04 - 11s/epoch - 132us/sample
Epoch 45/80
84077/84077 - 11s - loss: 1.4489e-04 - val_loss: 1.3699e-04 - 11s/epoch - 134us/sample
Epoch 46/80
84077/84077 - 11s - loss: 1.4475e-04 - val_loss: 1.3495e-04 - 11s/epoch - 132us/sample
Epoch 47/80
84077/84077 - 11s - loss: 1.4428e-04 - val_loss: 1.3559e-04 - 11s/epoch - 132us/sample
Epoch 48/80
84077/84077 - 11s - loss: 1.4366e-04 - val_loss: 1.3542e-04 - 11s/epoch - 132us/sample
Epoch 49/80
84077/84077 - 11s - loss: 1.4352e-04 - val_loss: 1.3438e-04 - 11s/epoch - 132us/sample
Epoch 50/80
84077/84077 - 11s - loss: 1.4305e-04 - val_loss: 1.3506e-04 - 11s/epoch - 132us/sample
Epoch 51/80
84077/84077 - 11s - loss: 1.4235e-04 - val_loss: 1.3438e-04 - 11s/epoch - 134us/sample
Epoch 52/80
84077/84077 - 11s - loss: 1.4181e-04 - val_loss: 1.3483e-04 - 11s/epoch - 132us/sample
Epoch 53/80
84077/84077 - 11s - loss: 1.4211e-04 - val_loss: 1.3361e-04 - 11s/epoch - 131us/sample
Epoch 54/80
84077/84077 - 11s - loss: 1.4155e-04 - val_loss: 1.3433e-04 - 11s/epoch - 132us/sample
Epoch 55/80
84077/84077 - 11s - loss: 1.4099e-04 - val_loss: 1.3406e-04 - 11s/epoch - 133us/sample
Epoch 56/80
84077/84077 - 11s - loss: 1.4086e-04 - val_loss: 1.3298e-04 - 11s/epoch - 132us/sample
Epoch 57/80
84077/84077 - 11s - loss: 1.4048e-04 - val_loss: 1.3367e-04 - 11s/epoch - 135us/sample
Epoch 58/80
84077/84077 - 11s - loss: 1.4023e-04 - val_loss: 1.3266e-04 - 11s/epoch - 133us/sample
Epoch 59/80
84077/84077 - 11s - loss: 1.3976e-04 - val_loss: 1.3323e-04 - 11s/epoch - 132us/sample
Epoch 60/80
84077/84077 - 11s - loss: 1.3966e-04 - val_loss: 1.3379e-04 - 11s/epoch - 132us/sample
Epoch 61/80
84077/84077 - 11s - loss: 1.3915e-04 - val_loss: 1.3144e-04 - 11s/epoch - 127us/sample
Epoch 62/80
84077/84077 - 10s - loss: 1.3920e-04 - val_loss: 1.3331e-04 - 10s/epoch - 124us/sample
Epoch 63/80
84077/84077 - 11s - loss: 1.3854e-04 - val_loss: 1.3154e-04 - 11s/epoch - 128us/sample
Epoch 64/80
84077/84077 - 10s - loss: 1.3820e-04 - val_loss: 1.3134e-04 - 10s/epoch - 122us/sample
Epoch 65/80
84077/84077 - 10s - loss: 1.3814e-04 - val_loss: 1.3144e-04 - 10s/epoch - 121us/sample
Epoch 66/80
84077/84077 - 10s - loss: 1.3781e-04 - val_loss: 1.2997e-04 - 10s/epoch - 122us/sample
Epoch 67/80
84077/84077 - 10s - loss: 1.3699e-04 - val_loss: 1.3190e-04 - 10s/epoch - 121us/sample
Epoch 68/80
84077/84077 - 10s - loss: 1.3726e-04 - val_loss: 1.3240e-04 - 10s/epoch - 122us/sample
Epoch 69/80
84077/84077 - 10s - loss: 1.3680e-04 - val_loss: 1.3085e-04 - 10s/epoch - 121us/sample
Epoch 70/80
84077/84077 - 10s - loss: 1.3673e-04 - val_loss: 1.3204e-04 - 10s/epoch - 124us/sample
Epoch 71/80
84077/84077 - 10s - loss: 1.3652e-04 - val_loss: 1.2969e-04 - 10s/epoch - 122us/sample
Epoch 72/80
84077/84077 - 10s - loss: 1.3617e-04 - val_loss: 1.3037e-04 - 10s/epoch - 122us/sample
Epoch 73/80
84077/84077 - 10s - loss: 1.3608e-04 - val_loss: 1.3056e-04 - 10s/epoch - 121us/sample
Epoch 74/80
84077/84077 - 10s - loss: 1.3591e-04 - val_loss: 1.3141e-04 - 10s/epoch - 122us/sample
Epoch 75/80
84077/84077 - 10s - loss: 1.3571e-04 - val_loss: 1.3049e-04 - 10s/epoch - 122us/sample
Epoch 76/80
84077/84077 - 10s - loss: 1.3565e-04 - val_loss: 1.2901e-04 - 10s/epoch - 121us/sample
Epoch 77/80
84077/84077 - 10s - loss: 1.3542e-04 - val_loss: 1.2898e-04 - 10s/epoch - 124us/sample
Epoch 78/80
84077/84077 - 10s - loss: 1.3519e-04 - val_loss: 1.2858e-04 - 10s/epoch - 122us/sample
Epoch 79/80
84077/84077 - 10s - loss: 1.3463e-04 - val_loss: 1.2920e-04 - 10s/epoch - 123us/sample
Epoch 80/80
84077/84077 - 10s - loss: 1.3459e-04 - val_loss: 1.2932e-04 - 10s/epoch - 121us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012932367164717782
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 01:00:20.069721: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_68/outputlayer/BiasAdd' id:85668 op device:{requested: '', assigned: ''} def:{{{node decoder_model_68/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_68/outputlayer/MatMul, decoder_model_68/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031358628274233157
cosine 0.030983175355100374
MAE: 0.0022125672598064623
RMSE: 0.010455046622744017
r2: 0.9147745891726016
RMSE zero-vector: 0.04004287452915337
['1.6999999999999997custom_VAE', 'logcosh', 64, 80, 0.001, 0.2, 188, 0.00013458591931214052, 0.00012932367164717782, 0.031358628274233157, 0.030983175355100374, 0.0022125672598064623, 0.010455046622744017, 0.9147745891726016, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 8
Fitness    = 515.1509017593391
Last generation's best solutions = [2.0 85 0.0012000000000000001 64 1] with fitness 515.1509017593391.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 85, 0.0012000000000000001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646]
[2.2 85 0.0008 32 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_204 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_204 (ReLU)               (None, 2074)         0           ['batch_normalization_204[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_204[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_204[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 01:00:56.266545: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_69/bias/Assign' id:86866 op device:{requested: '', assigned: ''} def:{{{node outputlayer_69/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_69/bias, outputlayer_69/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 01:01:21.377627: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_69/mul' id:86989 op device:{requested: '', assigned: ''} def:{{{node loss_69/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_69/mul/x, loss_69/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 39s - loss: 0.0032 - val_loss: 0.0015 - 39s/epoch - 466us/sample
Epoch 2/85
84077/84077 - 17s - loss: 0.0012 - val_loss: 5.9945e-04 - 17s/epoch - 206us/sample
Epoch 3/85
84077/84077 - 18s - loss: 6.4897e-04 - val_loss: 5.8606e-04 - 18s/epoch - 212us/sample
Epoch 4/85
84077/84077 - 17s - loss: 6.0347e-04 - val_loss: 5.1363e-04 - 17s/epoch - 202us/sample
Epoch 5/85
84077/84077 - 17s - loss: 4.6343e-04 - val_loss: 4.0551e-04 - 17s/epoch - 202us/sample
Epoch 6/85
84077/84077 - 17s - loss: 4.1832e-04 - val_loss: 3.7325e-04 - 17s/epoch - 202us/sample
Epoch 7/85
84077/84077 - 17s - loss: 3.9410e-04 - val_loss: 3.5909e-04 - 17s/epoch - 202us/sample
Epoch 8/85
84077/84077 - 17s - loss: 3.8262e-04 - val_loss: 3.5052e-04 - 17s/epoch - 204us/sample
Epoch 9/85
84077/84077 - 17s - loss: 3.7308e-04 - val_loss: 3.6903e-04 - 17s/epoch - 202us/sample
Epoch 10/85
84077/84077 - 17s - loss: 3.6641e-04 - val_loss: 3.3996e-04 - 17s/epoch - 202us/sample
Epoch 11/85
84077/84077 - 17s - loss: 3.5789e-04 - val_loss: 3.2967e-04 - 17s/epoch - 201us/sample
Epoch 12/85
84077/84077 - 17s - loss: 3.5271e-04 - val_loss: 3.2772e-04 - 17s/epoch - 204us/sample
Epoch 13/85
84077/84077 - 17s - loss: 3.4911e-04 - val_loss: 3.2325e-04 - 17s/epoch - 202us/sample
Epoch 14/85
84077/84077 - 17s - loss: 3.4343e-04 - val_loss: 3.1835e-04 - 17s/epoch - 202us/sample
Epoch 15/85
84077/84077 - 17s - loss: 3.4021e-04 - val_loss: 3.1472e-04 - 17s/epoch - 202us/sample
Epoch 16/85
84077/84077 - 17s - loss: 3.3707e-04 - val_loss: 3.1438e-04 - 17s/epoch - 202us/sample
Epoch 17/85
84077/84077 - 17s - loss: 3.3455e-04 - val_loss: 3.1584e-04 - 17s/epoch - 205us/sample
Epoch 18/85
84077/84077 - 17s - loss: 3.2926e-04 - val_loss: 3.2239e-04 - 17s/epoch - 202us/sample
Epoch 19/85
84077/84077 - 17s - loss: 3.2575e-04 - val_loss: 3.1648e-04 - 17s/epoch - 202us/sample
Epoch 20/85
84077/84077 - 17s - loss: 3.2316e-04 - val_loss: 3.0777e-04 - 17s/epoch - 201us/sample
Epoch 21/85
84077/84077 - 17s - loss: 3.2062e-04 - val_loss: 3.1839e-04 - 17s/epoch - 204us/sample
Epoch 22/85
84077/84077 - 17s - loss: 3.1932e-04 - val_loss: 3.2544e-04 - 17s/epoch - 201us/sample
Epoch 23/85
84077/84077 - 17s - loss: 3.1801e-04 - val_loss: 3.1415e-04 - 17s/epoch - 201us/sample
Epoch 24/85
84077/84077 - 17s - loss: 3.1612e-04 - val_loss: 3.0885e-04 - 17s/epoch - 201us/sample
Epoch 25/85
84077/84077 - 17s - loss: 3.1467e-04 - val_loss: 3.1599e-04 - 17s/epoch - 204us/sample
Epoch 26/85
84077/84077 - 17s - loss: 3.1292e-04 - val_loss: 2.9961e-04 - 17s/epoch - 203us/sample
Epoch 27/85
84077/84077 - 17s - loss: 3.1172e-04 - val_loss: 3.0626e-04 - 17s/epoch - 201us/sample
Epoch 28/85
84077/84077 - 17s - loss: 3.1018e-04 - val_loss: 3.0338e-04 - 17s/epoch - 201us/sample
Epoch 29/85
84077/84077 - 17s - loss: 3.0941e-04 - val_loss: 3.0390e-04 - 17s/epoch - 202us/sample
Epoch 30/85
84077/84077 - 17s - loss: 3.0719e-04 - val_loss: 2.9546e-04 - 17s/epoch - 205us/sample
Epoch 31/85
84077/84077 - 17s - loss: 3.0634e-04 - val_loss: 2.9684e-04 - 17s/epoch - 202us/sample
Epoch 32/85
84077/84077 - 17s - loss: 3.0501e-04 - val_loss: 2.9361e-04 - 17s/epoch - 202us/sample
Epoch 33/85
84077/84077 - 17s - loss: 3.0291e-04 - val_loss: 2.9565e-04 - 17s/epoch - 202us/sample
Epoch 34/85
84077/84077 - 17s - loss: 3.0210e-04 - val_loss: 2.8975e-04 - 17s/epoch - 205us/sample
Epoch 35/85
84077/84077 - 17s - loss: 3.0086e-04 - val_loss: 2.8583e-04 - 17s/epoch - 203us/sample
Epoch 36/85
84077/84077 - 17s - loss: 2.9985e-04 - val_loss: 2.8010e-04 - 17s/epoch - 201us/sample
Epoch 37/85
84077/84077 - 17s - loss: 2.9889e-04 - val_loss: 2.7949e-04 - 17s/epoch - 202us/sample
Epoch 38/85
84077/84077 - 17s - loss: 2.9791e-04 - val_loss: 2.8389e-04 - 17s/epoch - 202us/sample
Epoch 39/85
84077/84077 - 17s - loss: 2.9762e-04 - val_loss: 2.8227e-04 - 17s/epoch - 206us/sample
Epoch 40/85
84077/84077 - 17s - loss: 2.9669e-04 - val_loss: 2.8198e-04 - 17s/epoch - 202us/sample
Epoch 41/85
84077/84077 - 17s - loss: 2.9558e-04 - val_loss: 2.7868e-04 - 17s/epoch - 202us/sample
Epoch 42/85
84077/84077 - 17s - loss: 2.9467e-04 - val_loss: 2.7641e-04 - 17s/epoch - 202us/sample
Epoch 43/85
84077/84077 - 17s - loss: 2.9413e-04 - val_loss: 2.7552e-04 - 17s/epoch - 203us/sample
Epoch 44/85
84077/84077 - 17s - loss: 2.9362e-04 - val_loss: 2.7439e-04 - 17s/epoch - 200us/sample
Epoch 45/85
84077/84077 - 17s - loss: 2.9326e-04 - val_loss: 2.7367e-04 - 17s/epoch - 201us/sample
Epoch 46/85
84077/84077 - 17s - loss: 2.9180e-04 - val_loss: 2.7989e-04 - 17s/epoch - 201us/sample
Epoch 47/85
84077/84077 - 17s - loss: 2.9146e-04 - val_loss: 2.7186e-04 - 17s/epoch - 200us/sample
Epoch 48/85
84077/84077 - 17s - loss: 2.9027e-04 - val_loss: 2.6953e-04 - 17s/epoch - 203us/sample
Epoch 49/85
84077/84077 - 17s - loss: 2.8910e-04 - val_loss: 2.7126e-04 - 17s/epoch - 201us/sample
Epoch 50/85
84077/84077 - 17s - loss: 2.8770e-04 - val_loss: 2.7114e-04 - 17s/epoch - 201us/sample
Epoch 51/85
84077/84077 - 17s - loss: 2.8697e-04 - val_loss: 2.6702e-04 - 17s/epoch - 201us/sample
Epoch 52/85
84077/84077 - 17s - loss: 2.8635e-04 - val_loss: 2.6750e-04 - 17s/epoch - 202us/sample
Epoch 53/85
84077/84077 - 17s - loss: 2.8607e-04 - val_loss: 2.6667e-04 - 17s/epoch - 201us/sample
Epoch 54/85
84077/84077 - 17s - loss: 2.8509e-04 - val_loss: 2.6741e-04 - 17s/epoch - 200us/sample
Epoch 55/85
84077/84077 - 17s - loss: 2.8479e-04 - val_loss: 2.6389e-04 - 17s/epoch - 201us/sample
Epoch 56/85
84077/84077 - 17s - loss: 2.8448e-04 - val_loss: 2.6458e-04 - 17s/epoch - 201us/sample
Epoch 57/85
84077/84077 - 17s - loss: 2.8354e-04 - val_loss: 2.6455e-04 - 17s/epoch - 203us/sample
Epoch 58/85
84077/84077 - 17s - loss: 2.8293e-04 - val_loss: 2.6495e-04 - 17s/epoch - 199us/sample
Epoch 59/85
84077/84077 - 17s - loss: 2.8330e-04 - val_loss: 2.6255e-04 - 17s/epoch - 201us/sample
Epoch 60/85
84077/84077 - 17s - loss: 2.8249e-04 - val_loss: 2.6236e-04 - 17s/epoch - 201us/sample
Epoch 61/85
84077/84077 - 17s - loss: 2.8162e-04 - val_loss: 2.6216e-04 - 17s/epoch - 203us/sample
Epoch 62/85
84077/84077 - 17s - loss: 2.8183e-04 - val_loss: 2.6210e-04 - 17s/epoch - 201us/sample
Epoch 63/85
84077/84077 - 17s - loss: 2.8169e-04 - val_loss: 2.6270e-04 - 17s/epoch - 200us/sample
Epoch 64/85
84077/84077 - 17s - loss: 2.8164e-04 - val_loss: 2.6053e-04 - 17s/epoch - 201us/sample
Epoch 65/85
84077/84077 - 17s - loss: 2.8095e-04 - val_loss: 2.6045e-04 - 17s/epoch - 200us/sample
Epoch 66/85
84077/84077 - 17s - loss: 2.8084e-04 - val_loss: 2.6171e-04 - 17s/epoch - 204us/sample
Epoch 67/85
84077/84077 - 17s - loss: 2.8060e-04 - val_loss: 2.5999e-04 - 17s/epoch - 200us/sample
Epoch 68/85
84077/84077 - 17s - loss: 2.8008e-04 - val_loss: 2.5905e-04 - 17s/epoch - 200us/sample
Epoch 69/85
84077/84077 - 17s - loss: 2.7920e-04 - val_loss: 2.5896e-04 - 17s/epoch - 201us/sample
Epoch 70/85
84077/84077 - 17s - loss: 2.7921e-04 - val_loss: 2.5887e-04 - 17s/epoch - 204us/sample
Epoch 71/85
84077/84077 - 17s - loss: 2.7898e-04 - val_loss: 2.5736e-04 - 17s/epoch - 201us/sample
Epoch 72/85
84077/84077 - 17s - loss: 2.7856e-04 - val_loss: 2.5691e-04 - 17s/epoch - 200us/sample
Epoch 73/85
84077/84077 - 17s - loss: 2.7829e-04 - val_loss: 2.5663e-04 - 17s/epoch - 201us/sample
Epoch 74/85
84077/84077 - 17s - loss: 2.7851e-04 - val_loss: 2.5683e-04 - 17s/epoch - 201us/sample
Epoch 75/85
84077/84077 - 17s - loss: 2.7776e-04 - val_loss: 2.5696e-04 - 17s/epoch - 204us/sample
Epoch 76/85
84077/84077 - 17s - loss: 2.7773e-04 - val_loss: 2.5837e-04 - 17s/epoch - 200us/sample
Epoch 77/85
84077/84077 - 17s - loss: 2.7701e-04 - val_loss: 2.5619e-04 - 17s/epoch - 200us/sample
Epoch 78/85
84077/84077 - 17s - loss: 2.7744e-04 - val_loss: 2.5739e-04 - 17s/epoch - 200us/sample
Epoch 79/85
84077/84077 - 17s - loss: 2.7731e-04 - val_loss: 2.5714e-04 - 17s/epoch - 203us/sample
Epoch 80/85
84077/84077 - 17s - loss: 2.7702e-04 - val_loss: 2.5730e-04 - 17s/epoch - 202us/sample
Epoch 81/85
84077/84077 - 17s - loss: 2.7634e-04 - val_loss: 2.5568e-04 - 17s/epoch - 201us/sample
Epoch 82/85
84077/84077 - 17s - loss: 2.7612e-04 - val_loss: 2.5533e-04 - 17s/epoch - 200us/sample
Epoch 83/85
84077/84077 - 17s - loss: 2.7538e-04 - val_loss: 2.5551e-04 - 17s/epoch - 200us/sample
Epoch 84/85
84077/84077 - 17s - loss: 2.7506e-04 - val_loss: 2.5574e-04 - 17s/epoch - 203us/sample
Epoch 85/85
84077/84077 - 17s - loss: 2.7428e-04 - val_loss: 2.5413e-04 - 17s/epoch - 201us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00025413424175195167
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 01:25:15.349291: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_69/outputlayer/BiasAdd' id:86953 op device:{requested: '', assigned: ''} def:{{{node decoder_model_69/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_69/outputlayer/MatMul, decoder_model_69/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.11717604333285468
cosine 0.11560122945042316
MAE: 0.0037464024699394856
RMSE: 0.02023682767232679
r2: 0.6800828213428214
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 32, 85, 0.0008, 0.2, 188, 0.0002742847571834355, 0.00025413424175195167, 0.11717604333285468, 0.11560122945042316, 0.0037464024699394856, 0.02023682767232679, 0.6800828213428214, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0006000000000000001 64 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_207 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_207 (ReLU)               (None, 2074)         0           ['batch_normalization_207[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_207[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_207[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 01:25:52.575888: W tensorflow/c/c_api.cc:291] Operation '{name:'training_136/Adam/dense_dec0_70/bias/v/Assign' id:88997 op device:{requested: '', assigned: ''} def:{{{node training_136/Adam/dense_dec0_70/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_136/Adam/dense_dec0_70/bias/v, training_136/Adam/dense_dec0_70/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 01:26:12.808960: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_70/mul' id:88286 op device:{requested: '', assigned: ''} def:{{{node loss_70/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_70/mul/x, loss_70/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 34s - loss: 0.8752 - val_loss: 0.0701 - 34s/epoch - 409us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0685 - val_loss: 0.0687 - 12s/epoch - 141us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0687 - val_loss: 101.3636 - 12s/epoch - 140us/sample
Epoch 4/85
84077/84077 - 12s - loss: 0.0691 - val_loss: 0.1554 - 12s/epoch - 143us/sample
Epoch 5/85
84077/84077 - 12s - loss: 0.0672 - val_loss: 0.0676 - 12s/epoch - 142us/sample
Epoch 6/85
84077/84077 - 12s - loss: 0.0670 - val_loss: 0.0675 - 12s/epoch - 141us/sample
Epoch 7/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0675 - 12s/epoch - 140us/sample
Epoch 8/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 9/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 140us/sample
Epoch 10/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 144us/sample
Epoch 11/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 142us/sample
Epoch 12/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 13/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 14/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0674 - 12s/epoch - 140us/sample
Epoch 15/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 16/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 17/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 18/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 19/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 20/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 21/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 22/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 145us/sample
Epoch 23/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 24/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 25/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 26/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 27/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 28/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 29/85
84077/84077 - 12s - loss: 0.0674 - val_loss: 0.0674 - 12s/epoch - 142us/sample
Epoch 30/85
84077/84077 - 12s - loss: 0.0697 - val_loss: 0.0719 - 12s/epoch - 140us/sample
Epoch 31/85
84077/84077 - 12s - loss: 0.0698 - val_loss: 0.0719 - 12s/epoch - 141us/sample
Epoch 32/85
84077/84077 - 12s - loss: 0.0682 - val_loss: 0.0678 - 12s/epoch - 140us/sample
Epoch 33/85
84077/84077 - 12s - loss: 0.0672 - val_loss: 0.0676 - 12s/epoch - 143us/sample
Epoch 34/85
84077/84077 - 12s - loss: 0.0671 - val_loss: 0.0674 - 12s/epoch - 143us/sample
Epoch 35/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 36/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 37/85
84077/84077 - 12s - loss: 0.0671 - val_loss: 0.0675 - 12s/epoch - 140us/sample
Epoch 38/85
84077/84077 - 12s - loss: 0.0675 - val_loss: 0.0679 - 12s/epoch - 141us/sample
Epoch 39/85
84077/84077 - 12s - loss: 0.0676 - val_loss: 0.0677 - 12s/epoch - 143us/sample
Epoch 40/85
84077/84077 - 12s - loss: 0.0672 - val_loss: 0.0675 - 12s/epoch - 143us/sample
Epoch 41/85
84077/84077 - 12s - loss: 0.0670 - val_loss: 0.0674 - 12s/epoch - 140us/sample
Epoch 42/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 43/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 44/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 140us/sample
Epoch 45/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 145us/sample
Epoch 46/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 47/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0674 - 12s/epoch - 142us/sample
Epoch 48/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 49/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 50/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 51/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 52/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 53/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 54/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 55/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 56/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 57/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 145us/sample
Epoch 58/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 59/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 60/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 61/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 62/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 148us/sample
Epoch 63/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 145us/sample
Epoch 64/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 65/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 66/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 67/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 68/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 69/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 70/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 71/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 72/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 73/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 74/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 143us/sample
Epoch 75/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 76/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 77/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 78/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 79/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 80/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 81/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 82/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 83/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 84/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 85/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734574236832296
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 01:43:00.003839: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_70/outputlayer/BiasAdd' id:88238 op device:{requested: '', assigned: ''} def:{{{node decoder_model_70/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_70/outputlayer/MatMul, decoder_model_70/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.3151976937703393
cosine 1.2577328628386442
MAE: 6.45846788349368
RMSE: 8.24900520061393
r2: -50824.703555104104
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'binary_crossentropy', 64, 85, 0.0006000000000000001, 0.2, 188, 0.0668369294413994, 0.06734574236832296, 1.3151976937703393, 1.2577328628386442, 6.45846788349368, 8.24900520061393, -50824.703555104104, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0008 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_210 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_210 (ReLU)               (None, 1980)         0           ['batch_normalization_210[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_210[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_210[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 01:43:37.278393: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_enc0_71/kernel/Assign' id:89171 op device:{requested: '', assigned: ''} def:{{{node dense_enc0_71/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_enc0_71/kernel, dense_enc0_71/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 01:43:59.184122: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_71/mul' id:89611 op device:{requested: '', assigned: ''} def:{{{node loss_71/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_71/mul/x, loss_71/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 37s - loss: 0.0039 - val_loss: 0.0011 - 37s/epoch - 437us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.0012 - val_loss: 7.3137e-04 - 13s/epoch - 158us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0014 - val_loss: 0.6380 - 13s/epoch - 156us/sample
Epoch 4/85
84077/84077 - 13s - loss: 8.1997e-04 - val_loss: 6.7238e-04 - 13s/epoch - 155us/sample
Epoch 5/85
84077/84077 - 13s - loss: 6.8335e-04 - val_loss: 7.6961e-04 - 13s/epoch - 155us/sample
Epoch 6/85
84077/84077 - 13s - loss: 5.3702e-04 - val_loss: 4.9792e-04 - 13s/epoch - 156us/sample
Epoch 7/85
84077/84077 - 13s - loss: 4.5786e-04 - val_loss: 4.3972e-04 - 13s/epoch - 158us/sample
Epoch 8/85
84077/84077 - 13s - loss: 4.0471e-04 - val_loss: 3.5587e-04 - 13s/epoch - 155us/sample
Epoch 9/85
84077/84077 - 13s - loss: 3.5349e-04 - val_loss: 3.0573e-04 - 13s/epoch - 156us/sample
Epoch 10/85
84077/84077 - 13s - loss: 3.2099e-04 - val_loss: 2.7861e-04 - 13s/epoch - 155us/sample
Epoch 11/85
84077/84077 - 13s - loss: 2.9611e-04 - val_loss: 2.5135e-04 - 13s/epoch - 155us/sample
Epoch 12/85
84077/84077 - 14s - loss: 2.7498e-04 - val_loss: 3.7143e-04 - 14s/epoch - 161us/sample
Epoch 13/85
84077/84077 - 13s - loss: 2.6537e-04 - val_loss: 2.2601e-04 - 13s/epoch - 155us/sample
Epoch 14/85
84077/84077 - 12s - loss: 2.5018e-04 - val_loss: 2.1995e-04 - 12s/epoch - 143us/sample
Epoch 15/85
84077/84077 - 11s - loss: 2.3497e-04 - val_loss: 2.1246e-04 - 11s/epoch - 135us/sample
Epoch 16/85
84077/84077 - 11s - loss: 2.2491e-04 - val_loss: 1.9544e-04 - 11s/epoch - 136us/sample
Epoch 17/85
84077/84077 - 12s - loss: 2.1469e-04 - val_loss: 1.8524e-04 - 12s/epoch - 140us/sample
Epoch 18/85
84077/84077 - 11s - loss: 2.0964e-04 - val_loss: 1.8062e-04 - 11s/epoch - 136us/sample
Epoch 19/85
84077/84077 - 11s - loss: 2.0079e-04 - val_loss: 1.7772e-04 - 11s/epoch - 136us/sample
Epoch 20/85
84077/84077 - 11s - loss: 1.9404e-04 - val_loss: 1.7665e-04 - 11s/epoch - 136us/sample
Epoch 21/85
84077/84077 - 11s - loss: 1.9073e-04 - val_loss: 1.6748e-04 - 11s/epoch - 136us/sample
Epoch 22/85
84077/84077 - 11s - loss: 1.8571e-04 - val_loss: 1.6895e-04 - 11s/epoch - 136us/sample
Epoch 23/85
84077/84077 - 12s - loss: 1.8295e-04 - val_loss: 1.6257e-04 - 12s/epoch - 139us/sample
Epoch 24/85
84077/84077 - 11s - loss: 1.7982e-04 - val_loss: 1.6521e-04 - 11s/epoch - 136us/sample
Epoch 25/85
84077/84077 - 11s - loss: 1.7700e-04 - val_loss: 1.5884e-04 - 11s/epoch - 135us/sample
Epoch 26/85
84077/84077 - 11s - loss: 1.7373e-04 - val_loss: 1.5793e-04 - 11s/epoch - 136us/sample
Epoch 27/85
84077/84077 - 11s - loss: 1.7181e-04 - val_loss: 1.5637e-04 - 11s/epoch - 136us/sample
Epoch 28/85
84077/84077 - 11s - loss: 1.6967e-04 - val_loss: 1.5323e-04 - 11s/epoch - 136us/sample
Epoch 29/85
84077/84077 - 12s - loss: 1.6767e-04 - val_loss: 1.5298e-04 - 12s/epoch - 139us/sample
Epoch 30/85
84077/84077 - 11s - loss: 1.6585e-04 - val_loss: 1.4925e-04 - 11s/epoch - 136us/sample
Epoch 31/85
84077/84077 - 11s - loss: 1.6383e-04 - val_loss: 1.4906e-04 - 11s/epoch - 136us/sample
Epoch 32/85
84077/84077 - 11s - loss: 1.6197e-04 - val_loss: 1.4726e-04 - 11s/epoch - 136us/sample
Epoch 33/85
84077/84077 - 11s - loss: 1.6073e-04 - val_loss: 1.4531e-04 - 11s/epoch - 135us/sample
Epoch 34/85
84077/84077 - 11s - loss: 1.5986e-04 - val_loss: 1.4528e-04 - 11s/epoch - 136us/sample
Epoch 35/85
84077/84077 - 12s - loss: 1.5775e-04 - val_loss: 1.4534e-04 - 12s/epoch - 139us/sample
Epoch 36/85
84077/84077 - 11s - loss: 1.5729e-04 - val_loss: 1.4430e-04 - 11s/epoch - 136us/sample
Epoch 37/85
84077/84077 - 11s - loss: 1.5656e-04 - val_loss: 1.4425e-04 - 11s/epoch - 136us/sample
Epoch 38/85
84077/84077 - 11s - loss: 1.5527e-04 - val_loss: 1.4235e-04 - 11s/epoch - 136us/sample
Epoch 39/85
84077/84077 - 11s - loss: 1.5475e-04 - val_loss: 1.4238e-04 - 11s/epoch - 135us/sample
Epoch 40/85
84077/84077 - 11s - loss: 1.5340e-04 - val_loss: 1.4247e-04 - 11s/epoch - 136us/sample
Epoch 41/85
84077/84077 - 12s - loss: 1.5303e-04 - val_loss: 1.3933e-04 - 12s/epoch - 139us/sample
Epoch 42/85
84077/84077 - 11s - loss: 1.5235e-04 - val_loss: 1.4030e-04 - 11s/epoch - 136us/sample
Epoch 43/85
84077/84077 - 11s - loss: 1.5149e-04 - val_loss: 1.4048e-04 - 11s/epoch - 135us/sample
Epoch 44/85
84077/84077 - 11s - loss: 1.5063e-04 - val_loss: 1.3802e-04 - 11s/epoch - 136us/sample
Epoch 45/85
84077/84077 - 11s - loss: 1.5019e-04 - val_loss: 1.3913e-04 - 11s/epoch - 136us/sample
Epoch 46/85
84077/84077 - 11s - loss: 1.5007e-04 - val_loss: 1.3793e-04 - 11s/epoch - 135us/sample
Epoch 47/85
84077/84077 - 12s - loss: 1.4891e-04 - val_loss: 1.3525e-04 - 12s/epoch - 139us/sample
Epoch 48/85
84077/84077 - 11s - loss: 1.4888e-04 - val_loss: 1.3715e-04 - 11s/epoch - 136us/sample
Epoch 49/85
84077/84077 - 11s - loss: 1.4796e-04 - val_loss: 1.3699e-04 - 11s/epoch - 135us/sample
Epoch 50/85
84077/84077 - 11s - loss: 1.4713e-04 - val_loss: 1.3726e-04 - 11s/epoch - 135us/sample
Epoch 51/85
84077/84077 - 11s - loss: 1.4690e-04 - val_loss: 1.3902e-04 - 11s/epoch - 135us/sample
Epoch 52/85
84077/84077 - 11s - loss: 1.4661e-04 - val_loss: 1.3810e-04 - 11s/epoch - 135us/sample
Epoch 53/85
84077/84077 - 12s - loss: 1.4636e-04 - val_loss: 1.3575e-04 - 12s/epoch - 139us/sample
Epoch 54/85
84077/84077 - 11s - loss: 1.4596e-04 - val_loss: 1.3506e-04 - 11s/epoch - 136us/sample
Epoch 55/85
84077/84077 - 11s - loss: 1.4544e-04 - val_loss: 1.3609e-04 - 11s/epoch - 136us/sample
Epoch 56/85
84077/84077 - 11s - loss: 1.4487e-04 - val_loss: 1.3453e-04 - 11s/epoch - 136us/sample
Epoch 57/85
84077/84077 - 11s - loss: 1.4412e-04 - val_loss: 1.3338e-04 - 11s/epoch - 135us/sample
Epoch 58/85
84077/84077 - 11s - loss: 1.4456e-04 - val_loss: 1.3460e-04 - 11s/epoch - 136us/sample
Epoch 59/85
84077/84077 - 12s - loss: 1.4414e-04 - val_loss: 1.3327e-04 - 12s/epoch - 140us/sample
Epoch 60/85
84077/84077 - 11s - loss: 1.4312e-04 - val_loss: 1.3469e-04 - 11s/epoch - 135us/sample
Epoch 61/85
84077/84077 - 11s - loss: 1.4313e-04 - val_loss: 1.3502e-04 - 11s/epoch - 135us/sample
Epoch 62/85
84077/84077 - 11s - loss: 1.4285e-04 - val_loss: 1.3458e-04 - 11s/epoch - 135us/sample
Epoch 63/85
84077/84077 - 11s - loss: 1.4231e-04 - val_loss: 1.3263e-04 - 11s/epoch - 136us/sample
Epoch 64/85
84077/84077 - 11s - loss: 1.4219e-04 - val_loss: 1.3226e-04 - 11s/epoch - 135us/sample
Epoch 65/85
84077/84077 - 12s - loss: 1.4210e-04 - val_loss: 1.3325e-04 - 12s/epoch - 140us/sample
Epoch 66/85
84077/84077 - 11s - loss: 1.4118e-04 - val_loss: 1.3303e-04 - 11s/epoch - 135us/sample
Epoch 67/85
84077/84077 - 11s - loss: 1.4147e-04 - val_loss: 1.3229e-04 - 11s/epoch - 136us/sample
Epoch 68/85
84077/84077 - 11s - loss: 1.4122e-04 - val_loss: 1.3238e-04 - 11s/epoch - 135us/sample
Epoch 69/85
84077/84077 - 11s - loss: 1.4076e-04 - val_loss: 1.3221e-04 - 11s/epoch - 135us/sample
Epoch 70/85
84077/84077 - 11s - loss: 1.4059e-04 - val_loss: 1.3125e-04 - 11s/epoch - 136us/sample
Epoch 71/85
84077/84077 - 12s - loss: 1.4003e-04 - val_loss: 1.3186e-04 - 12s/epoch - 140us/sample
Epoch 72/85
84077/84077 - 11s - loss: 1.4020e-04 - val_loss: 1.3201e-04 - 11s/epoch - 135us/sample
Epoch 73/85
84077/84077 - 11s - loss: 1.3978e-04 - val_loss: 1.3209e-04 - 11s/epoch - 136us/sample
Epoch 74/85
84077/84077 - 11s - loss: 1.3955e-04 - val_loss: 1.3088e-04 - 11s/epoch - 136us/sample
Epoch 75/85
84077/84077 - 11s - loss: 1.3996e-04 - val_loss: 1.3129e-04 - 11s/epoch - 135us/sample
Epoch 76/85
84077/84077 - 11s - loss: 1.3908e-04 - val_loss: 1.3136e-04 - 11s/epoch - 136us/sample
Epoch 77/85
84077/84077 - 12s - loss: 1.3891e-04 - val_loss: 1.3160e-04 - 12s/epoch - 139us/sample
Epoch 78/85
84077/84077 - 11s - loss: 1.3890e-04 - val_loss: 1.3097e-04 - 11s/epoch - 135us/sample
Epoch 79/85
84077/84077 - 11s - loss: 1.3871e-04 - val_loss: 1.3184e-04 - 11s/epoch - 135us/sample
Epoch 80/85
84077/84077 - 11s - loss: 1.3794e-04 - val_loss: 1.2919e-04 - 11s/epoch - 136us/sample
Epoch 81/85
84077/84077 - 11s - loss: 1.3810e-04 - val_loss: 1.3027e-04 - 11s/epoch - 135us/sample
Epoch 82/85
84077/84077 - 11s - loss: 1.3781e-04 - val_loss: 1.3005e-04 - 11s/epoch - 135us/sample
Epoch 83/85
84077/84077 - 12s - loss: 1.3788e-04 - val_loss: 1.3030e-04 - 12s/epoch - 140us/sample
Epoch 84/85
84077/84077 - 11s - loss: 1.3733e-04 - val_loss: 1.2960e-04 - 11s/epoch - 136us/sample
Epoch 85/85
84077/84077 - 11s - loss: 1.3733e-04 - val_loss: 1.3008e-04 - 11s/epoch - 136us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00013007926253638894
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 02:00:30.203982: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_71/outputlayer/BiasAdd' id:89575 op device:{requested: '', assigned: ''} def:{{{node decoder_model_71/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_71/outputlayer/MatMul, decoder_model_71/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03223480615228583
cosine 0.03184002745461205
MAE: 0.0023731457182589654
RMSE: 0.010382131309696313
r2: 0.9159918937858891
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 85, 0.0008, 0.2, 188, 0.00013733217373389738, 0.00013007926253638894, 0.03223480615228583, 0.03184002745461205, 0.0023731457182589654, 0.010382131309696313, 0.9159918937858891, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.0012000000000000001 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_213 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_213 (ReLU)               (None, 1886)         0           ['batch_normalization_213[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          354756      ['re_lu_213[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          354756      ['re_lu_213[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2179723     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,677,163
Trainable params: 4,669,243
Non-trainable params: 7,920
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 02:01:08.859232: W tensorflow/c/c_api.cc:291] Operation '{name:'training_140/Adam/dense_dec0_72/bias/v/Assign' id:91567 op device:{requested: '', assigned: ''} def:{{{node training_140/Adam/dense_dec0_72/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_140/Adam/dense_dec0_72/bias/v, training_140/Adam/dense_dec0_72/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 02:01:29.198333: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_72/mul' id:90896 op device:{requested: '', assigned: ''} def:{{{node loss_72/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_72/mul/x, loss_72/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 36s - loss: 0.1752 - val_loss: 9.7037e-04 - 36s/epoch - 424us/sample
Epoch 2/90
84077/84077 - 11s - loss: 9.8630e-04 - val_loss: 0.0014 - 11s/epoch - 134us/sample
Epoch 3/90
84077/84077 - 11s - loss: 8.7778e-04 - val_loss: 0.0010 - 11s/epoch - 135us/sample
Epoch 4/90
84077/84077 - 12s - loss: 8.4231e-04 - val_loss: 8.1027e-04 - 12s/epoch - 138us/sample
Epoch 5/90
84077/84077 - 11s - loss: 6.2975e-04 - val_loss: 6.7431e-04 - 11s/epoch - 135us/sample
Epoch 6/90
84077/84077 - 11s - loss: 4.7804e-04 - val_loss: 3.8959e-04 - 11s/epoch - 135us/sample
Epoch 7/90
84077/84077 - 11s - loss: 4.2185e-04 - val_loss: 3.9528e-04 - 11s/epoch - 135us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.8081e-04 - val_loss: 3.3044e-04 - 11s/epoch - 135us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.5604e-04 - val_loss: 3.0328e-04 - 11s/epoch - 135us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.2326e-04 - val_loss: 2.8316e-04 - 12s/epoch - 138us/sample
Epoch 11/90
84077/84077 - 11s - loss: 2.9673e-04 - val_loss: 2.5301e-04 - 11s/epoch - 136us/sample
Epoch 12/90
84077/84077 - 11s - loss: 2.7525e-04 - val_loss: 2.4021e-04 - 11s/epoch - 135us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.6021e-04 - val_loss: 2.2879e-04 - 11s/epoch - 135us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.4556e-04 - val_loss: 2.2114e-04 - 11s/epoch - 135us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.3601e-04 - val_loss: 2.1033e-04 - 11s/epoch - 135us/sample
Epoch 16/90
84077/84077 - 12s - loss: 2.2646e-04 - val_loss: 2.0361e-04 - 12s/epoch - 139us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.1866e-04 - val_loss: 1.9463e-04 - 11s/epoch - 135us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.1196e-04 - val_loss: 1.8536e-04 - 11s/epoch - 135us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.0514e-04 - val_loss: 1.8421e-04 - 11s/epoch - 135us/sample
Epoch 20/90
84077/84077 - 11s - loss: 1.9862e-04 - val_loss: 1.7342e-04 - 11s/epoch - 135us/sample
Epoch 21/90
84077/84077 - 11s - loss: 1.9286e-04 - val_loss: 1.7244e-04 - 11s/epoch - 134us/sample
Epoch 22/90
84077/84077 - 12s - loss: 1.8853e-04 - val_loss: 1.6791e-04 - 12s/epoch - 138us/sample
Epoch 23/90
84077/84077 - 11s - loss: 1.8285e-04 - val_loss: 1.6083e-04 - 11s/epoch - 135us/sample
Epoch 24/90
84077/84077 - 11s - loss: 1.7858e-04 - val_loss: 1.5769e-04 - 11s/epoch - 135us/sample
Epoch 25/90
84077/84077 - 11s - loss: 1.7592e-04 - val_loss: 1.5699e-04 - 11s/epoch - 135us/sample
Epoch 26/90
84077/84077 - 11s - loss: 1.7242e-04 - val_loss: 1.5411e-04 - 11s/epoch - 135us/sample
Epoch 27/90
84077/84077 - 11s - loss: 1.6994e-04 - val_loss: 1.5236e-04 - 11s/epoch - 135us/sample
Epoch 28/90
84077/84077 - 12s - loss: 1.6855e-04 - val_loss: 1.5062e-04 - 12s/epoch - 138us/sample
Epoch 29/90
84077/84077 - 11s - loss: 1.7023e-04 - val_loss: 1.5110e-04 - 11s/epoch - 135us/sample
Epoch 30/90
84077/84077 - 11s - loss: 1.6568e-04 - val_loss: 1.4897e-04 - 11s/epoch - 135us/sample
Epoch 31/90
84077/84077 - 11s - loss: 1.6377e-04 - val_loss: 1.4782e-04 - 11s/epoch - 135us/sample
Epoch 32/90
84077/84077 - 11s - loss: 1.6246e-04 - val_loss: 1.4557e-04 - 11s/epoch - 135us/sample
Epoch 33/90
84077/84077 - 11s - loss: 1.6091e-04 - val_loss: 1.4533e-04 - 11s/epoch - 135us/sample
Epoch 34/90
84077/84077 - 12s - loss: 1.5952e-04 - val_loss: 1.4389e-04 - 12s/epoch - 138us/sample
Epoch 35/90
84077/84077 - 11s - loss: 1.5805e-04 - val_loss: 1.4468e-04 - 11s/epoch - 135us/sample
Epoch 36/90
84077/84077 - 11s - loss: 1.5958e-04 - val_loss: 1.4303e-04 - 11s/epoch - 135us/sample
Epoch 37/90
84077/84077 - 11s - loss: 1.5733e-04 - val_loss: 1.4206e-04 - 11s/epoch - 135us/sample
Epoch 38/90
84077/84077 - 11s - loss: 1.5538e-04 - val_loss: 1.4156e-04 - 11s/epoch - 135us/sample
Epoch 39/90
84077/84077 - 11s - loss: 1.5473e-04 - val_loss: 1.3930e-04 - 11s/epoch - 135us/sample
Epoch 40/90
84077/84077 - 12s - loss: 1.5416e-04 - val_loss: 1.4054e-04 - 12s/epoch - 139us/sample
Epoch 41/90
84077/84077 - 11s - loss: 1.5384e-04 - val_loss: 1.3975e-04 - 11s/epoch - 136us/sample
Epoch 42/90
84077/84077 - 11s - loss: 1.5212e-04 - val_loss: 1.3835e-04 - 11s/epoch - 135us/sample
Epoch 43/90
84077/84077 - 11s - loss: 1.5147e-04 - val_loss: 1.4010e-04 - 11s/epoch - 135us/sample
Epoch 44/90
84077/84077 - 11s - loss: 1.5064e-04 - val_loss: 1.3705e-04 - 11s/epoch - 135us/sample
Epoch 45/90
84077/84077 - 11s - loss: 1.4997e-04 - val_loss: 1.3573e-04 - 11s/epoch - 135us/sample
Epoch 46/90
84077/84077 - 12s - loss: 1.4888e-04 - val_loss: 1.3586e-04 - 12s/epoch - 137us/sample
Epoch 47/90
84077/84077 - 11s - loss: 1.4818e-04 - val_loss: 1.3503e-04 - 11s/epoch - 135us/sample
Epoch 48/90
84077/84077 - 11s - loss: 1.4797e-04 - val_loss: 1.3529e-04 - 11s/epoch - 135us/sample
Epoch 49/90
84077/84077 - 11s - loss: 1.4718e-04 - val_loss: 1.3464e-04 - 11s/epoch - 135us/sample
Epoch 50/90
84077/84077 - 11s - loss: 1.4699e-04 - val_loss: 1.3391e-04 - 11s/epoch - 135us/sample
Epoch 51/90
84077/84077 - 11s - loss: 1.4666e-04 - val_loss: 1.3500e-04 - 11s/epoch - 135us/sample
Epoch 52/90
84077/84077 - 12s - loss: 1.4615e-04 - val_loss: 1.3417e-04 - 12s/epoch - 138us/sample
Epoch 53/90
84077/84077 - 11s - loss: 1.4575e-04 - val_loss: 1.3412e-04 - 11s/epoch - 136us/sample
Epoch 54/90
84077/84077 - 11s - loss: 1.4553e-04 - val_loss: 1.3309e-04 - 11s/epoch - 135us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.4469e-04 - val_loss: 1.3262e-04 - 11s/epoch - 135us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.4496e-04 - val_loss: 1.3251e-04 - 11s/epoch - 135us/sample
Epoch 57/90
84077/84077 - 11s - loss: 1.4441e-04 - val_loss: 1.3199e-04 - 11s/epoch - 134us/sample
Epoch 58/90
84077/84077 - 12s - loss: 1.4382e-04 - val_loss: 1.3099e-04 - 12s/epoch - 138us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.4365e-04 - val_loss: 1.3160e-04 - 11s/epoch - 135us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.4270e-04 - val_loss: 1.3241e-04 - 11s/epoch - 135us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.4479e-04 - val_loss: 1.3105e-04 - 11s/epoch - 135us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.4247e-04 - val_loss: 1.3053e-04 - 11s/epoch - 135us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.4215e-04 - val_loss: 1.3034e-04 - 11s/epoch - 135us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.4169e-04 - val_loss: 1.3062e-04 - 11s/epoch - 136us/sample
Epoch 65/90
84077/84077 - 12s - loss: 1.4185e-04 - val_loss: 1.2959e-04 - 12s/epoch - 138us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.4129e-04 - val_loss: 1.2981e-04 - 11s/epoch - 135us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.4101e-04 - val_loss: 1.2903e-04 - 11s/epoch - 135us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.4072e-04 - val_loss: 1.2870e-04 - 11s/epoch - 135us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.4009e-04 - val_loss: 1.3005e-04 - 11s/epoch - 135us/sample
Epoch 70/90
84077/84077 - 12s - loss: 1.4043e-04 - val_loss: 1.2964e-04 - 12s/epoch - 137us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.3988e-04 - val_loss: 1.2915e-04 - 11s/epoch - 137us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.3947e-04 - val_loss: 1.2846e-04 - 11s/epoch - 135us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.4016e-04 - val_loss: 1.2877e-04 - 11s/epoch - 135us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.3908e-04 - val_loss: 1.2881e-04 - 11s/epoch - 135us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.3885e-04 - val_loss: 1.2874e-04 - 11s/epoch - 135us/sample
Epoch 76/90
84077/84077 - 12s - loss: 1.3872e-04 - val_loss: 1.2795e-04 - 12s/epoch - 137us/sample
Epoch 77/90
84077/84077 - 12s - loss: 1.3865e-04 - val_loss: 1.2740e-04 - 12s/epoch - 140us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.3814e-04 - val_loss: 1.2810e-04 - 11s/epoch - 136us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.3794e-04 - val_loss: 1.2760e-04 - 11s/epoch - 135us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.3788e-04 - val_loss: 1.2720e-04 - 11s/epoch - 135us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.3783e-04 - val_loss: 1.2721e-04 - 11s/epoch - 135us/sample
Epoch 82/90
84077/84077 - 12s - loss: 1.3691e-04 - val_loss: 1.2693e-04 - 12s/epoch - 139us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.3719e-04 - val_loss: 1.2749e-04 - 11s/epoch - 136us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.3718e-04 - val_loss: 1.2727e-04 - 11s/epoch - 135us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.3697e-04 - val_loss: 1.2689e-04 - 11s/epoch - 135us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.3633e-04 - val_loss: 1.2579e-04 - 11s/epoch - 135us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.3605e-04 - val_loss: 1.2664e-04 - 11s/epoch - 135us/sample
Epoch 88/90
84077/84077 - 12s - loss: 1.3649e-04 - val_loss: 1.2600e-04 - 12s/epoch - 138us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.3608e-04 - val_loss: 1.2661e-04 - 11s/epoch - 135us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.3593e-04 - val_loss: 1.2567e-04 - 11s/epoch - 135us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012567036116526234
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 02:18:31.438207: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_72/outputlayer/BiasAdd' id:90860 op device:{requested: '', assigned: ''} def:{{{node decoder_model_72/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_72/outputlayer/MatMul, decoder_model_72/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03184067408398352
cosine 0.03145293977129441
MAE: 0.0025294309427951857
RMSE: 0.010130600079400216
r2: 0.9200104718892987
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 90, 0.0012000000000000001, 0.2, 188, 0.00013593491130535517, 0.00012567036116526234, 0.03184067408398352, 0.03145293977129441, 0.0025294309427951857, 0.010130600079400216, 0.9200104718892987, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0008 64 0] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_216 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_216 (ReLU)               (None, 2074)         0           ['batch_normalization_216[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_216[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_216[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 02:19:12.109256: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_217/moving_mean/Assign' id:91911 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_217/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_217/moving_mean, batch_normalization_217/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 02:19:33.540083: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_73/mul' id:92193 op device:{requested: '', assigned: ''} def:{{{node loss_73/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_73/mul/x, loss_73/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 38s - loss: 0.0733 - val_loss: 0.0706 - 38s/epoch - 448us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0789 - val_loss: 0.0805 - 12s/epoch - 141us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0674 - val_loss: 0.0676 - 12s/epoch - 141us/sample
Epoch 4/85
84077/84077 - 12s - loss: 0.1044 - val_loss: 0.0683 - 12s/epoch - 145us/sample
Epoch 5/85
84077/84077 - 12s - loss: 0.0673 - val_loss: 0.0676 - 12s/epoch - 142us/sample
Epoch 6/85
84077/84077 - 12s - loss: 0.0670 - val_loss: 0.0675 - 12s/epoch - 141us/sample
Epoch 7/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 8/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 9/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 10/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 146us/sample
Epoch 11/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 12/85
84077/84077 - 12s - loss: 0.0669 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 13/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0674 - 12s/epoch - 141us/sample
Epoch 14/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 15/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 16/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 17/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 18/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 19/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 20/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 21/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 22/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 145us/sample
Epoch 23/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 24/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 25/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 26/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 27/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 28/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 143us/sample
Epoch 29/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 30/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 31/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 32/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 33/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 143us/sample
Epoch 34/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 143us/sample
Epoch 35/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 36/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 37/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 38/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 39/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 40/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 41/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 42/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 43/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 44/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 45/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 46/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 47/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 48/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 49/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 50/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 51/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 145us/sample
Epoch 52/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 53/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 54/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 55/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 56/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 57/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 146us/sample
Epoch 58/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 59/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 60/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 61/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 62/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 63/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 64/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 65/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 66/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 140us/sample
Epoch 67/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 68/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 69/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 142us/sample
Epoch 70/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 71/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 72/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 73/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 74/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 145us/sample
Epoch 75/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 76/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 77/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 78/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 79/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 80/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 144us/sample
Epoch 81/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 82/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 83/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 84/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
Epoch 85/85
84077/84077 - 12s - loss: 0.0668 - val_loss: 0.0673 - 12s/epoch - 141us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734573190624671
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 02:36:21.905082: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_73/outputlayer/BiasAdd' id:92145 op device:{requested: '', assigned: ''} def:{{{node decoder_model_73/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_73/outputlayer/MatMul, decoder_model_73/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1417832564809425
cosine 1.13853175499133
MAE: 5.805163813553572
RMSE: 6.044107745588389
r2: -26756.703412088544
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'binary_crossentropy', 64, 85, 0.0008, 0.2, 188, 0.06683691600993716, 0.06734573190624671, 1.1417832564809425, 1.13853175499133, 5.805163813553572, 6.044107745588389, -26756.703412088544, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0008 8 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_219 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_219 (ReLU)               (None, 2074)         0           ['batch_normalization_219[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          390100      ['re_lu_219[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          390100      ['re_lu_219[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2393291     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,139,643
Trainable params: 5,130,971
Non-trainable params: 8,672
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 02:37:03.858093: W tensorflow/c/c_api.cc:291] Operation '{name:'training_144/Adam/outputlayer_74/kernel/m/Assign' id:94096 op device:{requested: '', assigned: ''} def:{{{node training_144/Adam/outputlayer_74/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_144/Adam/outputlayer_74/kernel/m, training_144/Adam/outputlayer_74/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 02:38:04.587757: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_74/mul' id:93518 op device:{requested: '', assigned: ''} def:{{{node loss_74/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_74/mul/x, loss_74/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 81s - loss: 0.0021 - val_loss: 8.0684e-04 - 81s/epoch - 960us/sample
Epoch 2/85
84077/84077 - 54s - loss: 6.3403e-04 - val_loss: 0.0070 - 54s/epoch - 637us/sample
Epoch 3/85
84077/84077 - 53s - loss: 5.2759e-04 - val_loss: 0.0016 - 53s/epoch - 634us/sample
Epoch 4/85
84077/84077 - 54s - loss: 4.9121e-04 - val_loss: 0.0017 - 54s/epoch - 638us/sample
Epoch 5/85
84077/84077 - 54s - loss: 4.7127e-04 - val_loss: 0.0020 - 54s/epoch - 638us/sample
Epoch 6/85
84077/84077 - 53s - loss: 4.6046e-04 - val_loss: 0.0038 - 53s/epoch - 633us/sample
Epoch 7/85
84077/84077 - 54s - loss: 4.5169e-04 - val_loss: 0.0042 - 54s/epoch - 637us/sample
Epoch 8/85
84077/84077 - 53s - loss: 4.4474e-04 - val_loss: 0.0055 - 53s/epoch - 636us/sample
Epoch 9/85
84077/84077 - 53s - loss: 4.3652e-04 - val_loss: 0.0069 - 53s/epoch - 630us/sample
Epoch 10/85
84077/84077 - 54s - loss: 4.3093e-04 - val_loss: 0.0096 - 54s/epoch - 637us/sample
Epoch 11/85
84077/84077 - 54s - loss: 4.2768e-04 - val_loss: 0.0080 - 54s/epoch - 637us/sample
Epoch 12/85
84077/84077 - 53s - loss: 4.2547e-04 - val_loss: 0.0094 - 53s/epoch - 629us/sample
Epoch 13/85
84077/84077 - 53s - loss: 4.2266e-04 - val_loss: 0.0097 - 53s/epoch - 635us/sample
Epoch 14/85
84077/84077 - 53s - loss: 4.2005e-04 - val_loss: 0.0114 - 53s/epoch - 634us/sample
Epoch 15/85
84077/84077 - 53s - loss: 4.1678e-04 - val_loss: 0.0091 - 53s/epoch - 631us/sample
Epoch 16/85
84077/84077 - 53s - loss: 4.1575e-04 - val_loss: 0.0102 - 53s/epoch - 633us/sample
Epoch 17/85
84077/84077 - 53s - loss: 4.1418e-04 - val_loss: 0.0097 - 53s/epoch - 632us/sample
Epoch 18/85
84077/84077 - 53s - loss: 4.1190e-04 - val_loss: 0.0094 - 53s/epoch - 633us/sample
Epoch 19/85
84077/84077 - 53s - loss: 4.1030e-04 - val_loss: 0.0095 - 53s/epoch - 634us/sample
Epoch 20/85
84077/84077 - 53s - loss: 4.0911e-04 - val_loss: 0.0106 - 53s/epoch - 631us/sample
Epoch 21/85
84077/84077 - 53s - loss: 4.0786e-04 - val_loss: 0.0119 - 53s/epoch - 633us/sample
Epoch 22/85
84077/84077 - 53s - loss: 4.0692e-04 - val_loss: 0.0101 - 53s/epoch - 633us/sample
Epoch 23/85
84077/84077 - 53s - loss: 4.0562e-04 - val_loss: 0.0097 - 53s/epoch - 629us/sample
Epoch 24/85
84077/84077 - 53s - loss: 4.0545e-04 - val_loss: 0.0106 - 53s/epoch - 633us/sample
Epoch 25/85
84077/84077 - 53s - loss: 4.0414e-04 - val_loss: 0.0110 - 53s/epoch - 634us/sample
Epoch 26/85
84077/84077 - 53s - loss: 4.0327e-04 - val_loss: 0.0113 - 53s/epoch - 629us/sample
Epoch 27/85
84077/84077 - 53s - loss: 4.0244e-04 - val_loss: 0.0099 - 53s/epoch - 633us/sample
Epoch 28/85
84077/84077 - 54s - loss: 4.0210e-04 - val_loss: 0.0104 - 54s/epoch - 643us/sample
Epoch 29/85
84077/84077 - 54s - loss: 4.0082e-04 - val_loss: 0.0091 - 54s/epoch - 637us/sample
Epoch 30/85
84077/84077 - 54s - loss: 4.0023e-04 - val_loss: 0.0104 - 54s/epoch - 642us/sample
Epoch 31/85
84077/84077 - 54s - loss: 3.9987e-04 - val_loss: 0.0084 - 54s/epoch - 641us/sample
Epoch 32/85
84077/84077 - 54s - loss: 3.9930e-04 - val_loss: 0.0095 - 54s/epoch - 638us/sample
Epoch 33/85
84077/84077 - 54s - loss: 3.9810e-04 - val_loss: 0.0091 - 54s/epoch - 643us/sample
Epoch 34/85
84077/84077 - 53s - loss: 3.9781e-04 - val_loss: 0.0097 - 53s/epoch - 635us/sample
Epoch 35/85
84077/84077 - 54s - loss: 3.9741e-04 - val_loss: 0.0103 - 54s/epoch - 640us/sample
Epoch 36/85
84077/84077 - 54s - loss: 3.9597e-04 - val_loss: 0.0110 - 54s/epoch - 642us/sample
Epoch 37/85
84077/84077 - 54s - loss: 3.9655e-04 - val_loss: 0.0090 - 54s/epoch - 637us/sample
Epoch 38/85
84077/84077 - 54s - loss: 3.9529e-04 - val_loss: 0.0100 - 54s/epoch - 643us/sample
Epoch 39/85
84077/84077 - 54s - loss: 3.9420e-04 - val_loss: 0.0129 - 54s/epoch - 642us/sample
Epoch 40/85
84077/84077 - 54s - loss: 3.9330e-04 - val_loss: 0.0097 - 54s/epoch - 638us/sample
Epoch 41/85
84077/84077 - 54s - loss: 3.9312e-04 - val_loss: 0.0093 - 54s/epoch - 643us/sample
Epoch 42/85
84077/84077 - 54s - loss: 3.9201e-04 - val_loss: 0.0117 - 54s/epoch - 642us/sample
Epoch 43/85
84077/84077 - 54s - loss: 3.9063e-04 - val_loss: 0.0128 - 54s/epoch - 638us/sample
Epoch 44/85
84077/84077 - 54s - loss: 3.9049e-04 - val_loss: 0.0108 - 54s/epoch - 642us/sample
Epoch 45/85
84077/84077 - 54s - loss: 3.8966e-04 - val_loss: 0.0092 - 54s/epoch - 641us/sample
Epoch 46/85
84077/84077 - 54s - loss: 3.8806e-04 - val_loss: 0.0102 - 54s/epoch - 639us/sample
Epoch 47/85
84077/84077 - 54s - loss: 3.8796e-04 - val_loss: 0.0104 - 54s/epoch - 643us/sample
Epoch 48/85
84077/84077 - 54s - loss: 3.8656e-04 - val_loss: 0.0099 - 54s/epoch - 643us/sample
Epoch 49/85
84077/84077 - 54s - loss: 3.8684e-04 - val_loss: 0.0091 - 54s/epoch - 640us/sample
Epoch 50/85
84077/84077 - 54s - loss: 3.8496e-04 - val_loss: 0.0092 - 54s/epoch - 645us/sample
Epoch 51/85
84077/84077 - 54s - loss: 3.8506e-04 - val_loss: 0.0111 - 54s/epoch - 643us/sample
Epoch 52/85
84077/84077 - 54s - loss: 3.8465e-04 - val_loss: 0.0092 - 54s/epoch - 641us/sample
Epoch 53/85
84077/84077 - 54s - loss: 3.8337e-04 - val_loss: 0.0101 - 54s/epoch - 645us/sample
Epoch 54/85
84077/84077 - 54s - loss: 3.8396e-04 - val_loss: 0.0116 - 54s/epoch - 644us/sample
Epoch 55/85
84077/84077 - 54s - loss: 3.8354e-04 - val_loss: 0.0108 - 54s/epoch - 642us/sample
Epoch 56/85
84077/84077 - 54s - loss: 3.8246e-04 - val_loss: 0.0109 - 54s/epoch - 645us/sample
Epoch 57/85
84077/84077 - 54s - loss: 3.8277e-04 - val_loss: 0.0108 - 54s/epoch - 645us/sample
Epoch 58/85
84077/84077 - 54s - loss: 3.8245e-04 - val_loss: 0.0123 - 54s/epoch - 639us/sample
Epoch 59/85
84077/84077 - 54s - loss: 3.8119e-04 - val_loss: 0.0096 - 54s/epoch - 645us/sample
Epoch 60/85
84077/84077 - 54s - loss: 3.8133e-04 - val_loss: 0.0111 - 54s/epoch - 639us/sample
Epoch 61/85
84077/84077 - 54s - loss: 3.8063e-04 - val_loss: 0.0094 - 54s/epoch - 645us/sample
Epoch 62/85
84077/84077 - 54s - loss: 3.8068e-04 - val_loss: 0.0138 - 54s/epoch - 644us/sample
Epoch 63/85
84077/84077 - 54s - loss: 3.8005e-04 - val_loss: 0.0111 - 54s/epoch - 642us/sample
Epoch 64/85
84077/84077 - 54s - loss: 3.7957e-04 - val_loss: 0.0105 - 54s/epoch - 644us/sample
Epoch 65/85
84077/84077 - 54s - loss: 3.7911e-04 - val_loss: 0.0134 - 54s/epoch - 646us/sample
Epoch 66/85
84077/84077 - 54s - loss: 3.7904e-04 - val_loss: 0.0106 - 54s/epoch - 639us/sample
Epoch 67/85
84077/84077 - 54s - loss: 3.7902e-04 - val_loss: 0.0122 - 54s/epoch - 644us/sample
Epoch 68/85
84077/84077 - 54s - loss: 3.7878e-04 - val_loss: 0.0118 - 54s/epoch - 641us/sample
Epoch 69/85
84077/84077 - 54s - loss: 3.7816e-04 - val_loss: 0.0114 - 54s/epoch - 639us/sample
Epoch 70/85
84077/84077 - 54s - loss: 3.7789e-04 - val_loss: 0.0118 - 54s/epoch - 642us/sample
Epoch 71/85
84077/84077 - 54s - loss: 3.7754e-04 - val_loss: 0.0116 - 54s/epoch - 645us/sample
Epoch 72/85
84077/84077 - 54s - loss: 3.7717e-04 - val_loss: 0.0121 - 54s/epoch - 639us/sample
Epoch 73/85
84077/84077 - 54s - loss: 3.7719e-04 - val_loss: 0.0097 - 54s/epoch - 645us/sample
Epoch 74/85
84077/84077 - 54s - loss: 3.7672e-04 - val_loss: 0.0103 - 54s/epoch - 643us/sample
Epoch 75/85
84077/84077 - 54s - loss: 3.7663e-04 - val_loss: 0.0115 - 54s/epoch - 638us/sample
Epoch 76/85
84077/84077 - 54s - loss: 3.7628e-04 - val_loss: 0.0099 - 54s/epoch - 642us/sample
Epoch 77/85
84077/84077 - 54s - loss: 3.7633e-04 - val_loss: 0.0132 - 54s/epoch - 642us/sample
Epoch 78/85
84077/84077 - 54s - loss: 3.7562e-04 - val_loss: 0.0113 - 54s/epoch - 640us/sample
Epoch 79/85
84077/84077 - 54s - loss: 3.7518e-04 - val_loss: 0.0109 - 54s/epoch - 640us/sample
Epoch 80/85
84077/84077 - 54s - loss: 3.7562e-04 - val_loss: 0.0113 - 54s/epoch - 638us/sample
Epoch 81/85
84077/84077 - 54s - loss: 3.7451e-04 - val_loss: 0.0120 - 54s/epoch - 641us/sample
Epoch 82/85
84077/84077 - 54s - loss: 3.7446e-04 - val_loss: 0.0123 - 54s/epoch - 641us/sample
Epoch 83/85
84077/84077 - 54s - loss: 3.7428e-04 - val_loss: 0.0100 - 54s/epoch - 639us/sample
Epoch 84/85
84077/84077 - 54s - loss: 3.7362e-04 - val_loss: 0.0117 - 54s/epoch - 643us/sample
Epoch 85/85
84077/84077 - 54s - loss: 3.7409e-04 - val_loss: 0.0106 - 54s/epoch - 642us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.010611729976957428
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 03:53:30.404599: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_74/outputlayer/BiasAdd' id:93482 op device:{requested: '', assigned: ''} def:{{{node decoder_model_74/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_74/outputlayer/MatMul, decoder_model_74/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2264310494058134
cosine 0.22385986200715816
MAE: 0.024369773799768738
RMSE: 0.3950159160628673
r2: -120.88465710395172
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 8, 85, 0.0008, 0.2, 188, 0.00037409325092259193, 0.010611729976957428, 0.2264310494058134, 0.22385986200715816, 0.024369773799768738, 0.3950159160628673, -120.88465710395172, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 90 0.0008 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_222 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_222 (ReLU)               (None, 2168)         0           ['batch_normalization_222[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_222[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_222[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 03:54:16.946551: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_223/moving_variance/Assign' id:94538 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_223/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_223/moving_variance, batch_normalization_223/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 03:54:39.544197: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_75/mul' id:94803 op device:{requested: '', assigned: ''} def:{{{node loss_75/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_75/mul/x, loss_75/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 40s - loss: 0.0048 - val_loss: 0.0013 - 40s/epoch - 476us/sample
Epoch 2/90
84077/84077 - 12s - loss: 0.0012 - val_loss: 9.8068e-04 - 12s/epoch - 147us/sample
Epoch 3/90
84077/84077 - 13s - loss: 0.0025 - val_loss: 8.7361e-04 - 13s/epoch - 151us/sample
Epoch 4/90
84077/84077 - 13s - loss: 8.8354e-04 - val_loss: 5.5807e-04 - 13s/epoch - 150us/sample
Epoch 5/90
84077/84077 - 12s - loss: 6.2080e-04 - val_loss: 5.7470e-04 - 12s/epoch - 149us/sample
Epoch 6/90
84077/84077 - 12s - loss: 6.4326e-04 - val_loss: 9.1397e-04 - 12s/epoch - 147us/sample
Epoch 7/90
84077/84077 - 12s - loss: 0.0016 - val_loss: 7.3840e-04 - 12s/epoch - 149us/sample
Epoch 8/90
84077/84077 - 12s - loss: 5.3713e-04 - val_loss: 4.3274e-04 - 12s/epoch - 148us/sample
Epoch 9/90
84077/84077 - 13s - loss: 4.2316e-04 - val_loss: 3.6487e-04 - 13s/epoch - 152us/sample
Epoch 10/90
84077/84077 - 12s - loss: 3.6949e-04 - val_loss: 3.2628e-04 - 12s/epoch - 147us/sample
Epoch 11/90
84077/84077 - 13s - loss: 3.4374e-04 - val_loss: 3.0458e-04 - 13s/epoch - 149us/sample
Epoch 12/90
84077/84077 - 12s - loss: 3.2138e-04 - val_loss: 2.7671e-04 - 12s/epoch - 149us/sample
Epoch 13/90
84077/84077 - 12s - loss: 2.9417e-04 - val_loss: 2.6089e-04 - 12s/epoch - 147us/sample
Epoch 14/90
84077/84077 - 13s - loss: 2.7163e-04 - val_loss: 2.4045e-04 - 13s/epoch - 153us/sample
Epoch 15/90
84077/84077 - 12s - loss: 2.5425e-04 - val_loss: 2.2106e-04 - 12s/epoch - 148us/sample
Epoch 16/90
84077/84077 - 12s - loss: 2.4044e-04 - val_loss: 2.1464e-04 - 12s/epoch - 149us/sample
Epoch 17/90
84077/84077 - 12s - loss: 2.2932e-04 - val_loss: 2.0091e-04 - 12s/epoch - 147us/sample
Epoch 18/90
84077/84077 - 13s - loss: 2.1804e-04 - val_loss: 1.9348e-04 - 13s/epoch - 149us/sample
Epoch 19/90
84077/84077 - 12s - loss: 2.0932e-04 - val_loss: 1.8411e-04 - 12s/epoch - 147us/sample
Epoch 20/90
84077/84077 - 13s - loss: 2.0168e-04 - val_loss: 1.7623e-04 - 13s/epoch - 155us/sample
Epoch 21/90
84077/84077 - 12s - loss: 1.9374e-04 - val_loss: 1.7089e-04 - 12s/epoch - 148us/sample
Epoch 22/90
84077/84077 - 13s - loss: 1.8748e-04 - val_loss: 1.6647e-04 - 13s/epoch - 149us/sample
Epoch 23/90
84077/84077 - 12s - loss: 1.8322e-04 - val_loss: 1.6465e-04 - 12s/epoch - 147us/sample
Epoch 24/90
84077/84077 - 12s - loss: 1.7796e-04 - val_loss: 1.5829e-04 - 12s/epoch - 149us/sample
Epoch 25/90
84077/84077 - 13s - loss: 1.7478e-04 - val_loss: 1.5616e-04 - 13s/epoch - 151us/sample
Epoch 26/90
84077/84077 - 13s - loss: 1.7121e-04 - val_loss: 1.5435e-04 - 13s/epoch - 149us/sample
Epoch 27/90
84077/84077 - 12s - loss: 1.6859e-04 - val_loss: 1.5029e-04 - 12s/epoch - 148us/sample
Epoch 28/90
84077/84077 - 12s - loss: 1.6576e-04 - val_loss: 1.4929e-04 - 12s/epoch - 147us/sample
Epoch 29/90
84077/84077 - 13s - loss: 1.6392e-04 - val_loss: 1.4922e-04 - 13s/epoch - 149us/sample
Epoch 30/90
84077/84077 - 12s - loss: 1.6244e-04 - val_loss: 1.4663e-04 - 12s/epoch - 149us/sample
Epoch 31/90
84077/84077 - 13s - loss: 1.6106e-04 - val_loss: 1.4648e-04 - 13s/epoch - 153us/sample
Epoch 32/90
84077/84077 - 12s - loss: 1.5928e-04 - val_loss: 1.4601e-04 - 12s/epoch - 148us/sample
Epoch 33/90
84077/84077 - 12s - loss: 1.5786e-04 - val_loss: 1.4353e-04 - 12s/epoch - 149us/sample
Epoch 34/90
84077/84077 - 12s - loss: 1.5687e-04 - val_loss: 1.4352e-04 - 12s/epoch - 148us/sample
Epoch 35/90
84077/84077 - 13s - loss: 1.5584e-04 - val_loss: 1.4110e-04 - 13s/epoch - 149us/sample
Epoch 36/90
84077/84077 - 13s - loss: 1.5508e-04 - val_loss: 1.4359e-04 - 13s/epoch - 152us/sample
Epoch 37/90
84077/84077 - 13s - loss: 1.5518e-04 - val_loss: 1.4195e-04 - 13s/epoch - 149us/sample
Epoch 38/90
84077/84077 - 12s - loss: 1.5310e-04 - val_loss: 1.4125e-04 - 12s/epoch - 147us/sample
Epoch 39/90
84077/84077 - 12s - loss: 1.5201e-04 - val_loss: 1.4010e-04 - 12s/epoch - 148us/sample
Epoch 40/90
84077/84077 - 12s - loss: 1.5200e-04 - val_loss: 1.3967e-04 - 12s/epoch - 147us/sample
Epoch 41/90
84077/84077 - 13s - loss: 1.5076e-04 - val_loss: 1.4095e-04 - 13s/epoch - 151us/sample
Epoch 42/90
84077/84077 - 13s - loss: 1.4948e-04 - val_loss: 1.3969e-04 - 13s/epoch - 149us/sample
Epoch 43/90
84077/84077 - 13s - loss: 1.4901e-04 - val_loss: 1.3856e-04 - 13s/epoch - 149us/sample
Epoch 44/90
84077/84077 - 13s - loss: 1.4774e-04 - val_loss: 1.3929e-04 - 13s/epoch - 149us/sample
Epoch 45/90
84077/84077 - 12s - loss: 1.4746e-04 - val_loss: 1.3874e-04 - 12s/epoch - 147us/sample
Epoch 46/90
84077/84077 - 13s - loss: 1.4690e-04 - val_loss: 1.3778e-04 - 13s/epoch - 149us/sample
Epoch 47/90
84077/84077 - 13s - loss: 1.4630e-04 - val_loss: 1.3618e-04 - 13s/epoch - 151us/sample
Epoch 48/90
84077/84077 - 13s - loss: 1.4556e-04 - val_loss: 1.3542e-04 - 13s/epoch - 149us/sample
Epoch 49/90
84077/84077 - 12s - loss: 1.4523e-04 - val_loss: 1.3514e-04 - 12s/epoch - 147us/sample
Epoch 50/90
84077/84077 - 12s - loss: 1.4449e-04 - val_loss: 1.3545e-04 - 12s/epoch - 148us/sample
Epoch 51/90
84077/84077 - 12s - loss: 1.4441e-04 - val_loss: 1.3627e-04 - 12s/epoch - 148us/sample
Epoch 52/90
84077/84077 - 13s - loss: 1.4350e-04 - val_loss: 1.3483e-04 - 13s/epoch - 154us/sample
Epoch 53/90
84077/84077 - 12s - loss: 1.4282e-04 - val_loss: 1.3563e-04 - 12s/epoch - 148us/sample
Epoch 54/90
84077/84077 - 12s - loss: 1.4292e-04 - val_loss: 1.3370e-04 - 12s/epoch - 149us/sample
Epoch 55/90
84077/84077 - 12s - loss: 1.4298e-04 - val_loss: 1.3356e-04 - 12s/epoch - 148us/sample
Epoch 56/90
84077/84077 - 13s - loss: 1.4226e-04 - val_loss: 1.3264e-04 - 13s/epoch - 149us/sample
Epoch 57/90
84077/84077 - 12s - loss: 1.4152e-04 - val_loss: 1.3481e-04 - 12s/epoch - 148us/sample
Epoch 58/90
84077/84077 - 13s - loss: 1.4167e-04 - val_loss: 1.3201e-04 - 13s/epoch - 152us/sample
Epoch 59/90
84077/84077 - 12s - loss: 1.4087e-04 - val_loss: 1.3254e-04 - 12s/epoch - 148us/sample
Epoch 60/90
84077/84077 - 12s - loss: 1.4060e-04 - val_loss: 1.3095e-04 - 12s/epoch - 148us/sample
Epoch 61/90
84077/84077 - 13s - loss: 1.4028e-04 - val_loss: 1.3087e-04 - 13s/epoch - 149us/sample
Epoch 62/90
84077/84077 - 12s - loss: 1.4020e-04 - val_loss: 1.3149e-04 - 12s/epoch - 147us/sample
Epoch 63/90
84077/84077 - 13s - loss: 1.3947e-04 - val_loss: 1.3159e-04 - 13s/epoch - 154us/sample
Epoch 64/90
84077/84077 - 12s - loss: 1.3926e-04 - val_loss: 1.3143e-04 - 12s/epoch - 148us/sample
Epoch 65/90
84077/84077 - 13s - loss: 1.3918e-04 - val_loss: 1.3046e-04 - 13s/epoch - 149us/sample
Epoch 66/90
84077/84077 - 12s - loss: 1.3880e-04 - val_loss: 1.3215e-04 - 12s/epoch - 147us/sample
Epoch 67/90
84077/84077 - 12s - loss: 1.3814e-04 - val_loss: 1.2956e-04 - 12s/epoch - 148us/sample
Epoch 68/90
84077/84077 - 13s - loss: 1.3880e-04 - val_loss: 1.3033e-04 - 13s/epoch - 150us/sample
Epoch 69/90
84077/84077 - 13s - loss: 1.3785e-04 - val_loss: 1.3135e-04 - 13s/epoch - 154us/sample
Epoch 70/90
84077/84077 - 13s - loss: 1.3795e-04 - val_loss: 1.3079e-04 - 13s/epoch - 149us/sample
Epoch 71/90
84077/84077 - 13s - loss: 1.3773e-04 - val_loss: 1.3091e-04 - 13s/epoch - 149us/sample
Epoch 72/90
84077/84077 - 12s - loss: 1.3760e-04 - val_loss: 1.3071e-04 - 12s/epoch - 148us/sample
Epoch 73/90
84077/84077 - 13s - loss: 1.3749e-04 - val_loss: 1.2932e-04 - 13s/epoch - 149us/sample
Epoch 74/90
84077/84077 - 13s - loss: 1.3651e-04 - val_loss: 1.2865e-04 - 13s/epoch - 152us/sample
Epoch 75/90
84077/84077 - 13s - loss: 1.3993e-04 - val_loss: 1.3930e-04 - 13s/epoch - 149us/sample
Epoch 76/90
84077/84077 - 12s - loss: 1.3813e-04 - val_loss: 1.2887e-04 - 12s/epoch - 148us/sample
Epoch 77/90
84077/84077 - 12s - loss: 1.3647e-04 - val_loss: 1.2884e-04 - 12s/epoch - 149us/sample
Epoch 78/90
84077/84077 - 12s - loss: 1.3617e-04 - val_loss: 1.2933e-04 - 12s/epoch - 148us/sample
Epoch 79/90
84077/84077 - 13s - loss: 1.3575e-04 - val_loss: 1.2812e-04 - 13s/epoch - 152us/sample
Epoch 80/90
84077/84077 - 13s - loss: 1.3597e-04 - val_loss: 1.2796e-04 - 13s/epoch - 149us/sample
Epoch 81/90
84077/84077 - 12s - loss: 1.3569e-04 - val_loss: 1.2823e-04 - 12s/epoch - 148us/sample
Epoch 82/90
84077/84077 - 13s - loss: 1.3557e-04 - val_loss: 1.2794e-04 - 13s/epoch - 149us/sample
Epoch 83/90
84077/84077 - 12s - loss: 1.3531e-04 - val_loss: 1.2914e-04 - 12s/epoch - 148us/sample
Epoch 84/90
84077/84077 - 12s - loss: 1.3510e-04 - val_loss: 1.2761e-04 - 12s/epoch - 148us/sample
Epoch 85/90
84077/84077 - 13s - loss: 1.3476e-04 - val_loss: 1.2822e-04 - 13s/epoch - 154us/sample
Epoch 86/90
84077/84077 - 13s - loss: 1.3488e-04 - val_loss: 1.2760e-04 - 13s/epoch - 155us/sample
Epoch 87/90
84077/84077 - 13s - loss: 1.3439e-04 - val_loss: 1.2723e-04 - 13s/epoch - 155us/sample
Epoch 88/90
84077/84077 - 13s - loss: 1.3435e-04 - val_loss: 1.2808e-04 - 13s/epoch - 156us/sample
Epoch 89/90
84077/84077 - 13s - loss: 1.3430e-04 - val_loss: 1.2784e-04 - 13s/epoch - 155us/sample
Epoch 90/90
84077/84077 - 13s - loss: 1.3421e-04 - val_loss: 1.2782e-04 - 13s/epoch - 159us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001278182156003417
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 04:13:27.144230: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_75/outputlayer/BiasAdd' id:94767 op device:{requested: '', assigned: ''} def:{{{node decoder_model_75/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_75/outputlayer/MatMul, decoder_model_75/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.029534772641229085
cosine 0.029170596163941357
MAE: 0.0021812093895417313
RMSE: 0.0101705289735466
r2: 0.9193197141559882
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'logcosh', 64, 90, 0.0008, 0.2, 188, 0.00013420583536101924, 0.0001278182156003417, 0.029534772641229085, 0.029170596163941357, 0.0021812093895417313, 0.0101705289735466, 0.9193197141559882, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 80 0.0008 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_225 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_225 (ReLU)               (None, 2168)         0           ['batch_normalization_225[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_225[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_225[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 04:14:13.065149: W tensorflow/c/c_api.cc:291] Operation '{name:'training_148/Adam/batch_normalization_226/beta/v/Assign' id:96748 op device:{requested: '', assigned: ''} def:{{{node training_148/Adam/batch_normalization_226/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_148/Adam/batch_normalization_226/beta/v, training_148/Adam/batch_normalization_226/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 04:14:37.155074: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_76/mul' id:96091 op device:{requested: '', assigned: ''} def:{{{node loss_76/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_76/mul/x, loss_76/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 42s - loss: 0.3318 - val_loss: 0.0010 - 42s/epoch - 503us/sample
Epoch 2/80
84077/84077 - 13s - loss: 0.0011 - val_loss: 8.8552e-04 - 13s/epoch - 154us/sample
Epoch 3/80
84077/84077 - 13s - loss: 0.0023 - val_loss: 0.0010 - 13s/epoch - 154us/sample
Epoch 4/80
84077/84077 - 13s - loss: 8.4807e-04 - val_loss: 6.7534e-04 - 13s/epoch - 159us/sample
Epoch 5/80
84077/84077 - 13s - loss: 7.9632e-04 - val_loss: 8.0546e-04 - 13s/epoch - 156us/sample
Epoch 6/80
84077/84077 - 13s - loss: 6.3230e-04 - val_loss: 5.0430e-04 - 13s/epoch - 154us/sample
Epoch 7/80
84077/84077 - 13s - loss: 4.9070e-04 - val_loss: 4.2190e-04 - 13s/epoch - 155us/sample
Epoch 8/80
84077/84077 - 13s - loss: 3.9882e-04 - val_loss: 3.4923e-04 - 13s/epoch - 155us/sample
Epoch 9/80
84077/84077 - 13s - loss: 3.5481e-04 - val_loss: 3.0528e-04 - 13s/epoch - 158us/sample
Epoch 10/80
84077/84077 - 13s - loss: 3.1626e-04 - val_loss: 2.8091e-04 - 13s/epoch - 156us/sample
Epoch 11/80
84077/84077 - 13s - loss: 2.9287e-04 - val_loss: 2.5554e-04 - 13s/epoch - 155us/sample
Epoch 12/80
84077/84077 - 13s - loss: 2.7647e-04 - val_loss: 2.3801e-04 - 13s/epoch - 155us/sample
Epoch 13/80
84077/84077 - 13s - loss: 2.5096e-04 - val_loss: 2.2094e-04 - 13s/epoch - 154us/sample
Epoch 14/80
84077/84077 - 13s - loss: 2.3690e-04 - val_loss: 2.1421e-04 - 13s/epoch - 156us/sample
Epoch 15/80
84077/84077 - 13s - loss: 2.2442e-04 - val_loss: 2.0468e-04 - 13s/epoch - 158us/sample
Epoch 16/80
84077/84077 - 13s - loss: 2.1311e-04 - val_loss: 1.9014e-04 - 13s/epoch - 154us/sample
Epoch 17/80
84077/84077 - 13s - loss: 2.0477e-04 - val_loss: 1.8211e-04 - 13s/epoch - 155us/sample
Epoch 18/80
84077/84077 - 13s - loss: 1.9654e-04 - val_loss: 1.7995e-04 - 13s/epoch - 155us/sample
Epoch 19/80
84077/84077 - 13s - loss: 1.9094e-04 - val_loss: 1.7235e-04 - 13s/epoch - 155us/sample
Epoch 20/80
84077/84077 - 13s - loss: 1.8639e-04 - val_loss: 1.7414e-04 - 13s/epoch - 160us/sample
Epoch 21/80
84077/84077 - 13s - loss: 1.8040e-04 - val_loss: 1.6357e-04 - 13s/epoch - 154us/sample
Epoch 22/80
84077/84077 - 13s - loss: 1.7590e-04 - val_loss: 1.6147e-04 - 13s/epoch - 154us/sample
Epoch 23/80
84077/84077 - 13s - loss: 1.7302e-04 - val_loss: 1.5796e-04 - 13s/epoch - 154us/sample
Epoch 24/80
84077/84077 - 13s - loss: 1.6930e-04 - val_loss: 1.5422e-04 - 13s/epoch - 155us/sample
Epoch 25/80
84077/84077 - 14s - loss: 1.6611e-04 - val_loss: 1.5125e-04 - 14s/epoch - 161us/sample
Epoch 26/80
84077/84077 - 13s - loss: 1.6304e-04 - val_loss: 1.4877e-04 - 13s/epoch - 155us/sample
Epoch 27/80
84077/84077 - 13s - loss: 1.6191e-04 - val_loss: 1.4730e-04 - 13s/epoch - 154us/sample
Epoch 28/80
84077/84077 - 13s - loss: 1.5951e-04 - val_loss: 1.4614e-04 - 13s/epoch - 155us/sample
Epoch 29/80
84077/84077 - 13s - loss: 1.5795e-04 - val_loss: 1.4553e-04 - 13s/epoch - 155us/sample
Epoch 30/80
84077/84077 - 13s - loss: 1.5669e-04 - val_loss: 1.4441e-04 - 13s/epoch - 158us/sample
Epoch 31/80
84077/84077 - 13s - loss: 1.5505e-04 - val_loss: 1.4226e-04 - 13s/epoch - 155us/sample
Epoch 32/80
84077/84077 - 13s - loss: 1.5422e-04 - val_loss: 1.4170e-04 - 13s/epoch - 154us/sample
Epoch 33/80
84077/84077 - 13s - loss: 1.5289e-04 - val_loss: 1.4195e-04 - 13s/epoch - 155us/sample
Epoch 34/80
84077/84077 - 13s - loss: 1.5196e-04 - val_loss: 1.4099e-04 - 13s/epoch - 154us/sample
Epoch 35/80
84077/84077 - 13s - loss: 1.5101e-04 - val_loss: 1.3895e-04 - 13s/epoch - 159us/sample
Epoch 36/80
84077/84077 - 13s - loss: 1.4976e-04 - val_loss: 1.3727e-04 - 13s/epoch - 155us/sample
Epoch 37/80
84077/84077 - 13s - loss: 1.4881e-04 - val_loss: 1.3824e-04 - 13s/epoch - 154us/sample
Epoch 38/80
84077/84077 - 13s - loss: 1.4835e-04 - val_loss: 1.3707e-04 - 13s/epoch - 155us/sample
Epoch 39/80
84077/84077 - 13s - loss: 1.4806e-04 - val_loss: 1.3803e-04 - 13s/epoch - 154us/sample
Epoch 40/80
84077/84077 - 13s - loss: 1.4688e-04 - val_loss: 1.3712e-04 - 13s/epoch - 160us/sample
Epoch 41/80
84077/84077 - 13s - loss: 1.4631e-04 - val_loss: 1.3549e-04 - 13s/epoch - 155us/sample
Epoch 42/80
84077/84077 - 13s - loss: 1.4588e-04 - val_loss: 1.3559e-04 - 13s/epoch - 155us/sample
Epoch 43/80
84077/84077 - 13s - loss: 1.4490e-04 - val_loss: 1.3535e-04 - 13s/epoch - 155us/sample
Epoch 44/80
84077/84077 - 13s - loss: 1.4426e-04 - val_loss: 1.3597e-04 - 13s/epoch - 154us/sample
Epoch 45/80
84077/84077 - 13s - loss: 1.4407e-04 - val_loss: 1.3436e-04 - 13s/epoch - 157us/sample
Epoch 46/80
84077/84077 - 13s - loss: 1.4351e-04 - val_loss: 1.3413e-04 - 13s/epoch - 156us/sample
Epoch 47/80
84077/84077 - 13s - loss: 1.4333e-04 - val_loss: 1.3328e-04 - 13s/epoch - 155us/sample
Epoch 48/80
84077/84077 - 13s - loss: 1.4248e-04 - val_loss: 1.3417e-04 - 13s/epoch - 155us/sample
Epoch 49/80
84077/84077 - 13s - loss: 1.4215e-04 - val_loss: 1.3326e-04 - 13s/epoch - 155us/sample
Epoch 50/80
84077/84077 - 13s - loss: 1.4154e-04 - val_loss: 1.3497e-04 - 13s/epoch - 156us/sample
Epoch 51/80
84077/84077 - 13s - loss: 1.4135e-04 - val_loss: 1.3494e-04 - 13s/epoch - 158us/sample
Epoch 52/80
84077/84077 - 13s - loss: 1.4073e-04 - val_loss: 1.3314e-04 - 13s/epoch - 154us/sample
Epoch 53/80
84077/84077 - 13s - loss: 1.4049e-04 - val_loss: 1.3203e-04 - 13s/epoch - 154us/sample
Epoch 54/80
84077/84077 - 13s - loss: 1.4028e-04 - val_loss: 1.3233e-04 - 13s/epoch - 155us/sample
Epoch 55/80
84077/84077 - 13s - loss: 1.3972e-04 - val_loss: 1.3102e-04 - 13s/epoch - 155us/sample
Epoch 56/80
84077/84077 - 13s - loss: 1.3924e-04 - val_loss: 1.3156e-04 - 13s/epoch - 160us/sample
Epoch 57/80
84077/84077 - 13s - loss: 1.3901e-04 - val_loss: 1.3063e-04 - 13s/epoch - 154us/sample
Epoch 58/80
84077/84077 - 13s - loss: 1.3892e-04 - val_loss: 1.3088e-04 - 13s/epoch - 154us/sample
Epoch 59/80
84077/84077 - 13s - loss: 1.3847e-04 - val_loss: 1.3079e-04 - 13s/epoch - 154us/sample
Epoch 60/80
84077/84077 - 13s - loss: 1.3824e-04 - val_loss: 1.3127e-04 - 13s/epoch - 155us/sample
Epoch 61/80
84077/84077 - 13s - loss: 1.3785e-04 - val_loss: 1.2974e-04 - 13s/epoch - 159us/sample
Epoch 62/80
84077/84077 - 13s - loss: 1.3746e-04 - val_loss: 1.3024e-04 - 13s/epoch - 155us/sample
Epoch 63/80
84077/84077 - 13s - loss: 1.3755e-04 - val_loss: 1.2927e-04 - 13s/epoch - 154us/sample
Epoch 64/80
84077/84077 - 13s - loss: 1.3696e-04 - val_loss: 1.3032e-04 - 13s/epoch - 154us/sample
Epoch 65/80
84077/84077 - 13s - loss: 1.3686e-04 - val_loss: 1.2957e-04 - 13s/epoch - 154us/sample
Epoch 66/80
84077/84077 - 14s - loss: 1.3649e-04 - val_loss: 1.3020e-04 - 14s/epoch - 161us/sample
Epoch 67/80
84077/84077 - 13s - loss: 1.3646e-04 - val_loss: 1.2937e-04 - 13s/epoch - 155us/sample
Epoch 68/80
84077/84077 - 13s - loss: 1.3590e-04 - val_loss: 1.2905e-04 - 13s/epoch - 155us/sample
Epoch 69/80
84077/84077 - 13s - loss: 1.3596e-04 - val_loss: 1.2980e-04 - 13s/epoch - 154us/sample
Epoch 70/80
84077/84077 - 13s - loss: 1.3555e-04 - val_loss: 1.2833e-04 - 13s/epoch - 153us/sample
Epoch 71/80
84077/84077 - 13s - loss: 1.3543e-04 - val_loss: 1.2746e-04 - 13s/epoch - 159us/sample
Epoch 72/80
84077/84077 - 13s - loss: 1.3549e-04 - val_loss: 1.2766e-04 - 13s/epoch - 155us/sample
Epoch 73/80
84077/84077 - 13s - loss: 1.3464e-04 - val_loss: 1.2694e-04 - 13s/epoch - 154us/sample
Epoch 74/80
84077/84077 - 13s - loss: 1.3512e-04 - val_loss: 1.2775e-04 - 13s/epoch - 154us/sample
Epoch 75/80
84077/84077 - 13s - loss: 1.3481e-04 - val_loss: 1.2977e-04 - 13s/epoch - 154us/sample
Epoch 76/80
84077/84077 - 13s - loss: 1.3423e-04 - val_loss: 1.2630e-04 - 13s/epoch - 159us/sample
Epoch 77/80
84077/84077 - 13s - loss: 1.3427e-04 - val_loss: 1.2943e-04 - 13s/epoch - 155us/sample
Epoch 78/80
84077/84077 - 13s - loss: 1.3411e-04 - val_loss: 1.2647e-04 - 13s/epoch - 155us/sample
Epoch 79/80
84077/84077 - 13s - loss: 1.3384e-04 - val_loss: 1.2728e-04 - 13s/epoch - 154us/sample
Epoch 80/80
84077/84077 - 13s - loss: 1.3332e-04 - val_loss: 1.2600e-04 - 13s/epoch - 154us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012599537340978774
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 04:31:58.887611: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_76/outputlayer/BiasAdd' id:96055 op device:{requested: '', assigned: ''} def:{{{node decoder_model_76/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_76/outputlayer/MatMul, decoder_model_76/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031800693296669236
cosine 0.03141478811368882
MAE: 0.002247936212733801
RMSE: 0.010024461226393411
r2: 0.9218194659485438
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'logcosh', 64, 80, 0.0008, 0.2, 188, 0.00013331896528687582, 0.00012599537340978774, 0.031800693296669236, 0.03141478811368882, 0.002247936212733801, 0.010024461226393411, 0.9218194659485438, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 9
Fitness    = 515.1509017593391
Last generation's best solutions = [2.0 85 0.0012000000000000001 64 1] with fitness 515.1509017593391.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 85, 0.0012000000000000001, 64, 1], dtype=object), array([2.0, 85, 0.0012000000000000001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 515.1509017593391]
[2.1 90 0.0012000000000000001 8 2] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_228 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_228 (ReLU)               (None, 1980)         0           ['batch_normalization_228[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          372428      ['re_lu_228[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          372428      ['re_lu_228[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2286507     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,908,403
Trainable params: 4,900,107
Non-trainable params: 8,296
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 04:32:45.140230: W tensorflow/c/c_api.cc:291] Operation '{name:'training_150/Adam/batch_normalization_228/gamma/m/Assign' id:97866 op device:{requested: '', assigned: ''} def:{{{node training_150/Adam/batch_normalization_228/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_150/Adam/batch_normalization_228/gamma/m, training_150/Adam/batch_normalization_228/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 04:33:49.397488: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_77/mul' id:97376 op device:{requested: '', assigned: ''} def:{{{node loss_77/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_77/mul/x, loss_77/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 86s - loss: 0.0021 - val_loss: 7.0117e-04 - 86s/epoch - 1ms/sample
Epoch 2/90
84077/84077 - 58s - loss: 6.1418e-04 - val_loss: 5.0843e-04 - 58s/epoch - 685us/sample
Epoch 3/90
84077/84077 - 56s - loss: 4.9998e-04 - val_loss: 0.0022 - 56s/epoch - 666us/sample
Epoch 4/90
84077/84077 - 54s - loss: 4.6668e-04 - val_loss: 0.0051 - 54s/epoch - 646us/sample
Epoch 5/90
84077/84077 - 54s - loss: 4.4698e-04 - val_loss: 0.0112 - 54s/epoch - 645us/sample
Epoch 6/90
84077/84077 - 54s - loss: 4.3623e-04 - val_loss: 0.0169 - 54s/epoch - 644us/sample
Epoch 7/90
84077/84077 - 54s - loss: 4.3073e-04 - val_loss: 0.0168 - 54s/epoch - 645us/sample
Epoch 8/90
84077/84077 - 54s - loss: 4.2593e-04 - val_loss: 0.0258 - 54s/epoch - 645us/sample
Epoch 9/90
84077/84077 - 54s - loss: 4.2304e-04 - val_loss: 0.0159 - 54s/epoch - 642us/sample
Epoch 10/90
84077/84077 - 54s - loss: 4.2043e-04 - val_loss: 0.0213 - 54s/epoch - 643us/sample
Epoch 11/90
84077/84077 - 54s - loss: 4.1730e-04 - val_loss: 0.0249 - 54s/epoch - 645us/sample
Epoch 12/90
84077/84077 - 54s - loss: 4.1471e-04 - val_loss: 0.0269 - 54s/epoch - 644us/sample
Epoch 13/90
84077/84077 - 54s - loss: 4.1253e-04 - val_loss: 0.0273 - 54s/epoch - 645us/sample
Epoch 14/90
84077/84077 - 54s - loss: 4.1021e-04 - val_loss: 0.0283 - 54s/epoch - 646us/sample
Epoch 15/90
84077/84077 - 55s - loss: 4.0738e-04 - val_loss: 0.0235 - 55s/epoch - 650us/sample
Epoch 16/90
84077/84077 - 54s - loss: 4.0517e-04 - val_loss: 0.0215 - 54s/epoch - 642us/sample
Epoch 17/90
84077/84077 - 54s - loss: 4.0358e-04 - val_loss: 0.0261 - 54s/epoch - 646us/sample
Epoch 18/90
84077/84077 - 54s - loss: 4.0197e-04 - val_loss: 0.0250 - 54s/epoch - 648us/sample
Epoch 19/90
84077/84077 - 54s - loss: 4.0056e-04 - val_loss: 0.0235 - 54s/epoch - 642us/sample
Epoch 20/90
84077/84077 - 54s - loss: 3.9922e-04 - val_loss: 0.0228 - 54s/epoch - 647us/sample
Epoch 21/90
84077/84077 - 54s - loss: 3.9739e-04 - val_loss: 0.0217 - 54s/epoch - 648us/sample
Epoch 22/90
84077/84077 - 54s - loss: 3.9635e-04 - val_loss: 0.0210 - 54s/epoch - 642us/sample
Epoch 23/90
84077/84077 - 54s - loss: 3.9440e-04 - val_loss: 0.0243 - 54s/epoch - 647us/sample
Epoch 24/90
84077/84077 - 54s - loss: 3.9308e-04 - val_loss: 0.0223 - 54s/epoch - 648us/sample
Epoch 25/90
84077/84077 - 55s - loss: 3.9243e-04 - val_loss: 0.0216 - 55s/epoch - 656us/sample
Epoch 26/90
84077/84077 - 57s - loss: 3.9187e-04 - val_loss: 0.0245 - 57s/epoch - 683us/sample
Epoch 27/90
84077/84077 - 57s - loss: 3.8943e-04 - val_loss: 0.0223 - 57s/epoch - 684us/sample
Epoch 28/90
84077/84077 - 57s - loss: 3.8841e-04 - val_loss: 0.0218 - 57s/epoch - 678us/sample
Epoch 29/90
84077/84077 - 57s - loss: 3.8646e-04 - val_loss: 0.0210 - 57s/epoch - 682us/sample
Epoch 30/90
84077/84077 - 57s - loss: 3.8534e-04 - val_loss: 0.0210 - 57s/epoch - 683us/sample
Epoch 31/90
84077/84077 - 58s - loss: 3.8346e-04 - val_loss: 0.0227 - 58s/epoch - 686us/sample
Epoch 32/90
84077/84077 - 57s - loss: 3.8328e-04 - val_loss: 0.0198 - 57s/epoch - 679us/sample
Epoch 33/90
84077/84077 - 57s - loss: 3.8180e-04 - val_loss: 0.0205 - 57s/epoch - 683us/sample
Epoch 34/90
84077/84077 - 56s - loss: 3.8054e-04 - val_loss: 0.0187 - 56s/epoch - 663us/sample
Epoch 35/90
84077/84077 - 56s - loss: 3.7915e-04 - val_loss: 0.0207 - 56s/epoch - 664us/sample
Epoch 36/90
84077/84077 - 56s - loss: 3.7845e-04 - val_loss: 0.0202 - 56s/epoch - 671us/sample
Epoch 37/90
84077/84077 - 56s - loss: 3.7717e-04 - val_loss: 0.0191 - 56s/epoch - 669us/sample
Epoch 38/90
84077/84077 - 56s - loss: 3.7729e-04 - val_loss: 0.0196 - 56s/epoch - 667us/sample
Epoch 39/90
84077/84077 - 56s - loss: 3.7614e-04 - val_loss: 0.0218 - 56s/epoch - 668us/sample
Epoch 40/90
84077/84077 - 56s - loss: 3.7568e-04 - val_loss: 0.0221 - 56s/epoch - 670us/sample
Epoch 41/90
84077/84077 - 56s - loss: 3.7496e-04 - val_loss: 0.0206 - 56s/epoch - 670us/sample
Epoch 42/90
84077/84077 - 56s - loss: 3.7477e-04 - val_loss: 0.0227 - 56s/epoch - 667us/sample
Epoch 43/90
84077/84077 - 56s - loss: 3.7428e-04 - val_loss: 0.0210 - 56s/epoch - 671us/sample
Epoch 44/90
84077/84077 - 56s - loss: 3.7385e-04 - val_loss: 0.0229 - 56s/epoch - 667us/sample
Epoch 45/90
84077/84077 - 56s - loss: 3.7332e-04 - val_loss: 0.0215 - 56s/epoch - 668us/sample
Epoch 46/90
84077/84077 - 56s - loss: 3.7263e-04 - val_loss: 0.0214 - 56s/epoch - 664us/sample
Epoch 47/90
84077/84077 - 56s - loss: 3.7255e-04 - val_loss: 0.0202 - 56s/epoch - 662us/sample
Epoch 48/90
84077/84077 - 55s - loss: 3.7229e-04 - val_loss: 0.0198 - 55s/epoch - 657us/sample
Epoch 49/90
84077/84077 - 56s - loss: 3.7167e-04 - val_loss: 0.0221 - 56s/epoch - 663us/sample
Epoch 50/90
84077/84077 - 56s - loss: 3.7129e-04 - val_loss: 0.0196 - 56s/epoch - 660us/sample
Epoch 51/90
84077/84077 - 55s - loss: 3.7137e-04 - val_loss: 0.0207 - 55s/epoch - 658us/sample
Epoch 52/90
84077/84077 - 56s - loss: 3.6976e-04 - val_loss: 0.0224 - 56s/epoch - 666us/sample
Epoch 53/90
84077/84077 - 56s - loss: 3.7031e-04 - val_loss: 0.0207 - 56s/epoch - 663us/sample
Epoch 54/90
84077/84077 - 56s - loss: 3.6966e-04 - val_loss: 0.0196 - 56s/epoch - 660us/sample
Epoch 55/90
84077/84077 - 55s - loss: 3.6872e-04 - val_loss: 0.0181 - 55s/epoch - 655us/sample
Epoch 56/90
84077/84077 - 56s - loss: 3.6941e-04 - val_loss: 0.0182 - 56s/epoch - 661us/sample
Epoch 57/90
84077/84077 - 56s - loss: 3.6836e-04 - val_loss: 0.0202 - 56s/epoch - 660us/sample
Epoch 58/90
84077/84077 - 55s - loss: 3.6839e-04 - val_loss: 0.0196 - 55s/epoch - 656us/sample
Epoch 59/90
84077/84077 - 55s - loss: 3.6704e-04 - val_loss: 0.0218 - 55s/epoch - 658us/sample
Epoch 60/90
84077/84077 - 56s - loss: 3.6737e-04 - val_loss: 0.0195 - 56s/epoch - 660us/sample
Epoch 61/90
84077/84077 - 55s - loss: 3.6675e-04 - val_loss: 0.0208 - 55s/epoch - 656us/sample
Epoch 62/90
84077/84077 - 55s - loss: 3.6642e-04 - val_loss: 0.0196 - 55s/epoch - 659us/sample
Epoch 63/90
84077/84077 - 55s - loss: 3.6572e-04 - val_loss: 0.0209 - 55s/epoch - 660us/sample
Epoch 64/90
84077/84077 - 55s - loss: 3.6565e-04 - val_loss: 0.0173 - 55s/epoch - 658us/sample
Epoch 65/90
84077/84077 - 55s - loss: 3.6585e-04 - val_loss: 0.0190 - 55s/epoch - 660us/sample
Epoch 66/90
84077/84077 - 56s - loss: 3.6524e-04 - val_loss: 0.0196 - 56s/epoch - 661us/sample
Epoch 67/90
84077/84077 - 55s - loss: 3.6446e-04 - val_loss: 0.0201 - 55s/epoch - 655us/sample
Epoch 68/90
84077/84077 - 55s - loss: 3.6350e-04 - val_loss: 0.0191 - 55s/epoch - 656us/sample
Epoch 69/90
84077/84077 - 56s - loss: 3.6332e-04 - val_loss: 0.0161 - 56s/epoch - 661us/sample
Epoch 70/90
84077/84077 - 55s - loss: 3.6357e-04 - val_loss: 0.0172 - 55s/epoch - 656us/sample
Epoch 71/90
84077/84077 - 55s - loss: 3.6330e-04 - val_loss: 0.0174 - 55s/epoch - 660us/sample
Epoch 72/90
84077/84077 - 56s - loss: 3.6346e-04 - val_loss: 0.0181 - 56s/epoch - 664us/sample
Epoch 73/90
84077/84077 - 55s - loss: 3.6328e-04 - val_loss: 0.0175 - 55s/epoch - 657us/sample
Epoch 74/90
84077/84077 - 56s - loss: 3.6209e-04 - val_loss: 0.0183 - 56s/epoch - 662us/sample
Epoch 75/90
84077/84077 - 56s - loss: 3.6172e-04 - val_loss: 0.0177 - 56s/epoch - 661us/sample
Epoch 76/90
84077/84077 - 55s - loss: 3.6147e-04 - val_loss: 0.0168 - 55s/epoch - 660us/sample
Epoch 77/90
84077/84077 - 56s - loss: 3.6147e-04 - val_loss: 0.0166 - 56s/epoch - 661us/sample
Epoch 78/90
84077/84077 - 56s - loss: 3.6123e-04 - val_loss: 0.0170 - 56s/epoch - 663us/sample
Epoch 79/90
84077/84077 - 56s - loss: 3.6127e-04 - val_loss: 0.0184 - 56s/epoch - 664us/sample
Epoch 80/90
84077/84077 - 55s - loss: 3.6122e-04 - val_loss: 0.0175 - 55s/epoch - 657us/sample
Epoch 81/90
84077/84077 - 56s - loss: 3.6050e-04 - val_loss: 0.0156 - 56s/epoch - 662us/sample
Epoch 82/90
84077/84077 - 56s - loss: 3.6103e-04 - val_loss: 0.0161 - 56s/epoch - 663us/sample
Epoch 83/90
84077/84077 - 55s - loss: 3.6010e-04 - val_loss: 0.0167 - 55s/epoch - 660us/sample
Epoch 84/90
84077/84077 - 56s - loss: 3.5972e-04 - val_loss: 0.0178 - 56s/epoch - 665us/sample
Epoch 85/90
84077/84077 - 56s - loss: 3.6016e-04 - val_loss: 0.0169 - 56s/epoch - 664us/sample
Epoch 86/90
84077/84077 - 55s - loss: 3.6004e-04 - val_loss: 0.0161 - 55s/epoch - 658us/sample
Epoch 87/90
84077/84077 - 56s - loss: 3.5923e-04 - val_loss: 0.0152 - 56s/epoch - 662us/sample
Epoch 88/90
84077/84077 - 56s - loss: 3.5889e-04 - val_loss: 0.0178 - 56s/epoch - 666us/sample
Epoch 89/90
84077/84077 - 56s - loss: 3.5931e-04 - val_loss: 0.0164 - 56s/epoch - 668us/sample
Epoch 90/90
84077/84077 - 57s - loss: 3.5907e-04 - val_loss: 0.0167 - 57s/epoch - 684us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.016733153531983517
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 05:56:23.999570: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_77/outputlayer/BiasAdd' id:97340 op device:{requested: '', assigned: ''} def:{{{node decoder_model_77/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_77/outputlayer/MatMul, decoder_model_77/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.22652855091032903
cosine 0.2241470348367711
MAE: 0.03536688631234848
RMSE: 0.45964071251318944
r2: -164.0343504383958
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 8, 90, 0.0012000000000000001, 0.2, 188, 0.0003590685362086571, 0.016733153531983517, 0.22652855091032903, 0.2241470348367711, 0.03536688631234848, 0.45964071251318944, -164.0343504383958, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 90 0.0008 64 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_231 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_231 (ReLU)               (None, 2168)         0           ['batch_normalization_231[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_231[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_231[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 05:57:11.351637: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_78/kernel/Assign' id:98533 op device:{requested: '', assigned: ''} def:{{{node outputlayer_78/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_78/kernel, outputlayer_78/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 05:57:37.494574: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_78/mul' id:98673 op device:{requested: '', assigned: ''} def:{{{node loss_78/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_78/mul/x, loss_78/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 46s - loss: 0.0700 - val_loss: 0.0677 - 46s/epoch - 545us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0670 - val_loss: 0.0675 - 14s/epoch - 165us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 166us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 166us/sample
Epoch 5/90
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 167us/sample
Epoch 6/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0674 - 14s/epoch - 171us/sample
Epoch 7/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0674 - 14s/epoch - 166us/sample
Epoch 8/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0674 - 14s/epoch - 167us/sample
Epoch 9/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 10/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 11/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 12/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 13/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 14/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 15/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 171us/sample
Epoch 16/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 17/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 18/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 19/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 20/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 21/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 22/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 23/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 24/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 25/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 171us/sample
Epoch 26/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 27/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 28/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 29/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 30/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 171us/sample
Epoch 31/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 32/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 33/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 34/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 35/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 36/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 37/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 38/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 39/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 40/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 41/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 42/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 43/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 44/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 45/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 46/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 47/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 48/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 49/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 50/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 51/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 52/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 53/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 54/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 55/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 56/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 57/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 58/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 59/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 60/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 61/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 62/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 63/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 64/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 65/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 66/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 67/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 68/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 171us/sample
Epoch 69/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 70/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 71/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 72/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 73/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 171us/sample
Epoch 74/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 75/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 76/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 77/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 78/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 79/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 80/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 81/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 82/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 83/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 84/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 85/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 86/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 87/90
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 88/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 89/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 166us/sample
Epoch 90/90
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734574807150234
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 06:18:41.788196: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_78/outputlayer/BiasAdd' id:98625 op device:{requested: '', assigned: ''} def:{{{node decoder_model_78/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_78/outputlayer/MatMul, decoder_model_78/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.0644715739897905
cosine 1.1216266330352012
MAE: 5.910000100090628
RMSE: 6.038141905508381
r2: -26711.973071946308
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'binary_crossentropy', 64, 90, 0.0008, 0.2, 188, 0.06683692408012193, 0.06734574807150234, 1.0644715739897905, 1.1216266330352012, 5.910000100090628, 6.038141905508381, -26711.973071946308, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4000000000000004 80 0.0006000000000000001 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2263)         2136272     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_234 (Batch  (None, 2263)        9052        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_234 (ReLU)               (None, 2263)         0           ['batch_normalization_234[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          425632      ['re_lu_234[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          425632      ['re_lu_234[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2607995     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,604,583
Trainable params: 5,595,155
Non-trainable params: 9,428
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-12 06:19:31.974855: W tensorflow/c/c_api.cc:291] Operation '{name:'training_154/Adam/dense_dec0_79/bias/m/Assign' id:100558 op device:{requested: '', assigned: ''} def:{{{node training_154/Adam/dense_dec0_79/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_154/Adam/dense_dec0_79/bias/m, training_154/Adam/dense_dec0_79/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 06:19:58.241833: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_79/mul' id:99998 op device:{requested: '', assigned: ''} def:{{{node loss_79/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_79/mul/x, loss_79/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 47s - loss: 0.0056 - val_loss: 0.0011 - 47s/epoch - 555us/sample
Epoch 2/80
84077/84077 - 14s - loss: 0.0018 - val_loss: 0.0012 - 14s/epoch - 164us/sample
Epoch 3/80
84077/84077 - 14s - loss: 9.1633e-04 - val_loss: 8.3511e-04 - 14s/epoch - 164us/sample
Epoch 4/80
84077/84077 - 14s - loss: 9.8556e-04 - val_loss: 5.9196e-04 - 14s/epoch - 165us/sample
Epoch 5/80
84077/84077 - 14s - loss: 5.2922e-04 - val_loss: 4.6036e-04 - 14s/epoch - 164us/sample
Epoch 6/80
84077/84077 - 13s - loss: 4.5832e-04 - val_loss: 4.1098e-04 - 13s/epoch - 157us/sample
Epoch 7/80
84077/84077 - 12s - loss: 4.2887e-04 - val_loss: 3.5983e-04 - 12s/epoch - 147us/sample
Epoch 8/80
84077/84077 - 13s - loss: 4.1314e-04 - val_loss: 3.3027e-04 - 13s/epoch - 156us/sample
Epoch 9/80
84077/84077 - 13s - loss: 4.0959e-04 - val_loss: 3.0652e-04 - 13s/epoch - 158us/sample
Epoch 10/80
84077/84077 - 13s - loss: 3.2203e-04 - val_loss: 2.8206e-04 - 13s/epoch - 158us/sample
Epoch 11/80
84077/84077 - 14s - loss: 2.9612e-04 - val_loss: 2.5826e-04 - 14s/epoch - 164us/sample
Epoch 12/80
84077/84077 - 13s - loss: 2.7614e-04 - val_loss: 2.4607e-04 - 13s/epoch - 158us/sample
Epoch 13/80
84077/84077 - 13s - loss: 2.5868e-04 - val_loss: 2.2773e-04 - 13s/epoch - 159us/sample
Epoch 14/80
84077/84077 - 13s - loss: 2.4399e-04 - val_loss: 2.1925e-04 - 13s/epoch - 158us/sample
Epoch 15/80
84077/84077 - 13s - loss: 2.3374e-04 - val_loss: 2.0674e-04 - 13s/epoch - 159us/sample
Epoch 16/80
84077/84077 - 14s - loss: 2.2140e-04 - val_loss: 1.9518e-04 - 14s/epoch - 164us/sample
Epoch 17/80
84077/84077 - 13s - loss: 2.1314e-04 - val_loss: 1.8740e-04 - 13s/epoch - 159us/sample
Epoch 18/80
84077/84077 - 13s - loss: 2.0388e-04 - val_loss: 1.8031e-04 - 13s/epoch - 159us/sample
Epoch 19/80
84077/84077 - 13s - loss: 1.9630e-04 - val_loss: 1.7308e-04 - 13s/epoch - 158us/sample
Epoch 20/80
84077/84077 - 13s - loss: 1.9093e-04 - val_loss: 1.6828e-04 - 13s/epoch - 157us/sample
Epoch 21/80
84077/84077 - 14s - loss: 1.8514e-04 - val_loss: 1.6525e-04 - 14s/epoch - 164us/sample
Epoch 22/80
84077/84077 - 13s - loss: 1.8147e-04 - val_loss: 1.6060e-04 - 13s/epoch - 159us/sample
Epoch 23/80
84077/84077 - 13s - loss: 1.7748e-04 - val_loss: 1.5903e-04 - 13s/epoch - 159us/sample
Epoch 24/80
84077/84077 - 13s - loss: 1.7442e-04 - val_loss: 1.5507e-04 - 13s/epoch - 158us/sample
Epoch 25/80
84077/84077 - 13s - loss: 1.7277e-04 - val_loss: 1.5507e-04 - 13s/epoch - 158us/sample
Epoch 26/80
84077/84077 - 14s - loss: 1.7035e-04 - val_loss: 1.5210e-04 - 14s/epoch - 163us/sample
Epoch 27/80
84077/84077 - 13s - loss: 1.6831e-04 - val_loss: 1.5148e-04 - 13s/epoch - 159us/sample
Epoch 28/80
84077/84077 - 13s - loss: 1.6651e-04 - val_loss: 1.4995e-04 - 13s/epoch - 159us/sample
Epoch 29/80
84077/84077 - 13s - loss: 1.6539e-04 - val_loss: 1.4671e-04 - 13s/epoch - 158us/sample
Epoch 30/80
84077/84077 - 13s - loss: 1.6319e-04 - val_loss: 1.4953e-04 - 13s/epoch - 159us/sample
Epoch 31/80
84077/84077 - 14s - loss: 1.6208e-04 - val_loss: 1.4499e-04 - 14s/epoch - 163us/sample
Epoch 32/80
84077/84077 - 13s - loss: 1.6061e-04 - val_loss: 1.4498e-04 - 13s/epoch - 159us/sample
Epoch 33/80
84077/84077 - 13s - loss: 1.5949e-04 - val_loss: 1.4354e-04 - 13s/epoch - 158us/sample
Epoch 34/80
84077/84077 - 13s - loss: 1.5814e-04 - val_loss: 1.4286e-04 - 13s/epoch - 159us/sample
Epoch 35/80
84077/84077 - 13s - loss: 1.5808e-04 - val_loss: 1.4284e-04 - 13s/epoch - 159us/sample
Epoch 36/80
84077/84077 - 14s - loss: 1.5672e-04 - val_loss: 1.4186e-04 - 14s/epoch - 162us/sample
Epoch 37/80
84077/84077 - 13s - loss: 1.5570e-04 - val_loss: 1.4133e-04 - 13s/epoch - 159us/sample
Epoch 38/80
84077/84077 - 13s - loss: 1.5447e-04 - val_loss: 1.4081e-04 - 13s/epoch - 158us/sample
Epoch 39/80
84077/84077 - 13s - loss: 1.5421e-04 - val_loss: 1.4069e-04 - 13s/epoch - 158us/sample
Epoch 40/80
84077/84077 - 13s - loss: 1.5358e-04 - val_loss: 1.4098e-04 - 13s/epoch - 159us/sample
Epoch 41/80
84077/84077 - 14s - loss: 1.5238e-04 - val_loss: 1.3930e-04 - 14s/epoch - 163us/sample
Epoch 42/80
84077/84077 - 13s - loss: 1.5204e-04 - val_loss: 1.3931e-04 - 13s/epoch - 159us/sample
Epoch 43/80
84077/84077 - 13s - loss: 1.5116e-04 - val_loss: 1.3803e-04 - 13s/epoch - 159us/sample
Epoch 44/80
84077/84077 - 13s - loss: 1.5002e-04 - val_loss: 1.3660e-04 - 13s/epoch - 159us/sample
Epoch 45/80
84077/84077 - 13s - loss: 1.5001e-04 - val_loss: 1.3766e-04 - 13s/epoch - 158us/sample
Epoch 46/80
84077/84077 - 14s - loss: 1.4948e-04 - val_loss: 1.3769e-04 - 14s/epoch - 165us/sample
Epoch 47/80
84077/84077 - 13s - loss: 1.4866e-04 - val_loss: 1.3608e-04 - 13s/epoch - 159us/sample
Epoch 48/80
84077/84077 - 13s - loss: 1.4822e-04 - val_loss: 1.3499e-04 - 13s/epoch - 159us/sample
Epoch 49/80
84077/84077 - 13s - loss: 1.4753e-04 - val_loss: 1.3527e-04 - 13s/epoch - 159us/sample
Epoch 50/80
84077/84077 - 13s - loss: 1.4711e-04 - val_loss: 1.3506e-04 - 13s/epoch - 158us/sample
Epoch 51/80
84077/84077 - 14s - loss: 1.4664e-04 - val_loss: 1.3492e-04 - 14s/epoch - 161us/sample
Epoch 52/80
84077/84077 - 14s - loss: 1.4571e-04 - val_loss: 1.3501e-04 - 14s/epoch - 161us/sample
Epoch 53/80
84077/84077 - 13s - loss: 1.4596e-04 - val_loss: 1.3572e-04 - 13s/epoch - 159us/sample
Epoch 54/80
84077/84077 - 13s - loss: 1.4577e-04 - val_loss: 1.3445e-04 - 13s/epoch - 158us/sample
Epoch 55/80
84077/84077 - 13s - loss: 1.4517e-04 - val_loss: 1.3373e-04 - 13s/epoch - 158us/sample
Epoch 56/80
84077/84077 - 14s - loss: 1.4435e-04 - val_loss: 1.3358e-04 - 14s/epoch - 161us/sample
Epoch 57/80
84077/84077 - 13s - loss: 1.4421e-04 - val_loss: 1.3346e-04 - 13s/epoch - 160us/sample
Epoch 58/80
84077/84077 - 13s - loss: 1.4370e-04 - val_loss: 1.3236e-04 - 13s/epoch - 159us/sample
Epoch 59/80
84077/84077 - 13s - loss: 1.4361e-04 - val_loss: 1.3257e-04 - 13s/epoch - 159us/sample
Epoch 60/80
84077/84077 - 13s - loss: 1.4286e-04 - val_loss: 1.3168e-04 - 13s/epoch - 158us/sample
Epoch 61/80
84077/84077 - 13s - loss: 1.4246e-04 - val_loss: 1.3195e-04 - 13s/epoch - 159us/sample
Epoch 62/80
84077/84077 - 14s - loss: 1.4262e-04 - val_loss: 1.3225e-04 - 14s/epoch - 163us/sample
Epoch 63/80
84077/84077 - 13s - loss: 1.4253e-04 - val_loss: 1.3224e-04 - 13s/epoch - 158us/sample
Epoch 64/80
84077/84077 - 13s - loss: 1.4154e-04 - val_loss: 1.3148e-04 - 13s/epoch - 159us/sample
Epoch 65/80
84077/84077 - 13s - loss: 1.4155e-04 - val_loss: 1.3055e-04 - 13s/epoch - 159us/sample
Epoch 66/80
84077/84077 - 13s - loss: 1.4127e-04 - val_loss: 1.3156e-04 - 13s/epoch - 159us/sample
Epoch 67/80
84077/84077 - 14s - loss: 1.4115e-04 - val_loss: 1.3007e-04 - 14s/epoch - 162us/sample
Epoch 68/80
84077/84077 - 13s - loss: 1.4029e-04 - val_loss: 1.2890e-04 - 13s/epoch - 159us/sample
Epoch 69/80
84077/84077 - 13s - loss: 1.4035e-04 - val_loss: 1.3137e-04 - 13s/epoch - 158us/sample
Epoch 70/80
84077/84077 - 13s - loss: 1.4012e-04 - val_loss: 1.2985e-04 - 13s/epoch - 158us/sample
Epoch 71/80
84077/84077 - 13s - loss: 1.4007e-04 - val_loss: 1.2965e-04 - 13s/epoch - 159us/sample
Epoch 72/80
84077/84077 - 14s - loss: 1.3959e-04 - val_loss: 1.3101e-04 - 14s/epoch - 162us/sample
Epoch 73/80
84077/84077 - 13s - loss: 1.3940e-04 - val_loss: 1.2939e-04 - 13s/epoch - 159us/sample
Epoch 74/80
84077/84077 - 13s - loss: 1.3891e-04 - val_loss: 1.3012e-04 - 13s/epoch - 159us/sample
Epoch 75/80
84077/84077 - 13s - loss: 1.3825e-04 - val_loss: 1.2803e-04 - 13s/epoch - 157us/sample
Epoch 76/80
84077/84077 - 13s - loss: 1.3879e-04 - val_loss: 1.2932e-04 - 13s/epoch - 159us/sample
Epoch 77/80
84077/84077 - 14s - loss: 1.3830e-04 - val_loss: 1.2954e-04 - 14s/epoch - 164us/sample
Epoch 78/80
84077/84077 - 13s - loss: 1.3821e-04 - val_loss: 1.2917e-04 - 13s/epoch - 159us/sample
Epoch 79/80
84077/84077 - 13s - loss: 1.3805e-04 - val_loss: 1.2789e-04 - 13s/epoch - 159us/sample
Epoch 80/80
84077/84077 - 13s - loss: 1.3824e-04 - val_loss: 1.2983e-04 - 13s/epoch - 158us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012982684853209573
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 06:37:48.106307: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_79/outputlayer/BiasAdd' id:99962 op device:{requested: '', assigned: ''} def:{{{node decoder_model_79/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_79/outputlayer/MatMul, decoder_model_79/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03182586885688839
cosine 0.03143477067923326
MAE: 0.002432055883331891
RMSE: 0.010362557082468946
r2: 0.9162748386200117
RMSE zero-vector: 0.04004287452915337
['2.4000000000000004custom_VAE', 'logcosh', 64, 80, 0.0006000000000000001, 0.2, 188, 0.0001382365218349903, 0.00012982684853209573, 0.03182586885688839, 0.03143477067923326, 0.002432055883331891, 0.010362557082468946, 0.9162748386200117, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 85 0.0014000000000000002 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_237 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_237 (ReLU)               (None, 2168)         0           ['batch_normalization_237[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_237[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_237[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 06:38:38.462757: W tensorflow/c/c_api.cc:291] Operation '{name:'training_156/Adam/batch_normalization_239/beta/v/Assign' id:101971 op device:{requested: '', assigned: ''} def:{{{node training_156/Adam/batch_normalization_239/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_156/Adam/batch_normalization_239/beta/v, training_156/Adam/batch_normalization_239/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 06:39:04.806801: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_80/mul' id:101286 op device:{requested: '', assigned: ''} def:{{{node loss_80/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_80/mul/x, loss_80/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 47s - loss: 0.0430 - val_loss: 0.0010 - 47s/epoch - 557us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.6073 - val_loss: 9.1958e-04 - 13s/epoch - 160us/sample
Epoch 3/85
84077/84077 - 13s - loss: 8.0517e-04 - val_loss: 9.8342e-04 - 13s/epoch - 160us/sample
Epoch 4/85
84077/84077 - 13s - loss: 7.0212e-04 - val_loss: 6.6039e-04 - 13s/epoch - 160us/sample
Epoch 5/85
84077/84077 - 14s - loss: 9.0488e-04 - val_loss: 0.0015 - 14s/epoch - 164us/sample
Epoch 6/85
84077/84077 - 14s - loss: 6.0211e-04 - val_loss: 4.5127e-04 - 14s/epoch - 161us/sample
Epoch 7/85
84077/84077 - 14s - loss: 4.3336e-04 - val_loss: 3.8268e-04 - 14s/epoch - 161us/sample
Epoch 8/85
84077/84077 - 13s - loss: 4.0273e-04 - val_loss: 3.6426e-04 - 13s/epoch - 160us/sample
Epoch 9/85
84077/84077 - 14s - loss: 3.7404e-04 - val_loss: 3.0750e-04 - 14s/epoch - 161us/sample
Epoch 10/85
84077/84077 - 14s - loss: 3.3370e-04 - val_loss: 3.0019e-04 - 14s/epoch - 163us/sample
Epoch 11/85
84077/84077 - 14s - loss: 3.0567e-04 - val_loss: 2.6893e-04 - 14s/epoch - 164us/sample
Epoch 12/85
84077/84077 - 13s - loss: 2.8623e-04 - val_loss: 2.5113e-04 - 13s/epoch - 159us/sample
Epoch 13/85
84077/84077 - 14s - loss: 2.7290e-04 - val_loss: 2.3608e-04 - 14s/epoch - 161us/sample
Epoch 14/85
84077/84077 - 13s - loss: 2.5113e-04 - val_loss: 2.2532e-04 - 13s/epoch - 161us/sample
Epoch 15/85
84077/84077 - 14s - loss: 2.4443e-04 - val_loss: 2.1427e-04 - 14s/epoch - 164us/sample
Epoch 16/85
84077/84077 - 14s - loss: 2.2868e-04 - val_loss: 2.0894e-04 - 14s/epoch - 162us/sample
Epoch 17/85
84077/84077 - 14s - loss: 2.1990e-04 - val_loss: 2.0168e-04 - 14s/epoch - 161us/sample
Epoch 18/85
84077/84077 - 13s - loss: 2.1298e-04 - val_loss: 1.8600e-04 - 13s/epoch - 160us/sample
Epoch 19/85
84077/84077 - 13s - loss: 2.0595e-04 - val_loss: 1.8478e-04 - 13s/epoch - 160us/sample
Epoch 20/85
84077/84077 - 14s - loss: 2.0001e-04 - val_loss: 1.8355e-04 - 14s/epoch - 163us/sample
Epoch 21/85
84077/84077 - 14s - loss: 1.9458e-04 - val_loss: 1.7311e-04 - 14s/epoch - 164us/sample
Epoch 22/85
84077/84077 - 13s - loss: 1.9343e-04 - val_loss: 1.7200e-04 - 13s/epoch - 160us/sample
Epoch 23/85
84077/84077 - 14s - loss: 1.8543e-04 - val_loss: 1.6716e-04 - 14s/epoch - 161us/sample
Epoch 24/85
84077/84077 - 13s - loss: 1.8274e-04 - val_loss: 1.6443e-04 - 13s/epoch - 160us/sample
Epoch 25/85
84077/84077 - 14s - loss: 1.7728e-04 - val_loss: 1.5964e-04 - 14s/epoch - 164us/sample
Epoch 26/85
84077/84077 - 14s - loss: 1.7438e-04 - val_loss: 1.5732e-04 - 14s/epoch - 163us/sample
Epoch 27/85
84077/84077 - 14s - loss: 1.7162e-04 - val_loss: 1.5657e-04 - 14s/epoch - 161us/sample
Epoch 28/85
84077/84077 - 13s - loss: 1.6881e-04 - val_loss: 1.5496e-04 - 13s/epoch - 160us/sample
Epoch 29/85
84077/84077 - 14s - loss: 1.6678e-04 - val_loss: 1.5080e-04 - 14s/epoch - 161us/sample
Epoch 30/85
84077/84077 - 14s - loss: 1.6562e-04 - val_loss: 1.5043e-04 - 14s/epoch - 164us/sample
Epoch 31/85
84077/84077 - 14s - loss: 1.6359e-04 - val_loss: 1.4982e-04 - 14s/epoch - 165us/sample
Epoch 32/85
84077/84077 - 13s - loss: 1.6204e-04 - val_loss: 1.4723e-04 - 13s/epoch - 160us/sample
Epoch 33/85
84077/84077 - 13s - loss: 1.6071e-04 - val_loss: 1.4614e-04 - 13s/epoch - 161us/sample
Epoch 34/85
84077/84077 - 13s - loss: 1.5904e-04 - val_loss: 1.4468e-04 - 13s/epoch - 160us/sample
Epoch 35/85
84077/84077 - 14s - loss: 1.5899e-04 - val_loss: 1.4371e-04 - 14s/epoch - 163us/sample
Epoch 36/85
84077/84077 - 14s - loss: 1.5726e-04 - val_loss: 1.4443e-04 - 14s/epoch - 163us/sample
Epoch 37/85
84077/84077 - 14s - loss: 1.5623e-04 - val_loss: 1.4284e-04 - 14s/epoch - 161us/sample
Epoch 38/85
84077/84077 - 13s - loss: 1.5568e-04 - val_loss: 1.4082e-04 - 13s/epoch - 159us/sample
Epoch 39/85
84077/84077 - 14s - loss: 1.5439e-04 - val_loss: 1.4207e-04 - 14s/epoch - 161us/sample
Epoch 40/85
84077/84077 - 14s - loss: 1.5349e-04 - val_loss: 1.4053e-04 - 14s/epoch - 163us/sample
Epoch 41/85
84077/84077 - 14s - loss: 1.5302e-04 - val_loss: 1.4031e-04 - 14s/epoch - 163us/sample
Epoch 42/85
84077/84077 - 13s - loss: 1.5265e-04 - val_loss: 1.4083e-04 - 13s/epoch - 160us/sample
Epoch 43/85
84077/84077 - 13s - loss: 1.5175e-04 - val_loss: 1.3993e-04 - 13s/epoch - 161us/sample
Epoch 44/85
84077/84077 - 13s - loss: 1.5121e-04 - val_loss: 1.4002e-04 - 13s/epoch - 160us/sample
Epoch 45/85
84077/84077 - 14s - loss: 1.5080e-04 - val_loss: 1.3818e-04 - 14s/epoch - 163us/sample
Epoch 46/85
84077/84077 - 14s - loss: 1.4972e-04 - val_loss: 1.3722e-04 - 14s/epoch - 163us/sample
Epoch 47/85
84077/84077 - 13s - loss: 1.4938e-04 - val_loss: 1.3669e-04 - 13s/epoch - 160us/sample
Epoch 48/85
84077/84077 - 13s - loss: 1.4887e-04 - val_loss: 1.3634e-04 - 13s/epoch - 160us/sample
Epoch 49/85
84077/84077 - 13s - loss: 1.4803e-04 - val_loss: 1.3586e-04 - 13s/epoch - 160us/sample
Epoch 50/85
84077/84077 - 14s - loss: 1.4817e-04 - val_loss: 1.3822e-04 - 14s/epoch - 162us/sample
Epoch 51/85
84077/84077 - 14s - loss: 1.4674e-04 - val_loss: 1.3657e-04 - 14s/epoch - 162us/sample
Epoch 52/85
84077/84077 - 13s - loss: 1.4658e-04 - val_loss: 1.3477e-04 - 13s/epoch - 160us/sample
Epoch 53/85
84077/84077 - 13s - loss: 1.4637e-04 - val_loss: 1.3474e-04 - 13s/epoch - 160us/sample
Epoch 54/85
84077/84077 - 13s - loss: 1.4551e-04 - val_loss: 1.3355e-04 - 13s/epoch - 159us/sample
Epoch 55/85
84077/84077 - 14s - loss: 1.4523e-04 - val_loss: 1.3443e-04 - 14s/epoch - 164us/sample
Epoch 56/85
84077/84077 - 14s - loss: 1.4471e-04 - val_loss: 1.3314e-04 - 14s/epoch - 162us/sample
Epoch 57/85
84077/84077 - 14s - loss: 1.4426e-04 - val_loss: 1.3291e-04 - 14s/epoch - 161us/sample
Epoch 58/85
84077/84077 - 13s - loss: 1.4394e-04 - val_loss: 1.3341e-04 - 13s/epoch - 159us/sample
Epoch 59/85
84077/84077 - 13s - loss: 1.4382e-04 - val_loss: 1.3389e-04 - 13s/epoch - 160us/sample
Epoch 60/85
84077/84077 - 14s - loss: 1.4413e-04 - val_loss: 1.3211e-04 - 14s/epoch - 162us/sample
Epoch 61/85
84077/84077 - 14s - loss: 1.4289e-04 - val_loss: 1.3306e-04 - 14s/epoch - 162us/sample
Epoch 62/85
84077/84077 - 13s - loss: 1.4283e-04 - val_loss: 1.3217e-04 - 13s/epoch - 160us/sample
Epoch 63/85
84077/84077 - 13s - loss: 1.4242e-04 - val_loss: 1.3217e-04 - 13s/epoch - 160us/sample
Epoch 64/85
84077/84077 - 13s - loss: 1.4228e-04 - val_loss: 1.3187e-04 - 13s/epoch - 159us/sample
Epoch 65/85
84077/84077 - 14s - loss: 1.4201e-04 - val_loss: 1.3145e-04 - 14s/epoch - 164us/sample
Epoch 66/85
84077/84077 - 14s - loss: 1.4129e-04 - val_loss: 1.3063e-04 - 14s/epoch - 162us/sample
Epoch 67/85
84077/84077 - 13s - loss: 1.4102e-04 - val_loss: 1.3165e-04 - 13s/epoch - 161us/sample
Epoch 68/85
84077/84077 - 13s - loss: 1.4128e-04 - val_loss: 1.3122e-04 - 13s/epoch - 160us/sample
Epoch 69/85
84077/84077 - 13s - loss: 1.4059e-04 - val_loss: 1.3132e-04 - 13s/epoch - 160us/sample
Epoch 70/85
84077/84077 - 14s - loss: 1.4055e-04 - val_loss: 1.3114e-04 - 14s/epoch - 163us/sample
Epoch 71/85
84077/84077 - 14s - loss: 1.4022e-04 - val_loss: 1.3131e-04 - 14s/epoch - 163us/sample
Epoch 72/85
84077/84077 - 13s - loss: 1.4011e-04 - val_loss: 1.3085e-04 - 13s/epoch - 160us/sample
Epoch 73/85
84077/84077 - 14s - loss: 1.3950e-04 - val_loss: 1.2990e-04 - 14s/epoch - 161us/sample
Epoch 74/85
84077/84077 - 13s - loss: 1.3957e-04 - val_loss: 1.3067e-04 - 13s/epoch - 160us/sample
Epoch 75/85
84077/84077 - 14s - loss: 1.3894e-04 - val_loss: 1.2901e-04 - 14s/epoch - 163us/sample
Epoch 76/85
84077/84077 - 14s - loss: 1.3921e-04 - val_loss: 1.2902e-04 - 14s/epoch - 162us/sample
Epoch 77/85
84077/84077 - 13s - loss: 1.3849e-04 - val_loss: 1.3001e-04 - 13s/epoch - 160us/sample
Epoch 78/85
84077/84077 - 13s - loss: 1.3827e-04 - val_loss: 1.2775e-04 - 13s/epoch - 160us/sample
Epoch 79/85
84077/84077 - 13s - loss: 1.3856e-04 - val_loss: 1.2926e-04 - 13s/epoch - 160us/sample
Epoch 80/85
84077/84077 - 14s - loss: 1.3820e-04 - val_loss: 1.2736e-04 - 14s/epoch - 162us/sample
Epoch 81/85
84077/84077 - 14s - loss: 1.3765e-04 - val_loss: 1.2759e-04 - 14s/epoch - 162us/sample
Epoch 82/85
84077/84077 - 13s - loss: 1.3756e-04 - val_loss: 1.2759e-04 - 13s/epoch - 160us/sample
Epoch 83/85
84077/84077 - 14s - loss: 1.3753e-04 - val_loss: 1.2793e-04 - 14s/epoch - 161us/sample
Epoch 84/85
84077/84077 - 13s - loss: 1.3709e-04 - val_loss: 1.2739e-04 - 13s/epoch - 159us/sample
Epoch 85/85
84077/84077 - 14s - loss: 1.3715e-04 - val_loss: 1.2860e-04 - 14s/epoch - 163us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012860396827010426
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 06:58:14.796371: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_80/outputlayer/BiasAdd' id:101250 op device:{requested: '', assigned: ''} def:{{{node decoder_model_80/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_80/outputlayer/MatMul, decoder_model_80/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03153271356577371
cosine 0.031151565298870188
MAE: 0.0024630974156008897
RMSE: 0.010279320722781838
r2: 0.9175863012667111
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'logcosh', 64, 85, 0.0014000000000000002, 0.2, 188, 0.0001371479411896947, 0.00012860396827010426, 0.03153271356577371, 0.031151565298870188, 0.0024630974156008897, 0.010279320722781838, 0.9175863012667111, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 85 0.001 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_240 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_240 (ReLU)               (None, 2168)         0           ['batch_normalization_240[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_240[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_240[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 06:59:06.659346: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_81/kernel/Assign' id:102277 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_81/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_81/kernel, dense_dec1_81/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 06:59:33.174868: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_81/mul' id:102571 op device:{requested: '', assigned: ''} def:{{{node loss_81/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_81/mul/x, loss_81/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 48s - loss: 0.0170 - val_loss: 9.6270e-04 - 48s/epoch - 567us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0011 - val_loss: 0.0010 - 14s/epoch - 162us/sample
Epoch 3/85
84077/84077 - 14s - loss: 0.0028 - val_loss: 0.0011 - 14s/epoch - 162us/sample
Epoch 4/85
84077/84077 - 14s - loss: 0.0012 - val_loss: 6.3007e-04 - 14s/epoch - 166us/sample
Epoch 5/85
84077/84077 - 14s - loss: 6.2579e-04 - val_loss: 7.1545e-04 - 14s/epoch - 162us/sample
Epoch 6/85
84077/84077 - 14s - loss: 0.0012 - val_loss: 5.7948e-04 - 14s/epoch - 163us/sample
Epoch 7/85
84077/84077 - 14s - loss: 4.5961e-04 - val_loss: 3.8083e-04 - 14s/epoch - 163us/sample
Epoch 8/85
84077/84077 - 14s - loss: 3.8339e-04 - val_loss: 3.2844e-04 - 14s/epoch - 162us/sample
Epoch 9/85
84077/84077 - 14s - loss: 3.4724e-04 - val_loss: 3.0666e-04 - 14s/epoch - 163us/sample
Epoch 10/85
84077/84077 - 12s - loss: 3.3241e-04 - val_loss: 3.1147e-04 - 12s/epoch - 147us/sample
Epoch 11/85
84077/84077 - 14s - loss: 3.0772e-04 - val_loss: 2.7086e-04 - 14s/epoch - 161us/sample
Epoch 12/85
84077/84077 - 13s - loss: 2.9225e-04 - val_loss: 2.5706e-04 - 13s/epoch - 160us/sample
Epoch 13/85
84077/84077 - 14s - loss: 2.7010e-04 - val_loss: 2.3740e-04 - 14s/epoch - 164us/sample
Epoch 14/85
84077/84077 - 14s - loss: 2.5181e-04 - val_loss: 2.2264e-04 - 14s/epoch - 169us/sample
Epoch 15/85
84077/84077 - 14s - loss: 2.4025e-04 - val_loss: 2.1017e-04 - 14s/epoch - 163us/sample
Epoch 16/85
84077/84077 - 14s - loss: 2.2910e-04 - val_loss: 2.0031e-04 - 14s/epoch - 163us/sample
Epoch 17/85
84077/84077 - 14s - loss: 2.1723e-04 - val_loss: 1.8916e-04 - 14s/epoch - 164us/sample
Epoch 18/85
84077/84077 - 14s - loss: 2.0778e-04 - val_loss: 1.8251e-04 - 14s/epoch - 163us/sample
Epoch 19/85
84077/84077 - 14s - loss: 1.9948e-04 - val_loss: 1.7434e-04 - 14s/epoch - 168us/sample
Epoch 20/85
84077/84077 - 14s - loss: 1.9238e-04 - val_loss: 1.7105e-04 - 14s/epoch - 163us/sample
Epoch 21/85
84077/84077 - 14s - loss: 1.8858e-04 - val_loss: 1.6872e-04 - 14s/epoch - 164us/sample
Epoch 22/85
84077/84077 - 14s - loss: 1.8380e-04 - val_loss: 1.6346e-04 - 14s/epoch - 163us/sample
Epoch 23/85
84077/84077 - 14s - loss: 1.8047e-04 - val_loss: 1.6057e-04 - 14s/epoch - 164us/sample
Epoch 24/85
84077/84077 - 14s - loss: 1.7586e-04 - val_loss: 1.5611e-04 - 14s/epoch - 168us/sample
Epoch 25/85
84077/84077 - 14s - loss: 1.7328e-04 - val_loss: 1.5576e-04 - 14s/epoch - 164us/sample
Epoch 26/85
84077/84077 - 14s - loss: 1.7065e-04 - val_loss: 1.5191e-04 - 14s/epoch - 164us/sample
Epoch 27/85
84077/84077 - 14s - loss: 1.6882e-04 - val_loss: 1.4956e-04 - 14s/epoch - 163us/sample
Epoch 28/85
84077/84077 - 14s - loss: 1.6647e-04 - val_loss: 1.5054e-04 - 14s/epoch - 165us/sample
Epoch 29/85
84077/84077 - 14s - loss: 1.6465e-04 - val_loss: 1.4765e-04 - 14s/epoch - 166us/sample
Epoch 30/85
84077/84077 - 14s - loss: 1.6358e-04 - val_loss: 1.4661e-04 - 14s/epoch - 164us/sample
Epoch 31/85
84077/84077 - 14s - loss: 1.6193e-04 - val_loss: 1.4516e-04 - 14s/epoch - 163us/sample
Epoch 32/85
84077/84077 - 14s - loss: 1.5990e-04 - val_loss: 1.4497e-04 - 14s/epoch - 163us/sample
Epoch 33/85
84077/84077 - 14s - loss: 1.5942e-04 - val_loss: 1.4351e-04 - 14s/epoch - 169us/sample
Epoch 34/85
84077/84077 - 14s - loss: 1.5844e-04 - val_loss: 1.4356e-04 - 14s/epoch - 163us/sample
Epoch 35/85
84077/84077 - 14s - loss: 1.5753e-04 - val_loss: 1.4190e-04 - 14s/epoch - 163us/sample
Epoch 36/85
84077/84077 - 14s - loss: 1.5617e-04 - val_loss: 1.4245e-04 - 14s/epoch - 163us/sample
Epoch 37/85
84077/84077 - 14s - loss: 1.5581e-04 - val_loss: 1.4250e-04 - 14s/epoch - 163us/sample
Epoch 38/85
84077/84077 - 14s - loss: 1.5433e-04 - val_loss: 1.4053e-04 - 14s/epoch - 168us/sample
Epoch 39/85
84077/84077 - 14s - loss: 1.5380e-04 - val_loss: 1.4050e-04 - 14s/epoch - 163us/sample
Epoch 40/85
84077/84077 - 14s - loss: 1.5347e-04 - val_loss: 1.4046e-04 - 14s/epoch - 163us/sample
Epoch 41/85
84077/84077 - 14s - loss: 1.5206e-04 - val_loss: 1.3846e-04 - 14s/epoch - 163us/sample
Epoch 42/85
84077/84077 - 14s - loss: 1.5135e-04 - val_loss: 1.3772e-04 - 14s/epoch - 163us/sample
Epoch 43/85
84077/84077 - 14s - loss: 1.5104e-04 - val_loss: 1.3668e-04 - 14s/epoch - 168us/sample
Epoch 44/85
84077/84077 - 14s - loss: 1.5066e-04 - val_loss: 1.3666e-04 - 14s/epoch - 163us/sample
Epoch 45/85
84077/84077 - 14s - loss: 1.4934e-04 - val_loss: 1.3729e-04 - 14s/epoch - 163us/sample
Epoch 46/85
84077/84077 - 14s - loss: 1.4894e-04 - val_loss: 1.3653e-04 - 14s/epoch - 163us/sample
Epoch 47/85
84077/84077 - 14s - loss: 1.4883e-04 - val_loss: 1.3685e-04 - 14s/epoch - 164us/sample
Epoch 48/85
84077/84077 - 14s - loss: 1.4779e-04 - val_loss: 1.3530e-04 - 14s/epoch - 168us/sample
Epoch 49/85
84077/84077 - 14s - loss: 1.4751e-04 - val_loss: 1.3603e-04 - 14s/epoch - 164us/sample
Epoch 50/85
84077/84077 - 14s - loss: 1.4711e-04 - val_loss: 1.3454e-04 - 14s/epoch - 163us/sample
Epoch 51/85
84077/84077 - 14s - loss: 1.4646e-04 - val_loss: 1.3445e-04 - 14s/epoch - 163us/sample
Epoch 52/85
84077/84077 - 14s - loss: 1.4634e-04 - val_loss: 1.3396e-04 - 14s/epoch - 164us/sample
Epoch 53/85
84077/84077 - 14s - loss: 1.4546e-04 - val_loss: 1.3390e-04 - 14s/epoch - 167us/sample
Epoch 54/85
84077/84077 - 14s - loss: 1.4497e-04 - val_loss: 1.3423e-04 - 14s/epoch - 163us/sample
Epoch 55/85
84077/84077 - 14s - loss: 1.4499e-04 - val_loss: 1.3459e-04 - 14s/epoch - 164us/sample
Epoch 56/85
84077/84077 - 14s - loss: 1.4422e-04 - val_loss: 1.3358e-04 - 14s/epoch - 163us/sample
Epoch 57/85
84077/84077 - 14s - loss: 1.4400e-04 - val_loss: 1.3230e-04 - 14s/epoch - 167us/sample
Epoch 58/85
84077/84077 - 14s - loss: 1.4344e-04 - val_loss: 1.3357e-04 - 14s/epoch - 163us/sample
Epoch 59/85
84077/84077 - 14s - loss: 1.4316e-04 - val_loss: 1.3262e-04 - 14s/epoch - 164us/sample
Epoch 60/85
84077/84077 - 14s - loss: 1.4317e-04 - val_loss: 1.3324e-04 - 14s/epoch - 163us/sample
Epoch 61/85
84077/84077 - 14s - loss: 1.4260e-04 - val_loss: 1.3221e-04 - 14s/epoch - 163us/sample
Epoch 62/85
84077/84077 - 14s - loss: 1.4233e-04 - val_loss: 1.3211e-04 - 14s/epoch - 169us/sample
Epoch 63/85
84077/84077 - 14s - loss: 1.4211e-04 - val_loss: 1.3117e-04 - 14s/epoch - 163us/sample
Epoch 64/85
84077/84077 - 14s - loss: 1.4171e-04 - val_loss: 1.3159e-04 - 14s/epoch - 163us/sample
Epoch 65/85
84077/84077 - 14s - loss: 1.4181e-04 - val_loss: 1.3019e-04 - 14s/epoch - 163us/sample
Epoch 66/85
84077/84077 - 14s - loss: 1.4100e-04 - val_loss: 1.3122e-04 - 14s/epoch - 163us/sample
Epoch 67/85
84077/84077 - 14s - loss: 1.4065e-04 - val_loss: 1.3230e-04 - 14s/epoch - 168us/sample
Epoch 68/85
84077/84077 - 14s - loss: 1.4035e-04 - val_loss: 1.3037e-04 - 14s/epoch - 163us/sample
Epoch 69/85
84077/84077 - 14s - loss: 1.4006e-04 - val_loss: 1.3081e-04 - 14s/epoch - 163us/sample
Epoch 70/85
84077/84077 - 14s - loss: 1.4006e-04 - val_loss: 1.3025e-04 - 14s/epoch - 163us/sample
Epoch 71/85
84077/84077 - 14s - loss: 1.3998e-04 - val_loss: 1.2902e-04 - 14s/epoch - 163us/sample
Epoch 72/85
84077/84077 - 14s - loss: 1.3974e-04 - val_loss: 1.2976e-04 - 14s/epoch - 170us/sample
Epoch 73/85
84077/84077 - 14s - loss: 1.3937e-04 - val_loss: 1.2982e-04 - 14s/epoch - 163us/sample
Epoch 74/85
84077/84077 - 14s - loss: 1.3903e-04 - val_loss: 1.2975e-04 - 14s/epoch - 164us/sample
Epoch 75/85
84077/84077 - 14s - loss: 1.3883e-04 - val_loss: 1.2989e-04 - 14s/epoch - 163us/sample
Epoch 76/85
84077/84077 - 14s - loss: 1.3857e-04 - val_loss: 1.2877e-04 - 14s/epoch - 164us/sample
Epoch 77/85
84077/84077 - 14s - loss: 1.3858e-04 - val_loss: 1.2966e-04 - 14s/epoch - 168us/sample
Epoch 78/85
84077/84077 - 14s - loss: 1.3845e-04 - val_loss: 1.2759e-04 - 14s/epoch - 162us/sample
Epoch 79/85
84077/84077 - 14s - loss: 1.3811e-04 - val_loss: 1.2857e-04 - 14s/epoch - 164us/sample
Epoch 80/85
84077/84077 - 14s - loss: 1.3802e-04 - val_loss: 1.2693e-04 - 14s/epoch - 163us/sample
Epoch 81/85
84077/84077 - 14s - loss: 1.3767e-04 - val_loss: 1.2783e-04 - 14s/epoch - 168us/sample
Epoch 82/85
84077/84077 - 14s - loss: 1.3714e-04 - val_loss: 1.2772e-04 - 14s/epoch - 165us/sample
Epoch 83/85
84077/84077 - 14s - loss: 1.3745e-04 - val_loss: 1.2700e-04 - 14s/epoch - 163us/sample
Epoch 84/85
84077/84077 - 14s - loss: 1.3724e-04 - val_loss: 1.2676e-04 - 14s/epoch - 164us/sample
Epoch 85/85
84077/84077 - 14s - loss: 1.3687e-04 - val_loss: 1.2689e-04 - 14s/epoch - 163us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0001268902529666874
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 07:19:01.700712: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_81/outputlayer/BiasAdd' id:102535 op device:{requested: '', assigned: ''} def:{{{node decoder_model_81/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_81/outputlayer/MatMul, decoder_model_81/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031807038972045246
cosine 0.03142302786725214
MAE: 0.0023986169593635544
RMSE: 0.009897775086994786
r2: 0.9237097212966145
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'logcosh', 64, 85, 0.001, 0.2, 188, 0.00013686628322307355, 0.0001268902529666874, 0.031807038972045246, 0.03142302786725214, 0.0023986169593635544, 0.009897775086994786, 0.9237097212966145, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 75 0.0008 64 0] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_243 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_243 (ReLU)               (None, 2168)         0           ['batch_normalization_243[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_243[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_243[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/75
2023-02-12 07:19:55.414077: W tensorflow/c/c_api.cc:291] Operation '{name:'training_160/Adam/dense_enc0_82/kernel/m/Assign' id:104384 op device:{requested: '', assigned: ''} def:{{{node training_160/Adam/dense_enc0_82/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_160/Adam/dense_enc0_82/kernel/m, training_160/Adam/dense_enc0_82/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 07:20:23.184316: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_82/mul' id:103868 op device:{requested: '', assigned: ''} def:{{{node loss_82/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_82/mul/x, loss_82/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 49s - loss: 0.0696 - val_loss: 0.0676 - 49s/epoch - 586us/sample
Epoch 2/75
84077/84077 - 14s - loss: 0.0670 - val_loss: 0.0674 - 14s/epoch - 168us/sample
Epoch 3/75
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 168us/sample
Epoch 4/75
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 171us/sample
Epoch 5/75
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 170us/sample
Epoch 6/75
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0674 - 14s/epoch - 167us/sample
Epoch 7/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0674 - 14s/epoch - 168us/sample
Epoch 8/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0674 - 14s/epoch - 168us/sample
Epoch 9/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 10/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 11/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 12/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 13/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 14/75
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 172us/sample
Epoch 15/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 16/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 17/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 18/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 19/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 20/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 21/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 22/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 23/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 24/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 171us/sample
Epoch 25/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 26/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 27/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 28/75
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 29/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 30/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 31/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 32/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 33/75
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 34/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 35/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 36/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 37/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 38/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 39/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 40/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 41/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 42/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 170us/sample
Epoch 43/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 44/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 45/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 46/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 47/75
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 48/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 49/75
84077/84077 - 14s - loss: 0.0669 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 50/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 51/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 52/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 53/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 54/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 55/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 56/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 57/75
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 58/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 59/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 60/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 61/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 62/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 172us/sample
Epoch 63/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 64/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 65/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 66/75
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 67/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 68/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 69/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 70/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
Epoch 71/75
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 173us/sample
Epoch 72/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 169us/sample
Epoch 73/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 74/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 167us/sample
Epoch 75/75
84077/84077 - 14s - loss: 0.0668 - val_loss: 0.0673 - 14s/epoch - 168us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.06734575275383575
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 07:38:05.545392: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_82/outputlayer/BiasAdd' id:103820 op device:{requested: '', assigned: ''} def:{{{node decoder_model_82/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_82/outputlayer/MatMul, decoder_model_82/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1204097449865456
cosine 1.1358297951945378
MAE: 5.962914593965255
RMSE: 6.249891449990877
r2: -28653.13452189953
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'binary_crossentropy', 64, 75, 0.0008, 0.2, 188, 0.06683693936339599, 0.06734575275383575, 1.1204097449865456, 1.1358297951945378, 5.962914593965255, 6.249891449990877, -28653.13452189953, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 85 0.0008 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_246 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_246 (ReLU)               (None, 2168)         0           ['batch_normalization_246[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          407772      ['re_lu_246[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          407772      ['re_lu_246[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2500075     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,370,883
Trainable params: 5,361,835
Non-trainable params: 9,048
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-12 07:39:00.175998: W tensorflow/c/c_api.cc:291] Operation '{name:'training_162/Adam/batch_normalization_247/beta/m/Assign' id:105736 op device:{requested: '', assigned: ''} def:{{{node training_162/Adam/batch_normalization_247/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_162/Adam/batch_normalization_247/beta/m, training_162/Adam/batch_normalization_247/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 07:39:28.006330: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_83/mul' id:105193 op device:{requested: '', assigned: ''} def:{{{node loss_83/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_83/mul/x, loss_83/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 50s - loss: 0.0050 - val_loss: 0.0011 - 50s/epoch - 597us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0013 - val_loss: 8.1026e-04 - 14s/epoch - 169us/sample
Epoch 3/85
84077/84077 - 14s - loss: 9.1560e-04 - val_loss: 7.8878e-04 - 14s/epoch - 163us/sample
Epoch 4/85
84077/84077 - 13s - loss: 0.0014 - val_loss: 6.8231e-04 - 13s/epoch - 157us/sample
Epoch 5/85
84077/84077 - 13s - loss: 7.1626e-04 - val_loss: 4.5608e-04 - 13s/epoch - 155us/sample
Epoch 6/85
84077/84077 - 13s - loss: 5.2415e-04 - val_loss: 4.9447e-04 - 13s/epoch - 155us/sample
Epoch 7/85
84077/84077 - 13s - loss: 4.5365e-04 - val_loss: 3.8841e-04 - 13s/epoch - 155us/sample
Epoch 8/85
84077/84077 - 13s - loss: 4.0933e-04 - val_loss: 3.3927e-04 - 13s/epoch - 156us/sample
Epoch 9/85
84077/84077 - 13s - loss: 4.4058e-04 - val_loss: 3.1091e-04 - 13s/epoch - 159us/sample
Epoch 10/85
84077/84077 - 13s - loss: 3.2892e-04 - val_loss: 2.8303e-04 - 13s/epoch - 156us/sample
Epoch 11/85
84077/84077 - 13s - loss: 3.3523e-04 - val_loss: 2.5989e-04 - 13s/epoch - 155us/sample
Epoch 12/85
84077/84077 - 13s - loss: 2.8318e-04 - val_loss: 2.4299e-04 - 13s/epoch - 157us/sample
Epoch 13/85
84077/84077 - 13s - loss: 2.6263e-04 - val_loss: 2.3121e-04 - 13s/epoch - 155us/sample
Epoch 14/85
84077/84077 - 13s - loss: 2.4674e-04 - val_loss: 2.1852e-04 - 13s/epoch - 160us/sample
Epoch 15/85
84077/84077 - 13s - loss: 2.3561e-04 - val_loss: 2.0432e-04 - 13s/epoch - 157us/sample
Epoch 16/85
84077/84077 - 13s - loss: 2.2358e-04 - val_loss: 1.9474e-04 - 13s/epoch - 155us/sample
Epoch 17/85
84077/84077 - 13s - loss: 2.1515e-04 - val_loss: 1.8966e-04 - 13s/epoch - 156us/sample
Epoch 18/85
84077/84077 - 13s - loss: 2.0953e-04 - val_loss: 1.9104e-04 - 13s/epoch - 155us/sample
Epoch 19/85
84077/84077 - 13s - loss: 2.0196e-04 - val_loss: 1.7413e-04 - 13s/epoch - 159us/sample
Epoch 20/85
84077/84077 - 13s - loss: 1.9537e-04 - val_loss: 1.7416e-04 - 13s/epoch - 156us/sample
Epoch 21/85
84077/84077 - 13s - loss: 1.9014e-04 - val_loss: 1.6582e-04 - 13s/epoch - 156us/sample
Epoch 22/85
84077/84077 - 13s - loss: 1.8535e-04 - val_loss: 1.6289e-04 - 13s/epoch - 155us/sample
Epoch 23/85
84077/84077 - 13s - loss: 1.8141e-04 - val_loss: 1.5984e-04 - 13s/epoch - 156us/sample
Epoch 24/85
84077/84077 - 13s - loss: 1.7785e-04 - val_loss: 1.5751e-04 - 13s/epoch - 157us/sample
Epoch 25/85
84077/84077 - 13s - loss: 1.7431e-04 - val_loss: 1.5490e-04 - 13s/epoch - 157us/sample
Epoch 26/85
84077/84077 - 13s - loss: 1.7180e-04 - val_loss: 1.5141e-04 - 13s/epoch - 156us/sample
Epoch 27/85
84077/84077 - 13s - loss: 1.6959e-04 - val_loss: 1.5008e-04 - 13s/epoch - 155us/sample
Epoch 28/85
84077/84077 - 13s - loss: 1.6740e-04 - val_loss: 1.5051e-04 - 13s/epoch - 156us/sample
Epoch 29/85
84077/84077 - 13s - loss: 1.6547e-04 - val_loss: 1.4853e-04 - 13s/epoch - 155us/sample
Epoch 30/85
84077/84077 - 13s - loss: 1.6342e-04 - val_loss: 1.4696e-04 - 13s/epoch - 160us/sample
Epoch 31/85
84077/84077 - 13s - loss: 1.6237e-04 - val_loss: 1.4471e-04 - 13s/epoch - 155us/sample
Epoch 32/85
84077/84077 - 13s - loss: 1.6051e-04 - val_loss: 1.4410e-04 - 13s/epoch - 156us/sample
Epoch 33/85
84077/84077 - 13s - loss: 1.5927e-04 - val_loss: 1.4362e-04 - 13s/epoch - 155us/sample
Epoch 34/85
84077/84077 - 13s - loss: 1.5825e-04 - val_loss: 1.4253e-04 - 13s/epoch - 156us/sample
Epoch 35/85
84077/84077 - 14s - loss: 1.5715e-04 - val_loss: 1.4195e-04 - 14s/epoch - 161us/sample
Epoch 36/85
84077/84077 - 13s - loss: 1.5588e-04 - val_loss: 1.4127e-04 - 13s/epoch - 155us/sample
Epoch 37/85
84077/84077 - 13s - loss: 1.5524e-04 - val_loss: 1.3977e-04 - 13s/epoch - 156us/sample
Epoch 38/85
84077/84077 - 13s - loss: 1.5448e-04 - val_loss: 1.3911e-04 - 13s/epoch - 155us/sample
Epoch 39/85
84077/84077 - 13s - loss: 1.5322e-04 - val_loss: 1.3911e-04 - 13s/epoch - 156us/sample
Epoch 40/85
84077/84077 - 13s - loss: 1.5248e-04 - val_loss: 1.3818e-04 - 13s/epoch - 160us/sample
Epoch 41/85
84077/84077 - 13s - loss: 1.5155e-04 - val_loss: 1.3670e-04 - 13s/epoch - 156us/sample
Epoch 42/85
84077/84077 - 13s - loss: 1.5084e-04 - val_loss: 1.3764e-04 - 13s/epoch - 155us/sample
Epoch 43/85
84077/84077 - 13s - loss: 1.5036e-04 - val_loss: 1.3683e-04 - 13s/epoch - 156us/sample
Epoch 44/85
84077/84077 - 13s - loss: 1.4969e-04 - val_loss: 1.3658e-04 - 13s/epoch - 156us/sample
Epoch 45/85
84077/84077 - 13s - loss: 1.4894e-04 - val_loss: 1.3508e-04 - 13s/epoch - 159us/sample
Epoch 46/85
84077/84077 - 13s - loss: 1.4832e-04 - val_loss: 1.3529e-04 - 13s/epoch - 156us/sample
Epoch 47/85
84077/84077 - 13s - loss: 1.4786e-04 - val_loss: 1.3520e-04 - 13s/epoch - 155us/sample
Epoch 48/85
84077/84077 - 13s - loss: 1.4746e-04 - val_loss: 1.3532e-04 - 13s/epoch - 156us/sample
Epoch 49/85
84077/84077 - 13s - loss: 1.4711e-04 - val_loss: 1.3403e-04 - 13s/epoch - 155us/sample
Epoch 50/85
84077/84077 - 14s - loss: 1.4667e-04 - val_loss: 1.3264e-04 - 14s/epoch - 161us/sample
Epoch 51/85
84077/84077 - 13s - loss: 1.4566e-04 - val_loss: 1.3449e-04 - 13s/epoch - 156us/sample
Epoch 52/85
84077/84077 - 13s - loss: 1.4523e-04 - val_loss: 1.3338e-04 - 13s/epoch - 156us/sample
Epoch 53/85
84077/84077 - 13s - loss: 1.4507e-04 - val_loss: 1.3254e-04 - 13s/epoch - 156us/sample
Epoch 54/85
84077/84077 - 13s - loss: 1.4485e-04 - val_loss: 1.3350e-04 - 13s/epoch - 155us/sample
Epoch 55/85
84077/84077 - 13s - loss: 1.4423e-04 - val_loss: 1.3204e-04 - 13s/epoch - 158us/sample
Epoch 56/85
84077/84077 - 13s - loss: 1.4372e-04 - val_loss: 1.3226e-04 - 13s/epoch - 158us/sample
Epoch 57/85
84077/84077 - 13s - loss: 1.4342e-04 - val_loss: 1.3198e-04 - 13s/epoch - 156us/sample
Epoch 58/85
84077/84077 - 13s - loss: 1.4353e-04 - val_loss: 1.3140e-04 - 13s/epoch - 155us/sample
Epoch 59/85
84077/84077 - 13s - loss: 1.4286e-04 - val_loss: 1.3112e-04 - 13s/epoch - 156us/sample
Epoch 60/85
84077/84077 - 13s - loss: 1.4206e-04 - val_loss: 1.3156e-04 - 13s/epoch - 155us/sample
Epoch 61/85
84077/84077 - 14s - loss: 1.4198e-04 - val_loss: 1.3045e-04 - 14s/epoch - 161us/sample
Epoch 62/85
84077/84077 - 13s - loss: 1.4196e-04 - val_loss: 1.3049e-04 - 13s/epoch - 155us/sample
Epoch 63/85
84077/84077 - 13s - loss: 1.4137e-04 - val_loss: 1.3003e-04 - 13s/epoch - 156us/sample
Epoch 64/85
84077/84077 - 13s - loss: 1.4097e-04 - val_loss: 1.2888e-04 - 13s/epoch - 156us/sample
Epoch 65/85
84077/84077 - 13s - loss: 1.4099e-04 - val_loss: 1.3023e-04 - 13s/epoch - 155us/sample
Epoch 66/85
84077/84077 - 13s - loss: 1.4077e-04 - val_loss: 1.2969e-04 - 13s/epoch - 160us/sample
Epoch 67/85
84077/84077 - 13s - loss: 1.4037e-04 - val_loss: 1.3067e-04 - 13s/epoch - 155us/sample
Epoch 68/85
84077/84077 - 13s - loss: 1.3958e-04 - val_loss: 1.2913e-04 - 13s/epoch - 156us/sample
Epoch 69/85
84077/84077 - 13s - loss: 1.3967e-04 - val_loss: 1.2835e-04 - 13s/epoch - 155us/sample
Epoch 70/85
84077/84077 - 13s - loss: 1.3965e-04 - val_loss: 1.2992e-04 - 13s/epoch - 156us/sample
Epoch 71/85
84077/84077 - 13s - loss: 1.3911e-04 - val_loss: 1.2797e-04 - 13s/epoch - 161us/sample
Epoch 72/85
84077/84077 - 13s - loss: 1.3910e-04 - val_loss: 1.2772e-04 - 13s/epoch - 156us/sample
Epoch 73/85
84077/84077 - 13s - loss: 1.3885e-04 - val_loss: 1.2834e-04 - 13s/epoch - 156us/sample
Epoch 74/85
84077/84077 - 13s - loss: 1.3859e-04 - val_loss: 1.2718e-04 - 13s/epoch - 155us/sample
Epoch 75/85
84077/84077 - 13s - loss: 1.3870e-04 - val_loss: 1.2653e-04 - 13s/epoch - 156us/sample
Epoch 76/85
84077/84077 - 13s - loss: 1.3792e-04 - val_loss: 1.2698e-04 - 13s/epoch - 159us/sample
Epoch 77/85
84077/84077 - 13s - loss: 1.3771e-04 - val_loss: 1.2992e-04 - 13s/epoch - 156us/sample
Epoch 78/85
84077/84077 - 13s - loss: 1.3770e-04 - val_loss: 1.2735e-04 - 13s/epoch - 155us/sample
Epoch 79/85
84077/84077 - 13s - loss: 1.3733e-04 - val_loss: 1.2570e-04 - 13s/epoch - 156us/sample
Epoch 80/85
84077/84077 - 13s - loss: 1.3705e-04 - val_loss: 1.2669e-04 - 13s/epoch - 155us/sample
Epoch 81/85
84077/84077 - 13s - loss: 1.3709e-04 - val_loss: 1.2691e-04 - 13s/epoch - 160us/sample
Epoch 82/85
84077/84077 - 13s - loss: 1.3705e-04 - val_loss: 1.2732e-04 - 13s/epoch - 156us/sample
Epoch 83/85
84077/84077 - 13s - loss: 1.3720e-04 - val_loss: 1.2720e-04 - 13s/epoch - 156us/sample
Epoch 84/85
84077/84077 - 13s - loss: 1.3680e-04 - val_loss: 1.2670e-04 - 13s/epoch - 156us/sample
Epoch 85/85
84077/84077 - 13s - loss: 1.3607e-04 - val_loss: 1.2672e-04 - 13s/epoch - 155us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.00012672215733259737
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 07:58:05.751711: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_83/outputlayer/BiasAdd' id:105157 op device:{requested: '', assigned: ''} def:{{{node decoder_model_83/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_83/outputlayer/MatMul, decoder_model_83/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.031447994129772824
cosine 0.03107466680587935
MAE: 0.0026038527217994096
RMSE: 0.009994081081979371
r2: 0.9221833125644683
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'logcosh', 64, 85, 0.0008, 0.2, 188, 0.00013607436022045118, 0.00012672215733259737, 0.031447994129772824, 0.03107466680587935, 0.0026038527217994096, 0.009994081081979371, 0.9221833125644683, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0008 8 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_249 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_249 (ReLU)               (None, 1791)         0           ['batch_normalization_249[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 188)          336896      ['re_lu_249[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 188)          336896      ['re_lu_249[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 188)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2071803     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,443,463
Trainable params: 4,435,923
Non-trainable params: 7,540
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-12 07:59:02.216961: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_84/bias/Assign' id:106154 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_84/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_84/bias, bottleneck_zlog_84/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-12 08:00:10.766140: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_84/mul' id:106478 op device:{requested: '', assigned: ''} def:{{{node loss_84/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_84/mul/x, loss_84/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 94s - loss: 0.0020 - val_loss: 7.6497e-04 - 94s/epoch - 1ms/sample
Epoch 2/90
84077/84077 - 59s - loss: 5.8462e-04 - val_loss: 7.2503e-04 - 59s/epoch - 697us/sample
Epoch 3/90
84077/84077 - 59s - loss: 5.0952e-04 - val_loss: 8.2248e-04 - 59s/epoch - 699us/sample
Epoch 4/90
84077/84077 - 59s - loss: 4.8040e-04 - val_loss: 0.0032 - 59s/epoch - 697us/sample
Epoch 5/90
84077/84077 - 58s - loss: 4.6763e-04 - val_loss: 0.0052 - 58s/epoch - 692us/sample
Epoch 6/90
84077/84077 - 59s - loss: 4.6070e-04 - val_loss: 0.0055 - 59s/epoch - 700us/sample
Epoch 7/90
84077/84077 - 59s - loss: 4.5159e-04 - val_loss: 0.0077 - 59s/epoch - 697us/sample
Epoch 8/90
84077/84077 - 58s - loss: 4.4271e-04 - val_loss: 0.0099 - 58s/epoch - 695us/sample
Epoch 9/90
84077/84077 - 59s - loss: 4.3321e-04 - val_loss: 0.0080 - 59s/epoch - 698us/sample
Epoch 10/90
84077/84077 - 59s - loss: 4.2679e-04 - val_loss: 0.0091 - 59s/epoch - 698us/sample
Epoch 11/90
84077/84077 - 59s - loss: 4.2355e-04 - val_loss: 0.0106 - 59s/epoch - 700us/sample
Epoch 12/90
84077/84077 - 58s - loss: 4.1932e-04 - val_loss: 0.0100 - 58s/epoch - 695us/sample
Epoch 13/90
84077/84077 - 59s - loss: 4.1647e-04 - val_loss: 0.0101 - 59s/epoch - 702us/sample
Epoch 14/90
84077/84077 - 59s - loss: 4.1488e-04 - val_loss: 0.0103 - 59s/epoch - 701us/sample
Epoch 15/90
84077/84077 - 59s - loss: 4.1244e-04 - val_loss: 0.0119 - 59s/epoch - 696us/sample
Epoch 16/90
84077/84077 - 59s - loss: 4.1107e-04 - val_loss: 0.0120 - 59s/epoch - 700us/sample
Epoch 17/90
84077/84077 - 59s - loss: 4.1007e-04 - val_loss: 0.0119 - 59s/epoch - 702us/sample
Epoch 18/90
84077/84077 - 59s - loss: 4.0824e-04 - val_loss: 0.0114 - 59s/epoch - 702us/sample
Epoch 19/90
84077/84077 - 59s - loss: 4.0635e-04 - val_loss: 0.0117 - 59s/epoch - 697us/sample
Epoch 20/90
84077/84077 - 59s - loss: 4.0483e-04 - val_loss: 0.0115 - 59s/epoch - 702us/sample
Epoch 21/90
84077/84077 - 59s - loss: 4.0415e-04 - val_loss: 0.0111 - 59s/epoch - 702us/sample
Epoch 22/90
84077/84077 - 59s - loss: 4.0265e-04 - val_loss: 0.0111 - 59s/epoch - 702us/sample
Epoch 23/90
84077/84077 - 59s - loss: 4.0218e-04 - val_loss: 0.0098 - 59s/epoch - 698us/sample
Epoch 24/90
84077/84077 - 59s - loss: 4.0096e-04 - val_loss: 0.0104 - 59s/epoch - 705us/sample
Epoch 25/90
84077/84077 - 59s - loss: 4.0128e-04 - val_loss: 0.0115 - 59s/epoch - 702us/sample
Epoch 26/90
84077/84077 - 59s - loss: 4.0057e-04 - val_loss: 0.0107 - 59s/epoch - 699us/sample
Epoch 27/90
84077/84077 - 59s - loss: 3.9966e-04 - val_loss: 0.0103 - 59s/epoch - 701us/sample
Epoch 28/90
84077/84077 - 59s - loss: 3.9896e-04 - val_loss: 0.0097 - 59s/epoch - 707us/sample
Epoch 29/90
84077/84077 - 59s - loss: 3.9782e-04 - val_loss: 0.0112 - 59s/epoch - 700us/sample
Epoch 30/90
84077/84077 - 58s - loss: 3.9724e-04 - val_loss: 0.0102 - 58s/epoch - 691us/sample
Epoch 31/90
84077/84077 - 59s - loss: 3.9670e-04 - val_loss: 0.0102 - 59s/epoch - 698us/sample
Epoch 32/90
84077/84077 - 59s - loss: 3.9645e-04 - val_loss: 0.0093 - 59s/epoch - 697us/sample
Epoch 33/90
84077/84077 - 59s - loss: 3.9511e-04 - val_loss: 0.0094 - 59s/epoch - 699us/sample
Epoch 34/90
84077/84077 - 58s - loss: 3.9453e-04 - val_loss: 0.0101 - 58s/epoch - 694us/sample
Epoch 35/90
84077/84077 - 59s - loss: 3.9403e-04 - val_loss: 0.0094 - 59s/epoch - 700us/sample
Epoch 36/90
84077/84077 - 59s - loss: 3.9352e-04 - val_loss: 0.0092 - 59s/epoch - 699us/sample
Epoch 37/90
84077/84077 - 58s - loss: 3.9345e-04 - val_loss: 0.0093 - 58s/epoch - 695us/sample
Epoch 38/90
84077/84077 - 59s - loss: 3.9241e-04 - val_loss: 0.0078 - 59s/epoch - 699us/sample
Epoch 39/90
84077/84077 - 59s - loss: 3.9186e-04 - val_loss: 0.0087 - 59s/epoch - 701us/sample
Epoch 40/90
84077/84077 - 59s - loss: 3.9094e-04 - val_loss: 0.0077 - 59s/epoch - 700us/sample
Epoch 41/90
84077/84077 - 59s - loss: 3.8953e-04 - val_loss: 0.0069 - 59s/epoch - 696us/sample
Epoch 42/90
84077/84077 - 59s - loss: 3.8989e-04 - val_loss: 0.0067 - 59s/epoch - 706us/sample
Epoch 43/90
84077/84077 - 60s - loss: 3.8915e-04 - val_loss: 0.0077 - 60s/epoch - 717us/sample
Epoch 44/90
84077/84077 - 60s - loss: 3.8912e-04 - val_loss: 0.0071 - 60s/epoch - 713us/sample
Epoch 45/90
84077/84077 - 60s - loss: 3.8882e-04 - val_loss: 0.0070 - 60s/epoch - 715us/sample
Epoch 46/90
84077/84077 - 60s - loss: 3.8784e-04 - val_loss: 0.0072 - 60s/epoch - 718us/sample
Epoch 47/90
84077/84077 - 60s - loss: 3.8787e-04 - val_loss: 0.0070 - 60s/epoch - 718us/sample
Epoch 48/90
84077/84077 - 60s - loss: 3.8718e-04 - val_loss: 0.0078 - 60s/epoch - 714us/sample
Epoch 49/90
84077/84077 - 60s - loss: 3.8708e-04 - val_loss: 0.0064 - 60s/epoch - 718us/sample
Epoch 50/90
84077/84077 - 60s - loss: 3.8654e-04 - val_loss: 0.0070 - 60s/epoch - 718us/sample
Epoch 51/90
84077/84077 - 60s - loss: 3.8614e-04 - val_loss: 0.0062 - 60s/epoch - 718us/sample
Epoch 52/90
84077/84077 - 60s - loss: 3.8534e-04 - val_loss: 0.0065 - 60s/epoch - 712us/sample
Epoch 53/90
84077/84077 - 60s - loss: 3.8535e-04 - val_loss: 0.0058 - 60s/epoch - 717us/sample
Epoch 54/90
84077/84077 - 61s - loss: 3.8558e-04 - val_loss: 0.0056 - 61s/epoch - 720us/sample
Epoch 55/90
84077/84077 - 60s - loss: 3.8516e-04 - val_loss: 0.0054 - 60s/epoch - 717us/sample
Epoch 56/90
84077/84077 - 60s - loss: 3.8456e-04 - val_loss: 0.0055 - 60s/epoch - 710us/sample
Epoch 57/90
84077/84077 - 60s - loss: 3.8397e-04 - val_loss: 0.0060 - 60s/epoch - 716us/sample
Epoch 58/90
84077/84077 - 60s - loss: 3.8406e-04 - val_loss: 0.0050 - 60s/epoch - 715us/sample
Epoch 59/90
84077/84077 - 60s - loss: 3.8398e-04 - val_loss: 0.0048 - 60s/epoch - 711us/sample
Epoch 60/90
84077/84077 - 60s - loss: 3.8411e-04 - val_loss: 0.0059 - 60s/epoch - 711us/sample
Epoch 61/90
84077/84077 - 60s - loss: 3.8408e-04 - val_loss: 0.0051 - 60s/epoch - 717us/sample
Epoch 62/90
84077/84077 - 60s - loss: 3.8384e-04 - val_loss: 0.0052 - 60s/epoch - 716us/sample
Epoch 63/90
84077/84077 - 60s - loss: 3.8355e-04 - val_loss: 0.0050 - 60s/epoch - 712us/sample
Epoch 64/90
84077/84077 - 60s - loss: 3.8333e-04 - val_loss: 0.0055 - 60s/epoch - 712us/sample
Epoch 65/90
84077/84077 - 60s - loss: 3.8294e-04 - val_loss: 0.0048 - 60s/epoch - 712us/sample
Epoch 66/90
84077/84077 - 60s - loss: 3.8316e-04 - val_loss: 0.0050 - 60s/epoch - 713us/sample
Epoch 67/90
84077/84077 - 60s - loss: 3.8292e-04 - val_loss: 0.0046 - 60s/epoch - 712us/sample
Epoch 68/90
84077/84077 - 60s - loss: 3.8279e-04 - val_loss: 0.0048 - 60s/epoch - 716us/sample
Epoch 69/90
84077/84077 - 60s - loss: 3.8230e-04 - val_loss: 0.0053 - 60s/epoch - 716us/sample
Epoch 70/90
84077/84077 - 60s - loss: 3.8267e-04 - val_loss: 0.0045 - 60s/epoch - 716us/sample
Epoch 71/90
84077/84077 - 60s - loss: 3.8196e-04 - val_loss: 0.0042 - 60s/epoch - 714us/sample
Epoch 72/90
84077/84077 - 60s - loss: 3.8183e-04 - val_loss: 0.0043 - 60s/epoch - 714us/sample
Epoch 73/90
84077/84077 - 61s - loss: 3.8166e-04 - val_loss: 0.0047 - 61s/epoch - 723us/sample
Epoch 74/90
84077/84077 - 60s - loss: 3.8153e-04 - val_loss: 0.0042 - 60s/epoch - 719us/sample
Epoch 75/90
84077/84077 - 60s - loss: 3.8117e-04 - val_loss: 0.0042 - 60s/epoch - 716us/sample
Epoch 76/90
84077/84077 - 60s - loss: 3.8062e-04 - val_loss: 0.0048 - 60s/epoch - 718us/sample
Epoch 77/90
84077/84077 - 61s - loss: 3.8044e-04 - val_loss: 0.0046 - 61s/epoch - 723us/sample
Epoch 78/90
84077/84077 - 60s - loss: 3.8072e-04 - val_loss: 0.0047 - 60s/epoch - 719us/sample
Epoch 79/90
84077/84077 - 60s - loss: 3.8011e-04 - val_loss: 0.0042 - 60s/epoch - 716us/sample
Epoch 80/90
84077/84077 - 60s - loss: 3.7951e-04 - val_loss: 0.0041 - 60s/epoch - 717us/sample
Epoch 81/90
84077/84077 - 59s - loss: 3.7972e-04 - val_loss: 0.0037 - 59s/epoch - 708us/sample
Epoch 82/90
84077/84077 - 60s - loss: 3.7911e-04 - val_loss: 0.0040 - 60s/epoch - 719us/sample
Epoch 83/90
84077/84077 - 60s - loss: 3.7951e-04 - val_loss: 0.0043 - 60s/epoch - 712us/sample
Epoch 84/90
84077/84077 - 60s - loss: 3.7848e-04 - val_loss: 0.0039 - 60s/epoch - 713us/sample
Epoch 85/90
84077/84077 - 60s - loss: 3.7866e-04 - val_loss: 0.0039 - 60s/epoch - 716us/sample
Epoch 86/90
84077/84077 - 60s - loss: 3.7819e-04 - val_loss: 0.0036 - 60s/epoch - 716us/sample
Epoch 87/90
84077/84077 - 59s - loss: 3.7814e-04 - val_loss: 0.0033 - 59s/epoch - 707us/sample
Epoch 88/90
84077/84077 - 60s - loss: 3.7855e-04 - val_loss: 0.0033 - 60s/epoch - 715us/sample
Epoch 89/90
84077/84077 - 60s - loss: 3.7782e-04 - val_loss: 0.0034 - 60s/epoch - 712us/sample
Epoch 90/90
84077/84077 - 60s - loss: 3.7799e-04 - val_loss: 0.0036 - 60s/epoch - 712us/sample
COMPRESSED VECTOR SIZE: 188
Loss in the autoencoder: 0.0036441989299942214
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-12 09:28:41.799412: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_84/outputlayer/BiasAdd' id:106442 op device:{requested: '', assigned: ''} def:{{{node decoder_model_84/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_84/outputlayer/MatMul, decoder_model_84/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2164811513335394
cosine 0.213782954708004
MAE: 0.012841382166220255
RMSE: 0.15269147652871914
r2: -17.21313022064956
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 8, 90, 0.0008, 0.2, 188, 0.00037798893037860424, 0.0036441989299942214, 0.2164811513335394, 0.213782954708004, 0.012841382166220255, 0.15269147652871914, -17.21313022064956, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 10
Fitness    = 515.1509017593391
Last generation's best solutions = [2.0 85 0.0012000000000000001 64 1] with fitness 515.1509017593391.
Best solutions :  [array([2.0, 90, 0.001, 32, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 85, 0.0012000000000000001, 64, 1], dtype=object), array([2.0, 85, 0.0012000000000000001, 64, 1], dtype=object), array([2.0, 85, 0.0012000000000000001, 64, 1], dtype=object)]
Best solutions fitness :  [452.88345516762706, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 512.1418577388646, 515.1509017593391, 515.1509017593391]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:3345: UserWarning: Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.
  warnings.warn("Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.")
Traceback (most recent call last):
  File "genetic.py", line 237, in <module>
    genetic_hypertune_autoencoder(prefix_name = 'geneticVAE_OFM93k',
  File "genetic.py", line 194, in genetic_hypertune_autoencoder
    ga_instance.save(filename=savedir+'ga_instance')
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 3677, in save
    pickle.dump(self, file)
AttributeError: Can't pickle local object 'genetic_hypertune_autoencoder.<locals>.mutation_func'
Sun Feb 12 09:29:43 CET 2023
done
