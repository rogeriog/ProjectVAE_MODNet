start
Tue Feb 14 17:40:30 CET 2023
2023-02-14 17:40:32.164131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 17:40:32.335880: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
         OFM: s^1 - s^1  OFM: s^1 - s^2  ...  OFM: f^14 - f^13  OFM: f^14 - f^14
id                                       ...                                    
id40482        0.000000        0.000000  ...               0.0          0.000000
id40486        0.000000        0.000000  ...               0.0          0.000000
id40487        0.000000        0.000000  ...               0.0          0.000022
id40490        0.000000        0.080160  ...               0.0          0.000000
id40496        0.000000        0.000000  ...               0.0          0.000000
...                 ...             ...  ...               ...               ...
id83741        0.174544        0.746256  ...               0.0          0.000000
id83742        0.000000        0.000000  ...               0.0          1.815492
id83743        0.007311        0.241355  ...               0.0          0.000000
id83744        0.000000        0.000000  ...               0.0          0.439306
id83745        0.000000        0.203940  ...               0.0          0.000000

[93419 rows x 1024 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 90 0.001 64 1]
 [2.1 90 0.001 64 1]
 [1.5 70 0.0005 16 0]
 [2.5 50 0.0005 64 2]
 [2.0 110 0.0005 8 0]
 [2.0 170 0.0005 16 0]
 [2.0 25 0.001 16 1]
 [1.5 90 0.001 32 1]
 [0.5 25 0.001 64 0]
 [1.5 30 0.0005 8 1]]
[2.0 90 0.001 64 1] 0
Shape of dataset to encode: (93419, 943)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1886)        7544        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1886)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-14 17:41:01.079660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 17:41:02.300198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9634 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5
2023-02-14 17:41:02.301467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9634 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:1d:00.0, compute capability: 7.5
2023-02-14 17:41:02.325721: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-14 17:41:02.645753: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense_dec0/bias/v/Assign' id:1104 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_dec0/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_dec0/bias/v, training/Adam/dense_dec0/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 17:41:10.198928: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:450 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 10s - loss: 0.0079 - val_loss: 0.0017 - 10s/epoch - 114us/sample
Epoch 2/90
84077/84077 - 7s - loss: 0.0015 - val_loss: 0.0014 - 7s/epoch - 80us/sample
Epoch 3/90
84077/84077 - 7s - loss: 0.0016 - val_loss: 0.0011 - 7s/epoch - 78us/sample
Epoch 4/90
84077/84077 - 7s - loss: 0.0013 - val_loss: 0.0013 - 7s/epoch - 80us/sample
Epoch 5/90
84077/84077 - 7s - loss: 0.0010 - val_loss: 0.0023 - 7s/epoch - 80us/sample
Epoch 6/90
84077/84077 - 7s - loss: 8.8069e-04 - val_loss: 7.2997e-04 - 7s/epoch - 78us/sample
Epoch 7/90
84077/84077 - 7s - loss: 7.3499e-04 - val_loss: 6.3659e-04 - 7s/epoch - 79us/sample
Epoch 8/90
84077/84077 - 7s - loss: 6.6750e-04 - val_loss: 5.6890e-04 - 7s/epoch - 79us/sample
Epoch 9/90
84077/84077 - 7s - loss: 5.9728e-04 - val_loss: 5.1876e-04 - 7s/epoch - 78us/sample
Epoch 10/90
84077/84077 - 7s - loss: 5.6898e-04 - val_loss: 4.9727e-04 - 7s/epoch - 79us/sample
Epoch 11/90
84077/84077 - 7s - loss: 5.3083e-04 - val_loss: 4.6509e-04 - 7s/epoch - 79us/sample
Epoch 12/90
84077/84077 - 7s - loss: 5.2002e-04 - val_loss: 4.6202e-04 - 7s/epoch - 78us/sample
Epoch 13/90
84077/84077 - 7s - loss: 4.8274e-04 - val_loss: 4.3219e-04 - 7s/epoch - 79us/sample
Epoch 14/90
84077/84077 - 7s - loss: 4.5859e-04 - val_loss: 4.0886e-04 - 7s/epoch - 79us/sample
Epoch 15/90
84077/84077 - 7s - loss: 4.3574e-04 - val_loss: 3.8078e-04 - 7s/epoch - 79us/sample
Epoch 16/90
84077/84077 - 7s - loss: 4.1790e-04 - val_loss: 3.6815e-04 - 7s/epoch - 78us/sample
Epoch 17/90
84077/84077 - 7s - loss: 4.0197e-04 - val_loss: 3.5733e-04 - 7s/epoch - 78us/sample
Epoch 18/90
84077/84077 - 7s - loss: 3.8962e-04 - val_loss: 3.4472e-04 - 7s/epoch - 78us/sample
Epoch 19/90
84077/84077 - 7s - loss: 3.7846e-04 - val_loss: 3.4403e-04 - 7s/epoch - 79us/sample
Epoch 20/90
84077/84077 - 7s - loss: 3.7103e-04 - val_loss: 3.3266e-04 - 7s/epoch - 79us/sample
Epoch 21/90
84077/84077 - 7s - loss: 3.6413e-04 - val_loss: 3.2754e-04 - 7s/epoch - 78us/sample
Epoch 22/90
84077/84077 - 7s - loss: 3.5473e-04 - val_loss: 3.2240e-04 - 7s/epoch - 78us/sample
Epoch 23/90
84077/84077 - 7s - loss: 3.5252e-04 - val_loss: 3.2198e-04 - 7s/epoch - 78us/sample
Epoch 24/90
84077/84077 - 7s - loss: 3.5380e-04 - val_loss: 3.1957e-04 - 7s/epoch - 78us/sample
Epoch 25/90
84077/84077 - 7s - loss: 3.4385e-04 - val_loss: 3.1394e-04 - 7s/epoch - 78us/sample
Epoch 26/90
84077/84077 - 7s - loss: 3.3868e-04 - val_loss: 3.0650e-04 - 7s/epoch - 79us/sample
Epoch 27/90
84077/84077 - 7s - loss: 3.3474e-04 - val_loss: 3.0635e-04 - 7s/epoch - 79us/sample
Epoch 28/90
84077/84077 - 7s - loss: 3.3504e-04 - val_loss: 3.0518e-04 - 7s/epoch - 78us/sample
Epoch 29/90
84077/84077 - 7s - loss: 3.2942e-04 - val_loss: 3.0176e-04 - 7s/epoch - 79us/sample
Epoch 30/90
84077/84077 - 7s - loss: 3.2549e-04 - val_loss: 2.9969e-04 - 7s/epoch - 78us/sample
Epoch 31/90
84077/84077 - 7s - loss: 3.2561e-04 - val_loss: 2.9782e-04 - 7s/epoch - 78us/sample
Epoch 32/90
84077/84077 - 7s - loss: 3.2082e-04 - val_loss: 2.9528e-04 - 7s/epoch - 79us/sample
Epoch 33/90
84077/84077 - 7s - loss: 3.2151e-04 - val_loss: 2.9363e-04 - 7s/epoch - 79us/sample
Epoch 34/90
84077/84077 - 7s - loss: 3.1690e-04 - val_loss: 2.9321e-04 - 7s/epoch - 78us/sample
Epoch 35/90
84077/84077 - 7s - loss: 3.1605e-04 - val_loss: 2.9210e-04 - 7s/epoch - 78us/sample
Epoch 36/90
84077/84077 - 7s - loss: 3.1382e-04 - val_loss: 2.8963e-04 - 7s/epoch - 78us/sample
Epoch 37/90
84077/84077 - 7s - loss: 3.1269e-04 - val_loss: 2.8749e-04 - 7s/epoch - 78us/sample
Epoch 38/90
84077/84077 - 7s - loss: 3.1105e-04 - val_loss: 2.8852e-04 - 7s/epoch - 78us/sample
Epoch 39/90
84077/84077 - 7s - loss: 3.0984e-04 - val_loss: 2.8460e-04 - 7s/epoch - 79us/sample
Epoch 40/90
84077/84077 - 7s - loss: 3.0885e-04 - val_loss: 2.8582e-04 - 7s/epoch - 78us/sample
Epoch 41/90
84077/84077 - 7s - loss: 3.0678e-04 - val_loss: 2.8664e-04 - 7s/epoch - 78us/sample
Epoch 42/90
84077/84077 - 7s - loss: 3.0555e-04 - val_loss: 2.8472e-04 - 7s/epoch - 78us/sample
Epoch 43/90
84077/84077 - 7s - loss: 3.0467e-04 - val_loss: 2.8396e-04 - 7s/epoch - 78us/sample
Epoch 44/90
84077/84077 - 7s - loss: 3.0404e-04 - val_loss: 2.8129e-04 - 7s/epoch - 79us/sample
Epoch 45/90
84077/84077 - 7s - loss: 3.0144e-04 - val_loss: 2.7939e-04 - 7s/epoch - 79us/sample
Epoch 46/90
84077/84077 - 7s - loss: 3.0179e-04 - val_loss: 2.7914e-04 - 7s/epoch - 79us/sample
Epoch 47/90
84077/84077 - 7s - loss: 3.0002e-04 - val_loss: 2.7758e-04 - 7s/epoch - 79us/sample
Epoch 48/90
84077/84077 - 7s - loss: 2.9930e-04 - val_loss: 2.7790e-04 - 7s/epoch - 78us/sample
Epoch 49/90
84077/84077 - 7s - loss: 3.0107e-04 - val_loss: 8.8612e-04 - 7s/epoch - 78us/sample
Epoch 50/90
84077/84077 - 7s - loss: 3.0685e-04 - val_loss: 2.7929e-04 - 7s/epoch - 78us/sample
Epoch 51/90
84077/84077 - 7s - loss: 2.9591e-04 - val_loss: 2.7538e-04 - 7s/epoch - 79us/sample
Epoch 52/90
84077/84077 - 7s - loss: 2.9546e-04 - val_loss: 2.7608e-04 - 7s/epoch - 79us/sample
Epoch 53/90
84077/84077 - 7s - loss: 2.9538e-04 - val_loss: 2.7558e-04 - 7s/epoch - 79us/sample
Epoch 54/90
84077/84077 - 7s - loss: 2.9439e-04 - val_loss: 2.7706e-04 - 7s/epoch - 78us/sample
Epoch 55/90
84077/84077 - 7s - loss: 2.9414e-04 - val_loss: 2.7391e-04 - 7s/epoch - 79us/sample
Epoch 56/90
84077/84077 - 7s - loss: 2.9790e-04 - val_loss: 2.7367e-04 - 7s/epoch - 78us/sample
Epoch 57/90
84077/84077 - 7s - loss: 2.9307e-04 - val_loss: 2.7189e-04 - 7s/epoch - 79us/sample
Epoch 58/90
84077/84077 - 7s - loss: 2.9218e-04 - val_loss: 2.7224e-04 - 7s/epoch - 78us/sample
Epoch 59/90
84077/84077 - 7s - loss: 2.9031e-04 - val_loss: 2.7067e-04 - 7s/epoch - 79us/sample
Epoch 60/90
84077/84077 - 7s - loss: 2.9092e-04 - val_loss: 2.7269e-04 - 7s/epoch - 79us/sample
Epoch 61/90
84077/84077 - 7s - loss: 2.8939e-04 - val_loss: 2.6874e-04 - 7s/epoch - 79us/sample
Epoch 62/90
84077/84077 - 7s - loss: 2.8926e-04 - val_loss: 2.6934e-04 - 7s/epoch - 78us/sample
Epoch 63/90
84077/84077 - 7s - loss: 2.8832e-04 - val_loss: 2.6695e-04 - 7s/epoch - 78us/sample
Epoch 64/90
84077/84077 - 7s - loss: 2.8879e-04 - val_loss: 2.6896e-04 - 7s/epoch - 78us/sample
Epoch 65/90
84077/84077 - 7s - loss: 2.8648e-04 - val_loss: 2.6498e-04 - 7s/epoch - 78us/sample
Epoch 66/90
84077/84077 - 7s - loss: 2.8582e-04 - val_loss: 2.6699e-04 - 7s/epoch - 78us/sample
Epoch 67/90
84077/84077 - 7s - loss: 2.8689e-04 - val_loss: 2.6646e-04 - 7s/epoch - 79us/sample
Epoch 68/90
84077/84077 - 7s - loss: 2.8542e-04 - val_loss: 2.6630e-04 - 7s/epoch - 80us/sample
Epoch 69/90
84077/84077 - 7s - loss: 2.8517e-04 - val_loss: 2.6553e-04 - 7s/epoch - 79us/sample
Epoch 70/90
84077/84077 - 7s - loss: 2.8425e-04 - val_loss: 2.6200e-04 - 7s/epoch - 78us/sample
Epoch 71/90
84077/84077 - 7s - loss: 2.8463e-04 - val_loss: 2.6668e-04 - 7s/epoch - 78us/sample
Epoch 72/90
84077/84077 - 7s - loss: 2.8460e-04 - val_loss: 2.6575e-04 - 7s/epoch - 78us/sample
Epoch 73/90
84077/84077 - 7s - loss: 2.8292e-04 - val_loss: 2.6270e-04 - 7s/epoch - 78us/sample
Epoch 74/90
84077/84077 - 7s - loss: 2.8270e-04 - val_loss: 2.6119e-04 - 7s/epoch - 78us/sample
Epoch 75/90
84077/84077 - 7s - loss: 2.8237e-04 - val_loss: 2.6219e-04 - 7s/epoch - 79us/sample
Epoch 76/90
84077/84077 - 7s - loss: 2.8233e-04 - val_loss: 2.6236e-04 - 7s/epoch - 79us/sample
Epoch 77/90
84077/84077 - 7s - loss: 2.8172e-04 - val_loss: 2.6139e-04 - 7s/epoch - 79us/sample
Epoch 78/90
84077/84077 - 7s - loss: 2.8121e-04 - val_loss: 2.6233e-04 - 7s/epoch - 78us/sample
Epoch 79/90
84077/84077 - 7s - loss: 2.8023e-04 - val_loss: 2.6064e-04 - 7s/epoch - 78us/sample
Epoch 80/90
84077/84077 - 7s - loss: 2.8077e-04 - val_loss: 2.6233e-04 - 7s/epoch - 78us/sample
Epoch 81/90
84077/84077 - 7s - loss: 2.8286e-04 - val_loss: 2.6255e-04 - 7s/epoch - 78us/sample
Epoch 82/90
84077/84077 - 7s - loss: 2.7954e-04 - val_loss: 2.5891e-04 - 7s/epoch - 78us/sample
Epoch 83/90
84077/84077 - 7s - loss: 2.7982e-04 - val_loss: 2.5997e-04 - 7s/epoch - 79us/sample
Epoch 84/90
84077/84077 - 7s - loss: 2.8083e-04 - val_loss: 2.6081e-04 - 7s/epoch - 79us/sample
Epoch 85/90
84077/84077 - 7s - loss: 2.7774e-04 - val_loss: 2.5676e-04 - 7s/epoch - 78us/sample
Epoch 86/90
84077/84077 - 7s - loss: 2.7767e-04 - val_loss: 2.5719e-04 - 7s/epoch - 78us/sample
Epoch 87/90
84077/84077 - 7s - loss: 2.7835e-04 - val_loss: 2.5987e-04 - 7s/epoch - 78us/sample
Epoch 88/90
84077/84077 - 7s - loss: 2.7765e-04 - val_loss: 2.5982e-04 - 7s/epoch - 78us/sample
Epoch 89/90
84077/84077 - 7s - loss: 2.7686e-04 - val_loss: 2.5724e-04 - 7s/epoch - 78us/sample
Epoch 90/90
84077/84077 - 7s - loss: 2.7748e-04 - val_loss: 2.5858e-04 - 7s/epoch - 79us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00025857719936812055
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 17:50:58.344977: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:421 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03305430431782609
cosine 0.03265518468475762
MAE: 0.0026447137863143855
RMSE: 0.01015842261106522
r2: 0.9196500518890846
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 90, 0.001, 0.1, 94, 0.00027748171522448705, 0.00025857719936812055, 0.03305430431782609, 0.03265518468475762, 0.0026447137863143855, 0.01015842261106522, 0.9196500518890846, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1980)        7920        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1980)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           186214      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           186214      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2073409     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,322,877
Trainable params: 4,314,769
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 17:51:04.025255: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/batch_normalization_4/beta/v/Assign' id:2348 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/batch_normalization_4/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/batch_normalization_4/beta/v, training_2/Adam/batch_normalization_4/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 17:51:11.033373: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1711 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 8s - loss: 0.0076 - val_loss: 0.0019 - 8s/epoch - 91us/sample
Epoch 2/90
84077/84077 - 7s - loss: 0.0017 - val_loss: 0.0015 - 7s/epoch - 80us/sample
Epoch 3/90
84077/84077 - 7s - loss: 0.0015 - val_loss: 0.0012 - 7s/epoch - 80us/sample
Epoch 4/90
84077/84077 - 7s - loss: 0.0013 - val_loss: 0.0011 - 7s/epoch - 80us/sample
Epoch 5/90
84077/84077 - 7s - loss: 0.0011 - val_loss: 8.0198e-04 - 7s/epoch - 80us/sample
Epoch 6/90
84077/84077 - 7s - loss: 8.7449e-04 - val_loss: 8.0210e-04 - 7s/epoch - 80us/sample
Epoch 7/90
84077/84077 - 7s - loss: 7.2070e-04 - val_loss: 5.9074e-04 - 7s/epoch - 80us/sample
Epoch 8/90
84077/84077 - 7s - loss: 6.2311e-04 - val_loss: 5.5937e-04 - 7s/epoch - 80us/sample
Epoch 9/90
84077/84077 - 7s - loss: 5.6943e-04 - val_loss: 4.9335e-04 - 7s/epoch - 80us/sample
Epoch 10/90
84077/84077 - 7s - loss: 5.2648e-04 - val_loss: 4.5596e-04 - 7s/epoch - 80us/sample
Epoch 11/90
84077/84077 - 7s - loss: 4.9575e-04 - val_loss: 4.3680e-04 - 7s/epoch - 80us/sample
Epoch 12/90
84077/84077 - 7s - loss: 4.7277e-04 - val_loss: 4.0940e-04 - 7s/epoch - 80us/sample
Epoch 13/90
84077/84077 - 7s - loss: 4.4133e-04 - val_loss: 3.8800e-04 - 7s/epoch - 80us/sample
Epoch 14/90
84077/84077 - 7s - loss: 4.2139e-04 - val_loss: 3.7045e-04 - 7s/epoch - 80us/sample
Epoch 15/90
84077/84077 - 7s - loss: 4.0541e-04 - val_loss: 3.5947e-04 - 7s/epoch - 80us/sample
Epoch 16/90
84077/84077 - 7s - loss: 3.9576e-04 - val_loss: 3.5282e-04 - 7s/epoch - 80us/sample
Epoch 17/90
84077/84077 - 7s - loss: 3.8319e-04 - val_loss: 3.4394e-04 - 7s/epoch - 81us/sample
Epoch 18/90
84077/84077 - 7s - loss: 3.7293e-04 - val_loss: 3.3496e-04 - 7s/epoch - 80us/sample
Epoch 19/90
84077/84077 - 7s - loss: 3.6686e-04 - val_loss: 3.3205e-04 - 7s/epoch - 80us/sample
Epoch 20/90
84077/84077 - 7s - loss: 3.5963e-04 - val_loss: 3.2411e-04 - 7s/epoch - 80us/sample
Epoch 21/90
84077/84077 - 7s - loss: 3.5353e-04 - val_loss: 3.1779e-04 - 7s/epoch - 80us/sample
Epoch 22/90
84077/84077 - 7s - loss: 3.4934e-04 - val_loss: 3.1727e-04 - 7s/epoch - 80us/sample
Epoch 23/90
84077/84077 - 7s - loss: 3.4373e-04 - val_loss: 3.1409e-04 - 7s/epoch - 80us/sample
Epoch 24/90
84077/84077 - 7s - loss: 3.4062e-04 - val_loss: 3.0961e-04 - 7s/epoch - 80us/sample
Epoch 25/90
84077/84077 - 7s - loss: 3.3641e-04 - val_loss: 3.0521e-04 - 7s/epoch - 81us/sample
Epoch 26/90
84077/84077 - 7s - loss: 3.3314e-04 - val_loss: 3.0444e-04 - 7s/epoch - 80us/sample
Epoch 27/90
84077/84077 - 7s - loss: 3.2953e-04 - val_loss: 3.0645e-04 - 7s/epoch - 80us/sample
Epoch 28/90
84077/84077 - 7s - loss: 3.2724e-04 - val_loss: 3.1090e-04 - 7s/epoch - 80us/sample
Epoch 29/90
84077/84077 - 7s - loss: 3.2602e-04 - val_loss: 3.0337e-04 - 7s/epoch - 79us/sample
Epoch 30/90
84077/84077 - 7s - loss: 3.2303e-04 - val_loss: 2.9906e-04 - 7s/epoch - 80us/sample
Epoch 31/90
84077/84077 - 7s - loss: 3.1996e-04 - val_loss: 2.9480e-04 - 7s/epoch - 81us/sample
Epoch 32/90
84077/84077 - 7s - loss: 3.1805e-04 - val_loss: 2.9344e-04 - 7s/epoch - 80us/sample
Epoch 33/90
84077/84077 - 7s - loss: 3.1665e-04 - val_loss: 2.9387e-04 - 7s/epoch - 80us/sample
Epoch 34/90
84077/84077 - 7s - loss: 3.1461e-04 - val_loss: 2.9371e-04 - 7s/epoch - 80us/sample
Epoch 35/90
84077/84077 - 7s - loss: 3.1292e-04 - val_loss: 2.9442e-04 - 7s/epoch - 80us/sample
Epoch 36/90
84077/84077 - 7s - loss: 3.1123e-04 - val_loss: 2.8936e-04 - 7s/epoch - 80us/sample
Epoch 37/90
84077/84077 - 7s - loss: 3.0953e-04 - val_loss: 2.8765e-04 - 7s/epoch - 80us/sample
Epoch 38/90
84077/84077 - 7s - loss: 3.0879e-04 - val_loss: 2.8613e-04 - 7s/epoch - 80us/sample
Epoch 39/90
84077/84077 - 7s - loss: 3.0727e-04 - val_loss: 2.8386e-04 - 7s/epoch - 80us/sample
Epoch 40/90
84077/84077 - 7s - loss: 3.0703e-04 - val_loss: 2.8800e-04 - 7s/epoch - 80us/sample
Epoch 41/90
84077/84077 - 7s - loss: 3.0424e-04 - val_loss: 2.8579e-04 - 7s/epoch - 80us/sample
Epoch 42/90
84077/84077 - 7s - loss: 3.0245e-04 - val_loss: 2.8443e-04 - 7s/epoch - 80us/sample
Epoch 43/90
84077/84077 - 7s - loss: 3.0138e-04 - val_loss: 2.7997e-04 - 7s/epoch - 80us/sample
Epoch 44/90
84077/84077 - 7s - loss: 3.0141e-04 - val_loss: 2.8369e-04 - 7s/epoch - 80us/sample
Epoch 45/90
84077/84077 - 7s - loss: 2.9948e-04 - val_loss: 2.8119e-04 - 7s/epoch - 80us/sample
Epoch 46/90
84077/84077 - 7s - loss: 2.9836e-04 - val_loss: 2.7942e-04 - 7s/epoch - 80us/sample
Epoch 47/90
84077/84077 - 7s - loss: 2.9758e-04 - val_loss: 2.7989e-04 - 7s/epoch - 80us/sample
Epoch 48/90
84077/84077 - 7s - loss: 2.9577e-04 - val_loss: 2.7932e-04 - 7s/epoch - 80us/sample
Epoch 49/90
84077/84077 - 7s - loss: 2.9594e-04 - val_loss: 2.7871e-04 - 7s/epoch - 80us/sample
Epoch 50/90
84077/84077 - 7s - loss: 2.9483e-04 - val_loss: 2.7854e-04 - 7s/epoch - 80us/sample
Epoch 51/90
84077/84077 - 7s - loss: 2.9410e-04 - val_loss: 2.7682e-04 - 7s/epoch - 80us/sample
Epoch 52/90
84077/84077 - 7s - loss: 2.9393e-04 - val_loss: 2.7477e-04 - 7s/epoch - 80us/sample
Epoch 53/90
84077/84077 - 7s - loss: 2.9202e-04 - val_loss: 2.7376e-04 - 7s/epoch - 80us/sample
Epoch 54/90
84077/84077 - 7s - loss: 2.9143e-04 - val_loss: 2.7532e-04 - 7s/epoch - 80us/sample
Epoch 55/90
84077/84077 - 7s - loss: 2.9066e-04 - val_loss: 2.7197e-04 - 7s/epoch - 80us/sample
Epoch 56/90
84077/84077 - 7s - loss: 2.8981e-04 - val_loss: 2.7263e-04 - 7s/epoch - 80us/sample
Epoch 57/90
84077/84077 - 7s - loss: 2.8973e-04 - val_loss: 2.7325e-04 - 7s/epoch - 80us/sample
Epoch 58/90
84077/84077 - 7s - loss: 2.8818e-04 - val_loss: 2.6952e-04 - 7s/epoch - 79us/sample
Epoch 59/90
84077/84077 - 7s - loss: 2.8789e-04 - val_loss: 2.7383e-04 - 7s/epoch - 80us/sample
Epoch 60/90
84077/84077 - 7s - loss: 2.8689e-04 - val_loss: 2.7140e-04 - 7s/epoch - 81us/sample
Epoch 61/90
84077/84077 - 7s - loss: 2.8808e-04 - val_loss: 2.7162e-04 - 7s/epoch - 80us/sample
Epoch 62/90
84077/84077 - 7s - loss: 2.8699e-04 - val_loss: 2.7049e-04 - 7s/epoch - 80us/sample
Epoch 63/90
84077/84077 - 7s - loss: 2.8682e-04 - val_loss: 2.6753e-04 - 7s/epoch - 80us/sample
Epoch 64/90
84077/84077 - 7s - loss: 2.8620e-04 - val_loss: 2.6589e-04 - 7s/epoch - 80us/sample
Epoch 65/90
84077/84077 - 7s - loss: 2.8481e-04 - val_loss: 2.6851e-04 - 7s/epoch - 80us/sample
Epoch 66/90
84077/84077 - 7s - loss: 2.8377e-04 - val_loss: 2.6737e-04 - 7s/epoch - 81us/sample
Epoch 67/90
84077/84077 - 7s - loss: 2.8356e-04 - val_loss: 2.6269e-04 - 7s/epoch - 80us/sample
Epoch 68/90
84077/84077 - 7s - loss: 2.8343e-04 - val_loss: 2.6712e-04 - 7s/epoch - 80us/sample
Epoch 69/90
84077/84077 - 7s - loss: 2.8228e-04 - val_loss: 2.6622e-04 - 7s/epoch - 80us/sample
Epoch 70/90
84077/84077 - 7s - loss: 2.8228e-04 - val_loss: 2.6426e-04 - 7s/epoch - 80us/sample
Epoch 71/90
84077/84077 - 7s - loss: 2.8196e-04 - val_loss: 2.6439e-04 - 7s/epoch - 80us/sample
Epoch 72/90
84077/84077 - 7s - loss: 2.8202e-04 - val_loss: 2.6259e-04 - 7s/epoch - 80us/sample
Epoch 73/90
84077/84077 - 7s - loss: 2.8078e-04 - val_loss: 2.6259e-04 - 7s/epoch - 80us/sample
Epoch 74/90
84077/84077 - 7s - loss: 2.8038e-04 - val_loss: 2.6144e-04 - 7s/epoch - 80us/sample
Epoch 75/90
84077/84077 - 7s - loss: 2.8054e-04 - val_loss: 2.6156e-04 - 7s/epoch - 80us/sample
Epoch 76/90
84077/84077 - 7s - loss: 2.7975e-04 - val_loss: 2.6494e-04 - 7s/epoch - 80us/sample
Epoch 77/90
84077/84077 - 7s - loss: 2.7859e-04 - val_loss: 2.6012e-04 - 7s/epoch - 80us/sample
Epoch 78/90
84077/84077 - 7s - loss: 2.7902e-04 - val_loss: 2.6156e-04 - 7s/epoch - 80us/sample
Epoch 79/90
84077/84077 - 7s - loss: 2.7813e-04 - val_loss: 2.6327e-04 - 7s/epoch - 81us/sample
Epoch 80/90
84077/84077 - 7s - loss: 2.7820e-04 - val_loss: 2.5967e-04 - 7s/epoch - 80us/sample
Epoch 81/90
84077/84077 - 7s - loss: 2.7886e-04 - val_loss: 2.6146e-04 - 7s/epoch - 80us/sample
Epoch 82/90
84077/84077 - 7s - loss: 2.7760e-04 - val_loss: 2.5964e-04 - 7s/epoch - 80us/sample
Epoch 83/90
84077/84077 - 7s - loss: 2.7734e-04 - val_loss: 2.5940e-04 - 7s/epoch - 80us/sample
Epoch 84/90
84077/84077 - 7s - loss: 2.7705e-04 - val_loss: 2.5966e-04 - 7s/epoch - 80us/sample
Epoch 85/90
84077/84077 - 7s - loss: 2.7558e-04 - val_loss: 2.5981e-04 - 7s/epoch - 80us/sample
Epoch 86/90
84077/84077 - 7s - loss: 2.7588e-04 - val_loss: 2.6314e-04 - 7s/epoch - 80us/sample
Epoch 87/90
84077/84077 - 7s - loss: 2.7589e-04 - val_loss: 2.5768e-04 - 7s/epoch - 80us/sample
Epoch 88/90
84077/84077 - 7s - loss: 2.7590e-04 - val_loss: 2.5947e-04 - 7s/epoch - 80us/sample
Epoch 89/90
84077/84077 - 7s - loss: 2.7588e-04 - val_loss: 2.5578e-04 - 7s/epoch - 80us/sample
Epoch 90/90
84077/84077 - 7s - loss: 2.7449e-04 - val_loss: 2.5779e-04 - 7s/epoch - 80us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002577936554896749
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:01:09.695835: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1682 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03116168907290548
cosine 0.03078813070412019
MAE: 0.0023557748467938704
RMSE: 0.010007373023629987
r2: 0.9219246203393552
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.001, 0.1, 94, 0.0002744917465101689, 0.0002577936554896749, 0.03116168907290548, 0.03078813070412019, 0.0023557748467938704, 0.010007373023629987, 0.9219246203393552, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 70 0.0005 16 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1414)        5656        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1414)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           133010      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           133010      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1483637     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,090,129
Trainable params: 3,084,285
Non-trainable params: 5,844
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/70
2023-02-14 18:01:15.999729: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/bottleneck_zmean_2/kernel/v/Assign' id:3649 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/bottleneck_zmean_2/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/bottleneck_zmean_2/kernel/v, training_4/Adam/bottleneck_zmean_2/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:01:33.031463: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2988 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0675 - val_loss: 0.0674 - 18s/epoch - 217us/sample
Epoch 2/70
84077/84077 - 17s - loss: 0.0669 - val_loss: 0.0674 - 17s/epoch - 203us/sample
Epoch 3/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 4/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 5/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 6/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 7/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 8/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 9/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 10/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 11/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 12/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 13/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 14/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 201us/sample
Epoch 15/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 16/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 17/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 18/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 19/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 20/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 21/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 22/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 23/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 24/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 25/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 26/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 27/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 28/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 29/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 30/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 31/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 32/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 33/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 34/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 35/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 36/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 37/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 38/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 39/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 40/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 41/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 42/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 43/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 44/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 45/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 46/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 47/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 48/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 49/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 50/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 51/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 52/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 53/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 54/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 55/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 56/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 202us/sample
Epoch 57/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 58/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 59/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 60/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 61/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 62/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 63/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 64/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 65/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 66/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 203us/sample
Epoch 67/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 68/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 69/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 70/70
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.06734572882616298
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:21:16.852724: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2940 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1420899524905745
cosine 1.148874689754992
MAE: 4.424408163427705
RMSE: 4.696073574800814
r2: -16295.475781117299
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'binary_crossentropy', 16, 70, 0.0005, 0.1, 94, 0.06683691417779812, 0.06734572882616298, 1.1420899524905745, 1.148874689754992, 4.424408163427705, 4.696073574800814, -16295.475781117299, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 2357)        9428        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 2357)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 18:21:22.786863: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_3/bias/Assign' id:4193 op device:{requested: '', assigned: ''} def:{{{node outputlayer_3/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_3/bias, outputlayer_3/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:21:30.027328: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4316 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 8s - loss: 0.0342 - val_loss: 0.0013 - 8s/epoch - 96us/sample
Epoch 2/50
84077/84077 - 7s - loss: 0.0039 - val_loss: 0.0012 - 7s/epoch - 82us/sample
Epoch 3/50
84077/84077 - 7s - loss: 8.3331e-04 - val_loss: 8.8081e-04 - 7s/epoch - 82us/sample
Epoch 4/50
84077/84077 - 7s - loss: 8.4570e-04 - val_loss: 7.7785e-04 - 7s/epoch - 82us/sample
Epoch 5/50
84077/84077 - 7s - loss: 7.3205e-04 - val_loss: 7.4977e-04 - 7s/epoch - 81us/sample
Epoch 6/50
84077/84077 - 7s - loss: 6.2560e-04 - val_loss: 5.4081e-04 - 7s/epoch - 82us/sample
Epoch 7/50
84077/84077 - 7s - loss: 5.0776e-04 - val_loss: 4.5643e-04 - 7s/epoch - 82us/sample
Epoch 8/50
84077/84077 - 7s - loss: 4.4646e-04 - val_loss: 4.0527e-04 - 7s/epoch - 82us/sample
Epoch 9/50
84077/84077 - 7s - loss: 4.0858e-04 - val_loss: 3.7147e-04 - 7s/epoch - 82us/sample
Epoch 10/50
84077/84077 - 7s - loss: 3.8351e-04 - val_loss: 3.5420e-04 - 7s/epoch - 81us/sample
Epoch 11/50
84077/84077 - 7s - loss: 3.6299e-04 - val_loss: 3.3005e-04 - 7s/epoch - 82us/sample
Epoch 12/50
84077/84077 - 7s - loss: 3.4509e-04 - val_loss: 3.2221e-04 - 7s/epoch - 82us/sample
Epoch 13/50
84077/84077 - 7s - loss: 3.3203e-04 - val_loss: 3.0595e-04 - 7s/epoch - 82us/sample
Epoch 14/50
84077/84077 - 7s - loss: 3.1893e-04 - val_loss: 3.0083e-04 - 7s/epoch - 82us/sample
Epoch 15/50
84077/84077 - 7s - loss: 3.1043e-04 - val_loss: 2.9106e-04 - 7s/epoch - 82us/sample
Epoch 16/50
84077/84077 - 7s - loss: 3.0090e-04 - val_loss: 2.7916e-04 - 7s/epoch - 82us/sample
Epoch 17/50
84077/84077 - 7s - loss: 2.9345e-04 - val_loss: 2.7265e-04 - 7s/epoch - 81us/sample
Epoch 18/50
84077/84077 - 7s - loss: 2.8819e-04 - val_loss: 2.6817e-04 - 7s/epoch - 81us/sample
Epoch 19/50
84077/84077 - 7s - loss: 2.8348e-04 - val_loss: 2.6516e-04 - 7s/epoch - 82us/sample
Epoch 20/50
84077/84077 - 7s - loss: 2.7739e-04 - val_loss: 2.5976e-04 - 7s/epoch - 82us/sample
Epoch 21/50
84077/84077 - 7s - loss: 2.7187e-04 - val_loss: 2.5620e-04 - 7s/epoch - 82us/sample
Epoch 22/50
84077/84077 - 7s - loss: 2.6752e-04 - val_loss: 2.4762e-04 - 7s/epoch - 82us/sample
Epoch 23/50
84077/84077 - 7s - loss: 2.6329e-04 - val_loss: 2.4478e-04 - 7s/epoch - 82us/sample
Epoch 24/50
84077/84077 - 7s - loss: 2.5972e-04 - val_loss: 2.4471e-04 - 7s/epoch - 82us/sample
Epoch 25/50
84077/84077 - 7s - loss: 2.5654e-04 - val_loss: 2.3948e-04 - 7s/epoch - 82us/sample
Epoch 26/50
84077/84077 - 7s - loss: 2.5378e-04 - val_loss: 2.3760e-04 - 7s/epoch - 82us/sample
Epoch 27/50
84077/84077 - 7s - loss: 2.5192e-04 - val_loss: 2.3484e-04 - 7s/epoch - 82us/sample
Epoch 28/50
84077/84077 - 7s - loss: 2.4950e-04 - val_loss: 2.3377e-04 - 7s/epoch - 82us/sample
Epoch 29/50
84077/84077 - 7s - loss: 2.4692e-04 - val_loss: 2.2959e-04 - 7s/epoch - 82us/sample
Epoch 30/50
84077/84077 - 7s - loss: 2.4467e-04 - val_loss: 2.3030e-04 - 7s/epoch - 82us/sample
Epoch 31/50
84077/84077 - 7s - loss: 2.4283e-04 - val_loss: 2.2774e-04 - 7s/epoch - 82us/sample
Epoch 32/50
84077/84077 - 7s - loss: 2.4084e-04 - val_loss: 2.2687e-04 - 7s/epoch - 82us/sample
Epoch 33/50
84077/84077 - 7s - loss: 2.3873e-04 - val_loss: 2.2569e-04 - 7s/epoch - 82us/sample
Epoch 34/50
84077/84077 - 7s - loss: 2.3688e-04 - val_loss: 2.2178e-04 - 7s/epoch - 82us/sample
Epoch 35/50
84077/84077 - 7s - loss: 2.3547e-04 - val_loss: 2.2192e-04 - 7s/epoch - 82us/sample
Epoch 36/50
84077/84077 - 7s - loss: 2.3421e-04 - val_loss: 2.2139e-04 - 7s/epoch - 82us/sample
Epoch 37/50
84077/84077 - 7s - loss: 2.3250e-04 - val_loss: 2.1849e-04 - 7s/epoch - 82us/sample
Epoch 38/50
84077/84077 - 7s - loss: 2.3152e-04 - val_loss: 2.1900e-04 - 7s/epoch - 82us/sample
Epoch 39/50
84077/84077 - 7s - loss: 2.3058e-04 - val_loss: 2.1803e-04 - 7s/epoch - 83us/sample
Epoch 40/50
84077/84077 - 7s - loss: 2.2914e-04 - val_loss: 2.1813e-04 - 7s/epoch - 81us/sample
Epoch 41/50
84077/84077 - 7s - loss: 2.2827e-04 - val_loss: 2.1650e-04 - 7s/epoch - 82us/sample
Epoch 42/50
84077/84077 - 7s - loss: 2.2695e-04 - val_loss: 2.1372e-04 - 7s/epoch - 82us/sample
Epoch 43/50
84077/84077 - 7s - loss: 2.2579e-04 - val_loss: 2.1475e-04 - 7s/epoch - 82us/sample
Epoch 44/50
84077/84077 - 7s - loss: 2.2533e-04 - val_loss: 2.1328e-04 - 7s/epoch - 82us/sample
Epoch 45/50
84077/84077 - 7s - loss: 2.2491e-04 - val_loss: 2.1398e-04 - 7s/epoch - 82us/sample
Epoch 46/50
84077/84077 - 7s - loss: 2.2360e-04 - val_loss: 2.1069e-04 - 7s/epoch - 82us/sample
Epoch 47/50
84077/84077 - 7s - loss: 2.2348e-04 - val_loss: 2.1191e-04 - 7s/epoch - 82us/sample
Epoch 48/50
84077/84077 - 7s - loss: 2.2268e-04 - val_loss: 2.0983e-04 - 7s/epoch - 82us/sample
Epoch 49/50
84077/84077 - 7s - loss: 2.2174e-04 - val_loss: 2.1058e-04 - 7s/epoch - 82us/sample
Epoch 50/50
84077/84077 - 7s - loss: 2.2147e-04 - val_loss: 2.0902e-04 - 7s/epoch - 82us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00020901794444214936
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:27:07.651252: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4280 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.060868155038690974
cosine 0.06013082287759351
MAE: 0.003112934856061882
RMSE: 0.013811896311752852
r2: 0.8510516992643842
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 50, 0.0005, 0.1, 94, 0.00022146688246039802, 0.00020901794444214936, 0.060868155038690974, 0.06013082287759351, 0.003112934856061882, 0.013811896311752852, 0.8510516992643842, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 110 0.0005 8 0] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 1886)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/110
2023-02-14 18:27:13.825457: W tensorflow/c/c_api.cc:291] Operation '{name:'training_8/Adam/batch_normalization_14/beta/v/Assign' id:6341 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/batch_normalization_14/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/batch_normalization_14/beta/v, training_8/Adam/batch_normalization_14/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:27:44.738512: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5616 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 33s - loss: 0.0672 - val_loss: 0.0674 - 33s/epoch - 393us/sample
Epoch 2/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 3/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 4/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 5/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 6/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 7/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 8/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 9/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 10/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 11/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 12/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 375us/sample
Epoch 13/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 14/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 15/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 16/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 17/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 18/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 19/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 20/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 21/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 22/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 23/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 24/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 25/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 26/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 27/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 28/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 29/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 30/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 31/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 32/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 33/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 34/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 35/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 36/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 37/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 38/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 39/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 40/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 41/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 42/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 43/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 44/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 45/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 46/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 47/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 48/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 49/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 375us/sample
Epoch 50/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 51/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 52/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 53/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 54/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 55/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 56/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 57/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 58/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 59/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 60/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 61/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 62/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 63/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 64/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 65/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 66/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 67/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 68/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 69/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 70/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 71/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 72/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 73/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 74/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 75/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 76/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 77/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 78/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 79/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 80/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 81/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 82/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 83/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 84/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 85/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 86/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 87/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 88/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 89/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 90/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 91/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 375us/sample
Epoch 92/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 93/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 94/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 375us/sample
Epoch 95/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 96/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 97/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 98/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 99/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 100/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 101/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 102/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 103/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 376us/sample
Epoch 104/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 105/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
Epoch 106/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 377us/sample
Epoch 107/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 373us/sample
Epoch 108/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 109/110
84077/84077 - 32s - loss: 0.0668 - val_loss: 0.0673 - 32s/epoch - 375us/sample
Epoch 110/110
84077/84077 - 31s - loss: 0.0668 - val_loss: 0.0673 - 31s/epoch - 374us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0673457299076217
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:25:04.697956: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:5568 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.019091822361397
cosine 1.11081035554917
MAE: 5.636200129584476
RMSE: 6.19632038217284
r2: -28096.556537617373
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 8, 110, 0.0005, 0.1, 94, 0.06683690980609727, 0.0673457299076217, 1.019091822361397, 1.11081035554917, 5.636200129584476, 6.19632038217284, -28096.556537617373, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 170 0.0005 16 0] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 1886)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/170
2023-02-14 19:25:11.033128: W tensorflow/c/c_api.cc:291] Operation '{name:'training_10/Adam/outputlayer_5/kernel/v/Assign' id:7685 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/outputlayer_5/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_10/Adam/outputlayer_5/kernel/v, training_10/Adam/outputlayer_5/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:25:28.363932: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:6953 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0682 - val_loss: 0.1454 - 19s/epoch - 225us/sample
Epoch 2/170
84077/84077 - 17s - loss: 0.0669 - val_loss: 0.0674 - 17s/epoch - 206us/sample
Epoch 3/170
84077/84077 - 17s - loss: 0.0669 - val_loss: 0.0674 - 17s/epoch - 205us/sample
Epoch 4/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0674 - 17s/epoch - 205us/sample
Epoch 5/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 6/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 208us/sample
Epoch 7/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 8/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 9/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 10/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 11/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 12/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 13/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 14/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 15/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 16/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 17/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 18/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 19/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 20/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 21/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 22/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 23/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 24/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 204us/sample
Epoch 25/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 26/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 27/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 28/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 29/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 30/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 31/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 32/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 33/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 34/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 35/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 36/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 37/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 38/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 39/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 40/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 41/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 42/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 43/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 44/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 45/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 46/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 47/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 208us/sample
Epoch 48/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 49/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 50/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 51/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 52/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 53/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 54/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 55/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 56/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 57/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 58/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 59/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 60/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 61/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 62/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 63/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 64/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 65/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 66/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 67/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 68/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 69/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 70/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 71/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 72/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 73/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 74/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 75/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 76/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 77/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 78/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 79/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 80/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 81/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 82/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 83/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 84/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 85/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 86/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 87/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 88/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 89/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 90/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 91/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 92/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 93/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 94/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 95/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 96/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 97/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 98/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 99/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 100/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 101/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 102/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 103/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 104/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 105/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 106/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 107/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 108/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 109/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 110/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 111/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 112/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 113/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 114/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 115/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 116/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 117/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 118/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 119/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 120/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 121/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 122/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 123/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 124/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 125/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 126/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 127/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 128/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 129/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 130/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 131/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 132/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 133/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 208us/sample
Epoch 134/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 135/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 136/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 137/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 138/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 139/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 140/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 141/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 142/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 143/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 144/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 145/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 146/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 147/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 148/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 149/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 150/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 151/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 152/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 153/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 154/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 155/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 156/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 157/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 158/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 159/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 160/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 161/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 162/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 163/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 164/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 165/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 166/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 167/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
Epoch 168/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 206us/sample
Epoch 169/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 205us/sample
Epoch 170/170
84077/84077 - 17s - loss: 0.0668 - val_loss: 0.0673 - 17s/epoch - 207us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.06734572743207018
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:14:17.267448: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:6905 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.0605793785469022
cosine 1.1239328448711434
MAE: 5.3854965808295905
RMSE: 5.664960483712925
r2: -23683.577453690035
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 16, 170, 0.0005, 0.1, 94, 0.0668369098254156, 0.06734572743207018, 1.0605793785469022, 1.1239328448711434, 5.3854965808295905, 5.664960483712925, -23683.577453690035, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 25 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 1886)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 20:14:24.785633: W tensorflow/c/c_api.cc:291] Operation '{name:'training_12/Adam/dense_dec1_6/bias/v/Assign' id:8895 op device:{requested: '', assigned: ''} def:{{{node training_12/Adam/dense_dec1_6/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_12/Adam/dense_dec1_6/bias/v, training_12/Adam/dense_dec1_6/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:14:41.651685: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:8271 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0048 - val_loss: 0.0022 - 19s/epoch - 222us/sample
Epoch 2/25
84077/84077 - 17s - loss: 0.0015 - val_loss: 0.0011 - 17s/epoch - 199us/sample
Epoch 3/25
84077/84077 - 17s - loss: 0.0010 - val_loss: 8.6843e-04 - 17s/epoch - 199us/sample
Epoch 4/25
84077/84077 - 17s - loss: 8.9285e-04 - val_loss: 8.5132e-04 - 17s/epoch - 201us/sample
Epoch 5/25
84077/84077 - 17s - loss: 8.3708e-04 - val_loss: 8.3156e-04 - 17s/epoch - 199us/sample
Epoch 6/25
84077/84077 - 17s - loss: 7.9344e-04 - val_loss: 0.0010 - 17s/epoch - 199us/sample
Epoch 7/25
84077/84077 - 17s - loss: 7.6296e-04 - val_loss: 0.0011 - 17s/epoch - 201us/sample
Epoch 8/25
84077/84077 - 17s - loss: 7.4260e-04 - val_loss: 0.0015 - 17s/epoch - 200us/sample
Epoch 9/25
84077/84077 - 17s - loss: 7.2155e-04 - val_loss: 0.0017 - 17s/epoch - 199us/sample
Epoch 10/25
84077/84077 - 17s - loss: 6.9325e-04 - val_loss: 0.0020 - 17s/epoch - 200us/sample
Epoch 11/25
84077/84077 - 17s - loss: 6.6955e-04 - val_loss: 0.0020 - 17s/epoch - 200us/sample
Epoch 12/25
84077/84077 - 17s - loss: 6.5517e-04 - val_loss: 0.0017 - 17s/epoch - 200us/sample
Epoch 13/25
84077/84077 - 17s - loss: 6.4548e-04 - val_loss: 0.0018 - 17s/epoch - 199us/sample
Epoch 14/25
84077/84077 - 17s - loss: 6.3801e-04 - val_loss: 0.0016 - 17s/epoch - 200us/sample
Epoch 15/25
84077/84077 - 17s - loss: 6.3010e-04 - val_loss: 0.0016 - 17s/epoch - 199us/sample
Epoch 16/25
84077/84077 - 17s - loss: 6.2513e-04 - val_loss: 0.0014 - 17s/epoch - 199us/sample
Epoch 17/25
84077/84077 - 17s - loss: 6.2087e-04 - val_loss: 0.0012 - 17s/epoch - 200us/sample
Epoch 18/25
84077/84077 - 17s - loss: 6.1856e-04 - val_loss: 0.0011 - 17s/epoch - 200us/sample
Epoch 19/25
84077/84077 - 17s - loss: 6.1641e-04 - val_loss: 0.0012 - 17s/epoch - 200us/sample
Epoch 20/25
84077/84077 - 17s - loss: 6.1406e-04 - val_loss: 0.0011 - 17s/epoch - 200us/sample
Epoch 21/25
84077/84077 - 17s - loss: 6.1234e-04 - val_loss: 8.7805e-04 - 17s/epoch - 200us/sample
Epoch 22/25
84077/84077 - 17s - loss: 6.1148e-04 - val_loss: 8.8951e-04 - 17s/epoch - 199us/sample
Epoch 23/25
84077/84077 - 17s - loss: 6.0995e-04 - val_loss: 8.0465e-04 - 17s/epoch - 200us/sample
Epoch 24/25
84077/84077 - 17s - loss: 6.0909e-04 - val_loss: 7.4142e-04 - 17s/epoch - 200us/sample
Epoch 25/25
84077/84077 - 17s - loss: 6.0800e-04 - val_loss: 8.0616e-04 - 17s/epoch - 200us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0008061567597915451
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:21:25.996461: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:8242 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.1763072628621972
cosine 0.1739147759757515
MAE: 0.006273762882875504
RMSE: 0.026104199494864297
r2: 0.4677426452818963
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 16, 25, 0.001, 0.1, 94, 0.000608002949164407, 0.0008061567597915451, 0.1763072628621972, 0.1739147759757515, 0.006273762882875504, 0.026104199494864297, 0.4677426452818963, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 1414)         0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           133010      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           133010      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1483637     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,090,129
Trainable params: 3,084,285
Non-trainable params: 5,844
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 20:21:33.265213: W tensorflow/c/c_api.cc:291] Operation '{name:'training_14/Adam/dense_enc0_7/bias/m/Assign' id:9986 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/dense_enc0_7/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/dense_enc0_7/bias/m, training_14/Adam/dense_enc0_7/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:21:43.806943: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:9526 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0273 - val_loss: 0.0018 - 12s/epoch - 143us/sample
Epoch 2/90
84077/84077 - 10s - loss: 0.0018 - val_loss: 0.0011 - 10s/epoch - 119us/sample
Epoch 3/90
84077/84077 - 10s - loss: 0.0013 - val_loss: 9.6462e-04 - 10s/epoch - 119us/sample
Epoch 4/90
84077/84077 - 10s - loss: 0.0011 - val_loss: 7.9803e-04 - 10s/epoch - 119us/sample
Epoch 5/90
84077/84077 - 10s - loss: 8.6227e-04 - val_loss: 7.4007e-04 - 10s/epoch - 120us/sample
Epoch 6/90
84077/84077 - 10s - loss: 7.6240e-04 - val_loss: 6.4639e-04 - 10s/epoch - 119us/sample
Epoch 7/90
84077/84077 - 10s - loss: 6.9007e-04 - val_loss: 5.9251e-04 - 10s/epoch - 120us/sample
Epoch 8/90
84077/84077 - 10s - loss: 6.5205e-04 - val_loss: 5.5582e-04 - 10s/epoch - 120us/sample
Epoch 9/90
84077/84077 - 10s - loss: 5.9877e-04 - val_loss: 5.3771e-04 - 10s/epoch - 120us/sample
Epoch 10/90
84077/84077 - 10s - loss: 5.6712e-04 - val_loss: 5.1048e-04 - 10s/epoch - 120us/sample
Epoch 11/90
84077/84077 - 10s - loss: 5.4142e-04 - val_loss: 4.9258e-04 - 10s/epoch - 119us/sample
Epoch 12/90
84077/84077 - 10s - loss: 5.1993e-04 - val_loss: 4.7605e-04 - 10s/epoch - 120us/sample
Epoch 13/90
84077/84077 - 10s - loss: 5.0323e-04 - val_loss: 4.6780e-04 - 10s/epoch - 120us/sample
Epoch 14/90
84077/84077 - 10s - loss: 4.9023e-04 - val_loss: 4.4550e-04 - 10s/epoch - 120us/sample
Epoch 15/90
84077/84077 - 10s - loss: 4.7891e-04 - val_loss: 4.3975e-04 - 10s/epoch - 120us/sample
Epoch 16/90
84077/84077 - 10s - loss: 4.6842e-04 - val_loss: 4.3161e-04 - 10s/epoch - 119us/sample
Epoch 17/90
84077/84077 - 10s - loss: 4.5785e-04 - val_loss: 4.1815e-04 - 10s/epoch - 120us/sample
Epoch 18/90
84077/84077 - 10s - loss: 4.4977e-04 - val_loss: 4.1989e-04 - 10s/epoch - 120us/sample
Epoch 19/90
84077/84077 - 10s - loss: 4.4226e-04 - val_loss: 4.1850e-04 - 10s/epoch - 120us/sample
Epoch 20/90
84077/84077 - 10s - loss: 4.3545e-04 - val_loss: 4.0530e-04 - 10s/epoch - 119us/sample
Epoch 21/90
84077/84077 - 10s - loss: 4.3113e-04 - val_loss: 3.9965e-04 - 10s/epoch - 120us/sample
Epoch 22/90
84077/84077 - 10s - loss: 4.2506e-04 - val_loss: 3.9506e-04 - 10s/epoch - 120us/sample
Epoch 23/90
84077/84077 - 10s - loss: 4.1962e-04 - val_loss: 3.9076e-04 - 10s/epoch - 120us/sample
Epoch 24/90
84077/84077 - 10s - loss: 4.1597e-04 - val_loss: 3.8801e-04 - 10s/epoch - 120us/sample
Epoch 25/90
84077/84077 - 10s - loss: 4.1237e-04 - val_loss: 3.8547e-04 - 10s/epoch - 119us/sample
Epoch 26/90
84077/84077 - 10s - loss: 4.0936e-04 - val_loss: 3.8713e-04 - 10s/epoch - 120us/sample
Epoch 27/90
84077/84077 - 10s - loss: 4.0593e-04 - val_loss: 3.7930e-04 - 10s/epoch - 120us/sample
Epoch 28/90
84077/84077 - 10s - loss: 4.0506e-04 - val_loss: 3.9846e-04 - 10s/epoch - 121us/sample
Epoch 29/90
84077/84077 - 10s - loss: 4.0339e-04 - val_loss: 3.7294e-04 - 10s/epoch - 120us/sample
Epoch 30/90
84077/84077 - 10s - loss: 4.0124e-04 - val_loss: 3.7607e-04 - 10s/epoch - 120us/sample
Epoch 31/90
84077/84077 - 10s - loss: 3.9582e-04 - val_loss: 3.7256e-04 - 10s/epoch - 120us/sample
Epoch 32/90
84077/84077 - 10s - loss: 3.9363e-04 - val_loss: 3.6312e-04 - 10s/epoch - 120us/sample
Epoch 33/90
84077/84077 - 10s - loss: 3.9216e-04 - val_loss: 3.6391e-04 - 10s/epoch - 120us/sample
Epoch 34/90
84077/84077 - 10s - loss: 3.9104e-04 - val_loss: 3.6413e-04 - 10s/epoch - 120us/sample
Epoch 35/90
84077/84077 - 10s - loss: 3.8687e-04 - val_loss: 3.6206e-04 - 10s/epoch - 119us/sample
Epoch 36/90
84077/84077 - 10s - loss: 3.8681e-04 - val_loss: 3.5842e-04 - 10s/epoch - 119us/sample
Epoch 37/90
84077/84077 - 10s - loss: 3.8276e-04 - val_loss: 3.5669e-04 - 10s/epoch - 120us/sample
Epoch 38/90
84077/84077 - 10s - loss: 3.8280e-04 - val_loss: 3.6048e-04 - 10s/epoch - 121us/sample
Epoch 39/90
84077/84077 - 10s - loss: 3.7989e-04 - val_loss: 3.5547e-04 - 10s/epoch - 119us/sample
Epoch 40/90
84077/84077 - 10s - loss: 3.7948e-04 - val_loss: 3.5502e-04 - 10s/epoch - 120us/sample
Epoch 41/90
84077/84077 - 10s - loss: 3.7714e-04 - val_loss: 3.5328e-04 - 10s/epoch - 119us/sample
Epoch 42/90
84077/84077 - 10s - loss: 3.7648e-04 - val_loss: 3.5158e-04 - 10s/epoch - 119us/sample
Epoch 43/90
84077/84077 - 10s - loss: 3.7453e-04 - val_loss: 3.4673e-04 - 10s/epoch - 120us/sample
Epoch 44/90
84077/84077 - 10s - loss: 3.7396e-04 - val_loss: 3.4589e-04 - 10s/epoch - 121us/sample
Epoch 45/90
84077/84077 - 10s - loss: 3.7126e-04 - val_loss: 3.4853e-04 - 10s/epoch - 119us/sample
Epoch 46/90
84077/84077 - 10s - loss: 3.7085e-04 - val_loss: 3.4333e-04 - 10s/epoch - 119us/sample
Epoch 47/90
84077/84077 - 10s - loss: 3.7003e-04 - val_loss: 3.4061e-04 - 10s/epoch - 119us/sample
Epoch 48/90
84077/84077 - 10s - loss: 3.6720e-04 - val_loss: 3.3951e-04 - 10s/epoch - 120us/sample
Epoch 49/90
84077/84077 - 10s - loss: 3.6577e-04 - val_loss: 3.4540e-04 - 10s/epoch - 120us/sample
Epoch 50/90
84077/84077 - 10s - loss: 3.6538e-04 - val_loss: 3.3828e-04 - 10s/epoch - 119us/sample
Epoch 51/90
84077/84077 - 10s - loss: 3.6653e-04 - val_loss: 3.4176e-04 - 10s/epoch - 120us/sample
Epoch 52/90
84077/84077 - 10s - loss: 3.6484e-04 - val_loss: 3.3746e-04 - 10s/epoch - 119us/sample
Epoch 53/90
84077/84077 - 10s - loss: 3.6141e-04 - val_loss: 3.3813e-04 - 10s/epoch - 119us/sample
Epoch 54/90
84077/84077 - 10s - loss: 3.6263e-04 - val_loss: 3.3233e-04 - 10s/epoch - 120us/sample
Epoch 55/90
84077/84077 - 10s - loss: 3.6680e-04 - val_loss: 3.4146e-04 - 10s/epoch - 120us/sample
Epoch 56/90
84077/84077 - 10s - loss: 3.5938e-04 - val_loss: 3.3708e-04 - 10s/epoch - 120us/sample
Epoch 57/90
84077/84077 - 10s - loss: 3.5807e-04 - val_loss: 3.3053e-04 - 10s/epoch - 120us/sample
Epoch 58/90
84077/84077 - 10s - loss: 3.5801e-04 - val_loss: 3.3384e-04 - 10s/epoch - 119us/sample
Epoch 59/90
84077/84077 - 10s - loss: 3.5784e-04 - val_loss: 3.3195e-04 - 10s/epoch - 120us/sample
Epoch 60/90
84077/84077 - 10s - loss: 3.5810e-04 - val_loss: 3.2911e-04 - 10s/epoch - 121us/sample
Epoch 61/90
84077/84077 - 10s - loss: 3.5633e-04 - val_loss: 3.2957e-04 - 10s/epoch - 120us/sample
Epoch 62/90
84077/84077 - 10s - loss: 3.5651e-04 - val_loss: 3.3153e-04 - 10s/epoch - 119us/sample
Epoch 63/90
84077/84077 - 10s - loss: 3.5547e-04 - val_loss: 3.2927e-04 - 10s/epoch - 119us/sample
Epoch 64/90
84077/84077 - 10s - loss: 3.5531e-04 - val_loss: 3.3882e-04 - 10s/epoch - 120us/sample
Epoch 65/90
84077/84077 - 10s - loss: 3.5295e-04 - val_loss: 3.4365e-04 - 10s/epoch - 120us/sample
Epoch 66/90
84077/84077 - 10s - loss: 3.5170e-04 - val_loss: 3.2505e-04 - 10s/epoch - 120us/sample
Epoch 67/90
84077/84077 - 10s - loss: 3.5324e-04 - val_loss: 3.2833e-04 - 10s/epoch - 120us/sample
Epoch 68/90
84077/84077 - 10s - loss: 3.5092e-04 - val_loss: 3.3290e-04 - 10s/epoch - 120us/sample
Epoch 69/90
84077/84077 - 10s - loss: 3.5184e-04 - val_loss: 3.2377e-04 - 10s/epoch - 119us/sample
Epoch 70/90
84077/84077 - 10s - loss: 3.4902e-04 - val_loss: 3.2555e-04 - 10s/epoch - 120us/sample
Epoch 71/90
84077/84077 - 10s - loss: 3.4826e-04 - val_loss: 3.2098e-04 - 10s/epoch - 120us/sample
Epoch 72/90
84077/84077 - 10s - loss: 3.4792e-04 - val_loss: 3.2351e-04 - 10s/epoch - 120us/sample
Epoch 73/90
84077/84077 - 10s - loss: 3.4668e-04 - val_loss: 3.2464e-04 - 10s/epoch - 120us/sample
Epoch 74/90
84077/84077 - 10s - loss: 3.4657e-04 - val_loss: 3.2571e-04 - 10s/epoch - 120us/sample
Epoch 75/90
84077/84077 - 10s - loss: 3.4676e-04 - val_loss: 3.2073e-04 - 10s/epoch - 119us/sample
Epoch 76/90
84077/84077 - 10s - loss: 3.4573e-04 - val_loss: 3.2203e-04 - 10s/epoch - 121us/sample
Epoch 77/90
84077/84077 - 10s - loss: 3.4405e-04 - val_loss: 3.2080e-04 - 10s/epoch - 120us/sample
Epoch 78/90
84077/84077 - 10s - loss: 3.4472e-04 - val_loss: 3.2120e-04 - 10s/epoch - 120us/sample
Epoch 79/90
84077/84077 - 10s - loss: 3.4409e-04 - val_loss: 3.1982e-04 - 10s/epoch - 119us/sample
Epoch 80/90
84077/84077 - 10s - loss: 3.4230e-04 - val_loss: 3.2197e-04 - 10s/epoch - 119us/sample
Epoch 81/90
84077/84077 - 10s - loss: 3.4171e-04 - val_loss: 3.2049e-04 - 10s/epoch - 121us/sample
Epoch 82/90
84077/84077 - 10s - loss: 3.4389e-04 - val_loss: 3.2519e-04 - 10s/epoch - 120us/sample
Epoch 83/90
84077/84077 - 10s - loss: 3.4108e-04 - val_loss: 3.1639e-04 - 10s/epoch - 120us/sample
Epoch 84/90
84077/84077 - 10s - loss: 3.4056e-04 - val_loss: 3.1601e-04 - 10s/epoch - 119us/sample
Epoch 85/90
84077/84077 - 10s - loss: 3.3798e-04 - val_loss: 3.1595e-04 - 10s/epoch - 120us/sample
Epoch 86/90
84077/84077 - 10s - loss: 3.3935e-04 - val_loss: 3.1966e-04 - 10s/epoch - 120us/sample
Epoch 87/90
84077/84077 - 10s - loss: 3.4264e-04 - val_loss: 3.1498e-04 - 10s/epoch - 120us/sample
Epoch 88/90
84077/84077 - 10s - loss: 3.3784e-04 - val_loss: 3.1666e-04 - 10s/epoch - 120us/sample
Epoch 89/90
84077/84077 - 10s - loss: 3.3974e-04 - val_loss: 3.2167e-04 - 10s/epoch - 120us/sample
Epoch 90/90
84077/84077 - 10s - loss: 3.4217e-04 - val_loss: 3.1811e-04 - 10s/epoch - 120us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00031810763070922685
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:36:40.835544: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:9497 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04197143049670232
cosine 0.041484903542109285
MAE: 0.0027009617692904486
RMSE: 0.012342133287676449
r2: 0.8810671063563843
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 32, 90, 0.001, 0.1, 94, 0.00034216940645765196, 0.00031810763070922685, 0.04197143049670232, 0.041484903542109285, 0.0027009617692904486, 0.012342133287676449, 0.8810671063563843, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 25 0.001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 471)          444624      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 471)         1884        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 471)          0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           44368       ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           44368       ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          501031      ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,036,275
Trainable params: 1,034,203
Non-trainable params: 2,072
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 20:36:48.306609: W tensorflow/c/c_api.cc:291] Operation '{name:'training_16/Adam/dense_enc0_8/kernel/v/Assign' id:11401 op device:{requested: '', assigned: ''} def:{{{node training_16/Adam/dense_enc0_8/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_16/Adam/dense_enc0_8/kernel/v, training_16/Adam/dense_enc0_8/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:36:55.933270: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/mul' id:10780 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 9s - loss: 0.0638 - val_loss: 0.0630 - 9s/epoch - 109us/sample
Epoch 2/25
84077/84077 - 7s - loss: 0.0607 - val_loss: 0.0595 - 7s/epoch - 84us/sample
Epoch 3/25
84077/84077 - 7s - loss: 0.0565 - val_loss: 0.0542 - 7s/epoch - 83us/sample
Epoch 4/25
84077/84077 - 7s - loss: 0.0535 - val_loss: 0.0512 - 7s/epoch - 83us/sample
Epoch 5/25
84077/84077 - 7s - loss: 0.0497 - val_loss: 0.0484 - 7s/epoch - 83us/sample
Epoch 6/25
84077/84077 - 7s - loss: 0.0479 - val_loss: 0.0468 - 7s/epoch - 84us/sample
Epoch 7/25
84077/84077 - 7s - loss: 0.0486 - val_loss: 0.0460 - 7s/epoch - 83us/sample
Epoch 8/25
84077/84077 - 7s - loss: 0.0458 - val_loss: 0.0447 - 7s/epoch - 84us/sample
Epoch 9/25
84077/84077 - 7s - loss: 0.0454 - val_loss: 0.0451 - 7s/epoch - 84us/sample
Epoch 10/25
84077/84077 - 7s - loss: 0.0457 - val_loss: 0.0445 - 7s/epoch - 83us/sample
Epoch 11/25
84077/84077 - 7s - loss: 0.0444 - val_loss: 0.0444 - 7s/epoch - 84us/sample
Epoch 12/25
84077/84077 - 7s - loss: 0.0438 - val_loss: 0.0430 - 7s/epoch - 83us/sample
Epoch 13/25
84077/84077 - 7s - loss: 0.0442 - val_loss: 0.0429 - 7s/epoch - 83us/sample
Epoch 14/25
84077/84077 - 7s - loss: 0.0425 - val_loss: 0.0431 - 7s/epoch - 83us/sample
Epoch 15/25
84077/84077 - 7s - loss: 0.0425 - val_loss: 0.0424 - 7s/epoch - 84us/sample
Epoch 16/25
84077/84077 - 7s - loss: 0.0430 - val_loss: 0.0419 - 7s/epoch - 84us/sample
Epoch 17/25
84077/84077 - 7s - loss: 0.0419 - val_loss: 0.0419 - 7s/epoch - 84us/sample
Epoch 18/25
84077/84077 - 7s - loss: 0.0416 - val_loss: 0.0410 - 7s/epoch - 83us/sample
Epoch 19/25
84077/84077 - 7s - loss: 0.0408 - val_loss: 0.0404 - 7s/epoch - 84us/sample
Epoch 20/25
84077/84077 - 7s - loss: 0.0405 - val_loss: 0.0404 - 7s/epoch - 83us/sample
Epoch 21/25
84077/84077 - 7s - loss: 0.0406 - val_loss: 0.0409 - 7s/epoch - 83us/sample
Epoch 22/25
84077/84077 - 7s - loss: 0.0405 - val_loss: 0.0406 - 7s/epoch - 84us/sample
Epoch 23/25
84077/84077 - 7s - loss: 0.0406 - val_loss: 0.0405 - 7s/epoch - 84us/sample
Epoch 24/25
84077/84077 - 7s - loss: 0.0406 - val_loss: 0.0414 - 7s/epoch - 84us/sample
Epoch 25/25
84077/84077 - 7s - loss: 0.0409 - val_loss: 0.0410 - 7s/epoch - 84us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0409707740073642
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:39:45.664233: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_8/outputlayer/BiasAdd' id:10732 op device:{requested: '', assigned: ''} def:{{{node decoder_model_8/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_8/outputlayer/MatMul, decoder_model_8/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.6443039811771866
cosine 0.9899582472642267
MAE: 0.6562137753756118
RMSE: 2.829170403509087
r2: -5914.399910391385
RMSE zero-vector: 0.04004287452915337
['0.5custom_VAE', 'binary_crossentropy', 64, 25, 0.001, 0.1, 94, 0.04085013269629306, 0.0409707740073642, 0.6443039811771866, 0.9899582472642267, 0.6562137753756118, 2.829170403509087, -5914.399910391385, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 1414)         0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           133010      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           133010      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1483637     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,090,129
Trainable params: 3,084,285
Non-trainable params: 5,844
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/30
2023-02-14 20:39:53.610469: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/dense_enc0_9/bias/v/Assign' id:12651 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/dense_enc0_9/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/dense_enc0_9/bias/v, training_18/Adam/dense_enc0_9/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:40:23.514885: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_9/mul' id:12077 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 33s - loss: 0.0037 - val_loss: 0.0013 - 33s/epoch - 391us/sample
Epoch 2/30
84077/84077 - 30s - loss: 0.0012 - val_loss: 0.0011 - 30s/epoch - 358us/sample
Epoch 3/30
84077/84077 - 30s - loss: 0.0010 - val_loss: 0.0041 - 30s/epoch - 361us/sample
Epoch 4/30
84077/84077 - 30s - loss: 9.7642e-04 - val_loss: 0.0356 - 30s/epoch - 362us/sample
Epoch 5/30
84077/84077 - 30s - loss: 9.4002e-04 - val_loss: 0.0578 - 30s/epoch - 359us/sample
Epoch 6/30
84077/84077 - 30s - loss: 9.1704e-04 - val_loss: 0.0701 - 30s/epoch - 359us/sample
Epoch 7/30
84077/84077 - 30s - loss: 9.0411e-04 - val_loss: 0.0559 - 30s/epoch - 359us/sample
Epoch 8/30
84077/84077 - 30s - loss: 8.9108e-04 - val_loss: 0.0606 - 30s/epoch - 360us/sample
Epoch 9/30
84077/84077 - 30s - loss: 8.7907e-04 - val_loss: 0.0578 - 30s/epoch - 359us/sample
Epoch 10/30
84077/84077 - 30s - loss: 8.6794e-04 - val_loss: 0.0638 - 30s/epoch - 360us/sample
Epoch 11/30
84077/84077 - 30s - loss: 8.6034e-04 - val_loss: 0.0509 - 30s/epoch - 358us/sample
Epoch 12/30
84077/84077 - 30s - loss: 8.5425e-04 - val_loss: 0.0587 - 30s/epoch - 358us/sample
Epoch 13/30
84077/84077 - 30s - loss: 8.4740e-04 - val_loss: 0.0789 - 30s/epoch - 358us/sample
Epoch 14/30
84077/84077 - 30s - loss: 8.4256e-04 - val_loss: 0.0805 - 30s/epoch - 360us/sample
Epoch 15/30
84077/84077 - 30s - loss: 8.3634e-04 - val_loss: 0.0835 - 30s/epoch - 358us/sample
Epoch 16/30
84077/84077 - 30s - loss: 8.3065e-04 - val_loss: 0.0623 - 30s/epoch - 359us/sample
Epoch 17/30
84077/84077 - 30s - loss: 8.2541e-04 - val_loss: 0.0996 - 30s/epoch - 358us/sample
Epoch 18/30
84077/84077 - 30s - loss: 8.2136e-04 - val_loss: 0.0658 - 30s/epoch - 360us/sample
Epoch 19/30
84077/84077 - 30s - loss: 8.2014e-04 - val_loss: 0.0715 - 30s/epoch - 356us/sample
Epoch 20/30
84077/84077 - 30s - loss: 8.1527e-04 - val_loss: 0.0695 - 30s/epoch - 359us/sample
Epoch 21/30
84077/84077 - 30s - loss: 8.1327e-04 - val_loss: 0.0754 - 30s/epoch - 355us/sample
Epoch 22/30
84077/84077 - 30s - loss: 8.1113e-04 - val_loss: 0.0696 - 30s/epoch - 359us/sample
Epoch 23/30
84077/84077 - 30s - loss: 8.0931e-04 - val_loss: 0.0656 - 30s/epoch - 359us/sample
Epoch 24/30
84077/84077 - 30s - loss: 8.0820e-04 - val_loss: 0.0561 - 30s/epoch - 357us/sample
Epoch 25/30
84077/84077 - 30s - loss: 8.0562e-04 - val_loss: 0.0823 - 30s/epoch - 357us/sample
Epoch 26/30
84077/84077 - 30s - loss: 8.0299e-04 - val_loss: 0.0663 - 30s/epoch - 358us/sample
Epoch 27/30
84077/84077 - 30s - loss: 7.9940e-04 - val_loss: 0.0644 - 30s/epoch - 357us/sample
Epoch 28/30
84077/84077 - 30s - loss: 7.9755e-04 - val_loss: 0.0681 - 30s/epoch - 358us/sample
Epoch 29/30
84077/84077 - 30s - loss: 7.9488e-04 - val_loss: 0.0719 - 30s/epoch - 358us/sample
Epoch 30/30
84077/84077 - 30s - loss: 7.9288e-04 - val_loss: 0.0698 - 30s/epoch - 358us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.06980074794647577
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:54:59.753888: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:12048 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.22587944698293047
cosine 0.22338922647142115
MAE: 0.022083941763453706
RMSE: 0.26338421891645897
r2: -53.189134352012815
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 8, 30, 0.0005, 0.1, 94, 0.0007928820172973137, 0.06980074794647577, 0.22587944698293047, 0.22338922647142115, 0.022083941763453706, 0.26338421891645897, -53.189134352012815, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 30 0.0005 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 1980)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           186214      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           186214      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2073409     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,322,877
Trainable params: 4,314,769
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/30
2023-02-14 20:55:07.535962: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_32/moving_variance/Assign' id:13162 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_32/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_32/moving_variance, batch_normalization_32/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:55:15.574588: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:13339 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 10s - loss: 0.0035 - val_loss: 0.0011 - 10s/epoch - 117us/sample
Epoch 2/30
84077/84077 - 7s - loss: 0.0017 - val_loss: 8.5584e-04 - 7s/epoch - 84us/sample
Epoch 3/30
84077/84077 - 7s - loss: 8.1235e-04 - val_loss: 6.7193e-04 - 7s/epoch - 84us/sample
Epoch 4/30
84077/84077 - 7s - loss: 0.0016 - val_loss: 5.3503e-04 - 7s/epoch - 84us/sample
Epoch 5/30
84077/84077 - 7s - loss: 5.4496e-04 - val_loss: 4.9199e-04 - 7s/epoch - 84us/sample
Epoch 6/30
84077/84077 - 7s - loss: 5.0418e-04 - val_loss: 4.7959e-04 - 7s/epoch - 84us/sample
Epoch 7/30
84077/84077 - 7s - loss: 5.0642e-04 - val_loss: 4.3314e-04 - 7s/epoch - 84us/sample
Epoch 8/30
84077/84077 - 7s - loss: 4.5853e-04 - val_loss: 3.9480e-04 - 7s/epoch - 84us/sample
Epoch 9/30
84077/84077 - 7s - loss: 4.0782e-04 - val_loss: 3.6733e-04 - 7s/epoch - 84us/sample
Epoch 10/30
84077/84077 - 7s - loss: 3.7607e-04 - val_loss: 3.3969e-04 - 7s/epoch - 85us/sample
Epoch 11/30
84077/84077 - 7s - loss: 3.6891e-04 - val_loss: 3.2780e-04 - 7s/epoch - 84us/sample
Epoch 12/30
84077/84077 - 7s - loss: 3.4287e-04 - val_loss: 3.1312e-04 - 7s/epoch - 84us/sample
Epoch 13/30
84077/84077 - 7s - loss: 3.2852e-04 - val_loss: 3.0011e-04 - 7s/epoch - 84us/sample
Epoch 14/30
84077/84077 - 7s - loss: 3.1838e-04 - val_loss: 2.9711e-04 - 7s/epoch - 84us/sample
Epoch 15/30
84077/84077 - 7s - loss: 3.0812e-04 - val_loss: 2.8528e-04 - 7s/epoch - 84us/sample
Epoch 16/30
84077/84077 - 7s - loss: 3.0049e-04 - val_loss: 2.8030e-04 - 7s/epoch - 84us/sample
Epoch 17/30
84077/84077 - 7s - loss: 2.9429e-04 - val_loss: 2.7241e-04 - 7s/epoch - 84us/sample
Epoch 18/30
84077/84077 - 7s - loss: 2.8893e-04 - val_loss: 2.6654e-04 - 7s/epoch - 84us/sample
Epoch 19/30
84077/84077 - 7s - loss: 2.8309e-04 - val_loss: 2.6121e-04 - 7s/epoch - 84us/sample
Epoch 20/30
84077/84077 - 7s - loss: 2.7739e-04 - val_loss: 2.5652e-04 - 7s/epoch - 84us/sample
Epoch 21/30
84077/84077 - 7s - loss: 2.7252e-04 - val_loss: 2.5415e-04 - 7s/epoch - 84us/sample
Epoch 22/30
84077/84077 - 7s - loss: 2.6938e-04 - val_loss: 2.4814e-04 - 7s/epoch - 84us/sample
Epoch 23/30
84077/84077 - 7s - loss: 2.6436e-04 - val_loss: 2.4192e-04 - 7s/epoch - 83us/sample
Epoch 24/30
84077/84077 - 7s - loss: 2.6053e-04 - val_loss: 2.4114e-04 - 7s/epoch - 84us/sample
Epoch 25/30
84077/84077 - 7s - loss: 2.5662e-04 - val_loss: 2.3823e-04 - 7s/epoch - 84us/sample
Epoch 26/30
84077/84077 - 7s - loss: 2.5484e-04 - val_loss: 2.3731e-04 - 7s/epoch - 84us/sample
Epoch 27/30
84077/84077 - 7s - loss: 2.5178e-04 - val_loss: 2.3240e-04 - 7s/epoch - 84us/sample
Epoch 28/30
84077/84077 - 7s - loss: 2.4947e-04 - val_loss: 2.3160e-04 - 7s/epoch - 84us/sample
Epoch 29/30
84077/84077 - 7s - loss: 2.4591e-04 - val_loss: 2.2835e-04 - 7s/epoch - 84us/sample
Epoch 30/30
84077/84077 - 7s - loss: 2.4404e-04 - val_loss: 2.2568e-04 - 7s/epoch - 84us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00022568167571897663
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:58:41.069276: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:13303 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.07479372769694428
cosine 0.07385284724849923
MAE: 0.0037778421243459983
RMSE: 0.015362064887436935
r2: 0.8156957199707627
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 30, 0.0005, 0.1, 94, 0.0002440381770140782, 0.00022568167571897663, 0.07479372769694428, 0.07385284724849923, 0.0037778421243459983, 0.015362064887436935, 0.8156957199707627, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0008 16 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 2357)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 20:58:50.873618: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/batch_normalization_34/beta/v/Assign' id:15281 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/batch_normalization_34/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/batch_normalization_34/beta/v, training_22/Adam/batch_normalization_34/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:59:09.486281: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:14624 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0028 - val_loss: 8.5997e-04 - 21s/epoch - 250us/sample
Epoch 2/50
84077/84077 - 18s - loss: 0.0015 - val_loss: 7.3511e-04 - 18s/epoch - 216us/sample
Epoch 3/50
84077/84077 - 18s - loss: 6.5847e-04 - val_loss: 5.3845e-04 - 18s/epoch - 216us/sample
Epoch 4/50
84077/84077 - 18s - loss: 5.4664e-04 - val_loss: 5.0478e-04 - 18s/epoch - 217us/sample
Epoch 5/50
84077/84077 - 18s - loss: 4.9621e-04 - val_loss: 4.5655e-04 - 18s/epoch - 216us/sample
Epoch 6/50
84077/84077 - 18s - loss: 4.7026e-04 - val_loss: 4.9981e-04 - 18s/epoch - 217us/sample
Epoch 7/50
84077/84077 - 18s - loss: 4.5741e-04 - val_loss: 5.1505e-04 - 18s/epoch - 216us/sample
Epoch 8/50
84077/84077 - 18s - loss: 4.5193e-04 - val_loss: 5.5914e-04 - 18s/epoch - 216us/sample
Epoch 9/50
84077/84077 - 18s - loss: 4.4779e-04 - val_loss: 5.8416e-04 - 18s/epoch - 217us/sample
Epoch 10/50
84077/84077 - 18s - loss: 4.4357e-04 - val_loss: 6.1753e-04 - 18s/epoch - 216us/sample
Epoch 11/50
84077/84077 - 18s - loss: 4.3939e-04 - val_loss: 6.7404e-04 - 18s/epoch - 215us/sample
Epoch 12/50
84077/84077 - 18s - loss: 4.3484e-04 - val_loss: 8.3276e-04 - 18s/epoch - 217us/sample
Epoch 13/50
84077/84077 - 18s - loss: 4.3231e-04 - val_loss: 9.7392e-04 - 18s/epoch - 216us/sample
Epoch 14/50
84077/84077 - 18s - loss: 4.3059e-04 - val_loss: 8.2626e-04 - 18s/epoch - 216us/sample
Epoch 15/50
84077/84077 - 18s - loss: 4.2819e-04 - val_loss: 8.7359e-04 - 18s/epoch - 216us/sample
Epoch 16/50
84077/84077 - 18s - loss: 4.2664e-04 - val_loss: 9.0126e-04 - 18s/epoch - 216us/sample
Epoch 17/50
84077/84077 - 18s - loss: 4.2501e-04 - val_loss: 7.5046e-04 - 18s/epoch - 216us/sample
Epoch 18/50
84077/84077 - 18s - loss: 4.2387e-04 - val_loss: 8.2252e-04 - 18s/epoch - 218us/sample
Epoch 19/50
84077/84077 - 18s - loss: 4.2303e-04 - val_loss: 6.5849e-04 - 18s/epoch - 216us/sample
Epoch 20/50
84077/84077 - 18s - loss: 4.2156e-04 - val_loss: 8.2096e-04 - 18s/epoch - 216us/sample
Epoch 21/50
84077/84077 - 18s - loss: 4.2012e-04 - val_loss: 8.1700e-04 - 18s/epoch - 217us/sample
Epoch 22/50
84077/84077 - 18s - loss: 4.1831e-04 - val_loss: 7.0671e-04 - 18s/epoch - 216us/sample
Epoch 23/50
84077/84077 - 18s - loss: 4.1736e-04 - val_loss: 7.5586e-04 - 18s/epoch - 216us/sample
Epoch 24/50
84077/84077 - 18s - loss: 4.1629e-04 - val_loss: 8.7936e-04 - 18s/epoch - 217us/sample
Epoch 25/50
84077/84077 - 18s - loss: 4.1481e-04 - val_loss: 8.2992e-04 - 18s/epoch - 216us/sample
Epoch 26/50
84077/84077 - 18s - loss: 4.1389e-04 - val_loss: 8.2405e-04 - 18s/epoch - 217us/sample
Epoch 27/50
84077/84077 - 18s - loss: 4.1263e-04 - val_loss: 8.5740e-04 - 18s/epoch - 217us/sample
Epoch 28/50
84077/84077 - 18s - loss: 4.1129e-04 - val_loss: 8.6370e-04 - 18s/epoch - 216us/sample
Epoch 29/50
84077/84077 - 18s - loss: 4.0988e-04 - val_loss: 0.0010 - 18s/epoch - 216us/sample
Epoch 30/50
84077/84077 - 18s - loss: 4.0926e-04 - val_loss: 8.8177e-04 - 18s/epoch - 216us/sample
Epoch 31/50
84077/84077 - 18s - loss: 4.0885e-04 - val_loss: 0.0011 - 18s/epoch - 217us/sample
Epoch 32/50
84077/84077 - 18s - loss: 4.0795e-04 - val_loss: 9.6982e-04 - 18s/epoch - 216us/sample
Epoch 33/50
84077/84077 - 18s - loss: 4.0699e-04 - val_loss: 9.1331e-04 - 18s/epoch - 216us/sample
Epoch 34/50
84077/84077 - 18s - loss: 4.0628e-04 - val_loss: 0.0010 - 18s/epoch - 217us/sample
Epoch 35/50
84077/84077 - 18s - loss: 4.0452e-04 - val_loss: 9.7376e-04 - 18s/epoch - 216us/sample
Epoch 36/50
84077/84077 - 18s - loss: 4.0421e-04 - val_loss: 9.5151e-04 - 18s/epoch - 215us/sample
Epoch 37/50
84077/84077 - 18s - loss: 4.0353e-04 - val_loss: 8.6567e-04 - 18s/epoch - 217us/sample
Epoch 38/50
84077/84077 - 18s - loss: 4.0302e-04 - val_loss: 6.6130e-04 - 18s/epoch - 216us/sample
Epoch 39/50
84077/84077 - 18s - loss: 4.0169e-04 - val_loss: 7.2174e-04 - 18s/epoch - 215us/sample
Epoch 40/50
84077/84077 - 18s - loss: 4.0074e-04 - val_loss: 7.1493e-04 - 18s/epoch - 216us/sample
Epoch 41/50
84077/84077 - 18s - loss: 4.0045e-04 - val_loss: 6.2897e-04 - 18s/epoch - 217us/sample
Epoch 42/50
84077/84077 - 18s - loss: 3.9915e-04 - val_loss: 6.1544e-04 - 18s/epoch - 216us/sample
Epoch 43/50
84077/84077 - 18s - loss: 3.9820e-04 - val_loss: 5.6501e-04 - 18s/epoch - 216us/sample
Epoch 44/50
84077/84077 - 18s - loss: 3.9791e-04 - val_loss: 5.1872e-04 - 18s/epoch - 217us/sample
Epoch 45/50
84077/84077 - 18s - loss: 3.9712e-04 - val_loss: 5.3106e-04 - 18s/epoch - 216us/sample
Epoch 46/50
84077/84077 - 18s - loss: 3.9636e-04 - val_loss: 5.0526e-04 - 18s/epoch - 216us/sample
Epoch 47/50
84077/84077 - 18s - loss: 3.9571e-04 - val_loss: 4.9345e-04 - 18s/epoch - 215us/sample
Epoch 48/50
84077/84077 - 18s - loss: 3.9536e-04 - val_loss: 4.7731e-04 - 18s/epoch - 217us/sample
Epoch 49/50
84077/84077 - 18s - loss: 3.9491e-04 - val_loss: 4.2681e-04 - 18s/epoch - 216us/sample
Epoch 50/50
84077/84077 - 18s - loss: 3.9398e-04 - val_loss: 3.9909e-04 - 18s/epoch - 216us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0003990922840228382
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:14:02.138760: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:14588 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.19739168592160788
cosine 0.1945631204677535
MAE: 0.004984826031864494
RMSE: 0.02679504434316924
r2: 0.4391197219570453
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 16, 50, 0.0008, 0.1, 94, 0.00039398094071745147, 0.0003990922840228382, 0.19739168592160788, 0.1945631204677535, 0.004984826031864494, 0.02679504434316924, 0.4391197219570453, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4 50 0.0005 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2263)         2136272     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 2263)        9052        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 2263)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           212816      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           212816      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2368295     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,939,251
Trainable params: 4,930,011
Non-trainable params: 9,240
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 21:14:10.677046: W tensorflow/c/c_api.cc:291] Operation '{name:'training_24/Adam/bottleneck_zmean_12/kernel/m/Assign' id:16386 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/bottleneck_zmean_12/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_24/Adam/bottleneck_zmean_12/kernel/m, training_24/Adam/bottleneck_zmean_12/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:14:22.266552: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:15902 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0221 - val_loss: 0.0020 - 14s/epoch - 164us/sample
Epoch 2/50
84077/84077 - 11s - loss: 0.0023 - val_loss: 0.0024 - 11s/epoch - 128us/sample
Epoch 3/50
84077/84077 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 128us/sample
Epoch 4/50
84077/84077 - 11s - loss: 0.0012 - val_loss: 0.0015 - 11s/epoch - 128us/sample
Epoch 5/50
84077/84077 - 11s - loss: 0.0015 - val_loss: 9.8202e-04 - 11s/epoch - 128us/sample
Epoch 6/50
84077/84077 - 11s - loss: 9.2863e-04 - val_loss: 8.2593e-04 - 11s/epoch - 128us/sample
Epoch 7/50
84077/84077 - 11s - loss: 9.0135e-04 - val_loss: 7.3394e-04 - 11s/epoch - 128us/sample
Epoch 8/50
84077/84077 - 11s - loss: 7.7281e-04 - val_loss: 6.9732e-04 - 11s/epoch - 129us/sample
Epoch 9/50
84077/84077 - 11s - loss: 7.5002e-04 - val_loss: 6.4646e-04 - 11s/epoch - 128us/sample
Epoch 10/50
84077/84077 - 11s - loss: 6.9242e-04 - val_loss: 6.2318e-04 - 11s/epoch - 128us/sample
Epoch 11/50
84077/84077 - 11s - loss: 6.5893e-04 - val_loss: 6.4496e-04 - 11s/epoch - 128us/sample
Epoch 12/50
84077/84077 - 11s - loss: 6.3166e-04 - val_loss: 5.6950e-04 - 11s/epoch - 128us/sample
Epoch 13/50
84077/84077 - 11s - loss: 6.1152e-04 - val_loss: 5.6673e-04 - 11s/epoch - 129us/sample
Epoch 14/50
84077/84077 - 11s - loss: 5.9920e-04 - val_loss: 5.5870e-04 - 11s/epoch - 128us/sample
Epoch 15/50
84077/84077 - 11s - loss: 5.9115e-04 - val_loss: 5.5972e-04 - 11s/epoch - 128us/sample
Epoch 16/50
84077/84077 - 11s - loss: 5.8478e-04 - val_loss: 5.5116e-04 - 11s/epoch - 128us/sample
Epoch 17/50
84077/84077 - 11s - loss: 5.7655e-04 - val_loss: 5.4308e-04 - 11s/epoch - 128us/sample
Epoch 18/50
84077/84077 - 11s - loss: 5.7126e-04 - val_loss: 5.2981e-04 - 11s/epoch - 129us/sample
Epoch 19/50
84077/84077 - 11s - loss: 5.6370e-04 - val_loss: 5.2583e-04 - 11s/epoch - 129us/sample
Epoch 20/50
84077/84077 - 11s - loss: 5.5856e-04 - val_loss: 5.3044e-04 - 11s/epoch - 128us/sample
Epoch 21/50
84077/84077 - 11s - loss: 5.5439e-04 - val_loss: 5.2664e-04 - 11s/epoch - 128us/sample
Epoch 22/50
84077/84077 - 11s - loss: 5.5112e-04 - val_loss: 5.1841e-04 - 11s/epoch - 128us/sample
Epoch 23/50
84077/84077 - 11s - loss: 5.4807e-04 - val_loss: 5.1515e-04 - 11s/epoch - 129us/sample
Epoch 24/50
84077/84077 - 11s - loss: 5.4563e-04 - val_loss: 5.2005e-04 - 11s/epoch - 128us/sample
Epoch 25/50
84077/84077 - 11s - loss: 5.4292e-04 - val_loss: 5.1523e-04 - 11s/epoch - 128us/sample
Epoch 26/50
84077/84077 - 11s - loss: 5.4081e-04 - val_loss: 5.1326e-04 - 11s/epoch - 128us/sample
Epoch 27/50
84077/84077 - 11s - loss: 5.3887e-04 - val_loss: 5.1275e-04 - 11s/epoch - 128us/sample
Epoch 28/50
84077/84077 - 11s - loss: 5.3644e-04 - val_loss: 5.0852e-04 - 11s/epoch - 128us/sample
Epoch 29/50
84077/84077 - 11s - loss: 5.3448e-04 - val_loss: 5.0398e-04 - 11s/epoch - 129us/sample
Epoch 30/50
84077/84077 - 11s - loss: 5.3273e-04 - val_loss: 5.0091e-04 - 11s/epoch - 128us/sample
Epoch 31/50
84077/84077 - 11s - loss: 5.3165e-04 - val_loss: 5.0486e-04 - 11s/epoch - 128us/sample
Epoch 32/50
84077/84077 - 11s - loss: 5.3011e-04 - val_loss: 4.9769e-04 - 11s/epoch - 128us/sample
Epoch 33/50
84077/84077 - 11s - loss: 5.2793e-04 - val_loss: 4.9735e-04 - 11s/epoch - 129us/sample
Epoch 34/50
84077/84077 - 11s - loss: 5.2692e-04 - val_loss: 4.9823e-04 - 11s/epoch - 128us/sample
Epoch 35/50
84077/84077 - 11s - loss: 5.2603e-04 - val_loss: 4.9299e-04 - 11s/epoch - 128us/sample
Epoch 36/50
84077/84077 - 11s - loss: 5.2458e-04 - val_loss: 4.9684e-04 - 11s/epoch - 128us/sample
Epoch 37/50
84077/84077 - 11s - loss: 5.2399e-04 - val_loss: 4.9643e-04 - 11s/epoch - 128us/sample
Epoch 38/50
84077/84077 - 11s - loss: 5.2246e-04 - val_loss: 4.8991e-04 - 11s/epoch - 129us/sample
Epoch 39/50
84077/84077 - 11s - loss: 5.2145e-04 - val_loss: 4.9351e-04 - 11s/epoch - 128us/sample
Epoch 40/50
84077/84077 - 11s - loss: 5.2140e-04 - val_loss: 4.9205e-04 - 11s/epoch - 128us/sample
Epoch 41/50
84077/84077 - 11s - loss: 5.2011e-04 - val_loss: 4.8890e-04 - 11s/epoch - 128us/sample
Epoch 42/50
84077/84077 - 11s - loss: 5.1967e-04 - val_loss: 4.9833e-04 - 11s/epoch - 128us/sample
Epoch 43/50
84077/84077 - 11s - loss: 5.1828e-04 - val_loss: 4.8856e-04 - 11s/epoch - 128us/sample
Epoch 44/50
84077/84077 - 11s - loss: 5.1895e-04 - val_loss: 4.8811e-04 - 11s/epoch - 129us/sample
Epoch 45/50
84077/84077 - 11s - loss: 5.1826e-04 - val_loss: 4.8702e-04 - 11s/epoch - 128us/sample
Epoch 46/50
84077/84077 - 11s - loss: 5.1731e-04 - val_loss: 4.8878e-04 - 11s/epoch - 128us/sample
Epoch 47/50
84077/84077 - 11s - loss: 5.1651e-04 - val_loss: 4.8660e-04 - 11s/epoch - 128us/sample
Epoch 48/50
84077/84077 - 11s - loss: 5.1621e-04 - val_loss: 4.8679e-04 - 11s/epoch - 128us/sample
Epoch 49/50
84077/84077 - 11s - loss: 5.1547e-04 - val_loss: 4.8634e-04 - 11s/epoch - 129us/sample
Epoch 50/50
84077/84077 - 11s - loss: 5.1560e-04 - val_loss: 4.8617e-04 - 11s/epoch - 128us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00048616909674026657
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:23:11.814182: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:15873 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0978646793584191
cosine 0.09663202130938708
MAE: 0.00358158805853456
RMSE: 0.01879429311671185
r2: 0.7241133408041418
RMSE zero-vector: 0.04004287452915337
['2.4custom_VAE', 'mse', 32, 50, 0.0005, 0.1, 94, 0.0005155969874161061, 0.00048616909674026657, 0.0978646793584191, 0.09663202130938708, 0.00358158805853456, 0.01879429311671185, 0.7241133408041418, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 25 0.0005 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 1791)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 21:23:20.788658: W tensorflow/c/c_api.cc:291] Operation '{name:'training_26/Adam/dense_enc0_13/bias/m/Assign' id:17653 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/dense_enc0_13/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/dense_enc0_13/bias/m, training_26/Adam/dense_enc0_13/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:23:29.011122: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:17167 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 10s - loss: 0.0038 - val_loss: 9.5890e-04 - 10s/epoch - 124us/sample
Epoch 2/25
84077/84077 - 7s - loss: 0.0147 - val_loss: 7.8666e-04 - 7s/epoch - 84us/sample
Epoch 3/25
84077/84077 - 7s - loss: 9.2152e-04 - val_loss: 7.8160e-04 - 7s/epoch - 85us/sample
Epoch 4/25
84077/84077 - 7s - loss: 7.4475e-04 - val_loss: 6.4824e-04 - 7s/epoch - 84us/sample
Epoch 5/25
84077/84077 - 7s - loss: 6.6735e-04 - val_loss: 7.5677e-04 - 7s/epoch - 84us/sample
Epoch 6/25
84077/84077 - 7s - loss: 5.7464e-04 - val_loss: 4.5642e-04 - 7s/epoch - 85us/sample
Epoch 7/25
84077/84077 - 7s - loss: 4.6694e-04 - val_loss: 4.4320e-04 - 7s/epoch - 85us/sample
Epoch 8/25
84077/84077 - 7s - loss: 4.3499e-04 - val_loss: 4.5802e-04 - 7s/epoch - 85us/sample
Epoch 9/25
84077/84077 - 7s - loss: 4.1422e-04 - val_loss: 3.7960e-04 - 7s/epoch - 86us/sample
Epoch 10/25
84077/84077 - 7s - loss: 3.8180e-04 - val_loss: 3.6704e-04 - 7s/epoch - 85us/sample
Epoch 11/25
84077/84077 - 7s - loss: 3.5767e-04 - val_loss: 3.2676e-04 - 7s/epoch - 85us/sample
Epoch 12/25
84077/84077 - 7s - loss: 3.4285e-04 - val_loss: 3.1980e-04 - 7s/epoch - 85us/sample
Epoch 13/25
84077/84077 - 7s - loss: 3.3015e-04 - val_loss: 3.0523e-04 - 7s/epoch - 85us/sample
Epoch 14/25
84077/84077 - 7s - loss: 3.1973e-04 - val_loss: 2.9230e-04 - 7s/epoch - 84us/sample
Epoch 15/25
84077/84077 - 7s - loss: 3.0993e-04 - val_loss: 2.8607e-04 - 7s/epoch - 85us/sample
Epoch 16/25
84077/84077 - 7s - loss: 3.0162e-04 - val_loss: 2.7808e-04 - 7s/epoch - 86us/sample
Epoch 17/25
84077/84077 - 7s - loss: 2.9337e-04 - val_loss: 2.6999e-04 - 7s/epoch - 85us/sample
Epoch 18/25
84077/84077 - 7s - loss: 2.8670e-04 - val_loss: 2.6594e-04 - 7s/epoch - 85us/sample
Epoch 19/25
84077/84077 - 7s - loss: 2.8192e-04 - val_loss: 2.6217e-04 - 7s/epoch - 85us/sample
Epoch 20/25
84077/84077 - 7s - loss: 2.7750e-04 - val_loss: 2.5824e-04 - 7s/epoch - 85us/sample
Epoch 21/25
84077/84077 - 7s - loss: 2.7167e-04 - val_loss: 2.5443e-04 - 7s/epoch - 85us/sample
Epoch 22/25
84077/84077 - 7s - loss: 2.6635e-04 - val_loss: 2.4616e-04 - 7s/epoch - 85us/sample
Epoch 23/25
84077/84077 - 7s - loss: 2.6163e-04 - val_loss: 2.4332e-04 - 7s/epoch - 85us/sample
Epoch 24/25
84077/84077 - 7s - loss: 2.5817e-04 - val_loss: 2.3947e-04 - 7s/epoch - 85us/sample
Epoch 25/25
84077/84077 - 7s - loss: 2.5490e-04 - val_loss: 2.3654e-04 - 7s/epoch - 85us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002365417536888435
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:26:21.326891: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:17131 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.08283017279076796
cosine 0.08178543273666711
MAE: 0.004004785464920471
RMSE: 0.016092588507461706
r2: 0.7977642278472153
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 25, 0.0005, 0.1, 94, 0.0002548973654163472, 0.0002365417536888435, 0.08283017279076796, 0.08178543273666711, 0.004004785464920471, 0.016092588507461706, 0.7977642278472153, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 20 0.0008 16 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 1886)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/20
2023-02-14 21:26:30.526014: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_42/beta/Assign' id:18040 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_42/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_42/beta, batch_normalization_42/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:26:49.107906: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:18448 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0052 - val_loss: 0.0074 - 21s/epoch - 256us/sample
Epoch 2/20
84077/84077 - 18s - loss: 0.0017 - val_loss: 0.0013 - 18s/epoch - 213us/sample
Epoch 3/20
84077/84077 - 18s - loss: 0.0016 - val_loss: 0.0010 - 18s/epoch - 214us/sample
Epoch 4/20
84077/84077 - 18s - loss: 0.0012 - val_loss: 9.5368e-04 - 18s/epoch - 214us/sample
Epoch 5/20
84077/84077 - 18s - loss: 9.9813e-04 - val_loss: 9.1267e-04 - 18s/epoch - 213us/sample
Epoch 6/20
84077/84077 - 18s - loss: 9.3387e-04 - val_loss: 8.8405e-04 - 18s/epoch - 213us/sample
Epoch 7/20
84077/84077 - 18s - loss: 8.9097e-04 - val_loss: 8.6398e-04 - 18s/epoch - 214us/sample
Epoch 8/20
84077/84077 - 18s - loss: 8.5830e-04 - val_loss: 8.7203e-04 - 18s/epoch - 214us/sample
Epoch 9/20
84077/84077 - 18s - loss: 8.3262e-04 - val_loss: 8.5651e-04 - 18s/epoch - 213us/sample
Epoch 10/20
84077/84077 - 18s - loss: 8.1008e-04 - val_loss: 9.0434e-04 - 18s/epoch - 213us/sample
Epoch 11/20
84077/84077 - 18s - loss: 7.9343e-04 - val_loss: 0.0010 - 18s/epoch - 215us/sample
Epoch 12/20
84077/84077 - 18s - loss: 7.8129e-04 - val_loss: 0.0010 - 18s/epoch - 213us/sample
Epoch 13/20
84077/84077 - 18s - loss: 7.7011e-04 - val_loss: 0.0013 - 18s/epoch - 213us/sample
Epoch 14/20
84077/84077 - 18s - loss: 7.5695e-04 - val_loss: 0.0014 - 18s/epoch - 215us/sample
Epoch 15/20
84077/84077 - 18s - loss: 7.4093e-04 - val_loss: 0.0015 - 18s/epoch - 214us/sample
Epoch 16/20
84077/84077 - 18s - loss: 7.2944e-04 - val_loss: 0.0013 - 18s/epoch - 213us/sample
Epoch 17/20
84077/84077 - 18s - loss: 7.1776e-04 - val_loss: 0.0014 - 18s/epoch - 215us/sample
Epoch 18/20
84077/84077 - 18s - loss: 7.0581e-04 - val_loss: 0.0013 - 18s/epoch - 214us/sample
Epoch 19/20
84077/84077 - 18s - loss: 6.9764e-04 - val_loss: 0.0015 - 18s/epoch - 213us/sample
Epoch 20/20
84077/84077 - 18s - loss: 6.8915e-04 - val_loss: 0.0015 - 18s/epoch - 213us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0014500533533367316
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:32:32.389893: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:18419 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.15418708872937917
cosine 0.15220825956086717
MAE: 0.005656516616021906
RMSE: 0.036463979840402644
r2: -0.038651780932791154
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 16, 20, 0.0008, 0.1, 94, 0.0006891485698507036, 0.0014500533533367316, 0.15418708872937917, 0.15220825956086717, 0.005656516616021906, 0.036463979840402644, -0.038651780932791154, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 30 0.001 16 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 1886)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/30
2023-02-14 21:32:41.989161: W tensorflow/c/c_api.cc:291] Operation '{name:'training_30/Adam/bottleneck_zmean_15/kernel/v/Assign' id:20328 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/bottleneck_zmean_15/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_30/Adam/bottleneck_zmean_15/kernel/v, training_30/Adam/bottleneck_zmean_15/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:33:00.837475: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:19710 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0027 - val_loss: 8.6573e-04 - 22s/epoch - 261us/sample
Epoch 2/30
84077/84077 - 18s - loss: 6.5553e-04 - val_loss: 7.2320e-04 - 18s/epoch - 214us/sample
Epoch 3/30
84077/84077 - 18s - loss: 5.2195e-04 - val_loss: 4.9137e-04 - 18s/epoch - 214us/sample
Epoch 4/30
84077/84077 - 18s - loss: 4.8757e-04 - val_loss: 4.5991e-04 - 18s/epoch - 214us/sample
Epoch 5/30
84077/84077 - 18s - loss: 4.6871e-04 - val_loss: 4.4637e-04 - 18s/epoch - 216us/sample
Epoch 6/30
84077/84077 - 18s - loss: 4.5804e-04 - val_loss: 4.4066e-04 - 18s/epoch - 214us/sample
Epoch 7/30
84077/84077 - 18s - loss: 4.5312e-04 - val_loss: 4.3940e-04 - 18s/epoch - 214us/sample
Epoch 8/30
84077/84077 - 18s - loss: 4.4811e-04 - val_loss: 4.4676e-04 - 18s/epoch - 215us/sample
Epoch 9/30
84077/84077 - 18s - loss: 4.4524e-04 - val_loss: 4.3619e-04 - 18s/epoch - 215us/sample
Epoch 10/30
84077/84077 - 18s - loss: 4.4343e-04 - val_loss: 4.6488e-04 - 18s/epoch - 214us/sample
Epoch 11/30
84077/84077 - 18s - loss: 4.4172e-04 - val_loss: 4.3893e-04 - 18s/epoch - 215us/sample
Epoch 12/30
84077/84077 - 18s - loss: 4.4122e-04 - val_loss: 4.6402e-04 - 18s/epoch - 216us/sample
Epoch 13/30
84077/84077 - 18s - loss: 4.3982e-04 - val_loss: 4.4305e-04 - 18s/epoch - 215us/sample
Epoch 14/30
84077/84077 - 18s - loss: 4.3891e-04 - val_loss: 5.0562e-04 - 18s/epoch - 214us/sample
Epoch 15/30
84077/84077 - 18s - loss: 4.3855e-04 - val_loss: 4.7384e-04 - 18s/epoch - 216us/sample
Epoch 16/30
84077/84077 - 18s - loss: 4.3787e-04 - val_loss: 4.3322e-04 - 18s/epoch - 214us/sample
Epoch 17/30
84077/84077 - 18s - loss: 4.3761e-04 - val_loss: 4.7982e-04 - 18s/epoch - 214us/sample
Epoch 18/30
84077/84077 - 18s - loss: 4.3684e-04 - val_loss: 4.8764e-04 - 18s/epoch - 215us/sample
Epoch 19/30
84077/84077 - 18s - loss: 4.3594e-04 - val_loss: 4.2506e-04 - 18s/epoch - 215us/sample
Epoch 20/30
84077/84077 - 18s - loss: 4.3532e-04 - val_loss: 4.6423e-04 - 18s/epoch - 214us/sample
Epoch 21/30
84077/84077 - 18s - loss: 4.3490e-04 - val_loss: 4.3144e-04 - 18s/epoch - 216us/sample
Epoch 22/30
84077/84077 - 18s - loss: 4.3505e-04 - val_loss: 4.5593e-04 - 18s/epoch - 214us/sample
Epoch 23/30
84077/84077 - 18s - loss: 4.3446e-04 - val_loss: 4.2540e-04 - 18s/epoch - 214us/sample
Epoch 24/30
84077/84077 - 18s - loss: 4.3446e-04 - val_loss: 4.3814e-04 - 18s/epoch - 214us/sample
Epoch 25/30
84077/84077 - 18s - loss: 4.3445e-04 - val_loss: 4.4318e-04 - 18s/epoch - 215us/sample
Epoch 26/30
84077/84077 - 18s - loss: 4.3390e-04 - val_loss: 4.3704e-04 - 18s/epoch - 214us/sample
Epoch 27/30
84077/84077 - 18s - loss: 4.3415e-04 - val_loss: 4.4324e-04 - 18s/epoch - 215us/sample
Epoch 28/30
84077/84077 - 18s - loss: 4.3354e-04 - val_loss: 4.7314e-04 - 18s/epoch - 215us/sample
Epoch 29/30
84077/84077 - 18s - loss: 4.3329e-04 - val_loss: 5.1619e-04 - 18s/epoch - 216us/sample
Epoch 30/30
84077/84077 - 18s - loss: 4.3368e-04 - val_loss: 4.8228e-04 - 18s/epoch - 215us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00048227765379526205
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:41:46.352115: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:19674 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.290452171075134
cosine 0.28616839209190875
MAE: 0.00672363264159836
RMSE: 0.029827197945219906
r2: 0.3050035172818545
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 16, 30, 0.001, 0.1, 94, 0.000433682220296044, 0.00048227765379526205, 0.290452171075134, 0.28616839209190875, 0.00672363264159836, 0.029827197945219906, 0.3050035172818545, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 25 0.0012000000000000001 16 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 2357)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 21:41:56.272099: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/batch_normalization_48/beta/m/Assign' id:21462 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/batch_normalization_48/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/batch_normalization_48/beta/m, training_32/Adam/batch_normalization_48/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:42:15.620073: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:20988 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0161 - val_loss: 0.0104 - 23s/epoch - 269us/sample
Epoch 2/25
84077/84077 - 18s - loss: 0.0034 - val_loss: 0.0013 - 18s/epoch - 218us/sample
Epoch 3/25
84077/84077 - 18s - loss: 0.0014 - val_loss: 0.0011 - 18s/epoch - 219us/sample
Epoch 4/25
84077/84077 - 18s - loss: 0.0011 - val_loss: 9.7624e-04 - 18s/epoch - 218us/sample
Epoch 5/25
84077/84077 - 18s - loss: 0.0010 - val_loss: 9.2102e-04 - 18s/epoch - 219us/sample
Epoch 6/25
84077/84077 - 18s - loss: 9.4152e-04 - val_loss: 9.1225e-04 - 18s/epoch - 219us/sample
Epoch 7/25
84077/84077 - 18s - loss: 9.0281e-04 - val_loss: 8.6701e-04 - 18s/epoch - 218us/sample
Epoch 8/25
84077/84077 - 18s - loss: 8.7715e-04 - val_loss: 9.3741e-04 - 18s/epoch - 219us/sample
Epoch 9/25
84077/84077 - 18s - loss: 8.4874e-04 - val_loss: 0.0011 - 18s/epoch - 220us/sample
Epoch 10/25
84077/84077 - 18s - loss: 8.2761e-04 - val_loss: 0.0010 - 18s/epoch - 219us/sample
Epoch 11/25
84077/84077 - 18s - loss: 8.1342e-04 - val_loss: 0.0014 - 18s/epoch - 219us/sample
Epoch 12/25
84077/84077 - 18s - loss: 7.9518e-04 - val_loss: 0.0014 - 18s/epoch - 220us/sample
Epoch 13/25
84077/84077 - 18s - loss: 7.8015e-04 - val_loss: 0.0013 - 18s/epoch - 219us/sample
Epoch 14/25
84077/84077 - 18s - loss: 7.6425e-04 - val_loss: 0.0019 - 18s/epoch - 219us/sample
Epoch 15/25
84077/84077 - 18s - loss: 7.5170e-04 - val_loss: 0.0020 - 18s/epoch - 219us/sample
Epoch 16/25
84077/84077 - 19s - loss: 7.4232e-04 - val_loss: 0.0017 - 19s/epoch - 221us/sample
Epoch 17/25
84077/84077 - 18s - loss: 7.3031e-04 - val_loss: 0.0020 - 18s/epoch - 219us/sample
Epoch 18/25
84077/84077 - 18s - loss: 7.1677e-04 - val_loss: 0.0015 - 18s/epoch - 219us/sample
Epoch 19/25
84077/84077 - 18s - loss: 7.0896e-04 - val_loss: 0.0017 - 18s/epoch - 220us/sample
Epoch 20/25
84077/84077 - 18s - loss: 7.0202e-04 - val_loss: 0.0022 - 18s/epoch - 219us/sample
Epoch 21/25
84077/84077 - 18s - loss: 6.9700e-04 - val_loss: 0.0021 - 18s/epoch - 218us/sample
Epoch 22/25
84077/84077 - 18s - loss: 6.9174e-04 - val_loss: 0.0021 - 18s/epoch - 219us/sample
Epoch 23/25
84077/84077 - 18s - loss: 6.8632e-04 - val_loss: 0.0026 - 18s/epoch - 220us/sample
Epoch 24/25
84077/84077 - 18s - loss: 6.8157e-04 - val_loss: 0.0020 - 18s/epoch - 219us/sample
Epoch 25/25
84077/84077 - 18s - loss: 6.7601e-04 - val_loss: 0.0026 - 18s/epoch - 219us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.002603163908786926
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:49:39.727943: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:20959 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.1543840193907514
cosine 0.1523763990513337
MAE: 0.005866059688133569
RMSE: 0.050052351090535505
r2: -0.9570261183865326
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'mse', 16, 25, 0.0012000000000000001, 0.1, 94, 0.0006760056694650661, 0.002603163908786926, 0.1543840193907514, 0.1523763990513337, 0.005866059688133569, 0.050052351090535505, -0.9570261183865326, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 424.308669689168
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 424.308669689168.
Best solutions :  [array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [424.308669689168]
[2.5 50 0.00030000000000000003 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 2357)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 21:49:50.294188: W tensorflow/c/c_api.cc:291] Operation '{name:'training_34/Adam/batch_normalization_51/gamma/v/Assign' id:22854 op device:{requested: '', assigned: ''} def:{{{node training_34/Adam/batch_normalization_51/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_34/Adam/batch_normalization_51/gamma/v, training_34/Adam/batch_normalization_51/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:49:59.210410: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:22250 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0063 - val_loss: 0.0010 - 12s/epoch - 139us/sample
Epoch 2/50
84077/84077 - 7s - loss: 0.0021 - val_loss: 9.9270e-04 - 7s/epoch - 89us/sample
Epoch 3/50
84077/84077 - 7s - loss: 7.8976e-04 - val_loss: 8.5569e-04 - 7s/epoch - 89us/sample
Epoch 4/50
84077/84077 - 7s - loss: 0.0013 - val_loss: 6.3687e-04 - 7s/epoch - 88us/sample
Epoch 5/50
84077/84077 - 7s - loss: 6.2613e-04 - val_loss: 5.7116e-04 - 7s/epoch - 88us/sample
Epoch 6/50
84077/84077 - 7s - loss: 7.3650e-04 - val_loss: 5.2718e-04 - 7s/epoch - 88us/sample
Epoch 7/50
84077/84077 - 7s - loss: 5.2072e-04 - val_loss: 5.2285e-04 - 7s/epoch - 88us/sample
Epoch 8/50
84077/84077 - 7s - loss: 5.0855e-04 - val_loss: 5.8844e-04 - 7s/epoch - 88us/sample
Epoch 9/50
84077/84077 - 7s - loss: 4.8046e-04 - val_loss: 4.4395e-04 - 7s/epoch - 89us/sample
Epoch 10/50
84077/84077 - 7s - loss: 4.5230e-04 - val_loss: 4.2248e-04 - 7s/epoch - 89us/sample
Epoch 11/50
84077/84077 - 7s - loss: 4.2756e-04 - val_loss: 4.0253e-04 - 7s/epoch - 89us/sample
Epoch 12/50
84077/84077 - 7s - loss: 4.3211e-04 - val_loss: 3.8197e-04 - 7s/epoch - 89us/sample
Epoch 13/50
84077/84077 - 7s - loss: 0.0695 - val_loss: 5.1281e-04 - 7s/epoch - 89us/sample
Epoch 14/50
84077/84077 - 7s - loss: 3.8660e-04 - val_loss: 3.6021e-04 - 7s/epoch - 88us/sample
Epoch 15/50
84077/84077 - 7s - loss: 3.6875e-04 - val_loss: 3.4856e-04 - 7s/epoch - 89us/sample
Epoch 16/50
84077/84077 - 7s - loss: 3.5949e-04 - val_loss: 3.3723e-04 - 7s/epoch - 89us/sample
Epoch 17/50
84077/84077 - 7s - loss: 3.5263e-04 - val_loss: 3.3520e-04 - 7s/epoch - 88us/sample
Epoch 18/50
84077/84077 - 8s - loss: 3.4728e-04 - val_loss: 3.3116e-04 - 8s/epoch - 89us/sample
Epoch 19/50
84077/84077 - 7s - loss: 3.4195e-04 - val_loss: 3.1897e-04 - 7s/epoch - 89us/sample
Epoch 20/50
84077/84077 - 7s - loss: 3.3816e-04 - val_loss: 3.1588e-04 - 7s/epoch - 89us/sample
Epoch 21/50
84077/84077 - 7s - loss: 3.3352e-04 - val_loss: 3.1308e-04 - 7s/epoch - 88us/sample
Epoch 22/50
84077/84077 - 7s - loss: 3.3005e-04 - val_loss: 3.0798e-04 - 7s/epoch - 88us/sample
Epoch 23/50
84077/84077 - 7s - loss: 3.2676e-04 - val_loss: 3.0592e-04 - 7s/epoch - 88us/sample
Epoch 24/50
84077/84077 - 7s - loss: 3.2332e-04 - val_loss: 3.0208e-04 - 7s/epoch - 88us/sample
Epoch 25/50
84077/84077 - 7s - loss: 3.2016e-04 - val_loss: 2.9897e-04 - 7s/epoch - 89us/sample
Epoch 26/50
84077/84077 - 7s - loss: 3.1731e-04 - val_loss: 2.9447e-04 - 7s/epoch - 89us/sample
Epoch 27/50
84077/84077 - 7s - loss: 3.1399e-04 - val_loss: 2.9130e-04 - 7s/epoch - 88us/sample
Epoch 28/50
84077/84077 - 7s - loss: 3.1151e-04 - val_loss: 2.9086e-04 - 7s/epoch - 88us/sample
Epoch 29/50
84077/84077 - 7s - loss: 3.0885e-04 - val_loss: 2.9125e-04 - 7s/epoch - 88us/sample
Epoch 30/50
84077/84077 - 7s - loss: 3.0706e-04 - val_loss: 2.8711e-04 - 7s/epoch - 89us/sample
Epoch 31/50
84077/84077 - 7s - loss: 3.0445e-04 - val_loss: 2.8155e-04 - 7s/epoch - 88us/sample
Epoch 32/50
84077/84077 - 7s - loss: 3.0192e-04 - val_loss: 2.8101e-04 - 7s/epoch - 88us/sample
Epoch 33/50
84077/84077 - 8s - loss: 3.0000e-04 - val_loss: 2.7965e-04 - 8s/epoch - 89us/sample
Epoch 34/50
84077/84077 - 7s - loss: 2.9753e-04 - val_loss: 2.7618e-04 - 7s/epoch - 89us/sample
Epoch 35/50
84077/84077 - 7s - loss: 2.9570e-04 - val_loss: 2.7753e-04 - 7s/epoch - 89us/sample
Epoch 36/50
84077/84077 - 7s - loss: 2.9379e-04 - val_loss: 2.7562e-04 - 7s/epoch - 89us/sample
Epoch 37/50
84077/84077 - 7s - loss: 2.9229e-04 - val_loss: 2.7200e-04 - 7s/epoch - 88us/sample
Epoch 38/50
84077/84077 - 7s - loss: 2.9192e-04 - val_loss: 2.7348e-04 - 7s/epoch - 88us/sample
Epoch 39/50
84077/84077 - 7s - loss: 2.9046e-04 - val_loss: 2.7173e-04 - 7s/epoch - 89us/sample
Epoch 40/50
84077/84077 - 7s - loss: 2.8931e-04 - val_loss: 2.7098e-04 - 7s/epoch - 89us/sample
Epoch 41/50
84077/84077 - 7s - loss: 2.8857e-04 - val_loss: 2.7088e-04 - 7s/epoch - 89us/sample
Epoch 42/50
84077/84077 - 7s - loss: 2.8702e-04 - val_loss: 2.6693e-04 - 7s/epoch - 89us/sample
Epoch 43/50
84077/84077 - 7s - loss: 2.8574e-04 - val_loss: 2.6740e-04 - 7s/epoch - 89us/sample
Epoch 44/50
84077/84077 - 7s - loss: 2.8475e-04 - val_loss: 2.6407e-04 - 7s/epoch - 89us/sample
Epoch 45/50
84077/84077 - 7s - loss: 2.8329e-04 - val_loss: 2.6380e-04 - 7s/epoch - 88us/sample
Epoch 46/50
84077/84077 - 7s - loss: 2.8281e-04 - val_loss: 2.6371e-04 - 7s/epoch - 89us/sample
Epoch 47/50
84077/84077 - 7s - loss: 2.8177e-04 - val_loss: 2.6160e-04 - 7s/epoch - 88us/sample
Epoch 48/50
84077/84077 - 7s - loss: 2.8198e-04 - val_loss: 2.6127e-04 - 7s/epoch - 89us/sample
Epoch 49/50
84077/84077 - 8s - loss: 2.8076e-04 - val_loss: 2.6084e-04 - 8s/epoch - 89us/sample
Epoch 50/50
84077/84077 - 7s - loss: 2.7999e-04 - val_loss: 2.6142e-04 - 7s/epoch - 89us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.000261418087395186
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:56:06.029830: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:22214 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.09879401339471536
cosine 0.09752903841363343
MAE: 0.0035225147877752903
RMSE: 0.018158159113157362
r2: 0.7424546927751817
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 50, 0.00030000000000000003, 0.1, 94, 0.00027998889329623923, 0.000261418087395186, 0.09879401339471536, 0.09752903841363343, 0.0035225147877752903, 0.018158159113157362, 0.7424546927751817, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 45 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 2357)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/45
2023-02-14 21:56:16.323936: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_54/moving_mean/Assign' id:23127 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_54/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_54/moving_mean, batch_normalization_54/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:56:25.437374: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_18/mul' id:23535 op device:{requested: '', assigned: ''} def:{{{node loss_18/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_18/mul/x, loss_18/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0036 - val_loss: 0.0012 - 12s/epoch - 143us/sample
Epoch 2/45
84077/84077 - 7s - loss: 0.0024 - val_loss: 7.6895e-04 - 7s/epoch - 89us/sample
Epoch 3/45
84077/84077 - 7s - loss: 0.0020 - val_loss: 7.9837e-04 - 7s/epoch - 89us/sample
Epoch 4/45
84077/84077 - 8s - loss: 8.9371e-04 - val_loss: 6.2114e-04 - 8s/epoch - 89us/sample
Epoch 5/45
84077/84077 - 7s - loss: 6.2344e-04 - val_loss: 6.2997e-04 - 7s/epoch - 89us/sample
Epoch 6/45
84077/84077 - 7s - loss: 5.8030e-04 - val_loss: 5.1001e-04 - 7s/epoch - 88us/sample
Epoch 7/45
84077/84077 - 7s - loss: 5.2911e-04 - val_loss: 4.5788e-04 - 7s/epoch - 88us/sample
Epoch 8/45
84077/84077 - 7s - loss: 4.8073e-04 - val_loss: 4.1772e-04 - 7s/epoch - 88us/sample
Epoch 9/45
84077/84077 - 7s - loss: 4.3251e-04 - val_loss: 3.8917e-04 - 7s/epoch - 89us/sample
Epoch 10/45
84077/84077 - 7s - loss: 4.1154e-04 - val_loss: 3.8274e-04 - 7s/epoch - 89us/sample
Epoch 11/45
84077/84077 - 7s - loss: 3.9729e-04 - val_loss: 3.9579e-04 - 7s/epoch - 88us/sample
Epoch 12/45
84077/84077 - 7s - loss: 3.8126e-04 - val_loss: 3.4675e-04 - 7s/epoch - 88us/sample
Epoch 13/45
84077/84077 - 7s - loss: 3.6536e-04 - val_loss: 3.3322e-04 - 7s/epoch - 88us/sample
Epoch 14/45
84077/84077 - 7s - loss: 3.5074e-04 - val_loss: 3.2353e-04 - 7s/epoch - 89us/sample
Epoch 15/45
84077/84077 - 8s - loss: 3.3817e-04 - val_loss: 3.1320e-04 - 8s/epoch - 89us/sample
Epoch 16/45
84077/84077 - 7s - loss: 3.2779e-04 - val_loss: 3.0238e-04 - 7s/epoch - 88us/sample
Epoch 17/45
84077/84077 - 7s - loss: 3.2081e-04 - val_loss: 2.9421e-04 - 7s/epoch - 89us/sample
Epoch 18/45
84077/84077 - 7s - loss: 3.1366e-04 - val_loss: 2.9399e-04 - 7s/epoch - 89us/sample
Epoch 19/45
84077/84077 - 7s - loss: 3.0902e-04 - val_loss: 2.8493e-04 - 7s/epoch - 88us/sample
Epoch 20/45
84077/84077 - 7s - loss: 3.0380e-04 - val_loss: 2.7936e-04 - 7s/epoch - 89us/sample
Epoch 21/45
84077/84077 - 7s - loss: 3.0002e-04 - val_loss: 2.7729e-04 - 7s/epoch - 88us/sample
Epoch 22/45
84077/84077 - 7s - loss: 2.9619e-04 - val_loss: 2.7330e-04 - 7s/epoch - 89us/sample
Epoch 23/45
84077/84077 - 8s - loss: 2.9271e-04 - val_loss: 2.7201e-04 - 8s/epoch - 89us/sample
Epoch 24/45
84077/84077 - 7s - loss: 2.8984e-04 - val_loss: 2.7007e-04 - 7s/epoch - 89us/sample
Epoch 25/45
84077/84077 - 7s - loss: 2.8759e-04 - val_loss: 2.6759e-04 - 7s/epoch - 89us/sample
Epoch 26/45
84077/84077 - 7s - loss: 2.8561e-04 - val_loss: 2.6509e-04 - 7s/epoch - 88us/sample
Epoch 27/45
84077/84077 - 7s - loss: 2.8388e-04 - val_loss: 2.6343e-04 - 7s/epoch - 88us/sample
Epoch 28/45
84077/84077 - 7s - loss: 2.8251e-04 - val_loss: 2.6145e-04 - 7s/epoch - 88us/sample
Epoch 29/45
84077/84077 - 7s - loss: 2.8047e-04 - val_loss: 2.6213e-04 - 7s/epoch - 89us/sample
Epoch 30/45
84077/84077 - 8s - loss: 2.7928e-04 - val_loss: 2.6083e-04 - 8s/epoch - 89us/sample
Epoch 31/45
84077/84077 - 7s - loss: 2.7872e-04 - val_loss: 2.5719e-04 - 7s/epoch - 89us/sample
Epoch 32/45
84077/84077 - 7s - loss: 2.7691e-04 - val_loss: 2.5620e-04 - 7s/epoch - 89us/sample
Epoch 33/45
84077/84077 - 7s - loss: 2.7535e-04 - val_loss: 2.5623e-04 - 7s/epoch - 89us/sample
Epoch 34/45
84077/84077 - 7s - loss: 2.7449e-04 - val_loss: 2.5613e-04 - 7s/epoch - 89us/sample
Epoch 35/45
84077/84077 - 7s - loss: 2.7288e-04 - val_loss: 2.5474e-04 - 7s/epoch - 88us/sample
Epoch 36/45
84077/84077 - 7s - loss: 2.7255e-04 - val_loss: 2.5457e-04 - 7s/epoch - 89us/sample
Epoch 37/45
84077/84077 - 7s - loss: 2.7093e-04 - val_loss: 2.5196e-04 - 7s/epoch - 89us/sample
Epoch 38/45
84077/84077 - 8s - loss: 2.6953e-04 - val_loss: 2.5069e-04 - 8s/epoch - 90us/sample
Epoch 39/45
84077/84077 - 8s - loss: 2.6850e-04 - val_loss: 2.4831e-04 - 8s/epoch - 90us/sample
Epoch 40/45
84077/84077 - 7s - loss: 2.6740e-04 - val_loss: 2.4899e-04 - 7s/epoch - 89us/sample
Epoch 41/45
84077/84077 - 7s - loss: 2.6644e-04 - val_loss: 2.4689e-04 - 7s/epoch - 89us/sample
Epoch 42/45
84077/84077 - 7s - loss: 2.6540e-04 - val_loss: 2.4630e-04 - 7s/epoch - 89us/sample
Epoch 43/45
84077/84077 - 7s - loss: 2.6429e-04 - val_loss: 2.4700e-04 - 7s/epoch - 89us/sample
Epoch 44/45
84077/84077 - 7s - loss: 2.6380e-04 - val_loss: 2.4576e-04 - 7s/epoch - 89us/sample
Epoch 45/45
84077/84077 - 7s - loss: 2.6260e-04 - val_loss: 2.4555e-04 - 7s/epoch - 88us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002455469699740939
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:01:55.304139: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_18/outputlayer/BiasAdd' id:23499 op device:{requested: '', assigned: ''} def:{{{node decoder_model_18/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_18/outputlayer/MatMul, decoder_model_18/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.08878037128008373
cosine 0.08765533884544141
MAE: 0.0036289242088466123
RMSE: 0.01695449869021812
r2: 0.7754918831972135
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 45, 0.0005, 0.1, 94, 0.00026259663283132286, 0.0002455469699740939, 0.08878037128008373, 0.08765533884544141, 0.0036289242088466123, 0.01695449869021812, 0.7754918831972135, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 50 0.0005 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2451)         2313744     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 2451)        9804        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 2451)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           230488      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           230488      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2564191     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,348,715
Trainable params: 5,338,723
Non-trainable params: 9,992
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 22:02:07.527451: W tensorflow/c/c_api.cc:291] Operation '{name:'training_38/Adam/dense_dec1_19/kernel/v/Assign' id:25465 op device:{requested: '', assigned: ''} def:{{{node training_38/Adam/dense_dec1_19/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_38/Adam/dense_dec1_19/kernel/v, training_38/Adam/dense_dec1_19/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:02:16.721311: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_19/mul' id:24820 op device:{requested: '', assigned: ''} def:{{{node loss_19/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_19/mul/x, loss_19/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0162 - val_loss: 0.0014 - 12s/epoch - 147us/sample
Epoch 2/50
84077/84077 - 7s - loss: 0.0017 - val_loss: 7.6889e-04 - 7s/epoch - 89us/sample
Epoch 3/50
84077/84077 - 7s - loss: 8.5767e-04 - val_loss: 0.0020 - 7s/epoch - 89us/sample
Epoch 4/50
84077/84077 - 7s - loss: 0.0016 - val_loss: 9.0239e-04 - 7s/epoch - 89us/sample
Epoch 5/50
84077/84077 - 7s - loss: 6.9698e-04 - val_loss: 5.8108e-04 - 7s/epoch - 89us/sample
Epoch 6/50
84077/84077 - 7s - loss: 5.6791e-04 - val_loss: 5.2950e-04 - 7s/epoch - 89us/sample
Epoch 7/50
84077/84077 - 8s - loss: 5.2436e-04 - val_loss: 4.9551e-04 - 8s/epoch - 89us/sample
Epoch 8/50
84077/84077 - 8s - loss: 5.0216e-04 - val_loss: 5.0993e-04 - 8s/epoch - 89us/sample
Epoch 9/50
84077/84077 - 8s - loss: 4.7730e-04 - val_loss: 4.4981e-04 - 8s/epoch - 90us/sample
Epoch 10/50
84077/84077 - 8s - loss: 4.5002e-04 - val_loss: 4.2524e-04 - 8s/epoch - 89us/sample
Epoch 11/50
84077/84077 - 8s - loss: 4.3358e-04 - val_loss: 4.2234e-04 - 8s/epoch - 89us/sample
Epoch 12/50
84077/84077 - 8s - loss: 4.1783e-04 - val_loss: 3.8914e-04 - 8s/epoch - 89us/sample
Epoch 13/50
84077/84077 - 7s - loss: 4.0327e-04 - val_loss: 3.8226e-04 - 7s/epoch - 89us/sample
Epoch 14/50
84077/84077 - 7s - loss: 3.9477e-04 - val_loss: 3.6878e-04 - 7s/epoch - 89us/sample
Epoch 15/50
84077/84077 - 7s - loss: 3.7486e-04 - val_loss: 3.6059e-04 - 7s/epoch - 89us/sample
Epoch 16/50
84077/84077 - 8s - loss: 3.6332e-04 - val_loss: 3.4126e-04 - 8s/epoch - 90us/sample
Epoch 17/50
84077/84077 - 8s - loss: 3.5590e-04 - val_loss: 3.3208e-04 - 8s/epoch - 90us/sample
Epoch 18/50
84077/84077 - 7s - loss: 3.4826e-04 - val_loss: 3.2604e-04 - 7s/epoch - 89us/sample
Epoch 19/50
84077/84077 - 7s - loss: 3.4211e-04 - val_loss: 3.1881e-04 - 7s/epoch - 89us/sample
Epoch 20/50
84077/84077 - 7s - loss: 3.3599e-04 - val_loss: 3.1287e-04 - 7s/epoch - 89us/sample
Epoch 21/50
84077/84077 - 8s - loss: 3.3185e-04 - val_loss: 3.0853e-04 - 8s/epoch - 89us/sample
Epoch 22/50
84077/84077 - 7s - loss: 3.2744e-04 - val_loss: 3.0645e-04 - 7s/epoch - 89us/sample
Epoch 23/50
84077/84077 - 7s - loss: 3.2419e-04 - val_loss: 3.0208e-04 - 7s/epoch - 89us/sample
Epoch 24/50
84077/84077 - 8s - loss: 3.2120e-04 - val_loss: 2.9939e-04 - 8s/epoch - 90us/sample
Epoch 25/50
84077/84077 - 7s - loss: 3.1855e-04 - val_loss: 2.9733e-04 - 7s/epoch - 89us/sample
Epoch 26/50
84077/84077 - 8s - loss: 3.1456e-04 - val_loss: 2.9257e-04 - 8s/epoch - 89us/sample
Epoch 27/50
84077/84077 - 7s - loss: 3.1136e-04 - val_loss: 2.9027e-04 - 7s/epoch - 89us/sample
Epoch 28/50
84077/84077 - 7s - loss: 3.0910e-04 - val_loss: 2.8836e-04 - 7s/epoch - 89us/sample
Epoch 29/50
84077/84077 - 7s - loss: 3.0701e-04 - val_loss: 2.8782e-04 - 7s/epoch - 89us/sample
Epoch 30/50
84077/84077 - 7s - loss: 3.0536e-04 - val_loss: 2.8423e-04 - 7s/epoch - 89us/sample
Epoch 31/50
84077/84077 - 8s - loss: 3.0281e-04 - val_loss: 2.8280e-04 - 8s/epoch - 92us/sample
Epoch 32/50
84077/84077 - 8s - loss: 3.0137e-04 - val_loss: 2.8250e-04 - 8s/epoch - 90us/sample
Epoch 33/50
84077/84077 - 8s - loss: 2.9977e-04 - val_loss: 2.8024e-04 - 8s/epoch - 89us/sample
Epoch 34/50
84077/84077 - 7s - loss: 2.9763e-04 - val_loss: 2.7779e-04 - 7s/epoch - 89us/sample
Epoch 35/50
84077/84077 - 8s - loss: 2.9604e-04 - val_loss: 2.7901e-04 - 8s/epoch - 89us/sample
Epoch 36/50
84077/84077 - 7s - loss: 2.9401e-04 - val_loss: 2.7520e-04 - 7s/epoch - 89us/sample
Epoch 37/50
84077/84077 - 7s - loss: 2.9299e-04 - val_loss: 2.7543e-04 - 7s/epoch - 89us/sample
Epoch 38/50
84077/84077 - 7s - loss: 2.9250e-04 - val_loss: 2.7482e-04 - 7s/epoch - 89us/sample
Epoch 39/50
84077/84077 - 8s - loss: 2.9045e-04 - val_loss: 2.7554e-04 - 8s/epoch - 90us/sample
Epoch 40/50
84077/84077 - 8s - loss: 2.8990e-04 - val_loss: 2.7279e-04 - 8s/epoch - 90us/sample
Epoch 41/50
84077/84077 - 7s - loss: 2.8869e-04 - val_loss: 2.7084e-04 - 7s/epoch - 89us/sample
Epoch 42/50
84077/84077 - 7s - loss: 2.8813e-04 - val_loss: 2.7102e-04 - 7s/epoch - 89us/sample
Epoch 43/50
84077/84077 - 7s - loss: 2.8680e-04 - val_loss: 2.7018e-04 - 7s/epoch - 89us/sample
Epoch 44/50
84077/84077 - 8s - loss: 2.8537e-04 - val_loss: 2.6805e-04 - 8s/epoch - 89us/sample
Epoch 45/50
84077/84077 - 7s - loss: 2.8425e-04 - val_loss: 2.6724e-04 - 7s/epoch - 89us/sample
Epoch 46/50
84077/84077 - 8s - loss: 2.8279e-04 - val_loss: 2.6519e-04 - 8s/epoch - 89us/sample
Epoch 47/50
84077/84077 - 8s - loss: 2.8150e-04 - val_loss: 2.6540e-04 - 8s/epoch - 90us/sample
Epoch 48/50
84077/84077 - 8s - loss: 2.8051e-04 - val_loss: 2.6541e-04 - 8s/epoch - 90us/sample
Epoch 49/50
84077/84077 - 7s - loss: 2.7998e-04 - val_loss: 2.6278e-04 - 7s/epoch - 89us/sample
Epoch 50/50
84077/84077 - 7s - loss: 2.7810e-04 - val_loss: 2.6257e-04 - 7s/epoch - 89us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002625707826048809
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:08:26.364386: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:24784 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10522384077976674
cosine 0.10384206950108844
MAE: 0.00367663666318169
RMSE: 0.01860170084364874
r2: 0.7297031140324802
RMSE zero-vector: 0.04004287452915337
['2.6custom_VAE', 'logcosh', 64, 50, 0.0005, 0.1, 94, 0.00027809843811624533, 0.0002625707826048809, 0.10522384077976674, 0.10384206950108844, 0.00367663666318169, 0.01860170084364874, 0.7297031140324802, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 25 0.0007 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 1791)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 22:08:37.374630: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_60/gamma/Assign' id:25686 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_60/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_60/gamma, batch_normalization_60/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:08:46.388875: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:26108 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0043 - val_loss: 0.0010 - 12s/epoch - 146us/sample
Epoch 2/25
84077/84077 - 7s - loss: 9.9991e-04 - val_loss: 0.0024 - 7s/epoch - 88us/sample
Epoch 3/25
84077/84077 - 7s - loss: 0.0011 - val_loss: 8.2224e-04 - 7s/epoch - 87us/sample
Epoch 4/25
84077/84077 - 7s - loss: 7.5368e-04 - val_loss: 6.3177e-04 - 7s/epoch - 88us/sample
Epoch 5/25
84077/84077 - 7s - loss: 6.8926e-04 - val_loss: 5.1751e-04 - 7s/epoch - 87us/sample
Epoch 6/25
84077/84077 - 7s - loss: 4.8489e-04 - val_loss: 4.4090e-04 - 7s/epoch - 87us/sample
Epoch 7/25
84077/84077 - 7s - loss: 4.4698e-04 - val_loss: 4.3819e-04 - 7s/epoch - 87us/sample
Epoch 8/25
84077/84077 - 7s - loss: 4.2277e-04 - val_loss: 3.8671e-04 - 7s/epoch - 87us/sample
Epoch 9/25
84077/84077 - 7s - loss: 3.9939e-04 - val_loss: 3.6436e-04 - 7s/epoch - 87us/sample
Epoch 10/25
84077/84077 - 7s - loss: 3.7879e-04 - val_loss: 3.3920e-04 - 7s/epoch - 87us/sample
Epoch 11/25
84077/84077 - 7s - loss: 3.5654e-04 - val_loss: 3.2395e-04 - 7s/epoch - 87us/sample
Epoch 12/25
84077/84077 - 7s - loss: 3.4090e-04 - val_loss: 3.0805e-04 - 7s/epoch - 87us/sample
Epoch 13/25
84077/84077 - 7s - loss: 3.2463e-04 - val_loss: 2.9911e-04 - 7s/epoch - 87us/sample
Epoch 14/25
84077/84077 - 7s - loss: 3.1398e-04 - val_loss: 2.8680e-04 - 7s/epoch - 88us/sample
Epoch 15/25
84077/84077 - 7s - loss: 3.0506e-04 - val_loss: 2.8115e-04 - 7s/epoch - 87us/sample
Epoch 16/25
84077/84077 - 7s - loss: 2.9725e-04 - val_loss: 2.7339e-04 - 7s/epoch - 87us/sample
Epoch 17/25
84077/84077 - 7s - loss: 2.9052e-04 - val_loss: 2.6696e-04 - 7s/epoch - 87us/sample
Epoch 18/25
84077/84077 - 7s - loss: 2.8535e-04 - val_loss: 2.6777e-04 - 7s/epoch - 87us/sample
Epoch 19/25
84077/84077 - 7s - loss: 2.7976e-04 - val_loss: 2.5777e-04 - 7s/epoch - 87us/sample
Epoch 20/25
84077/84077 - 7s - loss: 2.7475e-04 - val_loss: 2.5537e-04 - 7s/epoch - 87us/sample
Epoch 21/25
84077/84077 - 7s - loss: 2.6970e-04 - val_loss: 2.5002e-04 - 7s/epoch - 87us/sample
Epoch 22/25
84077/84077 - 7s - loss: 2.6569e-04 - val_loss: 2.4558e-04 - 7s/epoch - 88us/sample
Epoch 23/25
84077/84077 - 7s - loss: 2.6130e-04 - val_loss: 2.4223e-04 - 7s/epoch - 87us/sample
Epoch 24/25
84077/84077 - 7s - loss: 2.5721e-04 - val_loss: 2.3768e-04 - 7s/epoch - 87us/sample
Epoch 25/25
84077/84077 - 7s - loss: 2.5368e-04 - val_loss: 2.3266e-04 - 7s/epoch - 87us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00023266437545184861
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:11:44.053767: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_20/outputlayer/BiasAdd' id:26072 op device:{requested: '', assigned: ''} def:{{{node decoder_model_20/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_20/outputlayer/MatMul, decoder_model_20/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.08081534968563385
cosine 0.07978951901265195
MAE: 0.004029819365598715
RMSE: 0.01593679036197778
r2: 0.8016276770608858
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 25, 0.0007, 0.1, 94, 0.000253679825522104, 0.00023266437545184861, 0.08081534968563385, 0.07978951901265195, 0.004029819365598715, 0.01593679036197778, 0.8016276770608858, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4 50 0.0005 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2263)         2136272     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_63 (BatchN  (None, 2263)        9052        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_63 (ReLU)                (None, 2263)         0           ['batch_normalization_63[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           212816      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           212816      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2368295     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,939,251
Trainable params: 4,930,011
Non-trainable params: 9,240
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 22:11:56.487883: W tensorflow/c/c_api.cc:291] Operation '{name:'training_42/Adam/dense_dec0_21/bias/v/Assign' id:28064 op device:{requested: '', assigned: ''} def:{{{node training_42/Adam/dense_dec0_21/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_42/Adam/dense_dec0_21/bias/v, training_42/Adam/dense_dec0_21/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:12:06.179279: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_21/mul' id:27393 op device:{requested: '', assigned: ''} def:{{{node loss_21/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_21/mul/x, loss_21/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 13s - loss: 0.0145 - val_loss: 0.0010 - 13s/epoch - 160us/sample
Epoch 2/50
84077/84077 - 8s - loss: 0.0018 - val_loss: 9.3129e-04 - 8s/epoch - 91us/sample
Epoch 3/50
84077/84077 - 8s - loss: 8.2682e-04 - val_loss: 7.3041e-04 - 8s/epoch - 90us/sample
Epoch 4/50
84077/84077 - 8s - loss: 7.5202e-04 - val_loss: 8.2960e-04 - 8s/epoch - 90us/sample
Epoch 5/50
84077/84077 - 8s - loss: 0.0049 - val_loss: 7.2360e-04 - 8s/epoch - 90us/sample
Epoch 6/50
84077/84077 - 8s - loss: 1.5649 - val_loss: 5.8250e-04 - 8s/epoch - 90us/sample
Epoch 7/50
84077/84077 - 8s - loss: 0.0667 - val_loss: 6.7115e-04 - 8s/epoch - 90us/sample
Epoch 8/50
84077/84077 - 8s - loss: 5.9980e-04 - val_loss: 0.0014 - 8s/epoch - 90us/sample
Epoch 9/50
84077/84077 - 8s - loss: 6.7973e-04 - val_loss: 5.2211e-04 - 8s/epoch - 90us/sample
Epoch 10/50
84077/84077 - 8s - loss: 0.0045 - val_loss: 0.0022 - 8s/epoch - 90us/sample
Epoch 11/50
84077/84077 - 8s - loss: 9.2674e-04 - val_loss: 6.0163e-04 - 8s/epoch - 90us/sample
Epoch 12/50
84077/84077 - 8s - loss: 5.8510e-04 - val_loss: 4.9068e-04 - 8s/epoch - 91us/sample
Epoch 13/50
84077/84077 - 8s - loss: 5.5028e-04 - val_loss: 0.0012 - 8s/epoch - 91us/sample
Epoch 14/50
84077/84077 - 8s - loss: 5.1435e-04 - val_loss: 4.2012e-04 - 8s/epoch - 90us/sample
Epoch 15/50
84077/84077 - 8s - loss: 4.3690e-04 - val_loss: 4.1049e-04 - 8s/epoch - 90us/sample
Epoch 16/50
84077/84077 - 8s - loss: 4.6131e-04 - val_loss: 4.4654e-04 - 8s/epoch - 90us/sample
Epoch 17/50
84077/84077 - 8s - loss: 0.0050 - val_loss: 5.9592e-04 - 8s/epoch - 90us/sample
Epoch 18/50
84077/84077 - 8s - loss: 4.9329e-04 - val_loss: 4.2515e-04 - 8s/epoch - 90us/sample
Epoch 19/50
84077/84077 - 8s - loss: 4.3666e-04 - val_loss: 4.2968e-04 - 8s/epoch - 90us/sample
Epoch 20/50
84077/84077 - 8s - loss: 4.9883e-04 - val_loss: 5.6738e-04 - 8s/epoch - 90us/sample
Epoch 21/50
84077/84077 - 8s - loss: 5.3780e-04 - val_loss: 4.8048e-04 - 8s/epoch - 91us/sample
Epoch 22/50
84077/84077 - 8s - loss: 4.8724e-04 - val_loss: 4.6886e-04 - 8s/epoch - 90us/sample
Epoch 23/50
84077/84077 - 8s - loss: 4.5740e-04 - val_loss: 3.9800e-04 - 8s/epoch - 90us/sample
Epoch 24/50
84077/84077 - 8s - loss: 9.4929 - val_loss: 0.0011 - 8s/epoch - 90us/sample
Epoch 25/50
84077/84077 - 8s - loss: 8.1358e-04 - val_loss: 7.6831e-04 - 8s/epoch - 90us/sample
Epoch 26/50
84077/84077 - 8s - loss: 7.9496e-04 - val_loss: 7.4582e-04 - 8s/epoch - 91us/sample
Epoch 27/50
84077/84077 - 8s - loss: 7.7582e-04 - val_loss: 7.4489e-04 - 8s/epoch - 90us/sample
Epoch 28/50
84077/84077 - 8s - loss: 7.6743e-04 - val_loss: 7.2965e-04 - 8s/epoch - 91us/sample
Epoch 29/50
84077/84077 - 8s - loss: 7.4030e-04 - val_loss: 7.5662e-04 - 8s/epoch - 90us/sample
Epoch 30/50
84077/84077 - 8s - loss: 7.4790e-04 - val_loss: 29161.7886 - 8s/epoch - 90us/sample
Epoch 31/50
84077/84077 - 8s - loss: 0.1946 - val_loss: 0.0011 - 8s/epoch - 90us/sample
Epoch 32/50
84077/84077 - 8s - loss: 0.0010 - val_loss: 0.0010 - 8s/epoch - 90us/sample
Epoch 33/50
84077/84077 - 8s - loss: 0.0010 - val_loss: 9.5731e-04 - 8s/epoch - 90us/sample
Epoch 34/50
84077/84077 - 8s - loss: 9.9136e-04 - val_loss: 9.3273e-04 - 8s/epoch - 90us/sample
Epoch 35/50
84077/84077 - 8s - loss: 9.7851e-04 - val_loss: 9.4787e-04 - 8s/epoch - 90us/sample
Epoch 36/50
84077/84077 - 8s - loss: 9.5066e-04 - val_loss: 8.9143e-04 - 8s/epoch - 91us/sample
Epoch 37/50
84077/84077 - 8s - loss: 0.0286 - val_loss: 9.9587e-04 - 8s/epoch - 90us/sample
Epoch 38/50
84077/84077 - 8s - loss: 9.9134e-04 - val_loss: 9.2971e-04 - 8s/epoch - 90us/sample
Epoch 39/50
84077/84077 - 8s - loss: 9.6212e-04 - val_loss: 9.0709e-04 - 8s/epoch - 90us/sample
Epoch 40/50
84077/84077 - 8s - loss: 9.4117e-04 - val_loss: 9.1311e-04 - 8s/epoch - 90us/sample
Epoch 41/50
84077/84077 - 8s - loss: 0.0073 - val_loss: 0.0010 - 8s/epoch - 91us/sample
Epoch 42/50
84077/84077 - 8s - loss: 9.7229e-04 - val_loss: 9.4179e-04 - 8s/epoch - 90us/sample
Epoch 43/50
84077/84077 - 8s - loss: 0.0010 - val_loss: 8.8780e-04 - 8s/epoch - 91us/sample
Epoch 44/50
84077/84077 - 8s - loss: 9.1582e-04 - val_loss: 8.4060e-04 - 8s/epoch - 90us/sample
Epoch 45/50
84077/84077 - 8s - loss: 0.0119 - val_loss: 9.2641e-04 - 8s/epoch - 90us/sample
Epoch 46/50
84077/84077 - 8s - loss: 0.0012 - val_loss: 9.2716e-04 - 8s/epoch - 90us/sample
Epoch 47/50
84077/84077 - 8s - loss: 0.0010 - val_loss: 9.1685e-04 - 8s/epoch - 90us/sample
Epoch 48/50
84077/84077 - 8s - loss: 0.0140 - val_loss: 5245.4175 - 8s/epoch - 90us/sample
Epoch 49/50
84077/84077 - 8s - loss: 0.7468 - val_loss: 0.0012 - 8s/epoch - 90us/sample
Epoch 50/50
84077/84077 - 8s - loss: 0.0016 - val_loss: 0.0011 - 8s/epoch - 90us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0010853551866867892
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:18:20.599072: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_21/outputlayer/BiasAdd' id:27357 op device:{requested: '', assigned: ''} def:{{{node decoder_model_21/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_21/outputlayer/MatMul, decoder_model_21/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.1313216624750749
cosine 0.12960946138507579
MAE: 0.0038234224775373295
RMSE: 0.02064859234313726
r2: 0.6669427258993009
RMSE zero-vector: 0.04004287452915337
['2.4custom_VAE', 'logcosh', 64, 50, 0.0005, 0.1, 94, 0.0015751723155783008, 0.0010853551866867892, 0.1313216624750749, 0.12960946138507579, 0.0038234224775373295, 0.02064859234313726, 0.6669427258993009, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 8 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_66 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_66 (ReLU)                (None, 2357)         0           ['batch_normalization_66[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 22:18:33.364510: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_68/moving_variance/Assign' id:28501 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_68/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_68/moving_variance, batch_normalization_68/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:19:09.281671: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_22/mul' id:28678 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 41s - loss: 0.0025 - val_loss: 6.7099e-04 - 41s/epoch - 491us/sample
Epoch 2/50
84077/84077 - 35s - loss: 5.8977e-04 - val_loss: 5.6963e-04 - 35s/epoch - 420us/sample
Epoch 3/50
84077/84077 - 35s - loss: 5.5579e-04 - val_loss: 5.2149e-04 - 35s/epoch - 421us/sample
Epoch 4/50
84077/84077 - 35s - loss: 5.2306e-04 - val_loss: 5.1997e-04 - 35s/epoch - 421us/sample
Epoch 5/50
84077/84077 - 35s - loss: 5.0345e-04 - val_loss: 6.0006e-04 - 35s/epoch - 421us/sample
Epoch 6/50
84077/84077 - 35s - loss: 4.9360e-04 - val_loss: 7.1519e-04 - 35s/epoch - 422us/sample
Epoch 7/50
84077/84077 - 35s - loss: 4.8939e-04 - val_loss: 6.1186e-04 - 35s/epoch - 421us/sample
Epoch 8/50
84077/84077 - 35s - loss: 4.8789e-04 - val_loss: 7.7706e-04 - 35s/epoch - 420us/sample
Epoch 9/50
84077/84077 - 35s - loss: 4.8777e-04 - val_loss: 5.7278e-04 - 35s/epoch - 421us/sample
Epoch 10/50
84077/84077 - 35s - loss: 4.8688e-04 - val_loss: 8.0153e-04 - 35s/epoch - 422us/sample
Epoch 11/50
84077/84077 - 35s - loss: 4.8696e-04 - val_loss: 5.7826e-04 - 35s/epoch - 420us/sample
Epoch 12/50
84077/84077 - 35s - loss: 4.8660e-04 - val_loss: 6.9911e-04 - 35s/epoch - 422us/sample
Epoch 13/50
84077/84077 - 35s - loss: 4.8623e-04 - val_loss: 6.0847e-04 - 35s/epoch - 421us/sample
Epoch 14/50
84077/84077 - 35s - loss: 4.8570e-04 - val_loss: 6.1750e-04 - 35s/epoch - 420us/sample
Epoch 15/50
84077/84077 - 35s - loss: 4.8430e-04 - val_loss: 6.7292e-04 - 35s/epoch - 421us/sample
Epoch 16/50
84077/84077 - 35s - loss: 4.8176e-04 - val_loss: 7.6173e-04 - 35s/epoch - 420us/sample
Epoch 17/50
84077/84077 - 35s - loss: 4.8004e-04 - val_loss: 5.8317e-04 - 35s/epoch - 421us/sample
Epoch 18/50
84077/84077 - 35s - loss: 4.7777e-04 - val_loss: 5.2982e-04 - 35s/epoch - 420us/sample
Epoch 19/50
84077/84077 - 35s - loss: 4.7633e-04 - val_loss: 5.7251e-04 - 35s/epoch - 421us/sample
Epoch 20/50
84077/84077 - 35s - loss: 4.7583e-04 - val_loss: 5.0935e-04 - 35s/epoch - 419us/sample
Epoch 21/50
84077/84077 - 35s - loss: 4.7573e-04 - val_loss: 5.9908e-04 - 35s/epoch - 421us/sample
Epoch 22/50
84077/84077 - 35s - loss: 4.7531e-04 - val_loss: 8.6495e-04 - 35s/epoch - 420us/sample
Epoch 23/50
84077/84077 - 35s - loss: 4.7474e-04 - val_loss: 6.4783e-04 - 35s/epoch - 422us/sample
Epoch 24/50
84077/84077 - 35s - loss: 4.7458e-04 - val_loss: 5.3406e-04 - 35s/epoch - 420us/sample
Epoch 25/50
84077/84077 - 35s - loss: 4.7449e-04 - val_loss: 5.4135e-04 - 35s/epoch - 422us/sample
Epoch 26/50
84077/84077 - 35s - loss: 4.7430e-04 - val_loss: 7.0828e-04 - 35s/epoch - 419us/sample
Epoch 27/50
84077/84077 - 35s - loss: 4.7458e-04 - val_loss: 8.8466e-04 - 35s/epoch - 421us/sample
Epoch 28/50
84077/84077 - 35s - loss: 4.7370e-04 - val_loss: 5.5475e-04 - 35s/epoch - 421us/sample
Epoch 29/50
84077/84077 - 35s - loss: 4.7427e-04 - val_loss: 6.1847e-04 - 35s/epoch - 422us/sample
Epoch 30/50
84077/84077 - 35s - loss: 4.7427e-04 - val_loss: 8.0891e-04 - 35s/epoch - 421us/sample
Epoch 31/50
84077/84077 - 35s - loss: 4.7393e-04 - val_loss: 5.9627e-04 - 35s/epoch - 421us/sample
Epoch 32/50
84077/84077 - 35s - loss: 4.7400e-04 - val_loss: 6.2657e-04 - 35s/epoch - 421us/sample
Epoch 33/50
84077/84077 - 35s - loss: 4.7402e-04 - val_loss: 5.3143e-04 - 35s/epoch - 420us/sample
Epoch 34/50
84077/84077 - 35s - loss: 4.7318e-04 - val_loss: 5.7930e-04 - 35s/epoch - 422us/sample
Epoch 35/50
84077/84077 - 35s - loss: 4.7371e-04 - val_loss: 5.6235e-04 - 35s/epoch - 420us/sample
Epoch 36/50
84077/84077 - 35s - loss: 4.7324e-04 - val_loss: 5.9185e-04 - 35s/epoch - 420us/sample
Epoch 37/50
84077/84077 - 35s - loss: 4.7285e-04 - val_loss: 8.6029e-04 - 35s/epoch - 421us/sample
Epoch 38/50
84077/84077 - 35s - loss: 4.7291e-04 - val_loss: 5.7367e-04 - 35s/epoch - 420us/sample
Epoch 39/50
84077/84077 - 35s - loss: 4.7249e-04 - val_loss: 6.8190e-04 - 35s/epoch - 421us/sample
Epoch 40/50
84077/84077 - 35s - loss: 4.7274e-04 - val_loss: 5.8336e-04 - 35s/epoch - 420us/sample
Epoch 41/50
84077/84077 - 35s - loss: 4.7270e-04 - val_loss: 5.6190e-04 - 35s/epoch - 421us/sample
Epoch 42/50
84077/84077 - 35s - loss: 4.7236e-04 - val_loss: 5.7415e-04 - 35s/epoch - 419us/sample
Epoch 43/50
84077/84077 - 35s - loss: 4.7249e-04 - val_loss: 5.3462e-04 - 35s/epoch - 420us/sample
Epoch 44/50
84077/84077 - 35s - loss: 4.7216e-04 - val_loss: 5.8081e-04 - 35s/epoch - 421us/sample
Epoch 45/50
84077/84077 - 35s - loss: 4.7268e-04 - val_loss: 8.6709e-04 - 35s/epoch - 419us/sample
Epoch 46/50
84077/84077 - 35s - loss: 4.7241e-04 - val_loss: 7.3250e-04 - 35s/epoch - 421us/sample
Epoch 47/50
84077/84077 - 35s - loss: 4.7179e-04 - val_loss: 0.0017 - 35s/epoch - 419us/sample
Epoch 48/50
84077/84077 - 35s - loss: 4.7194e-04 - val_loss: 5.5072e-04 - 35s/epoch - 421us/sample
Epoch 49/50
84077/84077 - 35s - loss: 4.7152e-04 - val_loss: 8.2964e-04 - 35s/epoch - 419us/sample
Epoch 50/50
84077/84077 - 35s - loss: 4.7164e-04 - val_loss: 6.8247e-04 - 35s/epoch - 421us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0006824656432097547
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:48:05.880884: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_22/outputlayer/BiasAdd' id:28642 op device:{requested: '', assigned: ''} def:{{{node decoder_model_22/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_22/outputlayer/MatMul, decoder_model_22/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.40035606866150397
cosine 0.3946046365745665
MAE: 0.008417170098539
RMSE: 0.03609823437253129
r2: -0.017956951635342746
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 8, 50, 0.0005, 0.1, 94, 0.00047163666786860216, 0.0006824656432097547, 0.40035606866150397, 0.3946046365745665, 0.008417170098539, 0.03609823437253129, -0.017956951635342746, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7999999999999998 25 0.0005 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1697)         1601968     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_69 (BatchN  (None, 1697)        6788        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_69 (ReLU)                (None, 1697)         0           ['batch_normalization_69[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           159612      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           159612      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1778523     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,706,503
Trainable params: 3,699,527
Non-trainable params: 6,976
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 22:48:18.935062: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_71/beta/Assign' id:29772 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_71/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_71/beta, batch_normalization_71/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:48:28.696047: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_23/mul' id:29963 op device:{requested: '', assigned: ''} def:{{{node loss_23/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_23/mul/x, loss_23/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0057 - val_loss: 0.0011 - 14s/epoch - 162us/sample
Epoch 2/25
84077/84077 - 8s - loss: 0.0038 - val_loss: 0.0010 - 8s/epoch - 90us/sample
Epoch 3/25
84077/84077 - 8s - loss: 9.2082e-04 - val_loss: 0.0026 - 8s/epoch - 90us/sample
Epoch 4/25
84077/84077 - 8s - loss: 8.3016e-04 - val_loss: 6.7478e-04 - 8s/epoch - 90us/sample
Epoch 5/25
84077/84077 - 8s - loss: 6.3115e-04 - val_loss: 6.0213e-04 - 8s/epoch - 90us/sample
Epoch 6/25
84077/84077 - 8s - loss: 5.9451e-04 - val_loss: 4.8752e-04 - 8s/epoch - 90us/sample
Epoch 7/25
84077/84077 - 8s - loss: 4.9804e-04 - val_loss: 4.4053e-04 - 8s/epoch - 90us/sample
Epoch 8/25
84077/84077 - 8s - loss: 4.3185e-04 - val_loss: 3.9863e-04 - 8s/epoch - 90us/sample
Epoch 9/25
84077/84077 - 8s - loss: 4.0289e-04 - val_loss: 3.6567e-04 - 8s/epoch - 90us/sample
Epoch 10/25
84077/84077 - 8s - loss: 3.7999e-04 - val_loss: 3.4751e-04 - 8s/epoch - 90us/sample
Epoch 11/25
84077/84077 - 8s - loss: 3.5776e-04 - val_loss: 3.3334e-04 - 8s/epoch - 91us/sample
Epoch 12/25
84077/84077 - 8s - loss: 3.3927e-04 - val_loss: 3.0512e-04 - 8s/epoch - 90us/sample
Epoch 13/25
84077/84077 - 8s - loss: 3.2613e-04 - val_loss: 3.0126e-04 - 8s/epoch - 90us/sample
Epoch 14/25
84077/84077 - 8s - loss: 3.1400e-04 - val_loss: 2.8957e-04 - 8s/epoch - 90us/sample
Epoch 15/25
84077/84077 - 8s - loss: 3.0598e-04 - val_loss: 2.8977e-04 - 8s/epoch - 90us/sample
Epoch 16/25
84077/84077 - 8s - loss: 2.9897e-04 - val_loss: 2.7890e-04 - 8s/epoch - 90us/sample
Epoch 17/25
84077/84077 - 8s - loss: 2.9163e-04 - val_loss: 2.6596e-04 - 8s/epoch - 90us/sample
Epoch 18/25
84077/84077 - 8s - loss: 2.8370e-04 - val_loss: 2.6297e-04 - 8s/epoch - 90us/sample
Epoch 19/25
84077/84077 - 8s - loss: 2.7658e-04 - val_loss: 2.5288e-04 - 8s/epoch - 91us/sample
Epoch 20/25
84077/84077 - 8s - loss: 2.7121e-04 - val_loss: 2.5111e-04 - 8s/epoch - 90us/sample
Epoch 21/25
84077/84077 - 8s - loss: 2.6772e-04 - val_loss: 2.4910e-04 - 8s/epoch - 90us/sample
Epoch 22/25
84077/84077 - 8s - loss: 2.6316e-04 - val_loss: 2.4448e-04 - 8s/epoch - 90us/sample
Epoch 23/25
84077/84077 - 8s - loss: 2.5978e-04 - val_loss: 2.4086e-04 - 8s/epoch - 90us/sample
Epoch 24/25
84077/84077 - 8s - loss: 2.5629e-04 - val_loss: 2.3683e-04 - 8s/epoch - 90us/sample
Epoch 25/25
84077/84077 - 8s - loss: 2.5344e-04 - val_loss: 2.3743e-04 - 8s/epoch - 90us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00023742854091353467
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:51:32.526850: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_23/outputlayer/BiasAdd' id:29927 op device:{requested: '', assigned: ''} def:{{{node decoder_model_23/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_23/outputlayer/MatMul, decoder_model_23/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.07908665463000955
cosine 0.07810442730354464
MAE: 0.003957049846263813
RMSE: 0.01594710720592974
r2: 0.8014132670815086
RMSE zero-vector: 0.04004287452915337
['1.7999999999999998custom_VAE', 'logcosh', 64, 25, 0.0005, 0.1, 94, 0.0002534427409815276, 0.00023742854091353467, 0.07908665463000955, 0.07810442730354464, 0.003957049846263813, 0.01594710720592974, 0.8014132670815086, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 2
Fitness    = 424.308669689168
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 424.308669689168.
Best solutions :  [array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [424.308669689168, 424.308669689168]
[2.0 85 0.001 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_72 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_72 (ReLU)                (None, 1886)         0           ['batch_normalization_72[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-14 22:51:45.499719: W tensorflow/c/c_api.cc:291] Operation '{name:'training_48/Adam/dense_dec0_24/kernel/m/Assign' id:31801 op device:{requested: '', assigned: ''} def:{{{node training_48/Adam/dense_dec0_24/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_48/Adam/dense_dec0_24/kernel/m, training_48/Adam/dense_dec0_24/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:51:55.163947: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_24/mul' id:31251 op device:{requested: '', assigned: ''} def:{{{node loss_24/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_24/mul/x, loss_24/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0772 - val_loss: 9.5900e-04 - 14s/epoch - 162us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0013 - val_loss: 8.2276e-04 - 8s/epoch - 89us/sample
Epoch 3/85
84077/84077 - 7s - loss: 8.2730e-04 - val_loss: 0.0011 - 7s/epoch - 89us/sample
Epoch 4/85
84077/84077 - 8s - loss: 8.0019e-04 - val_loss: 6.0896e-04 - 8s/epoch - 89us/sample
Epoch 5/85
84077/84077 - 7s - loss: 6.6090e-04 - val_loss: 7.0573e-04 - 7s/epoch - 89us/sample
Epoch 6/85
84077/84077 - 7s - loss: 5.6958e-04 - val_loss: 4.5337e-04 - 7s/epoch - 89us/sample
Epoch 7/85
84077/84077 - 7s - loss: 4.6637e-04 - val_loss: 4.3771e-04 - 7s/epoch - 89us/sample
Epoch 8/85
84077/84077 - 8s - loss: 4.3162e-04 - val_loss: 3.9261e-04 - 8s/epoch - 89us/sample
Epoch 9/85
84077/84077 - 8s - loss: 4.1569e-04 - val_loss: 3.7346e-04 - 8s/epoch - 90us/sample
Epoch 10/85
84077/84077 - 8s - loss: 3.8374e-04 - val_loss: 3.5050e-04 - 8s/epoch - 89us/sample
Epoch 11/85
84077/84077 - 7s - loss: 3.6615e-04 - val_loss: 3.4193e-04 - 7s/epoch - 89us/sample
Epoch 12/85
84077/84077 - 8s - loss: 3.5167e-04 - val_loss: 3.2478e-04 - 8s/epoch - 89us/sample
Epoch 13/85
84077/84077 - 7s - loss: 3.3864e-04 - val_loss: 3.1465e-04 - 7s/epoch - 89us/sample
Epoch 14/85
84077/84077 - 8s - loss: 3.2811e-04 - val_loss: 3.0230e-04 - 8s/epoch - 89us/sample
Epoch 15/85
84077/84077 - 7s - loss: 3.2185e-04 - val_loss: 2.9439e-04 - 7s/epoch - 89us/sample
Epoch 16/85
84077/84077 - 7s - loss: 3.1204e-04 - val_loss: 2.8958e-04 - 7s/epoch - 89us/sample
Epoch 17/85
84077/84077 - 8s - loss: 3.0443e-04 - val_loss: 2.8563e-04 - 8s/epoch - 90us/sample
Epoch 18/85
84077/84077 - 8s - loss: 2.9591e-04 - val_loss: 2.7315e-04 - 8s/epoch - 90us/sample
Epoch 19/85
84077/84077 - 8s - loss: 2.9087e-04 - val_loss: 2.6845e-04 - 8s/epoch - 89us/sample
Epoch 20/85
84077/84077 - 7s - loss: 2.8450e-04 - val_loss: 2.6548e-04 - 7s/epoch - 89us/sample
Epoch 21/85
84077/84077 - 7s - loss: 2.7914e-04 - val_loss: 2.5899e-04 - 7s/epoch - 89us/sample
Epoch 22/85
84077/84077 - 7s - loss: 2.7344e-04 - val_loss: 2.5241e-04 - 7s/epoch - 89us/sample
Epoch 23/85
84077/84077 - 7s - loss: 2.6865e-04 - val_loss: 2.5038e-04 - 7s/epoch - 89us/sample
Epoch 24/85
84077/84077 - 8s - loss: 2.6423e-04 - val_loss: 2.4604e-04 - 8s/epoch - 89us/sample
Epoch 25/85
84077/84077 - 8s - loss: 2.6089e-04 - val_loss: 2.4350e-04 - 8s/epoch - 90us/sample
Epoch 26/85
84077/84077 - 8s - loss: 2.5727e-04 - val_loss: 2.3848e-04 - 8s/epoch - 90us/sample
Epoch 27/85
84077/84077 - 7s - loss: 2.5370e-04 - val_loss: 2.3442e-04 - 7s/epoch - 89us/sample
Epoch 28/85
84077/84077 - 8s - loss: 2.5039e-04 - val_loss: 2.3227e-04 - 8s/epoch - 89us/sample
Epoch 29/85
84077/84077 - 7s - loss: 2.4824e-04 - val_loss: 2.3013e-04 - 7s/epoch - 89us/sample
Epoch 30/85
84077/84077 - 8s - loss: 2.4491e-04 - val_loss: 2.2733e-04 - 8s/epoch - 89us/sample
Epoch 31/85
84077/84077 - 7s - loss: 2.4315e-04 - val_loss: 2.2629e-04 - 7s/epoch - 89us/sample
Epoch 32/85
84077/84077 - 7s - loss: 2.4112e-04 - val_loss: 2.2331e-04 - 7s/epoch - 89us/sample
Epoch 33/85
84077/84077 - 8s - loss: 2.3968e-04 - val_loss: 2.2252e-04 - 8s/epoch - 89us/sample
Epoch 34/85
84077/84077 - 8s - loss: 2.3764e-04 - val_loss: 2.2327e-04 - 8s/epoch - 90us/sample
Epoch 35/85
84077/84077 - 7s - loss: 2.3624e-04 - val_loss: 2.2194e-04 - 7s/epoch - 89us/sample
Epoch 36/85
84077/84077 - 8s - loss: 2.3549e-04 - val_loss: 2.1865e-04 - 8s/epoch - 89us/sample
Epoch 37/85
84077/84077 - 8s - loss: 2.3356e-04 - val_loss: 2.1993e-04 - 8s/epoch - 89us/sample
Epoch 38/85
84077/84077 - 7s - loss: 2.3235e-04 - val_loss: 2.1744e-04 - 7s/epoch - 89us/sample
Epoch 39/85
84077/84077 - 7s - loss: 2.3095e-04 - val_loss: 2.1451e-04 - 7s/epoch - 89us/sample
Epoch 40/85
84077/84077 - 7s - loss: 2.3018e-04 - val_loss: 2.1596e-04 - 7s/epoch - 89us/sample
Epoch 41/85
84077/84077 - 8s - loss: 2.2880e-04 - val_loss: 2.1416e-04 - 8s/epoch - 90us/sample
Epoch 42/85
84077/84077 - 8s - loss: 2.2778e-04 - val_loss: 2.1276e-04 - 8s/epoch - 90us/sample
Epoch 43/85
84077/84077 - 7s - loss: 2.2700e-04 - val_loss: 2.1035e-04 - 7s/epoch - 89us/sample
Epoch 44/85
84077/84077 - 8s - loss: 2.2631e-04 - val_loss: 2.1057e-04 - 8s/epoch - 89us/sample
Epoch 45/85
84077/84077 - 7s - loss: 2.2560e-04 - val_loss: 2.1091e-04 - 7s/epoch - 89us/sample
Epoch 46/85
84077/84077 - 7s - loss: 2.2474e-04 - val_loss: 2.1017e-04 - 7s/epoch - 89us/sample
Epoch 47/85
84077/84077 - 8s - loss: 2.2350e-04 - val_loss: 2.0952e-04 - 8s/epoch - 89us/sample
Epoch 48/85
84077/84077 - 7s - loss: 2.2339e-04 - val_loss: 2.0779e-04 - 7s/epoch - 89us/sample
Epoch 49/85
84077/84077 - 8s - loss: 2.2289e-04 - val_loss: 2.1033e-04 - 8s/epoch - 90us/sample
Epoch 50/85
84077/84077 - 8s - loss: 2.2221e-04 - val_loss: 2.0978e-04 - 8s/epoch - 90us/sample
Epoch 51/85
84077/84077 - 8s - loss: 2.2227e-04 - val_loss: 2.0727e-04 - 8s/epoch - 89us/sample
Epoch 52/85
84077/84077 - 7s - loss: 2.2142e-04 - val_loss: 2.0685e-04 - 7s/epoch - 89us/sample
Epoch 53/85
84077/84077 - 7s - loss: 2.2023e-04 - val_loss: 2.0616e-04 - 7s/epoch - 89us/sample
Epoch 54/85
84077/84077 - 8s - loss: 2.1995e-04 - val_loss: 2.0674e-04 - 8s/epoch - 89us/sample
Epoch 55/85
84077/84077 - 7s - loss: 2.1960e-04 - val_loss: 2.0585e-04 - 7s/epoch - 89us/sample
Epoch 56/85
84077/84077 - 7s - loss: 2.1917e-04 - val_loss: 2.0662e-04 - 7s/epoch - 89us/sample
Epoch 57/85
84077/84077 - 8s - loss: 2.1911e-04 - val_loss: 2.0505e-04 - 8s/epoch - 90us/sample
Epoch 58/85
84077/84077 - 8s - loss: 2.1841e-04 - val_loss: 2.0655e-04 - 8s/epoch - 90us/sample
Epoch 59/85
84077/84077 - 8s - loss: 2.1794e-04 - val_loss: 2.0453e-04 - 8s/epoch - 90us/sample
Epoch 60/85
84077/84077 - 8s - loss: 2.1766e-04 - val_loss: 2.0433e-04 - 8s/epoch - 89us/sample
Epoch 61/85
84077/84077 - 8s - loss: 2.1748e-04 - val_loss: 2.0369e-04 - 8s/epoch - 89us/sample
Epoch 62/85
84077/84077 - 8s - loss: 2.1706e-04 - val_loss: 2.0437e-04 - 8s/epoch - 89us/sample
Epoch 63/85
84077/84077 - 7s - loss: 2.1681e-04 - val_loss: 2.0390e-04 - 7s/epoch - 89us/sample
Epoch 64/85
84077/84077 - 7s - loss: 2.1629e-04 - val_loss: 2.0427e-04 - 7s/epoch - 89us/sample
Epoch 65/85
84077/84077 - 7s - loss: 2.1566e-04 - val_loss: 2.0365e-04 - 7s/epoch - 89us/sample
Epoch 66/85
84077/84077 - 8s - loss: 2.1596e-04 - val_loss: 2.0332e-04 - 8s/epoch - 91us/sample
Epoch 67/85
84077/84077 - 8s - loss: 2.1578e-04 - val_loss: 2.0462e-04 - 8s/epoch - 89us/sample
Epoch 68/85
84077/84077 - 8s - loss: 2.1558e-04 - val_loss: 2.0445e-04 - 8s/epoch - 89us/sample
Epoch 69/85
84077/84077 - 7s - loss: 2.1518e-04 - val_loss: 2.0204e-04 - 7s/epoch - 89us/sample
Epoch 70/85
84077/84077 - 7s - loss: 2.1452e-04 - val_loss: 2.0261e-04 - 7s/epoch - 89us/sample
Epoch 71/85
84077/84077 - 7s - loss: 2.1427e-04 - val_loss: 2.0273e-04 - 7s/epoch - 89us/sample
Epoch 72/85
84077/84077 - 7s - loss: 2.1364e-04 - val_loss: 2.0248e-04 - 7s/epoch - 89us/sample
Epoch 73/85
84077/84077 - 8s - loss: 2.1424e-04 - val_loss: 2.0311e-04 - 8s/epoch - 90us/sample
Epoch 74/85
84077/84077 - 8s - loss: 2.1365e-04 - val_loss: 2.0109e-04 - 8s/epoch - 90us/sample
Epoch 75/85
84077/84077 - 8s - loss: 2.1340e-04 - val_loss: 2.0125e-04 - 8s/epoch - 90us/sample
Epoch 76/85
84077/84077 - 8s - loss: 2.1272e-04 - val_loss: 2.0087e-04 - 8s/epoch - 89us/sample
Epoch 77/85
84077/84077 - 7s - loss: 2.1287e-04 - val_loss: 2.0083e-04 - 7s/epoch - 89us/sample
Epoch 78/85
84077/84077 - 8s - loss: 2.1262e-04 - val_loss: 2.0136e-04 - 8s/epoch - 89us/sample
Epoch 79/85
84077/84077 - 7s - loss: 2.1228e-04 - val_loss: 2.0010e-04 - 7s/epoch - 89us/sample
Epoch 80/85
84077/84077 - 7s - loss: 2.1240e-04 - val_loss: 2.0115e-04 - 7s/epoch - 89us/sample
Epoch 81/85
84077/84077 - 8s - loss: 2.1225e-04 - val_loss: 2.0038e-04 - 8s/epoch - 90us/sample
Epoch 82/85
84077/84077 - 8s - loss: 2.1150e-04 - val_loss: 2.0109e-04 - 8s/epoch - 90us/sample
Epoch 83/85
84077/84077 - 7s - loss: 2.1125e-04 - val_loss: 2.0034e-04 - 7s/epoch - 89us/sample
Epoch 84/85
84077/84077 - 8s - loss: 2.1107e-04 - val_loss: 1.9905e-04 - 8s/epoch - 89us/sample
Epoch 85/85
84077/84077 - 7s - loss: 2.1153e-04 - val_loss: 1.9972e-04 - 7s/epoch - 89us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00019971617740864367
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:02:28.008067: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_24/outputlayer/BiasAdd' id:31215 op device:{requested: '', assigned: ''} def:{{{node decoder_model_24/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_24/outputlayer/MatMul, decoder_model_24/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.057185684978696465
cosine 0.05648267122719815
MAE: 0.0029012988837445693
RMSE: 0.013154345313807462
r2: 0.8649221623146808
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 85, 0.001, 0.1, 94, 0.0002115268469854804, 0.00019971617740864367, 0.057185684978696465, 0.05648267122719815, 0.0029012988837445693, 0.013154345313807462, 0.8649221623146808, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4 45 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2263)         2136272     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_75 (BatchN  (None, 2263)        9052        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_75 (ReLU)                (None, 2263)         0           ['batch_normalization_75[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           212816      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           212816      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2368295     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,939,251
Trainable params: 4,930,011
Non-trainable params: 9,240
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/45
2023-02-14 23:02:41.485390: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_76/moving_variance/Assign' id:32271 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_76/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_76/moving_variance, batch_normalization_76/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:02:51.472136: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_25/mul' id:32536 op device:{requested: '', assigned: ''} def:{{{node loss_25/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul/x, loss_25/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0038 - val_loss: 8.9457e-04 - 14s/epoch - 168us/sample
Epoch 2/45
84077/84077 - 8s - loss: 0.0016 - val_loss: 8.3099e-04 - 8s/epoch - 93us/sample
Epoch 3/45
84077/84077 - 8s - loss: 0.0016 - val_loss: 9.1372e-04 - 8s/epoch - 92us/sample
Epoch 4/45
84077/84077 - 8s - loss: 0.0032 - val_loss: 0.0015 - 8s/epoch - 92us/sample
Epoch 5/45
84077/84077 - 8s - loss: 7.8342e-04 - val_loss: 5.6649e-04 - 8s/epoch - 92us/sample
Epoch 6/45
84077/84077 - 8s - loss: 5.5493e-04 - val_loss: 5.3269e-04 - 8s/epoch - 92us/sample
Epoch 7/45
84077/84077 - 8s - loss: 5.0862e-04 - val_loss: 4.8692e-04 - 8s/epoch - 92us/sample
Epoch 8/45
84077/84077 - 8s - loss: 5.0266e-04 - val_loss: 4.5929e-04 - 8s/epoch - 92us/sample
Epoch 9/45
84077/84077 - 8s - loss: 4.7422e-04 - val_loss: 4.4461e-04 - 8s/epoch - 92us/sample
Epoch 10/45
84077/84077 - 8s - loss: 4.5878e-04 - val_loss: 4.3860e-04 - 8s/epoch - 92us/sample
Epoch 11/45
84077/84077 - 8s - loss: 4.5570e-04 - val_loss: 4.1630e-04 - 8s/epoch - 92us/sample
Epoch 12/45
84077/84077 - 8s - loss: 4.2744e-04 - val_loss: 4.0008e-04 - 8s/epoch - 92us/sample
Epoch 13/45
84077/84077 - 8s - loss: 4.1528e-04 - val_loss: 4.0637e-04 - 8s/epoch - 92us/sample
Epoch 14/45
84077/84077 - 8s - loss: 3.9703e-04 - val_loss: 3.7107e-04 - 8s/epoch - 92us/sample
Epoch 15/45
84077/84077 - 8s - loss: 3.8957e-04 - val_loss: 3.5662e-04 - 8s/epoch - 92us/sample
Epoch 16/45
84077/84077 - 8s - loss: 3.6982e-04 - val_loss: 3.4446e-04 - 8s/epoch - 92us/sample
Epoch 17/45
84077/84077 - 8s - loss: 3.6687e-04 - val_loss: 3.3893e-04 - 8s/epoch - 92us/sample
Epoch 18/45
84077/84077 - 8s - loss: 3.5299e-04 - val_loss: 3.3162e-04 - 8s/epoch - 92us/sample
Epoch 19/45
84077/84077 - 8s - loss: 3.4604e-04 - val_loss: 3.2405e-04 - 8s/epoch - 92us/sample
Epoch 20/45
84077/84077 - 8s - loss: 3.4136e-04 - val_loss: 3.1759e-04 - 8s/epoch - 93us/sample
Epoch 21/45
84077/84077 - 8s - loss: 3.3615e-04 - val_loss: 3.1554e-04 - 8s/epoch - 92us/sample
Epoch 22/45
84077/84077 - 8s - loss: 3.3255e-04 - val_loss: 3.1003e-04 - 8s/epoch - 92us/sample
Epoch 23/45
84077/84077 - 8s - loss: 3.2923e-04 - val_loss: 3.0757e-04 - 8s/epoch - 92us/sample
Epoch 24/45
84077/84077 - 8s - loss: 3.2602e-04 - val_loss: 3.0425e-04 - 8s/epoch - 92us/sample
Epoch 25/45
84077/84077 - 8s - loss: 3.2322e-04 - val_loss: 3.0042e-04 - 8s/epoch - 92us/sample
Epoch 26/45
84077/84077 - 8s - loss: 3.2014e-04 - val_loss: 2.9916e-04 - 8s/epoch - 92us/sample
Epoch 27/45
84077/84077 - 8s - loss: 3.1877e-04 - val_loss: 2.9733e-04 - 8s/epoch - 92us/sample
Epoch 28/45
84077/84077 - 8s - loss: 3.1650e-04 - val_loss: 2.9659e-04 - 8s/epoch - 93us/sample
Epoch 29/45
84077/84077 - 8s - loss: 3.1427e-04 - val_loss: 2.9427e-04 - 8s/epoch - 92us/sample
Epoch 30/45
84077/84077 - 8s - loss: 3.1238e-04 - val_loss: 2.9213e-04 - 8s/epoch - 92us/sample
Epoch 31/45
84077/84077 - 8s - loss: 3.0977e-04 - val_loss: 2.9099e-04 - 8s/epoch - 93us/sample
Epoch 32/45
84077/84077 - 8s - loss: 3.0801e-04 - val_loss: 2.8938e-04 - 8s/epoch - 92us/sample
Epoch 33/45
84077/84077 - 8s - loss: 3.0631e-04 - val_loss: 2.8610e-04 - 8s/epoch - 92us/sample
Epoch 34/45
84077/84077 - 8s - loss: 3.0495e-04 - val_loss: 2.8445e-04 - 8s/epoch - 92us/sample
Epoch 35/45
84077/84077 - 8s - loss: 3.0369e-04 - val_loss: 2.8475e-04 - 8s/epoch - 92us/sample
Epoch 36/45
84077/84077 - 8s - loss: 3.0240e-04 - val_loss: 2.8336e-04 - 8s/epoch - 93us/sample
Epoch 37/45
84077/84077 - 8s - loss: 3.0088e-04 - val_loss: 2.8307e-04 - 8s/epoch - 93us/sample
Epoch 38/45
84077/84077 - 8s - loss: 2.9967e-04 - val_loss: 2.8210e-04 - 8s/epoch - 92us/sample
Epoch 39/45
84077/84077 - 8s - loss: 2.9911e-04 - val_loss: 2.8239e-04 - 8s/epoch - 92us/sample
Epoch 40/45
84077/84077 - 8s - loss: 2.9793e-04 - val_loss: 2.8033e-04 - 8s/epoch - 92us/sample
Epoch 41/45
84077/84077 - 8s - loss: 2.9717e-04 - val_loss: 2.7912e-04 - 8s/epoch - 92us/sample
Epoch 42/45
84077/84077 - 8s - loss: 2.9597e-04 - val_loss: 2.7797e-04 - 8s/epoch - 92us/sample
Epoch 43/45
84077/84077 - 8s - loss: 2.9496e-04 - val_loss: 2.7832e-04 - 8s/epoch - 92us/sample
Epoch 44/45
84077/84077 - 8s - loss: 2.9434e-04 - val_loss: 2.7670e-04 - 8s/epoch - 93us/sample
Epoch 45/45
84077/84077 - 8s - loss: 2.9394e-04 - val_loss: 2.7643e-04 - 8s/epoch - 93us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002764274853731188
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:08:34.672655: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_25/outputlayer/BiasAdd' id:32500 op device:{requested: '', assigned: ''} def:{{{node decoder_model_25/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_25/outputlayer/MatMul, decoder_model_25/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.11391787618903647
cosine 0.11243483412334859
MAE: 0.0036731355570175247
RMSE: 0.01934702993785156
r2: 0.707639418177454
RMSE zero-vector: 0.04004287452915337
['2.4custom_VAE', 'logcosh', 64, 45, 0.0005, 0.1, 94, 0.00029394272758662897, 0.0002764274853731188, 0.11391787618903647, 0.11243483412334859, 0.0036731355570175247, 0.01934702993785156, 0.707639418177454, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4 50 0.0005 16 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2263)         2136272     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_78 (BatchN  (None, 2263)        9052        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_78 (ReLU)                (None, 2263)         0           ['batch_normalization_78[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           212816      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           212816      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2368295     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,939,251
Trainable params: 4,930,011
Non-trainable params: 9,240
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 23:08:47.995959: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_80/moving_variance/Assign' id:33644 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_80/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_80/moving_variance, batch_normalization_80/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:09:09.825746: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_26/mul' id:33821 op device:{requested: '', assigned: ''} def:{{{node loss_26/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_26/mul/x, loss_26/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0031 - val_loss: 0.0014 - 27s/epoch - 318us/sample
Epoch 2/50
84077/84077 - 20s - loss: 7.1509e-04 - val_loss: 5.4967e-04 - 20s/epoch - 241us/sample
Epoch 3/50
84077/84077 - 20s - loss: 5.3761e-04 - val_loss: 7.2079e-04 - 20s/epoch - 241us/sample
Epoch 4/50
84077/84077 - 20s - loss: 4.7795e-04 - val_loss: 5.3660e-04 - 20s/epoch - 240us/sample
Epoch 5/50
84077/84077 - 20s - loss: 4.5724e-04 - val_loss: 8.1224e-04 - 20s/epoch - 240us/sample
Epoch 6/50
84077/84077 - 20s - loss: 4.4787e-04 - val_loss: 7.9219e-04 - 20s/epoch - 241us/sample
Epoch 7/50
84077/84077 - 20s - loss: 4.4344e-04 - val_loss: 7.9266e-04 - 20s/epoch - 242us/sample
Epoch 8/50
84077/84077 - 20s - loss: 4.4144e-04 - val_loss: 7.2288e-04 - 20s/epoch - 241us/sample
Epoch 9/50
84077/84077 - 20s - loss: 4.3975e-04 - val_loss: 9.5952e-04 - 20s/epoch - 241us/sample
Epoch 10/50
84077/84077 - 20s - loss: 4.3799e-04 - val_loss: 7.9314e-04 - 20s/epoch - 241us/sample
Epoch 11/50
84077/84077 - 20s - loss: 4.3640e-04 - val_loss: 9.0450e-04 - 20s/epoch - 241us/sample
Epoch 12/50
84077/84077 - 20s - loss: 4.3600e-04 - val_loss: 8.9286e-04 - 20s/epoch - 240us/sample
Epoch 13/50
84077/84077 - 20s - loss: 4.3542e-04 - val_loss: 9.3502e-04 - 20s/epoch - 241us/sample
Epoch 14/50
84077/84077 - 20s - loss: 4.3473e-04 - val_loss: 8.9241e-04 - 20s/epoch - 242us/sample
Epoch 15/50
84077/84077 - 20s - loss: 4.3396e-04 - val_loss: 8.9512e-04 - 20s/epoch - 240us/sample
Epoch 16/50
84077/84077 - 20s - loss: 4.3361e-04 - val_loss: 6.9920e-04 - 20s/epoch - 240us/sample
Epoch 17/50
84077/84077 - 20s - loss: 4.3358e-04 - val_loss: 5.8761e-04 - 20s/epoch - 241us/sample
Epoch 18/50
84077/84077 - 20s - loss: 4.3336e-04 - val_loss: 5.4823e-04 - 20s/epoch - 242us/sample
Epoch 19/50
84077/84077 - 20s - loss: 4.3289e-04 - val_loss: 6.2327e-04 - 20s/epoch - 240us/sample
Epoch 20/50
84077/84077 - 20s - loss: 4.3285e-04 - val_loss: 5.8782e-04 - 20s/epoch - 240us/sample
Epoch 21/50
84077/84077 - 20s - loss: 4.3256e-04 - val_loss: 6.1984e-04 - 20s/epoch - 242us/sample
Epoch 22/50
84077/84077 - 20s - loss: 4.3303e-04 - val_loss: 5.3405e-04 - 20s/epoch - 241us/sample
Epoch 23/50
84077/84077 - 20s - loss: 4.3225e-04 - val_loss: 5.7474e-04 - 20s/epoch - 241us/sample
Epoch 24/50
84077/84077 - 20s - loss: 4.3243e-04 - val_loss: 5.1787e-04 - 20s/epoch - 242us/sample
Epoch 25/50
84077/84077 - 20s - loss: 4.3224e-04 - val_loss: 5.2458e-04 - 20s/epoch - 240us/sample
Epoch 26/50
84077/84077 - 20s - loss: 4.3205e-04 - val_loss: 5.1107e-04 - 20s/epoch - 240us/sample
Epoch 27/50
84077/84077 - 20s - loss: 4.3191e-04 - val_loss: 5.7561e-04 - 20s/epoch - 240us/sample
Epoch 28/50
84077/84077 - 20s - loss: 4.3202e-04 - val_loss: 5.0977e-04 - 20s/epoch - 241us/sample
Epoch 29/50
84077/84077 - 20s - loss: 4.3157e-04 - val_loss: 5.0917e-04 - 20s/epoch - 240us/sample
Epoch 30/50
84077/84077 - 20s - loss: 4.3193e-04 - val_loss: 4.8643e-04 - 20s/epoch - 240us/sample
Epoch 31/50
84077/84077 - 20s - loss: 4.3123e-04 - val_loss: 5.3724e-04 - 20s/epoch - 241us/sample
Epoch 32/50
84077/84077 - 20s - loss: 4.3160e-04 - val_loss: 5.4575e-04 - 20s/epoch - 241us/sample
Epoch 33/50
84077/84077 - 20s - loss: 4.3118e-04 - val_loss: 5.2035e-04 - 20s/epoch - 241us/sample
Epoch 34/50
84077/84077 - 20s - loss: 4.3147e-04 - val_loss: 4.9366e-04 - 20s/epoch - 241us/sample
Epoch 35/50
84077/84077 - 20s - loss: 4.3125e-04 - val_loss: 4.7941e-04 - 20s/epoch - 242us/sample
Epoch 36/50
84077/84077 - 20s - loss: 4.3150e-04 - val_loss: 5.1247e-04 - 20s/epoch - 241us/sample
Epoch 37/50
84077/84077 - 20s - loss: 4.3096e-04 - val_loss: 4.8166e-04 - 20s/epoch - 241us/sample
Epoch 38/50
84077/84077 - 20s - loss: 4.3112e-04 - val_loss: 4.7482e-04 - 20s/epoch - 242us/sample
Epoch 39/50
84077/84077 - 20s - loss: 4.3089e-04 - val_loss: 4.5968e-04 - 20s/epoch - 241us/sample
Epoch 40/50
84077/84077 - 20s - loss: 4.3041e-04 - val_loss: 4.7452e-04 - 20s/epoch - 240us/sample
Epoch 41/50
84077/84077 - 20s - loss: 4.3045e-04 - val_loss: 4.6771e-04 - 20s/epoch - 241us/sample
Epoch 42/50
84077/84077 - 20s - loss: 4.3080e-04 - val_loss: 4.4883e-04 - 20s/epoch - 241us/sample
Epoch 43/50
84077/84077 - 20s - loss: 4.3091e-04 - val_loss: 4.5437e-04 - 20s/epoch - 240us/sample
Epoch 44/50
84077/84077 - 20s - loss: 4.3053e-04 - val_loss: 4.6753e-04 - 20s/epoch - 240us/sample
Epoch 45/50
84077/84077 - 20s - loss: 4.3068e-04 - val_loss: 4.5711e-04 - 20s/epoch - 242us/sample
Epoch 46/50
84077/84077 - 20s - loss: 4.3063e-04 - val_loss: 4.6770e-04 - 20s/epoch - 241us/sample
Epoch 47/50
84077/84077 - 20s - loss: 4.3068e-04 - val_loss: 4.6393e-04 - 20s/epoch - 240us/sample
Epoch 48/50
84077/84077 - 20s - loss: 4.3054e-04 - val_loss: 4.5890e-04 - 20s/epoch - 242us/sample
Epoch 49/50
84077/84077 - 20s - loss: 4.3057e-04 - val_loss: 4.5688e-04 - 20s/epoch - 240us/sample
Epoch 50/50
84077/84077 - 20s - loss: 4.3063e-04 - val_loss: 4.3705e-04 - 20s/epoch - 240us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00043705292077223846
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:25:45.269409: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_26/outputlayer/BiasAdd' id:33785 op device:{requested: '', assigned: ''} def:{{{node decoder_model_26/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_26/outputlayer/MatMul, decoder_model_26/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.244182565381968
cosine 0.24059244031246668
MAE: 0.005568813008257535
RMSE: 0.028235908821577543
r2: 0.3771854913641465
RMSE zero-vector: 0.04004287452915337
['2.4custom_VAE', 'logcosh', 16, 50, 0.0005, 0.1, 94, 0.0004306290249886677, 0.00043705292077223846, 0.244182565381968, 0.24059244031246668, 0.005568813008257535, 0.028235908821577543, 0.3771854913641465, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 45 0.0005 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_81 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_81 (ReLU)                (None, 2357)         0           ['batch_normalization_81[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/45
2023-02-14 23:25:58.864437: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_82/beta/Assign' id:34831 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_82/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_82/beta, batch_normalization_82/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:26:08.961827: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_27/mul' id:35099 op device:{requested: '', assigned: ''} def:{{{node loss_27/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_27/mul/x, loss_27/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0101 - val_loss: 0.0041 - 14s/epoch - 172us/sample
Epoch 2/45
84077/84077 - 8s - loss: 0.0021 - val_loss: 0.0049 - 8s/epoch - 92us/sample
Epoch 3/45
84077/84077 - 8s - loss: 32.4212 - val_loss: 0.0015 - 8s/epoch - 91us/sample
Epoch 4/45
84077/84077 - 8s - loss: 0.0043 - val_loss: 0.0012 - 8s/epoch - 91us/sample
Epoch 5/45
84077/84077 - 8s - loss: 0.0796 - val_loss: 0.0021 - 8s/epoch - 91us/sample
Epoch 6/45
84077/84077 - 8s - loss: 0.0013 - val_loss: 0.0011 - 8s/epoch - 91us/sample
Epoch 7/45
84077/84077 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 91us/sample
Epoch 8/45
84077/84077 - 8s - loss: 0.0013 - val_loss: 9.4696e-04 - 8s/epoch - 92us/sample
Epoch 9/45
84077/84077 - 8s - loss: 0.0012 - val_loss: 8.2721e-04 - 8s/epoch - 92us/sample
Epoch 10/45
84077/84077 - 8s - loss: 8.8689e-04 - val_loss: 7.5428e-04 - 8s/epoch - 91us/sample
Epoch 11/45
84077/84077 - 8s - loss: 8.3851e-04 - val_loss: 6.9618e-04 - 8s/epoch - 91us/sample
Epoch 12/45
84077/84077 - 8s - loss: 8.0353e-04 - val_loss: 6.6190e-04 - 8s/epoch - 91us/sample
Epoch 13/45
84077/84077 - 8s - loss: 6.8714e-04 - val_loss: 6.2337e-04 - 8s/epoch - 91us/sample
Epoch 14/45
84077/84077 - 8s - loss: 6.7367e-04 - val_loss: 7.0437e-04 - 8s/epoch - 91us/sample
Epoch 15/45
84077/84077 - 8s - loss: 6.1657e-04 - val_loss: 5.5197e-04 - 8s/epoch - 92us/sample
Epoch 16/45
84077/84077 - 8s - loss: 5.9029e-04 - val_loss: 5.4003e-04 - 8s/epoch - 92us/sample
Epoch 17/45
84077/84077 - 8s - loss: 5.6179e-04 - val_loss: 5.2138e-04 - 8s/epoch - 93us/sample
Epoch 18/45
84077/84077 - 8s - loss: 5.8952e-04 - val_loss: 5.0592e-04 - 8s/epoch - 91us/sample
Epoch 19/45
84077/84077 - 8s - loss: 5.3106e-04 - val_loss: 4.8365e-04 - 8s/epoch - 92us/sample
Epoch 20/45
84077/84077 - 8s - loss: 5.1582e-04 - val_loss: 4.8085e-04 - 8s/epoch - 91us/sample
Epoch 21/45
84077/84077 - 8s - loss: 5.0150e-04 - val_loss: 4.7313e-04 - 8s/epoch - 91us/sample
Epoch 22/45
84077/84077 - 8s - loss: 4.8947e-04 - val_loss: 4.6593e-04 - 8s/epoch - 91us/sample
Epoch 23/45
84077/84077 - 8s - loss: 4.7929e-04 - val_loss: 4.4305e-04 - 8s/epoch - 91us/sample
Epoch 24/45
84077/84077 - 8s - loss: 4.6518e-04 - val_loss: 4.1794e-04 - 8s/epoch - 91us/sample
Epoch 25/45
84077/84077 - 8s - loss: 4.4958e-04 - val_loss: 4.0822e-04 - 8s/epoch - 92us/sample
Epoch 26/45
84077/84077 - 8s - loss: 4.3637e-04 - val_loss: 4.1338e-04 - 8s/epoch - 92us/sample
Epoch 27/45
84077/84077 - 8s - loss: 4.2352e-04 - val_loss: 3.8677e-04 - 8s/epoch - 92us/sample
Epoch 28/45
84077/84077 - 8s - loss: 4.1290e-04 - val_loss: 3.7373e-04 - 8s/epoch - 92us/sample
Epoch 29/45
84077/84077 - 8s - loss: 4.0069e-04 - val_loss: 3.6883e-04 - 8s/epoch - 91us/sample
Epoch 30/45
84077/84077 - 8s - loss: 3.9145e-04 - val_loss: 3.5688e-04 - 8s/epoch - 91us/sample
Epoch 31/45
84077/84077 - 8s - loss: 3.8226e-04 - val_loss: 3.5097e-04 - 8s/epoch - 91us/sample
Epoch 32/45
84077/84077 - 8s - loss: 3.7503e-04 - val_loss: 3.4069e-04 - 8s/epoch - 92us/sample
Epoch 33/45
84077/84077 - 8s - loss: 3.6936e-04 - val_loss: 3.3636e-04 - 8s/epoch - 92us/sample
Epoch 34/45
84077/84077 - 8s - loss: 3.6286e-04 - val_loss: 3.3127e-04 - 8s/epoch - 92us/sample
Epoch 35/45
84077/84077 - 8s - loss: 3.5902e-04 - val_loss: 3.2895e-04 - 8s/epoch - 92us/sample
Epoch 36/45
84077/84077 - 8s - loss: 3.5375e-04 - val_loss: 3.2397e-04 - 8s/epoch - 91us/sample
Epoch 37/45
84077/84077 - 8s - loss: 3.5025e-04 - val_loss: 3.2276e-04 - 8s/epoch - 91us/sample
Epoch 38/45
84077/84077 - 8s - loss: 3.4770e-04 - val_loss: 3.1875e-04 - 8s/epoch - 91us/sample
Epoch 39/45
84077/84077 - 8s - loss: 3.4332e-04 - val_loss: 3.1563e-04 - 8s/epoch - 92us/sample
Epoch 40/45
84077/84077 - 8s - loss: 3.4290e-04 - val_loss: 3.1620e-04 - 8s/epoch - 92us/sample
Epoch 41/45
84077/84077 - 8s - loss: 3.4059e-04 - val_loss: 3.1302e-04 - 8s/epoch - 93us/sample
Epoch 42/45
84077/84077 - 8s - loss: 3.3756e-04 - val_loss: 3.1094e-04 - 8s/epoch - 92us/sample
Epoch 43/45
84077/84077 - 8s - loss: 3.3557e-04 - val_loss: 3.0948e-04 - 8s/epoch - 92us/sample
Epoch 44/45
84077/84077 - 8s - loss: 3.3403e-04 - val_loss: 3.0797e-04 - 8s/epoch - 92us/sample
Epoch 45/45
84077/84077 - 8s - loss: 3.3266e-04 - val_loss: 3.1009e-04 - 8s/epoch - 92us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0003100911154308751
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:31:50.260321: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_27/outputlayer/BiasAdd' id:35070 op device:{requested: '', assigned: ''} def:{{{node decoder_model_27/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_27/outputlayer/MatMul, decoder_model_27/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.044350986440359334
cosine 0.043812422717279045
MAE: 0.003053405342214164
RMSE: 0.01228469826830766
r2: 0.8822503267367243
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'mse', 64, 45, 0.0005, 0.1, 94, 0.0003326615866011659, 0.0003100911154308751, 0.044350986440359334, 0.043812422717279045, 0.003053405342214164, 0.01228469826830766, 0.8822503267367243, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 45 0.0005 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_84 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_84 (ReLU)                (None, 1980)         0           ['batch_normalization_84[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           186214      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           186214      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2073409     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,322,877
Trainable params: 4,314,769
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/45
2023-02-14 23:32:04.764647: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_28/bias/Assign' id:36152 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_28/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_28/bias, dense_dec0_28/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:32:15.266324: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_28/mul' id:36361 op device:{requested: '', assigned: ''} def:{{{node loss_28/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_28/mul/x, loss_28/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0077 - val_loss: 0.0011 - 15s/epoch - 182us/sample
Epoch 2/45
84077/84077 - 8s - loss: 0.0017 - val_loss: 8.2468e-04 - 8s/epoch - 92us/sample
Epoch 3/45
84077/84077 - 8s - loss: 8.9084e-04 - val_loss: 0.0111 - 8s/epoch - 92us/sample
Epoch 4/45
84077/84077 - 8s - loss: 0.0079 - val_loss: 8.0608e-04 - 8s/epoch - 92us/sample
Epoch 5/45
84077/84077 - 8s - loss: 7.2199e-04 - val_loss: 7.4681e-04 - 8s/epoch - 92us/sample
Epoch 6/45
84077/84077 - 8s - loss: 5.7982e-04 - val_loss: 5.2344e-04 - 8s/epoch - 92us/sample
Epoch 7/45
84077/84077 - 8s - loss: 5.1979e-04 - val_loss: 4.5948e-04 - 8s/epoch - 92us/sample
Epoch 8/45
84077/84077 - 8s - loss: 0.0024 - val_loss: 4.4979e-04 - 8s/epoch - 92us/sample
Epoch 9/45
84077/84077 - 8s - loss: 4.9087e-04 - val_loss: 4.3321e-04 - 8s/epoch - 92us/sample
Epoch 10/45
84077/84077 - 8s - loss: 4.2473e-04 - val_loss: 4.9715e-04 - 8s/epoch - 92us/sample
Epoch 11/45
84077/84077 - 8s - loss: 4.1163e-04 - val_loss: 3.6109e-04 - 8s/epoch - 92us/sample
Epoch 12/45
84077/84077 - 8s - loss: 3.7910e-04 - val_loss: 3.9783e-04 - 8s/epoch - 93us/sample
Epoch 13/45
84077/84077 - 8s - loss: 0.0364 - val_loss: 5.7803e-04 - 8s/epoch - 92us/sample
Epoch 14/45
84077/84077 - 8s - loss: 4.4932e-04 - val_loss: 4.0637e-04 - 8s/epoch - 92us/sample
Epoch 15/45
84077/84077 - 8s - loss: 4.3462e-04 - val_loss: 5.6647e-04 - 8s/epoch - 92us/sample
Epoch 16/45
84077/84077 - 8s - loss: 4.1552e-04 - val_loss: 3.6956e-04 - 8s/epoch - 92us/sample
Epoch 17/45
84077/84077 - 8s - loss: 0.0016 - val_loss: 0.0059 - 8s/epoch - 91us/sample
Epoch 18/45
84077/84077 - 8s - loss: 4.3573e-04 - val_loss: 4.0926e-04 - 8s/epoch - 92us/sample
Epoch 19/45
84077/84077 - 8s - loss: 0.0025 - val_loss: 6.6346e-04 - 8s/epoch - 92us/sample
Epoch 20/45
84077/84077 - 8s - loss: 4.3091e-04 - val_loss: 3.8699e-04 - 8s/epoch - 93us/sample
Epoch 21/45
84077/84077 - 8s - loss: 5.7855e-04 - val_loss: 4.7477e-04 - 8s/epoch - 92us/sample
Epoch 22/45
84077/84077 - 8s - loss: 4.8043e-04 - val_loss: 4.4079e-04 - 8s/epoch - 92us/sample
Epoch 23/45
84077/84077 - 8s - loss: 4.7052e-04 - val_loss: 4.4105e-04 - 8s/epoch - 92us/sample
Epoch 24/45
84077/84077 - 8s - loss: 0.0265 - val_loss: 8.4882e-04 - 8s/epoch - 92us/sample
Epoch 25/45
84077/84077 - 8s - loss: 6.1452e-04 - val_loss: 5.6468e-04 - 8s/epoch - 92us/sample
Epoch 26/45
84077/84077 - 8s - loss: 5.7212e-04 - val_loss: 5.1784e-04 - 8s/epoch - 92us/sample
Epoch 27/45
84077/84077 - 8s - loss: 5.2490e-04 - val_loss: 4.6209e-04 - 8s/epoch - 92us/sample
Epoch 28/45
84077/84077 - 8s - loss: 0.0533 - val_loss: 6.7858e-04 - 8s/epoch - 93us/sample
Epoch 29/45
84077/84077 - 8s - loss: 2096.9567 - val_loss: 67.8685 - 8s/epoch - 92us/sample
Epoch 30/45
84077/84077 - 8s - loss: 0.0223 - val_loss: 0.0011 - 8s/epoch - 92us/sample
Epoch 31/45
84077/84077 - 8s - loss: 8.3733e-04 - val_loss: 8.0257e-04 - 8s/epoch - 92us/sample
Epoch 32/45
84077/84077 - 8s - loss: 8.2336e-04 - val_loss: 7.5940e-04 - 8s/epoch - 92us/sample
Epoch 33/45
84077/84077 - 8s - loss: 7.9124e-04 - val_loss: 7.2363e-04 - 8s/epoch - 92us/sample
Epoch 34/45
84077/84077 - 8s - loss: 7.4868e-04 - val_loss: 6.8800e-04 - 8s/epoch - 92us/sample
Epoch 35/45
84077/84077 - 8s - loss: 7.0888e-04 - val_loss: 6.8106e-04 - 8s/epoch - 92us/sample
Epoch 36/45
84077/84077 - 8s - loss: 0.0863 - val_loss: 0.0019 - 8s/epoch - 93us/sample
Epoch 37/45
84077/84077 - 8s - loss: 2894.9912 - val_loss: 0.0011 - 8s/epoch - 92us/sample
Epoch 38/45
84077/84077 - 8s - loss: 9.4256e-04 - val_loss: 8.0582e-04 - 8s/epoch - 93us/sample
Epoch 39/45
84077/84077 - 8s - loss: 0.0018 - val_loss: 0.3445 - 8s/epoch - 92us/sample
Epoch 40/45
84077/84077 - 8s - loss: 0.1767 - val_loss: 0.0012 - 8s/epoch - 92us/sample
Epoch 41/45
84077/84077 - 8s - loss: 58.9375 - val_loss: 0.0013 - 8s/epoch - 92us/sample
Epoch 42/45
84077/84077 - 8s - loss: 0.0011 - val_loss: 0.0013 - 8s/epoch - 92us/sample
Epoch 43/45
84077/84077 - 8s - loss: 0.0304 - val_loss: 0.0015 - 8s/epoch - 92us/sample
Epoch 44/45
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0014 - 8s/epoch - 93us/sample
Epoch 45/45
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0013 - 8s/epoch - 92us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0012899061829530403
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:37:58.242669: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_28/outputlayer/BiasAdd' id:36325 op device:{requested: '', assigned: ''} def:{{{node decoder_model_28/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_28/outputlayer/MatMul, decoder_model_28/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.09764886796028009
cosine 0.09641767859230006
MAE: 0.004013252557363826
RMSE: 0.10323361946592338
r2: -7.308541732573341
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 45, 0.0005, 0.1, 94, 0.0011639892623221473, 0.0012899061829530403, 0.09764886796028009, 0.09641767859230006, 0.004013252557363826, 0.10323361946592338, -7.308541732573341, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0007 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_87 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_87 (ReLU)                (None, 2357)         0           ['batch_normalization_87[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 23:38:13.043428: W tensorflow/c/c_api.cc:291] Operation '{name:'training_58/Adam/batch_normalization_88/gamma/m/Assign' id:38184 op device:{requested: '', assigned: ''} def:{{{node training_58/Adam/batch_normalization_88/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_58/Adam/batch_normalization_88/gamma/m, training_58/Adam/batch_normalization_88/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:38:23.574903: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_29/mul' id:37646 op device:{requested: '', assigned: ''} def:{{{node loss_29/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_29/mul/x, loss_29/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0050 - val_loss: 0.0010 - 15s/epoch - 182us/sample
Epoch 2/50
84077/84077 - 8s - loss: 0.0026 - val_loss: 8.1596e-04 - 8s/epoch - 94us/sample
Epoch 3/50
84077/84077 - 8s - loss: 8.0361e-04 - val_loss: 6.7243e-04 - 8s/epoch - 95us/sample
Epoch 4/50
84077/84077 - 8s - loss: 7.7926e-04 - val_loss: 7.5668e-04 - 8s/epoch - 95us/sample
Epoch 5/50
84077/84077 - 8s - loss: 7.4726e-04 - val_loss: 7.4653e-04 - 8s/epoch - 94us/sample
Epoch 6/50
84077/84077 - 8s - loss: 6.2415e-04 - val_loss: 7.4455e-04 - 8s/epoch - 94us/sample
Epoch 7/50
84077/84077 - 8s - loss: 5.3862e-04 - val_loss: 4.6007e-04 - 8s/epoch - 94us/sample
Epoch 8/50
84077/84077 - 8s - loss: 4.4306e-04 - val_loss: 4.0687e-04 - 8s/epoch - 94us/sample
Epoch 9/50
84077/84077 - 8s - loss: 4.3309e-04 - val_loss: 4.1179e-04 - 8s/epoch - 94us/sample
Epoch 10/50
84077/84077 - 8s - loss: 3.9462e-04 - val_loss: 3.5417e-04 - 8s/epoch - 94us/sample
Epoch 11/50
84077/84077 - 8s - loss: 3.7067e-04 - val_loss: 3.3428e-04 - 8s/epoch - 94us/sample
Epoch 12/50
84077/84077 - 8s - loss: 3.5248e-04 - val_loss: 3.2138e-04 - 8s/epoch - 94us/sample
Epoch 13/50
84077/84077 - 8s - loss: 3.3671e-04 - val_loss: 3.1346e-04 - 8s/epoch - 94us/sample
Epoch 14/50
84077/84077 - 8s - loss: 3.2537e-04 - val_loss: 3.0276e-04 - 8s/epoch - 95us/sample
Epoch 15/50
84077/84077 - 8s - loss: 3.1387e-04 - val_loss: 2.9353e-04 - 8s/epoch - 94us/sample
Epoch 16/50
84077/84077 - 8s - loss: 3.0542e-04 - val_loss: 2.8222e-04 - 8s/epoch - 94us/sample
Epoch 17/50
84077/84077 - 8s - loss: 2.9705e-04 - val_loss: 2.7522e-04 - 8s/epoch - 94us/sample
Epoch 18/50
84077/84077 - 8s - loss: 2.8949e-04 - val_loss: 2.6575e-04 - 8s/epoch - 94us/sample
Epoch 19/50
84077/84077 - 8s - loss: 2.8347e-04 - val_loss: 2.6200e-04 - 8s/epoch - 94us/sample
Epoch 20/50
84077/84077 - 8s - loss: 2.7821e-04 - val_loss: 2.5834e-04 - 8s/epoch - 94us/sample
Epoch 21/50
84077/84077 - 8s - loss: 2.7365e-04 - val_loss: 2.5295e-04 - 8s/epoch - 95us/sample
Epoch 22/50
84077/84077 - 8s - loss: 2.6989e-04 - val_loss: 2.4840e-04 - 8s/epoch - 95us/sample
Epoch 23/50
84077/84077 - 8s - loss: 2.6578e-04 - val_loss: 2.4522e-04 - 8s/epoch - 94us/sample
Epoch 24/50
84077/84077 - 8s - loss: 2.6108e-04 - val_loss: 2.4129e-04 - 8s/epoch - 94us/sample
Epoch 25/50
84077/84077 - 8s - loss: 2.5751e-04 - val_loss: 2.3898e-04 - 8s/epoch - 94us/sample
Epoch 26/50
84077/84077 - 8s - loss: 2.5513e-04 - val_loss: 2.3641e-04 - 8s/epoch - 94us/sample
Epoch 27/50
84077/84077 - 8s - loss: 2.5236e-04 - val_loss: 2.3550e-04 - 8s/epoch - 94us/sample
Epoch 28/50
84077/84077 - 8s - loss: 2.4912e-04 - val_loss: 2.3304e-04 - 8s/epoch - 94us/sample
Epoch 29/50
84077/84077 - 8s - loss: 2.4745e-04 - val_loss: 2.3082e-04 - 8s/epoch - 95us/sample
Epoch 30/50
84077/84077 - 8s - loss: 2.4511e-04 - val_loss: 2.2793e-04 - 8s/epoch - 94us/sample
Epoch 31/50
84077/84077 - 8s - loss: 2.4250e-04 - val_loss: 2.2704e-04 - 8s/epoch - 94us/sample
Epoch 32/50
84077/84077 - 8s - loss: 2.4003e-04 - val_loss: 2.2390e-04 - 8s/epoch - 94us/sample
Epoch 33/50
84077/84077 - 8s - loss: 2.3834e-04 - val_loss: 2.2180e-04 - 8s/epoch - 94us/sample
Epoch 34/50
84077/84077 - 8s - loss: 2.3692e-04 - val_loss: 2.2158e-04 - 8s/epoch - 94us/sample
Epoch 35/50
84077/84077 - 8s - loss: 2.3460e-04 - val_loss: 2.1986e-04 - 8s/epoch - 94us/sample
Epoch 36/50
84077/84077 - 8s - loss: 2.3337e-04 - val_loss: 2.1846e-04 - 8s/epoch - 94us/sample
Epoch 37/50
84077/84077 - 8s - loss: 2.3206e-04 - val_loss: 2.1747e-04 - 8s/epoch - 95us/sample
Epoch 38/50
84077/84077 - 8s - loss: 2.3123e-04 - val_loss: 2.1699e-04 - 8s/epoch - 94us/sample
Epoch 39/50
84077/84077 - 8s - loss: 2.3008e-04 - val_loss: 2.1481e-04 - 8s/epoch - 94us/sample
Epoch 40/50
84077/84077 - 8s - loss: 2.2898e-04 - val_loss: 2.1476e-04 - 8s/epoch - 94us/sample
Epoch 41/50
84077/84077 - 8s - loss: 2.2804e-04 - val_loss: 2.1518e-04 - 8s/epoch - 94us/sample
Epoch 42/50
84077/84077 - 8s - loss: 2.2731e-04 - val_loss: 2.1175e-04 - 8s/epoch - 94us/sample
Epoch 43/50
84077/84077 - 8s - loss: 2.2562e-04 - val_loss: 2.1231e-04 - 8s/epoch - 94us/sample
Epoch 44/50
84077/84077 - 8s - loss: 2.2542e-04 - val_loss: 2.1220e-04 - 8s/epoch - 95us/sample
Epoch 45/50
84077/84077 - 8s - loss: 2.2438e-04 - val_loss: 2.1056e-04 - 8s/epoch - 95us/sample
Epoch 46/50
84077/84077 - 8s - loss: 2.2349e-04 - val_loss: 2.0936e-04 - 8s/epoch - 94us/sample
Epoch 47/50
84077/84077 - 8s - loss: 2.2242e-04 - val_loss: 2.0976e-04 - 8s/epoch - 95us/sample
Epoch 48/50
84077/84077 - 8s - loss: 2.2202e-04 - val_loss: 2.1057e-04 - 8s/epoch - 94us/sample
Epoch 49/50
84077/84077 - 8s - loss: 2.2114e-04 - val_loss: 2.0789e-04 - 8s/epoch - 94us/sample
Epoch 50/50
84077/84077 - 8s - loss: 2.2083e-04 - val_loss: 2.0913e-04 - 8s/epoch - 94us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002091288447781426
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:44:54.577285: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_29/outputlayer/BiasAdd' id:37610 op device:{requested: '', assigned: ''} def:{{{node decoder_model_29/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_29/outputlayer/MatMul, decoder_model_29/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05931729822395035
cosine 0.05857301800586808
MAE: 0.0031660901061549944
RMSE: 0.013835027656379558
r2: 0.8505797922466147
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 50, 0.0007, 0.1, 94, 0.00022083424046171285, 0.0002091288447781426, 0.05931729822395035, 0.05857301800586808, 0.0031660901061549944, 0.013835027656379558, 0.8505797922466147, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 50 0.0005 8 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2451)         2313744     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_90 (BatchN  (None, 2451)        9804        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_90 (ReLU)                (None, 2451)         0           ['batch_normalization_90[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           230488      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           230488      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2564191     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,348,715
Trainable params: 5,338,723
Non-trainable params: 9,992
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 23:45:09.482762: W tensorflow/c/c_api.cc:291] Operation '{name:'training_60/Adam/batch_normalization_90/gamma/m/Assign' id:39421 op device:{requested: '', assigned: ''} def:{{{node training_60/Adam/batch_normalization_90/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_60/Adam/batch_normalization_90/gamma/m, training_60/Adam/batch_normalization_90/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:45:48.542710: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_30/mul' id:38931 op device:{requested: '', assigned: ''} def:{{{node loss_30/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_30/mul/x, loss_30/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 46s - loss: 0.0026 - val_loss: 6.3552e-04 - 46s/epoch - 544us/sample
Epoch 2/50
84077/84077 - 38s - loss: 6.1848e-04 - val_loss: 6.1863e-04 - 38s/epoch - 451us/sample
Epoch 3/50
84077/84077 - 38s - loss: 6.1204e-04 - val_loss: 6.1790e-04 - 38s/epoch - 450us/sample
Epoch 4/50
84077/84077 - 38s - loss: 6.1165e-04 - val_loss: 6.1734e-04 - 38s/epoch - 451us/sample
Epoch 5/50
84077/84077 - 38s - loss: 6.1159e-04 - val_loss: 6.1768e-04 - 38s/epoch - 449us/sample
Epoch 6/50
84077/84077 - 38s - loss: 6.1155e-04 - val_loss: 6.1698e-04 - 38s/epoch - 451us/sample
Epoch 7/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1716e-04 - 38s/epoch - 449us/sample
Epoch 8/50
84077/84077 - 38s - loss: 6.1154e-04 - val_loss: 6.1777e-04 - 38s/epoch - 452us/sample
Epoch 9/50
84077/84077 - 38s - loss: 6.1156e-04 - val_loss: 6.1752e-04 - 38s/epoch - 451us/sample
Epoch 10/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1685e-04 - 38s/epoch - 451us/sample
Epoch 11/50
84077/84077 - 38s - loss: 6.1153e-04 - val_loss: 6.1686e-04 - 38s/epoch - 451us/sample
Epoch 12/50
84077/84077 - 38s - loss: 6.1153e-04 - val_loss: 6.1726e-04 - 38s/epoch - 450us/sample
Epoch 13/50
84077/84077 - 38s - loss: 6.1154e-04 - val_loss: 6.1717e-04 - 38s/epoch - 451us/sample
Epoch 14/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1695e-04 - 38s/epoch - 450us/sample
Epoch 15/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1748e-04 - 38s/epoch - 451us/sample
Epoch 16/50
84077/84077 - 38s - loss: 6.1153e-04 - val_loss: 6.1749e-04 - 38s/epoch - 450us/sample
Epoch 17/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1701e-04 - 38s/epoch - 451us/sample
Epoch 18/50
84077/84077 - 38s - loss: 6.1148e-04 - val_loss: 6.1702e-04 - 38s/epoch - 451us/sample
Epoch 19/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1742e-04 - 38s/epoch - 452us/sample
Epoch 20/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1735e-04 - 38s/epoch - 451us/sample
Epoch 21/50
84077/84077 - 38s - loss: 6.1154e-04 - val_loss: 6.1728e-04 - 38s/epoch - 450us/sample
Epoch 22/50
84077/84077 - 38s - loss: 6.1150e-04 - val_loss: 6.1694e-04 - 38s/epoch - 451us/sample
Epoch 23/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1717e-04 - 38s/epoch - 450us/sample
Epoch 24/50
84077/84077 - 38s - loss: 6.1154e-04 - val_loss: 6.1718e-04 - 38s/epoch - 451us/sample
Epoch 25/50
84077/84077 - 38s - loss: 6.1154e-04 - val_loss: 6.1700e-04 - 38s/epoch - 450us/sample
Epoch 26/50
84077/84077 - 38s - loss: 6.1157e-04 - val_loss: 6.1716e-04 - 38s/epoch - 452us/sample
Epoch 27/50
84077/84077 - 38s - loss: 6.1153e-04 - val_loss: 6.1748e-04 - 38s/epoch - 451us/sample
Epoch 28/50
84077/84077 - 38s - loss: 6.1153e-04 - val_loss: 6.1696e-04 - 38s/epoch - 450us/sample
Epoch 29/50
84077/84077 - 38s - loss: 6.1149e-04 - val_loss: 6.1722e-04 - 38s/epoch - 451us/sample
Epoch 30/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1692e-04 - 38s/epoch - 450us/sample
Epoch 31/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1678e-04 - 38s/epoch - 451us/sample
Epoch 32/50
84077/84077 - 38s - loss: 6.1154e-04 - val_loss: 6.1709e-04 - 38s/epoch - 451us/sample
Epoch 33/50
84077/84077 - 38s - loss: 6.1149e-04 - val_loss: 6.1731e-04 - 38s/epoch - 451us/sample
Epoch 34/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1730e-04 - 38s/epoch - 452us/sample
Epoch 35/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1725e-04 - 38s/epoch - 450us/sample
Epoch 36/50
84077/84077 - 38s - loss: 6.1153e-04 - val_loss: 6.1716e-04 - 38s/epoch - 452us/sample
Epoch 37/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1676e-04 - 38s/epoch - 449us/sample
Epoch 38/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1729e-04 - 38s/epoch - 451us/sample
Epoch 39/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1710e-04 - 38s/epoch - 452us/sample
Epoch 40/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1675e-04 - 38s/epoch - 450us/sample
Epoch 41/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1785e-04 - 38s/epoch - 451us/sample
Epoch 42/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1712e-04 - 38s/epoch - 450us/sample
Epoch 43/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1754e-04 - 38s/epoch - 452us/sample
Epoch 44/50
84077/84077 - 38s - loss: 6.1154e-04 - val_loss: 6.1738e-04 - 38s/epoch - 450us/sample
Epoch 45/50
84077/84077 - 38s - loss: 6.1149e-04 - val_loss: 6.1692e-04 - 38s/epoch - 450us/sample
Epoch 46/50
84077/84077 - 38s - loss: 6.1150e-04 - val_loss: 6.1728e-04 - 38s/epoch - 451us/sample
Epoch 47/50
84077/84077 - 38s - loss: 6.1152e-04 - val_loss: 6.1739e-04 - 38s/epoch - 450us/sample
Epoch 48/50
84077/84077 - 38s - loss: 6.1149e-04 - val_loss: 6.1730e-04 - 38s/epoch - 451us/sample
Epoch 49/50
84077/84077 - 38s - loss: 6.1153e-04 - val_loss: 6.1759e-04 - 38s/epoch - 450us/sample
Epoch 50/50
84077/84077 - 38s - loss: 6.1151e-04 - val_loss: 6.1700e-04 - 38s/epoch - 452us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0006169968167677988
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:16:50.054933: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_30/outputlayer/BiasAdd' id:38895 op device:{requested: '', assigned: ''} def:{{{node decoder_model_30/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_30/outputlayer/MatMul, decoder_model_30/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.5094963252683272
cosine 0.4999494033749267
MAE: 0.006453717898796869
RMSE: 0.03583071602979755
r2: -0.002932741177981639
RMSE zero-vector: 0.04004287452915337
['2.6custom_VAE', 'logcosh', 8, 50, 0.0005, 0.1, 94, 0.0006115140083964731, 0.0006169968167677988, 0.5094963252683272, 0.4999494033749267, 0.006453717898796869, 0.03583071602979755, -0.002932741177981639, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 45 0.0005 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_93 (BatchN  (None, 2357)        9428        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_93 (ReLU)                (None, 2357)         0           ['batch_normalization_93[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/45
2023-02-15 00:17:05.369578: W tensorflow/c/c_api.cc:291] Operation '{name:'training_62/Adam/dense_dec0_31/bias/v/Assign' id:40857 op device:{requested: '', assigned: ''} def:{{{node training_62/Adam/dense_dec0_31/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_62/Adam/dense_dec0_31/bias/v, training_62/Adam/dense_dec0_31/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:17:44.269029: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_31/mul' id:40209 op device:{requested: '', assigned: ''} def:{{{node loss_31/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_31/mul/x, loss_31/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 46s - loss: 0.1172 - val_loss: 0.0020 - 46s/epoch - 545us/sample
Epoch 2/45
84077/84077 - 37s - loss: 0.0012 - val_loss: 0.0014 - 37s/epoch - 441us/sample
Epoch 3/45
84077/84077 - 37s - loss: 0.0010 - val_loss: 0.0064 - 37s/epoch - 444us/sample
Epoch 4/45
84077/84077 - 37s - loss: 9.4695e-04 - val_loss: 0.0098 - 37s/epoch - 441us/sample
Epoch 5/45
84077/84077 - 37s - loss: 9.2711e-04 - val_loss: 0.0066 - 37s/epoch - 444us/sample
Epoch 6/45
84077/84077 - 37s - loss: 9.1202e-04 - val_loss: 0.0027 - 37s/epoch - 442us/sample
Epoch 7/45
84077/84077 - 37s - loss: 8.9606e-04 - val_loss: 0.0014 - 37s/epoch - 444us/sample
Epoch 8/45
84077/84077 - 37s - loss: 8.8704e-04 - val_loss: 0.0012 - 37s/epoch - 443us/sample
Epoch 9/45
84077/84077 - 37s - loss: 8.8277e-04 - val_loss: 0.0012 - 37s/epoch - 443us/sample
Epoch 10/45
84077/84077 - 37s - loss: 8.8098e-04 - val_loss: 0.0012 - 37s/epoch - 442us/sample
Epoch 11/45
84077/84077 - 37s - loss: 8.7931e-04 - val_loss: 0.0012 - 37s/epoch - 444us/sample
Epoch 12/45
84077/84077 - 37s - loss: 8.7842e-04 - val_loss: 0.0013 - 37s/epoch - 442us/sample
Epoch 13/45
84077/84077 - 37s - loss: 8.7809e-04 - val_loss: 0.0011 - 37s/epoch - 444us/sample
Epoch 14/45
84077/84077 - 37s - loss: 8.7632e-04 - val_loss: 0.0013 - 37s/epoch - 441us/sample
Epoch 15/45
84077/84077 - 37s - loss: 8.7555e-04 - val_loss: 0.0014 - 37s/epoch - 443us/sample
Epoch 16/45
84077/84077 - 37s - loss: 8.7479e-04 - val_loss: 0.0015 - 37s/epoch - 442us/sample
Epoch 17/45
84077/84077 - 37s - loss: 8.7432e-04 - val_loss: 0.0013 - 37s/epoch - 444us/sample
Epoch 18/45
84077/84077 - 37s - loss: 8.7448e-04 - val_loss: 0.0014 - 37s/epoch - 441us/sample
Epoch 19/45
84077/84077 - 37s - loss: 8.7402e-04 - val_loss: 0.0016 - 37s/epoch - 443us/sample
Epoch 20/45
84077/84077 - 37s - loss: 8.7351e-04 - val_loss: 0.0015 - 37s/epoch - 442us/sample
Epoch 21/45
84077/84077 - 37s - loss: 8.7389e-04 - val_loss: 0.0014 - 37s/epoch - 443us/sample
Epoch 22/45
84077/84077 - 37s - loss: 8.7418e-04 - val_loss: 0.0013 - 37s/epoch - 443us/sample
Epoch 23/45
84077/84077 - 37s - loss: 8.7276e-04 - val_loss: 0.0016 - 37s/epoch - 442us/sample
Epoch 24/45
84077/84077 - 37s - loss: 8.7257e-04 - val_loss: 0.0014 - 37s/epoch - 444us/sample
Epoch 25/45
84077/84077 - 37s - loss: 8.7267e-04 - val_loss: 0.0015 - 37s/epoch - 442us/sample
Epoch 26/45
84077/84077 - 37s - loss: 8.7324e-04 - val_loss: 0.0016 - 37s/epoch - 442us/sample
Epoch 27/45
84077/84077 - 37s - loss: 8.7199e-04 - val_loss: 0.0015 - 37s/epoch - 443us/sample
Epoch 28/45
84077/84077 - 37s - loss: 8.7283e-04 - val_loss: 0.0013 - 37s/epoch - 443us/sample
Epoch 29/45
84077/84077 - 37s - loss: 8.7326e-04 - val_loss: 0.0013 - 37s/epoch - 442us/sample
Epoch 30/45
84077/84077 - 37s - loss: 8.7353e-04 - val_loss: 0.0014 - 37s/epoch - 444us/sample
Epoch 31/45
84077/84077 - 37s - loss: 8.7205e-04 - val_loss: 0.0015 - 37s/epoch - 442us/sample
Epoch 32/45
84077/84077 - 37s - loss: 8.7125e-04 - val_loss: 0.0015 - 37s/epoch - 444us/sample
Epoch 33/45
84077/84077 - 37s - loss: 8.7177e-04 - val_loss: 0.0020 - 37s/epoch - 441us/sample
Epoch 34/45
84077/84077 - 37s - loss: 8.7211e-04 - val_loss: 0.0016 - 37s/epoch - 443us/sample
Epoch 35/45
84077/84077 - 37s - loss: 8.7217e-04 - val_loss: 0.0020 - 37s/epoch - 442us/sample
Epoch 36/45
84077/84077 - 37s - loss: 8.7097e-04 - val_loss: 0.0018 - 37s/epoch - 443us/sample
Epoch 37/45
84077/84077 - 37s - loss: 8.7139e-04 - val_loss: 0.0016 - 37s/epoch - 441us/sample
Epoch 38/45
84077/84077 - 37s - loss: 8.7195e-04 - val_loss: 0.0020 - 37s/epoch - 444us/sample
Epoch 39/45
84077/84077 - 37s - loss: 8.7157e-04 - val_loss: 0.0014 - 37s/epoch - 441us/sample
Epoch 40/45
84077/84077 - 37s - loss: 8.7157e-04 - val_loss: 0.0013 - 37s/epoch - 444us/sample
Epoch 41/45
84077/84077 - 37s - loss: 8.7052e-04 - val_loss: 0.0016 - 37s/epoch - 441us/sample
Epoch 42/45
84077/84077 - 37s - loss: 8.7170e-04 - val_loss: 0.0016 - 37s/epoch - 443us/sample
Epoch 43/45
84077/84077 - 37s - loss: 8.7094e-04 - val_loss: 0.0017 - 37s/epoch - 442us/sample
Epoch 44/45
84077/84077 - 37s - loss: 8.7133e-04 - val_loss: 0.0016 - 37s/epoch - 442us/sample
Epoch 45/45
84077/84077 - 37s - loss: 8.7121e-04 - val_loss: 0.0017 - 37s/epoch - 442us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.001713085255159029
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:45:05.868631: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_31/outputlayer/BiasAdd' id:40180 op device:{requested: '', assigned: ''} def:{{{node decoder_model_31/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_31/outputlayer/MatMul, decoder_model_31/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.3292073309312825
cosine 0.32467386453034364
MAE: 0.007489018643830697
RMSE: 0.040893986204189714
r2: -0.3057568807316583
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'mse', 8, 45, 0.0005, 0.1, 94, 0.0008712106018804995, 0.001713085255159029, 0.3292073309312825, 0.32467386453034364, 0.007489018643830697, 0.040893986204189714, -0.3057568807316583, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 3
Fitness    = 424.308669689168
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 424.308669689168.
Best solutions :  [array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [424.308669689168, 424.308669689168, 424.308669689168]
[2.1 45 0.0007 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_96 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_96 (ReLU)                (None, 1980)         0           ['batch_normalization_96[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           186214      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           186214      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2073409     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,322,877
Trainable params: 4,314,769
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/45
2023-02-15 00:45:21.010114: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_32/bias/Assign' id:41147 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_32/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_32/bias, bottleneck_zlog_32/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:45:31.738940: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_32/mul' id:41471 op device:{requested: '', assigned: ''} def:{{{node loss_32/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_32/mul/x, loss_32/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.0228 - val_loss: 0.0010 - 16s/epoch - 188us/sample
Epoch 2/45
84077/84077 - 8s - loss: 0.0014 - val_loss: 8.7690e-04 - 8s/epoch - 93us/sample
Epoch 3/45
84077/84077 - 8s - loss: 8.8603e-04 - val_loss: 7.9868e-04 - 8s/epoch - 93us/sample
Epoch 4/45
84077/84077 - 8s - loss: 7.2346e-04 - val_loss: 7.5071e-04 - 8s/epoch - 93us/sample
Epoch 5/45
84077/84077 - 8s - loss: 6.5458e-04 - val_loss: 5.9727e-04 - 8s/epoch - 93us/sample
Epoch 6/45
84077/84077 - 8s - loss: 5.7143e-04 - val_loss: 4.7946e-04 - 8s/epoch - 95us/sample
Epoch 7/45
84077/84077 - 8s - loss: 4.7878e-04 - val_loss: 4.2534e-04 - 8s/epoch - 93us/sample
Epoch 8/45
84077/84077 - 8s - loss: 4.3521e-04 - val_loss: 3.9348e-04 - 8s/epoch - 93us/sample
Epoch 9/45
84077/84077 - 8s - loss: 4.0689e-04 - val_loss: 3.7053e-04 - 8s/epoch - 94us/sample
Epoch 10/45
84077/84077 - 8s - loss: 3.8888e-04 - val_loss: 3.4966e-04 - 8s/epoch - 94us/sample
Epoch 11/45
84077/84077 - 8s - loss: 3.6499e-04 - val_loss: 3.3999e-04 - 8s/epoch - 93us/sample
Epoch 12/45
84077/84077 - 8s - loss: 3.5219e-04 - val_loss: 3.2085e-04 - 8s/epoch - 94us/sample
Epoch 13/45
84077/84077 - 8s - loss: 3.3687e-04 - val_loss: 3.0964e-04 - 8s/epoch - 93us/sample
Epoch 14/45
84077/84077 - 8s - loss: 3.2592e-04 - val_loss: 3.0264e-04 - 8s/epoch - 94us/sample
Epoch 15/45
84077/84077 - 8s - loss: 3.1870e-04 - val_loss: 2.9529e-04 - 8s/epoch - 94us/sample
Epoch 16/45
84077/84077 - 8s - loss: 3.1025e-04 - val_loss: 2.8402e-04 - 8s/epoch - 93us/sample
Epoch 17/45
84077/84077 - 8s - loss: 3.0112e-04 - val_loss: 2.7730e-04 - 8s/epoch - 94us/sample
Epoch 18/45
84077/84077 - 8s - loss: 2.9239e-04 - val_loss: 2.6631e-04 - 8s/epoch - 93us/sample
Epoch 19/45
84077/84077 - 8s - loss: 2.8512e-04 - val_loss: 2.6335e-04 - 8s/epoch - 93us/sample
Epoch 20/45
84077/84077 - 8s - loss: 2.7765e-04 - val_loss: 2.5409e-04 - 8s/epoch - 93us/sample
Epoch 21/45
84077/84077 - 8s - loss: 2.7295e-04 - val_loss: 2.4987e-04 - 8s/epoch - 93us/sample
Epoch 22/45
84077/84077 - 8s - loss: 2.6873e-04 - val_loss: 2.4714e-04 - 8s/epoch - 93us/sample
Epoch 23/45
84077/84077 - 8s - loss: 2.6513e-04 - val_loss: 2.4240e-04 - 8s/epoch - 95us/sample
Epoch 24/45
84077/84077 - 8s - loss: 2.6192e-04 - val_loss: 2.4210e-04 - 8s/epoch - 93us/sample
Epoch 25/45
84077/84077 - 8s - loss: 2.5905e-04 - val_loss: 2.3826e-04 - 8s/epoch - 94us/sample
Epoch 26/45
84077/84077 - 8s - loss: 2.5571e-04 - val_loss: 2.3623e-04 - 8s/epoch - 93us/sample
Epoch 27/45
84077/84077 - 8s - loss: 2.5234e-04 - val_loss: 2.3311e-04 - 8s/epoch - 94us/sample
Epoch 28/45
84077/84077 - 8s - loss: 2.5075e-04 - val_loss: 2.3149e-04 - 8s/epoch - 93us/sample
Epoch 29/45
84077/84077 - 8s - loss: 2.4882e-04 - val_loss: 2.2930e-04 - 8s/epoch - 93us/sample
Epoch 30/45
84077/84077 - 8s - loss: 2.4683e-04 - val_loss: 2.2826e-04 - 8s/epoch - 93us/sample
Epoch 31/45
84077/84077 - 8s - loss: 2.4504e-04 - val_loss: 2.2905e-04 - 8s/epoch - 95us/sample
Epoch 32/45
84077/84077 - 8s - loss: 2.4280e-04 - val_loss: 2.2501e-04 - 8s/epoch - 93us/sample
Epoch 33/45
84077/84077 - 8s - loss: 2.4101e-04 - val_loss: 2.2407e-04 - 8s/epoch - 93us/sample
Epoch 34/45
84077/84077 - 8s - loss: 2.3960e-04 - val_loss: 2.2239e-04 - 8s/epoch - 93us/sample
Epoch 35/45
84077/84077 - 8s - loss: 2.3829e-04 - val_loss: 2.2097e-04 - 8s/epoch - 93us/sample
Epoch 36/45
84077/84077 - 8s - loss: 2.3673e-04 - val_loss: 2.2230e-04 - 8s/epoch - 94us/sample
Epoch 37/45
84077/84077 - 8s - loss: 2.3513e-04 - val_loss: 2.2038e-04 - 8s/epoch - 93us/sample
Epoch 38/45
84077/84077 - 8s - loss: 2.3362e-04 - val_loss: 2.1902e-04 - 8s/epoch - 94us/sample
Epoch 39/45
84077/84077 - 8s - loss: 2.3302e-04 - val_loss: 2.1711e-04 - 8s/epoch - 94us/sample
Epoch 40/45
84077/84077 - 8s - loss: 2.3120e-04 - val_loss: 2.1476e-04 - 8s/epoch - 94us/sample
Epoch 41/45
84077/84077 - 8s - loss: 2.3011e-04 - val_loss: 2.1397e-04 - 8s/epoch - 94us/sample
Epoch 42/45
84077/84077 - 8s - loss: 2.2974e-04 - val_loss: 2.1543e-04 - 8s/epoch - 93us/sample
Epoch 43/45
84077/84077 - 8s - loss: 2.2871e-04 - val_loss: 2.1401e-04 - 8s/epoch - 93us/sample
Epoch 44/45
84077/84077 - 8s - loss: 2.2761e-04 - val_loss: 2.1272e-04 - 8s/epoch - 93us/sample
Epoch 45/45
84077/84077 - 8s - loss: 2.2658e-04 - val_loss: 2.1236e-04 - 8s/epoch - 94us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00021236381023074776
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:51:20.627355: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_32/outputlayer/BiasAdd' id:41435 op device:{requested: '', assigned: ''} def:{{{node decoder_model_32/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_32/outputlayer/MatMul, decoder_model_32/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.06579437185524524
cosine 0.06497532542680737
MAE: 0.003407823218255146
RMSE: 0.014344855577794245
r2: 0.8393593410014573
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 45, 0.0007, 0.1, 94, 0.00022658112789522672, 0.00021236381023074776, 0.06579437185524524, 0.06497532542680737, 0.003407823218255146, 0.014344855577794245, 0.8393593410014573, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 85 0.001 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2451)         2313744     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_99 (BatchN  (None, 2451)        9804        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_99 (ReLU)                (None, 2451)         0           ['batch_normalization_99[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           230488      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           230488      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2564191     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,348,715
Trainable params: 5,338,723
Non-trainable params: 9,992
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 00:51:36.452109: W tensorflow/c/c_api.cc:291] Operation '{name:'training_66/Adam/dense_dec0_33/kernel/v/Assign' id:43420 op device:{requested: '', assigned: ''} def:{{{node training_66/Adam/dense_dec0_33/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_66/Adam/dense_dec0_33/kernel/v, training_66/Adam/dense_dec0_33/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:51:47.530038: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_33/mul' id:42756 op device:{requested: '', assigned: ''} def:{{{node loss_33/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_33/mul/x, loss_33/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.1661 - val_loss: 0.0011 - 16s/epoch - 194us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0025 - val_loss: 9.8533e-04 - 8s/epoch - 97us/sample
Epoch 3/85
84077/84077 - 8s - loss: 8.7223e-04 - val_loss: 7.0236e-04 - 8s/epoch - 96us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0015 - val_loss: 0.0054 - 8s/epoch - 96us/sample
Epoch 5/85
84077/84077 - 8s - loss: 8.1070e-04 - val_loss: 5.8879e-04 - 8s/epoch - 96us/sample
Epoch 6/85
84077/84077 - 8s - loss: 6.2770e-04 - val_loss: 7.7626e-04 - 8s/epoch - 96us/sample
Epoch 7/85
84077/84077 - 8s - loss: 5.1624e-04 - val_loss: 5.0078e-04 - 8s/epoch - 96us/sample
Epoch 8/85
84077/84077 - 8s - loss: 4.9781e-04 - val_loss: 4.8666e-04 - 8s/epoch - 98us/sample
Epoch 9/85
84077/84077 - 8s - loss: 4.8451e-04 - val_loss: 4.5632e-04 - 8s/epoch - 96us/sample
Epoch 10/85
84077/84077 - 8s - loss: 4.7061e-04 - val_loss: 4.4633e-04 - 8s/epoch - 96us/sample
Epoch 11/85
84077/84077 - 8s - loss: 4.5869e-04 - val_loss: 5.5984e-04 - 8s/epoch - 96us/sample
Epoch 12/85
84077/84077 - 8s - loss: 4.6902e-04 - val_loss: 4.2770e-04 - 8s/epoch - 96us/sample
Epoch 13/85
84077/84077 - 8s - loss: 4.3681e-04 - val_loss: 4.1772e-04 - 8s/epoch - 96us/sample
Epoch 14/85
84077/84077 - 8s - loss: 4.3382e-04 - val_loss: 4.2592e-04 - 8s/epoch - 96us/sample
Epoch 15/85
84077/84077 - 8s - loss: 4.3007e-04 - val_loss: 4.1153e-04 - 8s/epoch - 96us/sample
Epoch 16/85
84077/84077 - 8s - loss: 4.2095e-04 - val_loss: 4.0860e-04 - 8s/epoch - 98us/sample
Epoch 17/85
84077/84077 - 8s - loss: 4.2525e-04 - val_loss: 4.0622e-04 - 8s/epoch - 96us/sample
Epoch 18/85
84077/84077 - 8s - loss: 4.1132e-04 - val_loss: 3.9363e-04 - 8s/epoch - 97us/sample
Epoch 19/85
84077/84077 - 8s - loss: 4.0929e-04 - val_loss: 3.9120e-04 - 8s/epoch - 96us/sample
Epoch 20/85
84077/84077 - 8s - loss: 4.0503e-04 - val_loss: 3.8886e-04 - 8s/epoch - 96us/sample
Epoch 21/85
84077/84077 - 8s - loss: 4.0240e-04 - val_loss: 3.8425e-04 - 8s/epoch - 96us/sample
Epoch 22/85
84077/84077 - 8s - loss: 3.9525e-04 - val_loss: 3.7715e-04 - 8s/epoch - 96us/sample
Epoch 23/85
84077/84077 - 8s - loss: 3.9176e-04 - val_loss: 3.7468e-04 - 8s/epoch - 96us/sample
Epoch 24/85
84077/84077 - 8s - loss: 3.8839e-04 - val_loss: 3.7197e-04 - 8s/epoch - 97us/sample
Epoch 25/85
84077/84077 - 8s - loss: 3.8607e-04 - val_loss: 3.7084e-04 - 8s/epoch - 96us/sample
Epoch 26/85
84077/84077 - 8s - loss: 3.8373e-04 - val_loss: 3.6820e-04 - 8s/epoch - 96us/sample
Epoch 27/85
84077/84077 - 8s - loss: 3.8100e-04 - val_loss: 3.6580e-04 - 8s/epoch - 96us/sample
Epoch 28/85
84077/84077 - 8s - loss: 3.7888e-04 - val_loss: 3.6323e-04 - 8s/epoch - 96us/sample
Epoch 29/85
84077/84077 - 8s - loss: 3.7703e-04 - val_loss: 3.6179e-04 - 8s/epoch - 96us/sample
Epoch 30/85
84077/84077 - 8s - loss: 3.7570e-04 - val_loss: 3.6068e-04 - 8s/epoch - 96us/sample
Epoch 31/85
84077/84077 - 8s - loss: 3.7443e-04 - val_loss: 3.5944e-04 - 8s/epoch - 96us/sample
Epoch 32/85
84077/84077 - 8s - loss: 3.7403e-04 - val_loss: 3.5812e-04 - 8s/epoch - 97us/sample
Epoch 33/85
84077/84077 - 8s - loss: 3.7246e-04 - val_loss: 3.5720e-04 - 8s/epoch - 97us/sample
Epoch 34/85
84077/84077 - 8s - loss: 3.7118e-04 - val_loss: 3.5586e-04 - 8s/epoch - 97us/sample
Epoch 35/85
84077/84077 - 8s - loss: 3.6979e-04 - val_loss: 3.5630e-04 - 8s/epoch - 96us/sample
Epoch 36/85
84077/84077 - 8s - loss: 3.6959e-04 - val_loss: 3.5408e-04 - 8s/epoch - 96us/sample
Epoch 37/85
84077/84077 - 8s - loss: 3.6855e-04 - val_loss: 3.5548e-04 - 8s/epoch - 96us/sample
Epoch 38/85
84077/84077 - 8s - loss: 3.6739e-04 - val_loss: 3.5391e-04 - 8s/epoch - 96us/sample
Epoch 39/85
84077/84077 - 8s - loss: 3.6581e-04 - val_loss: 3.5189e-04 - 8s/epoch - 96us/sample
Epoch 40/85
84077/84077 - 8s - loss: 3.6347e-04 - val_loss: 3.4786e-04 - 8s/epoch - 98us/sample
Epoch 41/85
84077/84077 - 8s - loss: 3.6188e-04 - val_loss: 3.4566e-04 - 8s/epoch - 96us/sample
Epoch 42/85
84077/84077 - 8s - loss: 3.6006e-04 - val_loss: 3.4448e-04 - 8s/epoch - 96us/sample
Epoch 43/85
84077/84077 - 8s - loss: 3.5808e-04 - val_loss: 3.4104e-04 - 8s/epoch - 96us/sample
Epoch 44/85
84077/84077 - 8s - loss: 3.5548e-04 - val_loss: 3.3951e-04 - 8s/epoch - 96us/sample
Epoch 45/85
84077/84077 - 8s - loss: 3.5425e-04 - val_loss: 3.3883e-04 - 8s/epoch - 96us/sample
Epoch 46/85
84077/84077 - 8s - loss: 3.5277e-04 - val_loss: 3.3687e-04 - 8s/epoch - 96us/sample
Epoch 47/85
84077/84077 - 8s - loss: 3.5176e-04 - val_loss: 3.3591e-04 - 8s/epoch - 97us/sample
Epoch 48/85
84077/84077 - 8s - loss: 3.5056e-04 - val_loss: 3.3396e-04 - 8s/epoch - 97us/sample
Epoch 49/85
84077/84077 - 8s - loss: 3.4932e-04 - val_loss: 3.3310e-04 - 8s/epoch - 97us/sample
Epoch 50/85
84077/84077 - 8s - loss: 3.4885e-04 - val_loss: 3.3193e-04 - 8s/epoch - 96us/sample
Epoch 51/85
84077/84077 - 8s - loss: 3.4758e-04 - val_loss: 3.3051e-04 - 8s/epoch - 96us/sample
Epoch 52/85
84077/84077 - 8s - loss: 3.4599e-04 - val_loss: 3.3076e-04 - 8s/epoch - 96us/sample
Epoch 53/85
84077/84077 - 8s - loss: 3.4473e-04 - val_loss: 3.2693e-04 - 8s/epoch - 96us/sample
Epoch 54/85
84077/84077 - 8s - loss: 3.4391e-04 - val_loss: 3.2862e-04 - 8s/epoch - 96us/sample
Epoch 55/85
84077/84077 - 8s - loss: 3.4345e-04 - val_loss: 3.2712e-04 - 8s/epoch - 96us/sample
Epoch 56/85
84077/84077 - 8s - loss: 3.4294e-04 - val_loss: 3.2788e-04 - 8s/epoch - 98us/sample
Epoch 57/85
84077/84077 - 8s - loss: 3.4242e-04 - val_loss: 3.2587e-04 - 8s/epoch - 96us/sample
Epoch 58/85
84077/84077 - 8s - loss: 3.4215e-04 - val_loss: 3.2575e-04 - 8s/epoch - 97us/sample
Epoch 59/85
84077/84077 - 8s - loss: 3.4151e-04 - val_loss: 3.2584e-04 - 8s/epoch - 96us/sample
Epoch 60/85
84077/84077 - 8s - loss: 3.4133e-04 - val_loss: 3.2635e-04 - 8s/epoch - 96us/sample
Epoch 61/85
84077/84077 - 8s - loss: 3.4070e-04 - val_loss: 3.2584e-04 - 8s/epoch - 96us/sample
Epoch 62/85
84077/84077 - 8s - loss: 3.4030e-04 - val_loss: 3.2482e-04 - 8s/epoch - 97us/sample
Epoch 63/85
84077/84077 - 8s - loss: 3.4013e-04 - val_loss: 3.2484e-04 - 8s/epoch - 97us/sample
Epoch 64/85
84077/84077 - 8s - loss: 3.3960e-04 - val_loss: 3.2439e-04 - 8s/epoch - 98us/sample
Epoch 65/85
84077/84077 - 8s - loss: 3.3919e-04 - val_loss: 3.2296e-04 - 8s/epoch - 96us/sample
Epoch 66/85
84077/84077 - 8s - loss: 3.3953e-04 - val_loss: 3.2431e-04 - 8s/epoch - 97us/sample
Epoch 67/85
84077/84077 - 8s - loss: 3.3913e-04 - val_loss: 3.2417e-04 - 8s/epoch - 96us/sample
Epoch 68/85
84077/84077 - 8s - loss: 3.3882e-04 - val_loss: 3.2368e-04 - 8s/epoch - 96us/sample
Epoch 69/85
84077/84077 - 8s - loss: 3.3861e-04 - val_loss: 3.2362e-04 - 8s/epoch - 96us/sample
Epoch 70/85
84077/84077 - 8s - loss: 3.3868e-04 - val_loss: 3.2392e-04 - 8s/epoch - 96us/sample
Epoch 71/85
84077/84077 - 8s - loss: 3.3833e-04 - val_loss: 3.2383e-04 - 8s/epoch - 97us/sample
Epoch 72/85
84077/84077 - 8s - loss: 3.3833e-04 - val_loss: 3.2239e-04 - 8s/epoch - 98us/sample
Epoch 73/85
84077/84077 - 8s - loss: 3.3815e-04 - val_loss: 3.2274e-04 - 8s/epoch - 97us/sample
Epoch 74/85
84077/84077 - 8s - loss: 3.3800e-04 - val_loss: 3.2225e-04 - 8s/epoch - 97us/sample
Epoch 75/85
84077/84077 - 8s - loss: 3.3737e-04 - val_loss: 3.2214e-04 - 8s/epoch - 96us/sample
Epoch 76/85
84077/84077 - 8s - loss: 3.3745e-04 - val_loss: 3.2195e-04 - 8s/epoch - 97us/sample
Epoch 77/85
84077/84077 - 8s - loss: 3.3717e-04 - val_loss: 3.2179e-04 - 8s/epoch - 97us/sample
Epoch 78/85
84077/84077 - 8s - loss: 3.3675e-04 - val_loss: 3.2267e-04 - 8s/epoch - 96us/sample
Epoch 79/85
84077/84077 - 8s - loss: 3.3735e-04 - val_loss: 3.2217e-04 - 8s/epoch - 96us/sample
Epoch 80/85
84077/84077 - 8s - loss: 3.3690e-04 - val_loss: 3.2101e-04 - 8s/epoch - 97us/sample
Epoch 81/85
84077/84077 - 8s - loss: 3.3530e-04 - val_loss: 3.2027e-04 - 8s/epoch - 96us/sample
Epoch 82/85
84077/84077 - 8s - loss: 3.3507e-04 - val_loss: 3.1938e-04 - 8s/epoch - 97us/sample
Epoch 83/85
84077/84077 - 8s - loss: 3.3445e-04 - val_loss: 3.1832e-04 - 8s/epoch - 96us/sample
Epoch 84/85
84077/84077 - 8s - loss: 3.3463e-04 - val_loss: 3.1835e-04 - 8s/epoch - 97us/sample
Epoch 85/85
84077/84077 - 8s - loss: 3.3352e-04 - val_loss: 3.1738e-04 - 8s/epoch - 96us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00031737970426947625
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:03:12.032891: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_33/outputlayer/BiasAdd' id:42720 op device:{requested: '', assigned: ''} def:{{{node decoder_model_33/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_33/outputlayer/MatMul, decoder_model_33/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.15237558671990659
cosine 0.15031902312704212
MAE: 0.004029424553624935
RMSE: 0.022281555146831552
r2: 0.612187126487593
RMSE zero-vector: 0.04004287452915337
['2.6custom_VAE', 'logcosh', 64, 85, 0.001, 0.1, 94, 0.0003335210929525497, 0.00031737970426947625, 0.15237558671990659, 0.15031902312704212, 0.004029424553624935, 0.022281555146831552, 0.612187126487593, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.001 64 0] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_102 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_102 (ReLU)               (None, 1980)         0           ['batch_normalization_102[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           186214      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           186214      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2073409     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,322,877
Trainable params: 4,314,769
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 01:03:28.177619: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_102/moving_mean/Assign' id:43633 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_102/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_102/moving_mean, batch_normalization_102/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:03:39.315543: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_34/mul' id:44053 op device:{requested: '', assigned: ''} def:{{{node loss_34/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_34/mul/x, loss_34/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 17s - loss: 0.0721 - val_loss: 0.0697 - 17s/epoch - 197us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0698 - val_loss: 0.0683 - 8s/epoch - 96us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0770 - val_loss: 85.0614 - 8s/epoch - 96us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0932 - val_loss: 0.0703 - 8s/epoch - 96us/sample
Epoch 5/85
84077/84077 - 8s - loss: 0.0692 - val_loss: 0.0688 - 8s/epoch - 96us/sample
Epoch 6/85
84077/84077 - 8s - loss: 0.0678 - val_loss: 0.0680 - 8s/epoch - 96us/sample
Epoch 7/85
84077/84077 - 8s - loss: 0.0673 - val_loss: 0.0677 - 8s/epoch - 96us/sample
Epoch 8/85
84077/84077 - 8s - loss: 0.0671 - val_loss: 0.0676 - 8s/epoch - 97us/sample
Epoch 9/85
84077/84077 - 8s - loss: 0.0670 - val_loss: 0.0674 - 8s/epoch - 96us/sample
Epoch 10/85
84077/84077 - 8s - loss: 0.0670 - val_loss: 0.0674 - 8s/epoch - 96us/sample
Epoch 11/85
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 96us/sample
Epoch 12/85
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 96us/sample
Epoch 13/85
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 96us/sample
Epoch 14/85
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 96us/sample
Epoch 15/85
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 96us/sample
Epoch 16/85
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 97us/sample
Epoch 17/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0674 - 8s/epoch - 97us/sample
Epoch 18/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 19/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 20/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 21/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 22/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 23/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 24/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 25/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 26/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 27/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 28/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 29/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 30/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 31/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 32/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 33/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 34/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 35/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 36/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 37/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 38/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 39/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 40/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 41/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 42/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 43/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 44/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 45/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 46/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 47/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 48/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 49/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 50/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 51/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 52/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 53/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 54/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 55/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 56/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 99us/sample
Epoch 57/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 58/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 59/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 60/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 61/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 62/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 63/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 64/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 65/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 66/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 67/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 68/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 69/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 70/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 71/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 72/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 73/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 74/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 75/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 76/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 77/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 78/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 79/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 80/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 97us/sample
Epoch 81/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 82/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 83/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 84/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
Epoch 85/85
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 96us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.06734573959927824
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:15:02.364822: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_34/outputlayer/BiasAdd' id:44005 op device:{requested: '', assigned: ''} def:{{{node decoder_model_34/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_34/outputlayer/MatMul, decoder_model_34/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.183380747911029
cosine 1.170462839693321
MAE: 5.836446939396636
RMSE: 6.628074602792658
r2: -32540.01141202409
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'binary_crossentropy', 64, 85, 0.001, 0.1, 94, 0.0668369209958366, 0.06734573959927824, 1.183380747911029, 1.170462839693321, 5.836446939396636, 6.628074602792658, -32540.01141202409, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0007 32 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_105 (Batch  (None, 2357)        9428        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_105 (ReLU)               (None, 2357)         0           ['batch_normalization_105[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 01:15:19.223116: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_35/bias/Assign' id:45054 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_35/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_35/bias, bottleneck_zlog_35/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:15:34.891742: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_35/mul' id:45378 op device:{requested: '', assigned: ''} def:{{{node loss_35/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_35/mul/x, loss_35/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0035 - val_loss: 9.7415e-04 - 22s/epoch - 256us/sample
Epoch 2/50
84077/84077 - 13s - loss: 0.0013 - val_loss: 6.8433e-04 - 13s/epoch - 150us/sample
Epoch 3/50
84077/84077 - 13s - loss: 6.5371e-04 - val_loss: 5.8700e-04 - 13s/epoch - 150us/sample
Epoch 4/50
84077/84077 - 13s - loss: 5.3161e-04 - val_loss: 4.9482e-04 - 13s/epoch - 151us/sample
Epoch 5/50
84077/84077 - 13s - loss: 5.0045e-04 - val_loss: 4.8452e-04 - 13s/epoch - 151us/sample
Epoch 6/50
84077/84077 - 13s - loss: 4.8166e-04 - val_loss: 4.6641e-04 - 13s/epoch - 152us/sample
Epoch 7/50
84077/84077 - 13s - loss: 4.7289e-04 - val_loss: 4.5847e-04 - 13s/epoch - 151us/sample
Epoch 8/50
84077/84077 - 13s - loss: 4.6567e-04 - val_loss: 4.5444e-04 - 13s/epoch - 151us/sample
Epoch 9/50
84077/84077 - 13s - loss: 4.6120e-04 - val_loss: 4.4994e-04 - 13s/epoch - 150us/sample
Epoch 10/50
84077/84077 - 13s - loss: 4.5914e-04 - val_loss: 4.4799e-04 - 13s/epoch - 150us/sample
Epoch 11/50
84077/84077 - 13s - loss: 4.5780e-04 - val_loss: 4.4702e-04 - 13s/epoch - 151us/sample
Epoch 12/50
84077/84077 - 13s - loss: 4.5713e-04 - val_loss: 4.4768e-04 - 13s/epoch - 151us/sample
Epoch 13/50
84077/84077 - 13s - loss: 4.5648e-04 - val_loss: 4.6276e-04 - 13s/epoch - 150us/sample
Epoch 14/50
84077/84077 - 13s - loss: 4.5599e-04 - val_loss: 4.4710e-04 - 13s/epoch - 150us/sample
Epoch 15/50
84077/84077 - 13s - loss: 4.5525e-04 - val_loss: 4.4538e-04 - 13s/epoch - 150us/sample
Epoch 16/50
84077/84077 - 13s - loss: 4.5490e-04 - val_loss: 4.5179e-04 - 13s/epoch - 150us/sample
Epoch 17/50
84077/84077 - 13s - loss: 4.5462e-04 - val_loss: 4.4863e-04 - 13s/epoch - 152us/sample
Epoch 18/50
84077/84077 - 13s - loss: 4.5470e-04 - val_loss: 4.4909e-04 - 13s/epoch - 151us/sample
Epoch 19/50
84077/84077 - 13s - loss: 4.5435e-04 - val_loss: 4.4760e-04 - 13s/epoch - 150us/sample
Epoch 20/50
84077/84077 - 13s - loss: 4.5389e-04 - val_loss: 4.5716e-04 - 13s/epoch - 150us/sample
Epoch 21/50
84077/84077 - 13s - loss: 4.5383e-04 - val_loss: 4.4602e-04 - 13s/epoch - 150us/sample
Epoch 22/50
84077/84077 - 13s - loss: 4.5393e-04 - val_loss: 4.5441e-04 - 13s/epoch - 151us/sample
Epoch 23/50
84077/84077 - 13s - loss: 4.5391e-04 - val_loss: 4.4684e-04 - 13s/epoch - 151us/sample
Epoch 24/50
84077/84077 - 13s - loss: 4.5356e-04 - val_loss: 4.5510e-04 - 13s/epoch - 151us/sample
Epoch 25/50
84077/84077 - 13s - loss: 4.5348e-04 - val_loss: 4.5716e-04 - 13s/epoch - 150us/sample
Epoch 26/50
84077/84077 - 13s - loss: 4.5322e-04 - val_loss: 4.5110e-04 - 13s/epoch - 150us/sample
Epoch 27/50
84077/84077 - 13s - loss: 4.5301e-04 - val_loss: 4.5585e-04 - 13s/epoch - 150us/sample
Epoch 28/50
84077/84077 - 13s - loss: 4.5338e-04 - val_loss: 4.5904e-04 - 13s/epoch - 152us/sample
Epoch 29/50
84077/84077 - 13s - loss: 4.5284e-04 - val_loss: 4.4592e-04 - 13s/epoch - 151us/sample
Epoch 30/50
84077/84077 - 13s - loss: 4.5264e-04 - val_loss: 4.4626e-04 - 13s/epoch - 151us/sample
Epoch 31/50
84077/84077 - 13s - loss: 4.5298e-04 - val_loss: 4.4480e-04 - 13s/epoch - 150us/sample
Epoch 32/50
84077/84077 - 13s - loss: 4.5269e-04 - val_loss: 4.5724e-04 - 13s/epoch - 150us/sample
Epoch 33/50
84077/84077 - 13s - loss: 4.5266e-04 - val_loss: 4.5123e-04 - 13s/epoch - 152us/sample
Epoch 34/50
84077/84077 - 13s - loss: 4.5253e-04 - val_loss: 4.4933e-04 - 13s/epoch - 151us/sample
Epoch 35/50
84077/84077 - 13s - loss: 4.5229e-04 - val_loss: 4.5478e-04 - 13s/epoch - 150us/sample
Epoch 36/50
84077/84077 - 13s - loss: 4.5228e-04 - val_loss: 4.5645e-04 - 13s/epoch - 150us/sample
Epoch 37/50
84077/84077 - 13s - loss: 4.5256e-04 - val_loss: 4.5491e-04 - 13s/epoch - 150us/sample
Epoch 38/50
84077/84077 - 13s - loss: 4.5189e-04 - val_loss: 4.4936e-04 - 13s/epoch - 151us/sample
Epoch 39/50
84077/84077 - 13s - loss: 4.5181e-04 - val_loss: 4.4331e-04 - 13s/epoch - 152us/sample
Epoch 40/50
84077/84077 - 13s - loss: 4.5225e-04 - val_loss: 4.4607e-04 - 13s/epoch - 151us/sample
Epoch 41/50
84077/84077 - 13s - loss: 4.5191e-04 - val_loss: 4.4700e-04 - 13s/epoch - 150us/sample
Epoch 42/50
84077/84077 - 13s - loss: 4.5214e-04 - val_loss: 4.4786e-04 - 13s/epoch - 150us/sample
Epoch 43/50
84077/84077 - 13s - loss: 4.5225e-04 - val_loss: 4.5803e-04 - 13s/epoch - 150us/sample
Epoch 44/50
84077/84077 - 13s - loss: 4.5177e-04 - val_loss: 4.5978e-04 - 13s/epoch - 151us/sample
Epoch 45/50
84077/84077 - 13s - loss: 4.5191e-04 - val_loss: 4.5542e-04 - 13s/epoch - 152us/sample
Epoch 46/50
84077/84077 - 13s - loss: 4.5175e-04 - val_loss: 4.4901e-04 - 13s/epoch - 151us/sample
Epoch 47/50
84077/84077 - 13s - loss: 4.5143e-04 - val_loss: 4.5051e-04 - 13s/epoch - 150us/sample
Epoch 48/50
84077/84077 - 13s - loss: 4.5186e-04 - val_loss: 4.4327e-04 - 13s/epoch - 151us/sample
Epoch 49/50
84077/84077 - 13s - loss: 4.5148e-04 - val_loss: 4.8219e-04 - 13s/epoch - 151us/sample
Epoch 50/50
84077/84077 - 13s - loss: 4.5195e-04 - val_loss: 4.5046e-04 - 13s/epoch - 152us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00045046400004105405
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:25:59.002961: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_35/outputlayer/BiasAdd' id:45342 op device:{requested: '', assigned: ''} def:{{{node decoder_model_35/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_35/outputlayer/MatMul, decoder_model_35/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2871048320036793
cosine 0.2827074461499406
MAE: 0.0056839449975395065
RMSE: 0.02914753994961641
r2: 0.3363153220584473
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 32, 50, 0.0007, 0.1, 94, 0.0004519463393897185, 0.00045046400004105405, 0.2871048320036793, 0.2827074461499406, 0.0056839449975395065, 0.02914753994961641, 0.3363153220584473, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 50 0.001 16 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2451)         2313744     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_108 (Batch  (None, 2451)        9804        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_108 (ReLU)               (None, 2451)         0           ['batch_normalization_108[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           230488      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           230488      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2564191     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,348,715
Trainable params: 5,338,723
Non-trainable params: 9,992
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 01:26:15.445975: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_36/kernel/Assign' id:46535 op device:{requested: '', assigned: ''} def:{{{node outputlayer_36/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_36/kernel, outputlayer_36/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:26:40.099858: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_36/mul' id:46663 op device:{requested: '', assigned: ''} def:{{{node loss_36/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_36/mul/x, loss_36/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 31s - loss: 0.0027 - val_loss: 7.4918e-04 - 31s/epoch - 372us/sample
Epoch 2/50
84077/84077 - 22s - loss: 6.4244e-04 - val_loss: 5.5367e-04 - 22s/epoch - 263us/sample
Epoch 3/50
84077/84077 - 22s - loss: 5.1688e-04 - val_loss: 4.7734e-04 - 22s/epoch - 261us/sample
Epoch 4/50
84077/84077 - 22s - loss: 4.7943e-04 - val_loss: 5.2295e-04 - 22s/epoch - 262us/sample
Epoch 5/50
84077/84077 - 22s - loss: 4.6725e-04 - val_loss: 5.9551e-04 - 22s/epoch - 262us/sample
Epoch 6/50
84077/84077 - 22s - loss: 4.6091e-04 - val_loss: 6.1576e-04 - 22s/epoch - 262us/sample
Epoch 7/50
84077/84077 - 22s - loss: 4.5607e-04 - val_loss: 6.0919e-04 - 22s/epoch - 264us/sample
Epoch 8/50
84077/84077 - 22s - loss: 4.5256e-04 - val_loss: 5.5752e-04 - 22s/epoch - 261us/sample
Epoch 9/50
84077/84077 - 22s - loss: 4.5081e-04 - val_loss: 7.0462e-04 - 22s/epoch - 262us/sample
Epoch 10/50
84077/84077 - 22s - loss: 4.5015e-04 - val_loss: 5.5769e-04 - 22s/epoch - 263us/sample
Epoch 11/50
84077/84077 - 22s - loss: 4.4918e-04 - val_loss: 6.1800e-04 - 22s/epoch - 262us/sample
Epoch 12/50
84077/84077 - 22s - loss: 4.4810e-04 - val_loss: 6.7604e-04 - 22s/epoch - 262us/sample
Epoch 13/50
84077/84077 - 22s - loss: 4.4730e-04 - val_loss: 6.2733e-04 - 22s/epoch - 263us/sample
Epoch 14/50
84077/84077 - 22s - loss: 4.4652e-04 - val_loss: 6.2282e-04 - 22s/epoch - 263us/sample
Epoch 15/50
84077/84077 - 22s - loss: 4.4612e-04 - val_loss: 5.3199e-04 - 22s/epoch - 262us/sample
Epoch 16/50
84077/84077 - 22s - loss: 4.4479e-04 - val_loss: 5.2675e-04 - 22s/epoch - 261us/sample
Epoch 17/50
84077/84077 - 22s - loss: 4.4426e-04 - val_loss: 5.6534e-04 - 22s/epoch - 263us/sample
Epoch 18/50
84077/84077 - 22s - loss: 4.4181e-04 - val_loss: 5.8585e-04 - 22s/epoch - 262us/sample
Epoch 19/50
84077/84077 - 22s - loss: 4.4048e-04 - val_loss: 6.6392e-04 - 22s/epoch - 261us/sample
Epoch 20/50
84077/84077 - 22s - loss: 4.3898e-04 - val_loss: 7.0337e-04 - 22s/epoch - 264us/sample
Epoch 21/50
84077/84077 - 22s - loss: 4.3751e-04 - val_loss: 6.0905e-04 - 22s/epoch - 262us/sample
Epoch 22/50
84077/84077 - 22s - loss: 4.3678e-04 - val_loss: 5.9408e-04 - 22s/epoch - 261us/sample
Epoch 23/50
84077/84077 - 22s - loss: 4.3640e-04 - val_loss: 5.9410e-04 - 22s/epoch - 263us/sample
Epoch 24/50
84077/84077 - 22s - loss: 4.3555e-04 - val_loss: 6.7895e-04 - 22s/epoch - 262us/sample
Epoch 25/50
84077/84077 - 22s - loss: 4.3488e-04 - val_loss: 5.2058e-04 - 22s/epoch - 262us/sample
Epoch 26/50
84077/84077 - 22s - loss: 4.3501e-04 - val_loss: 5.8993e-04 - 22s/epoch - 264us/sample
Epoch 27/50
84077/84077 - 22s - loss: 4.3376e-04 - val_loss: 5.4116e-04 - 22s/epoch - 263us/sample
Epoch 28/50
84077/84077 - 22s - loss: 4.3375e-04 - val_loss: 5.5197e-04 - 22s/epoch - 263us/sample
Epoch 29/50
84077/84077 - 22s - loss: 4.3352e-04 - val_loss: 5.6740e-04 - 22s/epoch - 264us/sample
Epoch 30/50
84077/84077 - 22s - loss: 4.3277e-04 - val_loss: 5.2461e-04 - 22s/epoch - 262us/sample
Epoch 31/50
84077/84077 - 22s - loss: 4.3298e-04 - val_loss: 5.2262e-04 - 22s/epoch - 261us/sample
Epoch 32/50
84077/84077 - 22s - loss: 4.3327e-04 - val_loss: 5.4492e-04 - 22s/epoch - 263us/sample
Epoch 33/50
84077/84077 - 22s - loss: 4.3263e-04 - val_loss: 5.2739e-04 - 22s/epoch - 262us/sample
Epoch 34/50
84077/84077 - 22s - loss: 4.3276e-04 - val_loss: 6.0244e-04 - 22s/epoch - 261us/sample
Epoch 35/50
84077/84077 - 22s - loss: 4.3222e-04 - val_loss: 5.3505e-04 - 22s/epoch - 262us/sample
Epoch 36/50
84077/84077 - 22s - loss: 4.3271e-04 - val_loss: 5.4074e-04 - 22s/epoch - 264us/sample
Epoch 37/50
84077/84077 - 22s - loss: 4.3255e-04 - val_loss: 4.9452e-04 - 22s/epoch - 262us/sample
Epoch 38/50
84077/84077 - 22s - loss: 4.3222e-04 - val_loss: 5.1275e-04 - 22s/epoch - 262us/sample
Epoch 39/50
84077/84077 - 22s - loss: 4.3238e-04 - val_loss: 4.8317e-04 - 22s/epoch - 263us/sample
Epoch 40/50
84077/84077 - 22s - loss: 4.3208e-04 - val_loss: 4.7032e-04 - 22s/epoch - 262us/sample
Epoch 41/50
84077/84077 - 22s - loss: 4.3237e-04 - val_loss: 5.0801e-04 - 22s/epoch - 262us/sample
Epoch 42/50
84077/84077 - 22s - loss: 4.3188e-04 - val_loss: 5.3169e-04 - 22s/epoch - 264us/sample
Epoch 43/50
84077/84077 - 22s - loss: 4.3201e-04 - val_loss: 4.9240e-04 - 22s/epoch - 262us/sample
Epoch 44/50
84077/84077 - 22s - loss: 4.3190e-04 - val_loss: 5.2709e-04 - 22s/epoch - 261us/sample
Epoch 45/50
84077/84077 - 22s - loss: 4.3204e-04 - val_loss: 4.7634e-04 - 22s/epoch - 262us/sample
Epoch 46/50
84077/84077 - 22s - loss: 4.3145e-04 - val_loss: 5.0339e-04 - 22s/epoch - 263us/sample
Epoch 47/50
84077/84077 - 22s - loss: 4.3183e-04 - val_loss: 4.8285e-04 - 22s/epoch - 261us/sample
Epoch 48/50
84077/84077 - 22s - loss: 4.3120e-04 - val_loss: 5.0296e-04 - 22s/epoch - 263us/sample
Epoch 49/50
84077/84077 - 22s - loss: 4.3161e-04 - val_loss: 4.9187e-04 - 22s/epoch - 262us/sample
Epoch 50/50
84077/84077 - 22s - loss: 4.3086e-04 - val_loss: 4.8063e-04 - 22s/epoch - 261us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00048062938131142574
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:44:44.843262: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_36/outputlayer/BiasAdd' id:46627 op device:{requested: '', assigned: ''} def:{{{node decoder_model_36/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_36/outputlayer/MatMul, decoder_model_36/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.24889210527231437
cosine 0.2452783304902052
MAE: 0.005688741168768662
RMSE: 0.030194602612060004
r2: 0.28777242032888967
RMSE zero-vector: 0.04004287452915337
['2.6custom_VAE', 'logcosh', 16, 50, 0.001, 0.1, 94, 0.0004308589121947571, 0.00048062938131142574, 0.24889210527231437, 0.2452783304902052, 0.005688741168768662, 0.030194602612060004, 0.28777242032888967, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 16 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_111 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_111 (ReLU)               (None, 1886)         0           ['batch_normalization_111[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 01:45:02.736027: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_113/beta/Assign' id:47757 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_113/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_113/beta, batch_normalization_113/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:45:27.667907: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_37/mul' id:47948 op device:{requested: '', assigned: ''} def:{{{node loss_37/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_37/mul/x, loss_37/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 32s - loss: 0.0024 - val_loss: 7.1083e-04 - 32s/epoch - 380us/sample
Epoch 2/85
84077/84077 - 22s - loss: 6.6483e-04 - val_loss: 5.2001e-04 - 22s/epoch - 262us/sample
Epoch 3/85
84077/84077 - 22s - loss: 5.1466e-04 - val_loss: 4.7838e-04 - 22s/epoch - 263us/sample
Epoch 4/85
84077/84077 - 22s - loss: 4.8258e-04 - val_loss: 4.6173e-04 - 22s/epoch - 263us/sample
Epoch 5/85
84077/84077 - 22s - loss: 4.7012e-04 - val_loss: 4.6006e-04 - 22s/epoch - 262us/sample
Epoch 6/85
84077/84077 - 22s - loss: 4.6344e-04 - val_loss: 4.6043e-04 - 22s/epoch - 263us/sample
Epoch 7/85
84077/84077 - 22s - loss: 4.5996e-04 - val_loss: 4.8423e-04 - 22s/epoch - 262us/sample
Epoch 8/85
84077/84077 - 22s - loss: 4.5777e-04 - val_loss: 4.6802e-04 - 22s/epoch - 261us/sample
Epoch 9/85
84077/84077 - 22s - loss: 4.5641e-04 - val_loss: 4.6352e-04 - 22s/epoch - 262us/sample
Epoch 10/85
84077/84077 - 22s - loss: 4.5462e-04 - val_loss: 4.7143e-04 - 22s/epoch - 263us/sample
Epoch 11/85
84077/84077 - 22s - loss: 4.5381e-04 - val_loss: 4.5665e-04 - 22s/epoch - 261us/sample
Epoch 12/85
84077/84077 - 22s - loss: 4.5317e-04 - val_loss: 4.5104e-04 - 22s/epoch - 262us/sample
Epoch 13/85
84077/84077 - 22s - loss: 4.5307e-04 - val_loss: 4.5621e-04 - 22s/epoch - 264us/sample
Epoch 14/85
84077/84077 - 22s - loss: 4.5223e-04 - val_loss: 4.4316e-04 - 22s/epoch - 261us/sample
Epoch 15/85
84077/84077 - 22s - loss: 4.5189e-04 - val_loss: 4.5016e-04 - 22s/epoch - 261us/sample
Epoch 16/85
84077/84077 - 22s - loss: 4.5131e-04 - val_loss: 4.4533e-04 - 22s/epoch - 263us/sample
Epoch 17/85
84077/84077 - 22s - loss: 4.5101e-04 - val_loss: 4.5354e-04 - 22s/epoch - 264us/sample
Epoch 18/85
84077/84077 - 22s - loss: 4.5095e-04 - val_loss: 4.4983e-04 - 22s/epoch - 263us/sample
Epoch 19/85
84077/84077 - 22s - loss: 4.4990e-04 - val_loss: 4.6194e-04 - 22s/epoch - 262us/sample
Epoch 20/85
84077/84077 - 22s - loss: 4.4889e-04 - val_loss: 4.4678e-04 - 22s/epoch - 262us/sample
Epoch 21/85
84077/84077 - 22s - loss: 4.4771e-04 - val_loss: 4.4647e-04 - 22s/epoch - 262us/sample
Epoch 22/85
84077/84077 - 22s - loss: 4.4626e-04 - val_loss: 4.5390e-04 - 22s/epoch - 262us/sample
Epoch 23/85
84077/84077 - 22s - loss: 4.4554e-04 - val_loss: 4.5041e-04 - 22s/epoch - 264us/sample
Epoch 24/85
84077/84077 - 22s - loss: 4.4514e-04 - val_loss: 4.3906e-04 - 22s/epoch - 262us/sample
Epoch 25/85
84077/84077 - 22s - loss: 4.4488e-04 - val_loss: 4.4058e-04 - 22s/epoch - 262us/sample
Epoch 26/85
84077/84077 - 22s - loss: 4.4437e-04 - val_loss: 4.4727e-04 - 22s/epoch - 263us/sample
Epoch 27/85
84077/84077 - 22s - loss: 4.4448e-04 - val_loss: 4.4459e-04 - 22s/epoch - 263us/sample
Epoch 28/85
84077/84077 - 22s - loss: 4.4465e-04 - val_loss: 4.4968e-04 - 22s/epoch - 261us/sample
Epoch 29/85
84077/84077 - 22s - loss: 4.4451e-04 - val_loss: 4.7363e-04 - 22s/epoch - 263us/sample
Epoch 30/85
84077/84077 - 22s - loss: 4.4427e-04 - val_loss: 4.7119e-04 - 22s/epoch - 263us/sample
Epoch 31/85
84077/84077 - 22s - loss: 4.4384e-04 - val_loss: 4.7483e-04 - 22s/epoch - 262us/sample
Epoch 32/85
84077/84077 - 22s - loss: 4.4385e-04 - val_loss: 4.7171e-04 - 22s/epoch - 263us/sample
Epoch 33/85
84077/84077 - 22s - loss: 4.4397e-04 - val_loss: 4.6764e-04 - 22s/epoch - 263us/sample
Epoch 34/85
84077/84077 - 22s - loss: 4.4375e-04 - val_loss: 4.4539e-04 - 22s/epoch - 261us/sample
Epoch 35/85
84077/84077 - 22s - loss: 4.4356e-04 - val_loss: 4.5197e-04 - 22s/epoch - 262us/sample
Epoch 36/85
84077/84077 - 22s - loss: 4.4408e-04 - val_loss: 4.3946e-04 - 22s/epoch - 262us/sample
Epoch 37/85
84077/84077 - 22s - loss: 4.4361e-04 - val_loss: 4.4823e-04 - 22s/epoch - 261us/sample
Epoch 38/85
84077/84077 - 22s - loss: 4.4347e-04 - val_loss: 4.4504e-04 - 22s/epoch - 262us/sample
Epoch 39/85
84077/84077 - 22s - loss: 4.4393e-04 - val_loss: 4.4104e-04 - 22s/epoch - 263us/sample
Epoch 40/85
84077/84077 - 22s - loss: 4.4343e-04 - val_loss: 4.3938e-04 - 22s/epoch - 261us/sample
Epoch 41/85
84077/84077 - 22s - loss: 4.4311e-04 - val_loss: 4.3953e-04 - 22s/epoch - 262us/sample
Epoch 42/85
84077/84077 - 22s - loss: 4.4331e-04 - val_loss: 4.5024e-04 - 22s/epoch - 263us/sample
Epoch 43/85
84077/84077 - 22s - loss: 4.4345e-04 - val_loss: 4.5756e-04 - 22s/epoch - 262us/sample
Epoch 44/85
84077/84077 - 22s - loss: 4.4318e-04 - val_loss: 4.6643e-04 - 22s/epoch - 262us/sample
Epoch 45/85
84077/84077 - 22s - loss: 4.4290e-04 - val_loss: 4.4182e-04 - 22s/epoch - 264us/sample
Epoch 46/85
84077/84077 - 22s - loss: 4.4339e-04 - val_loss: 4.4793e-04 - 22s/epoch - 262us/sample
Epoch 47/85
84077/84077 - 22s - loss: 4.4321e-04 - val_loss: 4.7384e-04 - 22s/epoch - 262us/sample
Epoch 48/85
84077/84077 - 22s - loss: 4.4292e-04 - val_loss: 4.5812e-04 - 22s/epoch - 263us/sample
Epoch 49/85
84077/84077 - 22s - loss: 4.4316e-04 - val_loss: 4.7787e-04 - 22s/epoch - 262us/sample
Epoch 50/85
84077/84077 - 22s - loss: 4.4321e-04 - val_loss: 4.9723e-04 - 22s/epoch - 261us/sample
Epoch 51/85
84077/84077 - 22s - loss: 4.4294e-04 - val_loss: 4.8897e-04 - 22s/epoch - 262us/sample
Epoch 52/85
84077/84077 - 22s - loss: 4.4335e-04 - val_loss: 4.7963e-04 - 22s/epoch - 263us/sample
Epoch 53/85
84077/84077 - 22s - loss: 4.4289e-04 - val_loss: 4.4641e-04 - 22s/epoch - 261us/sample
Epoch 54/85
84077/84077 - 22s - loss: 4.4242e-04 - val_loss: 4.4713e-04 - 22s/epoch - 262us/sample
Epoch 55/85
84077/84077 - 22s - loss: 4.4312e-04 - val_loss: 4.7690e-04 - 22s/epoch - 263us/sample
Epoch 56/85
84077/84077 - 22s - loss: 4.4256e-04 - val_loss: 4.4364e-04 - 22s/epoch - 262us/sample
Epoch 57/85
84077/84077 - 22s - loss: 4.4233e-04 - val_loss: 4.5824e-04 - 22s/epoch - 262us/sample
Epoch 58/85
84077/84077 - 22s - loss: 4.4265e-04 - val_loss: 4.6192e-04 - 22s/epoch - 263us/sample
Epoch 59/85
84077/84077 - 22s - loss: 4.4277e-04 - val_loss: 4.7678e-04 - 22s/epoch - 263us/sample
Epoch 60/85
84077/84077 - 22s - loss: 4.4260e-04 - val_loss: 4.5203e-04 - 22s/epoch - 261us/sample
Epoch 61/85
84077/84077 - 22s - loss: 4.4300e-04 - val_loss: 4.4774e-04 - 22s/epoch - 262us/sample
Epoch 62/85
84077/84077 - 22s - loss: 4.4268e-04 - val_loss: 4.6522e-04 - 22s/epoch - 263us/sample
Epoch 63/85
84077/84077 - 22s - loss: 4.4275e-04 - val_loss: 4.4496e-04 - 22s/epoch - 261us/sample
Epoch 64/85
84077/84077 - 22s - loss: 4.4289e-04 - val_loss: 4.9009e-04 - 22s/epoch - 263us/sample
Epoch 65/85
84077/84077 - 22s - loss: 4.4243e-04 - val_loss: 4.4986e-04 - 22s/epoch - 262us/sample
Epoch 66/85
84077/84077 - 22s - loss: 4.4241e-04 - val_loss: 4.6205e-04 - 22s/epoch - 261us/sample
Epoch 67/85
84077/84077 - 22s - loss: 4.4270e-04 - val_loss: 4.6293e-04 - 22s/epoch - 263us/sample
Epoch 68/85
84077/84077 - 22s - loss: 4.4244e-04 - val_loss: 4.4546e-04 - 22s/epoch - 263us/sample
Epoch 69/85
84077/84077 - 22s - loss: 4.4261e-04 - val_loss: 4.7552e-04 - 22s/epoch - 261us/sample
Epoch 70/85
84077/84077 - 22s - loss: 4.4250e-04 - val_loss: 4.5452e-04 - 22s/epoch - 262us/sample
Epoch 71/85
84077/84077 - 22s - loss: 4.4235e-04 - val_loss: 4.5989e-04 - 22s/epoch - 264us/sample
Epoch 72/85
84077/84077 - 22s - loss: 4.4277e-04 - val_loss: 4.6014e-04 - 22s/epoch - 262us/sample
Epoch 73/85
84077/84077 - 22s - loss: 4.4251e-04 - val_loss: 4.7516e-04 - 22s/epoch - 262us/sample
Epoch 74/85
84077/84077 - 22s - loss: 4.4270e-04 - val_loss: 4.7457e-04 - 22s/epoch - 263us/sample
Epoch 75/85
84077/84077 - 22s - loss: 4.4252e-04 - val_loss: 4.6872e-04 - 22s/epoch - 262us/sample
Epoch 76/85
84077/84077 - 22s - loss: 4.4256e-04 - val_loss: 4.5154e-04 - 22s/epoch - 261us/sample
Epoch 77/85
84077/84077 - 22s - loss: 4.4218e-04 - val_loss: 4.6613e-04 - 22s/epoch - 262us/sample
Epoch 78/85
84077/84077 - 22s - loss: 4.4247e-04 - val_loss: 4.5509e-04 - 22s/epoch - 262us/sample
Epoch 79/85
84077/84077 - 22s - loss: 4.4216e-04 - val_loss: 4.5942e-04 - 22s/epoch - 261us/sample
Epoch 80/85
84077/84077 - 22s - loss: 4.4236e-04 - val_loss: 4.7538e-04 - 22s/epoch - 261us/sample
Epoch 81/85
84077/84077 - 22s - loss: 4.4223e-04 - val_loss: 4.9807e-04 - 22s/epoch - 263us/sample
Epoch 82/85
84077/84077 - 22s - loss: 4.4213e-04 - val_loss: 4.4830e-04 - 22s/epoch - 261us/sample
Epoch 83/85
84077/84077 - 22s - loss: 4.4238e-04 - val_loss: 4.5618e-04 - 22s/epoch - 263us/sample
Epoch 84/85
84077/84077 - 22s - loss: 4.4207e-04 - val_loss: 4.5897e-04 - 22s/epoch - 263us/sample
Epoch 85/85
84077/84077 - 22s - loss: 4.4176e-04 - val_loss: 4.5039e-04 - 22s/epoch - 262us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0004503898911151295
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:16:24.202734: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_37/outputlayer/BiasAdd' id:47912 op device:{requested: '', assigned: ''} def:{{{node decoder_model_37/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_37/outputlayer/MatMul, decoder_model_37/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.27735408800890604
cosine 0.2731991404111299
MAE: 0.006065102661865702
RMSE: 0.028865295493973105
r2: 0.34909850381854785
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 16, 85, 0.001, 0.1, 94, 0.0004417647704630837, 0.0004503898911151295, 0.27735408800890604, 0.2731991404111299, 0.006065102661865702, 0.028865295493973105, 0.34909850381854785, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.001 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_114 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_114 (ReLU)               (None, 1791)         0           ['batch_normalization_114[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 02:16:42.750897: W tensorflow/c/c_api.cc:291] Operation '{name:'training_76/Adam/iter/Assign' id:49682 op device:{requested: '', assigned: ''} def:{{{node training_76/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_76/Adam/iter, training_76/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:16:54.523342: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_38/mul' id:49233 op device:{requested: '', assigned: ''} def:{{{node loss_38/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_38/mul/x, loss_38/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0041 - val_loss: 9.2447e-04 - 18s/epoch - 215us/sample
Epoch 2/85
84077/84077 - 8s - loss: 9.9332e-04 - val_loss: 9.1447e-04 - 8s/epoch - 96us/sample
Epoch 3/85
84077/84077 - 8s - loss: 74.1178 - val_loss: 8.5862e-04 - 8s/epoch - 95us/sample
Epoch 4/85
84077/84077 - 8s - loss: 8.6726e-04 - val_loss: 6.8351e-04 - 8s/epoch - 96us/sample
Epoch 5/85
84077/84077 - 8s - loss: 6.6746e-04 - val_loss: 5.9788e-04 - 8s/epoch - 96us/sample
Epoch 6/85
84077/84077 - 8s - loss: 6.0151e-04 - val_loss: 5.3870e-04 - 8s/epoch - 96us/sample
Epoch 7/85
84077/84077 - 8s - loss: 4.9901e-04 - val_loss: 4.6809e-04 - 8s/epoch - 96us/sample
Epoch 8/85
84077/84077 - 8s - loss: 4.7277e-04 - val_loss: 4.7278e-04 - 8s/epoch - 96us/sample
Epoch 9/85
84077/84077 - 8s - loss: 0.0014 - val_loss: 4.1881e-04 - 8s/epoch - 96us/sample
Epoch 10/85
84077/84077 - 8s - loss: 4.3017e-04 - val_loss: 3.9506e-04 - 8s/epoch - 97us/sample
Epoch 11/85
84077/84077 - 8s - loss: 4.0692e-04 - val_loss: 3.7520e-04 - 8s/epoch - 96us/sample
Epoch 12/85
84077/84077 - 8s - loss: 5.9350e-04 - val_loss: 3.9942e-04 - 8s/epoch - 96us/sample
Epoch 13/85
84077/84077 - 8s - loss: 3.9184e-04 - val_loss: 3.5861e-04 - 8s/epoch - 96us/sample
Epoch 14/85
84077/84077 - 8s - loss: 3.7367e-04 - val_loss: 3.5422e-04 - 8s/epoch - 95us/sample
Epoch 15/85
84077/84077 - 8s - loss: 3.6554e-04 - val_loss: 3.4263e-04 - 8s/epoch - 96us/sample
Epoch 16/85
84077/84077 - 8s - loss: 3.5687e-04 - val_loss: 3.3477e-04 - 8s/epoch - 96us/sample
Epoch 17/85
84077/84077 - 8s - loss: 3.5229e-04 - val_loss: 3.2544e-04 - 8s/epoch - 96us/sample
Epoch 18/85
84077/84077 - 8s - loss: 3.3955e-04 - val_loss: 3.1609e-04 - 8s/epoch - 97us/sample
Epoch 19/85
84077/84077 - 8s - loss: 3.6130e-04 - val_loss: 3.1596e-04 - 8s/epoch - 96us/sample
Epoch 20/85
84077/84077 - 8s - loss: 3.2899e-04 - val_loss: 3.0962e-04 - 8s/epoch - 96us/sample
Epoch 21/85
84077/84077 - 8s - loss: 3.2467e-04 - val_loss: 3.0520e-04 - 8s/epoch - 95us/sample
Epoch 22/85
84077/84077 - 8s - loss: 3.2092e-04 - val_loss: 3.0577e-04 - 8s/epoch - 96us/sample
Epoch 23/85
84077/84077 - 8s - loss: 3.1735e-04 - val_loss: 2.9688e-04 - 8s/epoch - 96us/sample
Epoch 24/85
84077/84077 - 8s - loss: 3.1471e-04 - val_loss: 2.9582e-04 - 8s/epoch - 96us/sample
Epoch 25/85
84077/84077 - 8s - loss: 3.1193e-04 - val_loss: 2.9549e-04 - 8s/epoch - 96us/sample
Epoch 26/85
84077/84077 - 8s - loss: 3.1004e-04 - val_loss: 2.9142e-04 - 8s/epoch - 97us/sample
Epoch 27/85
84077/84077 - 8s - loss: 3.0745e-04 - val_loss: 2.9171e-04 - 8s/epoch - 96us/sample
Epoch 28/85
84077/84077 - 8s - loss: 3.0442e-04 - val_loss: 2.8559e-04 - 8s/epoch - 96us/sample
Epoch 29/85
84077/84077 - 8s - loss: 3.0161e-04 - val_loss: 2.8215e-04 - 8s/epoch - 96us/sample
Epoch 30/85
84077/84077 - 8s - loss: 2.9957e-04 - val_loss: 2.8176e-04 - 8s/epoch - 96us/sample
Epoch 31/85
84077/84077 - 8s - loss: 2.9733e-04 - val_loss: 2.8073e-04 - 8s/epoch - 96us/sample
Epoch 32/85
84077/84077 - 8s - loss: 2.9497e-04 - val_loss: 2.7791e-04 - 8s/epoch - 96us/sample
Epoch 33/85
84077/84077 - 8s - loss: 2.9280e-04 - val_loss: 2.7455e-04 - 8s/epoch - 96us/sample
Epoch 34/85
84077/84077 - 8s - loss: 2.9097e-04 - val_loss: 2.7336e-04 - 8s/epoch - 97us/sample
Epoch 35/85
84077/84077 - 8s - loss: 2.8845e-04 - val_loss: 2.7377e-04 - 8s/epoch - 96us/sample
Epoch 36/85
84077/84077 - 8s - loss: 2.8653e-04 - val_loss: 2.6903e-04 - 8s/epoch - 96us/sample
Epoch 37/85
84077/84077 - 8s - loss: 2.8507e-04 - val_loss: 2.7051e-04 - 8s/epoch - 96us/sample
Epoch 38/85
84077/84077 - 8s - loss: 2.8371e-04 - val_loss: 2.6686e-04 - 8s/epoch - 96us/sample
Epoch 39/85
84077/84077 - 8s - loss: 2.8260e-04 - val_loss: 2.6711e-04 - 8s/epoch - 96us/sample
Epoch 40/85
84077/84077 - 8s - loss: 2.8121e-04 - val_loss: 2.6626e-04 - 8s/epoch - 96us/sample
Epoch 41/85
84077/84077 - 8s - loss: 2.7943e-04 - val_loss: 2.6633e-04 - 8s/epoch - 96us/sample
Epoch 42/85
84077/84077 - 8s - loss: 2.7925e-04 - val_loss: 2.6285e-04 - 8s/epoch - 96us/sample
Epoch 43/85
84077/84077 - 8s - loss: 2.7807e-04 - val_loss: 2.6148e-04 - 8s/epoch - 97us/sample
Epoch 44/85
84077/84077 - 8s - loss: 2.7764e-04 - val_loss: 2.6300e-04 - 8s/epoch - 96us/sample
Epoch 45/85
84077/84077 - 8s - loss: 2.7699e-04 - val_loss: 2.6060e-04 - 8s/epoch - 96us/sample
Epoch 46/85
84077/84077 - 8s - loss: 2.7609e-04 - val_loss: 2.6012e-04 - 8s/epoch - 96us/sample
Epoch 47/85
84077/84077 - 8s - loss: 2.7515e-04 - val_loss: 2.6091e-04 - 8s/epoch - 96us/sample
Epoch 48/85
84077/84077 - 8s - loss: 2.7484e-04 - val_loss: 2.5987e-04 - 8s/epoch - 96us/sample
Epoch 49/85
84077/84077 - 8s - loss: 2.7448e-04 - val_loss: 2.6083e-04 - 8s/epoch - 96us/sample
Epoch 50/85
84077/84077 - 8s - loss: 2.7434e-04 - val_loss: 2.5997e-04 - 8s/epoch - 96us/sample
Epoch 51/85
84077/84077 - 8s - loss: 2.7319e-04 - val_loss: 2.5955e-04 - 8s/epoch - 96us/sample
Epoch 52/85
84077/84077 - 8s - loss: 2.7260e-04 - val_loss: 2.5843e-04 - 8s/epoch - 96us/sample
Epoch 53/85
84077/84077 - 8s - loss: 2.7235e-04 - val_loss: 2.5961e-04 - 8s/epoch - 96us/sample
Epoch 54/85
84077/84077 - 8s - loss: 2.7172e-04 - val_loss: 2.5771e-04 - 8s/epoch - 96us/sample
Epoch 55/85
84077/84077 - 8s - loss: 2.7113e-04 - val_loss: 2.5802e-04 - 8s/epoch - 96us/sample
Epoch 56/85
84077/84077 - 8s - loss: 2.7086e-04 - val_loss: 2.5682e-04 - 8s/epoch - 96us/sample
Epoch 57/85
84077/84077 - 8s - loss: 2.7040e-04 - val_loss: 2.5580e-04 - 8s/epoch - 96us/sample
Epoch 58/85
84077/84077 - 8s - loss: 2.6989e-04 - val_loss: 2.5620e-04 - 8s/epoch - 96us/sample
Epoch 59/85
84077/84077 - 8s - loss: 2.6964e-04 - val_loss: 2.5541e-04 - 8s/epoch - 96us/sample
Epoch 60/85
84077/84077 - 8s - loss: 2.6914e-04 - val_loss: 2.5401e-04 - 8s/epoch - 96us/sample
Epoch 61/85
84077/84077 - 8s - loss: 2.6877e-04 - val_loss: 2.5572e-04 - 8s/epoch - 96us/sample
Epoch 62/85
84077/84077 - 8s - loss: 2.6783e-04 - val_loss: 2.5341e-04 - 8s/epoch - 96us/sample
Epoch 63/85
84077/84077 - 8s - loss: 2.6704e-04 - val_loss: 2.5277e-04 - 8s/epoch - 96us/sample
Epoch 64/85
84077/84077 - 8s - loss: 2.6663e-04 - val_loss: 2.5328e-04 - 8s/epoch - 96us/sample
Epoch 65/85
84077/84077 - 8s - loss: 2.6585e-04 - val_loss: 2.5248e-04 - 8s/epoch - 96us/sample
Epoch 66/85
84077/84077 - 8s - loss: 2.6491e-04 - val_loss: 2.5074e-04 - 8s/epoch - 96us/sample
Epoch 67/85
84077/84077 - 8s - loss: 2.6409e-04 - val_loss: 2.5056e-04 - 8s/epoch - 97us/sample
Epoch 68/85
84077/84077 - 8s - loss: 2.6387e-04 - val_loss: 2.5025e-04 - 8s/epoch - 96us/sample
Epoch 69/85
84077/84077 - 8s - loss: 2.6236e-04 - val_loss: 2.4865e-04 - 8s/epoch - 96us/sample
Epoch 70/85
84077/84077 - 8s - loss: 2.6181e-04 - val_loss: 2.4880e-04 - 8s/epoch - 96us/sample
Epoch 71/85
84077/84077 - 8s - loss: 2.6103e-04 - val_loss: 2.4794e-04 - 8s/epoch - 96us/sample
Epoch 72/85
84077/84077 - 8s - loss: 2.6046e-04 - val_loss: 2.4537e-04 - 8s/epoch - 96us/sample
Epoch 73/85
84077/84077 - 8s - loss: 2.5937e-04 - val_loss: 2.4581e-04 - 8s/epoch - 96us/sample
Epoch 74/85
84077/84077 - 8s - loss: 2.5900e-04 - val_loss: 2.4468e-04 - 8s/epoch - 97us/sample
Epoch 75/85
84077/84077 - 8s - loss: 2.5833e-04 - val_loss: 2.4546e-04 - 8s/epoch - 97us/sample
Epoch 76/85
84077/84077 - 8s - loss: 2.5784e-04 - val_loss: 2.4378e-04 - 8s/epoch - 96us/sample
Epoch 77/85
84077/84077 - 8s - loss: 2.5705e-04 - val_loss: 2.4388e-04 - 8s/epoch - 96us/sample
Epoch 78/85
84077/84077 - 8s - loss: 2.5648e-04 - val_loss: 2.4347e-04 - 8s/epoch - 96us/sample
Epoch 79/85
84077/84077 - 8s - loss: 2.5571e-04 - val_loss: 2.4281e-04 - 8s/epoch - 96us/sample
Epoch 80/85
84077/84077 - 8s - loss: 2.5471e-04 - val_loss: 2.4266e-04 - 8s/epoch - 96us/sample
Epoch 81/85
84077/84077 - 8s - loss: 2.5404e-04 - val_loss: 2.4008e-04 - 8s/epoch - 96us/sample
Epoch 82/85
84077/84077 - 8s - loss: 2.5432e-04 - val_loss: 2.4070e-04 - 8s/epoch - 96us/sample
Epoch 83/85
84077/84077 - 8s - loss: 2.5317e-04 - val_loss: 2.4022e-04 - 8s/epoch - 97us/sample
Epoch 84/85
84077/84077 - 8s - loss: 2.5285e-04 - val_loss: 2.4065e-04 - 8s/epoch - 96us/sample
Epoch 85/85
84077/84077 - 8s - loss: 2.5255e-04 - val_loss: 2.3957e-04 - 8s/epoch - 96us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00023957278219934332
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:28:15.985701: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_38/outputlayer/BiasAdd' id:49197 op device:{requested: '', assigned: ''} def:{{{node decoder_model_38/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_38/outputlayer/MatMul, decoder_model_38/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.08561215974069888
cosine 0.08452543362224613
MAE: 0.003227286307339242
RMSE: 0.016898386804111994
r2: 0.7769819274962516
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 85, 0.001, 0.1, 94, 0.00025254900221318317, 0.00023957278219934332, 0.08561215974069888, 0.08452543362224613, 0.003227286307339242, 0.016898386804111994, 0.7769819274962516, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 4
Fitness    = 424.308669689168
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 424.308669689168.
Best solutions :  [array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168]
[2.5 50 0.001 16 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_117 (Batch  (None, 2357)        9428        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_117 (ReLU)               (None, 2357)         0           ['batch_normalization_117[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 02:28:34.372686: W tensorflow/c/c_api.cc:291] Operation '{name:'training_78/Adam/outputlayer_39/bias/v/Assign' id:51215 op device:{requested: '', assigned: ''} def:{{{node training_78/Adam/outputlayer_39/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_78/Adam/outputlayer_39/bias/v, training_78/Adam/outputlayer_39/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:28:59.692359: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_39/mul' id:50518 op device:{requested: '', assigned: ''} def:{{{node loss_39/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_39/mul/x, loss_39/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 33s - loss: 0.0028 - val_loss: 0.0016 - 33s/epoch - 388us/sample
Epoch 2/50
84077/84077 - 22s - loss: 7.4393e-04 - val_loss: 6.3088e-04 - 22s/epoch - 263us/sample
Epoch 3/50
84077/84077 - 22s - loss: 5.6839e-04 - val_loss: 5.7076e-04 - 22s/epoch - 264us/sample
Epoch 4/50
84077/84077 - 22s - loss: 5.0937e-04 - val_loss: 7.7388e-04 - 22s/epoch - 265us/sample
Epoch 5/50
84077/84077 - 22s - loss: 4.8527e-04 - val_loss: 9.0258e-04 - 22s/epoch - 265us/sample
Epoch 6/50
84077/84077 - 22s - loss: 4.7217e-04 - val_loss: 8.8893e-04 - 22s/epoch - 264us/sample
Epoch 7/50
84077/84077 - 22s - loss: 4.6848e-04 - val_loss: 7.9496e-04 - 22s/epoch - 265us/sample
Epoch 8/50
84077/84077 - 22s - loss: 4.6609e-04 - val_loss: 8.2132e-04 - 22s/epoch - 266us/sample
Epoch 9/50
84077/84077 - 22s - loss: 4.6437e-04 - val_loss: 6.9428e-04 - 22s/epoch - 264us/sample
Epoch 10/50
84077/84077 - 22s - loss: 4.6298e-04 - val_loss: 7.8385e-04 - 22s/epoch - 264us/sample
Epoch 11/50
84077/84077 - 22s - loss: 4.6158e-04 - val_loss: 5.8732e-04 - 22s/epoch - 265us/sample
Epoch 12/50
84077/84077 - 22s - loss: 4.6048e-04 - val_loss: 5.3329e-04 - 22s/epoch - 265us/sample
Epoch 13/50
84077/84077 - 22s - loss: 4.5975e-04 - val_loss: 5.9123e-04 - 22s/epoch - 264us/sample
Epoch 14/50
84077/84077 - 22s - loss: 4.5972e-04 - val_loss: 5.3253e-04 - 22s/epoch - 265us/sample
Epoch 15/50
84077/84077 - 22s - loss: 4.5985e-04 - val_loss: 4.9314e-04 - 22s/epoch - 266us/sample
Epoch 16/50
84077/84077 - 22s - loss: 4.5840e-04 - val_loss: 4.8548e-04 - 22s/epoch - 264us/sample
Epoch 17/50
84077/84077 - 22s - loss: 4.5799e-04 - val_loss: 5.3341e-04 - 22s/epoch - 265us/sample
Epoch 18/50
84077/84077 - 22s - loss: 4.5716e-04 - val_loss: 4.7230e-04 - 22s/epoch - 266us/sample
Epoch 19/50
84077/84077 - 22s - loss: 4.5709e-04 - val_loss: 4.5670e-04 - 22s/epoch - 264us/sample
Epoch 20/50
84077/84077 - 22s - loss: 4.5692e-04 - val_loss: 4.9022e-04 - 22s/epoch - 265us/sample
Epoch 21/50
84077/84077 - 22s - loss: 4.5685e-04 - val_loss: 4.5178e-04 - 22s/epoch - 266us/sample
Epoch 22/50
84077/84077 - 22s - loss: 4.5660e-04 - val_loss: 4.8039e-04 - 22s/epoch - 265us/sample
Epoch 23/50
84077/84077 - 22s - loss: 4.5552e-04 - val_loss: 5.1971e-04 - 22s/epoch - 264us/sample
Epoch 24/50
84077/84077 - 22s - loss: 4.5346e-04 - val_loss: 4.8169e-04 - 22s/epoch - 265us/sample
Epoch 25/50
84077/84077 - 22s - loss: 4.5239e-04 - val_loss: 4.7756e-04 - 22s/epoch - 265us/sample
Epoch 26/50
84077/84077 - 22s - loss: 4.5140e-04 - val_loss: 5.2838e-04 - 22s/epoch - 264us/sample
Epoch 27/50
84077/84077 - 22s - loss: 4.5105e-04 - val_loss: 5.3078e-04 - 22s/epoch - 265us/sample
Epoch 28/50
84077/84077 - 22s - loss: 4.5151e-04 - val_loss: 4.5301e-04 - 22s/epoch - 266us/sample
Epoch 29/50
84077/84077 - 22s - loss: 4.5053e-04 - val_loss: 4.7513e-04 - 22s/epoch - 265us/sample
Epoch 30/50
84077/84077 - 22s - loss: 4.5089e-04 - val_loss: 4.8830e-04 - 22s/epoch - 265us/sample
Epoch 31/50
84077/84077 - 22s - loss: 4.5065e-04 - val_loss: 4.7296e-04 - 22s/epoch - 265us/sample
Epoch 32/50
84077/84077 - 22s - loss: 4.5023e-04 - val_loss: 5.3384e-04 - 22s/epoch - 265us/sample
Epoch 33/50
84077/84077 - 22s - loss: 4.5045e-04 - val_loss: 4.5775e-04 - 22s/epoch - 265us/sample
Epoch 34/50
84077/84077 - 22s - loss: 4.5068e-04 - val_loss: 4.8670e-04 - 22s/epoch - 265us/sample
Epoch 35/50
84077/84077 - 22s - loss: 4.5059e-04 - val_loss: 5.1022e-04 - 22s/epoch - 265us/sample
Epoch 36/50
84077/84077 - 22s - loss: 4.5012e-04 - val_loss: 5.3099e-04 - 22s/epoch - 265us/sample
Epoch 37/50
84077/84077 - 22s - loss: 4.5018e-04 - val_loss: 4.7308e-04 - 22s/epoch - 265us/sample
Epoch 38/50
84077/84077 - 22s - loss: 4.5041e-04 - val_loss: 4.9234e-04 - 22s/epoch - 266us/sample
Epoch 39/50
84077/84077 - 22s - loss: 4.5031e-04 - val_loss: 5.3456e-04 - 22s/epoch - 265us/sample
Epoch 40/50
84077/84077 - 22s - loss: 4.5003e-04 - val_loss: 4.6330e-04 - 22s/epoch - 265us/sample
Epoch 41/50
84077/84077 - 22s - loss: 4.5004e-04 - val_loss: 4.5908e-04 - 22s/epoch - 266us/sample
Epoch 42/50
84077/84077 - 22s - loss: 4.4985e-04 - val_loss: 4.4895e-04 - 22s/epoch - 265us/sample
Epoch 43/50
84077/84077 - 22s - loss: 4.5015e-04 - val_loss: 5.1207e-04 - 22s/epoch - 265us/sample
Epoch 44/50
84077/84077 - 22s - loss: 4.4954e-04 - val_loss: 4.5935e-04 - 22s/epoch - 266us/sample
Epoch 45/50
84077/84077 - 22s - loss: 4.5004e-04 - val_loss: 4.6990e-04 - 22s/epoch - 265us/sample
Epoch 46/50
84077/84077 - 22s - loss: 4.5003e-04 - val_loss: 4.6756e-04 - 22s/epoch - 264us/sample
Epoch 47/50
84077/84077 - 22s - loss: 4.4955e-04 - val_loss: 4.9493e-04 - 22s/epoch - 266us/sample
Epoch 48/50
84077/84077 - 22s - loss: 4.4988e-04 - val_loss: 5.7476e-04 - 22s/epoch - 265us/sample
Epoch 49/50
84077/84077 - 22s - loss: 4.4960e-04 - val_loss: 4.9435e-04 - 22s/epoch - 265us/sample
Epoch 50/50
84077/84077 - 22s - loss: 4.4978e-04 - val_loss: 4.5169e-04 - 22s/epoch - 265us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0004516947762700712
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:47:15.791205: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_39/outputlayer/BiasAdd' id:50482 op device:{requested: '', assigned: ''} def:{{{node decoder_model_39/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_39/outputlayer/MatMul, decoder_model_39/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2785260960584359
cosine 0.27427404435116903
MAE: 0.005571126213503777
RMSE: 0.02900695787960847
r2: 0.34273331480794333
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 16, 50, 0.001, 0.1, 94, 0.0004497836896562733, 0.0004516947762700712, 0.2785260960584359, 0.27427404435116903, 0.005571126213503777, 0.02900695787960847, 0.34273331480794333, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.001 64 0] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_120 (Batch  (None, 2357)        9428        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_120 (ReLU)               (None, 2357)         0           ['batch_normalization_120[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 02:47:34.262587: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_121/moving_mean/Assign' id:51533 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_121/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_121/moving_mean, batch_normalization_121/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:47:46.546938: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_40/mul' id:51815 op device:{requested: '', assigned: ''} def:{{{node loss_40/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_40/mul/x, loss_40/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0696 - val_loss: 0.0673 - 19s/epoch - 223us/sample
Epoch 2/50
84077/84077 - 8s - loss: 0.0675 - val_loss: 0.0674 - 8s/epoch - 100us/sample
Epoch 3/50
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 100us/sample
Epoch 4/50
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 100us/sample
Epoch 5/50
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 100us/sample
Epoch 6/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0674 - 8s/epoch - 101us/sample
Epoch 7/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0674 - 8s/epoch - 100us/sample
Epoch 8/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0674 - 8s/epoch - 100us/sample
Epoch 9/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 10/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 11/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 12/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 13/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 14/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 15/50
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 101us/sample
Epoch 16/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 101us/sample
Epoch 17/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 18/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 19/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 20/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 21/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 22/50
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 101us/sample
Epoch 23/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 24/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 101us/sample
Epoch 25/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 26/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 27/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 28/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 29/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 30/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 101us/sample
Epoch 31/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 32/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 33/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 34/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 35/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 36/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 37/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 38/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 101us/sample
Epoch 39/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 101us/sample
Epoch 40/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 41/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 42/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 43/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 44/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 45/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 46/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 101us/sample
Epoch 47/50
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 101us/sample
Epoch 48/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 101us/sample
Epoch 49/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
Epoch 50/50
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 100us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.06734573649845857
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:54:43.415831: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_40/outputlayer/BiasAdd' id:51767 op device:{requested: '', assigned: ''} def:{{{node decoder_model_40/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_40/outputlayer/MatMul, decoder_model_40/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.0842789099675143
cosine 1.1253708734200265
MAE: 6.291296221738316
RMSE: 6.458366163168053
r2: -30509.378609027488
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'binary_crossentropy', 64, 50, 0.001, 0.1, 94, 0.066836919929607, 0.06734573649845857, 1.0842789099675143, 1.1253708734200265, 6.291296221738316, 6.458366163168053, -30509.378609027488, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 90 0.0008 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2546)         2403424     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_123 (Batch  (None, 2546)        10184       ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_123 (ReLU)               (None, 2546)         0           ['batch_normalization_123[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           239418      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           239418      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2663181     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,555,625
Trainable params: 5,545,253
Non-trainable params: 10,372
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 02:55:02.172203: W tensorflow/c/c_api.cc:291] Operation '{name:'training_82/Adam/outputlayer_41/bias/v/Assign' id:53810 op device:{requested: '', assigned: ''} def:{{{node training_82/Adam/outputlayer_41/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_82/Adam/outputlayer_41/bias/v, training_82/Adam/outputlayer_41/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:55:14.441695: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_41/mul' id:53133 op device:{requested: '', assigned: ''} def:{{{node loss_41/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_41/mul/x, loss_41/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.1021 - val_loss: 0.0017 - 19s/epoch - 226us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0020 - val_loss: 0.0017 - 8s/epoch - 99us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0022 - val_loss: 0.0056 - 8s/epoch - 98us/sample
Epoch 4/90
84077/84077 - 8s - loss: 0.0146 - val_loss: 0.0043 - 8s/epoch - 99us/sample
Epoch 5/90
84077/84077 - 8s - loss: 0.0016 - val_loss: 0.0012 - 8s/epoch - 98us/sample
Epoch 6/90
84077/84077 - 8s - loss: 0.0011 - val_loss: 0.0010 - 8s/epoch - 98us/sample
Epoch 7/90
84077/84077 - 8s - loss: 0.0083 - val_loss: 0.0086 - 8s/epoch - 99us/sample
Epoch 8/90
84077/84077 - 8s - loss: 0.0018 - val_loss: 0.0012 - 8s/epoch - 99us/sample
Epoch 9/90
84077/84077 - 8s - loss: 0.0014 - val_loss: 0.0010 - 8s/epoch - 99us/sample
Epoch 10/90
84077/84077 - 8s - loss: 0.0104 - val_loss: 0.0046 - 8s/epoch - 99us/sample
Epoch 11/90
84077/84077 - 8s - loss: 0.0017 - val_loss: 0.0012 - 8s/epoch - 99us/sample
Epoch 12/90
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0010 - 8s/epoch - 98us/sample
Epoch 13/90
84077/84077 - 8s - loss: 0.0010 - val_loss: 9.2288e-04 - 8s/epoch - 99us/sample
Epoch 14/90
84077/84077 - 8s - loss: 1.2295 - val_loss: 0.0033 - 8s/epoch - 99us/sample
Epoch 15/90
84077/84077 - 8s - loss: 0.0020 - val_loss: 0.0014 - 8s/epoch - 99us/sample
Epoch 16/90
84077/84077 - 8s - loss: 0.0013 - val_loss: 0.0011 - 8s/epoch - 98us/sample
Epoch 17/90
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0010 - 8s/epoch - 99us/sample
Epoch 18/90
84077/84077 - 8s - loss: 0.0010 - val_loss: 9.0597e-04 - 8s/epoch - 100us/sample
Epoch 19/90
84077/84077 - 8s - loss: 0.0013 - val_loss: 9.2966e-04 - 8s/epoch - 99us/sample
Epoch 20/90
84077/84077 - 8s - loss: 9.2533e-04 - val_loss: 8.5440e-04 - 8s/epoch - 99us/sample
Epoch 21/90
84077/84077 - 8s - loss: 9.3660e-04 - val_loss: 8.1080e-04 - 8s/epoch - 99us/sample
Epoch 22/90
84077/84077 - 8s - loss: 8.4690e-04 - val_loss: 8.0566e-04 - 8s/epoch - 99us/sample
Epoch 23/90
84077/84077 - 8s - loss: 0.0016 - val_loss: 8.1855e-04 - 8s/epoch - 98us/sample
Epoch 24/90
84077/84077 - 8s - loss: 0.0019 - val_loss: 0.0013 - 8s/epoch - 98us/sample
Epoch 25/90
84077/84077 - 8s - loss: 0.0010 - val_loss: 8.3116e-04 - 8s/epoch - 99us/sample
Epoch 26/90
84077/84077 - 8s - loss: 8.6322e-04 - val_loss: 7.9474e-04 - 8s/epoch - 100us/sample
Epoch 27/90
84077/84077 - 8s - loss: 8.8236e-04 - val_loss: 7.6603e-04 - 8s/epoch - 99us/sample
Epoch 28/90
84077/84077 - 8s - loss: 7.9264e-04 - val_loss: 7.4621e-04 - 8s/epoch - 99us/sample
Epoch 29/90
84077/84077 - 8s - loss: 7.7643e-04 - val_loss: 7.2670e-04 - 8s/epoch - 98us/sample
Epoch 30/90
84077/84077 - 8s - loss: 8.0657e-04 - val_loss: 7.2126e-04 - 8s/epoch - 98us/sample
Epoch 31/90
84077/84077 - 8s - loss: 7.8979e-04 - val_loss: 7.0579e-04 - 8s/epoch - 99us/sample
Epoch 32/90
84077/84077 - 8s - loss: 7.5171e-04 - val_loss: 6.9887e-04 - 8s/epoch - 98us/sample
Epoch 33/90
84077/84077 - 8s - loss: 7.2690e-04 - val_loss: 6.9353e-04 - 8s/epoch - 99us/sample
Epoch 34/90
84077/84077 - 8s - loss: 7.2010e-04 - val_loss: 6.6851e-04 - 8s/epoch - 100us/sample
Epoch 35/90
84077/84077 - 8s - loss: 6.9194e-04 - val_loss: 6.5841e-04 - 8s/epoch - 99us/sample
Epoch 36/90
84077/84077 - 8s - loss: 6.9781e-04 - val_loss: 6.4263e-04 - 8s/epoch - 99us/sample
Epoch 37/90
84077/84077 - 8s - loss: 6.7785e-04 - val_loss: 6.4029e-04 - 8s/epoch - 98us/sample
Epoch 38/90
84077/84077 - 8s - loss: 6.5899e-04 - val_loss: 6.2027e-04 - 8s/epoch - 99us/sample
Epoch 39/90
84077/84077 - 8s - loss: 6.4652e-04 - val_loss: 6.1171e-04 - 8s/epoch - 99us/sample
Epoch 40/90
84077/84077 - 8s - loss: 6.3554e-04 - val_loss: 6.0305e-04 - 8s/epoch - 99us/sample
Epoch 41/90
84077/84077 - 8s - loss: 6.2848e-04 - val_loss: 5.8953e-04 - 8s/epoch - 99us/sample
Epoch 42/90
84077/84077 - 8s - loss: 6.1458e-04 - val_loss: 5.8210e-04 - 8s/epoch - 99us/sample
Epoch 43/90
84077/84077 - 8s - loss: 6.0362e-04 - val_loss: 5.6893e-04 - 8s/epoch - 99us/sample
Epoch 44/90
84077/84077 - 8s - loss: 5.9324e-04 - val_loss: 5.5943e-04 - 8s/epoch - 99us/sample
Epoch 45/90
84077/84077 - 8s - loss: 5.8106e-04 - val_loss: 5.4493e-04 - 8s/epoch - 99us/sample
Epoch 46/90
84077/84077 - 8s - loss: 5.7033e-04 - val_loss: 5.3607e-04 - 8s/epoch - 99us/sample
Epoch 47/90
84077/84077 - 8s - loss: 5.6287e-04 - val_loss: 5.2954e-04 - 8s/epoch - 99us/sample
Epoch 48/90
84077/84077 - 8s - loss: 5.5614e-04 - val_loss: 5.2257e-04 - 8s/epoch - 99us/sample
Epoch 49/90
84077/84077 - 8s - loss: 5.5027e-04 - val_loss: 5.1662e-04 - 8s/epoch - 99us/sample
Epoch 50/90
84077/84077 - 8s - loss: 5.4449e-04 - val_loss: 5.1523e-04 - 8s/epoch - 99us/sample
Epoch 51/90
84077/84077 - 8s - loss: 5.4117e-04 - val_loss: 5.0990e-04 - 8s/epoch - 100us/sample
Epoch 52/90
84077/84077 - 8s - loss: 5.4000e-04 - val_loss: 5.0763e-04 - 8s/epoch - 99us/sample
Epoch 53/90
84077/84077 - 8s - loss: 5.3438e-04 - val_loss: 5.0597e-04 - 8s/epoch - 99us/sample
Epoch 54/90
84077/84077 - 8s - loss: 5.2988e-04 - val_loss: 5.0192e-04 - 8s/epoch - 98us/sample
Epoch 55/90
84077/84077 - 8s - loss: 5.2540e-04 - val_loss: 4.9755e-04 - 8s/epoch - 99us/sample
Epoch 56/90
84077/84077 - 8s - loss: 5.2158e-04 - val_loss: 4.9324e-04 - 8s/epoch - 98us/sample
Epoch 57/90
84077/84077 - 8s - loss: 5.1816e-04 - val_loss: 4.8957e-04 - 8s/epoch - 99us/sample
Epoch 58/90
84077/84077 - 8s - loss: 5.1537e-04 - val_loss: 4.8681e-04 - 8s/epoch - 99us/sample
Epoch 59/90
84077/84077 - 8s - loss: 5.1187e-04 - val_loss: 4.8596e-04 - 8s/epoch - 100us/sample
Epoch 60/90
84077/84077 - 8s - loss: 5.1046e-04 - val_loss: 4.8540e-04 - 8s/epoch - 99us/sample
Epoch 61/90
84077/84077 - 8s - loss: 5.0817e-04 - val_loss: 4.8298e-04 - 8s/epoch - 99us/sample
Epoch 62/90
84077/84077 - 8s - loss: 5.0710e-04 - val_loss: 4.8194e-04 - 8s/epoch - 98us/sample
Epoch 63/90
84077/84077 - 8s - loss: 5.0498e-04 - val_loss: 4.8120e-04 - 8s/epoch - 99us/sample
Epoch 64/90
84077/84077 - 8s - loss: 5.0261e-04 - val_loss: 4.7615e-04 - 8s/epoch - 98us/sample
Epoch 65/90
84077/84077 - 8s - loss: 4.9960e-04 - val_loss: 4.7720e-04 - 8s/epoch - 98us/sample
Epoch 66/90
84077/84077 - 8s - loss: 4.9632e-04 - val_loss: 4.7194e-04 - 8s/epoch - 98us/sample
Epoch 67/90
84077/84077 - 8s - loss: 4.9360e-04 - val_loss: 4.6871e-04 - 8s/epoch - 99us/sample
Epoch 68/90
84077/84077 - 8s - loss: 4.9194e-04 - val_loss: 4.6924e-04 - 8s/epoch - 99us/sample
Epoch 69/90
84077/84077 - 8s - loss: 4.8980e-04 - val_loss: 4.6737e-04 - 8s/epoch - 99us/sample
Epoch 70/90
84077/84077 - 8s - loss: 4.8823e-04 - val_loss: 4.6660e-04 - 8s/epoch - 99us/sample
Epoch 71/90
84077/84077 - 8s - loss: 4.8636e-04 - val_loss: 4.6194e-04 - 8s/epoch - 98us/sample
Epoch 72/90
84077/84077 - 8s - loss: 4.8449e-04 - val_loss: 4.5839e-04 - 8s/epoch - 98us/sample
Epoch 73/90
84077/84077 - 8s - loss: 4.8141e-04 - val_loss: 4.5724e-04 - 8s/epoch - 99us/sample
Epoch 74/90
84077/84077 - 8s - loss: 4.8097e-04 - val_loss: 4.5644e-04 - 8s/epoch - 99us/sample
Epoch 75/90
84077/84077 - 8s - loss: 4.7879e-04 - val_loss: 4.5459e-04 - 8s/epoch - 99us/sample
Epoch 76/90
84077/84077 - 8s - loss: 4.7571e-04 - val_loss: 4.5321e-04 - 8s/epoch - 100us/sample
Epoch 77/90
84077/84077 - 8s - loss: 4.7476e-04 - val_loss: 4.5064e-04 - 8s/epoch - 99us/sample
Epoch 78/90
84077/84077 - 8s - loss: 4.7197e-04 - val_loss: 4.5091e-04 - 8s/epoch - 99us/sample
Epoch 79/90
84077/84077 - 8s - loss: 4.6966e-04 - val_loss: 4.4726e-04 - 8s/epoch - 99us/sample
Epoch 80/90
84077/84077 - 8s - loss: 4.6761e-04 - val_loss: 4.4407e-04 - 8s/epoch - 99us/sample
Epoch 81/90
84077/84077 - 8s - loss: 4.6626e-04 - val_loss: 4.4256e-04 - 8s/epoch - 99us/sample
Epoch 82/90
84077/84077 - 8s - loss: 4.6497e-04 - val_loss: 4.3904e-04 - 8s/epoch - 99us/sample
Epoch 83/90
84077/84077 - 8s - loss: 4.6340e-04 - val_loss: 4.4045e-04 - 8s/epoch - 99us/sample
Epoch 84/90
84077/84077 - 8s - loss: 4.6313e-04 - val_loss: 4.3947e-04 - 8s/epoch - 100us/sample
Epoch 85/90
84077/84077 - 8s - loss: 4.6169e-04 - val_loss: 4.3885e-04 - 8s/epoch - 99us/sample
Epoch 86/90
84077/84077 - 8s - loss: 4.6136e-04 - val_loss: 4.3726e-04 - 8s/epoch - 99us/sample
Epoch 87/90
84077/84077 - 8s - loss: 4.6020e-04 - val_loss: 4.3796e-04 - 8s/epoch - 99us/sample
Epoch 88/90
84077/84077 - 8s - loss: 4.5919e-04 - val_loss: 4.3847e-04 - 8s/epoch - 99us/sample
Epoch 89/90
84077/84077 - 8s - loss: 4.5917e-04 - val_loss: 4.3644e-04 - 8s/epoch - 99us/sample
Epoch 90/90
84077/84077 - 8s - loss: 4.5845e-04 - val_loss: 4.3728e-04 - 8s/epoch - 99us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.000437280089677218
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:07:37.149617: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_41/outputlayer/BiasAdd' id:53104 op device:{requested: '', assigned: ''} def:{{{node decoder_model_41/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_41/outputlayer/MatMul, decoder_model_41/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.09054757481829906
cosine 0.08943572793319086
MAE: 0.0033736459863326538
RMSE: 0.01747905702817067
r2: 0.7613685333168217
RMSE zero-vector: 0.04004287452915337
['2.7custom_VAE', 'mse', 64, 90, 0.0008, 0.1, 94, 0.00045844977635558864, 0.000437280089677218, 0.09054757481829906, 0.08943572793319086, 0.0033736459863326538, 0.01747905702817067, 0.7613685333168217, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_126 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_126 (ReLU)               (None, 1886)         0           ['batch_normalization_126[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 03:07:56.141443: W tensorflow/c/c_api.cc:291] Operation '{name:'training_84/Adam/bottleneck_zmean_42/kernel/m/Assign' id:54872 op device:{requested: '', assigned: ''} def:{{{node training_84/Adam/bottleneck_zmean_42/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_84/Adam/bottleneck_zmean_42/kernel/m, training_84/Adam/bottleneck_zmean_42/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:08:08.197256: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_42/mul' id:54391 op device:{requested: '', assigned: ''} def:{{{node loss_42/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_42/mul/x, loss_42/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0110 - val_loss: 0.0017 - 19s/epoch - 224us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0020 - val_loss: 0.0013 - 8s/epoch - 98us/sample
Epoch 3/85
84077/84077 - 8s - loss: 0.0017 - val_loss: 0.0012 - 8s/epoch - 98us/sample
Epoch 4/85
84077/84077 - 8s - loss: 0.0012 - val_loss: 9.6712e-04 - 8s/epoch - 98us/sample
Epoch 5/85
84077/84077 - 8s - loss: 0.0017 - val_loss: 9.7827e-04 - 8s/epoch - 98us/sample
Epoch 6/85
84077/84077 - 8s - loss: 9.0261e-04 - val_loss: 8.0158e-04 - 8s/epoch - 98us/sample
Epoch 7/85
84077/84077 - 8s - loss: 8.0160e-04 - val_loss: 7.0738e-04 - 8s/epoch - 99us/sample
Epoch 8/85
84077/84077 - 8s - loss: 8.7427e-04 - val_loss: 6.6259e-04 - 8s/epoch - 98us/sample
Epoch 9/85
84077/84077 - 8s - loss: 6.8356e-04 - val_loss: 5.8868e-04 - 8s/epoch - 98us/sample
Epoch 10/85
84077/84077 - 8s - loss: 6.5397e-04 - val_loss: 5.5655e-04 - 8s/epoch - 98us/sample
Epoch 11/85
84077/84077 - 8s - loss: 5.9469e-04 - val_loss: 5.4978e-04 - 8s/epoch - 98us/sample
Epoch 12/85
84077/84077 - 8s - loss: 5.5916e-04 - val_loss: 4.8863e-04 - 8s/epoch - 98us/sample
Epoch 13/85
84077/84077 - 8s - loss: 5.2895e-04 - val_loss: 4.8646e-04 - 8s/epoch - 98us/sample
Epoch 14/85
84077/84077 - 8s - loss: 5.0816e-04 - val_loss: 4.6305e-04 - 8s/epoch - 98us/sample
Epoch 15/85
84077/84077 - 8s - loss: 4.9017e-04 - val_loss: 4.3495e-04 - 8s/epoch - 99us/sample
Epoch 16/85
84077/84077 - 8s - loss: 4.7350e-04 - val_loss: 4.2339e-04 - 8s/epoch - 98us/sample
Epoch 17/85
84077/84077 - 8s - loss: 4.5894e-04 - val_loss: 4.0943e-04 - 8s/epoch - 98us/sample
Epoch 18/85
84077/84077 - 8s - loss: 4.4717e-04 - val_loss: 4.0092e-04 - 8s/epoch - 98us/sample
Epoch 19/85
84077/84077 - 8s - loss: 4.3840e-04 - val_loss: 3.9625e-04 - 8s/epoch - 98us/sample
Epoch 20/85
84077/84077 - 8s - loss: 4.2840e-04 - val_loss: 3.8787e-04 - 8s/epoch - 98us/sample
Epoch 21/85
84077/84077 - 8s - loss: 4.2089e-04 - val_loss: 3.8019e-04 - 8s/epoch - 98us/sample
Epoch 22/85
84077/84077 - 8s - loss: 4.1461e-04 - val_loss: 3.7796e-04 - 8s/epoch - 98us/sample
Epoch 23/85
84077/84077 - 8s - loss: 4.0840e-04 - val_loss: 4.5856e-04 - 8s/epoch - 99us/sample
Epoch 24/85
84077/84077 - 8s - loss: 4.0457e-04 - val_loss: 3.6509e-04 - 8s/epoch - 99us/sample
Epoch 25/85
84077/84077 - 8s - loss: 3.9742e-04 - val_loss: 3.5879e-04 - 8s/epoch - 98us/sample
Epoch 26/85
84077/84077 - 8s - loss: 3.9133e-04 - val_loss: 3.5243e-04 - 8s/epoch - 98us/sample
Epoch 27/85
84077/84077 - 8s - loss: 3.8693e-04 - val_loss: 3.4936e-04 - 8s/epoch - 98us/sample
Epoch 28/85
84077/84077 - 8s - loss: 3.8218e-04 - val_loss: 3.4682e-04 - 8s/epoch - 98us/sample
Epoch 29/85
84077/84077 - 8s - loss: 3.7698e-04 - val_loss: 3.4183e-04 - 8s/epoch - 98us/sample
Epoch 30/85
84077/84077 - 8s - loss: 3.7346e-04 - val_loss: 3.3672e-04 - 8s/epoch - 98us/sample
Epoch 31/85
84077/84077 - 8s - loss: 3.6907e-04 - val_loss: 3.3452e-04 - 8s/epoch - 99us/sample
Epoch 32/85
84077/84077 - 8s - loss: 3.6634e-04 - val_loss: 3.2967e-04 - 8s/epoch - 98us/sample
Epoch 33/85
84077/84077 - 8s - loss: 3.6283e-04 - val_loss: 3.2960e-04 - 8s/epoch - 98us/sample
Epoch 34/85
84077/84077 - 8s - loss: 3.5988e-04 - val_loss: 3.2356e-04 - 8s/epoch - 98us/sample
Epoch 35/85
84077/84077 - 8s - loss: 3.5757e-04 - val_loss: 3.2481e-04 - 8s/epoch - 98us/sample
Epoch 36/85
84077/84077 - 8s - loss: 3.5432e-04 - val_loss: 3.2192e-04 - 8s/epoch - 98us/sample
Epoch 37/85
84077/84077 - 8s - loss: 3.5167e-04 - val_loss: 3.2097e-04 - 8s/epoch - 98us/sample
Epoch 38/85
84077/84077 - 8s - loss: 3.4923e-04 - val_loss: 3.1575e-04 - 8s/epoch - 98us/sample
Epoch 39/85
84077/84077 - 8s - loss: 3.4732e-04 - val_loss: 3.1393e-04 - 8s/epoch - 99us/sample
Epoch 40/85
84077/84077 - 8s - loss: 3.5560e-04 - val_loss: 3.1473e-04 - 8s/epoch - 98us/sample
Epoch 41/85
84077/84077 - 8s - loss: 3.4438e-04 - val_loss: 3.1326e-04 - 8s/epoch - 98us/sample
Epoch 42/85
84077/84077 - 8s - loss: 3.4194e-04 - val_loss: 3.1013e-04 - 8s/epoch - 98us/sample
Epoch 43/85
84077/84077 - 8s - loss: 3.3917e-04 - val_loss: 3.0925e-04 - 8s/epoch - 98us/sample
Epoch 44/85
84077/84077 - 8s - loss: 3.3841e-04 - val_loss: 3.0646e-04 - 8s/epoch - 98us/sample
Epoch 45/85
84077/84077 - 8s - loss: 3.3693e-04 - val_loss: 3.0543e-04 - 8s/epoch - 98us/sample
Epoch 46/85
84077/84077 - 8s - loss: 3.3525e-04 - val_loss: 3.0443e-04 - 8s/epoch - 98us/sample
Epoch 47/85
84077/84077 - 8s - loss: 3.3363e-04 - val_loss: 3.0390e-04 - 8s/epoch - 99us/sample
Epoch 48/85
84077/84077 - 8s - loss: 3.3249e-04 - val_loss: 3.0181e-04 - 8s/epoch - 98us/sample
Epoch 49/85
84077/84077 - 8s - loss: 3.3085e-04 - val_loss: 3.0209e-04 - 8s/epoch - 98us/sample
Epoch 50/85
84077/84077 - 8s - loss: 3.3086e-04 - val_loss: 3.0307e-04 - 8s/epoch - 98us/sample
Epoch 51/85
84077/84077 - 8s - loss: 3.3031e-04 - val_loss: 3.0202e-04 - 8s/epoch - 98us/sample
Epoch 52/85
84077/84077 - 8s - loss: 3.2757e-04 - val_loss: 2.9774e-04 - 8s/epoch - 98us/sample
Epoch 53/85
84077/84077 - 8s - loss: 3.2731e-04 - val_loss: 2.9926e-04 - 8s/epoch - 98us/sample
Epoch 54/85
84077/84077 - 8s - loss: 3.2608e-04 - val_loss: 2.9593e-04 - 8s/epoch - 98us/sample
Epoch 55/85
84077/84077 - 8s - loss: 3.2512e-04 - val_loss: 2.9735e-04 - 8s/epoch - 99us/sample
Epoch 56/85
84077/84077 - 8s - loss: 3.2368e-04 - val_loss: 2.9621e-04 - 8s/epoch - 98us/sample
Epoch 57/85
84077/84077 - 8s - loss: 3.2282e-04 - val_loss: 2.9571e-04 - 8s/epoch - 98us/sample
Epoch 58/85
84077/84077 - 8s - loss: 3.2283e-04 - val_loss: 2.9418e-04 - 8s/epoch - 98us/sample
Epoch 59/85
84077/84077 - 8s - loss: 3.2205e-04 - val_loss: 2.9427e-04 - 8s/epoch - 98us/sample
Epoch 60/85
84077/84077 - 8s - loss: 3.2073e-04 - val_loss: 2.9570e-04 - 8s/epoch - 98us/sample
Epoch 61/85
84077/84077 - 8s - loss: 3.1987e-04 - val_loss: 2.9396e-04 - 8s/epoch - 98us/sample
Epoch 62/85
84077/84077 - 8s - loss: 3.1953e-04 - val_loss: 2.9453e-04 - 8s/epoch - 98us/sample
Epoch 63/85
84077/84077 - 8s - loss: 3.1881e-04 - val_loss: 2.9413e-04 - 8s/epoch - 99us/sample
Epoch 64/85
84077/84077 - 8s - loss: 3.1807e-04 - val_loss: 2.9192e-04 - 8s/epoch - 98us/sample
Epoch 65/85
84077/84077 - 8s - loss: 3.1775e-04 - val_loss: 2.9103e-04 - 8s/epoch - 98us/sample
Epoch 66/85
84077/84077 - 8s - loss: 3.1795e-04 - val_loss: 2.9183e-04 - 8s/epoch - 98us/sample
Epoch 67/85
84077/84077 - 8s - loss: 3.1564e-04 - val_loss: 2.9063e-04 - 8s/epoch - 98us/sample
Epoch 68/85
84077/84077 - 8s - loss: 3.1578e-04 - val_loss: 2.9329e-04 - 8s/epoch - 98us/sample
Epoch 69/85
84077/84077 - 8s - loss: 3.1618e-04 - val_loss: 2.9120e-04 - 8s/epoch - 98us/sample
Epoch 70/85
84077/84077 - 8s - loss: 3.1464e-04 - val_loss: 2.8909e-04 - 8s/epoch - 98us/sample
Epoch 71/85
84077/84077 - 8s - loss: 3.1402e-04 - val_loss: 2.8990e-04 - 8s/epoch - 99us/sample
Epoch 72/85
84077/84077 - 8s - loss: 3.1404e-04 - val_loss: 2.9035e-04 - 8s/epoch - 98us/sample
Epoch 73/85
84077/84077 - 8s - loss: 3.1355e-04 - val_loss: 2.8912e-04 - 8s/epoch - 98us/sample
Epoch 74/85
84077/84077 - 8s - loss: 3.1263e-04 - val_loss: 2.8796e-04 - 8s/epoch - 98us/sample
Epoch 75/85
84077/84077 - 8s - loss: 3.1280e-04 - val_loss: 2.8812e-04 - 8s/epoch - 98us/sample
Epoch 76/85
84077/84077 - 8s - loss: 3.1290e-04 - val_loss: 2.8690e-04 - 8s/epoch - 98us/sample
Epoch 77/85
84077/84077 - 8s - loss: 3.1127e-04 - val_loss: 2.8499e-04 - 8s/epoch - 98us/sample
Epoch 78/85
84077/84077 - 8s - loss: 3.1136e-04 - val_loss: 2.8758e-04 - 8s/epoch - 98us/sample
Epoch 79/85
84077/84077 - 8s - loss: 3.0998e-04 - val_loss: 2.8934e-04 - 8s/epoch - 99us/sample
Epoch 80/85
84077/84077 - 8s - loss: 3.1019e-04 - val_loss: 2.8746e-04 - 8s/epoch - 98us/sample
Epoch 81/85
84077/84077 - 8s - loss: 3.1004e-04 - val_loss: 2.8778e-04 - 8s/epoch - 98us/sample
Epoch 82/85
84077/84077 - 8s - loss: 3.0961e-04 - val_loss: 2.8489e-04 - 8s/epoch - 98us/sample
Epoch 83/85
84077/84077 - 8s - loss: 3.0935e-04 - val_loss: 2.8398e-04 - 8s/epoch - 98us/sample
Epoch 84/85
84077/84077 - 8s - loss: 3.0851e-04 - val_loss: 2.8322e-04 - 8s/epoch - 98us/sample
Epoch 85/85
84077/84077 - 8s - loss: 3.0980e-04 - val_loss: 2.8375e-04 - 8s/epoch - 98us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002837500915737706
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:19:44.136334: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_42/outputlayer/BiasAdd' id:54362 op device:{requested: '', assigned: ''} def:{{{node decoder_model_42/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_42/outputlayer/MatMul, decoder_model_42/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03887334412937431
cosine 0.03839876893089335
MAE: 0.0028725043248518257
RMSE: 0.011493368149933536
r2: 0.8968646601264735
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 85, 0.001, 0.1, 94, 0.0003097979036591573, 0.0002837500915737706, 0.03887334412937431, 0.03839876893089335, 0.0028725043248518257, 0.011493368149933536, 0.8968646601264735, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0012000000000000001 16 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_129 (Batch  (None, 2357)        9428        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_129 (ReLU)               (None, 2357)         0           ['batch_normalization_129[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 03:20:03.670645: W tensorflow/c/c_api.cc:291] Operation '{name:'training_86/Adam/batch_normalization_129/gamma/v/Assign' id:56257 op device:{requested: '', assigned: ''} def:{{{node training_86/Adam/batch_normalization_129/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_86/Adam/batch_normalization_129/gamma/v, training_86/Adam/batch_normalization_129/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:20:29.798022: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_43/mul' id:55653 op device:{requested: '', assigned: ''} def:{{{node loss_43/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_43/mul/x, loss_43/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 34s - loss: 0.0030 - val_loss: 8.5968e-04 - 34s/epoch - 404us/sample
Epoch 2/50
84077/84077 - 23s - loss: 6.6285e-04 - val_loss: 0.0077 - 23s/epoch - 274us/sample
Epoch 3/50
84077/84077 - 23s - loss: 5.7325e-04 - val_loss: 5.6946e-04 - 23s/epoch - 276us/sample
Epoch 4/50
84077/84077 - 23s - loss: 5.6696e-04 - val_loss: 5.6483e-04 - 23s/epoch - 275us/sample
Epoch 5/50
84077/84077 - 23s - loss: 5.6469e-04 - val_loss: 5.6418e-04 - 23s/epoch - 274us/sample
Epoch 6/50
84077/84077 - 23s - loss: 5.6395e-04 - val_loss: 5.6254e-04 - 23s/epoch - 274us/sample
Epoch 7/50
84077/84077 - 23s - loss: 5.6335e-04 - val_loss: 5.6311e-04 - 23s/epoch - 275us/sample
Epoch 8/50
84077/84077 - 23s - loss: 5.6312e-04 - val_loss: 5.6271e-04 - 23s/epoch - 275us/sample
Epoch 9/50
84077/84077 - 23s - loss: 5.6289e-04 - val_loss: 5.6297e-04 - 23s/epoch - 276us/sample
Epoch 10/50
84077/84077 - 23s - loss: 5.6284e-04 - val_loss: 5.6251e-04 - 23s/epoch - 274us/sample
Epoch 11/50
84077/84077 - 23s - loss: 5.6292e-04 - val_loss: 5.6203e-04 - 23s/epoch - 273us/sample
Epoch 12/50
84077/84077 - 23s - loss: 5.6268e-04 - val_loss: 5.6176e-04 - 23s/epoch - 275us/sample
Epoch 13/50
84077/84077 - 23s - loss: 5.6270e-04 - val_loss: 5.6215e-04 - 23s/epoch - 275us/sample
Epoch 14/50
84077/84077 - 23s - loss: 5.6279e-04 - val_loss: 5.6214e-04 - 23s/epoch - 274us/sample
Epoch 15/50
84077/84077 - 23s - loss: 5.6269e-04 - val_loss: 5.6243e-04 - 23s/epoch - 276us/sample
Epoch 16/50
84077/84077 - 23s - loss: 5.6280e-04 - val_loss: 5.6222e-04 - 23s/epoch - 275us/sample
Epoch 17/50
84077/84077 - 23s - loss: 5.6271e-04 - val_loss: 5.6188e-04 - 23s/epoch - 274us/sample
Epoch 18/50
84077/84077 - 23s - loss: 5.6268e-04 - val_loss: 5.6200e-04 - 23s/epoch - 275us/sample
Epoch 19/50
84077/84077 - 23s - loss: 5.6275e-04 - val_loss: 5.6254e-04 - 23s/epoch - 275us/sample
Epoch 20/50
84077/84077 - 23s - loss: 5.6259e-04 - val_loss: 5.6208e-04 - 23s/epoch - 274us/sample
Epoch 21/50
84077/84077 - 23s - loss: 5.6275e-04 - val_loss: 5.6244e-04 - 23s/epoch - 274us/sample
Epoch 22/50
84077/84077 - 23s - loss: 5.6267e-04 - val_loss: 5.6175e-04 - 23s/epoch - 275us/sample
Epoch 23/50
84077/84077 - 23s - loss: 5.6271e-04 - val_loss: 5.6222e-04 - 23s/epoch - 274us/sample
Epoch 24/50
84077/84077 - 23s - loss: 5.6257e-04 - val_loss: 5.6205e-04 - 23s/epoch - 273us/sample
Epoch 25/50
84077/84077 - 23s - loss: 5.6234e-04 - val_loss: 5.6176e-04 - 23s/epoch - 275us/sample
Epoch 26/50
84077/84077 - 23s - loss: 5.6249e-04 - val_loss: 5.6209e-04 - 23s/epoch - 274us/sample
Epoch 27/50
84077/84077 - 23s - loss: 5.6257e-04 - val_loss: 5.6168e-04 - 23s/epoch - 274us/sample
Epoch 28/50
84077/84077 - 23s - loss: 5.6254e-04 - val_loss: 5.6189e-04 - 23s/epoch - 276us/sample
Epoch 29/50
84077/84077 - 23s - loss: 5.6267e-04 - val_loss: 5.6172e-04 - 23s/epoch - 274us/sample
Epoch 30/50
84077/84077 - 23s - loss: 5.6269e-04 - val_loss: 5.6198e-04 - 23s/epoch - 274us/sample
Epoch 31/50
84077/84077 - 23s - loss: 5.6251e-04 - val_loss: 5.6224e-04 - 23s/epoch - 275us/sample
Epoch 32/50
84077/84077 - 23s - loss: 5.6259e-04 - val_loss: 5.6204e-04 - 23s/epoch - 273us/sample
Epoch 33/50
84077/84077 - 23s - loss: 5.6230e-04 - val_loss: 5.6200e-04 - 23s/epoch - 273us/sample
Epoch 34/50
84077/84077 - 23s - loss: 5.6258e-04 - val_loss: 5.6201e-04 - 23s/epoch - 275us/sample
Epoch 35/50
84077/84077 - 23s - loss: 5.6259e-04 - val_loss: 5.6213e-04 - 23s/epoch - 274us/sample
Epoch 36/50
84077/84077 - 23s - loss: 5.6277e-04 - val_loss: 5.6201e-04 - 23s/epoch - 274us/sample
Epoch 37/50
84077/84077 - 23s - loss: 5.6253e-04 - val_loss: 5.6201e-04 - 23s/epoch - 275us/sample
Epoch 38/50
84077/84077 - 23s - loss: 5.6257e-04 - val_loss: 5.6228e-04 - 23s/epoch - 274us/sample
Epoch 39/50
84077/84077 - 23s - loss: 5.6250e-04 - val_loss: 5.6201e-04 - 23s/epoch - 274us/sample
Epoch 40/50
84077/84077 - 23s - loss: 5.6251e-04 - val_loss: 5.6260e-04 - 23s/epoch - 275us/sample
Epoch 41/50
84077/84077 - 23s - loss: 5.6245e-04 - val_loss: 5.6193e-04 - 23s/epoch - 273us/sample
Epoch 42/50
84077/84077 - 23s - loss: 5.6256e-04 - val_loss: 5.6217e-04 - 23s/epoch - 274us/sample
Epoch 43/50
84077/84077 - 23s - loss: 5.6260e-04 - val_loss: 5.6203e-04 - 23s/epoch - 275us/sample
Epoch 44/50
84077/84077 - 23s - loss: 5.6242e-04 - val_loss: 5.6210e-04 - 23s/epoch - 274us/sample
Epoch 45/50
84077/84077 - 23s - loss: 5.6248e-04 - val_loss: 5.6222e-04 - 23s/epoch - 273us/sample
Epoch 46/50
84077/84077 - 23s - loss: 5.6272e-04 - val_loss: 5.6170e-04 - 23s/epoch - 276us/sample
Epoch 47/50
84077/84077 - 23s - loss: 5.6247e-04 - val_loss: 5.6216e-04 - 23s/epoch - 274us/sample
Epoch 48/50
84077/84077 - 23s - loss: 5.6255e-04 - val_loss: 5.6204e-04 - 23s/epoch - 274us/sample
Epoch 49/50
84077/84077 - 23s - loss: 5.6244e-04 - val_loss: 5.6175e-04 - 23s/epoch - 275us/sample
Epoch 50/50
84077/84077 - 23s - loss: 5.6272e-04 - val_loss: 5.6149e-04 - 23s/epoch - 273us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0005614942814409095
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:39:25.128840: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_43/outputlayer/BiasAdd' id:55617 op device:{requested: '', assigned: ''} def:{{{node decoder_model_43/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_43/outputlayer/MatMul, decoder_model_43/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.42582587793433746
cosine 0.41858663882713276
MAE: 0.0062680710716785494
RMSE: 0.03379892622743718
r2: 0.10757934509163856
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 16, 50, 0.0012000000000000001, 0.1, 94, 0.0005627246269620897, 0.0005614942814409095, 0.42582587793433746, 0.41858663882713276, 0.0062680710716785494, 0.03379892622743718, 0.10757934509163856, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 85 0.001 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_132 (Batch  (None, 2357)        9428        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_132 (ReLU)               (None, 2357)         0           ['batch_normalization_132[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           221652      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           221652      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2466243     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,143,983
Trainable params: 5,134,367
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 03:39:44.632396: W tensorflow/c/c_api.cc:291] Operation '{name:'training_88/Adam/dense_dec1_44/bias/v/Assign' id:57585 op device:{requested: '', assigned: ''} def:{{{node training_88/Adam/dense_dec1_44/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_88/Adam/dense_dec1_44/bias/v, training_88/Adam/dense_dec1_44/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:39:57.277053: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_44/mul' id:56938 op device:{requested: '', assigned: ''} def:{{{node loss_44/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_44/mul/x, loss_44/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0129 - val_loss: 0.0025 - 20s/epoch - 236us/sample
Epoch 2/85
84077/84077 - 8s - loss: 0.0020 - val_loss: 9.6220e-04 - 8s/epoch - 101us/sample
Epoch 3/85
84077/84077 - 8s - loss: 8.2120e-04 - val_loss: 6.7996e-04 - 8s/epoch - 101us/sample
Epoch 4/85
84077/84077 - 8s - loss: 8.1591e-04 - val_loss: 0.0014 - 8s/epoch - 101us/sample
Epoch 5/85
84077/84077 - 8s - loss: 7.8745e-04 - val_loss: 6.4823e-04 - 8s/epoch - 101us/sample
Epoch 6/85
84077/84077 - 8s - loss: 5.5168e-04 - val_loss: 7528588458.3384 - 8s/epoch - 101us/sample
Epoch 7/85
84077/84077 - 8s - loss: 4.8501e-04 - val_loss: 4.3658e-04 - 8s/epoch - 101us/sample
Epoch 8/85
84077/84077 - 8s - loss: 4.3685e-04 - val_loss: 3.9966e-04 - 8s/epoch - 101us/sample
Epoch 9/85
84077/84077 - 9s - loss: 4.1215e-04 - val_loss: 3.6949e-04 - 9s/epoch - 101us/sample
Epoch 10/85
84077/84077 - 9s - loss: 3.8356e-04 - val_loss: 3.4762e-04 - 9s/epoch - 102us/sample
Epoch 11/85
84077/84077 - 9s - loss: 3.6019e-04 - val_loss: 3.3221e-04 - 9s/epoch - 101us/sample
Epoch 12/85
84077/84077 - 8s - loss: 3.4496e-04 - val_loss: 3.1863e-04 - 8s/epoch - 101us/sample
Epoch 13/85
84077/84077 - 8s - loss: 3.3198e-04 - val_loss: 3.0964e-04 - 8s/epoch - 101us/sample
Epoch 14/85
84077/84077 - 8s - loss: 3.1966e-04 - val_loss: 2.9659e-04 - 8s/epoch - 101us/sample
Epoch 15/85
84077/84077 - 8s - loss: 3.1032e-04 - val_loss: 3.4814e-04 - 8s/epoch - 101us/sample
Epoch 16/85
84077/84077 - 8s - loss: 3.0148e-04 - val_loss: 2.7935e-04 - 8s/epoch - 101us/sample
Epoch 17/85
84077/84077 - 9s - loss: 2.9416e-04 - val_loss: 2.7601e-04 - 9s/epoch - 101us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.8728e-04 - val_loss: 2.6221e-04 - 9s/epoch - 102us/sample
Epoch 19/85
84077/84077 - 9s - loss: 2.8109e-04 - val_loss: 2.5947e-04 - 9s/epoch - 101us/sample
Epoch 20/85
84077/84077 - 8s - loss: 2.7573e-04 - val_loss: 2.5493e-04 - 8s/epoch - 101us/sample
Epoch 21/85
84077/84077 - 8s - loss: 2.7136e-04 - val_loss: 2.4907e-04 - 8s/epoch - 101us/sample
Epoch 22/85
84077/84077 - 8s - loss: 2.6745e-04 - val_loss: 2.4710e-04 - 8s/epoch - 101us/sample
Epoch 23/85
84077/84077 - 8s - loss: 2.6277e-04 - val_loss: 2.4335e-04 - 8s/epoch - 101us/sample
Epoch 24/85
84077/84077 - 8s - loss: 2.5967e-04 - val_loss: 2.3919e-04 - 8s/epoch - 101us/sample
Epoch 25/85
84077/84077 - 9s - loss: 2.5763e-04 - val_loss: 2.3874e-04 - 9s/epoch - 102us/sample
Epoch 26/85
84077/84077 - 8s - loss: 2.5419e-04 - val_loss: 2.3415e-04 - 8s/epoch - 101us/sample
Epoch 27/85
84077/84077 - 9s - loss: 2.5212e-04 - val_loss: 2.3383e-04 - 9s/epoch - 101us/sample
Epoch 28/85
84077/84077 - 8s - loss: 2.5007e-04 - val_loss: 2.3209e-04 - 8s/epoch - 101us/sample
Epoch 29/85
84077/84077 - 8s - loss: 2.4787e-04 - val_loss: 2.2951e-04 - 8s/epoch - 101us/sample
Epoch 30/85
84077/84077 - 8s - loss: 2.4661e-04 - val_loss: 2.2727e-04 - 8s/epoch - 100us/sample
Epoch 31/85
84077/84077 - 8s - loss: 2.4498e-04 - val_loss: 2.2777e-04 - 8s/epoch - 101us/sample
Epoch 32/85
84077/84077 - 8s - loss: 2.4331e-04 - val_loss: 2.2550e-04 - 8s/epoch - 101us/sample
Epoch 33/85
84077/84077 - 9s - loss: 2.4107e-04 - val_loss: 2.2458e-04 - 9s/epoch - 102us/sample
Epoch 34/85
84077/84077 - 9s - loss: 2.3975e-04 - val_loss: 2.2329e-04 - 9s/epoch - 102us/sample
Epoch 35/85
84077/84077 - 9s - loss: 2.3706e-04 - val_loss: 2.2125e-04 - 9s/epoch - 102us/sample
Epoch 36/85
84077/84077 - 8s - loss: 2.3595e-04 - val_loss: 2.2031e-04 - 8s/epoch - 101us/sample
Epoch 37/85
84077/84077 - 8s - loss: 2.3442e-04 - val_loss: 2.1984e-04 - 8s/epoch - 101us/sample
Epoch 38/85
84077/84077 - 8s - loss: 2.3327e-04 - val_loss: 2.1990e-04 - 8s/epoch - 101us/sample
Epoch 39/85
84077/84077 - 8s - loss: 2.3266e-04 - val_loss: 2.1603e-04 - 8s/epoch - 101us/sample
Epoch 40/85
84077/84077 - 8s - loss: 2.3162e-04 - val_loss: 2.1637e-04 - 8s/epoch - 101us/sample
Epoch 41/85
84077/84077 - 9s - loss: 2.2987e-04 - val_loss: 2.1532e-04 - 9s/epoch - 102us/sample
Epoch 42/85
84077/84077 - 9s - loss: 2.2961e-04 - val_loss: 2.1393e-04 - 9s/epoch - 101us/sample
Epoch 43/85
84077/84077 - 9s - loss: 2.2840e-04 - val_loss: 2.1254e-04 - 9s/epoch - 101us/sample
Epoch 44/85
84077/84077 - 8s - loss: 2.2793e-04 - val_loss: 2.1338e-04 - 8s/epoch - 101us/sample
Epoch 45/85
84077/84077 - 8s - loss: 2.2708e-04 - val_loss: 2.1208e-04 - 8s/epoch - 101us/sample
Epoch 46/85
84077/84077 - 8s - loss: 2.2639e-04 - val_loss: 2.1246e-04 - 8s/epoch - 101us/sample
Epoch 47/85
84077/84077 - 8s - loss: 2.2499e-04 - val_loss: 2.1047e-04 - 8s/epoch - 101us/sample
Epoch 48/85
84077/84077 - 9s - loss: 2.2450e-04 - val_loss: 2.0968e-04 - 9s/epoch - 101us/sample
Epoch 49/85
84077/84077 - 9s - loss: 2.2423e-04 - val_loss: 2.0827e-04 - 9s/epoch - 102us/sample
Epoch 50/85
84077/84077 - 9s - loss: 2.2347e-04 - val_loss: 2.1087e-04 - 9s/epoch - 101us/sample
Epoch 51/85
84077/84077 - 9s - loss: 2.2315e-04 - val_loss: 2.0909e-04 - 9s/epoch - 101us/sample
Epoch 52/85
84077/84077 - 8s - loss: 2.2248e-04 - val_loss: 2.0854e-04 - 8s/epoch - 101us/sample
Epoch 53/85
84077/84077 - 8s - loss: 2.2185e-04 - val_loss: 2.0796e-04 - 8s/epoch - 101us/sample
Epoch 54/85
84077/84077 - 8s - loss: 2.2173e-04 - val_loss: 2.0948e-04 - 8s/epoch - 101us/sample
Epoch 55/85
84077/84077 - 8s - loss: 2.2100e-04 - val_loss: 2.0780e-04 - 8s/epoch - 101us/sample
Epoch 56/85
84077/84077 - 8s - loss: 2.2054e-04 - val_loss: 2.0880e-04 - 8s/epoch - 101us/sample
Epoch 57/85
84077/84077 - 9s - loss: 2.2018e-04 - val_loss: 2.0601e-04 - 9s/epoch - 102us/sample
Epoch 58/85
84077/84077 - 8s - loss: 2.1991e-04 - val_loss: 2.0852e-04 - 8s/epoch - 101us/sample
Epoch 59/85
84077/84077 - 9s - loss: 2.1936e-04 - val_loss: 2.0737e-04 - 9s/epoch - 101us/sample
Epoch 60/85
84077/84077 - 8s - loss: 2.1940e-04 - val_loss: 2.0569e-04 - 8s/epoch - 101us/sample
Epoch 61/85
84077/84077 - 8s - loss: 2.1770e-04 - val_loss: 2.0506e-04 - 8s/epoch - 101us/sample
Epoch 62/85
84077/84077 - 8s - loss: 2.1808e-04 - val_loss: 2.0594e-04 - 8s/epoch - 101us/sample
Epoch 63/85
84077/84077 - 8s - loss: 2.1755e-04 - val_loss: 2.0479e-04 - 8s/epoch - 101us/sample
Epoch 64/85
84077/84077 - 8s - loss: 2.1698e-04 - val_loss: 2.0590e-04 - 8s/epoch - 101us/sample
Epoch 65/85
84077/84077 - 9s - loss: 2.1672e-04 - val_loss: 2.0503e-04 - 9s/epoch - 102us/sample
Epoch 66/85
84077/84077 - 9s - loss: 2.1655e-04 - val_loss: 2.0373e-04 - 9s/epoch - 101us/sample
Epoch 67/85
84077/84077 - 9s - loss: 2.1555e-04 - val_loss: 2.0523e-04 - 9s/epoch - 101us/sample
Epoch 68/85
84077/84077 - 8s - loss: 2.1546e-04 - val_loss: 2.0204e-04 - 8s/epoch - 101us/sample
Epoch 69/85
84077/84077 - 8s - loss: 2.1461e-04 - val_loss: 2.0366e-04 - 8s/epoch - 101us/sample
Epoch 70/85
84077/84077 - 8s - loss: 2.1472e-04 - val_loss: 2.0219e-04 - 8s/epoch - 101us/sample
Epoch 71/85
84077/84077 - 8s - loss: 2.1457e-04 - val_loss: 2.0412e-04 - 8s/epoch - 101us/sample
Epoch 72/85
84077/84077 - 8s - loss: 2.1403e-04 - val_loss: 2.0298e-04 - 8s/epoch - 101us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.1402e-04 - val_loss: 2.0102e-04 - 9s/epoch - 102us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.1321e-04 - val_loss: 2.0194e-04 - 9s/epoch - 101us/sample
Epoch 75/85
84077/84077 - 8s - loss: 2.1372e-04 - val_loss: 2.0095e-04 - 8s/epoch - 101us/sample
Epoch 76/85
84077/84077 - 8s - loss: 2.1291e-04 - val_loss: 2.0280e-04 - 8s/epoch - 101us/sample
Epoch 77/85
84077/84077 - 8s - loss: 2.1325e-04 - val_loss: 2.0240e-04 - 8s/epoch - 101us/sample
Epoch 78/85
84077/84077 - 8s - loss: 2.1260e-04 - val_loss: 2.0232e-04 - 8s/epoch - 101us/sample
Epoch 79/85
84077/84077 - 8s - loss: 2.1218e-04 - val_loss: 2.0149e-04 - 8s/epoch - 101us/sample
Epoch 80/85
84077/84077 - 8s - loss: 2.1205e-04 - val_loss: 2.0080e-04 - 8s/epoch - 101us/sample
Epoch 81/85
84077/84077 - 9s - loss: 2.1226e-04 - val_loss: 2.0112e-04 - 9s/epoch - 102us/sample
Epoch 82/85
84077/84077 - 9s - loss: 2.1174e-04 - val_loss: 1.9938e-04 - 9s/epoch - 102us/sample
Epoch 83/85
84077/84077 - 9s - loss: 2.1170e-04 - val_loss: 2.0006e-04 - 9s/epoch - 101us/sample
Epoch 84/85
84077/84077 - 8s - loss: 2.1119e-04 - val_loss: 2.0064e-04 - 8s/epoch - 101us/sample
Epoch 85/85
84077/84077 - 8s - loss: 2.1073e-04 - val_loss: 2.0102e-04 - 8s/epoch - 101us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00020102210755372136
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:51:54.784443: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_44/outputlayer/BiasAdd' id:56902 op device:{requested: '', assigned: ''} def:{{{node decoder_model_44/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_44/outputlayer/MatMul, decoder_model_44/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.057386210964748746
cosine 0.05667814238722832
MAE: 0.0028799952897026132
RMSE: 0.013388191227887909
r2: 0.8600547012943499
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 85, 0.001, 0.1, 94, 0.00021073001163708407, 0.00020102210755372136, 0.057386210964748746, 0.05667814238722832, 0.0028799952897026132, 0.013388191227887909, 0.8600547012943499, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 80 0.001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2451)         2313744     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_135 (Batch  (None, 2451)        9804        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_135 (ReLU)               (None, 2451)         0           ['batch_normalization_135[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           230488      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           230488      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2564191     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,348,715
Trainable params: 5,338,723
Non-trainable params: 9,992
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-15 03:52:15.017745: W tensorflow/c/c_api.cc:291] Operation '{name:'training_90/Adam/dense_enc0_45/bias/m/Assign' id:58676 op device:{requested: '', assigned: ''} def:{{{node training_90/Adam/dense_enc0_45/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_90/Adam/dense_enc0_45/bias/m, training_90/Adam/dense_enc0_45/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:52:32.291290: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_45/mul' id:58216 op device:{requested: '', assigned: ''} def:{{{node loss_45/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_45/mul/x, loss_45/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 25s - loss: 0.1188 - val_loss: 0.0024 - 25s/epoch - 297us/sample
Epoch 2/80
84077/84077 - 13s - loss: 0.0341 - val_loss: 0.0030 - 13s/epoch - 158us/sample
Epoch 3/80
84077/84077 - 13s - loss: 0.0036 - val_loss: 17585735.3593 - 13s/epoch - 158us/sample
Epoch 4/80
84077/84077 - 13s - loss: 0.0274 - val_loss: 0.0013 - 13s/epoch - 158us/sample
Epoch 5/80
84077/84077 - 13s - loss: 0.0011 - val_loss: 0.0011 - 13s/epoch - 158us/sample
Epoch 6/80
84077/84077 - 13s - loss: 0.0014 - val_loss: 9.4906e-04 - 13s/epoch - 159us/sample
Epoch 7/80
84077/84077 - 13s - loss: 0.0012 - val_loss: 8.2114e-04 - 13s/epoch - 159us/sample
Epoch 8/80
84077/84077 - 13s - loss: 8.4868e-04 - val_loss: 8.1173e-04 - 13s/epoch - 159us/sample
Epoch 9/80
84077/84077 - 13s - loss: 8.8479e-04 - val_loss: 7.5610e-04 - 13s/epoch - 158us/sample
Epoch 10/80
84077/84077 - 13s - loss: 0.0010 - val_loss: 7.2684e-04 - 13s/epoch - 158us/sample
Epoch 11/80
84077/84077 - 13s - loss: 7.5266e-04 - val_loss: 6.9337e-04 - 13s/epoch - 160us/sample
Epoch 12/80
84077/84077 - 13s - loss: 7.2624e-04 - val_loss: 6.6244e-04 - 13s/epoch - 159us/sample
Epoch 13/80
84077/84077 - 13s - loss: 6.8548e-04 - val_loss: 6.3408e-04 - 13s/epoch - 158us/sample
Epoch 14/80
84077/84077 - 13s - loss: 6.5957e-04 - val_loss: 6.0152e-04 - 13s/epoch - 158us/sample
Epoch 15/80
84077/84077 - 13s - loss: 6.3819e-04 - val_loss: 5.8596e-04 - 13s/epoch - 158us/sample
Epoch 16/80
84077/84077 - 13s - loss: 6.2434e-04 - val_loss: 5.7197e-04 - 13s/epoch - 159us/sample
Epoch 17/80
84077/84077 - 13s - loss: 6.1033e-04 - val_loss: 5.6511e-04 - 13s/epoch - 160us/sample
Epoch 18/80
84077/84077 - 13s - loss: 5.9934e-04 - val_loss: 5.5211e-04 - 13s/epoch - 159us/sample
Epoch 19/80
84077/84077 - 13s - loss: 5.8842e-04 - val_loss: 5.5453e-04 - 13s/epoch - 159us/sample
Epoch 20/80
84077/84077 - 13s - loss: 5.8316e-04 - val_loss: 5.4418e-04 - 13s/epoch - 158us/sample
Epoch 21/80
84077/84077 - 13s - loss: 5.7758e-04 - val_loss: 5.3231e-04 - 13s/epoch - 159us/sample
Epoch 22/80
84077/84077 - 13s - loss: 5.7262e-04 - val_loss: 5.3545e-04 - 13s/epoch - 160us/sample
Epoch 23/80
84077/84077 - 13s - loss: 5.6961e-04 - val_loss: 5.3105e-04 - 13s/epoch - 159us/sample
Epoch 24/80
84077/84077 - 13s - loss: 5.6708e-04 - val_loss: 5.2880e-04 - 13s/epoch - 158us/sample
Epoch 25/80
84077/84077 - 13s - loss: 5.6353e-04 - val_loss: 5.2804e-04 - 13s/epoch - 158us/sample
Epoch 26/80
84077/84077 - 13s - loss: 5.6080e-04 - val_loss: 5.2671e-04 - 13s/epoch - 158us/sample
Epoch 27/80
84077/84077 - 13s - loss: 5.5830e-04 - val_loss: 5.2085e-04 - 13s/epoch - 159us/sample
Epoch 28/80
84077/84077 - 13s - loss: 5.5611e-04 - val_loss: 5.2165e-04 - 13s/epoch - 160us/sample
Epoch 29/80
84077/84077 - 13s - loss: 5.5326e-04 - val_loss: 5.1640e-04 - 13s/epoch - 159us/sample
Epoch 30/80
84077/84077 - 13s - loss: 5.5135e-04 - val_loss: 5.1587e-04 - 13s/epoch - 159us/sample
Epoch 31/80
84077/84077 - 13s - loss: 5.5031e-04 - val_loss: 5.1205e-04 - 13s/epoch - 158us/sample
Epoch 32/80
84077/84077 - 13s - loss: 5.4764e-04 - val_loss: 5.1043e-04 - 13s/epoch - 159us/sample
Epoch 33/80
84077/84077 - 13s - loss: 5.4602e-04 - val_loss: 5.0911e-04 - 13s/epoch - 159us/sample
Epoch 34/80
84077/84077 - 13s - loss: 5.4444e-04 - val_loss: 5.0744e-04 - 13s/epoch - 160us/sample
Epoch 35/80
84077/84077 - 13s - loss: 5.4375e-04 - val_loss: 5.0681e-04 - 13s/epoch - 158us/sample
Epoch 36/80
84077/84077 - 13s - loss: 5.4234e-04 - val_loss: 5.0463e-04 - 13s/epoch - 158us/sample
Epoch 37/80
84077/84077 - 13s - loss: 5.4044e-04 - val_loss: 5.0100e-04 - 13s/epoch - 158us/sample
Epoch 38/80
84077/84077 - 13s - loss: 5.3902e-04 - val_loss: 5.0305e-04 - 13s/epoch - 159us/sample
Epoch 39/80
84077/84077 - 13s - loss: 5.3915e-04 - val_loss: 5.0176e-04 - 13s/epoch - 160us/sample
Epoch 40/80
84077/84077 - 13s - loss: 5.3791e-04 - val_loss: 5.0137e-04 - 13s/epoch - 159us/sample
Epoch 41/80
84077/84077 - 13s - loss: 5.3645e-04 - val_loss: 4.9866e-04 - 13s/epoch - 158us/sample
Epoch 42/80
84077/84077 - 13s - loss: 5.3560e-04 - val_loss: 4.9688e-04 - 13s/epoch - 158us/sample
Epoch 43/80
84077/84077 - 13s - loss: 5.3354e-04 - val_loss: 4.9683e-04 - 13s/epoch - 159us/sample
Epoch 44/80
84077/84077 - 13s - loss: 5.3249e-04 - val_loss: 4.9508e-04 - 13s/epoch - 160us/sample
Epoch 45/80
84077/84077 - 13s - loss: 5.3239e-04 - val_loss: 4.9353e-04 - 13s/epoch - 159us/sample
Epoch 46/80
84077/84077 - 13s - loss: 5.3058e-04 - val_loss: 4.9444e-04 - 13s/epoch - 159us/sample
Epoch 47/80
84077/84077 - 13s - loss: 5.3039e-04 - val_loss: 4.9390e-04 - 13s/epoch - 158us/sample
Epoch 48/80
84077/84077 - 13s - loss: 5.2878e-04 - val_loss: 4.9400e-04 - 13s/epoch - 158us/sample
Epoch 49/80
84077/84077 - 13s - loss: 5.2883e-04 - val_loss: 4.9380e-04 - 13s/epoch - 159us/sample
Epoch 50/80
84077/84077 - 13s - loss: 5.2810e-04 - val_loss: 4.9186e-04 - 13s/epoch - 160us/sample
Epoch 51/80
84077/84077 - 13s - loss: 5.2836e-04 - val_loss: 4.8977e-04 - 13s/epoch - 158us/sample
Epoch 52/80
84077/84077 - 13s - loss: 5.2745e-04 - val_loss: 4.9090e-04 - 13s/epoch - 159us/sample
Epoch 53/80
84077/84077 - 13s - loss: 5.2698e-04 - val_loss: 4.8983e-04 - 13s/epoch - 159us/sample
Epoch 54/80
84077/84077 - 13s - loss: 5.2623e-04 - val_loss: 4.8900e-04 - 13s/epoch - 160us/sample
Epoch 55/80
84077/84077 - 13s - loss: 5.2504e-04 - val_loss: 4.8827e-04 - 13s/epoch - 159us/sample
Epoch 56/80
84077/84077 - 13s - loss: 5.2490e-04 - val_loss: 4.8698e-04 - 13s/epoch - 159us/sample
Epoch 57/80
84077/84077 - 13s - loss: 5.2395e-04 - val_loss: 4.8786e-04 - 13s/epoch - 159us/sample
Epoch 58/80
84077/84077 - 13s - loss: 5.2325e-04 - val_loss: 4.8718e-04 - 13s/epoch - 159us/sample
Epoch 59/80
84077/84077 - 13s - loss: 5.2288e-04 - val_loss: 4.8938e-04 - 13s/epoch - 160us/sample
Epoch 60/80
84077/84077 - 13s - loss: 5.2237e-04 - val_loss: 4.8752e-04 - 13s/epoch - 159us/sample
Epoch 61/80
84077/84077 - 13s - loss: 5.2185e-04 - val_loss: 4.8830e-04 - 13s/epoch - 158us/sample
Epoch 62/80
84077/84077 - 13s - loss: 5.2143e-04 - val_loss: 4.8806e-04 - 13s/epoch - 159us/sample
Epoch 63/80
84077/84077 - 13s - loss: 5.2162e-04 - val_loss: 4.8718e-04 - 13s/epoch - 159us/sample
Epoch 64/80
84077/84077 - 13s - loss: 5.2015e-04 - val_loss: 4.8519e-04 - 13s/epoch - 160us/sample
Epoch 65/80
84077/84077 - 13s - loss: 5.2045e-04 - val_loss: 4.8663e-04 - 13s/epoch - 159us/sample
Epoch 66/80
84077/84077 - 13s - loss: 5.1990e-04 - val_loss: 4.8692e-04 - 13s/epoch - 158us/sample
Epoch 67/80
84077/84077 - 13s - loss: 5.1964e-04 - val_loss: 4.8515e-04 - 13s/epoch - 158us/sample
Epoch 68/80
84077/84077 - 13s - loss: 5.1928e-04 - val_loss: 4.8506e-04 - 13s/epoch - 158us/sample
Epoch 69/80
84077/84077 - 13s - loss: 5.1879e-04 - val_loss: 4.8648e-04 - 13s/epoch - 159us/sample
Epoch 70/80
84077/84077 - 13s - loss: 5.1824e-04 - val_loss: 4.8452e-04 - 13s/epoch - 160us/sample
Epoch 71/80
84077/84077 - 13s - loss: 5.1804e-04 - val_loss: 4.8367e-04 - 13s/epoch - 159us/sample
Epoch 72/80
84077/84077 - 13s - loss: 5.1723e-04 - val_loss: 4.8464e-04 - 13s/epoch - 159us/sample
Epoch 73/80
84077/84077 - 13s - loss: 5.1717e-04 - val_loss: 4.8310e-04 - 13s/epoch - 158us/sample
Epoch 74/80
84077/84077 - 13s - loss: 5.1684e-04 - val_loss: 4.8411e-04 - 13s/epoch - 159us/sample
Epoch 75/80
84077/84077 - 13s - loss: 5.1688e-04 - val_loss: 4.8518e-04 - 13s/epoch - 160us/sample
Epoch 76/80
84077/84077 - 13s - loss: 5.1642e-04 - val_loss: 4.8168e-04 - 13s/epoch - 159us/sample
Epoch 77/80
84077/84077 - 13s - loss: 5.1624e-04 - val_loss: 4.8031e-04 - 13s/epoch - 158us/sample
Epoch 78/80
84077/84077 - 13s - loss: 5.1554e-04 - val_loss: 4.8286e-04 - 13s/epoch - 158us/sample
Epoch 79/80
84077/84077 - 13s - loss: 5.1617e-04 - val_loss: 4.8268e-04 - 13s/epoch - 159us/sample
Epoch 80/80
84077/84077 - 13s - loss: 5.1542e-04 - val_loss: 4.8243e-04 - 13s/epoch - 160us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00048242982741283
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:10:11.660606: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_45/outputlayer/BiasAdd' id:58187 op device:{requested: '', assigned: ''} def:{{{node decoder_model_45/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_45/outputlayer/MatMul, decoder_model_45/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.097702122286319
cosine 0.09644016342094805
MAE: 0.0036165527175892197
RMSE: 0.018719432799284296
r2: 0.726290611725797
RMSE zero-vector: 0.04004287452915337
['2.6custom_VAE', 'mse', 32, 80, 0.001, 0.1, 94, 0.0005154155557324758, 0.00048242982741283, 0.097702122286319, 0.09644016342094805, 0.0036165527175892197, 0.018719432799284296, 0.726290611725797, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 5
Fitness    = 424.308669689168
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 424.308669689168.
Best solutions :  [array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168]
[2.0 90 0.001 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_138 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_138 (ReLU)               (None, 1886)         0           ['batch_normalization_138[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 04:10:31.668104: W tensorflow/c/c_api.cc:291] Operation '{name:'training_92/Adam/bottleneck_zlog_46/kernel/m/Assign' id:59994 op device:{requested: '', assigned: ''} def:{{{node training_92/Adam/bottleneck_zlog_46/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_92/Adam/bottleneck_zlog_46/kernel/m, training_92/Adam/bottleneck_zlog_46/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:10:44.448456: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_46/mul' id:59478 op device:{requested: '', assigned: ''} def:{{{node loss_46/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_46/mul/x, loss_46/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0044 - val_loss: 0.0025 - 20s/epoch - 241us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0016 - val_loss: 7.5093e-04 - 8s/epoch - 100us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0013 - val_loss: 7.4946e-04 - 9s/epoch - 101us/sample
Epoch 4/90
84077/84077 - 8s - loss: 7.0869e-04 - val_loss: 6.4555e-04 - 8s/epoch - 100us/sample
Epoch 5/90
84077/84077 - 8s - loss: 6.4426e-04 - val_loss: 5.4166e-04 - 8s/epoch - 100us/sample
Epoch 6/90
84077/84077 - 8s - loss: 6.7046e-04 - val_loss: 4.7631e-04 - 8s/epoch - 100us/sample
Epoch 7/90
84077/84077 - 8s - loss: 4.5943e-04 - val_loss: 4.4188e-04 - 8s/epoch - 100us/sample
Epoch 8/90
84077/84077 - 8s - loss: 4.3259e-04 - val_loss: 3.9730e-04 - 8s/epoch - 100us/sample
Epoch 9/90
84077/84077 - 8s - loss: 4.0823e-04 - val_loss: 3.7706e-04 - 8s/epoch - 100us/sample
Epoch 10/90
84077/84077 - 8s - loss: 3.8612e-04 - val_loss: 3.4856e-04 - 8s/epoch - 100us/sample
Epoch 11/90
84077/84077 - 8s - loss: 3.5886e-04 - val_loss: 3.3113e-04 - 8s/epoch - 100us/sample
Epoch 12/90
84077/84077 - 8s - loss: 3.4143e-04 - val_loss: 3.1314e-04 - 8s/epoch - 100us/sample
Epoch 13/90
84077/84077 - 8s - loss: 3.2821e-04 - val_loss: 3.0285e-04 - 8s/epoch - 100us/sample
Epoch 14/90
84077/84077 - 8s - loss: 3.1438e-04 - val_loss: 2.9571e-04 - 8s/epoch - 100us/sample
Epoch 15/90
84077/84077 - 8s - loss: 3.0498e-04 - val_loss: 2.8199e-04 - 8s/epoch - 100us/sample
Epoch 16/90
84077/84077 - 8s - loss: 2.9718e-04 - val_loss: 2.7439e-04 - 8s/epoch - 100us/sample
Epoch 17/90
84077/84077 - 8s - loss: 2.8954e-04 - val_loss: 2.6788e-04 - 8s/epoch - 100us/sample
Epoch 18/90
84077/84077 - 8s - loss: 2.8262e-04 - val_loss: 2.6115e-04 - 8s/epoch - 100us/sample
Epoch 19/90
84077/84077 - 8s - loss: 2.7795e-04 - val_loss: 2.5683e-04 - 8s/epoch - 99us/sample
Epoch 20/90
84077/84077 - 8s - loss: 2.7265e-04 - val_loss: 2.5425e-04 - 8s/epoch - 100us/sample
Epoch 21/90
84077/84077 - 8s - loss: 2.6839e-04 - val_loss: 2.4872e-04 - 8s/epoch - 100us/sample
Epoch 22/90
84077/84077 - 8s - loss: 2.6488e-04 - val_loss: 2.4718e-04 - 8s/epoch - 101us/sample
Epoch 23/90
84077/84077 - 8s - loss: 2.6092e-04 - val_loss: 2.4152e-04 - 8s/epoch - 99us/sample
Epoch 24/90
84077/84077 - 8s - loss: 2.5825e-04 - val_loss: 2.4010e-04 - 8s/epoch - 100us/sample
Epoch 25/90
84077/84077 - 8s - loss: 2.5486e-04 - val_loss: 2.3896e-04 - 8s/epoch - 100us/sample
Epoch 26/90
84077/84077 - 8s - loss: 2.5303e-04 - val_loss: 2.3500e-04 - 8s/epoch - 100us/sample
Epoch 27/90
84077/84077 - 8s - loss: 2.4984e-04 - val_loss: 2.3452e-04 - 8s/epoch - 100us/sample
Epoch 28/90
84077/84077 - 8s - loss: 2.4732e-04 - val_loss: 2.3054e-04 - 8s/epoch - 100us/sample
Epoch 29/90
84077/84077 - 8s - loss: 2.4529e-04 - val_loss: 2.2964e-04 - 8s/epoch - 100us/sample
Epoch 30/90
84077/84077 - 8s - loss: 2.4333e-04 - val_loss: 2.2874e-04 - 8s/epoch - 101us/sample
Epoch 31/90
84077/84077 - 8s - loss: 2.4147e-04 - val_loss: 2.2714e-04 - 8s/epoch - 100us/sample
Epoch 32/90
84077/84077 - 8s - loss: 2.4004e-04 - val_loss: 2.2481e-04 - 8s/epoch - 100us/sample
Epoch 33/90
84077/84077 - 8s - loss: 2.3856e-04 - val_loss: 2.2238e-04 - 8s/epoch - 100us/sample
Epoch 34/90
84077/84077 - 8s - loss: 2.3689e-04 - val_loss: 2.2173e-04 - 8s/epoch - 100us/sample
Epoch 35/90
84077/84077 - 8s - loss: 2.3544e-04 - val_loss: 2.1972e-04 - 8s/epoch - 99us/sample
Epoch 36/90
84077/84077 - 8s - loss: 2.3438e-04 - val_loss: 2.1917e-04 - 8s/epoch - 99us/sample
Epoch 37/90
84077/84077 - 8s - loss: 2.3332e-04 - val_loss: 2.2023e-04 - 8s/epoch - 100us/sample
Epoch 38/90
84077/84077 - 8s - loss: 2.3241e-04 - val_loss: 2.1863e-04 - 8s/epoch - 100us/sample
Epoch 39/90
84077/84077 - 9s - loss: 2.3106e-04 - val_loss: 2.1605e-04 - 9s/epoch - 103us/sample
Epoch 40/90
84077/84077 - 8s - loss: 2.3073e-04 - val_loss: 2.1673e-04 - 8s/epoch - 100us/sample
Epoch 41/90
84077/84077 - 8s - loss: 2.2838e-04 - val_loss: 2.1396e-04 - 8s/epoch - 100us/sample
Epoch 42/90
84077/84077 - 8s - loss: 2.2807e-04 - val_loss: 2.1508e-04 - 8s/epoch - 100us/sample
Epoch 43/90
84077/84077 - 8s - loss: 2.2728e-04 - val_loss: 2.1242e-04 - 8s/epoch - 100us/sample
Epoch 44/90
84077/84077 - 8s - loss: 2.2602e-04 - val_loss: 2.1171e-04 - 8s/epoch - 100us/sample
Epoch 45/90
84077/84077 - 8s - loss: 2.2558e-04 - val_loss: 2.1148e-04 - 8s/epoch - 100us/sample
Epoch 46/90
84077/84077 - 8s - loss: 2.2465e-04 - val_loss: 2.1194e-04 - 8s/epoch - 100us/sample
Epoch 47/90
84077/84077 - 9s - loss: 2.2410e-04 - val_loss: 2.1033e-04 - 9s/epoch - 101us/sample
Epoch 48/90
84077/84077 - 8s - loss: 2.2353e-04 - val_loss: 2.0984e-04 - 8s/epoch - 100us/sample
Epoch 49/90
84077/84077 - 8s - loss: 2.2289e-04 - val_loss: 2.1060e-04 - 8s/epoch - 100us/sample
Epoch 50/90
84077/84077 - 8s - loss: 2.2195e-04 - val_loss: 2.0887e-04 - 8s/epoch - 100us/sample
Epoch 51/90
84077/84077 - 8s - loss: 2.2132e-04 - val_loss: 2.0768e-04 - 8s/epoch - 100us/sample
Epoch 52/90
84077/84077 - 8s - loss: 2.2132e-04 - val_loss: 2.0762e-04 - 8s/epoch - 100us/sample
Epoch 53/90
84077/84077 - 8s - loss: 2.2046e-04 - val_loss: 2.0702e-04 - 8s/epoch - 100us/sample
Epoch 54/90
84077/84077 - 8s - loss: 2.2060e-04 - val_loss: 2.0735e-04 - 8s/epoch - 100us/sample
Epoch 55/90
84077/84077 - 8s - loss: 2.1951e-04 - val_loss: 2.0699e-04 - 8s/epoch - 101us/sample
Epoch 56/90
84077/84077 - 8s - loss: 2.1897e-04 - val_loss: 2.0757e-04 - 8s/epoch - 99us/sample
Epoch 57/90
84077/84077 - 8s - loss: 2.1846e-04 - val_loss: 2.0551e-04 - 8s/epoch - 100us/sample
Epoch 58/90
84077/84077 - 8s - loss: 2.1810e-04 - val_loss: 2.0555e-04 - 8s/epoch - 100us/sample
Epoch 59/90
84077/84077 - 8s - loss: 2.1823e-04 - val_loss: 2.0534e-04 - 8s/epoch - 99us/sample
Epoch 60/90
84077/84077 - 8s - loss: 2.1787e-04 - val_loss: 2.0521e-04 - 8s/epoch - 100us/sample
Epoch 61/90
84077/84077 - 8s - loss: 2.1700e-04 - val_loss: 2.0506e-04 - 8s/epoch - 100us/sample
Epoch 62/90
84077/84077 - 8s - loss: 2.1685e-04 - val_loss: 2.0605e-04 - 8s/epoch - 100us/sample
Epoch 63/90
84077/84077 - 8s - loss: 2.1629e-04 - val_loss: 2.0396e-04 - 8s/epoch - 101us/sample
Epoch 64/90
84077/84077 - 8s - loss: 2.1651e-04 - val_loss: 2.0536e-04 - 8s/epoch - 100us/sample
Epoch 65/90
84077/84077 - 8s - loss: 2.1563e-04 - val_loss: 2.0246e-04 - 8s/epoch - 100us/sample
Epoch 66/90
84077/84077 - 8s - loss: 2.1523e-04 - val_loss: 2.0339e-04 - 8s/epoch - 100us/sample
Epoch 67/90
84077/84077 - 8s - loss: 2.1522e-04 - val_loss: 2.0316e-04 - 8s/epoch - 99us/sample
Epoch 68/90
84077/84077 - 8s - loss: 2.1469e-04 - val_loss: 2.0449e-04 - 8s/epoch - 100us/sample
Epoch 69/90
84077/84077 - 8s - loss: 2.1418e-04 - val_loss: 2.0264e-04 - 8s/epoch - 99us/sample
Epoch 70/90
84077/84077 - 8s - loss: 2.1450e-04 - val_loss: 2.0231e-04 - 8s/epoch - 100us/sample
Epoch 71/90
84077/84077 - 8s - loss: 2.1388e-04 - val_loss: 2.0309e-04 - 8s/epoch - 99us/sample
Epoch 72/90
84077/84077 - 8s - loss: 2.1420e-04 - val_loss: 2.0344e-04 - 8s/epoch - 101us/sample
Epoch 73/90
84077/84077 - 8s - loss: 2.1362e-04 - val_loss: 2.0101e-04 - 8s/epoch - 100us/sample
Epoch 74/90
84077/84077 - 8s - loss: 2.1401e-04 - val_loss: 2.0203e-04 - 8s/epoch - 100us/sample
Epoch 75/90
84077/84077 - 8s - loss: 2.1292e-04 - val_loss: 2.0152e-04 - 8s/epoch - 100us/sample
Epoch 76/90
84077/84077 - 8s - loss: 2.1263e-04 - val_loss: 2.0234e-04 - 8s/epoch - 100us/sample
Epoch 77/90
84077/84077 - 8s - loss: 2.1244e-04 - val_loss: 2.0169e-04 - 8s/epoch - 100us/sample
Epoch 78/90
84077/84077 - 8s - loss: 2.1234e-04 - val_loss: 2.0149e-04 - 8s/epoch - 100us/sample
Epoch 79/90
84077/84077 - 8s - loss: 2.1235e-04 - val_loss: 2.0026e-04 - 8s/epoch - 100us/sample
Epoch 80/90
84077/84077 - 9s - loss: 2.1192e-04 - val_loss: 2.0221e-04 - 9s/epoch - 101us/sample
Epoch 81/90
84077/84077 - 8s - loss: 2.1138e-04 - val_loss: 2.0043e-04 - 8s/epoch - 100us/sample
Epoch 82/90
84077/84077 - 8s - loss: 2.1179e-04 - val_loss: 2.0145e-04 - 8s/epoch - 100us/sample
Epoch 83/90
84077/84077 - 8s - loss: 2.1151e-04 - val_loss: 1.9949e-04 - 8s/epoch - 99us/sample
Epoch 84/90
84077/84077 - 8s - loss: 2.1135e-04 - val_loss: 2.0024e-04 - 8s/epoch - 100us/sample
Epoch 85/90
84077/84077 - 8s - loss: 2.1110e-04 - val_loss: 2.0054e-04 - 8s/epoch - 100us/sample
Epoch 86/90
84077/84077 - 8s - loss: 2.1065e-04 - val_loss: 1.9900e-04 - 8s/epoch - 100us/sample
Epoch 87/90
84077/84077 - 8s - loss: 2.1076e-04 - val_loss: 1.9878e-04 - 8s/epoch - 100us/sample
Epoch 88/90
84077/84077 - 8s - loss: 2.1035e-04 - val_loss: 1.9866e-04 - 8s/epoch - 100us/sample
Epoch 89/90
84077/84077 - 8s - loss: 2.1050e-04 - val_loss: 1.9950e-04 - 8s/epoch - 100us/sample
Epoch 90/90
84077/84077 - 8s - loss: 2.1022e-04 - val_loss: 1.9870e-04 - 8s/epoch - 100us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00019869975183178353
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:23:16.211805: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_46/outputlayer/BiasAdd' id:59442 op device:{requested: '', assigned: ''} def:{{{node decoder_model_46/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_46/outputlayer/MatMul, decoder_model_46/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.05851573005636955
cosine 0.057794477508784046
MAE: 0.00291885960862764
RMSE: 0.01336282699156837
r2: 0.8605625855822946
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 90, 0.001, 0.1, 94, 0.00021021777912509732, 0.00019869975183178353, 0.05851573005636955, 0.057794477508784046, 0.00291885960862764, 0.01336282699156837, 0.8605625855822946, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.001 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_141 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_141 (ReLU)               (None, 2074)         0           ['batch_normalization_141[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           195050      ['re_lu_141[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           195050      ['re_lu_141[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2171357     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,527,609
Trainable params: 4,519,125
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 04:23:36.676337: W tensorflow/c/c_api.cc:291] Operation '{name:'training_94/Adam/batch_normalization_141/gamma/v/Assign' id:61340 op device:{requested: '', assigned: ''} def:{{{node training_94/Adam/batch_normalization_141/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_94/Adam/batch_normalization_141/gamma/v, training_94/Adam/batch_normalization_141/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:23:49.420296: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_47/mul' id:60756 op device:{requested: '', assigned: ''} def:{{{node loss_47/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_47/mul/x, loss_47/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0095 - val_loss: 0.0031 - 21s/epoch - 244us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 101us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0030 - val_loss: 0.0014 - 8s/epoch - 100us/sample
Epoch 4/90
84077/84077 - 8s - loss: 0.0012 - val_loss: 0.0010 - 8s/epoch - 100us/sample
Epoch 5/90
84077/84077 - 8s - loss: 0.0010 - val_loss: 9.1787e-04 - 8s/epoch - 100us/sample
Epoch 6/90
84077/84077 - 8s - loss: 0.0018 - val_loss: 0.0012 - 8s/epoch - 100us/sample
Epoch 7/90
84077/84077 - 8s - loss: 0.3595 - val_loss: 0.0015 - 8s/epoch - 100us/sample
Epoch 8/90
84077/84077 - 8s - loss: 0.0010 - val_loss: 7.9193e-04 - 8s/epoch - 100us/sample
Epoch 9/90
84077/84077 - 8s - loss: 7.6277e-04 - val_loss: 6.8014e-04 - 8s/epoch - 100us/sample
Epoch 10/90
84077/84077 - 8s - loss: 7.2242e-04 - val_loss: 6.2831e-04 - 8s/epoch - 100us/sample
Epoch 11/90
84077/84077 - 8s - loss: 0.0290 - val_loss: 5.8593e-04 - 8s/epoch - 101us/sample
Epoch 12/90
84077/84077 - 8s - loss: 6.2046e-04 - val_loss: 5.8742e-04 - 8s/epoch - 100us/sample
Epoch 13/90
84077/84077 - 8s - loss: 6.0876e-04 - val_loss: 5.3212e-04 - 8s/epoch - 100us/sample
Epoch 14/90
84077/84077 - 8s - loss: 5.7324e-04 - val_loss: 5.2110e-04 - 8s/epoch - 100us/sample
Epoch 15/90
84077/84077 - 8s - loss: 5.4353e-04 - val_loss: 4.9489e-04 - 8s/epoch - 100us/sample
Epoch 16/90
84077/84077 - 8s - loss: 5.1932e-04 - val_loss: 4.6620e-04 - 8s/epoch - 100us/sample
Epoch 17/90
84077/84077 - 8s - loss: 5.0691e-04 - val_loss: 4.5963e-04 - 8s/epoch - 100us/sample
Epoch 18/90
84077/84077 - 8s - loss: 4.8730e-04 - val_loss: 4.3786e-04 - 8s/epoch - 100us/sample
Epoch 19/90
84077/84077 - 9s - loss: 4.7141e-04 - val_loss: 4.2617e-04 - 9s/epoch - 101us/sample
Epoch 20/90
84077/84077 - 8s - loss: 4.5719e-04 - val_loss: 4.1267e-04 - 8s/epoch - 101us/sample
Epoch 21/90
84077/84077 - 8s - loss: 4.4947e-04 - val_loss: 4.0958e-04 - 8s/epoch - 100us/sample
Epoch 22/90
84077/84077 - 8s - loss: 4.3686e-04 - val_loss: 3.9743e-04 - 8s/epoch - 100us/sample
Epoch 23/90
84077/84077 - 8s - loss: 4.2832e-04 - val_loss: 3.9488e-04 - 8s/epoch - 100us/sample
Epoch 24/90
84077/84077 - 8s - loss: 4.2045e-04 - val_loss: 3.9413e-04 - 8s/epoch - 100us/sample
Epoch 25/90
84077/84077 - 8s - loss: 4.1232e-04 - val_loss: 3.8083e-04 - 8s/epoch - 100us/sample
Epoch 26/90
84077/84077 - 8s - loss: 4.0364e-04 - val_loss: 3.7484e-04 - 8s/epoch - 100us/sample
Epoch 27/90
84077/84077 - 9s - loss: 3.9695e-04 - val_loss: 3.7232e-04 - 9s/epoch - 101us/sample
Epoch 28/90
84077/84077 - 8s - loss: 3.9013e-04 - val_loss: 3.6664e-04 - 8s/epoch - 100us/sample
Epoch 29/90
84077/84077 - 8s - loss: 3.8364e-04 - val_loss: 3.6157e-04 - 8s/epoch - 100us/sample
Epoch 30/90
84077/84077 - 8s - loss: 3.7797e-04 - val_loss: 3.5345e-04 - 8s/epoch - 100us/sample
Epoch 31/90
84077/84077 - 8s - loss: 3.7292e-04 - val_loss: 3.4887e-04 - 8s/epoch - 100us/sample
Epoch 32/90
84077/84077 - 8s - loss: 3.6759e-04 - val_loss: 3.4402e-04 - 8s/epoch - 100us/sample
Epoch 33/90
84077/84077 - 8s - loss: 3.6450e-04 - val_loss: 3.4176e-04 - 8s/epoch - 100us/sample
Epoch 34/90
84077/84077 - 8s - loss: 3.6112e-04 - val_loss: 3.3595e-04 - 8s/epoch - 100us/sample
Epoch 35/90
84077/84077 - 9s - loss: 3.5715e-04 - val_loss: 3.3929e-04 - 9s/epoch - 101us/sample
Epoch 36/90
84077/84077 - 8s - loss: 3.5469e-04 - val_loss: 3.3351e-04 - 8s/epoch - 100us/sample
Epoch 37/90
84077/84077 - 8s - loss: 3.5146e-04 - val_loss: 3.3013e-04 - 8s/epoch - 100us/sample
Epoch 38/90
84077/84077 - 8s - loss: 3.4885e-04 - val_loss: 3.2560e-04 - 8s/epoch - 100us/sample
Epoch 39/90
84077/84077 - 8s - loss: 3.4550e-04 - val_loss: 3.3112e-04 - 8s/epoch - 100us/sample
Epoch 40/90
84077/84077 - 8s - loss: 3.4441e-04 - val_loss: 3.2418e-04 - 8s/epoch - 100us/sample
Epoch 41/90
84077/84077 - 8s - loss: 3.4170e-04 - val_loss: 3.2293e-04 - 8s/epoch - 100us/sample
Epoch 42/90
84077/84077 - 8s - loss: 3.3933e-04 - val_loss: 3.2030e-04 - 8s/epoch - 100us/sample
Epoch 43/90
84077/84077 - 8s - loss: 3.3712e-04 - val_loss: 3.1957e-04 - 8s/epoch - 101us/sample
Epoch 44/90
84077/84077 - 8s - loss: 3.3603e-04 - val_loss: 3.1703e-04 - 8s/epoch - 100us/sample
Epoch 45/90
84077/84077 - 8s - loss: 3.3519e-04 - val_loss: 3.1303e-04 - 8s/epoch - 100us/sample
Epoch 46/90
84077/84077 - 8s - loss: 3.3254e-04 - val_loss: 3.1577e-04 - 8s/epoch - 100us/sample
Epoch 47/90
84077/84077 - 8s - loss: 3.3215e-04 - val_loss: 3.1519e-04 - 8s/epoch - 100us/sample
Epoch 48/90
84077/84077 - 8s - loss: 3.3049e-04 - val_loss: 3.1919e-04 - 8s/epoch - 100us/sample
Epoch 49/90
84077/84077 - 8s - loss: 3.2915e-04 - val_loss: 3.1478e-04 - 8s/epoch - 100us/sample
Epoch 50/90
84077/84077 - 8s - loss: 3.2840e-04 - val_loss: 3.0957e-04 - 8s/epoch - 100us/sample
Epoch 51/90
84077/84077 - 8s - loss: 3.2706e-04 - val_loss: 3.0912e-04 - 8s/epoch - 101us/sample
Epoch 52/90
84077/84077 - 8s - loss: 3.2710e-04 - val_loss: 3.1029e-04 - 8s/epoch - 101us/sample
Epoch 53/90
84077/84077 - 8s - loss: 3.2570e-04 - val_loss: 3.0977e-04 - 8s/epoch - 100us/sample
Epoch 54/90
84077/84077 - 8s - loss: 3.2480e-04 - val_loss: 3.0767e-04 - 8s/epoch - 100us/sample
Epoch 55/90
84077/84077 - 8s - loss: 3.2427e-04 - val_loss: 3.0900e-04 - 8s/epoch - 100us/sample
Epoch 56/90
84077/84077 - 8s - loss: 3.2314e-04 - val_loss: 3.0640e-04 - 8s/epoch - 100us/sample
Epoch 57/90
84077/84077 - 8s - loss: 3.2182e-04 - val_loss: 3.0534e-04 - 8s/epoch - 100us/sample
Epoch 58/90
84077/84077 - 8s - loss: 3.2137e-04 - val_loss: 3.0645e-04 - 8s/epoch - 100us/sample
Epoch 59/90
84077/84077 - 8s - loss: 3.2094e-04 - val_loss: 3.0413e-04 - 8s/epoch - 101us/sample
Epoch 60/90
84077/84077 - 8s - loss: 3.1920e-04 - val_loss: 3.0600e-04 - 8s/epoch - 100us/sample
Epoch 61/90
84077/84077 - 8s - loss: 3.1900e-04 - val_loss: 3.0381e-04 - 8s/epoch - 100us/sample
Epoch 62/90
84077/84077 - 8s - loss: 3.1863e-04 - val_loss: 3.0200e-04 - 8s/epoch - 100us/sample
Epoch 63/90
84077/84077 - 8s - loss: 3.1753e-04 - val_loss: 3.0412e-04 - 8s/epoch - 100us/sample
Epoch 64/90
84077/84077 - 8s - loss: 3.1677e-04 - val_loss: 3.0220e-04 - 8s/epoch - 100us/sample
Epoch 65/90
84077/84077 - 8s - loss: 3.1566e-04 - val_loss: 3.0086e-04 - 8s/epoch - 100us/sample
Epoch 66/90
84077/84077 - 8s - loss: 3.1541e-04 - val_loss: 2.9971e-04 - 8s/epoch - 100us/sample
Epoch 67/90
84077/84077 - 8s - loss: 3.1547e-04 - val_loss: 3.0196e-04 - 8s/epoch - 101us/sample
Epoch 68/90
84077/84077 - 8s - loss: 3.1478e-04 - val_loss: 3.0029e-04 - 8s/epoch - 100us/sample
Epoch 69/90
84077/84077 - 8s - loss: 3.1396e-04 - val_loss: 2.9807e-04 - 8s/epoch - 100us/sample
Epoch 70/90
84077/84077 - 8s - loss: 3.1360e-04 - val_loss: 3.0097e-04 - 8s/epoch - 100us/sample
Epoch 71/90
84077/84077 - 8s - loss: 3.1303e-04 - val_loss: 2.9897e-04 - 8s/epoch - 100us/sample
Epoch 72/90
84077/84077 - 8s - loss: 3.1288e-04 - val_loss: 2.9753e-04 - 8s/epoch - 100us/sample
Epoch 73/90
84077/84077 - 8s - loss: 3.1270e-04 - val_loss: 2.9904e-04 - 8s/epoch - 100us/sample
Epoch 74/90
84077/84077 - 8s - loss: 3.1173e-04 - val_loss: 2.9691e-04 - 8s/epoch - 100us/sample
Epoch 75/90
84077/84077 - 9s - loss: 3.1190e-04 - val_loss: 2.9721e-04 - 9s/epoch - 101us/sample
Epoch 76/90
84077/84077 - 8s - loss: 3.1149e-04 - val_loss: 2.9877e-04 - 8s/epoch - 101us/sample
Epoch 77/90
84077/84077 - 8s - loss: 3.1061e-04 - val_loss: 2.9801e-04 - 8s/epoch - 100us/sample
Epoch 78/90
84077/84077 - 8s - loss: 3.1030e-04 - val_loss: 2.9436e-04 - 8s/epoch - 100us/sample
Epoch 79/90
84077/84077 - 8s - loss: 3.1022e-04 - val_loss: 2.9703e-04 - 8s/epoch - 100us/sample
Epoch 80/90
84077/84077 - 8s - loss: 3.0987e-04 - val_loss: 2.9690e-04 - 8s/epoch - 100us/sample
Epoch 81/90
84077/84077 - 8s - loss: 3.0905e-04 - val_loss: 2.9370e-04 - 8s/epoch - 100us/sample
Epoch 82/90
84077/84077 - 8s - loss: 3.0936e-04 - val_loss: 2.9634e-04 - 8s/epoch - 100us/sample
Epoch 83/90
84077/84077 - 8s - loss: 3.0794e-04 - val_loss: 2.9411e-04 - 8s/epoch - 101us/sample
Epoch 84/90
84077/84077 - 8s - loss: 3.0778e-04 - val_loss: 2.9316e-04 - 8s/epoch - 100us/sample
Epoch 85/90
84077/84077 - 8s - loss: 3.0776e-04 - val_loss: 2.9641e-04 - 8s/epoch - 100us/sample
Epoch 86/90
84077/84077 - 8s - loss: 3.0729e-04 - val_loss: 2.9204e-04 - 8s/epoch - 100us/sample
Epoch 87/90
84077/84077 - 8s - loss: 3.0615e-04 - val_loss: 2.9287e-04 - 8s/epoch - 100us/sample
Epoch 88/90
84077/84077 - 8s - loss: 3.0684e-04 - val_loss: 2.9398e-04 - 8s/epoch - 100us/sample
Epoch 89/90
84077/84077 - 8s - loss: 3.0681e-04 - val_loss: 2.9429e-04 - 8s/epoch - 100us/sample
Epoch 90/90
84077/84077 - 8s - loss: 3.0573e-04 - val_loss: 2.9336e-04 - 8s/epoch - 100us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002933617453244372
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:36:23.561235: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_47/outputlayer/BiasAdd' id:60727 op device:{requested: '', assigned: ''} def:{{{node decoder_model_47/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_47/outputlayer/MatMul, decoder_model_47/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.038029963282445005
cosine 0.0375673489332829
MAE: 0.002367962263893908
RMSE: 0.01175609877130668
r2: 0.892105434841168
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 90, 0.001, 0.1, 94, 0.00030572904071856, 0.0002933617453244372, 0.038029963282445005, 0.0375673489332829, 0.002367962263893908, 0.01175609877130668, 0.892105434841168, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0008 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_144 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_144 (ReLU)               (None, 1886)         0           ['batch_normalization_144[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_144[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_144[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 04:36:46.929064: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_48/kernel/Assign' id:61727 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_48/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_48/kernel, dense_dec1_48/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:37:00.064900: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_48/mul' id:62014 op device:{requested: '', assigned: ''} def:{{{node loss_48/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_48/mul/x, loss_48/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0100 - val_loss: 0.0018 - 21s/epoch - 254us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 101us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0018 - val_loss: 0.0013 - 9s/epoch - 101us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0015 - 9s/epoch - 102us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0012 - 9s/epoch - 101us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0010 - val_loss: 8.4453e-04 - 9s/epoch - 102us/sample
Epoch 7/85
84077/84077 - 8s - loss: 8.1465e-04 - val_loss: 7.2654e-04 - 8s/epoch - 101us/sample
Epoch 8/85
84077/84077 - 9s - loss: 7.0510e-04 - val_loss: 6.0133e-04 - 9s/epoch - 101us/sample
Epoch 9/85
84077/84077 - 8s - loss: 6.3281e-04 - val_loss: 5.3307e-04 - 8s/epoch - 101us/sample
Epoch 10/85
84077/84077 - 9s - loss: 5.8922e-04 - val_loss: 4.9941e-04 - 9s/epoch - 102us/sample
Epoch 11/85
84077/84077 - 9s - loss: 5.4719e-04 - val_loss: 4.7324e-04 - 9s/epoch - 101us/sample
Epoch 12/85
84077/84077 - 9s - loss: 5.1613e-04 - val_loss: 4.5082e-04 - 9s/epoch - 101us/sample
Epoch 13/85
84077/84077 - 9s - loss: 4.8702e-04 - val_loss: 4.2040e-04 - 9s/epoch - 102us/sample
Epoch 14/85
84077/84077 - 9s - loss: 4.6186e-04 - val_loss: 4.0418e-04 - 9s/epoch - 102us/sample
Epoch 15/85
84077/84077 - 9s - loss: 4.3991e-04 - val_loss: 3.8756e-04 - 9s/epoch - 102us/sample
Epoch 16/85
84077/84077 - 9s - loss: 4.2304e-04 - val_loss: 3.7006e-04 - 9s/epoch - 101us/sample
Epoch 17/85
84077/84077 - 9s - loss: 4.1245e-04 - val_loss: 3.5775e-04 - 9s/epoch - 101us/sample
Epoch 18/85
84077/84077 - 9s - loss: 4.0042e-04 - val_loss: 3.5402e-04 - 9s/epoch - 101us/sample
Epoch 19/85
84077/84077 - 9s - loss: 3.9122e-04 - val_loss: 3.4663e-04 - 9s/epoch - 101us/sample
Epoch 20/85
84077/84077 - 9s - loss: 3.8166e-04 - val_loss: 3.3610e-04 - 9s/epoch - 101us/sample
Epoch 21/85
84077/84077 - 9s - loss: 3.7293e-04 - val_loss: 3.3495e-04 - 9s/epoch - 102us/sample
Epoch 22/85
84077/84077 - 9s - loss: 3.6676e-04 - val_loss: 3.2798e-04 - 9s/epoch - 102us/sample
Epoch 23/85
84077/84077 - 9s - loss: 3.6234e-04 - val_loss: 3.2786e-04 - 9s/epoch - 101us/sample
Epoch 24/85
84077/84077 - 9s - loss: 3.5567e-04 - val_loss: 3.1790e-04 - 9s/epoch - 101us/sample
Epoch 25/85
84077/84077 - 9s - loss: 3.5175e-04 - val_loss: 3.1787e-04 - 9s/epoch - 101us/sample
Epoch 26/85
84077/84077 - 8s - loss: 3.4660e-04 - val_loss: 3.1944e-04 - 8s/epoch - 101us/sample
Epoch 27/85
84077/84077 - 9s - loss: 3.4355e-04 - val_loss: 3.1092e-04 - 9s/epoch - 101us/sample
Epoch 28/85
84077/84077 - 8s - loss: 3.3883e-04 - val_loss: 3.0909e-04 - 8s/epoch - 101us/sample
Epoch 29/85
84077/84077 - 9s - loss: 3.3764e-04 - val_loss: 3.0419e-04 - 9s/epoch - 103us/sample
Epoch 30/85
84077/84077 - 9s - loss: 3.3238e-04 - val_loss: 3.0150e-04 - 9s/epoch - 102us/sample
Epoch 31/85
84077/84077 - 9s - loss: 3.3312e-04 - val_loss: 2.9724e-04 - 9s/epoch - 102us/sample
Epoch 32/85
84077/84077 - 9s - loss: 3.2696e-04 - val_loss: 2.9683e-04 - 9s/epoch - 102us/sample
Epoch 33/85
84077/84077 - 9s - loss: 3.2603e-04 - val_loss: 2.9825e-04 - 9s/epoch - 102us/sample
Epoch 34/85
84077/84077 - 9s - loss: 3.2637e-04 - val_loss: 2.9798e-04 - 9s/epoch - 102us/sample
Epoch 35/85
84077/84077 - 9s - loss: 3.2064e-04 - val_loss: 2.9294e-04 - 9s/epoch - 102us/sample
Epoch 36/85
84077/84077 - 9s - loss: 3.1955e-04 - val_loss: 2.9293e-04 - 9s/epoch - 101us/sample
Epoch 37/85
84077/84077 - 9s - loss: 3.1624e-04 - val_loss: 2.9145e-04 - 9s/epoch - 103us/sample
Epoch 38/85
84077/84077 - 9s - loss: 3.1582e-04 - val_loss: 2.9209e-04 - 9s/epoch - 102us/sample
Epoch 39/85
84077/84077 - 9s - loss: 3.1559e-04 - val_loss: 2.8963e-04 - 9s/epoch - 102us/sample
Epoch 40/85
84077/84077 - 9s - loss: 3.1432e-04 - val_loss: 2.8893e-04 - 9s/epoch - 101us/sample
Epoch 41/85
84077/84077 - 9s - loss: 3.1238e-04 - val_loss: 2.8703e-04 - 9s/epoch - 102us/sample
Epoch 42/85
84077/84077 - 9s - loss: 3.0966e-04 - val_loss: 2.8701e-04 - 9s/epoch - 101us/sample
Epoch 43/85
84077/84077 - 9s - loss: 3.0880e-04 - val_loss: 2.8322e-04 - 9s/epoch - 101us/sample
Epoch 44/85
84077/84077 - 8s - loss: 3.0851e-04 - val_loss: 2.8283e-04 - 8s/epoch - 101us/sample
Epoch 45/85
84077/84077 - 9s - loss: 3.0668e-04 - val_loss: 2.8564e-04 - 9s/epoch - 103us/sample
Epoch 46/85
84077/84077 - 9s - loss: 3.0508e-04 - val_loss: 2.8008e-04 - 9s/epoch - 101us/sample
Epoch 47/85
84077/84077 - 9s - loss: 3.0475e-04 - val_loss: 2.8198e-04 - 9s/epoch - 102us/sample
Epoch 48/85
84077/84077 - 9s - loss: 3.0369e-04 - val_loss: 2.8251e-04 - 9s/epoch - 102us/sample
Epoch 49/85
84077/84077 - 9s - loss: 3.0211e-04 - val_loss: 2.8048e-04 - 9s/epoch - 101us/sample
Epoch 50/85
84077/84077 - 9s - loss: 3.0068e-04 - val_loss: 2.7580e-04 - 9s/epoch - 101us/sample
Epoch 51/85
84077/84077 - 9s - loss: 3.0676e-04 - val_loss: 2.8090e-04 - 9s/epoch - 101us/sample
Epoch 52/85
84077/84077 - 9s - loss: 3.0051e-04 - val_loss: 2.7784e-04 - 9s/epoch - 101us/sample
Epoch 53/85
84077/84077 - 9s - loss: 2.9764e-04 - val_loss: 2.7648e-04 - 9s/epoch - 102us/sample
Epoch 54/85
84077/84077 - 9s - loss: 2.9731e-04 - val_loss: 2.7712e-04 - 9s/epoch - 102us/sample
Epoch 55/85
84077/84077 - 9s - loss: 2.9642e-04 - val_loss: 2.7456e-04 - 9s/epoch - 101us/sample
Epoch 56/85
84077/84077 - 9s - loss: 2.9717e-04 - val_loss: 2.7396e-04 - 9s/epoch - 101us/sample
Epoch 57/85
84077/84077 - 9s - loss: 2.9539e-04 - val_loss: 2.7677e-04 - 9s/epoch - 101us/sample
Epoch 58/85
84077/84077 - 9s - loss: 2.9557e-04 - val_loss: 2.7603e-04 - 9s/epoch - 101us/sample
Epoch 59/85
84077/84077 - 9s - loss: 2.9452e-04 - val_loss: 2.7354e-04 - 9s/epoch - 101us/sample
Epoch 60/85
84077/84077 - 9s - loss: 2.9342e-04 - val_loss: 2.7381e-04 - 9s/epoch - 102us/sample
Epoch 61/85
84077/84077 - 9s - loss: 2.9394e-04 - val_loss: 2.6949e-04 - 9s/epoch - 102us/sample
Epoch 62/85
84077/84077 - 9s - loss: 2.9228e-04 - val_loss: 2.7368e-04 - 9s/epoch - 102us/sample
Epoch 63/85
84077/84077 - 9s - loss: 2.9098e-04 - val_loss: 2.7012e-04 - 9s/epoch - 101us/sample
Epoch 64/85
84077/84077 - 9s - loss: 2.9029e-04 - val_loss: 2.6975e-04 - 9s/epoch - 101us/sample
Epoch 65/85
84077/84077 - 9s - loss: 2.9008e-04 - val_loss: 2.6914e-04 - 9s/epoch - 102us/sample
Epoch 66/85
84077/84077 - 9s - loss: 2.9012e-04 - val_loss: 2.7051e-04 - 9s/epoch - 101us/sample
Epoch 67/85
84077/84077 - 9s - loss: 2.8932e-04 - val_loss: 2.6896e-04 - 9s/epoch - 102us/sample
Epoch 68/85
84077/84077 - 9s - loss: 2.8936e-04 - val_loss: 2.7039e-04 - 9s/epoch - 103us/sample
Epoch 69/85
84077/84077 - 9s - loss: 2.9093e-04 - val_loss: 2.6868e-04 - 9s/epoch - 102us/sample
Epoch 70/85
84077/84077 - 9s - loss: 2.8845e-04 - val_loss: 2.6844e-04 - 9s/epoch - 101us/sample
Epoch 71/85
84077/84077 - 9s - loss: 2.8994e-04 - val_loss: 2.6808e-04 - 9s/epoch - 102us/sample
Epoch 72/85
84077/84077 - 9s - loss: 2.8691e-04 - val_loss: 2.6794e-04 - 9s/epoch - 101us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.8622e-04 - val_loss: 2.6832e-04 - 9s/epoch - 102us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.8579e-04 - val_loss: 2.6930e-04 - 9s/epoch - 101us/sample
Epoch 75/85
84077/84077 - 9s - loss: 2.8661e-04 - val_loss: 2.6551e-04 - 9s/epoch - 101us/sample
Epoch 76/85
84077/84077 - 9s - loss: 2.8456e-04 - val_loss: 2.6626e-04 - 9s/epoch - 103us/sample
Epoch 77/85
84077/84077 - 9s - loss: 2.8411e-04 - val_loss: 2.6680e-04 - 9s/epoch - 102us/sample
Epoch 78/85
84077/84077 - 9s - loss: 2.8443e-04 - val_loss: 2.6579e-04 - 9s/epoch - 101us/sample
Epoch 79/85
84077/84077 - 9s - loss: 2.8457e-04 - val_loss: 2.6695e-04 - 9s/epoch - 102us/sample
Epoch 80/85
84077/84077 - 9s - loss: 2.9152e-04 - val_loss: 2.6740e-04 - 9s/epoch - 101us/sample
Epoch 81/85
84077/84077 - 9s - loss: 2.8367e-04 - val_loss: 2.6340e-04 - 9s/epoch - 101us/sample
Epoch 82/85
84077/84077 - 9s - loss: 2.8222e-04 - val_loss: 2.6628e-04 - 9s/epoch - 101us/sample
Epoch 83/85
84077/84077 - 9s - loss: 2.8426e-04 - val_loss: 2.6411e-04 - 9s/epoch - 101us/sample
Epoch 84/85
84077/84077 - 9s - loss: 2.8282e-04 - val_loss: 2.6659e-04 - 9s/epoch - 103us/sample
Epoch 85/85
84077/84077 - 9s - loss: 2.8181e-04 - val_loss: 2.6224e-04 - 9s/epoch - 101us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002622431633306906
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:49:02.127097: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_48/outputlayer/BiasAdd' id:61985 op device:{requested: '', assigned: ''} def:{{{node decoder_model_48/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_48/outputlayer/MatMul, decoder_model_48/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03225576450088217
cosine 0.031871714858938784
MAE: 0.0024889095362811203
RMSE: 0.01030696436681505
r2: 0.9174675691375702
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 85, 0.0008, 0.1, 94, 0.0002818114688090634, 0.0002622431633306906, 0.03225576450088217, 0.031871714858938784, 0.0024889095362811203, 0.01030696436681505, 0.9174675691375702, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0008 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_147 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_147 (ReLU)               (None, 1886)         0           ['batch_normalization_147[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_147[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_147[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
File geneticVAE_OFMmp_gap_custom_VAE2.0_cr0.1_bs64_ep85_loss_mse_lr0.0008_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-15 04:49:24.628940: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_50/kernel/Assign' id:63284 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_50/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_50/kernel, dense_dec1_50/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-15 04:49:33.860630: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_50/bias/v/Assign' id:64039 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_50/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_50/bias/v, dense_dec1_50/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:49:42.798885: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_50/outputlayer/BiasAdd' id:63687 op device:{requested: '', assigned: ''} def:{{{node decoder_model_50/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_50/outputlayer/MatMul, decoder_model_50/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03160176541834715
cosine 0.03122233278743226
MAE: 0.002487649543820124
RMSE: 0.010258344166001666
r2: 0.9182437252169872
RMSE zero-vector: 0.04004287452915337
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.0custom_VAE', 'mse', 64, 85, 0.0008, 0.1, 94, '--', '--', 0.03160176541834715, 0.03122233278743226, 0.002487649543820124, 0.010258344166001666, 0.9182437252169872, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 6
Fitness    = 424.308669689168
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 424.308669689168.
Best solutions :  [array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168]
[2.1 85 0.0008 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_150 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_150 (ReLU)               (None, 1980)         0           ['batch_normalization_150[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           186214      ['re_lu_150[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           186214      ['re_lu_150[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2073409     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,322,877
Trainable params: 4,314,769
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 04:50:04.211657: W tensorflow/c/c_api.cc:291] Operation '{name:'training_98/Adam/dense_dec1_51/bias/v/Assign' id:65315 op device:{requested: '', assigned: ''} def:{{{node training_98/Adam/dense_dec1_51/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_98/Adam/dense_dec1_51/bias/v, training_98/Adam/dense_dec1_51/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:50:17.476750: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_51/mul' id:64691 op device:{requested: '', assigned: ''} def:{{{node loss_51/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_51/mul/x, loss_51/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0080 - val_loss: 0.0020 - 22s/epoch - 257us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0018 - 9s/epoch - 102us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.2439 - val_loss: 0.0047 - 9s/epoch - 102us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0021 - val_loss: 0.0018 - 9s/epoch - 103us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0023 - val_loss: 0.0029 - 9s/epoch - 102us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0012 - 9s/epoch - 102us/sample
Epoch 7/85
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 102us/sample
Epoch 8/85
84077/84077 - 9s - loss: 0.0010 - val_loss: 8.8711e-04 - 9s/epoch - 102us/sample
Epoch 9/85
84077/84077 - 9s - loss: 9.9335e-04 - val_loss: 8.0858e-04 - 9s/epoch - 102us/sample
Epoch 10/85
84077/84077 - 9s - loss: 9.2798e-04 - val_loss: 8.5348e-04 - 9s/epoch - 102us/sample
Epoch 11/85
84077/84077 - 9s - loss: 8.4656e-04 - val_loss: 6.6139e-04 - 9s/epoch - 102us/sample
Epoch 12/85
84077/84077 - 9s - loss: 6.8460e-04 - val_loss: 6.2843e-04 - 9s/epoch - 102us/sample
Epoch 13/85
84077/84077 - 9s - loss: 8.8435e-04 - val_loss: 6.2212e-04 - 9s/epoch - 102us/sample
Epoch 14/85
84077/84077 - 9s - loss: 8.4844e-04 - val_loss: 5.8819e-04 - 9s/epoch - 103us/sample
Epoch 15/85
84077/84077 - 9s - loss: 5.8611e-04 - val_loss: 5.4959e-04 - 9s/epoch - 102us/sample
Epoch 16/85
84077/84077 - 9s - loss: 5.7074e-04 - val_loss: 4.9895e-04 - 9s/epoch - 102us/sample
Epoch 17/85
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0024 - 9s/epoch - 102us/sample
Epoch 18/85
84077/84077 - 9s - loss: 7.6759e-04 - val_loss: 5.1651e-04 - 9s/epoch - 102us/sample
Epoch 19/85
84077/84077 - 9s - loss: 5.5034e-04 - val_loss: 4.8756e-04 - 9s/epoch - 102us/sample
Epoch 20/85
84077/84077 - 9s - loss: 5.2533e-04 - val_loss: 4.6914e-04 - 9s/epoch - 102us/sample
Epoch 21/85
84077/84077 - 9s - loss: 5.1642e-04 - val_loss: 4.6305e-04 - 9s/epoch - 102us/sample
Epoch 22/85
84077/84077 - 9s - loss: 5.0075e-04 - val_loss: 4.5526e-04 - 9s/epoch - 103us/sample
Epoch 23/85
84077/84077 - 9s - loss: 4.8565e-04 - val_loss: 4.3596e-04 - 9s/epoch - 102us/sample
Epoch 24/85
84077/84077 - 9s - loss: 4.7095e-04 - val_loss: 4.3005e-04 - 9s/epoch - 102us/sample
Epoch 25/85
84077/84077 - 9s - loss: 4.5558e-04 - val_loss: 4.1220e-04 - 9s/epoch - 102us/sample
Epoch 26/85
84077/84077 - 9s - loss: 4.4193e-04 - val_loss: 3.9348e-04 - 9s/epoch - 102us/sample
Epoch 27/85
84077/84077 - 9s - loss: 4.2531e-04 - val_loss: 3.8671e-04 - 9s/epoch - 102us/sample
Epoch 28/85
84077/84077 - 9s - loss: 4.1492e-04 - val_loss: 3.7263e-04 - 9s/epoch - 102us/sample
Epoch 29/85
84077/84077 - 9s - loss: 4.0351e-04 - val_loss: 3.6490e-04 - 9s/epoch - 102us/sample
Epoch 30/85
84077/84077 - 9s - loss: 3.9523e-04 - val_loss: 3.5647e-04 - 9s/epoch - 103us/sample
Epoch 31/85
84077/84077 - 9s - loss: 3.8528e-04 - val_loss: 3.4830e-04 - 9s/epoch - 102us/sample
Epoch 32/85
84077/84077 - 9s - loss: 3.7888e-04 - val_loss: 3.4585e-04 - 9s/epoch - 102us/sample
Epoch 33/85
84077/84077 - 9s - loss: 3.7349e-04 - val_loss: 3.3874e-04 - 9s/epoch - 102us/sample
Epoch 34/85
84077/84077 - 9s - loss: 3.6904e-04 - val_loss: 3.3354e-04 - 9s/epoch - 102us/sample
Epoch 35/85
84077/84077 - 9s - loss: 3.6309e-04 - val_loss: 3.3325e-04 - 9s/epoch - 102us/sample
Epoch 36/85
84077/84077 - 9s - loss: 3.6049e-04 - val_loss: 3.2830e-04 - 9s/epoch - 102us/sample
Epoch 37/85
84077/84077 - 9s - loss: 3.5474e-04 - val_loss: 3.2466e-04 - 9s/epoch - 103us/sample
Epoch 38/85
84077/84077 - 9s - loss: 3.5229e-04 - val_loss: 3.2216e-04 - 9s/epoch - 102us/sample
Epoch 39/85
84077/84077 - 9s - loss: 3.4874e-04 - val_loss: 3.1719e-04 - 9s/epoch - 102us/sample
Epoch 40/85
84077/84077 - 9s - loss: 3.4579e-04 - val_loss: 3.1563e-04 - 9s/epoch - 102us/sample
Epoch 41/85
84077/84077 - 9s - loss: 3.4251e-04 - val_loss: 3.1270e-04 - 9s/epoch - 102us/sample
Epoch 42/85
84077/84077 - 9s - loss: 3.3968e-04 - val_loss: 3.1005e-04 - 9s/epoch - 102us/sample
Epoch 43/85
84077/84077 - 9s - loss: 3.3793e-04 - val_loss: 3.0883e-04 - 9s/epoch - 102us/sample
Epoch 44/85
84077/84077 - 9s - loss: 3.3492e-04 - val_loss: 3.1097e-04 - 9s/epoch - 102us/sample
Epoch 45/85
84077/84077 - 9s - loss: 3.3353e-04 - val_loss: 3.0470e-04 - 9s/epoch - 103us/sample
Epoch 46/85
84077/84077 - 9s - loss: 3.3158e-04 - val_loss: 3.0320e-04 - 9s/epoch - 103us/sample
Epoch 47/85
84077/84077 - 9s - loss: 3.2947e-04 - val_loss: 3.0386e-04 - 9s/epoch - 102us/sample
Epoch 48/85
84077/84077 - 9s - loss: 3.2787e-04 - val_loss: 3.0286e-04 - 9s/epoch - 102us/sample
Epoch 49/85
84077/84077 - 9s - loss: 3.2648e-04 - val_loss: 2.9840e-04 - 9s/epoch - 102us/sample
Epoch 50/85
84077/84077 - 9s - loss: 3.2505e-04 - val_loss: 3.0042e-04 - 9s/epoch - 102us/sample
Epoch 51/85
84077/84077 - 9s - loss: 3.2458e-04 - val_loss: 2.9697e-04 - 9s/epoch - 102us/sample
Epoch 52/85
84077/84077 - 9s - loss: 3.2306e-04 - val_loss: 2.9504e-04 - 9s/epoch - 102us/sample
Epoch 53/85
84077/84077 - 9s - loss: 3.2168e-04 - val_loss: 2.9757e-04 - 9s/epoch - 103us/sample
Epoch 54/85
84077/84077 - 9s - loss: 3.1957e-04 - val_loss: 2.9568e-04 - 9s/epoch - 102us/sample
Epoch 55/85
84077/84077 - 9s - loss: 3.2078e-04 - val_loss: 2.9559e-04 - 9s/epoch - 102us/sample
Epoch 56/85
84077/84077 - 9s - loss: 3.1945e-04 - val_loss: 2.9307e-04 - 9s/epoch - 102us/sample
Epoch 57/85
84077/84077 - 9s - loss: 3.1744e-04 - val_loss: 2.9450e-04 - 9s/epoch - 102us/sample
Epoch 58/85
84077/84077 - 9s - loss: 3.1792e-04 - val_loss: 2.9260e-04 - 9s/epoch - 102us/sample
Epoch 59/85
84077/84077 - 9s - loss: 3.1578e-04 - val_loss: 2.9264e-04 - 9s/epoch - 102us/sample
Epoch 60/85
84077/84077 - 9s - loss: 3.1579e-04 - val_loss: 2.9167e-04 - 9s/epoch - 102us/sample
Epoch 61/85
84077/84077 - 9s - loss: 3.1388e-04 - val_loss: 2.9090e-04 - 9s/epoch - 103us/sample
Epoch 62/85
84077/84077 - 9s - loss: 3.1345e-04 - val_loss: 2.8961e-04 - 9s/epoch - 102us/sample
Epoch 63/85
84077/84077 - 9s - loss: 3.1323e-04 - val_loss: 2.9051e-04 - 9s/epoch - 102us/sample
Epoch 64/85
84077/84077 - 9s - loss: 3.1113e-04 - val_loss: 2.9068e-04 - 9s/epoch - 102us/sample
Epoch 65/85
84077/84077 - 9s - loss: 3.1148e-04 - val_loss: 2.8848e-04 - 9s/epoch - 102us/sample
Epoch 66/85
84077/84077 - 9s - loss: 3.1119e-04 - val_loss: 2.8763e-04 - 9s/epoch - 102us/sample
Epoch 67/85
84077/84077 - 9s - loss: 3.0987e-04 - val_loss: 2.8744e-04 - 9s/epoch - 102us/sample
Epoch 68/85
84077/84077 - 9s - loss: 3.0948e-04 - val_loss: 2.8609e-04 - 9s/epoch - 102us/sample
Epoch 69/85
84077/84077 - 9s - loss: 3.0926e-04 - val_loss: 2.8583e-04 - 9s/epoch - 103us/sample
Epoch 70/85
84077/84077 - 9s - loss: 3.0845e-04 - val_loss: 2.8632e-04 - 9s/epoch - 102us/sample
Epoch 71/85
84077/84077 - 9s - loss: 3.0725e-04 - val_loss: 2.8780e-04 - 9s/epoch - 102us/sample
Epoch 72/85
84077/84077 - 9s - loss: 3.0817e-04 - val_loss: 2.8560e-04 - 9s/epoch - 102us/sample
Epoch 73/85
84077/84077 - 9s - loss: 3.0714e-04 - val_loss: 2.8619e-04 - 9s/epoch - 102us/sample
Epoch 74/85
84077/84077 - 9s - loss: 3.0598e-04 - val_loss: 2.8500e-04 - 9s/epoch - 102us/sample
Epoch 75/85
84077/84077 - 9s - loss: 3.0532e-04 - val_loss: 2.8635e-04 - 9s/epoch - 102us/sample
Epoch 76/85
84077/84077 - 9s - loss: 3.0511e-04 - val_loss: 2.8519e-04 - 9s/epoch - 102us/sample
Epoch 77/85
84077/84077 - 9s - loss: 3.0414e-04 - val_loss: 2.8207e-04 - 9s/epoch - 103us/sample
Epoch 78/85
84077/84077 - 9s - loss: 3.0445e-04 - val_loss: 2.8251e-04 - 9s/epoch - 102us/sample
Epoch 79/85
84077/84077 - 9s - loss: 3.0293e-04 - val_loss: 2.8468e-04 - 9s/epoch - 102us/sample
Epoch 80/85
84077/84077 - 9s - loss: 3.0336e-04 - val_loss: 2.8055e-04 - 9s/epoch - 102us/sample
Epoch 81/85
84077/84077 - 9s - loss: 3.0300e-04 - val_loss: 2.8159e-04 - 9s/epoch - 102us/sample
Epoch 82/85
84077/84077 - 9s - loss: 3.0130e-04 - val_loss: 2.8347e-04 - 9s/epoch - 102us/sample
Epoch 83/85
84077/84077 - 9s - loss: 3.0211e-04 - val_loss: 2.8318e-04 - 9s/epoch - 102us/sample
Epoch 84/85
84077/84077 - 9s - loss: 3.0169e-04 - val_loss: 2.8368e-04 - 9s/epoch - 102us/sample
Epoch 85/85
84077/84077 - 9s - loss: 3.0176e-04 - val_loss: 2.8034e-04 - 9s/epoch - 103us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00028033802024398047
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:02:23.080164: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_51/outputlayer/BiasAdd' id:64662 op device:{requested: '', assigned: ''} def:{{{node decoder_model_51/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_51/outputlayer/MatMul, decoder_model_51/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.037014837986692056
cosine 0.03655931418055559
MAE: 0.002732903877451327
RMSE: 0.011131998251835171
r2: 0.9032997577460773
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.0008, 0.1, 94, 0.0003017587155507059, 0.00028033802024398047, 0.037014837986692056, 0.03655931418055559, 0.002732903877451327, 0.011131998251835171, 0.9032997577460773, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0008 16 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_153 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_153 (ReLU)               (None, 2074)         0           ['batch_normalization_153[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           195050      ['re_lu_153[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           195050      ['re_lu_153[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2171357     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,527,609
Trainable params: 4,519,125
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 05:02:45.616324: W tensorflow/c/c_api.cc:291] Operation '{name:'training_100/Adam/batch_normalization_153/gamma/v/Assign' id:66527 op device:{requested: '', assigned: ''} def:{{{node training_100/Adam/batch_normalization_153/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_100/Adam/batch_normalization_153/gamma/v, training_100/Adam/batch_normalization_153/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:03:13.818916: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_52/mul' id:65946 op device:{requested: '', assigned: ''} def:{{{node loss_52/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_52/mul/x, loss_52/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 38s - loss: 0.0705 - val_loss: 0.0014 - 38s/epoch - 450us/sample
Epoch 2/90
84077/84077 - 24s - loss: 0.0013 - val_loss: 0.0010 - 24s/epoch - 285us/sample
Epoch 3/90
84077/84077 - 24s - loss: 0.0010 - val_loss: 9.2827e-04 - 24s/epoch - 285us/sample
Epoch 4/90
84077/84077 - 24s - loss: 9.4326e-04 - val_loss: 8.3943e-04 - 24s/epoch - 286us/sample
Epoch 5/90
84077/84077 - 24s - loss: 8.8940e-04 - val_loss: 8.0030e-04 - 24s/epoch - 288us/sample
Epoch 6/90
84077/84077 - 24s - loss: 8.1250e-04 - val_loss: 7.8801e-04 - 24s/epoch - 285us/sample
Epoch 7/90
84077/84077 - 24s - loss: 7.5717e-04 - val_loss: 9.2648e-04 - 24s/epoch - 286us/sample
Epoch 8/90
84077/84077 - 24s - loss: 7.2444e-04 - val_loss: 0.0010 - 24s/epoch - 287us/sample
Epoch 9/90
84077/84077 - 24s - loss: 7.0166e-04 - val_loss: 0.0016 - 24s/epoch - 287us/sample
Epoch 10/90
84077/84077 - 24s - loss: 6.8607e-04 - val_loss: 0.0012 - 24s/epoch - 285us/sample
Epoch 11/90
84077/84077 - 24s - loss: 6.6997e-04 - val_loss: 0.0013 - 24s/epoch - 286us/sample
Epoch 12/90
84077/84077 - 24s - loss: 6.5921e-04 - val_loss: 0.0013 - 24s/epoch - 287us/sample
Epoch 13/90
84077/84077 - 24s - loss: 6.5105e-04 - val_loss: 0.0013 - 24s/epoch - 286us/sample
Epoch 14/90
84077/84077 - 24s - loss: 6.4498e-04 - val_loss: 0.0012 - 24s/epoch - 285us/sample
Epoch 15/90
84077/84077 - 24s - loss: 6.4063e-04 - val_loss: 0.0014 - 24s/epoch - 288us/sample
Epoch 16/90
84077/84077 - 24s - loss: 6.3591e-04 - val_loss: 0.0016 - 24s/epoch - 286us/sample
Epoch 17/90
84077/84077 - 24s - loss: 6.3127e-04 - val_loss: 0.0016 - 24s/epoch - 286us/sample
Epoch 18/90
84077/84077 - 24s - loss: 6.2775e-04 - val_loss: 0.0015 - 24s/epoch - 288us/sample
Epoch 19/90
84077/84077 - 24s - loss: 6.2276e-04 - val_loss: 0.0016 - 24s/epoch - 286us/sample
Epoch 20/90
84077/84077 - 24s - loss: 6.2050e-04 - val_loss: 0.0015 - 24s/epoch - 285us/sample
Epoch 21/90
84077/84077 - 24s - loss: 6.1688e-04 - val_loss: 0.0015 - 24s/epoch - 288us/sample
Epoch 22/90
84077/84077 - 24s - loss: 6.1408e-04 - val_loss: 0.0015 - 24s/epoch - 287us/sample
Epoch 23/90
84077/84077 - 24s - loss: 6.1075e-04 - val_loss: 0.0013 - 24s/epoch - 286us/sample
Epoch 24/90
84077/84077 - 24s - loss: 6.0804e-04 - val_loss: 0.0014 - 24s/epoch - 287us/sample
Epoch 25/90
84077/84077 - 24s - loss: 6.0619e-04 - val_loss: 0.0011 - 24s/epoch - 286us/sample
Epoch 26/90
84077/84077 - 24s - loss: 6.0383e-04 - val_loss: 0.0013 - 24s/epoch - 285us/sample
Epoch 27/90
84077/84077 - 24s - loss: 6.0092e-04 - val_loss: 0.0011 - 24s/epoch - 287us/sample
Epoch 28/90
84077/84077 - 24s - loss: 5.9862e-04 - val_loss: 0.0011 - 24s/epoch - 286us/sample
Epoch 29/90
84077/84077 - 24s - loss: 5.9728e-04 - val_loss: 0.0011 - 24s/epoch - 286us/sample
Epoch 30/90
84077/84077 - 24s - loss: 5.9493e-04 - val_loss: 0.0011 - 24s/epoch - 287us/sample
Epoch 31/90
84077/84077 - 24s - loss: 5.9328e-04 - val_loss: 0.0011 - 24s/epoch - 287us/sample
Epoch 32/90
84077/84077 - 24s - loss: 5.9180e-04 - val_loss: 0.0012 - 24s/epoch - 286us/sample
Epoch 33/90
84077/84077 - 24s - loss: 5.8824e-04 - val_loss: 0.0012 - 24s/epoch - 286us/sample
Epoch 34/90
84077/84077 - 24s - loss: 5.8709e-04 - val_loss: 0.0011 - 24s/epoch - 288us/sample
Epoch 35/90
84077/84077 - 24s - loss: 5.8570e-04 - val_loss: 0.0011 - 24s/epoch - 286us/sample
Epoch 36/90
84077/84077 - 24s - loss: 5.8298e-04 - val_loss: 0.0012 - 24s/epoch - 286us/sample
Epoch 37/90
84077/84077 - 24s - loss: 5.8265e-04 - val_loss: 0.0012 - 24s/epoch - 287us/sample
Epoch 38/90
84077/84077 - 24s - loss: 5.8033e-04 - val_loss: 0.0011 - 24s/epoch - 286us/sample
Epoch 39/90
84077/84077 - 24s - loss: 5.7979e-04 - val_loss: 0.0010 - 24s/epoch - 285us/sample
Epoch 40/90
84077/84077 - 24s - loss: 5.7815e-04 - val_loss: 0.0011 - 24s/epoch - 287us/sample
Epoch 41/90
84077/84077 - 24s - loss: 5.7782e-04 - val_loss: 0.0010 - 24s/epoch - 286us/sample
Epoch 42/90
84077/84077 - 24s - loss: 5.7725e-04 - val_loss: 0.0011 - 24s/epoch - 286us/sample
Epoch 43/90
84077/84077 - 24s - loss: 5.7711e-04 - val_loss: 0.0012 - 24s/epoch - 286us/sample
Epoch 44/90
84077/84077 - 24s - loss: 5.7462e-04 - val_loss: 9.2014e-04 - 24s/epoch - 287us/sample
Epoch 45/90
84077/84077 - 24s - loss: 5.7384e-04 - val_loss: 0.0013 - 24s/epoch - 286us/sample
Epoch 46/90
84077/84077 - 24s - loss: 5.7255e-04 - val_loss: 0.0010 - 24s/epoch - 285us/sample
Epoch 47/90
84077/84077 - 24s - loss: 5.7279e-04 - val_loss: 9.3259e-04 - 24s/epoch - 288us/sample
Epoch 48/90
84077/84077 - 24s - loss: 5.6926e-04 - val_loss: 9.3202e-04 - 24s/epoch - 286us/sample
Epoch 49/90
84077/84077 - 24s - loss: 5.7015e-04 - val_loss: 8.7607e-04 - 24s/epoch - 285us/sample
Epoch 50/90
84077/84077 - 24s - loss: 5.6912e-04 - val_loss: 9.2046e-04 - 24s/epoch - 287us/sample
Epoch 51/90
84077/84077 - 24s - loss: 5.6761e-04 - val_loss: 9.6330e-04 - 24s/epoch - 285us/sample
Epoch 52/90
84077/84077 - 24s - loss: 5.6738e-04 - val_loss: 9.4783e-04 - 24s/epoch - 285us/sample
Epoch 53/90
84077/84077 - 24s - loss: 5.6662e-04 - val_loss: 0.0010 - 24s/epoch - 287us/sample
Epoch 54/90
84077/84077 - 24s - loss: 5.6626e-04 - val_loss: 9.0445e-04 - 24s/epoch - 286us/sample
Epoch 55/90
84077/84077 - 24s - loss: 5.6501e-04 - val_loss: 8.8248e-04 - 24s/epoch - 286us/sample
Epoch 56/90
84077/84077 - 24s - loss: 5.6326e-04 - val_loss: 0.0011 - 24s/epoch - 288us/sample
Epoch 57/90
84077/84077 - 24s - loss: 5.6437e-04 - val_loss: 9.3618e-04 - 24s/epoch - 285us/sample
Epoch 58/90
84077/84077 - 24s - loss: 5.6309e-04 - val_loss: 0.0013 - 24s/epoch - 285us/sample
Epoch 59/90
84077/84077 - 24s - loss: 5.6052e-04 - val_loss: 9.3385e-04 - 24s/epoch - 288us/sample
Epoch 60/90
84077/84077 - 24s - loss: 5.6123e-04 - val_loss: 0.0012 - 24s/epoch - 285us/sample
Epoch 61/90
84077/84077 - 24s - loss: 5.6108e-04 - val_loss: 0.0013 - 24s/epoch - 286us/sample
Epoch 62/90
84077/84077 - 24s - loss: 5.6001e-04 - val_loss: 0.0011 - 24s/epoch - 288us/sample
Epoch 63/90
84077/84077 - 24s - loss: 5.6013e-04 - val_loss: 8.8146e-04 - 24s/epoch - 286us/sample
Epoch 64/90
84077/84077 - 24s - loss: 5.5893e-04 - val_loss: 0.0011 - 24s/epoch - 284us/sample
Epoch 65/90
84077/84077 - 24s - loss: 5.5877e-04 - val_loss: 9.4003e-04 - 24s/epoch - 286us/sample
Epoch 66/90
84077/84077 - 24s - loss: 5.5753e-04 - val_loss: 9.8093e-04 - 24s/epoch - 287us/sample
Epoch 67/90
84077/84077 - 24s - loss: 5.5668e-04 - val_loss: 9.3426e-04 - 24s/epoch - 285us/sample
Epoch 68/90
84077/84077 - 24s - loss: 5.5627e-04 - val_loss: 9.6629e-04 - 24s/epoch - 285us/sample
Epoch 69/90
84077/84077 - 24s - loss: 5.5599e-04 - val_loss: 8.3651e-04 - 24s/epoch - 287us/sample
Epoch 70/90
84077/84077 - 24s - loss: 5.5593e-04 - val_loss: 9.1390e-04 - 24s/epoch - 286us/sample
Epoch 71/90
84077/84077 - 24s - loss: 5.5472e-04 - val_loss: 9.1657e-04 - 24s/epoch - 286us/sample
Epoch 72/90
84077/84077 - 24s - loss: 5.5483e-04 - val_loss: 8.2699e-04 - 24s/epoch - 288us/sample
Epoch 73/90
84077/84077 - 24s - loss: 5.5381e-04 - val_loss: 8.8206e-04 - 24s/epoch - 285us/sample
Epoch 74/90
84077/84077 - 24s - loss: 5.5407e-04 - val_loss: 8.4207e-04 - 24s/epoch - 285us/sample
Epoch 75/90
84077/84077 - 24s - loss: 5.5329e-04 - val_loss: 7.7587e-04 - 24s/epoch - 287us/sample
Epoch 76/90
84077/84077 - 24s - loss: 5.5283e-04 - val_loss: 8.3840e-04 - 24s/epoch - 286us/sample
Epoch 77/90
84077/84077 - 24s - loss: 5.5264e-04 - val_loss: 7.9905e-04 - 24s/epoch - 285us/sample
Epoch 78/90
84077/84077 - 24s - loss: 5.5205e-04 - val_loss: 7.9586e-04 - 24s/epoch - 286us/sample
Epoch 79/90
84077/84077 - 24s - loss: 5.5255e-04 - val_loss: 7.6991e-04 - 24s/epoch - 287us/sample
Epoch 80/90
84077/84077 - 24s - loss: 5.5123e-04 - val_loss: 7.8404e-04 - 24s/epoch - 285us/sample
Epoch 81/90
84077/84077 - 24s - loss: 5.4988e-04 - val_loss: 9.1980e-04 - 24s/epoch - 285us/sample
Epoch 82/90
84077/84077 - 24s - loss: 5.4924e-04 - val_loss: 7.5790e-04 - 24s/epoch - 287us/sample
Epoch 83/90
84077/84077 - 24s - loss: 5.4867e-04 - val_loss: 7.6067e-04 - 24s/epoch - 286us/sample
Epoch 84/90
84077/84077 - 24s - loss: 5.4796e-04 - val_loss: 7.9517e-04 - 24s/epoch - 285us/sample
Epoch 85/90
84077/84077 - 24s - loss: 5.5017e-04 - val_loss: 7.9676e-04 - 24s/epoch - 286us/sample
Epoch 86/90
84077/84077 - 24s - loss: 5.4699e-04 - val_loss: 7.8762e-04 - 24s/epoch - 286us/sample
Epoch 87/90
84077/84077 - 24s - loss: 5.4798e-04 - val_loss: 7.1278e-04 - 24s/epoch - 286us/sample
Epoch 88/90
84077/84077 - 24s - loss: 5.4707e-04 - val_loss: 8.2054e-04 - 24s/epoch - 286us/sample
Epoch 89/90
84077/84077 - 24s - loss: 5.4600e-04 - val_loss: 7.7999e-04 - 24s/epoch - 286us/sample
Epoch 90/90
84077/84077 - 24s - loss: 5.4658e-04 - val_loss: 7.1247e-04 - 24s/epoch - 286us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0007124671795910089
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:39:00.919148: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_52/outputlayer/BiasAdd' id:65917 op device:{requested: '', assigned: ''} def:{{{node decoder_model_52/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_52/outputlayer/MatMul, decoder_model_52/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.10142523076625146
cosine 0.10020006742934642
MAE: 0.004099509330802435
RMSE: 0.024130402317590314
r2: 0.54515662849649
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 16, 90, 0.0008, 0.1, 94, 0.0005465848490277563, 0.0007124671795910089, 0.10142523076625146, 0.10020006742934642, 0.004099509330802435, 0.024130402317590314, 0.54515662849649, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0008 8 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_156 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_156 (ReLU)               (None, 1791)         0           ['batch_normalization_156[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_156[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_156[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 05:39:24.339793: W tensorflow/c/c_api.cc:291] Operation '{name:'training_102/Adam/batch_normalization_157/beta/m/Assign' id:67751 op device:{requested: '', assigned: ''} def:{{{node training_102/Adam/batch_normalization_157/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_102/Adam/batch_normalization_157/beta/m, training_102/Adam/batch_normalization_157/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:40:11.581476: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_53/mul' id:67208 op device:{requested: '', assigned: ''} def:{{{node loss_53/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_53/mul/x, loss_53/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 58s - loss: 0.0019 - val_loss: 7.3280e-04 - 58s/epoch - 694us/sample
Epoch 2/85
84077/84077 - 44s - loss: 6.3063e-04 - val_loss: 6.1881e-04 - 44s/epoch - 525us/sample
Epoch 3/85
84077/84077 - 44s - loss: 6.1206e-04 - val_loss: 6.1737e-04 - 44s/epoch - 525us/sample
Epoch 4/85
84077/84077 - 44s - loss: 6.1155e-04 - val_loss: 6.1671e-04 - 44s/epoch - 526us/sample
Epoch 5/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1712e-04 - 44s/epoch - 526us/sample
Epoch 6/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1691e-04 - 44s/epoch - 524us/sample
Epoch 7/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1752e-04 - 44s/epoch - 525us/sample
Epoch 8/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1703e-04 - 44s/epoch - 525us/sample
Epoch 9/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1736e-04 - 44s/epoch - 526us/sample
Epoch 10/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1757e-04 - 44s/epoch - 526us/sample
Epoch 11/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1691e-04 - 44s/epoch - 524us/sample
Epoch 12/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1761e-04 - 44s/epoch - 526us/sample
Epoch 13/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1717e-04 - 44s/epoch - 524us/sample
Epoch 14/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1694e-04 - 44s/epoch - 525us/sample
Epoch 15/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1735e-04 - 44s/epoch - 525us/sample
Epoch 16/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1715e-04 - 44s/epoch - 525us/sample
Epoch 17/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1742e-04 - 44s/epoch - 526us/sample
Epoch 18/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1673e-04 - 44s/epoch - 523us/sample
Epoch 19/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1693e-04 - 44s/epoch - 525us/sample
Epoch 20/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1689e-04 - 44s/epoch - 526us/sample
Epoch 21/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1701e-04 - 44s/epoch - 523us/sample
Epoch 22/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1814e-04 - 44s/epoch - 525us/sample
Epoch 23/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1744e-04 - 44s/epoch - 524us/sample
Epoch 24/85
84077/84077 - 44s - loss: 6.1149e-04 - val_loss: 6.1787e-04 - 44s/epoch - 524us/sample
Epoch 25/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1749e-04 - 44s/epoch - 527us/sample
Epoch 26/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1708e-04 - 44s/epoch - 524us/sample
Epoch 27/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1723e-04 - 44s/epoch - 525us/sample
Epoch 28/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1728e-04 - 44s/epoch - 526us/sample
Epoch 29/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1727e-04 - 44s/epoch - 523us/sample
Epoch 30/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1727e-04 - 44s/epoch - 526us/sample
Epoch 31/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1717e-04 - 44s/epoch - 524us/sample
Epoch 32/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1767e-04 - 44s/epoch - 526us/sample
Epoch 33/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1678e-04 - 44s/epoch - 525us/sample
Epoch 34/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1775e-04 - 44s/epoch - 525us/sample
Epoch 35/85
84077/84077 - 44s - loss: 6.1155e-04 - val_loss: 6.1679e-04 - 44s/epoch - 526us/sample
Epoch 36/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1771e-04 - 44s/epoch - 524us/sample
Epoch 37/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1742e-04 - 44s/epoch - 526us/sample
Epoch 38/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1719e-04 - 44s/epoch - 526us/sample
Epoch 39/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1729e-04 - 44s/epoch - 525us/sample
Epoch 40/85
84077/84077 - 44s - loss: 6.1155e-04 - val_loss: 6.1693e-04 - 44s/epoch - 526us/sample
Epoch 41/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1733e-04 - 44s/epoch - 526us/sample
Epoch 42/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1719e-04 - 44s/epoch - 526us/sample
Epoch 43/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1750e-04 - 44s/epoch - 526us/sample
Epoch 44/85
84077/84077 - 44s - loss: 6.1150e-04 - val_loss: 6.1698e-04 - 44s/epoch - 526us/sample
Epoch 45/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1741e-04 - 44s/epoch - 526us/sample
Epoch 46/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1700e-04 - 44s/epoch - 524us/sample
Epoch 47/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1711e-04 - 44s/epoch - 527us/sample
Epoch 48/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1793e-04 - 44s/epoch - 524us/sample
Epoch 49/85
84077/84077 - 44s - loss: 6.1155e-04 - val_loss: 6.1742e-04 - 44s/epoch - 526us/sample
Epoch 50/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1728e-04 - 44s/epoch - 526us/sample
Epoch 51/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1728e-04 - 44s/epoch - 527us/sample
Epoch 52/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1691e-04 - 44s/epoch - 526us/sample
Epoch 53/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1688e-04 - 44s/epoch - 525us/sample
Epoch 54/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1731e-04 - 44s/epoch - 526us/sample
Epoch 55/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1708e-04 - 44s/epoch - 525us/sample
Epoch 56/85
84077/84077 - 44s - loss: 6.1156e-04 - val_loss: 6.1711e-04 - 44s/epoch - 527us/sample
Epoch 57/85
84077/84077 - 44s - loss: 6.1155e-04 - val_loss: 6.1685e-04 - 44s/epoch - 528us/sample
Epoch 58/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1771e-04 - 44s/epoch - 525us/sample
Epoch 59/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1742e-04 - 44s/epoch - 526us/sample
Epoch 60/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1692e-04 - 44s/epoch - 525us/sample
Epoch 61/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1714e-04 - 44s/epoch - 527us/sample
Epoch 62/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1699e-04 - 44s/epoch - 527us/sample
Epoch 63/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1722e-04 - 44s/epoch - 525us/sample
Epoch 64/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1757e-04 - 44s/epoch - 527us/sample
Epoch 65/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1711e-04 - 44s/epoch - 525us/sample
Epoch 66/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1708e-04 - 44s/epoch - 527us/sample
Epoch 67/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1702e-04 - 44s/epoch - 525us/sample
Epoch 68/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1696e-04 - 44s/epoch - 526us/sample
Epoch 69/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1691e-04 - 44s/epoch - 526us/sample
Epoch 70/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1663e-04 - 44s/epoch - 525us/sample
Epoch 71/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1759e-04 - 44s/epoch - 525us/sample
Epoch 72/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1756e-04 - 44s/epoch - 524us/sample
Epoch 73/85
84077/84077 - 44s - loss: 6.1149e-04 - val_loss: 6.1821e-04 - 44s/epoch - 525us/sample
Epoch 74/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1693e-04 - 44s/epoch - 524us/sample
Epoch 75/85
84077/84077 - 44s - loss: 6.1150e-04 - val_loss: 6.1748e-04 - 44s/epoch - 525us/sample
Epoch 76/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1735e-04 - 44s/epoch - 525us/sample
Epoch 77/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1726e-04 - 44s/epoch - 523us/sample
Epoch 78/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1669e-04 - 44s/epoch - 526us/sample
Epoch 79/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1770e-04 - 44s/epoch - 522us/sample
Epoch 80/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1701e-04 - 44s/epoch - 524us/sample
Epoch 81/85
84077/84077 - 44s - loss: 6.1154e-04 - val_loss: 6.1732e-04 - 44s/epoch - 523us/sample
Epoch 82/85
84077/84077 - 44s - loss: 6.1153e-04 - val_loss: 6.1750e-04 - 44s/epoch - 525us/sample
Epoch 83/85
84077/84077 - 44s - loss: 6.1150e-04 - val_loss: 6.1743e-04 - 44s/epoch - 524us/sample
Epoch 84/85
84077/84077 - 44s - loss: 6.1152e-04 - val_loss: 6.1736e-04 - 44s/epoch - 524us/sample
Epoch 85/85
84077/84077 - 44s - loss: 6.1151e-04 - val_loss: 6.1700e-04 - 44s/epoch - 524us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0006170002095105401
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:42:07.399913: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_53/outputlayer/BiasAdd' id:67172 op device:{requested: '', assigned: ''} def:{{{node decoder_model_53/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_53/outputlayer/MatMul, decoder_model_53/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.510060527029195
cosine 0.5005173725679098
MAE: 0.006578314078783003
RMSE: 0.03582777018321707
r2: -0.0027700157997601112
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 8, 85, 0.0008, 0.1, 94, 0.0006115139615067571, 0.0006170002095105401, 0.510060527029195, 0.5005173725679098, 0.006578314078783003, 0.03582777018321707, -0.0027700157997601112, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_159 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_159 (ReLU)               (None, 1980)         0           ['batch_normalization_159[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           186214      ['re_lu_159[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           186214      ['re_lu_159[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2073409     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,322,877
Trainable params: 4,314,769
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 06:42:30.877316: W tensorflow/c/c_api.cc:291] Operation '{name:'training_104/Adam/outputlayer_54/kernel/m/Assign' id:69041 op device:{requested: '', assigned: ''} def:{{{node training_104/Adam/outputlayer_54/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_104/Adam/outputlayer_54/kernel/m, training_104/Adam/outputlayer_54/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:42:44.551855: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_54/mul' id:68486 op device:{requested: '', assigned: ''} def:{{{node loss_54/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_54/mul/x, loss_54/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0290 - val_loss: 0.0016 - 23s/epoch - 269us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0021 - val_loss: 0.0017 - 9s/epoch - 105us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0028 - val_loss: 0.0014 - 9s/epoch - 104us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0025 - 9s/epoch - 103us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 0.0035 - 9s/epoch - 103us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 0.0015 - 9s/epoch - 103us/sample
Epoch 7/85
84077/84077 - 9s - loss: 8.2314e-04 - val_loss: 6.9347e-04 - 9s/epoch - 103us/sample
Epoch 8/85
84077/84077 - 9s - loss: 7.8454e-04 - val_loss: 6.6160e-04 - 9s/epoch - 103us/sample
Epoch 9/85
84077/84077 - 9s - loss: 8.4477e-04 - val_loss: 6.1332e-04 - 9s/epoch - 103us/sample
Epoch 10/85
84077/84077 - 9s - loss: 6.7168e-04 - val_loss: 0.0022 - 9s/epoch - 103us/sample
Epoch 11/85
84077/84077 - 9s - loss: 6.6514e-04 - val_loss: 5.2810e-04 - 9s/epoch - 104us/sample
Epoch 12/85
84077/84077 - 9s - loss: 5.6117e-04 - val_loss: 4.8963e-04 - 9s/epoch - 103us/sample
Epoch 13/85
84077/84077 - 9s - loss: 5.2774e-04 - val_loss: 4.9500e-04 - 9s/epoch - 103us/sample
Epoch 14/85
84077/84077 - 9s - loss: 5.0240e-04 - val_loss: 4.4499e-04 - 9s/epoch - 103us/sample
Epoch 15/85
84077/84077 - 9s - loss: 4.7702e-04 - val_loss: 4.2337e-04 - 9s/epoch - 103us/sample
Epoch 16/85
84077/84077 - 9s - loss: 4.5548e-04 - val_loss: 4.0578e-04 - 9s/epoch - 103us/sample
Epoch 17/85
84077/84077 - 9s - loss: 4.4061e-04 - val_loss: 3.8662e-04 - 9s/epoch - 103us/sample
Epoch 18/85
84077/84077 - 9s - loss: 4.2017e-04 - val_loss: 3.7160e-04 - 9s/epoch - 104us/sample
Epoch 19/85
84077/84077 - 9s - loss: 4.0881e-04 - val_loss: 3.5852e-04 - 9s/epoch - 104us/sample
Epoch 20/85
84077/84077 - 9s - loss: 3.9560e-04 - val_loss: 3.5108e-04 - 9s/epoch - 103us/sample
Epoch 21/85
84077/84077 - 9s - loss: 3.8615e-04 - val_loss: 3.4768e-04 - 9s/epoch - 103us/sample
Epoch 22/85
84077/84077 - 9s - loss: 3.7880e-04 - val_loss: 3.4080e-04 - 9s/epoch - 103us/sample
Epoch 23/85
84077/84077 - 9s - loss: 3.7187e-04 - val_loss: 3.3543e-04 - 9s/epoch - 103us/sample
Epoch 24/85
84077/84077 - 9s - loss: 3.6626e-04 - val_loss: 3.2848e-04 - 9s/epoch - 103us/sample
Epoch 25/85
84077/84077 - 9s - loss: 3.6100e-04 - val_loss: 3.2476e-04 - 9s/epoch - 103us/sample
Epoch 26/85
84077/84077 - 9s - loss: 3.5846e-04 - val_loss: 3.6363e-04 - 9s/epoch - 105us/sample
Epoch 27/85
84077/84077 - 9s - loss: 3.5581e-04 - val_loss: 3.2018e-04 - 9s/epoch - 104us/sample
Epoch 28/85
84077/84077 - 9s - loss: 3.4965e-04 - val_loss: 3.1825e-04 - 9s/epoch - 104us/sample
Epoch 29/85
84077/84077 - 9s - loss: 3.4694e-04 - val_loss: 3.1553e-04 - 9s/epoch - 103us/sample
Epoch 30/85
84077/84077 - 9s - loss: 3.4345e-04 - val_loss: 3.0987e-04 - 9s/epoch - 103us/sample
Epoch 31/85
84077/84077 - 9s - loss: 3.4397e-04 - val_loss: 3.1070e-04 - 9s/epoch - 103us/sample
Epoch 32/85
84077/84077 - 9s - loss: 3.3798e-04 - val_loss: 3.0584e-04 - 9s/epoch - 103us/sample
Epoch 33/85
84077/84077 - 9s - loss: 3.3576e-04 - val_loss: 3.0640e-04 - 9s/epoch - 104us/sample
Epoch 34/85
84077/84077 - 9s - loss: 3.3377e-04 - val_loss: 3.0570e-04 - 9s/epoch - 104us/sample
Epoch 35/85
84077/84077 - 9s - loss: 3.3204e-04 - val_loss: 3.0249e-04 - 9s/epoch - 103us/sample
Epoch 36/85
84077/84077 - 9s - loss: 3.3120e-04 - val_loss: 2.9873e-04 - 9s/epoch - 103us/sample
Epoch 37/85
84077/84077 - 9s - loss: 3.2845e-04 - val_loss: 2.9828e-04 - 9s/epoch - 103us/sample
Epoch 38/85
84077/84077 - 9s - loss: 3.2648e-04 - val_loss: 2.9724e-04 - 9s/epoch - 103us/sample
Epoch 39/85
84077/84077 - 9s - loss: 3.2483e-04 - val_loss: 2.9629e-04 - 9s/epoch - 103us/sample
Epoch 40/85
84077/84077 - 9s - loss: 3.3070e-04 - val_loss: 2.9671e-04 - 9s/epoch - 103us/sample
Epoch 41/85
84077/84077 - 9s - loss: 3.2134e-04 - val_loss: 2.9423e-04 - 9s/epoch - 104us/sample
Epoch 42/85
84077/84077 - 9s - loss: 3.2070e-04 - val_loss: 2.9270e-04 - 9s/epoch - 104us/sample
Epoch 43/85
84077/84077 - 9s - loss: 3.1884e-04 - val_loss: 2.9076e-04 - 9s/epoch - 103us/sample
Epoch 44/85
84077/84077 - 9s - loss: 3.1776e-04 - val_loss: 2.8868e-04 - 9s/epoch - 103us/sample
Epoch 45/85
84077/84077 - 9s - loss: 3.1671e-04 - val_loss: 2.9367e-04 - 9s/epoch - 103us/sample
Epoch 46/85
84077/84077 - 9s - loss: 3.1480e-04 - val_loss: 2.8990e-04 - 9s/epoch - 103us/sample
Epoch 47/85
84077/84077 - 9s - loss: 3.1361e-04 - val_loss: 2.8842e-04 - 9s/epoch - 103us/sample
Epoch 48/85
84077/84077 - 9s - loss: 3.1281e-04 - val_loss: 2.8562e-04 - 9s/epoch - 104us/sample
Epoch 49/85
84077/84077 - 9s - loss: 3.1222e-04 - val_loss: 2.8655e-04 - 9s/epoch - 105us/sample
Epoch 50/85
84077/84077 - 9s - loss: 3.1178e-04 - val_loss: 2.8607e-04 - 9s/epoch - 103us/sample
Epoch 51/85
84077/84077 - 9s - loss: 3.0991e-04 - val_loss: 2.8528e-04 - 9s/epoch - 103us/sample
Epoch 52/85
84077/84077 - 9s - loss: 3.0926e-04 - val_loss: 2.8569e-04 - 9s/epoch - 103us/sample
Epoch 53/85
84077/84077 - 9s - loss: 3.1064e-04 - val_loss: 2.8252e-04 - 9s/epoch - 103us/sample
Epoch 54/85
84077/84077 - 9s - loss: 3.0820e-04 - val_loss: 2.8215e-04 - 9s/epoch - 103us/sample
Epoch 55/85
84077/84077 - 9s - loss: 3.0696e-04 - val_loss: 2.8224e-04 - 9s/epoch - 103us/sample
Epoch 56/85
84077/84077 - 9s - loss: 3.0680e-04 - val_loss: 2.8351e-04 - 9s/epoch - 104us/sample
Epoch 57/85
84077/84077 - 9s - loss: 3.0652e-04 - val_loss: 2.8276e-04 - 9s/epoch - 104us/sample
Epoch 58/85
84077/84077 - 9s - loss: 3.0512e-04 - val_loss: 2.7910e-04 - 9s/epoch - 103us/sample
Epoch 59/85
84077/84077 - 9s - loss: 3.0427e-04 - val_loss: 2.8105e-04 - 9s/epoch - 103us/sample
Epoch 60/85
84077/84077 - 9s - loss: 3.0466e-04 - val_loss: 2.7950e-04 - 9s/epoch - 103us/sample
Epoch 61/85
84077/84077 - 9s - loss: 3.0400e-04 - val_loss: 2.7827e-04 - 9s/epoch - 103us/sample
Epoch 62/85
84077/84077 - 9s - loss: 3.6126e-04 - val_loss: 3.0332e-04 - 9s/epoch - 103us/sample
Epoch 63/85
84077/84077 - 9s - loss: 3.1138e-04 - val_loss: 2.8241e-04 - 9s/epoch - 104us/sample
Epoch 64/85
84077/84077 - 9s - loss: 3.0430e-04 - val_loss: 2.7938e-04 - 9s/epoch - 104us/sample
Epoch 65/85
84077/84077 - 9s - loss: 3.0287e-04 - val_loss: 2.7842e-04 - 9s/epoch - 104us/sample
Epoch 66/85
84077/84077 - 9s - loss: 3.0134e-04 - val_loss: 2.7711e-04 - 9s/epoch - 104us/sample
Epoch 67/85
84077/84077 - 9s - loss: 3.0341e-04 - val_loss: 2.7949e-04 - 9s/epoch - 103us/sample
Epoch 68/85
84077/84077 - 9s - loss: 3.0189e-04 - val_loss: 2.7783e-04 - 9s/epoch - 103us/sample
Epoch 69/85
84077/84077 - 9s - loss: 3.0068e-04 - val_loss: 2.7565e-04 - 9s/epoch - 103us/sample
Epoch 70/85
84077/84077 - 9s - loss: 3.0020e-04 - val_loss: 2.7582e-04 - 9s/epoch - 103us/sample
Epoch 71/85
84077/84077 - 9s - loss: 2.9951e-04 - val_loss: 2.7704e-04 - 9s/epoch - 104us/sample
Epoch 72/85
84077/84077 - 9s - loss: 2.9984e-04 - val_loss: 2.7573e-04 - 9s/epoch - 104us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.9859e-04 - val_loss: 2.7325e-04 - 9s/epoch - 103us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.9764e-04 - val_loss: 2.7301e-04 - 9s/epoch - 103us/sample
Epoch 75/85
84077/84077 - 9s - loss: 2.9794e-04 - val_loss: 2.7377e-04 - 9s/epoch - 103us/sample
Epoch 76/85
84077/84077 - 9s - loss: 2.9734e-04 - val_loss: 2.7422e-04 - 9s/epoch - 103us/sample
Epoch 77/85
84077/84077 - 9s - loss: 2.9613e-04 - val_loss: 2.7416e-04 - 9s/epoch - 103us/sample
Epoch 78/85
84077/84077 - 9s - loss: 2.9606e-04 - val_loss: 2.7336e-04 - 9s/epoch - 103us/sample
Epoch 79/85
84077/84077 - 9s - loss: 3.0277e-04 - val_loss: 2.8051e-04 - 9s/epoch - 105us/sample
Epoch 80/85
84077/84077 - 9s - loss: 2.9535e-04 - val_loss: 2.7354e-04 - 9s/epoch - 104us/sample
Epoch 81/85
84077/84077 - 9s - loss: 2.9483e-04 - val_loss: 2.7257e-04 - 9s/epoch - 103us/sample
Epoch 82/85
84077/84077 - 9s - loss: 2.9512e-04 - val_loss: 2.7119e-04 - 9s/epoch - 103us/sample
Epoch 83/85
84077/84077 - 9s - loss: 2.9414e-04 - val_loss: 2.7378e-04 - 9s/epoch - 103us/sample
Epoch 84/85
84077/84077 - 9s - loss: 2.9434e-04 - val_loss: 2.7161e-04 - 9s/epoch - 103us/sample
Epoch 85/85
84077/84077 - 9s - loss: 2.9391e-04 - val_loss: 2.7097e-04 - 9s/epoch - 103us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002709651744319455
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:55:00.061595: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_54/outputlayer/BiasAdd' id:68457 op device:{requested: '', assigned: ''} def:{{{node decoder_model_54/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_54/outputlayer/MatMul, decoder_model_54/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.034313801087382904
cosine 0.033896626260651735
MAE: 0.0028255838702635175
RMSE: 0.010605784512934664
r2: 0.912358997103753
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.001, 0.1, 94, 0.00029391472017099277, 0.0002709651744319455, 0.034313801087382904, 0.033896626260651735, 0.0028255838702635175, 0.010605784512934664, 0.912358997103753, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 8 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_162 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_162 (ReLU)               (None, 1886)         0           ['batch_normalization_162[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           177378      ['re_lu_162[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           177378      ['re_lu_162[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1975461     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,118,145
Trainable params: 4,110,413
Non-trainable params: 7,732
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 06:55:23.405801: W tensorflow/c/c_api.cc:291] Operation '{name:'training_106/Adam/iter/Assign' id:70167 op device:{requested: '', assigned: ''} def:{{{node training_106/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_106/Adam/iter, training_106/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:56:10.232753: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_55/mul' id:69741 op device:{requested: '', assigned: ''} def:{{{node loss_55/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_55/mul/x, loss_55/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 58s - loss: 0.0039 - val_loss: 0.0014 - 58s/epoch - 694us/sample
Epoch 2/85
84077/84077 - 43s - loss: 0.0012 - val_loss: 0.0010 - 43s/epoch - 517us/sample
Epoch 3/85
84077/84077 - 43s - loss: 9.6038e-04 - val_loss: 0.0140 - 43s/epoch - 517us/sample
Epoch 4/85
84077/84077 - 43s - loss: 8.6567e-04 - val_loss: 0.0067 - 43s/epoch - 516us/sample
Epoch 5/85
84077/84077 - 44s - loss: 8.2547e-04 - val_loss: 0.0061 - 44s/epoch - 519us/sample
Epoch 6/85
84077/84077 - 43s - loss: 8.0021e-04 - val_loss: 0.0054 - 43s/epoch - 514us/sample
Epoch 7/85
84077/84077 - 44s - loss: 7.8676e-04 - val_loss: 0.0088 - 44s/epoch - 519us/sample
Epoch 8/85
84077/84077 - 43s - loss: 7.7681e-04 - val_loss: 0.0057 - 43s/epoch - 515us/sample
Epoch 9/85
84077/84077 - 44s - loss: 7.6543e-04 - val_loss: 0.0065 - 44s/epoch - 518us/sample
Epoch 10/85
84077/84077 - 43s - loss: 7.6097e-04 - val_loss: 0.0028 - 43s/epoch - 515us/sample
Epoch 11/85
84077/84077 - 44s - loss: 7.5511e-04 - val_loss: 0.0030 - 44s/epoch - 518us/sample
Epoch 12/85
84077/84077 - 43s - loss: 7.5108e-04 - val_loss: 0.0020 - 43s/epoch - 514us/sample
Epoch 13/85
84077/84077 - 43s - loss: 7.4576e-04 - val_loss: 0.0017 - 43s/epoch - 516us/sample
Epoch 14/85
84077/84077 - 43s - loss: 7.3966e-04 - val_loss: 0.0013 - 43s/epoch - 516us/sample
Epoch 15/85
84077/84077 - 44s - loss: 7.3590e-04 - val_loss: 0.0014 - 44s/epoch - 518us/sample
Epoch 16/85
84077/84077 - 44s - loss: 7.3209e-04 - val_loss: 0.0016 - 44s/epoch - 519us/sample
Epoch 17/85
84077/84077 - 43s - loss: 7.3184e-04 - val_loss: 0.0015 - 43s/epoch - 516us/sample
Epoch 18/85
84077/84077 - 43s - loss: 7.3096e-04 - val_loss: 0.0015 - 43s/epoch - 517us/sample
Epoch 19/85
84077/84077 - 44s - loss: 7.3054e-04 - val_loss: 0.0016 - 44s/epoch - 518us/sample
Epoch 20/85
84077/84077 - 43s - loss: 7.2900e-04 - val_loss: 0.0012 - 43s/epoch - 514us/sample
Epoch 21/85
84077/84077 - 43s - loss: 7.2754e-04 - val_loss: 0.0016 - 43s/epoch - 517us/sample
Epoch 22/85
84077/84077 - 43s - loss: 7.2688e-04 - val_loss: 0.0016 - 43s/epoch - 516us/sample
Epoch 23/85
84077/84077 - 43s - loss: 7.2760e-04 - val_loss: 0.0014 - 43s/epoch - 517us/sample
Epoch 24/85
84077/84077 - 44s - loss: 7.2586e-04 - val_loss: 0.0014 - 44s/epoch - 519us/sample
Epoch 25/85
84077/84077 - 43s - loss: 7.2614e-04 - val_loss: 0.0015 - 43s/epoch - 516us/sample
Epoch 26/85
84077/84077 - 44s - loss: 7.2534e-04 - val_loss: 0.0014 - 44s/epoch - 519us/sample
Epoch 27/85
84077/84077 - 43s - loss: 7.2481e-04 - val_loss: 0.0015 - 43s/epoch - 516us/sample
Epoch 28/85
84077/84077 - 43s - loss: 7.2460e-04 - val_loss: 0.0014 - 43s/epoch - 517us/sample
Epoch 29/85
84077/84077 - 43s - loss: 7.2405e-04 - val_loss: 0.0014 - 43s/epoch - 517us/sample
Epoch 30/85
84077/84077 - 43s - loss: 7.2301e-04 - val_loss: 0.0012 - 43s/epoch - 515us/sample
Epoch 31/85
84077/84077 - 43s - loss: 7.2259e-04 - val_loss: 0.0018 - 43s/epoch - 516us/sample
Epoch 32/85
84077/84077 - 43s - loss: 7.2298e-04 - val_loss: 0.0013 - 43s/epoch - 515us/sample
Epoch 33/85
84077/84077 - 44s - loss: 7.2240e-04 - val_loss: 0.0016 - 44s/epoch - 518us/sample
Epoch 34/85
84077/84077 - 43s - loss: 7.2239e-04 - val_loss: 0.0016 - 43s/epoch - 516us/sample
Epoch 35/85
84077/84077 - 43s - loss: 7.2179e-04 - val_loss: 0.0013 - 43s/epoch - 517us/sample
Epoch 36/85
84077/84077 - 44s - loss: 7.2137e-04 - val_loss: 0.0018 - 44s/epoch - 520us/sample
Epoch 37/85
84077/84077 - 44s - loss: 7.2151e-04 - val_loss: 0.0013 - 44s/epoch - 518us/sample
Epoch 38/85
84077/84077 - 44s - loss: 7.2113e-04 - val_loss: 0.0014 - 44s/epoch - 517us/sample
Epoch 39/85
84077/84077 - 43s - loss: 7.2062e-04 - val_loss: 0.0013 - 43s/epoch - 516us/sample
Epoch 40/85
84077/84077 - 43s - loss: 7.2026e-04 - val_loss: 0.0016 - 43s/epoch - 516us/sample
Epoch 41/85
84077/84077 - 43s - loss: 7.1949e-04 - val_loss: 0.0018 - 43s/epoch - 516us/sample
Epoch 42/85
84077/84077 - 44s - loss: 7.1998e-04 - val_loss: 0.0016 - 44s/epoch - 518us/sample
Epoch 43/85
84077/84077 - 43s - loss: 7.1984e-04 - val_loss: 0.0014 - 43s/epoch - 517us/sample
Epoch 44/85
84077/84077 - 44s - loss: 7.1869e-04 - val_loss: 0.0014 - 44s/epoch - 519us/sample
Epoch 45/85
84077/84077 - 43s - loss: 7.1950e-04 - val_loss: 0.0013 - 43s/epoch - 515us/sample
Epoch 46/85
84077/84077 - 44s - loss: 7.1826e-04 - val_loss: 0.0016 - 44s/epoch - 518us/sample
Epoch 47/85
84077/84077 - 43s - loss: 7.1775e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 48/85
84077/84077 - 43s - loss: 7.1818e-04 - val_loss: 0.0015 - 43s/epoch - 516us/sample
Epoch 49/85
84077/84077 - 43s - loss: 7.1824e-04 - val_loss: 0.0014 - 43s/epoch - 514us/sample
Epoch 50/85
84077/84077 - 44s - loss: 7.1726e-04 - val_loss: 0.0013 - 44s/epoch - 518us/sample
Epoch 51/85
84077/84077 - 43s - loss: 7.1715e-04 - val_loss: 0.0020 - 43s/epoch - 517us/sample
Epoch 52/85
84077/84077 - 43s - loss: 7.1732e-04 - val_loss: 0.0014 - 43s/epoch - 517us/sample
Epoch 53/85
84077/84077 - 43s - loss: 7.1630e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 54/85
84077/84077 - 43s - loss: 7.1687e-04 - val_loss: 0.0016 - 43s/epoch - 515us/sample
Epoch 55/85
84077/84077 - 43s - loss: 7.1728e-04 - val_loss: 0.0015 - 43s/epoch - 517us/sample
Epoch 56/85
84077/84077 - 43s - loss: 7.1666e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 57/85
84077/84077 - 44s - loss: 7.1620e-04 - val_loss: 0.0013 - 44s/epoch - 519us/sample
Epoch 58/85
84077/84077 - 43s - loss: 7.1574e-04 - val_loss: 0.0020 - 43s/epoch - 516us/sample
Epoch 59/85
84077/84077 - 43s - loss: 7.1657e-04 - val_loss: 0.0014 - 43s/epoch - 517us/sample
Epoch 60/85
84077/84077 - 44s - loss: 7.1633e-04 - val_loss: 0.0014 - 44s/epoch - 518us/sample
Epoch 61/85
84077/84077 - 43s - loss: 7.1565e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 62/85
84077/84077 - 43s - loss: 7.1682e-04 - val_loss: 0.0015 - 43s/epoch - 517us/sample
Epoch 63/85
84077/84077 - 43s - loss: 7.1531e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 64/85
84077/84077 - 44s - loss: 7.1636e-04 - val_loss: 0.0017 - 44s/epoch - 518us/sample
Epoch 65/85
84077/84077 - 43s - loss: 7.1672e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 66/85
84077/84077 - 43s - loss: 7.1608e-04 - val_loss: 0.0013 - 43s/epoch - 516us/sample
Epoch 67/85
84077/84077 - 44s - loss: 7.1585e-04 - val_loss: 0.0017 - 44s/epoch - 518us/sample
Epoch 68/85
84077/84077 - 43s - loss: 7.1574e-04 - val_loss: 0.0022 - 43s/epoch - 515us/sample
Epoch 69/85
84077/84077 - 43s - loss: 7.1530e-04 - val_loss: 0.0016 - 43s/epoch - 516us/sample
Epoch 70/85
84077/84077 - 44s - loss: 7.1565e-04 - val_loss: 0.0014 - 44s/epoch - 518us/sample
Epoch 71/85
84077/84077 - 43s - loss: 7.1505e-04 - val_loss: 0.0015 - 43s/epoch - 516us/sample
Epoch 72/85
84077/84077 - 44s - loss: 7.1483e-04 - val_loss: 0.0014 - 44s/epoch - 518us/sample
Epoch 73/85
84077/84077 - 43s - loss: 7.1557e-04 - val_loss: 0.0015 - 43s/epoch - 516us/sample
Epoch 74/85
84077/84077 - 43s - loss: 7.1514e-04 - val_loss: 0.0013 - 43s/epoch - 516us/sample
Epoch 75/85
84077/84077 - 44s - loss: 7.1543e-04 - val_loss: 0.0014 - 44s/epoch - 518us/sample
Epoch 76/85
84077/84077 - 43s - loss: 7.1507e-04 - val_loss: 0.0013 - 43s/epoch - 516us/sample
Epoch 77/85
84077/84077 - 44s - loss: 7.1446e-04 - val_loss: 0.0015 - 44s/epoch - 518us/sample
Epoch 78/85
84077/84077 - 43s - loss: 7.1543e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 79/85
84077/84077 - 44s - loss: 7.1550e-04 - val_loss: 0.0015 - 44s/epoch - 519us/sample
Epoch 80/85
84077/84077 - 43s - loss: 7.1374e-04 - val_loss: 0.0016 - 43s/epoch - 515us/sample
Epoch 81/85
84077/84077 - 44s - loss: 7.1507e-04 - val_loss: 0.0015 - 44s/epoch - 518us/sample
Epoch 82/85
84077/84077 - 43s - loss: 7.1513e-04 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 83/85
84077/84077 - 43s - loss: 7.1449e-04 - val_loss: 0.0014 - 43s/epoch - 517us/sample
Epoch 84/85
84077/84077 - 43s - loss: 7.1409e-04 - val_loss: 0.0015 - 43s/epoch - 516us/sample
Epoch 85/85
84077/84077 - 44s - loss: 7.1482e-04 - val_loss: 0.0016 - 44s/epoch - 518us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00155901394446395
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:57:07.142185: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_55/outputlayer/BiasAdd' id:69712 op device:{requested: '', assigned: ''} def:{{{node decoder_model_55/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_55/outputlayer/MatMul, decoder_model_55/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.3004174198258722
cosine 0.29592755937477333
MAE: 0.00853518855344808
RMSE: 0.03959222416031203
r2: -0.22453711076137206
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 8, 85, 0.001, 0.1, 94, 0.0007148166567513184, 0.00155901394446395, 0.3004174198258722, 0.29592755937477333, 0.00853518855344808, 0.03959222416031203, -0.22453711076137206, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0008 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_165 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_165 (ReLU)               (None, 2074)         0           ['batch_normalization_165[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           195050      ['re_lu_165[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           195050      ['re_lu_165[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2171357     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,527,609
Trainable params: 4,519,125
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 07:57:31.024143: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_165/gamma/Assign' id:70581 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_165/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_165/gamma, batch_normalization_165/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:57:45.154304: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_56/mul' id:70996 op device:{requested: '', assigned: ''} def:{{{node loss_56/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_56/mul/x, loss_56/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0088 - val_loss: 0.0020 - 23s/epoch - 277us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0014 - 9s/epoch - 104us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.8151 - val_loss: 0.0015 - 9s/epoch - 104us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0013 - val_loss: 0.0011 - 9s/epoch - 104us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 9.5571e-04 - 9s/epoch - 104us/sample
Epoch 6/90
84077/84077 - 9s - loss: 9.3456e-04 - val_loss: 0.0011 - 9s/epoch - 104us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0023 - val_loss: 0.0011 - 9s/epoch - 105us/sample
Epoch 8/90
84077/84077 - 9s - loss: 9.6616e-04 - val_loss: 7.6962e-04 - 9s/epoch - 106us/sample
Epoch 9/90
84077/84077 - 9s - loss: 4.5648 - val_loss: 0.0032 - 9s/epoch - 104us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0021 - val_loss: 0.0015 - 9s/epoch - 104us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0093 - val_loss: 0.0023 - 9s/epoch - 104us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0018 - val_loss: 0.0013 - 9s/epoch - 104us/sample
Epoch 13/90
84077/84077 - 9s - loss: 0.0024 - val_loss: 0.0165 - 9s/epoch - 104us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0037 - val_loss: 0.0016 - 9s/epoch - 104us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.7078 - val_loss: 0.0031 - 9s/epoch - 105us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0011 - 9s/epoch - 105us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0012 - val_loss: 0.0010 - 9s/epoch - 104us/sample
Epoch 18/90
84077/84077 - 9s - loss: 9.7575e-04 - val_loss: 0.0012 - 9s/epoch - 104us/sample
Epoch 19/90
84077/84077 - 9s - loss: 8.6499e-04 - val_loss: 7.7635e-04 - 9s/epoch - 104us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0018 - val_loss: 9.1097e-04 - 9s/epoch - 104us/sample
Epoch 21/90
84077/84077 - 9s - loss: 8.4274e-04 - val_loss: 7.2851e-04 - 9s/epoch - 104us/sample
Epoch 22/90
84077/84077 - 9s - loss: 7.5394e-04 - val_loss: 7.0264e-04 - 9s/epoch - 104us/sample
Epoch 23/90
84077/84077 - 9s - loss: 7.2185e-04 - val_loss: 6.7594e-04 - 9s/epoch - 106us/sample
Epoch 24/90
84077/84077 - 9s - loss: 7.0217e-04 - val_loss: 6.4707e-04 - 9s/epoch - 105us/sample
Epoch 25/90
84077/84077 - 9s - loss: 6.8438e-04 - val_loss: 6.4888e-04 - 9s/epoch - 105us/sample
Epoch 26/90
84077/84077 - 9s - loss: 6.7904e-04 - val_loss: 6.3752e-04 - 9s/epoch - 104us/sample
Epoch 27/90
84077/84077 - 9s - loss: 6.5922e-04 - val_loss: 6.2402e-04 - 9s/epoch - 104us/sample
Epoch 28/90
84077/84077 - 9s - loss: 6.3230e-04 - val_loss: 5.8568e-04 - 9s/epoch - 104us/sample
Epoch 29/90
84077/84077 - 9s - loss: 6.5051e-04 - val_loss: 5.7599e-04 - 9s/epoch - 104us/sample
Epoch 30/90
84077/84077 - 9s - loss: 6.1202e-04 - val_loss: 5.6867e-04 - 9s/epoch - 105us/sample
Epoch 31/90
84077/84077 - 9s - loss: 5.9942e-04 - val_loss: 5.6316e-04 - 9s/epoch - 106us/sample
Epoch 32/90
84077/84077 - 9s - loss: 5.8890e-04 - val_loss: 5.7900e-04 - 9s/epoch - 105us/sample
Epoch 33/90
84077/84077 - 9s - loss: 5.8130e-04 - val_loss: 5.4073e-04 - 9s/epoch - 104us/sample
Epoch 34/90
84077/84077 - 9s - loss: 5.7122e-04 - val_loss: 5.2634e-04 - 9s/epoch - 104us/sample
Epoch 35/90
84077/84077 - 9s - loss: 5.5364e-04 - val_loss: 5.2296e-04 - 9s/epoch - 104us/sample
Epoch 36/90
84077/84077 - 9s - loss: 5.4624e-04 - val_loss: 5.1326e-04 - 9s/epoch - 104us/sample
Epoch 37/90
84077/84077 - 9s - loss: 5.4055e-04 - val_loss: 5.0932e-04 - 9s/epoch - 104us/sample
Epoch 38/90
84077/84077 - 9s - loss: 5.3427e-04 - val_loss: 5.0291e-04 - 9s/epoch - 105us/sample
Epoch 39/90
84077/84077 - 9s - loss: 5.2775e-04 - val_loss: 4.9643e-04 - 9s/epoch - 105us/sample
Epoch 40/90
84077/84077 - 9s - loss: 5.2225e-04 - val_loss: 4.9041e-04 - 9s/epoch - 105us/sample
Epoch 41/90
84077/84077 - 9s - loss: 5.1696e-04 - val_loss: 4.8719e-04 - 9s/epoch - 104us/sample
Epoch 42/90
84077/84077 - 9s - loss: 5.1231e-04 - val_loss: 4.8072e-04 - 9s/epoch - 104us/sample
Epoch 43/90
84077/84077 - 9s - loss: 5.0902e-04 - val_loss: 4.7852e-04 - 9s/epoch - 104us/sample
Epoch 44/90
84077/84077 - 9s - loss: 5.0680e-04 - val_loss: 4.7780e-04 - 9s/epoch - 104us/sample
Epoch 45/90
84077/84077 - 9s - loss: 5.0369e-04 - val_loss: 4.7628e-04 - 9s/epoch - 104us/sample
Epoch 46/90
84077/84077 - 9s - loss: 5.0130e-04 - val_loss: 4.7013e-04 - 9s/epoch - 105us/sample
Epoch 47/90
84077/84077 - 9s - loss: 4.9799e-04 - val_loss: 4.7033e-04 - 9s/epoch - 105us/sample
Epoch 48/90
84077/84077 - 9s - loss: 4.9520e-04 - val_loss: 4.6757e-04 - 9s/epoch - 105us/sample
Epoch 49/90
84077/84077 - 9s - loss: 4.9145e-04 - val_loss: 4.6307e-04 - 9s/epoch - 104us/sample
Epoch 50/90
84077/84077 - 9s - loss: 4.8943e-04 - val_loss: 4.5952e-04 - 9s/epoch - 104us/sample
Epoch 51/90
84077/84077 - 9s - loss: 4.8762e-04 - val_loss: 4.5883e-04 - 9s/epoch - 104us/sample
Epoch 52/90
84077/84077 - 9s - loss: 4.8555e-04 - val_loss: 4.5676e-04 - 9s/epoch - 104us/sample
Epoch 53/90
84077/84077 - 9s - loss: 4.8244e-04 - val_loss: 4.5301e-04 - 9s/epoch - 104us/sample
Epoch 54/90
84077/84077 - 9s - loss: 4.7942e-04 - val_loss: 4.5226e-04 - 9s/epoch - 106us/sample
Epoch 55/90
84077/84077 - 9s - loss: 4.7593e-04 - val_loss: 4.4806e-04 - 9s/epoch - 105us/sample
Epoch 56/90
84077/84077 - 9s - loss: 4.7343e-04 - val_loss: 4.4520e-04 - 9s/epoch - 104us/sample
Epoch 57/90
84077/84077 - 9s - loss: 4.7104e-04 - val_loss: 4.4227e-04 - 9s/epoch - 104us/sample
Epoch 58/90
84077/84077 - 9s - loss: 4.6886e-04 - val_loss: 4.4129e-04 - 9s/epoch - 104us/sample
Epoch 59/90
84077/84077 - 9s - loss: 4.6604e-04 - val_loss: 4.4186e-04 - 9s/epoch - 105us/sample
Epoch 60/90
84077/84077 - 9s - loss: 4.6487e-04 - val_loss: 4.4119e-04 - 9s/epoch - 105us/sample
Epoch 61/90
84077/84077 - 9s - loss: 4.6360e-04 - val_loss: 4.3665e-04 - 9s/epoch - 105us/sample
Epoch 62/90
84077/84077 - 9s - loss: 4.6180e-04 - val_loss: 4.3414e-04 - 9s/epoch - 106us/sample
Epoch 63/90
84077/84077 - 9s - loss: 4.6024e-04 - val_loss: 4.3460e-04 - 9s/epoch - 104us/sample
Epoch 64/90
84077/84077 - 9s - loss: 4.5866e-04 - val_loss: 4.3011e-04 - 9s/epoch - 104us/sample
Epoch 65/90
84077/84077 - 9s - loss: 4.5753e-04 - val_loss: 4.3182e-04 - 9s/epoch - 104us/sample
Epoch 66/90
84077/84077 - 9s - loss: 4.5623e-04 - val_loss: 4.2995e-04 - 9s/epoch - 104us/sample
Epoch 67/90
84077/84077 - 9s - loss: 4.5553e-04 - val_loss: 4.2782e-04 - 9s/epoch - 104us/sample
Epoch 68/90
84077/84077 - 9s - loss: 4.5404e-04 - val_loss: 4.2669e-04 - 9s/epoch - 105us/sample
Epoch 69/90
84077/84077 - 9s - loss: 4.5375e-04 - val_loss: 4.2826e-04 - 9s/epoch - 105us/sample
Epoch 70/90
84077/84077 - 9s - loss: 4.5273e-04 - val_loss: 4.2822e-04 - 9s/epoch - 106us/sample
Epoch 71/90
84077/84077 - 9s - loss: 4.5130e-04 - val_loss: 4.2743e-04 - 9s/epoch - 105us/sample
Epoch 72/90
84077/84077 - 9s - loss: 4.5042e-04 - val_loss: 4.2399e-04 - 9s/epoch - 104us/sample
Epoch 73/90
84077/84077 - 9s - loss: 4.4966e-04 - val_loss: 4.2470e-04 - 9s/epoch - 104us/sample
Epoch 74/90
84077/84077 - 9s - loss: 4.4873e-04 - val_loss: 4.2189e-04 - 9s/epoch - 104us/sample
Epoch 75/90
84077/84077 - 9s - loss: 4.4741e-04 - val_loss: 4.2082e-04 - 9s/epoch - 105us/sample
Epoch 76/90
84077/84077 - 9s - loss: 4.4675e-04 - val_loss: 4.2068e-04 - 9s/epoch - 104us/sample
Epoch 77/90
84077/84077 - 9s - loss: 4.4499e-04 - val_loss: 4.2080e-04 - 9s/epoch - 105us/sample
Epoch 78/90
84077/84077 - 9s - loss: 4.4458e-04 - val_loss: 4.1741e-04 - 9s/epoch - 106us/sample
Epoch 79/90
84077/84077 - 9s - loss: 4.4497e-04 - val_loss: 4.2043e-04 - 9s/epoch - 105us/sample
Epoch 80/90
84077/84077 - 9s - loss: 4.4446e-04 - val_loss: 4.1699e-04 - 9s/epoch - 104us/sample
Epoch 81/90
84077/84077 - 9s - loss: 4.4376e-04 - val_loss: 4.1937e-04 - 9s/epoch - 104us/sample
Epoch 82/90
84077/84077 - 9s - loss: 4.4318e-04 - val_loss: 4.2173e-04 - 9s/epoch - 104us/sample
Epoch 83/90
84077/84077 - 9s - loss: 4.4260e-04 - val_loss: 4.1680e-04 - 9s/epoch - 104us/sample
Epoch 84/90
84077/84077 - 9s - loss: 4.4137e-04 - val_loss: 4.1797e-04 - 9s/epoch - 105us/sample
Epoch 85/90
84077/84077 - 9s - loss: 4.3972e-04 - val_loss: 4.1314e-04 - 9s/epoch - 105us/sample
Epoch 86/90
84077/84077 - 9s - loss: 4.3805e-04 - val_loss: 4.1373e-04 - 9s/epoch - 105us/sample
Epoch 87/90
84077/84077 - 9s - loss: 4.3768e-04 - val_loss: 4.1306e-04 - 9s/epoch - 104us/sample
Epoch 88/90
84077/84077 - 9s - loss: 4.3664e-04 - val_loss: 4.1360e-04 - 9s/epoch - 104us/sample
Epoch 89/90
84077/84077 - 9s - loss: 4.3708e-04 - val_loss: 4.0979e-04 - 9s/epoch - 105us/sample
Epoch 90/90
84077/84077 - 9s - loss: 4.3584e-04 - val_loss: 4.1102e-04 - 9s/epoch - 105us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00041101693527122434
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:10:52.445221: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_56/outputlayer/BiasAdd' id:70967 op device:{requested: '', assigned: ''} def:{{{node decoder_model_56/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_56/outputlayer/MatMul, decoder_model_56/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0777835239433303
cosine 0.07680255254023964
MAE: 0.003223625915422915
RMSE: 0.016439046691018564
r2: 0.7889528106205949
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 90, 0.0008, 0.1, 94, 0.00043584210399385485, 0.00041101693527122434, 0.0777835239433303, 0.07680255254023964, 0.003223625915422915, 0.016439046691018564, 0.7889528106205949, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 7
Fitness    = 424.308669689168
Last generation's best solutions = [2.1 90 0.001 64 1] with fitness 424.308669689168.
Best solutions :  [array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object), array([2.1, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168, 424.308669689168]
[1.9 85 0.0012000000000000001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_168 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_168 (ReLU)               (None, 1791)         0           ['batch_normalization_168[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_168[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_168[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 08:11:16.955463: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_169/gamma/Assign' id:71978 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_169/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_169/gamma, batch_normalization_169/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:11:30.956372: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_57/mul' id:72251 op device:{requested: '', assigned: ''} def:{{{node loss_57/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_57/mul/x, loss_57/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0086 - val_loss: 0.0015 - 23s/epoch - 277us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0021 - 9s/epoch - 104us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0013 - 9s/epoch - 103us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0037 - val_loss: 0.0015 - 9s/epoch - 103us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0013 - val_loss: 0.0011 - 9s/epoch - 104us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0010 - val_loss: 0.0011 - 9s/epoch - 104us/sample
Epoch 7/85
84077/84077 - 9s - loss: 0.0011 - val_loss: 9.4638e-04 - 9s/epoch - 105us/sample
Epoch 8/85
84077/84077 - 9s - loss: 8.2908e-04 - val_loss: 7.1701e-04 - 9s/epoch - 104us/sample
Epoch 9/85
84077/84077 - 9s - loss: 7.7578e-04 - val_loss: 6.5085e-04 - 9s/epoch - 103us/sample
Epoch 10/85
84077/84077 - 9s - loss: 6.6720e-04 - val_loss: 6.7581e-04 - 9s/epoch - 103us/sample
Epoch 11/85
84077/84077 - 9s - loss: 6.0087e-04 - val_loss: 5.3995e-04 - 9s/epoch - 103us/sample
Epoch 12/85
84077/84077 - 9s - loss: 5.6152e-04 - val_loss: 4.9291e-04 - 9s/epoch - 103us/sample
Epoch 13/85
84077/84077 - 9s - loss: 5.3042e-04 - val_loss: 4.6395e-04 - 9s/epoch - 104us/sample
Epoch 14/85
84077/84077 - 9s - loss: 5.0347e-04 - val_loss: 4.6030e-04 - 9s/epoch - 104us/sample
Epoch 15/85
84077/84077 - 9s - loss: 4.7899e-04 - val_loss: 4.2415e-04 - 9s/epoch - 105us/sample
Epoch 16/85
84077/84077 - 9s - loss: 4.6854e-04 - val_loss: 4.1429e-04 - 9s/epoch - 104us/sample
Epoch 17/85
84077/84077 - 9s - loss: 4.4361e-04 - val_loss: 3.9637e-04 - 9s/epoch - 103us/sample
Epoch 18/85
84077/84077 - 9s - loss: 4.2933e-04 - val_loss: 3.8289e-04 - 9s/epoch - 103us/sample
Epoch 19/85
84077/84077 - 9s - loss: 4.1340e-04 - val_loss: 3.6980e-04 - 9s/epoch - 103us/sample
Epoch 20/85
84077/84077 - 9s - loss: 4.0272e-04 - val_loss: 3.5774e-04 - 9s/epoch - 103us/sample
Epoch 21/85
84077/84077 - 9s - loss: 3.8890e-04 - val_loss: 3.4814e-04 - 9s/epoch - 103us/sample
Epoch 22/85
84077/84077 - 9s - loss: 3.8226e-04 - val_loss: 3.4153e-04 - 9s/epoch - 105us/sample
Epoch 23/85
84077/84077 - 9s - loss: 3.7358e-04 - val_loss: 3.3452e-04 - 9s/epoch - 104us/sample
Epoch 24/85
84077/84077 - 9s - loss: 3.6569e-04 - val_loss: 3.3107e-04 - 9s/epoch - 104us/sample
Epoch 25/85
84077/84077 - 9s - loss: 3.6128e-04 - val_loss: 3.2360e-04 - 9s/epoch - 103us/sample
Epoch 26/85
84077/84077 - 9s - loss: 3.5617e-04 - val_loss: 3.2102e-04 - 9s/epoch - 103us/sample
Epoch 27/85
84077/84077 - 9s - loss: 3.5171e-04 - val_loss: 3.1789e-04 - 9s/epoch - 103us/sample
Epoch 28/85
84077/84077 - 9s - loss: 3.4823e-04 - val_loss: 3.1633e-04 - 9s/epoch - 103us/sample
Epoch 29/85
84077/84077 - 9s - loss: 3.4628e-04 - val_loss: 3.1437e-04 - 9s/epoch - 104us/sample
Epoch 30/85
84077/84077 - 9s - loss: 3.4101e-04 - val_loss: 3.0952e-04 - 9s/epoch - 105us/sample
Epoch 31/85
84077/84077 - 9s - loss: 3.4205e-04 - val_loss: 3.0526e-04 - 9s/epoch - 104us/sample
Epoch 32/85
84077/84077 - 9s - loss: 3.3566e-04 - val_loss: 3.0559e-04 - 9s/epoch - 104us/sample
Epoch 33/85
84077/84077 - 9s - loss: 3.3279e-04 - val_loss: 3.0353e-04 - 9s/epoch - 103us/sample
Epoch 34/85
84077/84077 - 9s - loss: 3.2940e-04 - val_loss: 3.0091e-04 - 9s/epoch - 103us/sample
Epoch 35/85
84077/84077 - 9s - loss: 3.2735e-04 - val_loss: 2.9943e-04 - 9s/epoch - 104us/sample
Epoch 36/85
84077/84077 - 9s - loss: 3.2703e-04 - val_loss: 2.9862e-04 - 9s/epoch - 104us/sample
Epoch 37/85
84077/84077 - 9s - loss: 3.2364e-04 - val_loss: 2.9708e-04 - 9s/epoch - 104us/sample
Epoch 38/85
84077/84077 - 9s - loss: 3.2438e-04 - val_loss: 2.9715e-04 - 9s/epoch - 104us/sample
Epoch 39/85
84077/84077 - 9s - loss: 3.2073e-04 - val_loss: 2.9436e-04 - 9s/epoch - 104us/sample
Epoch 40/85
84077/84077 - 9s - loss: 3.2466e-04 - val_loss: 2.9844e-04 - 9s/epoch - 103us/sample
Epoch 41/85
84077/84077 - 9s - loss: 3.1883e-04 - val_loss: 2.9328e-04 - 9s/epoch - 104us/sample
Epoch 42/85
84077/84077 - 9s - loss: 3.2877e-04 - val_loss: 2.9201e-04 - 9s/epoch - 103us/sample
Epoch 43/85
84077/84077 - 9s - loss: 3.1546e-04 - val_loss: 2.9012e-04 - 9s/epoch - 104us/sample
Epoch 44/85
84077/84077 - 9s - loss: 3.1515e-04 - val_loss: 2.8938e-04 - 9s/epoch - 104us/sample
Epoch 45/85
84077/84077 - 9s - loss: 3.1506e-04 - val_loss: 2.8833e-04 - 9s/epoch - 104us/sample
Epoch 46/85
84077/84077 - 9s - loss: 3.1218e-04 - val_loss: 2.8687e-04 - 9s/epoch - 104us/sample
Epoch 47/85
84077/84077 - 9s - loss: 3.1038e-04 - val_loss: 2.8383e-04 - 9s/epoch - 103us/sample
Epoch 48/85
84077/84077 - 9s - loss: 3.0908e-04 - val_loss: 2.8515e-04 - 9s/epoch - 103us/sample
Epoch 49/85
84077/84077 - 9s - loss: 3.0896e-04 - val_loss: 2.8632e-04 - 9s/epoch - 103us/sample
Epoch 50/85
84077/84077 - 9s - loss: 3.0710e-04 - val_loss: 2.8410e-04 - 9s/epoch - 103us/sample
Epoch 51/85
84077/84077 - 9s - loss: 3.0613e-04 - val_loss: 2.8404e-04 - 9s/epoch - 103us/sample
Epoch 52/85
84077/84077 - 9s - loss: 3.0605e-04 - val_loss: 2.8361e-04 - 9s/epoch - 104us/sample
Epoch 53/85
84077/84077 - 9s - loss: 3.0504e-04 - val_loss: 2.8131e-04 - 9s/epoch - 105us/sample
Epoch 54/85
84077/84077 - 9s - loss: 3.0453e-04 - val_loss: 2.8014e-04 - 9s/epoch - 104us/sample
Epoch 55/85
84077/84077 - 9s - loss: 3.0442e-04 - val_loss: 2.8247e-04 - 9s/epoch - 103us/sample
Epoch 56/85
84077/84077 - 9s - loss: 3.0311e-04 - val_loss: 2.8331e-04 - 9s/epoch - 103us/sample
Epoch 57/85
84077/84077 - 9s - loss: 3.0208e-04 - val_loss: 2.7736e-04 - 9s/epoch - 103us/sample
Epoch 58/85
84077/84077 - 9s - loss: 3.0109e-04 - val_loss: 2.8069e-04 - 9s/epoch - 103us/sample
Epoch 59/85
84077/84077 - 9s - loss: 3.0005e-04 - val_loss: 2.8045e-04 - 9s/epoch - 104us/sample
Epoch 60/85
84077/84077 - 9s - loss: 3.0011e-04 - val_loss: 2.7865e-04 - 9s/epoch - 105us/sample
Epoch 61/85
84077/84077 - 9s - loss: 2.9878e-04 - val_loss: 2.7768e-04 - 9s/epoch - 104us/sample
Epoch 62/85
84077/84077 - 9s - loss: 2.9870e-04 - val_loss: 2.7934e-04 - 9s/epoch - 103us/sample
Epoch 63/85
84077/84077 - 9s - loss: 3.0262e-04 - val_loss: 2.8117e-04 - 9s/epoch - 103us/sample
Epoch 64/85
84077/84077 - 9s - loss: 2.9707e-04 - val_loss: 2.7796e-04 - 9s/epoch - 104us/sample
Epoch 65/85
84077/84077 - 9s - loss: 2.9931e-04 - val_loss: 2.7680e-04 - 9s/epoch - 103us/sample
Epoch 66/85
84077/84077 - 9s - loss: 2.9618e-04 - val_loss: 2.7673e-04 - 9s/epoch - 104us/sample
Epoch 67/85
84077/84077 - 9s - loss: 2.9464e-04 - val_loss: 2.7471e-04 - 9s/epoch - 104us/sample
Epoch 68/85
84077/84077 - 9s - loss: 2.9642e-04 - val_loss: 2.7670e-04 - 9s/epoch - 105us/sample
Epoch 69/85
84077/84077 - 9s - loss: 2.9456e-04 - val_loss: 2.7608e-04 - 9s/epoch - 104us/sample
Epoch 70/85
84077/84077 - 9s - loss: 2.9390e-04 - val_loss: 2.7373e-04 - 9s/epoch - 104us/sample
Epoch 71/85
84077/84077 - 9s - loss: 2.9344e-04 - val_loss: 2.7551e-04 - 9s/epoch - 103us/sample
Epoch 72/85
84077/84077 - 9s - loss: 2.9298e-04 - val_loss: 2.7226e-04 - 9s/epoch - 103us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.9203e-04 - val_loss: 2.7513e-04 - 9s/epoch - 103us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.9133e-04 - val_loss: 2.7205e-04 - 9s/epoch - 103us/sample
Epoch 75/85
84077/84077 - 9s - loss: 2.9179e-04 - val_loss: 2.7339e-04 - 9s/epoch - 104us/sample
Epoch 76/85
84077/84077 - 9s - loss: 2.9119e-04 - val_loss: 2.7053e-04 - 9s/epoch - 104us/sample
Epoch 77/85
84077/84077 - 9s - loss: 2.9211e-04 - val_loss: 2.7304e-04 - 9s/epoch - 104us/sample
Epoch 78/85
84077/84077 - 9s - loss: 2.9043e-04 - val_loss: 2.7133e-04 - 9s/epoch - 104us/sample
Epoch 79/85
84077/84077 - 9s - loss: 2.8969e-04 - val_loss: 2.7157e-04 - 9s/epoch - 104us/sample
Epoch 80/85
84077/84077 - 9s - loss: 2.8955e-04 - val_loss: 2.7042e-04 - 9s/epoch - 103us/sample
Epoch 81/85
84077/84077 - 9s - loss: 2.8881e-04 - val_loss: 2.7242e-04 - 9s/epoch - 103us/sample
Epoch 82/85
84077/84077 - 9s - loss: 2.8831e-04 - val_loss: 2.6932e-04 - 9s/epoch - 104us/sample
Epoch 83/85
84077/84077 - 9s - loss: 3.0425e-04 - val_loss: 3.9573e-04 - 9s/epoch - 105us/sample
Epoch 84/85
84077/84077 - 9s - loss: 3.0367e-04 - val_loss: 2.7218e-04 - 9s/epoch - 104us/sample
Epoch 85/85
84077/84077 - 9s - loss: 2.8858e-04 - val_loss: 2.7221e-04 - 9s/epoch - 103us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00027221472968481556
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:23:48.313404: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_57/outputlayer/BiasAdd' id:72222 op device:{requested: '', assigned: ''} def:{{{node decoder_model_57/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_57/outputlayer/MatMul, decoder_model_57/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03445691066846297
cosine 0.034033339465655614
MAE: 0.002679219448276414
RMSE: 0.010755692304942407
r2: 0.9098621572868959
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.1, 94, 0.000288575952793544, 0.00027221472968481556, 0.03445691066846297, 0.034033339465655614, 0.002679219448276414, 0.010755692304942407, 0.9098621572868959, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0008 16 0] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_171 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_171 (ReLU)               (None, 1791)         0           ['batch_normalization_171[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_171[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_171[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 08:24:13.280888: W tensorflow/c/c_api.cc:291] Operation '{name:'training_112/Adam/dense_enc0_58/bias/v/Assign' id:74162 op device:{requested: '', assigned: ''} def:{{{node training_112/Adam/dense_enc0_58/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_112/Adam/dense_enc0_58/bias/v, training_112/Adam/dense_enc0_58/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:24:43.434252: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_58/mul' id:73525 op device:{requested: '', assigned: ''} def:{{{node loss_58/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_58/mul/x, loss_58/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 41s - loss: 0.0693 - val_loss: 0.0675 - 41s/epoch - 485us/sample
Epoch 2/85
84077/84077 - 25s - loss: 0.0669 - val_loss: 0.0674 - 25s/epoch - 302us/sample
Epoch 3/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 4/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 5/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 6/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 7/85
84077/84077 - 25s - loss: 0.0678 - val_loss: 0.0675 - 25s/epoch - 302us/sample
Epoch 8/85
84077/84077 - 26s - loss: 0.0669 - val_loss: 0.0674 - 26s/epoch - 305us/sample
Epoch 9/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 10/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 11/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 12/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 13/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 14/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 15/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 302us/sample
Epoch 16/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 17/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 18/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 19/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 20/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 21/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 22/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 23/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 24/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 25/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 26/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 27/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 28/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 29/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 30/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 31/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 32/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 33/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 34/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 35/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 36/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 37/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 38/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 39/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 40/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 41/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 42/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 43/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 44/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 45/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 46/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 47/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 48/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 306us/sample
Epoch 49/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 50/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 51/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 52/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 53/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 54/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 55/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 56/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 57/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 58/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 59/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 302us/sample
Epoch 60/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 61/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 62/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 63/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 64/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 65/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 66/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 67/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 68/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 302us/sample
Epoch 69/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 301us/sample
Epoch 70/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 71/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 72/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 73/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 74/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 75/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 76/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 77/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 78/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 302us/sample
Epoch 79/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 80/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 304us/sample
Epoch 81/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 82/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
Epoch 83/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 305us/sample
Epoch 84/85
84077/84077 - 26s - loss: 0.0668 - val_loss: 0.0673 - 26s/epoch - 303us/sample
Epoch 85/85
84077/84077 - 25s - loss: 0.0668 - val_loss: 0.0673 - 25s/epoch - 303us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0673457291842566
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:00:34.703370: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_58/outputlayer/BiasAdd' id:73477 op device:{requested: '', assigned: ''} def:{{{node decoder_model_58/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_58/outputlayer/MatMul, decoder_model_58/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2928663853944307
cosine 1.2227513777904897
MAE: 5.635615127235907
RMSE: 7.450164617818831
r2: -40840.15957407675
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'binary_crossentropy', 16, 85, 0.0008, 0.1, 94, 0.06683690977215728, 0.0673457291842566, 1.2928663853944307, 1.2227513777904897, 5.635615127235907, 7.450164617818831, -40840.15957407675, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.0008 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_174 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_174 (ReLU)               (None, 1791)         0           ['batch_normalization_174[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_174[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_174[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 09:00:59.511903: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_176/gamma/Assign' id:74652 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_176/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_176/gamma, batch_normalization_176/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:01:14.018267: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_59/mul' id:74843 op device:{requested: '', assigned: ''} def:{{{node loss_59/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_59/mul/x, loss_59/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0161 - val_loss: 0.0016 - 24s/epoch - 287us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0013 - 9s/epoch - 106us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0013 - val_loss: 0.0011 - 9s/epoch - 105us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0011 - val_loss: 8.9432e-04 - 9s/epoch - 107us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 106us/sample
Epoch 6/85
84077/84077 - 9s - loss: 0.0019 - val_loss: 9.6436e-04 - 9s/epoch - 106us/sample
Epoch 7/85
84077/84077 - 9s - loss: 9.1737e-04 - val_loss: 7.7960e-04 - 9s/epoch - 106us/sample
Epoch 8/85
84077/84077 - 9s - loss: 7.9097e-04 - val_loss: 6.5936e-04 - 9s/epoch - 105us/sample
Epoch 9/85
84077/84077 - 9s - loss: 7.0229e-04 - val_loss: 6.0053e-04 - 9s/epoch - 106us/sample
Epoch 10/85
84077/84077 - 9s - loss: 6.5998e-04 - val_loss: 6.0210e-04 - 9s/epoch - 105us/sample
Epoch 11/85
84077/84077 - 9s - loss: 6.0427e-04 - val_loss: 5.2006e-04 - 9s/epoch - 106us/sample
Epoch 12/85
84077/84077 - 9s - loss: 5.7632e-04 - val_loss: 4.8260e-04 - 9s/epoch - 106us/sample
Epoch 13/85
84077/84077 - 9s - loss: 5.3074e-04 - val_loss: 4.9052e-04 - 9s/epoch - 106us/sample
Epoch 14/85
84077/84077 - 9s - loss: 5.0752e-04 - val_loss: 4.3407e-04 - 9s/epoch - 107us/sample
Epoch 15/85
84077/84077 - 9s - loss: 4.8602e-04 - val_loss: 4.1828e-04 - 9s/epoch - 106us/sample
Epoch 16/85
84077/84077 - 9s - loss: 4.5717e-04 - val_loss: 4.0106e-04 - 9s/epoch - 105us/sample
Epoch 17/85
84077/84077 - 9s - loss: 4.3737e-04 - val_loss: 3.8080e-04 - 9s/epoch - 106us/sample
Epoch 18/85
84077/84077 - 9s - loss: 4.2395e-04 - val_loss: 3.7399e-04 - 9s/epoch - 106us/sample
Epoch 19/85
84077/84077 - 9s - loss: 4.0770e-04 - val_loss: 3.5742e-04 - 9s/epoch - 106us/sample
Epoch 20/85
84077/84077 - 9s - loss: 3.9534e-04 - val_loss: 3.4709e-04 - 9s/epoch - 106us/sample
Epoch 21/85
84077/84077 - 9s - loss: 3.8599e-04 - val_loss: 3.4515e-04 - 9s/epoch - 106us/sample
Epoch 22/85
84077/84077 - 9s - loss: 3.7654e-04 - val_loss: 3.3736e-04 - 9s/epoch - 107us/sample
Epoch 23/85
84077/84077 - 9s - loss: 3.7023e-04 - val_loss: 3.3006e-04 - 9s/epoch - 106us/sample
Epoch 24/85
84077/84077 - 9s - loss: 3.6431e-04 - val_loss: 3.2681e-04 - 9s/epoch - 106us/sample
Epoch 25/85
84077/84077 - 9s - loss: 3.6045e-04 - val_loss: 3.2598e-04 - 9s/epoch - 105us/sample
Epoch 26/85
84077/84077 - 9s - loss: 3.5549e-04 - val_loss: 3.2486e-04 - 9s/epoch - 106us/sample
Epoch 27/85
84077/84077 - 9s - loss: 3.5197e-04 - val_loss: 3.1827e-04 - 9s/epoch - 105us/sample
Epoch 28/85
84077/84077 - 9s - loss: 3.4643e-04 - val_loss: 3.1474e-04 - 9s/epoch - 105us/sample
Epoch 29/85
84077/84077 - 9s - loss: 3.4330e-04 - val_loss: 3.1498e-04 - 9s/epoch - 107us/sample
Epoch 30/85
84077/84077 - 9s - loss: 3.3953e-04 - val_loss: 3.0732e-04 - 9s/epoch - 106us/sample
Epoch 31/85
84077/84077 - 9s - loss: 3.3555e-04 - val_loss: 3.0875e-04 - 9s/epoch - 106us/sample
Epoch 32/85
84077/84077 - 9s - loss: 3.3400e-04 - val_loss: 3.1020e-04 - 9s/epoch - 106us/sample
Epoch 33/85
84077/84077 - 9s - loss: 3.3125e-04 - val_loss: 3.0412e-04 - 9s/epoch - 106us/sample
Epoch 34/85
84077/84077 - 9s - loss: 3.2834e-04 - val_loss: 3.0261e-04 - 9s/epoch - 106us/sample
Epoch 35/85
84077/84077 - 9s - loss: 3.2619e-04 - val_loss: 2.9930e-04 - 9s/epoch - 106us/sample
Epoch 36/85
84077/84077 - 9s - loss: 3.2442e-04 - val_loss: 2.9759e-04 - 9s/epoch - 105us/sample
Epoch 37/85
84077/84077 - 9s - loss: 3.2267e-04 - val_loss: 2.9724e-04 - 9s/epoch - 106us/sample
Epoch 38/85
84077/84077 - 9s - loss: 3.2064e-04 - val_loss: 2.9542e-04 - 9s/epoch - 107us/sample
Epoch 39/85
84077/84077 - 9s - loss: 3.1788e-04 - val_loss: 2.9405e-04 - 9s/epoch - 106us/sample
Epoch 40/85
84077/84077 - 9s - loss: 3.1671e-04 - val_loss: 2.9448e-04 - 9s/epoch - 106us/sample
Epoch 41/85
84077/84077 - 9s - loss: 3.1533e-04 - val_loss: 2.9057e-04 - 9s/epoch - 105us/sample
Epoch 42/85
84077/84077 - 9s - loss: 3.1300e-04 - val_loss: 2.8902e-04 - 9s/epoch - 106us/sample
Epoch 43/85
84077/84077 - 9s - loss: 3.1173e-04 - val_loss: 2.8814e-04 - 9s/epoch - 105us/sample
Epoch 44/85
84077/84077 - 9s - loss: 3.1085e-04 - val_loss: 2.8755e-04 - 9s/epoch - 105us/sample
Epoch 45/85
84077/84077 - 9s - loss: 3.0917e-04 - val_loss: 2.8900e-04 - 9s/epoch - 107us/sample
Epoch 46/85
84077/84077 - 9s - loss: 3.0781e-04 - val_loss: 2.8631e-04 - 9s/epoch - 106us/sample
Epoch 47/85
84077/84077 - 9s - loss: 3.0656e-04 - val_loss: 2.8782e-04 - 9s/epoch - 105us/sample
Epoch 48/85
84077/84077 - 9s - loss: 3.0749e-04 - val_loss: 2.8566e-04 - 9s/epoch - 106us/sample
Epoch 49/85
84077/84077 - 9s - loss: 3.0446e-04 - val_loss: 2.8649e-04 - 9s/epoch - 106us/sample
Epoch 50/85
84077/84077 - 9s - loss: 3.0333e-04 - val_loss: 2.8928e-04 - 9s/epoch - 105us/sample
Epoch 51/85
84077/84077 - 9s - loss: 3.0331e-04 - val_loss: 2.8212e-04 - 9s/epoch - 106us/sample
Epoch 52/85
84077/84077 - 9s - loss: 3.0083e-04 - val_loss: 2.8018e-04 - 9s/epoch - 106us/sample
Epoch 53/85
84077/84077 - 9s - loss: 3.0002e-04 - val_loss: 2.8087e-04 - 9s/epoch - 107us/sample
Epoch 54/85
84077/84077 - 9s - loss: 3.0047e-04 - val_loss: 2.7969e-04 - 9s/epoch - 106us/sample
Epoch 55/85
84077/84077 - 9s - loss: 2.9858e-04 - val_loss: 2.8137e-04 - 9s/epoch - 106us/sample
Epoch 56/85
84077/84077 - 9s - loss: 2.9744e-04 - val_loss: 2.8010e-04 - 9s/epoch - 105us/sample
Epoch 57/85
84077/84077 - 9s - loss: 2.9714e-04 - val_loss: 2.7890e-04 - 9s/epoch - 105us/sample
Epoch 58/85
84077/84077 - 9s - loss: 2.9692e-04 - val_loss: 2.7855e-04 - 9s/epoch - 105us/sample
Epoch 59/85
84077/84077 - 9s - loss: 2.9489e-04 - val_loss: 2.7721e-04 - 9s/epoch - 106us/sample
Epoch 60/85
84077/84077 - 9s - loss: 2.9474e-04 - val_loss: 2.7390e-04 - 9s/epoch - 106us/sample
Epoch 61/85
84077/84077 - 9s - loss: 2.9335e-04 - val_loss: 2.7493e-04 - 9s/epoch - 107us/sample
Epoch 62/85
84077/84077 - 9s - loss: 2.9295e-04 - val_loss: 2.7475e-04 - 9s/epoch - 106us/sample
Epoch 63/85
84077/84077 - 9s - loss: 2.9274e-04 - val_loss: 2.7952e-04 - 9s/epoch - 105us/sample
Epoch 64/85
84077/84077 - 9s - loss: 2.9209e-04 - val_loss: 2.7454e-04 - 9s/epoch - 106us/sample
Epoch 65/85
84077/84077 - 9s - loss: 2.9099e-04 - val_loss: 2.7299e-04 - 9s/epoch - 105us/sample
Epoch 66/85
84077/84077 - 9s - loss: 2.8912e-04 - val_loss: 2.7209e-04 - 9s/epoch - 105us/sample
Epoch 67/85
84077/84077 - 9s - loss: 2.8952e-04 - val_loss: 2.7176e-04 - 9s/epoch - 105us/sample
Epoch 68/85
84077/84077 - 9s - loss: 2.8914e-04 - val_loss: 2.7418e-04 - 9s/epoch - 107us/sample
Epoch 69/85
84077/84077 - 9s - loss: 2.8907e-04 - val_loss: 2.7227e-04 - 9s/epoch - 107us/sample
Epoch 70/85
84077/84077 - 9s - loss: 2.8763e-04 - val_loss: 2.7586e-04 - 9s/epoch - 106us/sample
Epoch 71/85
84077/84077 - 9s - loss: 2.8732e-04 - val_loss: 2.6795e-04 - 9s/epoch - 106us/sample
Epoch 72/85
84077/84077 - 9s - loss: 2.8746e-04 - val_loss: 2.7232e-04 - 9s/epoch - 106us/sample
Epoch 73/85
84077/84077 - 9s - loss: 2.8635e-04 - val_loss: 2.6926e-04 - 9s/epoch - 106us/sample
Epoch 74/85
84077/84077 - 9s - loss: 2.8560e-04 - val_loss: 2.6776e-04 - 9s/epoch - 106us/sample
Epoch 75/85
84077/84077 - 9s - loss: 2.8563e-04 - val_loss: 2.6891e-04 - 9s/epoch - 105us/sample
Epoch 76/85
84077/84077 - 9s - loss: 2.8464e-04 - val_loss: 2.6856e-04 - 9s/epoch - 106us/sample
Epoch 77/85
84077/84077 - 9s - loss: 2.8333e-04 - val_loss: 2.6891e-04 - 9s/epoch - 106us/sample
Epoch 78/85
84077/84077 - 9s - loss: 2.8453e-04 - val_loss: 2.6892e-04 - 9s/epoch - 106us/sample
Epoch 79/85
84077/84077 - 9s - loss: 2.8315e-04 - val_loss: 2.6610e-04 - 9s/epoch - 106us/sample
Epoch 80/85
84077/84077 - 9s - loss: 2.8262e-04 - val_loss: 2.6781e-04 - 9s/epoch - 105us/sample
Epoch 81/85
84077/84077 - 9s - loss: 2.8231e-04 - val_loss: 2.6691e-04 - 9s/epoch - 106us/sample
Epoch 82/85
84077/84077 - 9s - loss: 2.8150e-04 - val_loss: 2.6740e-04 - 9s/epoch - 105us/sample
Epoch 83/85
84077/84077 - 9s - loss: 2.8150e-04 - val_loss: 2.6484e-04 - 9s/epoch - 106us/sample
Epoch 84/85
84077/84077 - 9s - loss: 2.8137e-04 - val_loss: 2.6338e-04 - 9s/epoch - 107us/sample
Epoch 85/85
84077/84077 - 9s - loss: 2.8042e-04 - val_loss: 2.6446e-04 - 9s/epoch - 106us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.0002644615881712248
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:13:46.243692: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_59/outputlayer/BiasAdd' id:74814 op device:{requested: '', assigned: ''} def:{{{node decoder_model_59/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_59/outputlayer/MatMul, decoder_model_59/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.032982610014471204
cosine 0.032588903902392205
MAE: 0.002390173887378177
RMSE: 0.010427510909360593
r2: 0.9153856656344297
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 64, 85, 0.0008, 0.1, 94, 0.0002804246498565364, 0.0002644615881712248, 0.032982610014471204, 0.032588903902392205, 0.002390173887378177, 0.010427510909360593, 0.9153856656344297, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0008 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_177 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_177 (ReLU)               (None, 2074)         0           ['batch_normalization_177[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           195050      ['re_lu_177[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           195050      ['re_lu_177[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2171357     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,527,609
Trainable params: 4,519,125
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 09:14:13.178539: W tensorflow/c/c_api.cc:291] Operation '{name:'training_116/Adam/batch_normalization_178/beta/m/Assign' id:76648 op device:{requested: '', assigned: ''} def:{{{node training_116/Adam/batch_normalization_178/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_116/Adam/batch_normalization_178/beta/m, training_116/Adam/batch_normalization_178/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:14:28.490986: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_60/mul' id:76105 op device:{requested: '', assigned: ''} def:{{{node loss_60/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_60/mul/x, loss_60/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 26s - loss: 0.0053 - val_loss: 9.9789e-04 - 26s/epoch - 307us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0041 - val_loss: 8.4952e-04 - 9s/epoch - 108us/sample
Epoch 3/90
84077/84077 - 9s - loss: 8.1714e-04 - val_loss: 7.5163e-04 - 9s/epoch - 107us/sample
Epoch 4/90
84077/84077 - 9s - loss: 8.7588e-04 - val_loss: 6.2544e-04 - 9s/epoch - 107us/sample
Epoch 5/90
84077/84077 - 9s - loss: 6.8948e-04 - val_loss: 5.9567e-04 - 9s/epoch - 107us/sample
Epoch 6/90
84077/84077 - 9s - loss: 6.1851e-04 - val_loss: 4.9928e-04 - 9s/epoch - 107us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0075 - val_loss: 8.2793e-04 - 9s/epoch - 107us/sample
Epoch 8/90
84077/84077 - 9s - loss: 5.0534e-04 - val_loss: 6.1744e-04 - 9s/epoch - 107us/sample
Epoch 9/90
84077/84077 - 9s - loss: 4.9780e-04 - val_loss: 4.5423e-04 - 9s/epoch - 108us/sample
Epoch 10/90
84077/84077 - 9s - loss: 5.1243e-04 - val_loss: 4.5313e-04 - 9s/epoch - 109us/sample
Epoch 11/90
84077/84077 - 9s - loss: 4.4086e-04 - val_loss: 3.9034e-04 - 9s/epoch - 107us/sample
Epoch 12/90
84077/84077 - 9s - loss: 4.1026e-04 - val_loss: 3.7455e-04 - 9s/epoch - 108us/sample
Epoch 13/90
84077/84077 - 9s - loss: 4.2561e-04 - val_loss: 5.9226e-04 - 9s/epoch - 108us/sample
Epoch 14/90
84077/84077 - 9s - loss: 3.9590e-04 - val_loss: 3.4870e-04 - 9s/epoch - 107us/sample
Epoch 15/90
84077/84077 - 9s - loss: 3.7465e-04 - val_loss: 3.6450e-04 - 9s/epoch - 108us/sample
Epoch 16/90
84077/84077 - 9s - loss: 3.7662e-04 - val_loss: 3.4772e-04 - 9s/epoch - 108us/sample
Epoch 17/90
84077/84077 - 9s - loss: 3.5401e-04 - val_loss: 3.2829e-04 - 9s/epoch - 108us/sample
Epoch 18/90
84077/84077 - 9s - loss: 3.3477e-04 - val_loss: 3.1045e-04 - 9s/epoch - 109us/sample
Epoch 19/90
84077/84077 - 9s - loss: 3.3468e-04 - val_loss: 3.1639e-04 - 9s/epoch - 108us/sample
Epoch 20/90
84077/84077 - 9s - loss: 3.2024e-04 - val_loss: 2.9293e-04 - 9s/epoch - 107us/sample
Epoch 21/90
84077/84077 - 9s - loss: 3.1564e-04 - val_loss: 2.9764e-04 - 9s/epoch - 107us/sample
Epoch 22/90
84077/84077 - 9s - loss: 3.6108e-04 - val_loss: 3.1110e-04 - 9s/epoch - 107us/sample
Epoch 23/90
84077/84077 - 9s - loss: 3.1793e-04 - val_loss: 5.1714e-04 - 9s/epoch - 107us/sample
Epoch 24/90
84077/84077 - 9s - loss: 3.2663e-04 - val_loss: 3.1678e-04 - 9s/epoch - 108us/sample
Epoch 25/90
84077/84077 - 9s - loss: 3.3008e-04 - val_loss: 2.9709e-04 - 9s/epoch - 108us/sample
Epoch 26/90
84077/84077 - 9s - loss: 3.1889e-04 - val_loss: 3.2505e-04 - 9s/epoch - 109us/sample
Epoch 27/90
84077/84077 - 9s - loss: 3.2488e-04 - val_loss: 2.9530e-04 - 9s/epoch - 107us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.6685 - val_loss: 5.9780e-04 - 9s/epoch - 107us/sample
Epoch 29/90
84077/84077 - 9s - loss: 3.3897e-04 - val_loss: 3.1777e-04 - 9s/epoch - 107us/sample
Epoch 30/90
84077/84077 - 9s - loss: 3.3573e-04 - val_loss: 3.3581e-04 - 9s/epoch - 107us/sample
Epoch 31/90
84077/84077 - 9s - loss: 3.9995e-04 - val_loss: 127.0136 - 9s/epoch - 107us/sample
Epoch 32/90
84077/84077 - 9s - loss: 3.5021e-04 - val_loss: 3.3259e-04 - 9s/epoch - 107us/sample
Epoch 33/90
84077/84077 - 9s - loss: 9.8557e-04 - val_loss: 3.9085e-04 - 9s/epoch - 108us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0082 - val_loss: 5.6454e-04 - 9s/epoch - 109us/sample
Epoch 35/90
84077/84077 - 9s - loss: 4.5383e-04 - val_loss: 3.9972e-04 - 9s/epoch - 107us/sample
Epoch 36/90
84077/84077 - 9s - loss: 4.6111e-04 - val_loss: 3.9954e-04 - 9s/epoch - 107us/sample
Epoch 37/90
84077/84077 - 9s - loss: 4.9690e-04 - val_loss: 4.3827e-04 - 9s/epoch - 107us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0026 - val_loss: 16.8048 - 9s/epoch - 107us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0235 - val_loss: 5.9941e-04 - 9s/epoch - 107us/sample
Epoch 40/90
84077/84077 - 9s - loss: 5.9426e-04 - val_loss: 9.3357e-04 - 9s/epoch - 108us/sample
Epoch 41/90
84077/84077 - 9s - loss: 17.4662 - val_loss: 0.0010 - 9s/epoch - 108us/sample
Epoch 42/90
84077/84077 - 9s - loss: 8.3604e-04 - val_loss: 7.4317e-04 - 9s/epoch - 109us/sample
Epoch 43/90
84077/84077 - 9s - loss: 7.3968e-04 - val_loss: 6.9698e-04 - 9s/epoch - 108us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0081 - val_loss: 8.6811e-04 - 9s/epoch - 107us/sample
Epoch 45/90
84077/84077 - 9s - loss: 8.4537e-04 - val_loss: 8.1802e-04 - 9s/epoch - 107us/sample
Epoch 46/90
84077/84077 - 9s - loss: 8.1176e-04 - val_loss: 7.5769e-04 - 9s/epoch - 107us/sample
Epoch 47/90
84077/84077 - 9s - loss: 0.0025 - val_loss: 9.0811e-04 - 9s/epoch - 108us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0013 - val_loss: 8.6243e-04 - 9s/epoch - 108us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0013 - val_loss: 0.0010 - 9s/epoch - 108us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0050 - val_loss: 0.0010 - 9s/epoch - 109us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0439 - val_loss: 0.0013 - 9s/epoch - 108us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 107us/sample
Epoch 53/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 107us/sample
Epoch 54/90
84077/84077 - 9s - loss: 0.0010 - val_loss: 0.0010 - 9s/epoch - 107us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 107us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0232 - val_loss: 0.0011 - 9s/epoch - 108us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 108us/sample
Epoch 58/90
84077/84077 - 9s - loss: 126.5399 - val_loss: 0.0016 - 9s/epoch - 109us/sample
Epoch 59/90
84077/84077 - 9s - loss: 7.4398 - val_loss: 0.0014 - 9s/epoch - 108us/sample
Epoch 60/90
84077/84077 - 9s - loss: 0.4945 - val_loss: 2.4243 - 9s/epoch - 107us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0355 - val_loss: 0.0015 - 9s/epoch - 107us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.2579 - val_loss: 0.0016 - 9s/epoch - 107us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0198 - val_loss: 0.0014 - 9s/epoch - 108us/sample
Epoch 64/90
84077/84077 - 9s - loss: 5.3555 - val_loss: 0.0016 - 9s/epoch - 108us/sample
Epoch 65/90
84077/84077 - 9s - loss: 564.4202 - val_loss: 0.0017 - 9s/epoch - 108us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0079 - val_loss: 0.0311 - 9s/epoch - 108us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0026 - val_loss: 0.0109 - 9s/epoch - 108us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0158 - 9s/epoch - 107us/sample
Epoch 69/90
84077/84077 - 9s - loss: 8121.1524 - val_loss: 0.0019 - 9s/epoch - 107us/sample
Epoch 70/90
84077/84077 - 9s - loss: 1.1633 - val_loss: 0.0016 - 9s/epoch - 108us/sample
Epoch 71/90
84077/84077 - 9s - loss: 0.0067 - val_loss: 0.0017 - 9s/epoch - 107us/sample
Epoch 72/90
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 107us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0060 - val_loss: 0.0016 - 9s/epoch - 108us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0033 - val_loss: 0.0016 - 9s/epoch - 108us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 107us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0034 - val_loss: 0.0019 - 9s/epoch - 107us/sample
Epoch 77/90
84077/84077 - 9s - loss: 38680436.0130 - val_loss: 0.0023 - 9s/epoch - 107us/sample
Epoch 78/90
84077/84077 - 9s - loss: 0.0089 - val_loss: 0.0023 - 9s/epoch - 108us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0020 - val_loss: 0.0019 - 9s/epoch - 108us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0018 - val_loss: 0.0018 - 9s/epoch - 108us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0019 - val_loss: 0.0017 - 9s/epoch - 108us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0141 - val_loss: 0.0019 - 9s/epoch - 108us/sample
Epoch 83/90
84077/84077 - 9s - loss: 0.0019 - val_loss: 0.0017 - 9s/epoch - 108us/sample
Epoch 84/90
84077/84077 - 9s - loss: 3.7960 - val_loss: 0.0018 - 9s/epoch - 107us/sample
Epoch 85/90
84077/84077 - 9s - loss: 158.9479 - val_loss: 0.0023 - 9s/epoch - 107us/sample
Epoch 86/90
84077/84077 - 9s - loss: 89.9282 - val_loss: 0.0024 - 9s/epoch - 107us/sample
Epoch 87/90
84077/84077 - 9s - loss: 7925.7697 - val_loss: 0.0023 - 9s/epoch - 107us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0194 - val_loss: 0.0022 - 9s/epoch - 108us/sample
Epoch 89/90
84077/84077 - 9s - loss: 0.0024 - val_loss: 0.0020 - 9s/epoch - 108us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0024 - val_loss: 0.0021 - 9s/epoch - 108us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.002135512485439837
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:27:59.571252: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_60/outputlayer/BiasAdd' id:76069 op device:{requested: '', assigned: ''} def:{{{node decoder_model_60/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_60/outputlayer/MatMul, decoder_model_60/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.17660635292947757
cosine 0.174211237550555
MAE: 0.004426384346661483
RMSE: 0.022939199803502553
r2: 0.5889768727754752
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'logcosh', 64, 90, 0.0008, 0.1, 94, 0.002402453315628889, 0.002135512485439837, 0.17660635292947757, 0.174211237550555, 0.004426384346661483, 0.022939199803502553, 0.5889768727754752, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 80 0.001 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_180 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_180 (ReLU)               (None, 1791)         0           ['batch_normalization_180[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_180[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_180[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-15 09:28:27.046473: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_182/beta/Assign' id:77199 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_182/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_182/beta, batch_normalization_182/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:28:42.161185: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_61/mul' id:77383 op device:{requested: '', assigned: ''} def:{{{node loss_61/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_61/mul/x, loss_61/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 26s - loss: 0.0094 - val_loss: 0.0018 - 26s/epoch - 305us/sample
Epoch 2/80
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0013 - 9s/epoch - 108us/sample
Epoch 3/80
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0018 - 9s/epoch - 106us/sample
Epoch 4/80
84077/84077 - 9s - loss: 0.0012 - val_loss: 0.0010 - 9s/epoch - 106us/sample
Epoch 5/80
84077/84077 - 9s - loss: 0.0011 - val_loss: 8.8732e-04 - 9s/epoch - 106us/sample
Epoch 6/80
84077/84077 - 9s - loss: 8.4619e-04 - val_loss: 7.4895e-04 - 9s/epoch - 106us/sample
Epoch 7/80
84077/84077 - 9s - loss: 7.7060e-04 - val_loss: 7.6944e-04 - 9s/epoch - 106us/sample
Epoch 8/80
84077/84077 - 9s - loss: 6.8016e-04 - val_loss: 5.8864e-04 - 9s/epoch - 106us/sample
Epoch 9/80
84077/84077 - 9s - loss: 6.1675e-04 - val_loss: 5.2408e-04 - 9s/epoch - 106us/sample
Epoch 10/80
84077/84077 - 9s - loss: 5.7641e-04 - val_loss: 5.1761e-04 - 9s/epoch - 107us/sample
Epoch 11/80
84077/84077 - 9s - loss: 5.2329e-04 - val_loss: 4.9888e-04 - 9s/epoch - 107us/sample
Epoch 12/80
84077/84077 - 9s - loss: 4.9817e-04 - val_loss: 4.3554e-04 - 9s/epoch - 106us/sample
Epoch 13/80
84077/84077 - 9s - loss: 4.7205e-04 - val_loss: 4.1420e-04 - 9s/epoch - 105us/sample
Epoch 14/80
84077/84077 - 9s - loss: 4.6383e-04 - val_loss: 4.1301e-04 - 9s/epoch - 106us/sample
Epoch 15/80
84077/84077 - 9s - loss: 4.4315e-04 - val_loss: 3.9098e-04 - 9s/epoch - 106us/sample
Epoch 16/80
84077/84077 - 9s - loss: 4.1916e-04 - val_loss: 3.6595e-04 - 9s/epoch - 106us/sample
Epoch 17/80
84077/84077 - 9s - loss: 4.0227e-04 - val_loss: 3.5764e-04 - 9s/epoch - 106us/sample
Epoch 18/80
84077/84077 - 9s - loss: 3.9321e-04 - val_loss: 3.4845e-04 - 9s/epoch - 107us/sample
Epoch 19/80
84077/84077 - 9s - loss: 3.8121e-04 - val_loss: 3.3695e-04 - 9s/epoch - 107us/sample
Epoch 20/80
84077/84077 - 9s - loss: 3.7086e-04 - val_loss: 3.3146e-04 - 9s/epoch - 106us/sample
Epoch 21/80
84077/84077 - 9s - loss: 3.6325e-04 - val_loss: 3.2376e-04 - 9s/epoch - 106us/sample
Epoch 22/80
84077/84077 - 9s - loss: 3.5754e-04 - val_loss: 3.1773e-04 - 9s/epoch - 105us/sample
Epoch 23/80
84077/84077 - 9s - loss: 3.5002e-04 - val_loss: 3.1483e-04 - 9s/epoch - 106us/sample
Epoch 24/80
84077/84077 - 9s - loss: 3.5176e-04 - val_loss: 3.0883e-04 - 9s/epoch - 106us/sample
Epoch 25/80
84077/84077 - 9s - loss: 3.4038e-04 - val_loss: 3.0423e-04 - 9s/epoch - 106us/sample
Epoch 26/80
84077/84077 - 9s - loss: 3.3669e-04 - val_loss: 3.0374e-04 - 9s/epoch - 108us/sample
Epoch 27/80
84077/84077 - 9s - loss: 3.3221e-04 - val_loss: 3.0212e-04 - 9s/epoch - 106us/sample
Epoch 28/80
84077/84077 - 9s - loss: 3.2945e-04 - val_loss: 3.0285e-04 - 9s/epoch - 106us/sample
Epoch 29/80
84077/84077 - 9s - loss: 3.2685e-04 - val_loss: 2.9796e-04 - 9s/epoch - 105us/sample
Epoch 30/80
84077/84077 - 9s - loss: 3.2475e-04 - val_loss: 2.9425e-04 - 9s/epoch - 106us/sample
Epoch 31/80
84077/84077 - 9s - loss: 3.2223e-04 - val_loss: 2.9120e-04 - 9s/epoch - 106us/sample
Epoch 32/80
84077/84077 - 9s - loss: 3.1967e-04 - val_loss: 2.9015e-04 - 9s/epoch - 106us/sample
Epoch 33/80
84077/84077 - 9s - loss: 3.1729e-04 - val_loss: 2.9464e-04 - 9s/epoch - 106us/sample
Epoch 34/80
84077/84077 - 9s - loss: 3.1678e-04 - val_loss: 2.8983e-04 - 9s/epoch - 107us/sample
Epoch 35/80
84077/84077 - 9s - loss: 3.1405e-04 - val_loss: 2.9021e-04 - 9s/epoch - 106us/sample
Epoch 36/80
84077/84077 - 9s - loss: 3.1252e-04 - val_loss: 2.8455e-04 - 9s/epoch - 106us/sample
Epoch 37/80
84077/84077 - 9s - loss: 3.0982e-04 - val_loss: 2.8710e-04 - 9s/epoch - 106us/sample
Epoch 38/80
84077/84077 - 9s - loss: 3.0931e-04 - val_loss: 2.8394e-04 - 9s/epoch - 106us/sample
Epoch 39/80
84077/84077 - 9s - loss: 3.0923e-04 - val_loss: 2.8187e-04 - 9s/epoch - 106us/sample
Epoch 40/80
84077/84077 - 9s - loss: 3.0589e-04 - val_loss: 2.8245e-04 - 9s/epoch - 106us/sample
Epoch 41/80
84077/84077 - 9s - loss: 3.0390e-04 - val_loss: 2.7970e-04 - 9s/epoch - 107us/sample
Epoch 42/80
84077/84077 - 9s - loss: 3.0377e-04 - val_loss: 2.7978e-04 - 9s/epoch - 106us/sample
Epoch 43/80
84077/84077 - 9s - loss: 3.0289e-04 - val_loss: 2.7884e-04 - 9s/epoch - 106us/sample
Epoch 44/80
84077/84077 - 9s - loss: 3.0206e-04 - val_loss: 2.7757e-04 - 9s/epoch - 106us/sample
Epoch 45/80
84077/84077 - 9s - loss: 3.0026e-04 - val_loss: 2.7916e-04 - 9s/epoch - 106us/sample
Epoch 46/80
84077/84077 - 9s - loss: 3.0042e-04 - val_loss: 2.7891e-04 - 9s/epoch - 106us/sample
Epoch 47/80
84077/84077 - 9s - loss: 2.9865e-04 - val_loss: 2.7390e-04 - 9s/epoch - 106us/sample
Epoch 48/80
84077/84077 - 9s - loss: 2.9836e-04 - val_loss: 2.7361e-04 - 9s/epoch - 106us/sample
Epoch 49/80
84077/84077 - 9s - loss: 2.9659e-04 - val_loss: 2.7586e-04 - 9s/epoch - 107us/sample
Epoch 50/80
84077/84077 - 9s - loss: 2.9617e-04 - val_loss: 2.7386e-04 - 9s/epoch - 106us/sample
Epoch 51/80
84077/84077 - 9s - loss: 2.9418e-04 - val_loss: 2.7301e-04 - 9s/epoch - 106us/sample
Epoch 52/80
84077/84077 - 9s - loss: 2.9502e-04 - val_loss: 2.6986e-04 - 9s/epoch - 106us/sample
Epoch 53/80
84077/84077 - 9s - loss: 2.9438e-04 - val_loss: 2.7035e-04 - 9s/epoch - 106us/sample
Epoch 54/80
84077/84077 - 9s - loss: 2.9279e-04 - val_loss: 2.7009e-04 - 9s/epoch - 106us/sample
Epoch 55/80
84077/84077 - 9s - loss: 2.9222e-04 - val_loss: 2.7341e-04 - 9s/epoch - 106us/sample
Epoch 56/80
84077/84077 - 9s - loss: 2.9178e-04 - val_loss: 2.7063e-04 - 9s/epoch - 106us/sample
Epoch 57/80
84077/84077 - 9s - loss: 2.9023e-04 - val_loss: 2.6848e-04 - 9s/epoch - 107us/sample
Epoch 58/80
84077/84077 - 9s - loss: 2.9009e-04 - val_loss: 2.7163e-04 - 9s/epoch - 106us/sample
Epoch 59/80
84077/84077 - 9s - loss: 2.8964e-04 - val_loss: 2.6862e-04 - 9s/epoch - 106us/sample
Epoch 60/80
84077/84077 - 9s - loss: 2.8968e-04 - val_loss: 2.6703e-04 - 9s/epoch - 105us/sample
Epoch 61/80
84077/84077 - 9s - loss: 2.8823e-04 - val_loss: 2.6904e-04 - 9s/epoch - 106us/sample
Epoch 62/80
84077/84077 - 9s - loss: 2.8758e-04 - val_loss: 2.6806e-04 - 9s/epoch - 106us/sample
Epoch 63/80
84077/84077 - 9s - loss: 2.8723e-04 - val_loss: 2.6592e-04 - 9s/epoch - 106us/sample
Epoch 64/80
84077/84077 - 9s - loss: 2.8650e-04 - val_loss: 2.6617e-04 - 9s/epoch - 106us/sample
Epoch 65/80
84077/84077 - 9s - loss: 2.8513e-04 - val_loss: 2.6588e-04 - 9s/epoch - 107us/sample
Epoch 66/80
84077/84077 - 9s - loss: 2.8541e-04 - val_loss: 2.6412e-04 - 9s/epoch - 106us/sample
Epoch 67/80
84077/84077 - 9s - loss: 2.8444e-04 - val_loss: 2.6545e-04 - 9s/epoch - 106us/sample
Epoch 68/80
84077/84077 - 9s - loss: 2.8474e-04 - val_loss: 2.6473e-04 - 9s/epoch - 106us/sample
Epoch 69/80
84077/84077 - 9s - loss: 2.8296e-04 - val_loss: 2.6427e-04 - 9s/epoch - 106us/sample
Epoch 70/80
84077/84077 - 9s - loss: 2.8369e-04 - val_loss: 2.6232e-04 - 9s/epoch - 106us/sample
Epoch 71/80
84077/84077 - 9s - loss: 2.8323e-04 - val_loss: 2.6464e-04 - 9s/epoch - 106us/sample
Epoch 72/80
84077/84077 - 9s - loss: 2.8230e-04 - val_loss: 2.6502e-04 - 9s/epoch - 107us/sample
Epoch 73/80
84077/84077 - 9s - loss: 2.8163e-04 - val_loss: 2.6144e-04 - 9s/epoch - 106us/sample
Epoch 74/80
84077/84077 - 9s - loss: 2.8167e-04 - val_loss: 2.6100e-04 - 9s/epoch - 106us/sample
Epoch 75/80
84077/84077 - 9s - loss: 2.8058e-04 - val_loss: 2.6224e-04 - 9s/epoch - 106us/sample
Epoch 76/80
84077/84077 - 9s - loss: 2.8022e-04 - val_loss: 2.6263e-04 - 9s/epoch - 106us/sample
Epoch 77/80
84077/84077 - 9s - loss: 2.7990e-04 - val_loss: 2.5780e-04 - 9s/epoch - 106us/sample
Epoch 78/80
84077/84077 - 9s - loss: 2.7976e-04 - val_loss: 2.5762e-04 - 9s/epoch - 106us/sample
Epoch 79/80
84077/84077 - 9s - loss: 2.7963e-04 - val_loss: 2.5741e-04 - 9s/epoch - 106us/sample
Epoch 80/80
84077/84077 - 9s - loss: 2.7937e-04 - val_loss: 2.5791e-04 - 9s/epoch - 107us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.00025790723411329855
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:40:32.392526: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_61/outputlayer/BiasAdd' id:77354 op device:{requested: '', assigned: ''} def:{{{node decoder_model_61/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_61/outputlayer/MatMul, decoder_model_61/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.032319595356926696
cosine 0.03193037235402707
MAE: 0.002659835723850434
RMSE: 0.010068416192001044
r2: 0.9209944529336889
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 64, 80, 0.001, 0.1, 94, 0.0002793689499863883, 0.00025790723411329855, 0.032319595356926696, 0.03193037235402707, 0.002659835723850434, 0.010068416192001044, 0.9209944529336889, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 85 0.001 8 2] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_183 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_183 (ReLU)               (None, 1791)         0           ['batch_normalization_183[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_183[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_183[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 09:41:00.082621: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_62/kernel/Assign' id:78429 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_62/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_62/kernel, dense_dec0_62/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:41:51.613160: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_62/mul' id:78645 op device:{requested: '', assigned: ''} def:{{{node loss_62/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_62/mul/x, loss_62/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 65s - loss: 0.0022 - val_loss: 8.5162e-04 - 65s/epoch - 769us/sample
Epoch 2/85
84077/84077 - 47s - loss: 7.2145e-04 - val_loss: 6.3201e-04 - 47s/epoch - 564us/sample
Epoch 3/85
84077/84077 - 48s - loss: 6.3147e-04 - val_loss: 6.2288e-04 - 48s/epoch - 566us/sample
Epoch 4/85
84077/84077 - 48s - loss: 6.1284e-04 - val_loss: 6.1867e-04 - 48s/epoch - 566us/sample
Epoch 5/85
84077/84077 - 47s - loss: 6.1164e-04 - val_loss: 6.1731e-04 - 47s/epoch - 565us/sample
Epoch 6/85
84077/84077 - 48s - loss: 6.1156e-04 - val_loss: 6.1708e-04 - 48s/epoch - 567us/sample
Epoch 7/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1743e-04 - 48s/epoch - 566us/sample
Epoch 8/85
84077/84077 - 47s - loss: 6.1149e-04 - val_loss: 6.1703e-04 - 47s/epoch - 564us/sample
Epoch 9/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1701e-04 - 48s/epoch - 566us/sample
Epoch 10/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1715e-04 - 47s/epoch - 563us/sample
Epoch 11/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1719e-04 - 47s/epoch - 564us/sample
Epoch 12/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1711e-04 - 48s/epoch - 567us/sample
Epoch 13/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1703e-04 - 48s/epoch - 565us/sample
Epoch 14/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1754e-04 - 48s/epoch - 566us/sample
Epoch 15/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1738e-04 - 48s/epoch - 565us/sample
Epoch 16/85
84077/84077 - 47s - loss: 6.1153e-04 - val_loss: 6.1764e-04 - 47s/epoch - 564us/sample
Epoch 17/85
84077/84077 - 48s - loss: 6.1150e-04 - val_loss: 6.1715e-04 - 48s/epoch - 565us/sample
Epoch 18/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1715e-04 - 47s/epoch - 563us/sample
Epoch 19/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1712e-04 - 47s/epoch - 565us/sample
Epoch 20/85
84077/84077 - 48s - loss: 6.1152e-04 - val_loss: 6.1741e-04 - 48s/epoch - 567us/sample
Epoch 21/85
84077/84077 - 47s - loss: 6.1149e-04 - val_loss: 6.1717e-04 - 47s/epoch - 564us/sample
Epoch 22/85
84077/84077 - 48s - loss: 6.1150e-04 - val_loss: 6.1708e-04 - 48s/epoch - 566us/sample
Epoch 23/85
84077/84077 - 48s - loss: 6.1152e-04 - val_loss: 6.1729e-04 - 48s/epoch - 565us/sample
Epoch 24/85
84077/84077 - 47s - loss: 6.1153e-04 - val_loss: 6.1758e-04 - 47s/epoch - 565us/sample
Epoch 25/85
84077/84077 - 48s - loss: 6.1154e-04 - val_loss: 6.1694e-04 - 48s/epoch - 567us/sample
Epoch 26/85
84077/84077 - 48s - loss: 6.1150e-04 - val_loss: 6.1721e-04 - 48s/epoch - 565us/sample
Epoch 27/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1698e-04 - 48s/epoch - 568us/sample
Epoch 28/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1693e-04 - 47s/epoch - 565us/sample
Epoch 29/85
84077/84077 - 47s - loss: 6.1155e-04 - val_loss: 6.1724e-04 - 47s/epoch - 565us/sample
Epoch 30/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1696e-04 - 48s/epoch - 568us/sample
Epoch 31/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1791e-04 - 47s/epoch - 563us/sample
Epoch 32/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1798e-04 - 48s/epoch - 568us/sample
Epoch 33/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1708e-04 - 47s/epoch - 564us/sample
Epoch 34/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1691e-04 - 47s/epoch - 564us/sample
Epoch 35/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1759e-04 - 48s/epoch - 567us/sample
Epoch 36/85
84077/84077 - 47s - loss: 6.1154e-04 - val_loss: 6.1720e-04 - 47s/epoch - 564us/sample
Epoch 37/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1685e-04 - 47s/epoch - 565us/sample
Epoch 38/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1718e-04 - 48s/epoch - 567us/sample
Epoch 39/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1738e-04 - 47s/epoch - 564us/sample
Epoch 40/85
84077/84077 - 48s - loss: 6.1152e-04 - val_loss: 6.1729e-04 - 48s/epoch - 567us/sample
Epoch 41/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1777e-04 - 47s/epoch - 564us/sample
Epoch 42/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1837e-04 - 47s/epoch - 564us/sample
Epoch 43/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1697e-04 - 48s/epoch - 566us/sample
Epoch 44/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1755e-04 - 47s/epoch - 563us/sample
Epoch 45/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1709e-04 - 48s/epoch - 568us/sample
Epoch 46/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1693e-04 - 47s/epoch - 564us/sample
Epoch 47/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1712e-04 - 48s/epoch - 566us/sample
Epoch 48/85
84077/84077 - 48s - loss: 6.1150e-04 - val_loss: 6.1736e-04 - 48s/epoch - 566us/sample
Epoch 49/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1706e-04 - 47s/epoch - 564us/sample
Epoch 50/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1716e-04 - 48s/epoch - 565us/sample
Epoch 51/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1729e-04 - 48s/epoch - 568us/sample
Epoch 52/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1727e-04 - 47s/epoch - 564us/sample
Epoch 53/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1691e-04 - 48s/epoch - 568us/sample
Epoch 54/85
84077/84077 - 48s - loss: 6.1155e-04 - val_loss: 6.1706e-04 - 48s/epoch - 566us/sample
Epoch 55/85
84077/84077 - 48s - loss: 6.1152e-04 - val_loss: 6.1734e-04 - 48s/epoch - 566us/sample
Epoch 56/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1701e-04 - 48s/epoch - 568us/sample
Epoch 57/85
84077/84077 - 48s - loss: 6.1155e-04 - val_loss: 6.1724e-04 - 48s/epoch - 567us/sample
Epoch 58/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1743e-04 - 48s/epoch - 567us/sample
Epoch 59/85
84077/84077 - 47s - loss: 6.1154e-04 - val_loss: 6.1697e-04 - 47s/epoch - 564us/sample
Epoch 60/85
84077/84077 - 47s - loss: 6.1153e-04 - val_loss: 6.1682e-04 - 47s/epoch - 564us/sample
Epoch 61/85
84077/84077 - 48s - loss: 6.1150e-04 - val_loss: 6.1759e-04 - 48s/epoch - 566us/sample
Epoch 62/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1724e-04 - 47s/epoch - 562us/sample
Epoch 63/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1690e-04 - 48s/epoch - 565us/sample
Epoch 64/85
84077/84077 - 47s - loss: 6.1158e-04 - val_loss: 6.1744e-04 - 47s/epoch - 565us/sample
Epoch 65/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1754e-04 - 47s/epoch - 564us/sample
Epoch 66/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1700e-04 - 48s/epoch - 566us/sample
Epoch 67/85
84077/84077 - 47s - loss: 6.1150e-04 - val_loss: 6.1678e-04 - 47s/epoch - 563us/sample
Epoch 68/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1724e-04 - 48s/epoch - 566us/sample
Epoch 69/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1745e-04 - 48s/epoch - 567us/sample
Epoch 70/85
84077/84077 - 47s - loss: 6.1150e-04 - val_loss: 6.1778e-04 - 47s/epoch - 563us/sample
Epoch 71/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1687e-04 - 47s/epoch - 565us/sample
Epoch 72/85
84077/84077 - 47s - loss: 6.1154e-04 - val_loss: 6.1723e-04 - 47s/epoch - 565us/sample
Epoch 73/85
84077/84077 - 48s - loss: 6.1150e-04 - val_loss: 6.1690e-04 - 48s/epoch - 567us/sample
Epoch 74/85
84077/84077 - 47s - loss: 6.1152e-04 - val_loss: 6.1699e-04 - 47s/epoch - 565us/sample
Epoch 75/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1770e-04 - 47s/epoch - 564us/sample
Epoch 76/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1759e-04 - 48s/epoch - 567us/sample
Epoch 77/85
84077/84077 - 47s - loss: 6.1153e-04 - val_loss: 6.1779e-04 - 47s/epoch - 565us/sample
Epoch 78/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1748e-04 - 48s/epoch - 566us/sample
Epoch 79/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1776e-04 - 48s/epoch - 566us/sample
Epoch 80/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1711e-04 - 47s/epoch - 564us/sample
Epoch 81/85
84077/84077 - 48s - loss: 6.1153e-04 - val_loss: 6.1746e-04 - 48s/epoch - 568us/sample
Epoch 82/85
84077/84077 - 47s - loss: 6.1151e-04 - val_loss: 6.1671e-04 - 47s/epoch - 564us/sample
Epoch 83/85
84077/84077 - 48s - loss: 6.1151e-04 - val_loss: 6.1712e-04 - 48s/epoch - 566us/sample
Epoch 84/85
84077/84077 - 47s - loss: 6.1150e-04 - val_loss: 6.1723e-04 - 47s/epoch - 564us/sample
Epoch 85/85
84077/84077 - 48s - loss: 6.1150e-04 - val_loss: 6.1744e-04 - 48s/epoch - 566us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.000617440412709712
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:48:32.627745: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_62/outputlayer/BiasAdd' id:78609 op device:{requested: '', assigned: ''} def:{{{node decoder_model_62/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_62/outputlayer/MatMul, decoder_model_62/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.5112003200752255
cosine 0.5016917083465909
MAE: 0.006704320225706003
RMSE: 0.03584026856553415
r2: -0.0033960658297960424
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 8, 85, 0.001, 0.1, 94, 0.0006115016579682689, 0.000617440412709712, 0.5112003200752255, 0.5016917083465909, 0.006704320225706003, 0.03584026856553415, -0.0033960658297960424, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.0006000000000000001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_186 (Batch  (None, 1791)        7164        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_186 (ReLU)               (None, 1791)         0           ['batch_normalization_186[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           168448      ['re_lu_186[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           168448      ['re_lu_186[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1876471     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,911,235
Trainable params: 3,903,883
Non-trainable params: 7,352
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 10:49:00.211120: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_186/gamma/Assign' id:79508 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_186/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_186/gamma, batch_normalization_186/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:49:15.665433: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_63/mul' id:79942 op device:{requested: '', assigned: ''} def:{{{node loss_63/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_63/mul/x, loss_63/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 26s - loss: 0.1847 - val_loss: 0.4225 - 26s/epoch - 309us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0687 - val_loss: 0.0683 - 9s/epoch - 108us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0673 - val_loss: 0.0676 - 9s/epoch - 109us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0671 - val_loss: 0.0678 - 9s/epoch - 109us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0680 - val_loss: 0.0675 - 9s/epoch - 108us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0683 - val_loss: 0.0809 - 9s/epoch - 109us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0684 - val_loss: 0.0677 - 9s/epoch - 110us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0675 - 9s/epoch - 109us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 108us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 13/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 18/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 19/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 21/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 22/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 23/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 24/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 25/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 26/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 27/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 29/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 30/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 31/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 32/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 33/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 35/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 36/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 37/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 40/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 41/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 42/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 43/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 45/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 46/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 47/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 53/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 54/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 58/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 59/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 60/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 64/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 65/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 69/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 70/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 71/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 72/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 77/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 78/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 83/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 84/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 85/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 86/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 87/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 89/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
COMPRESSED VECTOR SIZE: 94
Loss in the autoencoder: 0.06734573128097855
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 11:02:55.155307: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_63/outputlayer/BiasAdd' id:79894 op device:{requested: '', assigned: ''} def:{{{node decoder_model_63/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_63/outputlayer/MatMul, decoder_model_63/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2702435512906698
cosine 1.1893524622843832
MAE: 5.160335991224376
RMSE: 5.857296975963281
r2: -25278.271784654822
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'binary_crossentropy', 64, 90, 0.0006000000000000001, 0.1, 94, 0.06683691532759276, 0.06734573128097855, 1.2702435512906698, 1.1893524622843832, 5.160335991224376, 5.857296975963281, -25278.271784654822, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.001 8 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_189 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_189 (ReLU)               (None, 2074)         0           ['batch_normalization_189[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 94)           195050      ['re_lu_189[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 94)           195050      ['re_lu_189[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 94)           0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2171357     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,527,609
Trainable params: 4,519,125
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 11:03:22.641131: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_191/beta/Assign' id:81076 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_191/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_191/beta, batch_normalization_191/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 11:04:14.405951: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_64/mul' id:81267 op device:{requested: '', assigned: ''} def:{{{node loss_64/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_64/mul/x, loss_64/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 65s - loss: 0.0020 - val_loss: 6.6268e-04 - 65s/epoch - 776us/sample
Epoch 2/85
84077/84077 - 47s - loss: 6.3168e-04 - val_loss: 6.1798e-04 - 47s/epoch - 564us/sample
Epoch 3/85
84077/84077 - 48s - loss: 6.1176e-04 - val_loss: 6.1781e-04 - 48s/epoch - 571us/sample
Epoch 4/85
slurmstepd: error: *** JOB 36005764 ON mb-cas001 CANCELLED AT 2023-02-15T11:06:21 ***
