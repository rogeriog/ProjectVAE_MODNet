start
Wed Feb 15 10:24:55 CET 2023
2023-02-15 10:25:01.857570: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-15 10:25:03.582844: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
         OFM: s^1 - s^1  OFM: s^1 - s^2  ...  OFM: f^14 - f^13  OFM: f^14 - f^14
id                                       ...                                    
id40482        0.000000        0.000000  ...               0.0          0.000000
id40486        0.000000        0.000000  ...               0.0          0.000000
id40487        0.000000        0.000000  ...               0.0          0.000022
id40490        0.000000        0.080160  ...               0.0          0.000000
id40496        0.000000        0.000000  ...               0.0          0.000000
...                 ...             ...  ...               ...               ...
id83741        0.174544        0.746256  ...               0.0          0.000000
id83742        0.000000        0.000000  ...               0.0          1.815492
id83743        0.007311        0.241355  ...               0.0          0.000000
id83744        0.000000        0.000000  ...               0.0          0.439306
id83745        0.000000        0.203940  ...               0.0          0.000000

[93419 rows x 1024 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 90 0.001 64 1]
 [2.1 90 0.001 64 1]
 [1.5 70 0.0005 16 0]
 [2.5 50 0.0005 64 2]
 [2.0 110 0.0005 8 0]
 [2.0 170 0.0005 16 0]
 [2.0 25 0.001 16 1]
 [1.5 90 0.001 32 1]
 [0.5 25 0.001 64 0]
 [1.5 30 0.0005 8 1]]
[2.0 90 0.001 64 1] 0
Shape of dataset to encode: (93419, 943)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1886)        7544        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1886)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 471)          888777      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 471)          888777      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 471)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2901373     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,466,855
Trainable params: 6,458,369
Non-trainable params: 8,486
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-15 10:25:59.646364: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-15 10:26:02.620235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20633 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:0f:00.0, compute capability: 8.6
2023-02-15 10:26:02.621365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20633 MB memory:  -> device: 1, name: NVIDIA A10, pci bus id: 0000:2b:00.0, compute capability: 8.6
2023-02-15 10:26:02.680880: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-15 10:26:03.174697: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/batch_normalization/beta/v/Assign' id:1044 op device:{requested: '', assigned: ''} def:{{{node training/Adam/batch_normalization/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/batch_normalization/beta/v, training/Adam/batch_normalization/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-15 10:26:06.616823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:26:11.861710: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:450 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 13s - loss: 0.0072 - val_loss: 0.0015 - 13s/epoch - 149us/sample
Epoch 2/90
84077/84077 - 4s - loss: 0.0026 - val_loss: 0.0013 - 4s/epoch - 53us/sample
Epoch 3/90
84077/84077 - 4s - loss: 0.0013 - val_loss: 0.0014 - 4s/epoch - 53us/sample
Epoch 4/90
84077/84077 - 4s - loss: 0.0012 - val_loss: 0.0013 - 4s/epoch - 53us/sample
Epoch 5/90
84077/84077 - 4s - loss: 8.5885e-04 - val_loss: 8.5624e-04 - 4s/epoch - 53us/sample
Epoch 6/90
84077/84077 - 4s - loss: 6.7933e-04 - val_loss: 5.0709e-04 - 4s/epoch - 53us/sample
Epoch 7/90
84077/84077 - 4s - loss: 5.8673e-04 - val_loss: 4.6113e-04 - 4s/epoch - 53us/sample
Epoch 8/90
84077/84077 - 4s - loss: 4.5415e-04 - val_loss: 3.6710e-04 - 4s/epoch - 53us/sample
Epoch 9/90
84077/84077 - 4s - loss: 4.0328e-04 - val_loss: 3.3045e-04 - 4s/epoch - 53us/sample
Epoch 10/90
84077/84077 - 4s - loss: 3.3223e-04 - val_loss: 2.7758e-04 - 4s/epoch - 53us/sample
Epoch 11/90
84077/84077 - 4s - loss: 2.9515e-04 - val_loss: 2.3235e-04 - 4s/epoch - 53us/sample
Epoch 12/90
84077/84077 - 4s - loss: 2.7016e-04 - val_loss: 2.2569e-04 - 4s/epoch - 53us/sample
Epoch 13/90
84077/84077 - 4s - loss: 2.4774e-04 - val_loss: 2.1396e-04 - 4s/epoch - 53us/sample
Epoch 14/90
84077/84077 - 4s - loss: 2.3305e-04 - val_loss: 1.9492e-04 - 4s/epoch - 53us/sample
Epoch 15/90
84077/84077 - 4s - loss: 2.1586e-04 - val_loss: 1.8062e-04 - 4s/epoch - 53us/sample
Epoch 16/90
84077/84077 - 4s - loss: 2.0497e-04 - val_loss: 1.7223e-04 - 4s/epoch - 53us/sample
Epoch 17/90
84077/84077 - 4s - loss: 2.0902e-04 - val_loss: 1.7808e-04 - 4s/epoch - 53us/sample
Epoch 18/90
84077/84077 - 4s - loss: 2.0032e-04 - val_loss: 1.6579e-04 - 4s/epoch - 53us/sample
Epoch 19/90
84077/84077 - 4s - loss: 1.8009e-04 - val_loss: 1.5864e-04 - 4s/epoch - 53us/sample
Epoch 20/90
84077/84077 - 4s - loss: 1.8136e-04 - val_loss: 1.6089e-04 - 4s/epoch - 53us/sample
Epoch 21/90
84077/84077 - 4s - loss: 1.7178e-04 - val_loss: 1.5003e-04 - 4s/epoch - 53us/sample
Epoch 22/90
84077/84077 - 4s - loss: 1.6447e-04 - val_loss: 1.6185e-04 - 4s/epoch - 53us/sample
Epoch 23/90
84077/84077 - 4s - loss: 1.6520e-04 - val_loss: 1.9023e-04 - 4s/epoch - 53us/sample
Epoch 24/90
84077/84077 - 4s - loss: 1.6269e-04 - val_loss: 1.4320e-04 - 4s/epoch - 53us/sample
Epoch 25/90
84077/84077 - 4s - loss: 1.5500e-04 - val_loss: 1.3520e-04 - 4s/epoch - 53us/sample
Epoch 26/90
84077/84077 - 4s - loss: 1.5141e-04 - val_loss: 1.3792e-04 - 4s/epoch - 53us/sample
Epoch 27/90
84077/84077 - 4s - loss: 1.4735e-04 - val_loss: 1.3636e-04 - 4s/epoch - 53us/sample
Epoch 28/90
84077/84077 - 4s - loss: 1.4902e-04 - val_loss: 1.2930e-04 - 4s/epoch - 53us/sample
Epoch 29/90
84077/84077 - 4s - loss: 1.4278e-04 - val_loss: 1.3506e-04 - 4s/epoch - 53us/sample
Epoch 30/90
84077/84077 - 4s - loss: 1.4248e-04 - val_loss: 1.2545e-04 - 4s/epoch - 53us/sample
Epoch 31/90
84077/84077 - 4s - loss: 1.4295e-04 - val_loss: 1.4029e-04 - 4s/epoch - 53us/sample
Epoch 32/90
84077/84077 - 4s - loss: 1.3823e-04 - val_loss: 1.2560e-04 - 4s/epoch - 53us/sample
Epoch 33/90
84077/84077 - 4s - loss: 1.3773e-04 - val_loss: 1.2239e-04 - 4s/epoch - 53us/sample
Epoch 34/90
84077/84077 - 4s - loss: 1.3402e-04 - val_loss: 1.2205e-04 - 4s/epoch - 53us/sample
Epoch 35/90
84077/84077 - 4s - loss: 1.3387e-04 - val_loss: 1.2648e-04 - 4s/epoch - 53us/sample
Epoch 36/90
84077/84077 - 4s - loss: 1.3224e-04 - val_loss: 1.1859e-04 - 4s/epoch - 53us/sample
Epoch 37/90
84077/84077 - 4s - loss: 1.3239e-04 - val_loss: 1.2248e-04 - 4s/epoch - 53us/sample
Epoch 38/90
84077/84077 - 4s - loss: 1.3050e-04 - val_loss: 1.1935e-04 - 4s/epoch - 53us/sample
Epoch 39/90
84077/84077 - 4s - loss: 1.2820e-04 - val_loss: 1.1572e-04 - 4s/epoch - 53us/sample
Epoch 40/90
84077/84077 - 4s - loss: 1.3254e-04 - val_loss: 1.1990e-04 - 4s/epoch - 53us/sample
Epoch 41/90
84077/84077 - 4s - loss: 1.2691e-04 - val_loss: 1.1682e-04 - 4s/epoch - 53us/sample
Epoch 42/90
84077/84077 - 4s - loss: 1.2544e-04 - val_loss: 1.1436e-04 - 4s/epoch - 53us/sample
Epoch 43/90
84077/84077 - 4s - loss: 1.2491e-04 - val_loss: 1.1352e-04 - 4s/epoch - 53us/sample
Epoch 44/90
84077/84077 - 4s - loss: 1.2513e-04 - val_loss: 1.1543e-04 - 4s/epoch - 53us/sample
Epoch 45/90
84077/84077 - 4s - loss: 1.2350e-04 - val_loss: 1.1452e-04 - 4s/epoch - 53us/sample
Epoch 46/90
84077/84077 - 4s - loss: 1.2242e-04 - val_loss: 1.1231e-04 - 4s/epoch - 53us/sample
Epoch 47/90
84077/84077 - 4s - loss: 1.2109e-04 - val_loss: 1.1386e-04 - 4s/epoch - 53us/sample
Epoch 48/90
84077/84077 - 4s - loss: 1.2060e-04 - val_loss: 1.1133e-04 - 4s/epoch - 53us/sample
Epoch 49/90
84077/84077 - 4s - loss: 1.1963e-04 - val_loss: 1.1159e-04 - 4s/epoch - 53us/sample
Epoch 50/90
84077/84077 - 4s - loss: 1.1882e-04 - val_loss: 1.0932e-04 - 4s/epoch - 53us/sample
Epoch 51/90
84077/84077 - 4s - loss: 1.1856e-04 - val_loss: 1.1019e-04 - 4s/epoch - 53us/sample
Epoch 52/90
84077/84077 - 4s - loss: 1.1792e-04 - val_loss: 1.1124e-04 - 4s/epoch - 53us/sample
Epoch 53/90
84077/84077 - 4s - loss: 1.1684e-04 - val_loss: 1.0730e-04 - 4s/epoch - 53us/sample
Epoch 54/90
84077/84077 - 4s - loss: 1.1697e-04 - val_loss: 1.0722e-04 - 4s/epoch - 53us/sample
Epoch 55/90
84077/84077 - 4s - loss: 1.1657e-04 - val_loss: 1.0873e-04 - 4s/epoch - 53us/sample
Epoch 56/90
84077/84077 - 4s - loss: 1.1640e-04 - val_loss: 1.1213e-04 - 4s/epoch - 53us/sample
Epoch 57/90
84077/84077 - 4s - loss: 1.1578e-04 - val_loss: 1.1318e-04 - 4s/epoch - 53us/sample
Epoch 58/90
84077/84077 - 4s - loss: 1.1516e-04 - val_loss: 1.0750e-04 - 4s/epoch - 53us/sample
Epoch 59/90
84077/84077 - 4s - loss: 1.1418e-04 - val_loss: 1.0607e-04 - 4s/epoch - 53us/sample
Epoch 60/90
84077/84077 - 4s - loss: 1.1335e-04 - val_loss: 1.0350e-04 - 4s/epoch - 53us/sample
Epoch 61/90
84077/84077 - 4s - loss: 1.1525e-04 - val_loss: 1.0687e-04 - 4s/epoch - 53us/sample
Epoch 62/90
84077/84077 - 4s - loss: 1.1652e-04 - val_loss: 1.1067e-04 - 4s/epoch - 53us/sample
Epoch 63/90
84077/84077 - 4s - loss: 1.1415e-04 - val_loss: 1.0473e-04 - 4s/epoch - 53us/sample
Epoch 64/90
84077/84077 - 4s - loss: 1.1310e-04 - val_loss: 1.0732e-04 - 4s/epoch - 53us/sample
Epoch 65/90
84077/84077 - 4s - loss: 1.1175e-04 - val_loss: 1.0417e-04 - 4s/epoch - 53us/sample
Epoch 66/90
84077/84077 - 4s - loss: 1.1173e-04 - val_loss: 1.0347e-04 - 4s/epoch - 53us/sample
Epoch 67/90
84077/84077 - 4s - loss: 1.1091e-04 - val_loss: 1.0559e-04 - 4s/epoch - 53us/sample
Epoch 68/90
84077/84077 - 4s - loss: 1.1180e-04 - val_loss: 1.0439e-04 - 4s/epoch - 53us/sample
Epoch 69/90
84077/84077 - 4s - loss: 1.1079e-04 - val_loss: 1.0293e-04 - 4s/epoch - 53us/sample
Epoch 70/90
84077/84077 - 4s - loss: 1.1034e-04 - val_loss: 1.0174e-04 - 4s/epoch - 53us/sample
Epoch 71/90
84077/84077 - 4s - loss: 1.1105e-04 - val_loss: 1.0248e-04 - 4s/epoch - 53us/sample
Epoch 72/90
84077/84077 - 4s - loss: 1.1090e-04 - val_loss: 1.0989e-04 - 4s/epoch - 53us/sample
Epoch 73/90
84077/84077 - 4s - loss: 1.0942e-04 - val_loss: 1.0446e-04 - 4s/epoch - 53us/sample
Epoch 74/90
84077/84077 - 4s - loss: 1.0867e-04 - val_loss: 1.0093e-04 - 4s/epoch - 53us/sample
Epoch 75/90
84077/84077 - 4s - loss: 1.1015e-04 - val_loss: 1.0203e-04 - 4s/epoch - 53us/sample
Epoch 76/90
84077/84077 - 4s - loss: 1.0864e-04 - val_loss: 9.9296e-05 - 4s/epoch - 53us/sample
Epoch 77/90
84077/84077 - 4s - loss: 1.0817e-04 - val_loss: 9.9591e-05 - 4s/epoch - 53us/sample
Epoch 78/90
84077/84077 - 4s - loss: 1.0792e-04 - val_loss: 9.9032e-05 - 4s/epoch - 53us/sample
Epoch 79/90
84077/84077 - 4s - loss: 1.0807e-04 - val_loss: 9.9516e-05 - 4s/epoch - 53us/sample
Epoch 80/90
84077/84077 - 4s - loss: 1.0668e-04 - val_loss: 1.0037e-04 - 4s/epoch - 53us/sample
Epoch 81/90
84077/84077 - 4s - loss: 1.0666e-04 - val_loss: 1.0121e-04 - 4s/epoch - 53us/sample
Epoch 82/90
84077/84077 - 4s - loss: 1.1577e-04 - val_loss: 1.0694e-04 - 4s/epoch - 53us/sample
Epoch 83/90
84077/84077 - 4s - loss: 1.0630e-04 - val_loss: 1.0306e-04 - 4s/epoch - 53us/sample
Epoch 84/90
84077/84077 - 4s - loss: 1.0633e-04 - val_loss: 1.0321e-04 - 4s/epoch - 53us/sample
Epoch 85/90
84077/84077 - 4s - loss: 1.0611e-04 - val_loss: 1.0572e-04 - 4s/epoch - 53us/sample
Epoch 86/90
84077/84077 - 4s - loss: 1.0644e-04 - val_loss: 1.0473e-04 - 4s/epoch - 53us/sample
Epoch 87/90
84077/84077 - 4s - loss: 1.0600e-04 - val_loss: 1.0265e-04 - 4s/epoch - 53us/sample
Epoch 88/90
84077/84077 - 4s - loss: 1.0553e-04 - val_loss: 1.0164e-04 - 4s/epoch - 53us/sample
Epoch 89/90
84077/84077 - 4s - loss: 1.0538e-04 - val_loss: 1.0290e-04 - 4s/epoch - 53us/sample
Epoch 90/90
84077/84077 - 4s - loss: 1.0498e-04 - val_loss: 1.0454e-04 - 4s/epoch - 53us/sample
COMPRESSED VECTOR SIZE: 471
Loss in the autoencoder: 0.00010454250982009443
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:32:50.721979: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:421 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.011795665808373932
cosine 0.011650135563209917
MAE: 0.00169270434508035
RMSE: 0.006745563685867388
r2: 0.9646185546526578
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 90, 0.001, 0.5, 471, 0.00010498032675338147, 0.00010454250982009443, 0.011795665808373932, 0.011650135563209917, 0.00169270434508035, 0.006745563685867388, 0.9646185546526578, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1980)        7920        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1980)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 471)          933051      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 471)          933051      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 471)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          3034759     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,777,901
Trainable params: 6,769,039
Non-trainable params: 8,862
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 10:32:55.840111: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_4/moving_mean/Assign' id:1448 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_4/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_4/moving_mean, batch_normalization_4/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:33:00.509609: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1711 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 5s - loss: 0.0147 - val_loss: 0.0017 - 5s/epoch - 61us/sample
Epoch 2/90
84077/84077 - 4s - loss: 0.0031 - val_loss: 0.0016 - 4s/epoch - 53us/sample
Epoch 3/90
84077/84077 - 4s - loss: 0.0042 - val_loss: 0.0016 - 4s/epoch - 53us/sample
Epoch 4/90
84077/84077 - 4s - loss: 0.0011 - val_loss: 8.6063e-04 - 4s/epoch - 53us/sample
Epoch 5/90
84077/84077 - 4s - loss: 8.9771e-04 - val_loss: 7.0920e-04 - 4s/epoch - 53us/sample
Epoch 6/90
84077/84077 - 4s - loss: 7.9091e-04 - val_loss: 6.0393e-04 - 4s/epoch - 53us/sample
Epoch 7/90
84077/84077 - 4s - loss: 6.0638e-04 - val_loss: 6.7451e-04 - 4s/epoch - 53us/sample
Epoch 8/90
84077/84077 - 4s - loss: 5.2160e-04 - val_loss: 3.9105e-04 - 4s/epoch - 53us/sample
Epoch 9/90
84077/84077 - 4s - loss: 4.3547e-04 - val_loss: 3.6946e-04 - 4s/epoch - 53us/sample
Epoch 10/90
84077/84077 - 4s - loss: 3.7242e-04 - val_loss: 2.9899e-04 - 4s/epoch - 53us/sample
Epoch 11/90
84077/84077 - 4s - loss: 3.6565e-04 - val_loss: 3.0916e-04 - 4s/epoch - 53us/sample
Epoch 12/90
84077/84077 - 4s - loss: 3.0986e-04 - val_loss: 2.4729e-04 - 4s/epoch - 53us/sample
Epoch 13/90
84077/84077 - 4s - loss: 2.8028e-04 - val_loss: 2.2466e-04 - 4s/epoch - 53us/sample
Epoch 14/90
84077/84077 - 4s - loss: 2.4278e-04 - val_loss: 2.2143e-04 - 4s/epoch - 53us/sample
Epoch 15/90
84077/84077 - 4s - loss: 2.3514e-04 - val_loss: 1.9626e-04 - 4s/epoch - 53us/sample
Epoch 16/90
84077/84077 - 4s - loss: 2.1193e-04 - val_loss: 1.8262e-04 - 4s/epoch - 53us/sample
Epoch 17/90
84077/84077 - 4s - loss: 2.0084e-04 - val_loss: 1.7514e-04 - 4s/epoch - 53us/sample
Epoch 18/90
84077/84077 - 4s - loss: 1.9100e-04 - val_loss: 1.7076e-04 - 4s/epoch - 53us/sample
Epoch 19/90
84077/84077 - 4s - loss: 1.8205e-04 - val_loss: 1.5798e-04 - 4s/epoch - 53us/sample
Epoch 20/90
84077/84077 - 4s - loss: 1.7519e-04 - val_loss: 1.5399e-04 - 4s/epoch - 53us/sample
Epoch 21/90
84077/84077 - 4s - loss: 1.6872e-04 - val_loss: 1.4852e-04 - 4s/epoch - 53us/sample
Epoch 22/90
84077/84077 - 4s - loss: 1.6313e-04 - val_loss: 1.4915e-04 - 4s/epoch - 53us/sample
Epoch 23/90
84077/84077 - 4s - loss: 1.5874e-04 - val_loss: 1.4093e-04 - 4s/epoch - 53us/sample
Epoch 24/90
84077/84077 - 4s - loss: 1.5672e-04 - val_loss: 1.4124e-04 - 4s/epoch - 53us/sample
Epoch 25/90
84077/84077 - 4s - loss: 1.5230e-04 - val_loss: 1.3659e-04 - 4s/epoch - 53us/sample
Epoch 26/90
84077/84077 - 4s - loss: 1.4785e-04 - val_loss: 1.3529e-04 - 4s/epoch - 53us/sample
Epoch 27/90
84077/84077 - 4s - loss: 1.4597e-04 - val_loss: 1.3231e-04 - 4s/epoch - 53us/sample
Epoch 28/90
84077/84077 - 4s - loss: 1.4219e-04 - val_loss: 1.3058e-04 - 4s/epoch - 53us/sample
Epoch 29/90
84077/84077 - 4s - loss: 1.4220e-04 - val_loss: 1.3082e-04 - 4s/epoch - 53us/sample
Epoch 30/90
84077/84077 - 4s - loss: 1.4115e-04 - val_loss: 1.2766e-04 - 4s/epoch - 53us/sample
Epoch 31/90
84077/84077 - 4s - loss: 1.3672e-04 - val_loss: 1.2728e-04 - 4s/epoch - 53us/sample
Epoch 32/90
84077/84077 - 4s - loss: 1.3512e-04 - val_loss: 1.2307e-04 - 4s/epoch - 53us/sample
Epoch 33/90
84077/84077 - 4s - loss: 1.3444e-04 - val_loss: 1.2496e-04 - 4s/epoch - 53us/sample
Epoch 34/90
84077/84077 - 4s - loss: 1.3182e-04 - val_loss: 1.2340e-04 - 4s/epoch - 53us/sample
Epoch 35/90
84077/84077 - 4s - loss: 1.3055e-04 - val_loss: 1.1956e-04 - 4s/epoch - 53us/sample
Epoch 36/90
84077/84077 - 4s - loss: 1.2907e-04 - val_loss: 1.1902e-04 - 4s/epoch - 53us/sample
Epoch 37/90
84077/84077 - 4s - loss: 1.2826e-04 - val_loss: 1.1988e-04 - 4s/epoch - 53us/sample
Epoch 38/90
84077/84077 - 4s - loss: 1.2649e-04 - val_loss: 1.1805e-04 - 4s/epoch - 53us/sample
Epoch 39/90
84077/84077 - 4s - loss: 1.2516e-04 - val_loss: 1.1517e-04 - 4s/epoch - 53us/sample
Epoch 40/90
84077/84077 - 4s - loss: 1.2398e-04 - val_loss: 1.1374e-04 - 4s/epoch - 53us/sample
Epoch 41/90
84077/84077 - 4s - loss: 1.2355e-04 - val_loss: 1.1384e-04 - 4s/epoch - 53us/sample
Epoch 42/90
84077/84077 - 4s - loss: 1.2339e-04 - val_loss: 1.1544e-04 - 4s/epoch - 53us/sample
Epoch 43/90
84077/84077 - 4s - loss: 1.2148e-04 - val_loss: 1.1237e-04 - 4s/epoch - 53us/sample
Epoch 44/90
84077/84077 - 4s - loss: 1.2135e-04 - val_loss: 1.1036e-04 - 4s/epoch - 53us/sample
Epoch 45/90
84077/84077 - 4s - loss: 1.2007e-04 - val_loss: 1.1135e-04 - 4s/epoch - 53us/sample
Epoch 46/90
84077/84077 - 4s - loss: 1.1943e-04 - val_loss: 1.0948e-04 - 4s/epoch - 53us/sample
Epoch 47/90
84077/84077 - 4s - loss: 1.1938e-04 - val_loss: 1.1024e-04 - 4s/epoch - 53us/sample
Epoch 48/90
84077/84077 - 4s - loss: 1.1988e-04 - val_loss: 1.1024e-04 - 4s/epoch - 53us/sample
Epoch 49/90
84077/84077 - 4s - loss: 1.1763e-04 - val_loss: 1.0968e-04 - 4s/epoch - 53us/sample
Epoch 50/90
84077/84077 - 4s - loss: 1.1660e-04 - val_loss: 1.0792e-04 - 4s/epoch - 53us/sample
Epoch 51/90
84077/84077 - 4s - loss: 1.1701e-04 - val_loss: 1.0773e-04 - 4s/epoch - 53us/sample
Epoch 52/90
84077/84077 - 4s - loss: 1.1534e-04 - val_loss: 1.0716e-04 - 4s/epoch - 53us/sample
Epoch 53/90
84077/84077 - 4s - loss: 1.1509e-04 - val_loss: 1.0651e-04 - 4s/epoch - 53us/sample
Epoch 54/90
84077/84077 - 4s - loss: 1.1449e-04 - val_loss: 1.0747e-04 - 4s/epoch - 53us/sample
Epoch 55/90
84077/84077 - 4s - loss: 1.1412e-04 - val_loss: 1.0616e-04 - 4s/epoch - 53us/sample
Epoch 56/90
84077/84077 - 4s - loss: 1.1340e-04 - val_loss: 1.0594e-04 - 4s/epoch - 53us/sample
Epoch 57/90
84077/84077 - 4s - loss: 1.1219e-04 - val_loss: 1.0564e-04 - 4s/epoch - 53us/sample
Epoch 58/90
84077/84077 - 4s - loss: 1.1244e-04 - val_loss: 1.0585e-04 - 4s/epoch - 53us/sample
Epoch 59/90
84077/84077 - 4s - loss: 1.1157e-04 - val_loss: 1.0427e-04 - 4s/epoch - 53us/sample
Epoch 60/90
84077/84077 - 4s - loss: 1.1158e-04 - val_loss: 1.0293e-04 - 4s/epoch - 53us/sample
Epoch 61/90
84077/84077 - 4s - loss: 1.1049e-04 - val_loss: 1.0408e-04 - 4s/epoch - 53us/sample
Epoch 62/90
84077/84077 - 4s - loss: 1.1001e-04 - val_loss: 1.0478e-04 - 4s/epoch - 53us/sample
Epoch 63/90
84077/84077 - 4s - loss: 1.1655e-04 - val_loss: 1.0502e-04 - 4s/epoch - 53us/sample
Epoch 64/90
84077/84077 - 4s - loss: 1.1041e-04 - val_loss: 1.0239e-04 - 4s/epoch - 53us/sample
Epoch 65/90
84077/84077 - 4s - loss: 1.1020e-04 - val_loss: 1.0185e-04 - 4s/epoch - 53us/sample
Epoch 66/90
84077/84077 - 4s - loss: 1.0912e-04 - val_loss: 1.0324e-04 - 4s/epoch - 53us/sample
Epoch 67/90
84077/84077 - 4s - loss: 1.1102e-04 - val_loss: 1.0043e-04 - 4s/epoch - 53us/sample
Epoch 68/90
84077/84077 - 4s - loss: 1.1029e-04 - val_loss: 1.0358e-04 - 4s/epoch - 53us/sample
Epoch 69/90
84077/84077 - 4s - loss: 1.0827e-04 - val_loss: 1.0120e-04 - 4s/epoch - 53us/sample
Epoch 70/90
84077/84077 - 4s - loss: 1.0810e-04 - val_loss: 1.0064e-04 - 4s/epoch - 53us/sample
Epoch 71/90
84077/84077 - 4s - loss: 1.0757e-04 - val_loss: 1.0206e-04 - 4s/epoch - 53us/sample
Epoch 72/90
84077/84077 - 4s - loss: 1.0740e-04 - val_loss: 9.9982e-05 - 4s/epoch - 53us/sample
Epoch 73/90
84077/84077 - 4s - loss: 1.0708e-04 - val_loss: 9.9851e-05 - 4s/epoch - 53us/sample
Epoch 74/90
84077/84077 - 4s - loss: 1.0696e-04 - val_loss: 1.0055e-04 - 4s/epoch - 53us/sample
Epoch 75/90
84077/84077 - 4s - loss: 1.0569e-04 - val_loss: 1.0077e-04 - 4s/epoch - 53us/sample
Epoch 76/90
84077/84077 - 4s - loss: 1.0579e-04 - val_loss: 9.9052e-05 - 4s/epoch - 53us/sample
Epoch 77/90
84077/84077 - 4s - loss: 1.0611e-04 - val_loss: 9.9106e-05 - 4s/epoch - 53us/sample
Epoch 78/90
84077/84077 - 4s - loss: 1.0520e-04 - val_loss: 9.7386e-05 - 4s/epoch - 53us/sample
Epoch 79/90
84077/84077 - 4s - loss: 1.0510e-04 - val_loss: 9.7596e-05 - 4s/epoch - 53us/sample
Epoch 80/90
84077/84077 - 4s - loss: 1.0464e-04 - val_loss: 9.8064e-05 - 4s/epoch - 53us/sample
Epoch 81/90
84077/84077 - 4s - loss: 1.0720e-04 - val_loss: 9.7382e-05 - 4s/epoch - 53us/sample
Epoch 82/90
84077/84077 - 4s - loss: 1.0423e-04 - val_loss: 9.7938e-05 - 4s/epoch - 53us/sample
Epoch 83/90
84077/84077 - 4s - loss: 1.0413e-04 - val_loss: 9.7768e-05 - 4s/epoch - 53us/sample
Epoch 84/90
84077/84077 - 4s - loss: 1.0628e-04 - val_loss: 9.6289e-05 - 4s/epoch - 53us/sample
Epoch 85/90
84077/84077 - 4s - loss: 1.0386e-04 - val_loss: 9.5964e-05 - 4s/epoch - 53us/sample
Epoch 86/90
84077/84077 - 4s - loss: 1.0341e-04 - val_loss: 9.6145e-05 - 4s/epoch - 53us/sample
Epoch 87/90
84077/84077 - 4s - loss: 1.0449e-04 - val_loss: 9.6004e-05 - 4s/epoch - 53us/sample
Epoch 88/90
84077/84077 - 4s - loss: 1.0301e-04 - val_loss: 9.7254e-05 - 4s/epoch - 53us/sample
Epoch 89/90
84077/84077 - 4s - loss: 1.0862e-04 - val_loss: 9.8324e-05 - 4s/epoch - 53us/sample
Epoch 90/90
84077/84077 - 4s - loss: 1.0291e-04 - val_loss: 9.6467e-05 - 4s/epoch - 53us/sample
COMPRESSED VECTOR SIZE: 471
Loss in the autoencoder: 9.646674592043045e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:39:40.568654: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1682 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.011665628339503015
cosine 0.011528349705063124
MAE: 0.001630946828627857
RMSE: 0.006312402148285564
r2: 0.9691014736953841
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.001, 0.5, 471, 0.00010291190263651866, 9.646674592043045e-05, 0.011665628339503015, 0.011528349705063124, 0.001630946828627857, 0.006312402148285564, 0.9691014736953841, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 70 0.0005 16 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1414)        5656        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1414)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 471)          666465      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 471)          666465      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 471)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2231605     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,905,007
Trainable params: 4,898,409
Non-trainable params: 6,598
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/70
2023-02-15 10:39:45.354088: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/batch_normalization_8/beta/m/Assign' id:3602 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/batch_normalization_8/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/batch_normalization_8/beta/m, training_4/Adam/batch_normalization_8/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:40:00.044033: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2988 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.0679 - val_loss: 0.0674 - 16s/epoch - 187us/sample
Epoch 2/70
84077/84077 - 15s - loss: 0.0669 - val_loss: 0.0674 - 15s/epoch - 174us/sample
Epoch 3/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 4/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 5/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 6/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 7/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 8/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 9/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 10/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 11/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 12/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 13/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 14/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 15/70
84077/84077 - 15s - loss: 0.0669 - val_loss: 0.0674 - 15s/epoch - 174us/sample
Epoch 16/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 17/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 18/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 19/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 20/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 21/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 22/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 23/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 24/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 25/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 26/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 27/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 28/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 29/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 30/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 31/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 32/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 33/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 34/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 35/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 36/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 37/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 38/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 39/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 40/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 41/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 42/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 43/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 44/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 45/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 46/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 47/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 48/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 49/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 50/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 51/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 52/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 53/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 54/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 55/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 56/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 57/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 58/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 59/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 60/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 61/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 62/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 63/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 64/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 65/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 66/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 67/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 68/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 69/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
Epoch 70/70
84077/84077 - 15s - loss: 0.0668 - val_loss: 0.0673 - 15s/epoch - 174us/sample
COMPRESSED VECTOR SIZE: 471
Loss in the autoencoder: 0.06734573158244712
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:56:52.066996: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2940 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.178075795443139
cosine 1.1718723265776136
MAE: 4.7136955767769635
RMSE: 5.4721162641018894
r2: -22090.177916426506
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'binary_crossentropy', 16, 70, 0.0005, 0.5, 471, 0.06683690981956693, 0.06734573158244712, 1.178075795443139, 1.1718723265776136, 4.7136955767769635, 5.4721162641018894, -22090.177916426506, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 2357)        9428        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 2357)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 471)          1110618     ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 471)          1110618     ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 471)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          3569722     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 8,025,394
Trainable params: 8,015,024
Non-trainable params: 10,370
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-15 10:56:56.700675: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/dense_dec0_3/bias/m/Assign' id:4876 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/dense_dec0_3/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/dense_dec0_3/bias/m, training_6/Adam/dense_dec0_3/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:57:01.905635: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4316 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 6s - loss: 0.0036 - val_loss: 8.1919e-04 - 6s/epoch - 70us/sample
Epoch 2/50
84077/84077 - 5s - loss: 0.0028 - val_loss: 0.0018 - 5s/epoch - 57us/sample
Epoch 3/50
84077/84077 - 5s - loss: 0.0010 - val_loss: 6.2730e-04 - 5s/epoch - 57us/sample
Epoch 4/50
84077/84077 - 5s - loss: 5.8697e-04 - val_loss: 4.3783e-04 - 5s/epoch - 57us/sample
Epoch 5/50
84077/84077 - 5s - loss: 4.5275e-04 - val_loss: 3.9216e-04 - 5s/epoch - 57us/sample
Epoch 6/50
84077/84077 - 5s - loss: 4.3781e-04 - val_loss: 3.1853e-04 - 5s/epoch - 57us/sample
Epoch 7/50
84077/84077 - 5s - loss: 3.9028e-04 - val_loss: 3.2757e-04 - 5s/epoch - 57us/sample
Epoch 8/50
84077/84077 - 5s - loss: 5.8140e-04 - val_loss: 6.9741e-04 - 5s/epoch - 57us/sample
Epoch 9/50
84077/84077 - 5s - loss: 4.3053e-04 - val_loss: 2.7488e-04 - 5s/epoch - 57us/sample
Epoch 10/50
84077/84077 - 5s - loss: 2.7735e-04 - val_loss: 2.2574e-04 - 5s/epoch - 57us/sample
Epoch 11/50
84077/84077 - 5s - loss: 2.4474e-04 - val_loss: 1.9843e-04 - 5s/epoch - 57us/sample
Epoch 12/50
84077/84077 - 5s - loss: 2.1853e-04 - val_loss: 1.8522e-04 - 5s/epoch - 57us/sample
Epoch 13/50
84077/84077 - 5s - loss: 2.0381e-04 - val_loss: 1.7292e-04 - 5s/epoch - 57us/sample
Epoch 14/50
84077/84077 - 5s - loss: 1.8625e-04 - val_loss: 1.5622e-04 - 5s/epoch - 57us/sample
Epoch 15/50
84077/84077 - 5s - loss: 1.7048e-04 - val_loss: 1.4400e-04 - 5s/epoch - 57us/sample
Epoch 16/50
84077/84077 - 5s - loss: 1.6266e-04 - val_loss: 1.3498e-04 - 5s/epoch - 57us/sample
Epoch 17/50
84077/84077 - 5s - loss: 1.5411e-04 - val_loss: 1.2952e-04 - 5s/epoch - 57us/sample
Epoch 18/50
84077/84077 - 5s - loss: 1.4575e-04 - val_loss: 1.2409e-04 - 5s/epoch - 57us/sample
Epoch 19/50
84077/84077 - 5s - loss: 1.3995e-04 - val_loss: 1.1896e-04 - 5s/epoch - 57us/sample
Epoch 20/50
84077/84077 - 5s - loss: 1.3412e-04 - val_loss: 1.1448e-04 - 5s/epoch - 57us/sample
Epoch 21/50
84077/84077 - 5s - loss: 1.3009e-04 - val_loss: 1.1230e-04 - 5s/epoch - 57us/sample
Epoch 22/50
84077/84077 - 5s - loss: 1.2691e-04 - val_loss: 1.0937e-04 - 5s/epoch - 57us/sample
Epoch 23/50
84077/84077 - 5s - loss: 1.2353e-04 - val_loss: 1.0766e-04 - 5s/epoch - 57us/sample
Epoch 24/50
84077/84077 - 5s - loss: 1.2093e-04 - val_loss: 1.0440e-04 - 5s/epoch - 57us/sample
Epoch 25/50
84077/84077 - 5s - loss: 1.1807e-04 - val_loss: 1.0306e-04 - 5s/epoch - 57us/sample
Epoch 26/50
84077/84077 - 5s - loss: 1.1581e-04 - val_loss: 1.0137e-04 - 5s/epoch - 57us/sample
Epoch 27/50
84077/84077 - 5s - loss: 1.1417e-04 - val_loss: 1.0157e-04 - 5s/epoch - 57us/sample
Epoch 28/50
84077/84077 - 5s - loss: 1.1169e-04 - val_loss: 9.8359e-05 - 5s/epoch - 57us/sample
Epoch 29/50
84077/84077 - 5s - loss: 1.1150e-04 - val_loss: 9.8573e-05 - 5s/epoch - 57us/sample
Epoch 30/50
84077/84077 - 5s - loss: 1.0925e-04 - val_loss: 9.6691e-05 - 5s/epoch - 57us/sample
Epoch 31/50
84077/84077 - 5s - loss: 1.0742e-04 - val_loss: 9.6486e-05 - 5s/epoch - 57us/sample
Epoch 32/50
84077/84077 - 5s - loss: 1.0630e-04 - val_loss: 9.5685e-05 - 5s/epoch - 57us/sample
Epoch 33/50
84077/84077 - 5s - loss: 1.0495e-04 - val_loss: 9.4028e-05 - 5s/epoch - 57us/sample
Epoch 34/50
84077/84077 - 5s - loss: 1.0388e-04 - val_loss: 9.3646e-05 - 5s/epoch - 57us/sample
Epoch 35/50
84077/84077 - 5s - loss: 1.0264e-04 - val_loss: 9.2676e-05 - 5s/epoch - 57us/sample
Epoch 36/50
84077/84077 - 5s - loss: 1.0173e-04 - val_loss: 9.1590e-05 - 5s/epoch - 57us/sample
Epoch 37/50
84077/84077 - 5s - loss: 1.0141e-04 - val_loss: 9.0799e-05 - 5s/epoch - 57us/sample
Epoch 38/50
84077/84077 - 5s - loss: 9.9451e-05 - val_loss: 9.0177e-05 - 5s/epoch - 57us/sample
Epoch 39/50
84077/84077 - 5s - loss: 9.9401e-05 - val_loss: 8.9023e-05 - 5s/epoch - 57us/sample
Epoch 40/50
84077/84077 - 5s - loss: 9.8026e-05 - val_loss: 8.8774e-05 - 5s/epoch - 57us/sample
Epoch 41/50
84077/84077 - 5s - loss: 1.1170e-04 - val_loss: 9.1075e-05 - 5s/epoch - 57us/sample
Epoch 42/50
84077/84077 - 5s - loss: 1.0337e-04 - val_loss: 9.0889e-05 - 5s/epoch - 57us/sample
Epoch 43/50
84077/84077 - 5s - loss: 9.7882e-05 - val_loss: 8.8644e-05 - 5s/epoch - 57us/sample
Epoch 44/50
84077/84077 - 5s - loss: 9.6453e-05 - val_loss: 8.7235e-05 - 5s/epoch - 57us/sample
Epoch 45/50
84077/84077 - 5s - loss: 9.5788e-05 - val_loss: 8.8859e-05 - 5s/epoch - 57us/sample
Epoch 46/50
84077/84077 - 5s - loss: 9.4761e-05 - val_loss: 8.7230e-05 - 5s/epoch - 57us/sample
Epoch 47/50
84077/84077 - 5s - loss: 9.4340e-05 - val_loss: 8.5456e-05 - 5s/epoch - 57us/sample
Epoch 48/50
84077/84077 - 5s - loss: 9.4023e-05 - val_loss: 8.5802e-05 - 5s/epoch - 57us/sample
Epoch 49/50
84077/84077 - 5s - loss: 9.3187e-05 - val_loss: 8.5312e-05 - 5s/epoch - 57us/sample
Epoch 50/50
84077/84077 - 5s - loss: 9.2630e-05 - val_loss: 8.5138e-05 - 5s/epoch - 57us/sample
COMPRESSED VECTOR SIZE: 471
Loss in the autoencoder: 8.513805474603465e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 11:00:57.842523: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4280 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.022363212706205803
cosine 0.022086526638525943
MAE: 0.0021649619774950207
RMSE: 0.009142922411491719
r2: 0.9348093294826353
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 50, 0.0005, 0.5, 471, 9.262998909550649e-05, 8.513805474603465e-05, 0.022363212706205803, 0.022086526638525943, 0.0021649619774950207, 0.009142922411491719, 0.9348093294826353, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 110 0.0005 8 0] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 1886)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 471)          888777      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 471)          888777      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 471)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2901373     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,466,855
Trainable params: 6,458,369
Non-trainable params: 8,486
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/110
2023-02-15 11:01:03.402323: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_14/gamma/Assign' id:5406 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_14/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_14/gamma, batch_normalization_14/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 11:01:32.845963: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5616 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 31s - loss: 0.0673 - val_loss: 0.0674 - 31s/epoch - 372us/sample
Epoch 2/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0674 - 30s/epoch - 356us/sample
Epoch 3/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 4/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 5/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 6/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 7/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 8/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 9/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 10/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 356us/sample
Epoch 11/110
84077/84077 - 30s - loss: 0.0668 - val_loss: 0.0673 - 30s/epoch - 357us/sample
Epoch 12/110
slurmstepd: error: *** JOB 36005774 ON mb-icg101 CANCELLED AT 2023-02-15T11:06:42 ***
