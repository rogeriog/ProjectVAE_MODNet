start
Tue Feb 14 18:03:58 CET 2023
2023-02-14 18:04:02.168260: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
         OFM: s^1 - s^1  OFM: s^1 - s^2  ...  OFM: f^14 - f^13  OFM: f^14 - f^14
id                                       ...                                    
id40482        0.000000        0.000000  ...               0.0          0.000000
id40486        0.000000        0.000000  ...               0.0          0.000000
id40487        0.000000        0.000000  ...               0.0          0.000022
id40490        0.000000        0.080160  ...               0.0          0.000000
id40496        0.000000        0.000000  ...               0.0          0.000000
...                 ...             ...  ...               ...               ...
id83741        0.174544        0.746256  ...               0.0          0.000000
id83742        0.000000        0.000000  ...               0.0          1.815492
id83743        0.007311        0.241355  ...               0.0          0.000000
id83744        0.000000        0.000000  ...               0.0          0.439306
id83745        0.000000        0.203940  ...               0.0          0.000000

[93419 rows x 1024 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 90 0.001 64 1]
 [2.1 90 0.001 64 1]
 [1.5 70 0.0005 16 0]
 [2.5 50 0.0005 64 2]
 [2.0 110 0.0005 8 0]
 [2.0 170 0.0005 16 0]
 [2.0 25 0.001 16 1]
 [1.5 90 0.001 32 1]
 [0.5 25 0.001 64 0]
 [1.5 30 0.0005 8 1]]
[2.0 90 0.001 64 1] 0
Shape of dataset to encode: (93419, 943)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1886)        7544        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1886)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-14 18:05:18.051403: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 18:05:21.784119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38400 MB memory:  -> device: 0, name: A100-PCIE-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0
2023-02-14 18:05:21.786977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38400 MB memory:  -> device: 1, name: A100-PCIE-40GB, pci bus id: 0000:a1:00.0, compute capability: 8.0
2023-02-14 18:05:21.854778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-14 18:05:22.852272: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/batch_normalization_2/gamma/v/Assign' id:1111 op device:{requested: '', assigned: ''} def:{{{node training/Adam/batch_normalization_2/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/batch_normalization_2/gamma/v, training/Adam/batch_normalization_2/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-14 18:05:24.594549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:05:31.810042: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:450 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0078 - val_loss: 0.0015 - 14s/epoch - 169us/sample
Epoch 2/90
84077/84077 - 7s - loss: 0.0022 - val_loss: 196.3514 - 7s/epoch - 79us/sample
Epoch 3/90
84077/84077 - 7s - loss: 0.1270 - val_loss: 0.0010 - 7s/epoch - 78us/sample
Epoch 4/90
84077/84077 - 7s - loss: 0.0010 - val_loss: 8.1503e-04 - 7s/epoch - 79us/sample
Epoch 5/90
84077/84077 - 7s - loss: 0.0010 - val_loss: 7.7431e-04 - 7s/epoch - 79us/sample
Epoch 6/90
84077/84077 - 6s - loss: 7.8195e-04 - val_loss: 6.3122e-04 - 6s/epoch - 77us/sample
Epoch 7/90
84077/84077 - 7s - loss: 7.3343e-04 - val_loss: 6.0698e-04 - 7s/epoch - 78us/sample
Epoch 8/90
84077/84077 - 7s - loss: 5.6103e-04 - val_loss: 4.7823e-04 - 7s/epoch - 78us/sample
Epoch 9/90
84077/84077 - 7s - loss: 5.0279e-04 - val_loss: 4.2766e-04 - 7s/epoch - 77us/sample
Epoch 10/90
84077/84077 - 7s - loss: 4.3032e-04 - val_loss: 3.5622e-04 - 7s/epoch - 78us/sample
Epoch 11/90
84077/84077 - 6s - loss: 3.9272e-04 - val_loss: 3.1895e-04 - 6s/epoch - 77us/sample
Epoch 12/90
84077/84077 - 6s - loss: 3.4548e-04 - val_loss: 2.8981e-04 - 6s/epoch - 77us/sample
Epoch 13/90
84077/84077 - 6s - loss: 3.1436e-04 - val_loss: 2.6841e-04 - 6s/epoch - 77us/sample
Epoch 14/90
84077/84077 - 7s - loss: 3.1031e-04 - val_loss: 2.4950e-04 - 7s/epoch - 79us/sample
Epoch 15/90
84077/84077 - 6s - loss: 2.7684e-04 - val_loss: 2.4383e-04 - 6s/epoch - 77us/sample
Epoch 16/90
84077/84077 - 6s - loss: 2.6607e-04 - val_loss: 2.3266e-04 - 6s/epoch - 77us/sample
Epoch 17/90
84077/84077 - 6s - loss: 2.5107e-04 - val_loss: 2.1763e-04 - 6s/epoch - 77us/sample
Epoch 18/90
84077/84077 - 6s - loss: 2.3802e-04 - val_loss: 2.1571e-04 - 6s/epoch - 77us/sample
Epoch 19/90
84077/84077 - 7s - loss: 2.3046e-04 - val_loss: 2.0667e-04 - 7s/epoch - 77us/sample
Epoch 20/90
84077/84077 - 7s - loss: 2.2327e-04 - val_loss: 2.0738e-04 - 7s/epoch - 78us/sample
Epoch 21/90
84077/84077 - 6s - loss: 2.1464e-04 - val_loss: 1.9970e-04 - 6s/epoch - 77us/sample
Epoch 22/90
84077/84077 - 6s - loss: 2.0940e-04 - val_loss: 1.9344e-04 - 6s/epoch - 77us/sample
Epoch 23/90
84077/84077 - 6s - loss: 2.0246e-04 - val_loss: 1.9097e-04 - 6s/epoch - 77us/sample
Epoch 24/90
84077/84077 - 7s - loss: 2.0050e-04 - val_loss: 1.8705e-04 - 7s/epoch - 78us/sample
Epoch 25/90
84077/84077 - 7s - loss: 1.9513e-04 - val_loss: 1.8003e-04 - 7s/epoch - 78us/sample
Epoch 26/90
84077/84077 - 7s - loss: 1.9121e-04 - val_loss: 1.7700e-04 - 7s/epoch - 77us/sample
Epoch 27/90
84077/84077 - 6s - loss: 1.8955e-04 - val_loss: 1.7487e-04 - 6s/epoch - 77us/sample
Epoch 28/90
84077/84077 - 6s - loss: 1.8530e-04 - val_loss: 1.7893e-04 - 6s/epoch - 77us/sample
Epoch 29/90
84077/84077 - 7s - loss: 1.8346e-04 - val_loss: 1.7748e-04 - 7s/epoch - 77us/sample
Epoch 30/90
84077/84077 - 7s - loss: 1.8026e-04 - val_loss: 1.7304e-04 - 7s/epoch - 78us/sample
Epoch 31/90
84077/84077 - 6s - loss: 1.7766e-04 - val_loss: 1.6830e-04 - 6s/epoch - 77us/sample
Epoch 32/90
84077/84077 - 6s - loss: 1.7536e-04 - val_loss: 1.6877e-04 - 6s/epoch - 77us/sample
Epoch 33/90
84077/84077 - 6s - loss: 1.7361e-04 - val_loss: 1.6357e-04 - 6s/epoch - 77us/sample
Epoch 34/90
84077/84077 - 7s - loss: 1.7251e-04 - val_loss: 1.6265e-04 - 7s/epoch - 78us/sample
Epoch 35/90
84077/84077 - 7s - loss: 1.7115e-04 - val_loss: 1.6084e-04 - 7s/epoch - 78us/sample
Epoch 36/90
84077/84077 - 6s - loss: 1.6909e-04 - val_loss: 1.5717e-04 - 6s/epoch - 77us/sample
Epoch 37/90
84077/84077 - 6s - loss: 1.6787e-04 - val_loss: 1.5793e-04 - 6s/epoch - 77us/sample
Epoch 38/90
84077/84077 - 6s - loss: 1.6861e-04 - val_loss: 1.5671e-04 - 6s/epoch - 77us/sample
Epoch 39/90
84077/84077 - 6s - loss: 1.6584e-04 - val_loss: 1.5326e-04 - 6s/epoch - 77us/sample
Epoch 40/90
84077/84077 - 7s - loss: 1.6411e-04 - val_loss: 1.5376e-04 - 7s/epoch - 78us/sample
Epoch 41/90
84077/84077 - 7s - loss: 1.6377e-04 - val_loss: 1.5206e-04 - 7s/epoch - 78us/sample
Epoch 42/90
84077/84077 - 7s - loss: 1.6389e-04 - val_loss: 1.5037e-04 - 7s/epoch - 77us/sample
Epoch 43/90
84077/84077 - 6s - loss: 1.6127e-04 - val_loss: 1.5388e-04 - 6s/epoch - 77us/sample
Epoch 44/90
84077/84077 - 6s - loss: 1.6162e-04 - val_loss: 1.4892e-04 - 6s/epoch - 77us/sample
Epoch 45/90
84077/84077 - 6s - loss: 1.5970e-04 - val_loss: 1.5197e-04 - 6s/epoch - 77us/sample
Epoch 46/90
84077/84077 - 6s - loss: 1.5933e-04 - val_loss: 1.4964e-04 - 6s/epoch - 77us/sample
Epoch 47/90
84077/84077 - 6s - loss: 1.5816e-04 - val_loss: 1.5080e-04 - 6s/epoch - 77us/sample
Epoch 48/90
84077/84077 - 7s - loss: 1.5731e-04 - val_loss: 1.4801e-04 - 7s/epoch - 78us/sample
Epoch 49/90
84077/84077 - 6s - loss: 1.5619e-04 - val_loss: 1.4810e-04 - 6s/epoch - 77us/sample
Epoch 50/90
84077/84077 - 7s - loss: 1.6320e-04 - val_loss: 1.4816e-04 - 7s/epoch - 78us/sample
Epoch 51/90
84077/84077 - 6s - loss: 1.5546e-04 - val_loss: 1.4767e-04 - 6s/epoch - 77us/sample
Epoch 52/90
84077/84077 - 6s - loss: 1.5451e-04 - val_loss: 1.4649e-04 - 6s/epoch - 77us/sample
Epoch 53/90
84077/84077 - 6s - loss: 1.5423e-04 - val_loss: 1.4987e-04 - 6s/epoch - 77us/sample
Epoch 54/90
84077/84077 - 7s - loss: 1.5381e-04 - val_loss: 1.4569e-04 - 7s/epoch - 78us/sample
Epoch 55/90
84077/84077 - 7s - loss: 1.5311e-04 - val_loss: 1.4226e-04 - 7s/epoch - 78us/sample
Epoch 56/90
84077/84077 - 6s - loss: 1.5243e-04 - val_loss: 1.4325e-04 - 6s/epoch - 77us/sample
Epoch 57/90
84077/84077 - 6s - loss: 1.5173e-04 - val_loss: 1.4524e-04 - 6s/epoch - 77us/sample
Epoch 58/90
84077/84077 - 6s - loss: 1.5163e-04 - val_loss: 1.4358e-04 - 6s/epoch - 77us/sample
Epoch 59/90
84077/84077 - 6s - loss: 1.5100e-04 - val_loss: 1.4611e-04 - 6s/epoch - 77us/sample
Epoch 60/90
84077/84077 - 6s - loss: 1.5092e-04 - val_loss: 1.4045e-04 - 6s/epoch - 77us/sample
Epoch 61/90
84077/84077 - 7s - loss: 1.4958e-04 - val_loss: 1.4085e-04 - 7s/epoch - 78us/sample
Epoch 62/90
84077/84077 - 7s - loss: 1.4888e-04 - val_loss: 1.4133e-04 - 7s/epoch - 77us/sample
Epoch 63/90
84077/84077 - 6s - loss: 1.4884e-04 - val_loss: 1.4009e-04 - 6s/epoch - 77us/sample
Epoch 64/90
84077/84077 - 6s - loss: 1.4865e-04 - val_loss: 1.4083e-04 - 6s/epoch - 77us/sample
Epoch 65/90
84077/84077 - 6s - loss: 1.5475e-04 - val_loss: 1.3955e-04 - 6s/epoch - 77us/sample
Epoch 66/90
84077/84077 - 7s - loss: 1.4907e-04 - val_loss: 1.4005e-04 - 7s/epoch - 78us/sample
Epoch 67/90
84077/84077 - 7s - loss: 1.4738e-04 - val_loss: 1.3897e-04 - 7s/epoch - 78us/sample
Epoch 68/90
84077/84077 - 6s - loss: 1.4854e-04 - val_loss: 1.3917e-04 - 6s/epoch - 77us/sample
Epoch 69/90
84077/84077 - 7s - loss: 1.4664e-04 - val_loss: 1.3787e-04 - 7s/epoch - 77us/sample
Epoch 70/90
84077/84077 - 6s - loss: 1.4637e-04 - val_loss: 1.3958e-04 - 6s/epoch - 77us/sample
Epoch 71/90
84077/84077 - 6s - loss: 1.4622e-04 - val_loss: 1.3909e-04 - 6s/epoch - 77us/sample
Epoch 72/90
84077/84077 - 7s - loss: 1.4513e-04 - val_loss: 1.3708e-04 - 7s/epoch - 78us/sample
Epoch 73/90
84077/84077 - 7s - loss: 1.4466e-04 - val_loss: 1.3874e-04 - 7s/epoch - 77us/sample
Epoch 74/90
84077/84077 - 7s - loss: 1.4468e-04 - val_loss: 1.3818e-04 - 7s/epoch - 78us/sample
Epoch 75/90
84077/84077 - 6s - loss: 1.4469e-04 - val_loss: 1.3777e-04 - 6s/epoch - 77us/sample
Epoch 76/90
84077/84077 - 6s - loss: 1.4381e-04 - val_loss: 1.3728e-04 - 6s/epoch - 77us/sample
Epoch 77/90
84077/84077 - 6s - loss: 1.4365e-04 - val_loss: 1.3865e-04 - 6s/epoch - 77us/sample
Epoch 78/90
84077/84077 - 7s - loss: 1.4293e-04 - val_loss: 1.3863e-04 - 7s/epoch - 78us/sample
Epoch 79/90
84077/84077 - 7s - loss: 1.4324e-04 - val_loss: 1.3589e-04 - 7s/epoch - 78us/sample
Epoch 80/90
84077/84077 - 6s - loss: 1.4310e-04 - val_loss: 1.3558e-04 - 6s/epoch - 77us/sample
Epoch 81/90
84077/84077 - 6s - loss: 1.4308e-04 - val_loss: 1.3621e-04 - 6s/epoch - 77us/sample
Epoch 82/90
84077/84077 - 6s - loss: 1.4239e-04 - val_loss: 1.3610e-04 - 6s/epoch - 77us/sample
Epoch 83/90
84077/84077 - 7s - loss: 1.4372e-04 - val_loss: 1.3582e-04 - 7s/epoch - 78us/sample
Epoch 84/90
84077/84077 - 7s - loss: 1.4187e-04 - val_loss: 1.3584e-04 - 7s/epoch - 78us/sample
Epoch 85/90
84077/84077 - 6s - loss: 1.4138e-04 - val_loss: 1.3443e-04 - 6s/epoch - 77us/sample
Epoch 86/90
84077/84077 - 7s - loss: 1.4203e-04 - val_loss: 1.3385e-04 - 7s/epoch - 78us/sample
Epoch 87/90
84077/84077 - 6s - loss: 1.4109e-04 - val_loss: 1.3473e-04 - 6s/epoch - 77us/sample
Epoch 88/90
84077/84077 - 6s - loss: 1.4137e-04 - val_loss: 1.3408e-04 - 6s/epoch - 77us/sample
Epoch 89/90
84077/84077 - 7s - loss: 1.4043e-04 - val_loss: 1.3562e-04 - 7s/epoch - 78us/sample
Epoch 90/90
84077/84077 - 7s - loss: 1.4113e-04 - val_loss: 1.3410e-04 - 7s/epoch - 78us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0001341037520176585
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:15:11.567605: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:421 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01552412374378522
cosine 0.015334395476952924
MAE: 0.0016771773145040998
RMSE: 0.007381409521728568
r2: 0.9576043280202294
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 64, 90, 0.001, 0.3, 282, 0.00014112554247647214, 0.0001341037520176585, 0.01552412374378522, 0.015334395476952924, 0.0016771773145040998, 0.007381409521728568, 0.9576043280202294, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 1] 1
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1980)        7920        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1980)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 18:15:17.366946: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/dense_dec0_1/bias/m/Assign' id:2248 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_dec0_1/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_dec0_1/bias/m, training_2/Adam/dense_dec0_1/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:15:24.082856: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1711 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 7s - loss: 0.0267 - val_loss: 0.0015 - 7s/epoch - 88us/sample
Epoch 2/90
84077/84077 - 7s - loss: 0.0018 - val_loss: 0.0013 - 7s/epoch - 78us/sample
Epoch 3/90
84077/84077 - 7s - loss: 0.0024 - val_loss: 0.0012 - 7s/epoch - 78us/sample
Epoch 4/90
84077/84077 - 7s - loss: 0.0012 - val_loss: 0.0011 - 7s/epoch - 78us/sample
Epoch 5/90
84077/84077 - 7s - loss: 0.0210 - val_loss: 0.0022 - 7s/epoch - 78us/sample
Epoch 6/90
84077/84077 - 7s - loss: 0.0014 - val_loss: 0.0012 - 7s/epoch - 78us/sample
Epoch 7/90
84077/84077 - 7s - loss: 0.0022 - val_loss: 0.0011 - 7s/epoch - 79us/sample
Epoch 8/90
84077/84077 - 7s - loss: 9.7319e-04 - val_loss: 8.1920e-04 - 7s/epoch - 79us/sample
Epoch 9/90
84077/84077 - 7s - loss: 0.0011 - val_loss: 7.9354e-04 - 7s/epoch - 79us/sample
Epoch 10/90
84077/84077 - 7s - loss: 7.0450e-04 - val_loss: 5.6752e-04 - 7s/epoch - 79us/sample
Epoch 11/90
84077/84077 - 7s - loss: 5.9647e-04 - val_loss: 5.1956e-04 - 7s/epoch - 78us/sample
Epoch 12/90
84077/84077 - 7s - loss: 5.9026e-04 - val_loss: 4.4083e-04 - 7s/epoch - 78us/sample
Epoch 13/90
84077/84077 - 7s - loss: 4.7139e-04 - val_loss: 3.9556e-04 - 7s/epoch - 79us/sample
Epoch 14/90
84077/84077 - 7s - loss: 6.0196e-04 - val_loss: 4.5251e-04 - 7s/epoch - 79us/sample
Epoch 15/90
84077/84077 - 7s - loss: 7.9030e-04 - val_loss: 3.5625e-04 - 7s/epoch - 78us/sample
Epoch 16/90
84077/84077 - 7s - loss: 4.3710e-04 - val_loss: 3.3198e-04 - 7s/epoch - 79us/sample
Epoch 17/90
84077/84077 - 7s - loss: 3.7491e-04 - val_loss: 3.2363e-04 - 7s/epoch - 78us/sample
Epoch 18/90
84077/84077 - 7s - loss: 3.8382e-04 - val_loss: 2.9615e-04 - 7s/epoch - 78us/sample
Epoch 19/90
84077/84077 - 7s - loss: 3.3098e-04 - val_loss: 2.9134e-04 - 7s/epoch - 78us/sample
Epoch 20/90
84077/84077 - 7s - loss: 3.1567e-04 - val_loss: 2.7800e-04 - 7s/epoch - 79us/sample
Epoch 21/90
84077/84077 - 7s - loss: 3.0468e-04 - val_loss: 2.5618e-04 - 7s/epoch - 79us/sample
Epoch 22/90
84077/84077 - 7s - loss: 2.8904e-04 - val_loss: 2.5002e-04 - 7s/epoch - 79us/sample
Epoch 23/90
84077/84077 - 7s - loss: 2.7955e-04 - val_loss: 2.4516e-04 - 7s/epoch - 79us/sample
Epoch 24/90
84077/84077 - 7s - loss: 2.7093e-04 - val_loss: 2.3536e-04 - 7s/epoch - 79us/sample
Epoch 25/90
84077/84077 - 7s - loss: 2.6204e-04 - val_loss: 2.2871e-04 - 7s/epoch - 78us/sample
Epoch 26/90
84077/84077 - 7s - loss: 2.5810e-04 - val_loss: 2.2471e-04 - 7s/epoch - 79us/sample
Epoch 27/90
84077/84077 - 7s - loss: 2.5051e-04 - val_loss: 2.2215e-04 - 7s/epoch - 79us/sample
Epoch 28/90
84077/84077 - 7s - loss: 2.4575e-04 - val_loss: 2.2157e-04 - 7s/epoch - 78us/sample
Epoch 29/90
84077/84077 - 7s - loss: 2.4084e-04 - val_loss: 2.1575e-04 - 7s/epoch - 79us/sample
Epoch 30/90
84077/84077 - 7s - loss: 2.3609e-04 - val_loss: 2.1317e-04 - 7s/epoch - 78us/sample
Epoch 31/90
84077/84077 - 7s - loss: 2.3225e-04 - val_loss: 2.0918e-04 - 7s/epoch - 78us/sample
Epoch 32/90
84077/84077 - 7s - loss: 2.2858e-04 - val_loss: 2.0768e-04 - 7s/epoch - 79us/sample
Epoch 33/90
84077/84077 - 7s - loss: 2.2484e-04 - val_loss: 2.0342e-04 - 7s/epoch - 79us/sample
Epoch 34/90
84077/84077 - 7s - loss: 2.2199e-04 - val_loss: 2.0156e-04 - 7s/epoch - 78us/sample
Epoch 35/90
84077/84077 - 7s - loss: 2.1965e-04 - val_loss: 2.0056e-04 - 7s/epoch - 79us/sample
Epoch 36/90
84077/84077 - 7s - loss: 2.1670e-04 - val_loss: 1.9587e-04 - 7s/epoch - 78us/sample
Epoch 37/90
84077/84077 - 7s - loss: 2.1554e-04 - val_loss: 1.9656e-04 - 7s/epoch - 78us/sample
Epoch 38/90
84077/84077 - 7s - loss: 2.1341e-04 - val_loss: 1.9330e-04 - 7s/epoch - 79us/sample
Epoch 39/90
84077/84077 - 7s - loss: 2.1123e-04 - val_loss: 1.9206e-04 - 7s/epoch - 79us/sample
Epoch 40/90
84077/84077 - 7s - loss: 2.0789e-04 - val_loss: 1.9097e-04 - 7s/epoch - 79us/sample
Epoch 41/90
84077/84077 - 7s - loss: 2.0649e-04 - val_loss: 1.9177e-04 - 7s/epoch - 79us/sample
Epoch 42/90
84077/84077 - 7s - loss: 2.0382e-04 - val_loss: 1.8966e-04 - 7s/epoch - 79us/sample
Epoch 43/90
84077/84077 - 7s - loss: 2.0330e-04 - val_loss: 1.8795e-04 - 7s/epoch - 78us/sample
Epoch 44/90
84077/84077 - 7s - loss: 2.0178e-04 - val_loss: 1.8648e-04 - 7s/epoch - 79us/sample
Epoch 45/90
84077/84077 - 7s - loss: 2.0030e-04 - val_loss: 1.8691e-04 - 7s/epoch - 79us/sample
Epoch 46/90
84077/84077 - 7s - loss: 2.0192e-04 - val_loss: 1.8498e-04 - 7s/epoch - 79us/sample
Epoch 47/90
84077/84077 - 7s - loss: 1.9852e-04 - val_loss: 1.8566e-04 - 7s/epoch - 79us/sample
Epoch 48/90
84077/84077 - 7s - loss: 1.9648e-04 - val_loss: 1.8270e-04 - 7s/epoch - 78us/sample
Epoch 49/90
84077/84077 - 7s - loss: 1.9528e-04 - val_loss: 1.8138e-04 - 7s/epoch - 79us/sample
Epoch 50/90
84077/84077 - 7s - loss: 1.9404e-04 - val_loss: 1.7944e-04 - 7s/epoch - 79us/sample
Epoch 51/90
84077/84077 - 7s - loss: 1.9328e-04 - val_loss: 1.7984e-04 - 7s/epoch - 79us/sample
Epoch 52/90
84077/84077 - 7s - loss: 1.9255e-04 - val_loss: 1.7888e-04 - 7s/epoch - 79us/sample
Epoch 53/90
84077/84077 - 7s - loss: 1.9254e-04 - val_loss: 1.8497e-04 - 7s/epoch - 79us/sample
Epoch 54/90
84077/84077 - 7s - loss: 1.9085e-04 - val_loss: 1.7671e-04 - 7s/epoch - 79us/sample
Epoch 55/90
84077/84077 - 7s - loss: 1.9086e-04 - val_loss: 1.8117e-04 - 7s/epoch - 79us/sample
Epoch 56/90
84077/84077 - 7s - loss: 1.8898e-04 - val_loss: 1.7550e-04 - 7s/epoch - 78us/sample
Epoch 57/90
84077/84077 - 7s - loss: 1.8825e-04 - val_loss: 1.7478e-04 - 7s/epoch - 80us/sample
Epoch 58/90
84077/84077 - 7s - loss: 1.8774e-04 - val_loss: 1.7382e-04 - 7s/epoch - 79us/sample
Epoch 59/90
84077/84077 - 7s - loss: 1.9589e-04 - val_loss: 1.7697e-04 - 7s/epoch - 79us/sample
Epoch 60/90
84077/84077 - 7s - loss: 1.9061e-04 - val_loss: 1.7612e-04 - 7s/epoch - 79us/sample
Epoch 61/90
84077/84077 - 7s - loss: 1.8566e-04 - val_loss: 1.7247e-04 - 7s/epoch - 79us/sample
Epoch 62/90
84077/84077 - 7s - loss: 1.8495e-04 - val_loss: 1.7246e-04 - 7s/epoch - 79us/sample
Epoch 63/90
84077/84077 - 7s - loss: 1.8315e-04 - val_loss: 1.7187e-04 - 7s/epoch - 79us/sample
Epoch 64/90
84077/84077 - 7s - loss: 1.8430e-04 - val_loss: 1.7221e-04 - 7s/epoch - 79us/sample
Epoch 65/90
84077/84077 - 7s - loss: 1.8283e-04 - val_loss: 1.7314e-04 - 7s/epoch - 78us/sample
Epoch 66/90
84077/84077 - 7s - loss: 1.8223e-04 - val_loss: 1.7135e-04 - 7s/epoch - 79us/sample
Epoch 67/90
84077/84077 - 7s - loss: 1.8104e-04 - val_loss: 1.7108e-04 - 7s/epoch - 79us/sample
Epoch 68/90
84077/84077 - 7s - loss: 1.8010e-04 - val_loss: 1.7026e-04 - 7s/epoch - 79us/sample
Epoch 69/90
84077/84077 - 7s - loss: 1.8078e-04 - val_loss: 1.6974e-04 - 7s/epoch - 79us/sample
Epoch 70/90
84077/84077 - 7s - loss: 1.8229e-04 - val_loss: 1.7514e-04 - 7s/epoch - 79us/sample
Epoch 71/90
84077/84077 - 7s - loss: 1.8043e-04 - val_loss: 1.6753e-04 - 7s/epoch - 79us/sample
Epoch 72/90
84077/84077 - 7s - loss: 1.7832e-04 - val_loss: 1.6851e-04 - 7s/epoch - 79us/sample
Epoch 73/90
84077/84077 - 7s - loss: 1.7744e-04 - val_loss: 1.6649e-04 - 7s/epoch - 79us/sample
Epoch 74/90
84077/84077 - 7s - loss: 1.7723e-04 - val_loss: 1.6851e-04 - 7s/epoch - 79us/sample
Epoch 75/90
84077/84077 - 7s - loss: 1.7693e-04 - val_loss: 1.6794e-04 - 7s/epoch - 79us/sample
Epoch 76/90
84077/84077 - 7s - loss: 1.7613e-04 - val_loss: 1.6689e-04 - 7s/epoch - 78us/sample
Epoch 77/90
84077/84077 - 7s - loss: 1.7655e-04 - val_loss: 1.6601e-04 - 7s/epoch - 79us/sample
Epoch 78/90
84077/84077 - 7s - loss: 1.7582e-04 - val_loss: 1.6588e-04 - 7s/epoch - 80us/sample
Epoch 79/90
84077/84077 - 7s - loss: 1.7631e-04 - val_loss: 1.6587e-04 - 7s/epoch - 79us/sample
Epoch 80/90
84077/84077 - 7s - loss: 1.7408e-04 - val_loss: 1.6513e-04 - 7s/epoch - 79us/sample
Epoch 81/90
84077/84077 - 7s - loss: 1.7516e-04 - val_loss: 1.6454e-04 - 7s/epoch - 79us/sample
Epoch 82/90
84077/84077 - 7s - loss: 1.7303e-04 - val_loss: 1.6477e-04 - 7s/epoch - 79us/sample
Epoch 83/90
84077/84077 - 7s - loss: 1.7335e-04 - val_loss: 1.6378e-04 - 7s/epoch - 78us/sample
Epoch 84/90
84077/84077 - 7s - loss: 1.7305e-04 - val_loss: 1.6276e-04 - 7s/epoch - 79us/sample
Epoch 85/90
84077/84077 - 7s - loss: 1.7338e-04 - val_loss: 1.6237e-04 - 7s/epoch - 79us/sample
Epoch 86/90
84077/84077 - 7s - loss: 1.7297e-04 - val_loss: 1.6301e-04 - 7s/epoch - 79us/sample
Epoch 87/90
84077/84077 - 7s - loss: 1.7225e-04 - val_loss: 1.6378e-04 - 7s/epoch - 79us/sample
Epoch 88/90
84077/84077 - 7s - loss: 1.7331e-04 - val_loss: 1.6395e-04 - 7s/epoch - 79us/sample
Epoch 89/90
84077/84077 - 7s - loss: 1.7300e-04 - val_loss: 1.6353e-04 - 7s/epoch - 78us/sample
Epoch 90/90
84077/84077 - 7s - loss: 1.7157e-04 - val_loss: 1.6409e-04 - 7s/epoch - 79us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00016409312492464092
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:25:13.907933: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1682 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.021826927970830568
cosine 0.021567074102029557
MAE: 0.002566823365839086
RMSE: 0.008938988383877468
r2: 0.9377773225888217
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.001, 0.3, 282, 0.00017157138599112453, 0.00016409312492464092, 0.021826927970830568, 0.021567074102029557, 0.002566823365839086, 0.008938988383877468, 0.9377773225888217, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 70 0.0005 16 0] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1414)        5656        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1414)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          399030      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          399030      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1821097     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,959,629
Trainable params: 3,953,409
Non-trainable params: 6,220
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/70
2023-02-14 18:25:19.873808: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_2/bias/Assign' id:2853 op device:{requested: '', assigned: ''} def:{{{node outputlayer_2/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_2/bias, outputlayer_2/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:25:39.873304: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2988 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0678 - val_loss: 0.0674 - 21s/epoch - 254us/sample
Epoch 2/70
84077/84077 - 20s - loss: 0.0669 - val_loss: 0.0674 - 20s/epoch - 234us/sample
Epoch 3/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 4/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 5/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 6/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 7/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 8/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 9/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 10/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 11/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 12/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 13/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 14/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 15/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 16/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 17/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 18/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 19/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 20/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 21/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 22/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 23/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 24/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 25/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 26/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 27/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 28/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 29/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 30/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 31/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 32/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 33/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 34/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 35/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 36/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 37/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 38/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 39/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 40/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 41/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 42/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 43/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 44/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 45/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 46/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 47/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 48/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 49/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 50/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 51/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 52/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 53/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 54/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 55/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 56/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 57/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 58/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 59/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 60/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 61/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 62/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 63/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 64/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 65/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
Epoch 66/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 67/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 68/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 69/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 235us/sample
Epoch 70/70
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 234us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.06734573017080854
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:48:21.865942: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2940 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1122090065966084
cosine 1.1421539329671684
MAE: 4.553723537327303
RMSE: 4.865580840642783
r2: -17442.424016645415
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'binary_crossentropy', 16, 70, 0.0005, 0.3, 282, 0.06683691131921815, 0.06734573017080854, 1.1122090065966084, 1.1421539329671684, 4.553723537327303, 4.865580840642783, -17442.424016645415, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2357)         2225008     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 2357)        9428        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 2357)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          664956      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          664956      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2980987     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,545,335
Trainable params: 6,535,343
Non-trainable params: 9,992
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/50
2023-02-14 18:48:28.011629: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/bottleneck_zlog_3/bias/v/Assign' id:4954 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/bottleneck_zlog_3/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/bottleneck_zlog_3/bias/v, training_6/Adam/bottleneck_zlog_3/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:48:35.408846: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4316 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 8s - loss: 0.0272 - val_loss: 0.0010 - 8s/epoch - 100us/sample
Epoch 2/50
84077/84077 - 7s - loss: 0.0012 - val_loss: 0.0012 - 7s/epoch - 81us/sample
Epoch 3/50
84077/84077 - 7s - loss: 0.0180 - val_loss: 8.3823e-04 - 7s/epoch - 82us/sample
Epoch 4/50
84077/84077 - 7s - loss: 0.6496 - val_loss: 8.9925e-04 - 7s/epoch - 81us/sample
Epoch 5/50
84077/84077 - 7s - loss: 9.5022e-04 - val_loss: 6.9326e-04 - 7s/epoch - 81us/sample
Epoch 6/50
84077/84077 - 7s - loss: 4.9636e-04 - val_loss: 3.9515e-04 - 7s/epoch - 81us/sample
Epoch 7/50
84077/84077 - 7s - loss: 4.2558e-04 - val_loss: 3.6902e-04 - 7s/epoch - 81us/sample
Epoch 8/50
84077/84077 - 7s - loss: 5.6303e-04 - val_loss: 4.1131e-04 - 7s/epoch - 82us/sample
Epoch 9/50
84077/84077 - 7s - loss: 4.0395e-04 - val_loss: 2.9422e-04 - 7s/epoch - 82us/sample
Epoch 10/50
84077/84077 - 7s - loss: 3.2752e-04 - val_loss: 3.0563e-04 - 7s/epoch - 81us/sample
Epoch 11/50
84077/84077 - 7s - loss: 2.9407e-04 - val_loss: 2.5071e-04 - 7s/epoch - 81us/sample
Epoch 12/50
84077/84077 - 7s - loss: 2.7236e-04 - val_loss: 2.2910e-04 - 7s/epoch - 82us/sample
Epoch 13/50
84077/84077 - 7s - loss: 2.5110e-04 - val_loss: 2.1651e-04 - 7s/epoch - 82us/sample
Epoch 14/50
84077/84077 - 7s - loss: 2.4044e-04 - val_loss: 2.1574e-04 - 7s/epoch - 81us/sample
Epoch 15/50
84077/84077 - 7s - loss: 2.2774e-04 - val_loss: 1.9571e-04 - 7s/epoch - 81us/sample
Epoch 16/50
84077/84077 - 7s - loss: 2.1398e-04 - val_loss: 1.8717e-04 - 7s/epoch - 82us/sample
Epoch 17/50
84077/84077 - 7s - loss: 2.0401e-04 - val_loss: 1.9025e-04 - 7s/epoch - 82us/sample
Epoch 18/50
84077/84077 - 7s - loss: 1.9192e-04 - val_loss: 1.6769e-04 - 7s/epoch - 81us/sample
Epoch 19/50
84077/84077 - 7s - loss: 1.8419e-04 - val_loss: 1.5901e-04 - 7s/epoch - 82us/sample
Epoch 20/50
84077/84077 - 7s - loss: 1.7728e-04 - val_loss: 1.5107e-04 - 7s/epoch - 82us/sample
Epoch 21/50
84077/84077 - 7s - loss: 1.7048e-04 - val_loss: 1.4839e-04 - 7s/epoch - 82us/sample
Epoch 22/50
84077/84077 - 7s - loss: 1.6485e-04 - val_loss: 1.4445e-04 - 7s/epoch - 81us/sample
Epoch 23/50
84077/84077 - 7s - loss: 1.6192e-04 - val_loss: 1.4209e-04 - 7s/epoch - 82us/sample
Epoch 24/50
84077/84077 - 7s - loss: 1.5779e-04 - val_loss: 1.4437e-04 - 7s/epoch - 82us/sample
Epoch 25/50
84077/84077 - 7s - loss: 1.5335e-04 - val_loss: 1.3510e-04 - 7s/epoch - 81us/sample
Epoch 26/50
84077/84077 - 7s - loss: 1.5063e-04 - val_loss: 1.3235e-04 - 7s/epoch - 82us/sample
Epoch 27/50
84077/84077 - 7s - loss: 1.4682e-04 - val_loss: 1.3027e-04 - 7s/epoch - 82us/sample
Epoch 28/50
84077/84077 - 7s - loss: 1.4469e-04 - val_loss: 1.2698e-04 - 7s/epoch - 82us/sample
Epoch 29/50
84077/84077 - 7s - loss: 1.4204e-04 - val_loss: 1.2459e-04 - 7s/epoch - 82us/sample
Epoch 30/50
84077/84077 - 7s - loss: 1.3988e-04 - val_loss: 1.2252e-04 - 7s/epoch - 82us/sample
Epoch 31/50
84077/84077 - 7s - loss: 1.3733e-04 - val_loss: 1.2145e-04 - 7s/epoch - 82us/sample
Epoch 32/50
84077/84077 - 7s - loss: 1.3606e-04 - val_loss: 1.2149e-04 - 7s/epoch - 82us/sample
Epoch 33/50
84077/84077 - 7s - loss: 1.3471e-04 - val_loss: 1.1933e-04 - 7s/epoch - 81us/sample
Epoch 34/50
84077/84077 - 7s - loss: 1.3302e-04 - val_loss: 1.1894e-04 - 7s/epoch - 82us/sample
Epoch 35/50
84077/84077 - 7s - loss: 1.3194e-04 - val_loss: 1.1754e-04 - 7s/epoch - 82us/sample
Epoch 36/50
84077/84077 - 7s - loss: 1.3076e-04 - val_loss: 1.1658e-04 - 7s/epoch - 82us/sample
Epoch 37/50
84077/84077 - 7s - loss: 1.2937e-04 - val_loss: 1.1743e-04 - 7s/epoch - 82us/sample
Epoch 38/50
84077/84077 - 7s - loss: 1.2829e-04 - val_loss: 1.1554e-04 - 7s/epoch - 82us/sample
Epoch 39/50
84077/84077 - 7s - loss: 1.2831e-04 - val_loss: 1.1467e-04 - 7s/epoch - 82us/sample
Epoch 40/50
84077/84077 - 7s - loss: 1.2633e-04 - val_loss: 1.1460e-04 - 7s/epoch - 82us/sample
Epoch 41/50
84077/84077 - 7s - loss: 1.2606e-04 - val_loss: 1.1526e-04 - 7s/epoch - 83us/sample
Epoch 42/50
84077/84077 - 7s - loss: 1.2534e-04 - val_loss: 1.1294e-04 - 7s/epoch - 82us/sample
Epoch 43/50
84077/84077 - 7s - loss: 1.2409e-04 - val_loss: 1.1214e-04 - 7s/epoch - 82us/sample
Epoch 44/50
84077/84077 - 7s - loss: 1.2369e-04 - val_loss: 1.1197e-04 - 7s/epoch - 82us/sample
Epoch 45/50
84077/84077 - 7s - loss: 1.2343e-04 - val_loss: 1.1234e-04 - 7s/epoch - 81us/sample
Epoch 46/50
84077/84077 - 7s - loss: 1.2271e-04 - val_loss: 1.1036e-04 - 7s/epoch - 81us/sample
Epoch 47/50
84077/84077 - 7s - loss: 1.2232e-04 - val_loss: 1.1156e-04 - 7s/epoch - 82us/sample
Epoch 48/50
84077/84077 - 7s - loss: 1.2131e-04 - val_loss: 1.1049e-04 - 7s/epoch - 82us/sample
Epoch 49/50
84077/84077 - 7s - loss: 1.2109e-04 - val_loss: 1.1041e-04 - 7s/epoch - 82us/sample
Epoch 50/50
84077/84077 - 7s - loss: 1.2068e-04 - val_loss: 1.1003e-04 - 7s/epoch - 81us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00011002875314293212
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:54:12.653539: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4280 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.028521615208407257
cosine 0.028174453618723033
MAE: 0.0025066581601052855
RMSE: 0.009947497540716329
r2: 0.9228250679747505
RMSE zero-vector: 0.04004287452915337
['2.5custom_VAE', 'logcosh', 64, 50, 0.0005, 0.3, 282, 0.00012068321403982659, 0.00011002875314293212, 0.028521615208407257, 0.028174453618723033, 0.0025066581601052855, 0.009947497540716329, 0.9228250679747505, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 110 0.0005 8 0] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 1886)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/110
2023-02-14 18:54:19.362681: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_4/kernel/Assign' id:5476 op device:{requested: '', assigned: ''} def:{{{node outputlayer_4/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_4/kernel, outputlayer_4/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:54:56.498121: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5616 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 40s - loss: 0.0679 - val_loss: 0.0674 - 40s/epoch - 471us/sample
Epoch 2/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 3/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 4/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 449us/sample
Epoch 5/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 6/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 7/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 8/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 9/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 10/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 11/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 12/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 13/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 14/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 15/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 16/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 17/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 18/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 19/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 20/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 21/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 22/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 23/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 24/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 25/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 26/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 27/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 28/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 29/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 30/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 31/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 32/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 33/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 34/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 35/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 36/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 37/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 38/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 39/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 40/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 41/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 42/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 43/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 44/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 45/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 46/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 47/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 48/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 49/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 50/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 51/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 52/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 53/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 54/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 55/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 56/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 57/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 58/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 59/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 60/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 61/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 62/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 63/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 64/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 65/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 66/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 67/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 68/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 69/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 70/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 71/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 72/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 73/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 74/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 75/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 76/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 77/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 78/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 79/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 80/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 81/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 82/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 83/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 84/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 85/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 86/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 87/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 88/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 89/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 90/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 91/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 92/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 93/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 450us/sample
Epoch 94/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 95/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 96/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 97/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 98/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 99/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 100/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 101/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 102/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 103/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 104/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 452us/sample
Epoch 105/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 106/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 107/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 108/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 109/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
Epoch 110/110
84077/84077 - 38s - loss: 0.0668 - val_loss: 0.0673 - 38s/epoch - 451us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.06734573204103028
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:03:51.737099: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:5568 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.115999831845045
cosine 1.148878104916598
MAE: 5.792534818065063
RMSE: 6.396392150486637
r2: -29600.613186100534
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 8, 110, 0.0005, 0.3, 282, 0.06683690984818995, 0.06734573204103028, 1.115999831845045, 1.148878104916598, 5.792534818065063, 6.396392150486637, -29600.613186100534, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 170 0.0005 16 0] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 1886)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/170
2023-02-14 20:03:58.830175: W tensorflow/c/c_api.cc:291] Operation '{name:'training_10/Adam/dense_dec0_5/kernel/v/Assign' id:7657 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/dense_dec0_5/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_10/Adam/dense_dec0_5/kernel/v, training_10/Adam/dense_dec0_5/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:04:18.793804: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:6953 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0680 - val_loss: 0.0675 - 22s/epoch - 261us/sample
Epoch 2/170
84077/84077 - 20s - loss: 0.0669 - val_loss: 0.0674 - 20s/epoch - 236us/sample
Epoch 3/170
84077/84077 - 20s - loss: 0.0669 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 4/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 5/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 6/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 7/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 8/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 9/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 10/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 11/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 12/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 13/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 14/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 15/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 16/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 17/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 18/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 19/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 20/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 21/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 22/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 23/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 24/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 25/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 26/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 27/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 28/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 29/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 30/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 31/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 32/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 33/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 34/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 35/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 36/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 37/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 38/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 39/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 40/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 41/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 42/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 43/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 44/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 45/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 46/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 47/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 48/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 49/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 50/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 51/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 52/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 53/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 54/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 55/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 56/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 57/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 58/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 59/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 60/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 61/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 62/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 63/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 64/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 65/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 66/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 67/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 68/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 69/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 70/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 71/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 72/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 73/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 74/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 75/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 76/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 77/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 78/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 79/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 80/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 81/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 82/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 83/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 84/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 85/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 86/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 87/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 88/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 89/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 90/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 91/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 92/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 93/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 94/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 95/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 96/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 97/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 98/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 99/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 100/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 101/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 102/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 103/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 104/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 105/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 106/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 107/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 108/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 109/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 110/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 111/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 112/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 113/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 114/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 115/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 116/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 117/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 118/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 119/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 120/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 121/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 122/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 123/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 124/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 125/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 126/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 127/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 128/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 129/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 130/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 131/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 132/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 133/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 134/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 135/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 136/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 137/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 138/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 139/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 140/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 141/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 142/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 143/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 144/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 145/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 146/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 147/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 148/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 149/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 150/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 151/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 152/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 153/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 154/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 155/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 156/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 157/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 236us/sample
Epoch 158/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 159/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 160/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 238us/sample
Epoch 161/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 162/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 163/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 164/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 165/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 166/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 167/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 168/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 169/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
Epoch 170/170
84077/84077 - 20s - loss: 0.0668 - val_loss: 0.0673 - 20s/epoch - 237us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.06734573141655965
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:00:25.607804: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:6905 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.0928085546903765
cosine 1.1340365030740687
MAE: 5.5916117315450995
RMSE: 5.928691253248791
r2: -25825.696415918253
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'binary_crossentropy', 16, 170, 0.0005, 0.3, 282, 0.06683691059150226, 0.06734573141655965, 1.0928085546903765, 1.1340365030740687, 5.5916117315450995, 5.928691253248791, -25825.696415918253, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 25 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 1886)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 21:00:33.283898: W tensorflow/c/c_api.cc:291] Operation '{name:'training_12/Adam/dense_enc0_6/bias/v/Assign' id:8845 op device:{requested: '', assigned: ''} def:{{{node training_12/Adam/dense_enc0_6/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_12/Adam/dense_enc0_6/bias/v, training_12/Adam/dense_enc0_6/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:00:52.681239: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:8271 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0099 - val_loss: 0.0053 - 22s/epoch - 257us/sample
Epoch 2/25
84077/84077 - 19s - loss: 0.0017 - val_loss: 0.0012 - 19s/epoch - 226us/sample
Epoch 3/25
84077/84077 - 19s - loss: 0.0016 - val_loss: 0.0011 - 19s/epoch - 227us/sample
Epoch 4/25
84077/84077 - 19s - loss: 0.0018 - val_loss: 0.0011 - 19s/epoch - 227us/sample
Epoch 5/25
84077/84077 - 19s - loss: 0.0011 - val_loss: 7.9181e-04 - 19s/epoch - 226us/sample
Epoch 6/25
84077/84077 - 19s - loss: 8.3616e-04 - val_loss: 7.2670e-04 - 19s/epoch - 227us/sample
Epoch 7/25
84077/84077 - 19s - loss: 7.8204e-04 - val_loss: 6.9007e-04 - 19s/epoch - 226us/sample
Epoch 8/25
84077/84077 - 19s - loss: 7.1830e-04 - val_loss: 6.7102e-04 - 19s/epoch - 226us/sample
Epoch 9/25
84077/84077 - 19s - loss: 6.9080e-04 - val_loss: 6.9485e-04 - 19s/epoch - 227us/sample
Epoch 10/25
84077/84077 - 19s - loss: 6.6741e-04 - val_loss: 7.1046e-04 - 19s/epoch - 226us/sample
Epoch 11/25
84077/84077 - 19s - loss: 6.5478e-04 - val_loss: 6.4976e-04 - 19s/epoch - 227us/sample
Epoch 12/25
84077/84077 - 19s - loss: 6.4121e-04 - val_loss: 6.6391e-04 - 19s/epoch - 226us/sample
Epoch 13/25
84077/84077 - 19s - loss: 6.2797e-04 - val_loss: 6.9705e-04 - 19s/epoch - 228us/sample
Epoch 14/25
84077/84077 - 19s - loss: 6.2167e-04 - val_loss: 6.9150e-04 - 19s/epoch - 227us/sample
Epoch 15/25
84077/84077 - 19s - loss: 6.0736e-04 - val_loss: 6.5548e-04 - 19s/epoch - 227us/sample
Epoch 16/25
84077/84077 - 19s - loss: 6.0074e-04 - val_loss: 6.3209e-04 - 19s/epoch - 227us/sample
Epoch 17/25
84077/84077 - 19s - loss: 5.9333e-04 - val_loss: 6.1655e-04 - 19s/epoch - 227us/sample
Epoch 18/25
84077/84077 - 19s - loss: 5.7760e-04 - val_loss: 6.1394e-04 - 19s/epoch - 227us/sample
Epoch 19/25
84077/84077 - 19s - loss: 5.6410e-04 - val_loss: 6.2422e-04 - 19s/epoch - 227us/sample
Epoch 20/25
84077/84077 - 19s - loss: 5.5608e-04 - val_loss: 6.4609e-04 - 19s/epoch - 228us/sample
Epoch 21/25
84077/84077 - 19s - loss: 5.4792e-04 - val_loss: 6.0561e-04 - 19s/epoch - 226us/sample
Epoch 22/25
84077/84077 - 19s - loss: 5.4222e-04 - val_loss: 6.0238e-04 - 19s/epoch - 228us/sample
Epoch 23/25
84077/84077 - 19s - loss: 5.3516e-04 - val_loss: 5.9931e-04 - 19s/epoch - 227us/sample
Epoch 24/25
84077/84077 - 19s - loss: 5.3018e-04 - val_loss: 6.1302e-04 - 19s/epoch - 227us/sample
Epoch 25/25
84077/84077 - 19s - loss: 5.2627e-04 - val_loss: 6.8197e-04 - 19s/epoch - 227us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0006819714306223858
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:08:32.130469: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:8242 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.11766798124176547
cosine 0.11610440621320842
MAE: 0.004548558418606294
RMSE: 0.024894478059627217
r2: 0.5158919464198023
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 16, 25, 0.001, 0.3, 282, 0.0005262659104227913, 0.0006819714306223858, 0.11766798124176547, 0.11610440621320842, 0.004548558418606294, 0.024894478059627217, 0.5158919464198023, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 1414)         0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          399030      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          399030      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1821097     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,959,629
Trainable params: 3,953,409
Non-trainable params: 6,220
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 21:08:40.133261: W tensorflow/c/c_api.cc:291] Operation '{name:'training_14/Adam/outputlayer_7/bias/v/Assign' id:10200 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/outputlayer_7/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/outputlayer_7/bias/v, training_14/Adam/outputlayer_7/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:08:51.759110: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:9526 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0056 - val_loss: 0.0052 - 14s/epoch - 162us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0014 - val_loss: 0.0011 - 11s/epoch - 129us/sample
Epoch 3/90
84077/84077 - 11s - loss: 9.5514e-04 - val_loss: 6.8739e-04 - 11s/epoch - 129us/sample
Epoch 4/90
84077/84077 - 11s - loss: 6.7494e-04 - val_loss: 5.3487e-04 - 11s/epoch - 130us/sample
Epoch 5/90
84077/84077 - 11s - loss: 5.2823e-04 - val_loss: 4.2447e-04 - 11s/epoch - 129us/sample
Epoch 6/90
84077/84077 - 11s - loss: 4.3357e-04 - val_loss: 3.6571e-04 - 11s/epoch - 129us/sample
Epoch 7/90
84077/84077 - 11s - loss: 3.8804e-04 - val_loss: 3.5995e-04 - 11s/epoch - 129us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.4678e-04 - val_loss: 3.1911e-04 - 11s/epoch - 130us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.2265e-04 - val_loss: 2.9016e-04 - 11s/epoch - 129us/sample
Epoch 10/90
84077/84077 - 11s - loss: 3.0579e-04 - val_loss: 2.9175e-04 - 11s/epoch - 129us/sample
Epoch 11/90
84077/84077 - 11s - loss: 2.8932e-04 - val_loss: 2.6368e-04 - 11s/epoch - 129us/sample
Epoch 12/90
84077/84077 - 11s - loss: 2.7632e-04 - val_loss: 2.5988e-04 - 11s/epoch - 130us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.6824e-04 - val_loss: 2.4764e-04 - 11s/epoch - 129us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.6101e-04 - val_loss: 2.4323e-04 - 11s/epoch - 129us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.5313e-04 - val_loss: 2.3565e-04 - 11s/epoch - 129us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.4681e-04 - val_loss: 2.3762e-04 - 11s/epoch - 129us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.4218e-04 - val_loss: 2.3138e-04 - 11s/epoch - 130us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.3765e-04 - val_loss: 2.3097e-04 - 11s/epoch - 130us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.3334e-04 - val_loss: 2.2084e-04 - 11s/epoch - 129us/sample
Epoch 20/90
84077/84077 - 11s - loss: 2.3030e-04 - val_loss: 2.2116e-04 - 11s/epoch - 129us/sample
Epoch 21/90
84077/84077 - 11s - loss: 2.2664e-04 - val_loss: 2.2685e-04 - 11s/epoch - 130us/sample
Epoch 22/90
84077/84077 - 11s - loss: 2.2614e-04 - val_loss: 2.1994e-04 - 11s/epoch - 129us/sample
Epoch 23/90
84077/84077 - 11s - loss: 2.2210e-04 - val_loss: 2.1433e-04 - 11s/epoch - 130us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.1948e-04 - val_loss: 2.1517e-04 - 11s/epoch - 129us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.1826e-04 - val_loss: 2.1686e-04 - 11s/epoch - 129us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.1650e-04 - val_loss: 2.1359e-04 - 11s/epoch - 130us/sample
Epoch 27/90
84077/84077 - 11s - loss: 2.1275e-04 - val_loss: 2.1047e-04 - 11s/epoch - 129us/sample
Epoch 28/90
84077/84077 - 11s - loss: 2.1176e-04 - val_loss: 2.0790e-04 - 11s/epoch - 129us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.0892e-04 - val_loss: 2.0644e-04 - 11s/epoch - 129us/sample
Epoch 30/90
84077/84077 - 11s - loss: 2.0916e-04 - val_loss: 2.0364e-04 - 11s/epoch - 129us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.0512e-04 - val_loss: 2.0637e-04 - 11s/epoch - 130us/sample
Epoch 32/90
84077/84077 - 11s - loss: 2.0398e-04 - val_loss: 2.0426e-04 - 11s/epoch - 129us/sample
Epoch 33/90
84077/84077 - 11s - loss: 2.0205e-04 - val_loss: 1.9741e-04 - 11s/epoch - 129us/sample
Epoch 34/90
84077/84077 - 11s - loss: 2.0288e-04 - val_loss: 1.9876e-04 - 11s/epoch - 130us/sample
Epoch 35/90
84077/84077 - 11s - loss: 1.9983e-04 - val_loss: 1.9743e-04 - 11s/epoch - 129us/sample
Epoch 36/90
84077/84077 - 11s - loss: 2.0143e-04 - val_loss: 1.9881e-04 - 11s/epoch - 129us/sample
Epoch 37/90
84077/84077 - 11s - loss: 1.9943e-04 - val_loss: 1.9911e-04 - 11s/epoch - 130us/sample
Epoch 38/90
84077/84077 - 11s - loss: 1.9971e-04 - val_loss: 1.9607e-04 - 11s/epoch - 129us/sample
Epoch 39/90
84077/84077 - 11s - loss: 1.9764e-04 - val_loss: 1.9619e-04 - 11s/epoch - 130us/sample
Epoch 40/90
84077/84077 - 11s - loss: 1.9799e-04 - val_loss: 1.9482e-04 - 11s/epoch - 129us/sample
Epoch 41/90
84077/84077 - 11s - loss: 1.9457e-04 - val_loss: 1.8881e-04 - 11s/epoch - 129us/sample
Epoch 42/90
84077/84077 - 11s - loss: 1.9510e-04 - val_loss: 1.8880e-04 - 11s/epoch - 130us/sample
Epoch 43/90
84077/84077 - 11s - loss: 1.9431e-04 - val_loss: 1.8937e-04 - 11s/epoch - 129us/sample
Epoch 44/90
84077/84077 - 11s - loss: 1.9272e-04 - val_loss: 1.8886e-04 - 11s/epoch - 129us/sample
Epoch 45/90
84077/84077 - 11s - loss: 1.9158e-04 - val_loss: 1.8603e-04 - 11s/epoch - 130us/sample
Epoch 46/90
84077/84077 - 11s - loss: 1.9002e-04 - val_loss: 1.9075e-04 - 11s/epoch - 130us/sample
Epoch 47/90
84077/84077 - 11s - loss: 1.9131e-04 - val_loss: 1.8890e-04 - 11s/epoch - 129us/sample
Epoch 48/90
84077/84077 - 11s - loss: 1.8967e-04 - val_loss: 1.8589e-04 - 11s/epoch - 129us/sample
Epoch 49/90
84077/84077 - 11s - loss: 1.8831e-04 - val_loss: 1.8744e-04 - 11s/epoch - 130us/sample
Epoch 50/90
84077/84077 - 11s - loss: 1.8783e-04 - val_loss: 1.8422e-04 - 11s/epoch - 129us/sample
Epoch 51/90
84077/84077 - 11s - loss: 1.8830e-04 - val_loss: 1.8426e-04 - 11s/epoch - 130us/sample
Epoch 52/90
84077/84077 - 11s - loss: 1.8752e-04 - val_loss: 1.8426e-04 - 11s/epoch - 129us/sample
Epoch 53/90
84077/84077 - 11s - loss: 1.8560e-04 - val_loss: 1.9023e-04 - 11s/epoch - 129us/sample
Epoch 54/90
84077/84077 - 11s - loss: 1.8702e-04 - val_loss: 1.8550e-04 - 11s/epoch - 130us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.8470e-04 - val_loss: 1.8633e-04 - 11s/epoch - 129us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.8756e-04 - val_loss: 1.8543e-04 - 11s/epoch - 130us/sample
Epoch 57/90
84077/84077 - 11s - loss: 1.8489e-04 - val_loss: 1.8670e-04 - 11s/epoch - 129us/sample
Epoch 58/90
84077/84077 - 11s - loss: 1.8396e-04 - val_loss: 1.8531e-04 - 11s/epoch - 129us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.8309e-04 - val_loss: 1.8186e-04 - 11s/epoch - 130us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.8215e-04 - val_loss: 1.8337e-04 - 11s/epoch - 130us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.8187e-04 - val_loss: 1.7959e-04 - 11s/epoch - 129us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.8222e-04 - val_loss: 1.7837e-04 - 11s/epoch - 129us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.8145e-04 - val_loss: 1.7877e-04 - 11s/epoch - 130us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.8123e-04 - val_loss: 1.7908e-04 - 11s/epoch - 129us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.7947e-04 - val_loss: 1.8221e-04 - 11s/epoch - 129us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.8060e-04 - val_loss: 1.7577e-04 - 11s/epoch - 129us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.8165e-04 - val_loss: 1.7907e-04 - 11s/epoch - 130us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.8014e-04 - val_loss: 1.7852e-04 - 11s/epoch - 129us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.7784e-04 - val_loss: 1.7611e-04 - 11s/epoch - 130us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.7871e-04 - val_loss: 1.7773e-04 - 11s/epoch - 129us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.7815e-04 - val_loss: 1.7336e-04 - 11s/epoch - 130us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.8175e-04 - val_loss: 1.7263e-04 - 11s/epoch - 130us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.7874e-04 - val_loss: 1.7692e-04 - 11s/epoch - 129us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.7901e-04 - val_loss: 1.7727e-04 - 11s/epoch - 129us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.7660e-04 - val_loss: 1.7326e-04 - 11s/epoch - 130us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.7854e-04 - val_loss: 1.7466e-04 - 11s/epoch - 130us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.7628e-04 - val_loss: 1.7489e-04 - 11s/epoch - 129us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.7551e-04 - val_loss: 1.7090e-04 - 11s/epoch - 130us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.7512e-04 - val_loss: 1.7552e-04 - 11s/epoch - 130us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.7601e-04 - val_loss: 1.7322e-04 - 11s/epoch - 129us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.7556e-04 - val_loss: 1.7390e-04 - 11s/epoch - 130us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.7468e-04 - val_loss: 1.7487e-04 - 11s/epoch - 130us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.7599e-04 - val_loss: 1.7060e-04 - 11s/epoch - 129us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.7416e-04 - val_loss: 1.7507e-04 - 11s/epoch - 129us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.7367e-04 - val_loss: 1.7180e-04 - 11s/epoch - 130us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.7289e-04 - val_loss: 1.7085e-04 - 11s/epoch - 129us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.7340e-04 - val_loss: 1.7359e-04 - 11s/epoch - 129us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.7252e-04 - val_loss: 1.7482e-04 - 11s/epoch - 129us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.7333e-04 - val_loss: 1.7582e-04 - 11s/epoch - 129us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.7174e-04 - val_loss: 1.7018e-04 - 11s/epoch - 130us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00017018002158516765
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:25:01.094269: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:9497 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.019527116600056824
cosine 0.019290341759356712
MAE: 0.00192293338162092
RMSE: 0.009205361341217274
r2: 0.933950234409846
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 32, 90, 0.001, 0.3, 282, 0.00017173970213863542, 0.00017018002158516765, 0.019527116600056824, 0.019290341759356712, 0.00192293338162092, 0.009205361341217274, 0.933950234409846, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 25 0.001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 471)          444624      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 471)         1884        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 471)          0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          133104      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          133104      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          661207      ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,373,923
Trainable params: 1,371,475
Non-trainable params: 2,448
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/25
2023-02-14 21:25:09.577244: W tensorflow/c/c_api.cc:291] Operation '{name:'training_16/Adam/outputlayer_8/bias/m/Assign' id:11394 op device:{requested: '', assigned: ''} def:{{{node training_16/Adam/outputlayer_8/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_16/Adam/outputlayer_8/bias/m, training_16/Adam/outputlayer_8/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:25:17.992967: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/mul' id:10780 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 11s - loss: 0.0651 - val_loss: 1.1038 - 11s/epoch - 125us/sample
Epoch 2/25
84077/84077 - 7s - loss: 0.0637 - val_loss: 0.0618 - 7s/epoch - 89us/sample
Epoch 3/25
84077/84077 - 7s - loss: 0.0578 - val_loss: 0.4038 - 7s/epoch - 89us/sample
Epoch 4/25
84077/84077 - 7s - loss: 0.0545 - val_loss: 0.0521 - 7s/epoch - 89us/sample
Epoch 5/25
84077/84077 - 8s - loss: 0.0546 - val_loss: 0.0613 - 8s/epoch - 89us/sample
Epoch 6/25
84077/84077 - 8s - loss: 0.0531 - val_loss: 0.0571 - 8s/epoch - 90us/sample
Epoch 7/25
84077/84077 - 8s - loss: 0.0528 - val_loss: 0.0533 - 8s/epoch - 89us/sample
Epoch 8/25
84077/84077 - 7s - loss: 0.0498 - val_loss: 0.0554 - 7s/epoch - 89us/sample
Epoch 9/25
84077/84077 - 8s - loss: 0.0482 - val_loss: 0.0484 - 8s/epoch - 90us/sample
Epoch 10/25
84077/84077 - 7s - loss: 0.0476 - val_loss: 0.0475 - 7s/epoch - 89us/sample
Epoch 11/25
84077/84077 - 7s - loss: 0.0486 - val_loss: 0.0474 - 7s/epoch - 89us/sample
Epoch 12/25
84077/84077 - 8s - loss: 0.0466 - val_loss: 0.0470 - 8s/epoch - 90us/sample
Epoch 13/25
84077/84077 - 8s - loss: 0.0463 - val_loss: 0.0465 - 8s/epoch - 89us/sample
Epoch 14/25
84077/84077 - 8s - loss: 0.0458 - val_loss: 0.0454 - 8s/epoch - 90us/sample
Epoch 15/25
84077/84077 - 8s - loss: 0.0461 - val_loss: 0.0467 - 8s/epoch - 89us/sample
Epoch 16/25
84077/84077 - 8s - loss: 0.0463 - val_loss: 0.0447 - 8s/epoch - 89us/sample
Epoch 17/25
84077/84077 - 7s - loss: 0.0458 - val_loss: 0.0452 - 7s/epoch - 89us/sample
Epoch 18/25
84077/84077 - 7s - loss: 0.0448 - val_loss: 0.0443 - 7s/epoch - 89us/sample
Epoch 19/25
84077/84077 - 8s - loss: 0.0444 - val_loss: 0.0438 - 8s/epoch - 90us/sample
Epoch 20/25
84077/84077 - 8s - loss: 0.0440 - val_loss: 0.0439 - 8s/epoch - 89us/sample
Epoch 21/25
84077/84077 - 8s - loss: 0.0446 - val_loss: 0.0440 - 8s/epoch - 90us/sample
Epoch 22/25
84077/84077 - 7s - loss: 0.0438 - val_loss: 0.0438 - 7s/epoch - 89us/sample
Epoch 23/25
84077/84077 - 7s - loss: 0.0433 - val_loss: 0.0430 - 7s/epoch - 89us/sample
Epoch 24/25
84077/84077 - 8s - loss: 0.0431 - val_loss: 0.0431 - 8s/epoch - 90us/sample
Epoch 25/25
84077/84077 - 8s - loss: 0.0431 - val_loss: 0.0428 - 8s/epoch - 89us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.042846645820934545
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:28:19.529413: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_8/outputlayer/BiasAdd' id:10732 op device:{requested: '', assigned: ''} def:{{{node decoder_model_8/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_8/outputlayer/MatMul, decoder_model_8/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.6660572761058153
cosine 1.000775693785664
MAE: 0.8605995925587174
RMSE: 4.016415526104431
r2: -11947.065106836008
RMSE zero-vector: 0.04004287452915337
['0.5custom_VAE', 'binary_crossentropy', 64, 25, 0.001, 0.3, 282, 0.043141223800394736, 0.042846645820934545, 0.6660572761058153, 1.000775693785664, 0.8605995925587174, 4.016415526104431, -11947.065106836008, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 1414)         0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          399030      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          399030      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1821097     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,959,629
Trainable params: 3,953,409
Non-trainable params: 6,220
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/30
2023-02-14 21:28:28.238539: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/outputlayer_9/bias/m/Assign' id:12637 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/outputlayer_9/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/outputlayer_9/bias/m, training_18/Adam/outputlayer_9/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:29:05.284421: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_9/mul' id:12077 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 41s - loss: 0.0037 - val_loss: 0.0011 - 41s/epoch - 484us/sample
Epoch 2/30
84077/84077 - 37s - loss: 0.0011 - val_loss: 0.0013 - 37s/epoch - 440us/sample
Epoch 3/30
84077/84077 - 37s - loss: 8.9299e-04 - val_loss: 0.0137 - 37s/epoch - 441us/sample
Epoch 4/30
84077/84077 - 37s - loss: 8.2211e-04 - val_loss: 0.0319 - 37s/epoch - 441us/sample
Epoch 5/30
84077/84077 - 37s - loss: 7.9140e-04 - val_loss: 0.0548 - 37s/epoch - 440us/sample
Epoch 6/30
84077/84077 - 37s - loss: 7.7262e-04 - val_loss: 0.0710 - 37s/epoch - 441us/sample
Epoch 7/30
84077/84077 - 37s - loss: 7.5728e-04 - val_loss: 0.0919 - 37s/epoch - 441us/sample
Epoch 8/30
84077/84077 - 37s - loss: 7.4410e-04 - val_loss: 0.0828 - 37s/epoch - 440us/sample
Epoch 9/30
84077/84077 - 37s - loss: 7.3143e-04 - val_loss: 0.1195 - 37s/epoch - 441us/sample
Epoch 10/30
84077/84077 - 37s - loss: 7.1943e-04 - val_loss: 0.1198 - 37s/epoch - 440us/sample
Epoch 11/30
84077/84077 - 37s - loss: 7.1338e-04 - val_loss: 0.1243 - 37s/epoch - 441us/sample
Epoch 12/30
84077/84077 - 37s - loss: 7.0965e-04 - val_loss: 0.1072 - 37s/epoch - 440us/sample
Epoch 13/30
84077/84077 - 37s - loss: 7.0510e-04 - val_loss: 0.1050 - 37s/epoch - 441us/sample
Epoch 14/30
84077/84077 - 37s - loss: 6.9990e-04 - val_loss: 0.1169 - 37s/epoch - 440us/sample
Epoch 15/30
84077/84077 - 37s - loss: 6.9741e-04 - val_loss: 0.1283 - 37s/epoch - 441us/sample
Epoch 16/30
84077/84077 - 37s - loss: 6.9433e-04 - val_loss: 0.1282 - 37s/epoch - 441us/sample
Epoch 17/30
84077/84077 - 37s - loss: 6.9017e-04 - val_loss: 0.1247 - 37s/epoch - 440us/sample
Epoch 18/30
84077/84077 - 37s - loss: 6.8800e-04 - val_loss: 0.1492 - 37s/epoch - 441us/sample
Epoch 19/30
84077/84077 - 37s - loss: 6.8484e-04 - val_loss: 0.1647 - 37s/epoch - 441us/sample
Epoch 20/30
84077/84077 - 37s - loss: 6.8323e-04 - val_loss: 0.1416 - 37s/epoch - 440us/sample
Epoch 21/30
84077/84077 - 37s - loss: 6.8090e-04 - val_loss: 0.1362 - 37s/epoch - 439us/sample
Epoch 22/30
84077/84077 - 37s - loss: 6.7943e-04 - val_loss: 0.1718 - 37s/epoch - 441us/sample
Epoch 23/30
84077/84077 - 37s - loss: 6.7742e-04 - val_loss: 0.1812 - 37s/epoch - 440us/sample
Epoch 24/30
84077/84077 - 37s - loss: 6.7620e-04 - val_loss: 0.1909 - 37s/epoch - 439us/sample
Epoch 25/30
84077/84077 - 37s - loss: 6.7497e-04 - val_loss: 0.1521 - 37s/epoch - 440us/sample
Epoch 26/30
84077/84077 - 37s - loss: 6.7343e-04 - val_loss: 0.1145 - 37s/epoch - 440us/sample
Epoch 27/30
84077/84077 - 37s - loss: 6.7123e-04 - val_loss: 0.1723 - 37s/epoch - 440us/sample
Epoch 28/30
84077/84077 - 37s - loss: 6.7174e-04 - val_loss: 0.1523 - 37s/epoch - 440us/sample
Epoch 29/30
84077/84077 - 37s - loss: 6.6932e-04 - val_loss: 0.1600 - 37s/epoch - 440us/sample
Epoch 30/30
84077/84077 - 37s - loss: 6.6909e-04 - val_loss: 0.1289 - 37s/epoch - 441us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.12891647301628392
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:47:01.637167: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:12048 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.20335181035544408
cosine 0.20127371822577664
MAE: 0.025479317628196326
RMSE: 0.3584801052592781
r2: -99.38679769228284
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 8, 30, 0.0005, 0.3, 282, 0.0006690860441118638, 0.12891647301628392, 0.20335181035544408, 0.20127371822577664, 0.025479317628196326, 0.3584801052592781, -99.38679769228284, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.0012000000000000001 32 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 1414)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          399030      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          399030      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1821097     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,959,629
Trainable params: 3,953,409
Non-trainable params: 6,220
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 21:47:11.155515: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/dense_dec0_10/bias/v/Assign' id:13980 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/dense_dec0_10/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/dense_dec0_10/bias/v, training_20/Adam/dense_dec0_10/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:47:23.412339: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:13332 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0051 - val_loss: 0.0016 - 15s/epoch - 179us/sample
Epoch 2/90
84077/84077 - 11s - loss: 1.7253 - val_loss: 0.0025 - 11s/epoch - 133us/sample
Epoch 3/90
84077/84077 - 11s - loss: 0.0014 - val_loss: 0.0016 - 11s/epoch - 133us/sample
Epoch 4/90
84077/84077 - 11s - loss: 0.0010 - val_loss: 0.0014 - 11s/epoch - 134us/sample
Epoch 5/90
84077/84077 - 11s - loss: 0.0010 - val_loss: 7.2675e-04 - 11s/epoch - 133us/sample
Epoch 6/90
84077/84077 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 133us/sample
Epoch 7/90
84077/84077 - 11s - loss: 7.4354e-04 - val_loss: 5.5453e-04 - 11s/epoch - 133us/sample
Epoch 8/90
84077/84077 - 11s - loss: 5.9683e-04 - val_loss: 4.9623e-04 - 11s/epoch - 133us/sample
Epoch 9/90
84077/84077 - 11s - loss: 5.2001e-04 - val_loss: 4.2506e-04 - 11s/epoch - 134us/sample
Epoch 10/90
84077/84077 - 11s - loss: 4.5609e-04 - val_loss: 3.8530e-04 - 11s/epoch - 133us/sample
Epoch 11/90
84077/84077 - 11s - loss: 4.1496e-04 - val_loss: 3.5377e-04 - 11s/epoch - 133us/sample
Epoch 12/90
84077/84077 - 11s - loss: 3.7965e-04 - val_loss: 3.2348e-04 - 11s/epoch - 133us/sample
Epoch 13/90
84077/84077 - 11s - loss: 3.5376e-04 - val_loss: 3.0947e-04 - 11s/epoch - 133us/sample
Epoch 14/90
84077/84077 - 11s - loss: 3.2847e-04 - val_loss: 2.9852e-04 - 11s/epoch - 133us/sample
Epoch 15/90
84077/84077 - 11s - loss: 3.0996e-04 - val_loss: 2.7936e-04 - 11s/epoch - 134us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.9817e-04 - val_loss: 2.7165e-04 - 11s/epoch - 133us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.8543e-04 - val_loss: 2.6233e-04 - 11s/epoch - 133us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.7410e-04 - val_loss: 2.5680e-04 - 11s/epoch - 133us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.6647e-04 - val_loss: 2.4692e-04 - 11s/epoch - 133us/sample
Epoch 20/90
84077/84077 - 11s - loss: 2.5826e-04 - val_loss: 2.4094e-04 - 11s/epoch - 134us/sample
Epoch 21/90
84077/84077 - 11s - loss: 2.5772e-04 - val_loss: 2.4002e-04 - 11s/epoch - 133us/sample
Epoch 22/90
84077/84077 - 11s - loss: 2.5035e-04 - val_loss: 2.4095e-04 - 11s/epoch - 133us/sample
Epoch 23/90
84077/84077 - 11s - loss: 2.4738e-04 - val_loss: 2.3182e-04 - 11s/epoch - 133us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.4054e-04 - val_loss: 2.3812e-04 - 11s/epoch - 133us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.4025e-04 - val_loss: 2.2865e-04 - 11s/epoch - 133us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.3794e-04 - val_loss: 2.3108e-04 - 11s/epoch - 134us/sample
Epoch 27/90
84077/84077 - 11s - loss: 2.3267e-04 - val_loss: 2.1998e-04 - 11s/epoch - 133us/sample
Epoch 28/90
84077/84077 - 11s - loss: 2.3364e-04 - val_loss: 2.1850e-04 - 11s/epoch - 133us/sample
Epoch 29/90
84077/84077 - 11s - loss: 2.2696e-04 - val_loss: 2.1748e-04 - 11s/epoch - 133us/sample
Epoch 30/90
84077/84077 - 11s - loss: 2.2565e-04 - val_loss: 2.1425e-04 - 11s/epoch - 133us/sample
Epoch 31/90
84077/84077 - 11s - loss: 2.2483e-04 - val_loss: 2.1078e-04 - 11s/epoch - 134us/sample
Epoch 32/90
84077/84077 - 11s - loss: 2.2076e-04 - val_loss: 2.1229e-04 - 11s/epoch - 133us/sample
Epoch 33/90
84077/84077 - 11s - loss: 2.1731e-04 - val_loss: 2.2257e-04 - 11s/epoch - 133us/sample
Epoch 34/90
84077/84077 - 11s - loss: 2.1627e-04 - val_loss: 2.0723e-04 - 11s/epoch - 133us/sample
Epoch 35/90
84077/84077 - 11s - loss: 2.2305e-04 - val_loss: 2.1247e-04 - 11s/epoch - 133us/sample
Epoch 36/90
84077/84077 - 11s - loss: 2.1379e-04 - val_loss: 2.0144e-04 - 11s/epoch - 133us/sample
Epoch 37/90
84077/84077 - 11s - loss: 2.1540e-04 - val_loss: 2.0306e-04 - 11s/epoch - 134us/sample
Epoch 38/90
84077/84077 - 11s - loss: 2.1366e-04 - val_loss: 1.9865e-04 - 11s/epoch - 134us/sample
Epoch 39/90
84077/84077 - 11s - loss: 2.1263e-04 - val_loss: 2.7322e-04 - 11s/epoch - 133us/sample
Epoch 40/90
84077/84077 - 11s - loss: 2.1197e-04 - val_loss: 2.0114e-04 - 11s/epoch - 133us/sample
Epoch 41/90
84077/84077 - 11s - loss: 2.0728e-04 - val_loss: 1.9945e-04 - 11s/epoch - 134us/sample
Epoch 42/90
84077/84077 - 11s - loss: 2.0647e-04 - val_loss: 2.0296e-04 - 11s/epoch - 134us/sample
Epoch 43/90
84077/84077 - 11s - loss: 2.0811e-04 - val_loss: 2.0152e-04 - 11s/epoch - 133us/sample
Epoch 44/90
84077/84077 - 11s - loss: 2.0385e-04 - val_loss: 1.9976e-04 - 11s/epoch - 133us/sample
Epoch 45/90
84077/84077 - 11s - loss: 2.0734e-04 - val_loss: 1.9858e-04 - 11s/epoch - 134us/sample
Epoch 46/90
84077/84077 - 11s - loss: 2.0548e-04 - val_loss: 2.0441e-04 - 11s/epoch - 133us/sample
Epoch 47/90
84077/84077 - 11s - loss: 2.0733e-04 - val_loss: 1.9509e-04 - 11s/epoch - 133us/sample
Epoch 48/90
84077/84077 - 11s - loss: 2.0520e-04 - val_loss: 2.2173e-04 - 11s/epoch - 134us/sample
Epoch 49/90
84077/84077 - 11s - loss: 2.0625e-04 - val_loss: 1.9401e-04 - 11s/epoch - 134us/sample
Epoch 50/90
84077/84077 - 11s - loss: 2.0078e-04 - val_loss: 1.9636e-04 - 11s/epoch - 134us/sample
Epoch 51/90
84077/84077 - 11s - loss: 2.0060e-04 - val_loss: 1.9344e-04 - 11s/epoch - 133us/sample
Epoch 52/90
84077/84077 - 11s - loss: 2.0470e-04 - val_loss: 1.9266e-04 - 11s/epoch - 133us/sample
Epoch 53/90
84077/84077 - 11s - loss: 1.9949e-04 - val_loss: 1.9109e-04 - 11s/epoch - 133us/sample
Epoch 54/90
84077/84077 - 11s - loss: 2.0385e-04 - val_loss: 1.9077e-04 - 11s/epoch - 134us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.9904e-04 - val_loss: 1.8974e-04 - 11s/epoch - 133us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.9736e-04 - val_loss: 1.8970e-04 - 11s/epoch - 134us/sample
Epoch 57/90
84077/84077 - 11s - loss: 2.0078e-04 - val_loss: 1.9043e-04 - 11s/epoch - 133us/sample
Epoch 58/90
84077/84077 - 11s - loss: 2.0602e-04 - val_loss: 1.9439e-04 - 11s/epoch - 134us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.9715e-04 - val_loss: 1.8842e-04 - 11s/epoch - 134us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.9708e-04 - val_loss: 1.9674e-04 - 11s/epoch - 134us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.9448e-04 - val_loss: 1.8949e-04 - 11s/epoch - 133us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.9482e-04 - val_loss: 1.9295e-04 - 11s/epoch - 134us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.9954e-04 - val_loss: 1.8805e-04 - 11s/epoch - 134us/sample
Epoch 64/90
84077/84077 - 11s - loss: 2.0085e-04 - val_loss: 1.9012e-04 - 11s/epoch - 133us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.9285e-04 - val_loss: 1.9107e-04 - 11s/epoch - 133us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.9406e-04 - val_loss: 1.8682e-04 - 11s/epoch - 134us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.9362e-04 - val_loss: 1.8557e-04 - 11s/epoch - 134us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.9871e-04 - val_loss: 1.8675e-04 - 11s/epoch - 133us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.9155e-04 - val_loss: 1.8616e-04 - 11s/epoch - 133us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.9303e-04 - val_loss: 1.8754e-04 - 11s/epoch - 134us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.9255e-04 - val_loss: 1.8829e-04 - 11s/epoch - 133us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.9023e-04 - val_loss: 1.8342e-04 - 11s/epoch - 133us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.8942e-04 - val_loss: 1.8332e-04 - 11s/epoch - 134us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.8916e-04 - val_loss: 1.8619e-04 - 11s/epoch - 134us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.8990e-04 - val_loss: 1.8434e-04 - 11s/epoch - 133us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.8852e-04 - val_loss: 1.8336e-04 - 11s/epoch - 133us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.9051e-04 - val_loss: 1.8400e-04 - 11s/epoch - 133us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.9382e-04 - val_loss: 1.8215e-04 - 11s/epoch - 133us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.8658e-04 - val_loss: 1.8028e-04 - 11s/epoch - 134us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.8931e-04 - val_loss: 1.8514e-04 - 11s/epoch - 133us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.8649e-04 - val_loss: 1.8128e-04 - 11s/epoch - 133us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.9642e-04 - val_loss: 1.8752e-04 - 11s/epoch - 133us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.8769e-04 - val_loss: 1.8289e-04 - 11s/epoch - 134us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.8677e-04 - val_loss: 1.8186e-04 - 11s/epoch - 133us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.8690e-04 - val_loss: 1.8331e-04 - 11s/epoch - 133us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.8522e-04 - val_loss: 1.8312e-04 - 11s/epoch - 133us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.8498e-04 - val_loss: 1.8080e-04 - 11s/epoch - 133us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.8467e-04 - val_loss: 1.7953e-04 - 11s/epoch - 134us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.8438e-04 - val_loss: 1.8092e-04 - 11s/epoch - 133us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.8454e-04 - val_loss: 1.8171e-04 - 11s/epoch - 134us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00018170816982400253
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:04:02.335105: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:13303 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023457583953834165
cosine 0.023168873194177615
MAE: 0.0020217534318977756
RMSE: 0.009904014533955575
r2: 0.9235315928106357
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'mse', 32, 90, 0.0012000000000000001, 0.3, 282, 0.0001845390550162411, 0.00018170816982400253, 0.023457583953834165, 0.023168873194177615, 0.0020217534318977756, 0.009904014533955575, 0.9235315928106357, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 90 0.001 64 0] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1320)         1246080     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 1320)        5280        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 1320)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          372522      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          372522      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1705477     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,701,881
Trainable params: 3,696,037
Non-trainable params: 5,844
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 22:04:12.317731: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/beta_1/Assign' id:15103 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/beta_1, training_22/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:04:21.434957: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:14606 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0696 - val_loss: 100.3462 - 12s/epoch - 143us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0681 - val_loss: 0.0681 - 8s/epoch - 92us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0672 - val_loss: 0.0677 - 8s/epoch - 91us/sample
Epoch 4/90
84077/84077 - 8s - loss: 0.0690 - val_loss: 0.0688 - 8s/epoch - 92us/sample
Epoch 5/90
84077/84077 - 8s - loss: 0.0678 - val_loss: 0.0679 - 8s/epoch - 91us/sample
Epoch 6/90
84077/84077 - 8s - loss: 0.0672 - val_loss: 0.0676 - 8s/epoch - 91us/sample
Epoch 7/90
84077/84077 - 8s - loss: 0.0670 - val_loss: 0.0675 - 8s/epoch - 91us/sample
Epoch 8/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 91us/sample
Epoch 9/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 91us/sample
Epoch 10/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 91us/sample
Epoch 11/90
84077/84077 - 8s - loss: 0.0676 - val_loss: 0.0687 - 8s/epoch - 92us/sample
Epoch 12/90
84077/84077 - 8s - loss: 0.0672 - val_loss: 0.0675 - 8s/epoch - 92us/sample
Epoch 13/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 92us/sample
Epoch 14/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 92us/sample
Epoch 15/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 91us/sample
Epoch 16/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 91us/sample
Epoch 17/90
84077/84077 - 8s - loss: 0.0669 - val_loss: 0.0674 - 8s/epoch - 91us/sample
Epoch 18/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0674 - 8s/epoch - 92us/sample
Epoch 19/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 20/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 21/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 22/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 23/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 24/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 25/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 26/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 27/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 28/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 29/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 30/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 31/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 32/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 33/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 34/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 35/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 36/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 37/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 38/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 39/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 40/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 41/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 42/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 43/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 44/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 45/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 46/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 47/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 48/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 49/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 50/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 51/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 52/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 53/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 54/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 55/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 56/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 57/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 58/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 59/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 60/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 61/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 62/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 63/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 64/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 65/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 66/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 67/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 68/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 69/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 70/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 71/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 72/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 73/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 74/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 75/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 76/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 77/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 78/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 79/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 80/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 81/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 93us/sample
Epoch 82/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 83/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 84/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 85/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 91us/sample
Epoch 86/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 87/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 88/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 89/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
Epoch 90/90
84077/84077 - 8s - loss: 0.0668 - val_loss: 0.0673 - 8s/epoch - 92us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.06734574180845274
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:15:49.438237: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:14558 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2829294296610385
cosine 1.1954404610612743
MAE: 4.2456975322339074
RMSE: 4.662244972541075
r2: -16033.888949479482
RMSE zero-vector: 0.04004287452915337
['1.4custom_VAE', 'binary_crossentropy', 64, 90, 0.001, 0.3, 282, 0.06683692796469977, 0.06734574180845274, 1.2829294296610385, 1.1954404610612743, 4.2456975322339074, 4.662244972541075, -16033.888949479482, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 90 0.001 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 1791)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          505344      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          505344      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2284807     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,993,363
Trainable params: 4,985,635
Non-trainable params: 7,728
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 22:16:00.013955: W tensorflow/c/c_api.cc:291] Operation '{name:'training_24/Adam/outputlayer_12/bias/m/Assign' id:16520 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/outputlayer_12/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_24/Adam/outputlayer_12/bias/m, training_24/Adam/outputlayer_12/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:16:09.185040: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:15934 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 12s - loss: 0.0060 - val_loss: 8.8685e-04 - 12s/epoch - 147us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0010 - val_loss: 6.6628e-04 - 8s/epoch - 90us/sample
Epoch 3/90
84077/84077 - 7s - loss: 9.4502e-04 - val_loss: 0.0030 - 7s/epoch - 89us/sample
Epoch 4/90
84077/84077 - 8s - loss: 7.0827e-04 - val_loss: 7.4262e-04 - 8s/epoch - 89us/sample
Epoch 5/90
84077/84077 - 8s - loss: 5.7320e-04 - val_loss: 4.3502e-04 - 8s/epoch - 89us/sample
Epoch 6/90
84077/84077 - 7s - loss: 4.7422e-04 - val_loss: 4.5939e-04 - 7s/epoch - 89us/sample
Epoch 7/90
84077/84077 - 7s - loss: 3.7232e-04 - val_loss: 3.2094e-04 - 7s/epoch - 89us/sample
Epoch 8/90
84077/84077 - 7s - loss: 3.1279e-04 - val_loss: 2.6557e-04 - 7s/epoch - 89us/sample
Epoch 9/90
84077/84077 - 7s - loss: 2.8434e-04 - val_loss: 2.4589e-04 - 7s/epoch - 89us/sample
Epoch 10/90
84077/84077 - 7s - loss: 2.6153e-04 - val_loss: 2.1560e-04 - 7s/epoch - 89us/sample
Epoch 11/90
84077/84077 - 8s - loss: 2.4093e-04 - val_loss: 2.0473e-04 - 8s/epoch - 90us/sample
Epoch 12/90
84077/84077 - 8s - loss: 2.1658e-04 - val_loss: 1.8069e-04 - 8s/epoch - 90us/sample
Epoch 13/90
84077/84077 - 8s - loss: 2.0031e-04 - val_loss: 1.6773e-04 - 8s/epoch - 89us/sample
Epoch 14/90
84077/84077 - 8s - loss: 1.8807e-04 - val_loss: 1.5953e-04 - 8s/epoch - 89us/sample
Epoch 15/90
84077/84077 - 7s - loss: 1.7787e-04 - val_loss: 1.5153e-04 - 7s/epoch - 89us/sample
Epoch 16/90
84077/84077 - 7s - loss: 1.6857e-04 - val_loss: 1.4528e-04 - 7s/epoch - 89us/sample
Epoch 17/90
84077/84077 - 7s - loss: 1.6322e-04 - val_loss: 1.3808e-04 - 7s/epoch - 89us/sample
Epoch 18/90
84077/84077 - 8s - loss: 1.5770e-04 - val_loss: 1.3651e-04 - 8s/epoch - 90us/sample
Epoch 19/90
84077/84077 - 8s - loss: 1.5302e-04 - val_loss: 1.3323e-04 - 8s/epoch - 90us/sample
Epoch 20/90
84077/84077 - 7s - loss: 1.4959e-04 - val_loss: 1.2953e-04 - 7s/epoch - 89us/sample
Epoch 21/90
84077/84077 - 8s - loss: 1.4656e-04 - val_loss: 1.2749e-04 - 8s/epoch - 89us/sample
Epoch 22/90
84077/84077 - 8s - loss: 1.4533e-04 - val_loss: 1.3037e-04 - 8s/epoch - 90us/sample
Epoch 23/90
84077/84077 - 8s - loss: 1.4184e-04 - val_loss: 1.2373e-04 - 8s/epoch - 89us/sample
Epoch 24/90
84077/84077 - 7s - loss: 1.3966e-04 - val_loss: 1.2248e-04 - 7s/epoch - 89us/sample
Epoch 25/90
84077/84077 - 8s - loss: 1.3867e-04 - val_loss: 1.1950e-04 - 8s/epoch - 89us/sample
Epoch 26/90
84077/84077 - 8s - loss: 1.3539e-04 - val_loss: 1.1953e-04 - 8s/epoch - 90us/sample
Epoch 27/90
84077/84077 - 8s - loss: 1.3472e-04 - val_loss: 1.1818e-04 - 8s/epoch - 90us/sample
Epoch 28/90
84077/84077 - 8s - loss: 1.3288e-04 - val_loss: 1.1734e-04 - 8s/epoch - 90us/sample
Epoch 29/90
84077/84077 - 8s - loss: 1.3103e-04 - val_loss: 1.1590e-04 - 8s/epoch - 90us/sample
Epoch 30/90
84077/84077 - 8s - loss: 1.3016e-04 - val_loss: 1.1521e-04 - 8s/epoch - 89us/sample
Epoch 31/90
84077/84077 - 8s - loss: 1.2911e-04 - val_loss: 1.1491e-04 - 8s/epoch - 89us/sample
Epoch 32/90
84077/84077 - 8s - loss: 1.2741e-04 - val_loss: 1.1240e-04 - 8s/epoch - 90us/sample
Epoch 33/90
84077/84077 - 8s - loss: 1.2681e-04 - val_loss: 1.1358e-04 - 8s/epoch - 90us/sample
Epoch 34/90
84077/84077 - 8s - loss: 1.2575e-04 - val_loss: 1.1317e-04 - 8s/epoch - 89us/sample
Epoch 35/90
84077/84077 - 8s - loss: 1.2480e-04 - val_loss: 1.1404e-04 - 8s/epoch - 90us/sample
Epoch 36/90
84077/84077 - 8s - loss: 1.2417e-04 - val_loss: 1.1066e-04 - 8s/epoch - 89us/sample
Epoch 37/90
84077/84077 - 8s - loss: 1.2326e-04 - val_loss: 1.1046e-04 - 8s/epoch - 89us/sample
Epoch 38/90
84077/84077 - 8s - loss: 1.2289e-04 - val_loss: 1.1201e-04 - 8s/epoch - 90us/sample
Epoch 39/90
84077/84077 - 8s - loss: 1.2179e-04 - val_loss: 1.0989e-04 - 8s/epoch - 90us/sample
Epoch 40/90
84077/84077 - 8s - loss: 1.2062e-04 - val_loss: 1.0891e-04 - 8s/epoch - 90us/sample
Epoch 41/90
84077/84077 - 7s - loss: 1.2027e-04 - val_loss: 1.0696e-04 - 7s/epoch - 89us/sample
Epoch 42/90
84077/84077 - 8s - loss: 1.1975e-04 - val_loss: 1.1041e-04 - 8s/epoch - 90us/sample
Epoch 43/90
84077/84077 - 8s - loss: 1.1912e-04 - val_loss: 1.0712e-04 - 8s/epoch - 89us/sample
Epoch 44/90
84077/84077 - 8s - loss: 1.1813e-04 - val_loss: 1.0670e-04 - 8s/epoch - 89us/sample
Epoch 45/90
84077/84077 - 8s - loss: 1.1820e-04 - val_loss: 1.0717e-04 - 8s/epoch - 89us/sample
Epoch 46/90
84077/84077 - 8s - loss: 1.1744e-04 - val_loss: 1.0606e-04 - 8s/epoch - 89us/sample
Epoch 47/90
84077/84077 - 8s - loss: 1.1770e-04 - val_loss: 1.0690e-04 - 8s/epoch - 90us/sample
Epoch 48/90
84077/84077 - 8s - loss: 1.1629e-04 - val_loss: 1.0601e-04 - 8s/epoch - 89us/sample
Epoch 49/90
84077/84077 - 8s - loss: 1.1579e-04 - val_loss: 1.0516e-04 - 8s/epoch - 90us/sample
Epoch 50/90
84077/84077 - 8s - loss: 1.1529e-04 - val_loss: 1.0495e-04 - 8s/epoch - 90us/sample
Epoch 51/90
84077/84077 - 8s - loss: 1.1475e-04 - val_loss: 1.0415e-04 - 8s/epoch - 89us/sample
Epoch 52/90
84077/84077 - 8s - loss: 1.1463e-04 - val_loss: 1.0532e-04 - 8s/epoch - 89us/sample
Epoch 53/90
84077/84077 - 8s - loss: 1.1422e-04 - val_loss: 1.1161e-04 - 8s/epoch - 90us/sample
Epoch 54/90
84077/84077 - 8s - loss: 1.1419e-04 - val_loss: 1.0394e-04 - 8s/epoch - 90us/sample
Epoch 55/90
84077/84077 - 8s - loss: 1.1354e-04 - val_loss: 1.0389e-04 - 8s/epoch - 89us/sample
Epoch 56/90
84077/84077 - 8s - loss: 1.1309e-04 - val_loss: 1.0324e-04 - 8s/epoch - 90us/sample
Epoch 57/90
84077/84077 - 8s - loss: 1.1304e-04 - val_loss: 1.0295e-04 - 8s/epoch - 89us/sample
Epoch 58/90
84077/84077 - 8s - loss: 1.1254e-04 - val_loss: 1.0354e-04 - 8s/epoch - 89us/sample
Epoch 59/90
84077/84077 - 8s - loss: 1.1202e-04 - val_loss: 1.0272e-04 - 8s/epoch - 89us/sample
Epoch 60/90
84077/84077 - 8s - loss: 1.1188e-04 - val_loss: 1.0260e-04 - 8s/epoch - 90us/sample
Epoch 61/90
84077/84077 - 8s - loss: 1.1143e-04 - val_loss: 1.0236e-04 - 8s/epoch - 90us/sample
Epoch 62/90
84077/84077 - 8s - loss: 1.1176e-04 - val_loss: 1.0302e-04 - 8s/epoch - 90us/sample
Epoch 63/90
84077/84077 - 8s - loss: 1.1123e-04 - val_loss: 1.0193e-04 - 8s/epoch - 89us/sample
Epoch 64/90
84077/84077 - 8s - loss: 1.1027e-04 - val_loss: 1.0105e-04 - 8s/epoch - 90us/sample
Epoch 65/90
84077/84077 - 8s - loss: 1.1036e-04 - val_loss: 1.0140e-04 - 8s/epoch - 89us/sample
Epoch 66/90
84077/84077 - 7s - loss: 1.1011e-04 - val_loss: 1.0129e-04 - 7s/epoch - 89us/sample
Epoch 67/90
84077/84077 - 8s - loss: 1.0993e-04 - val_loss: 1.0131e-04 - 8s/epoch - 89us/sample
Epoch 68/90
84077/84077 - 8s - loss: 1.0964e-04 - val_loss: 1.0070e-04 - 8s/epoch - 89us/sample
Epoch 69/90
84077/84077 - 8s - loss: 1.1020e-04 - val_loss: 1.0118e-04 - 8s/epoch - 90us/sample
Epoch 70/90
84077/84077 - 8s - loss: 1.0920e-04 - val_loss: 1.0020e-04 - 8s/epoch - 89us/sample
Epoch 71/90
84077/84077 - 8s - loss: 1.0861e-04 - val_loss: 9.9381e-05 - 8s/epoch - 90us/sample
Epoch 72/90
84077/84077 - 7s - loss: 1.0872e-04 - val_loss: 9.9895e-05 - 7s/epoch - 89us/sample
Epoch 73/90
84077/84077 - 8s - loss: 1.0846e-04 - val_loss: 9.9004e-05 - 8s/epoch - 89us/sample
Epoch 74/90
84077/84077 - 7s - loss: 1.0799e-04 - val_loss: 9.9102e-05 - 7s/epoch - 89us/sample
Epoch 75/90
84077/84077 - 8s - loss: 1.0826e-04 - val_loss: 9.9292e-05 - 8s/epoch - 89us/sample
Epoch 76/90
84077/84077 - 8s - loss: 1.0814e-04 - val_loss: 9.9453e-05 - 8s/epoch - 90us/sample
Epoch 77/90
84077/84077 - 8s - loss: 1.0728e-04 - val_loss: 9.7716e-05 - 8s/epoch - 89us/sample
Epoch 78/90
84077/84077 - 8s - loss: 1.0729e-04 - val_loss: 9.8502e-05 - 8s/epoch - 90us/sample
Epoch 79/90
84077/84077 - 8s - loss: 1.0710e-04 - val_loss: 9.8504e-05 - 8s/epoch - 90us/sample
Epoch 80/90
84077/84077 - 8s - loss: 1.0688e-04 - val_loss: 9.8943e-05 - 8s/epoch - 89us/sample
Epoch 81/90
84077/84077 - 7s - loss: 1.0673e-04 - val_loss: 9.7953e-05 - 7s/epoch - 89us/sample
Epoch 82/90
84077/84077 - 8s - loss: 1.0645e-04 - val_loss: 9.8820e-05 - 8s/epoch - 89us/sample
Epoch 83/90
84077/84077 - 8s - loss: 1.0697e-04 - val_loss: 9.7615e-05 - 8s/epoch - 90us/sample
Epoch 84/90
84077/84077 - 8s - loss: 1.0604e-04 - val_loss: 9.7423e-05 - 8s/epoch - 90us/sample
Epoch 85/90
84077/84077 - 8s - loss: 1.0610e-04 - val_loss: 9.7602e-05 - 8s/epoch - 89us/sample
Epoch 86/90
84077/84077 - 8s - loss: 1.0586e-04 - val_loss: 9.7429e-05 - 8s/epoch - 90us/sample
Epoch 87/90
84077/84077 - 8s - loss: 1.0577e-04 - val_loss: 9.8108e-05 - 8s/epoch - 90us/sample
Epoch 88/90
84077/84077 - 7s - loss: 1.0574e-04 - val_loss: 9.8339e-05 - 7s/epoch - 89us/sample
Epoch 89/90
84077/84077 - 8s - loss: 1.0503e-04 - val_loss: 9.7432e-05 - 8s/epoch - 89us/sample
Epoch 90/90
84077/84077 - 8s - loss: 1.0541e-04 - val_loss: 9.6928e-05 - 8s/epoch - 89us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 9.692760120797513e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:27:20.565490: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:15898 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023342141137283334
cosine 0.02305668803181165
MAE: 0.0022330014166371715
RMSE: 0.008570223349837681
r2: 0.9428241050337918
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'logcosh', 64, 90, 0.001, 0.3, 282, 0.00010541230946622645, 9.692760120797513e-05, 0.023342141137283334, 0.02305668803181165, 0.0022330014166371715, 0.008570223349837681, 0.9428241050337918, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 90 0.0012000000000000001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1320)         1246080     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 1320)        5280        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 1320)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          372522      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          372522      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1705477     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,701,881
Trainable params: 3,696,037
Non-trainable params: 5,844
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 22:27:31.491917: W tensorflow/c/c_api.cc:291] Operation '{name:'training_26/Adam/batch_normalization_39/beta/m/Assign' id:17689 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/batch_normalization_39/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/batch_normalization_39/beta/m, training_26/Adam/batch_normalization_39/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:27:44.340355: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:17215 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.0058 - val_loss: 0.0014 - 16s/epoch - 195us/sample
Epoch 2/90
84077/84077 - 11s - loss: 0.0016 - val_loss: 0.0016 - 11s/epoch - 134us/sample
Epoch 3/90
84077/84077 - 11s - loss: 9.9286e-04 - val_loss: 7.1184e-04 - 11s/epoch - 134us/sample
Epoch 4/90
84077/84077 - 11s - loss: 7.0937e-04 - val_loss: 5.4628e-04 - 11s/epoch - 134us/sample
Epoch 5/90
84077/84077 - 11s - loss: 5.4794e-04 - val_loss: 4.3050e-04 - 11s/epoch - 135us/sample
Epoch 6/90
84077/84077 - 11s - loss: 4.3654e-04 - val_loss: 3.6037e-04 - 11s/epoch - 135us/sample
Epoch 7/90
84077/84077 - 11s - loss: 3.7393e-04 - val_loss: 3.2843e-04 - 11s/epoch - 135us/sample
Epoch 8/90
84077/84077 - 11s - loss: 3.3481e-04 - val_loss: 2.8555e-04 - 11s/epoch - 134us/sample
Epoch 9/90
84077/84077 - 11s - loss: 3.0668e-04 - val_loss: 2.6622e-04 - 11s/epoch - 136us/sample
Epoch 10/90
84077/84077 - 11s - loss: 2.8984e-04 - val_loss: 2.5323e-04 - 11s/epoch - 135us/sample
Epoch 11/90
84077/84077 - 11s - loss: 2.7522e-04 - val_loss: 2.4347e-04 - 11s/epoch - 134us/sample
Epoch 12/90
84077/84077 - 11s - loss: 2.6285e-04 - val_loss: 2.3235e-04 - 11s/epoch - 135us/sample
Epoch 13/90
84077/84077 - 11s - loss: 2.5194e-04 - val_loss: 2.2715e-04 - 11s/epoch - 135us/sample
Epoch 14/90
84077/84077 - 11s - loss: 2.4670e-04 - val_loss: 2.2098e-04 - 11s/epoch - 135us/sample
Epoch 15/90
84077/84077 - 11s - loss: 2.3742e-04 - val_loss: 2.1565e-04 - 11s/epoch - 134us/sample
Epoch 16/90
84077/84077 - 11s - loss: 2.3339e-04 - val_loss: 2.2891e-04 - 11s/epoch - 134us/sample
Epoch 17/90
84077/84077 - 11s - loss: 2.2890e-04 - val_loss: 2.0593e-04 - 11s/epoch - 135us/sample
Epoch 18/90
84077/84077 - 11s - loss: 2.2338e-04 - val_loss: 2.0348e-04 - 11s/epoch - 135us/sample
Epoch 19/90
84077/84077 - 11s - loss: 2.2121e-04 - val_loss: 1.9957e-04 - 11s/epoch - 134us/sample
Epoch 20/90
84077/84077 - 11s - loss: 2.1549e-04 - val_loss: 1.9968e-04 - 11s/epoch - 134us/sample
Epoch 21/90
84077/84077 - 11s - loss: 2.1308e-04 - val_loss: 1.9494e-04 - 11s/epoch - 135us/sample
Epoch 22/90
84077/84077 - 11s - loss: 2.1140e-04 - val_loss: 1.9192e-04 - 11s/epoch - 135us/sample
Epoch 23/90
84077/84077 - 11s - loss: 2.0775e-04 - val_loss: 1.9342e-04 - 11s/epoch - 135us/sample
Epoch 24/90
84077/84077 - 11s - loss: 2.0720e-04 - val_loss: 1.9289e-04 - 11s/epoch - 135us/sample
Epoch 25/90
84077/84077 - 11s - loss: 2.0368e-04 - val_loss: 1.9320e-04 - 11s/epoch - 134us/sample
Epoch 26/90
84077/84077 - 11s - loss: 2.0211e-04 - val_loss: 1.8586e-04 - 11s/epoch - 134us/sample
Epoch 27/90
84077/84077 - 11s - loss: 1.9853e-04 - val_loss: 1.9302e-04 - 11s/epoch - 134us/sample
Epoch 28/90
84077/84077 - 11s - loss: 1.9986e-04 - val_loss: 1.8947e-04 - 11s/epoch - 135us/sample
Epoch 29/90
84077/84077 - 11s - loss: 1.9642e-04 - val_loss: 1.8440e-04 - 11s/epoch - 134us/sample
Epoch 30/90
84077/84077 - 11s - loss: 1.9521e-04 - val_loss: 1.8838e-04 - 11s/epoch - 135us/sample
Epoch 31/90
84077/84077 - 11s - loss: 1.9813e-04 - val_loss: 1.8163e-04 - 11s/epoch - 135us/sample
Epoch 32/90
84077/84077 - 11s - loss: 1.9463e-04 - val_loss: 1.8298e-04 - 11s/epoch - 134us/sample
Epoch 33/90
84077/84077 - 11s - loss: 1.9187e-04 - val_loss: 1.7923e-04 - 11s/epoch - 134us/sample
Epoch 34/90
84077/84077 - 11s - loss: 1.9202e-04 - val_loss: 1.8047e-04 - 11s/epoch - 135us/sample
Epoch 35/90
84077/84077 - 11s - loss: 1.8986e-04 - val_loss: 1.7796e-04 - 11s/epoch - 135us/sample
Epoch 36/90
84077/84077 - 11s - loss: 1.8908e-04 - val_loss: 1.7285e-04 - 11s/epoch - 134us/sample
Epoch 37/90
84077/84077 - 11s - loss: 1.8792e-04 - val_loss: 1.7692e-04 - 11s/epoch - 135us/sample
Epoch 38/90
84077/84077 - 11s - loss: 1.8687e-04 - val_loss: 1.7603e-04 - 11s/epoch - 134us/sample
Epoch 39/90
84077/84077 - 11s - loss: 1.8756e-04 - val_loss: 1.7225e-04 - 11s/epoch - 134us/sample
Epoch 40/90
84077/84077 - 11s - loss: 1.8481e-04 - val_loss: 1.7216e-04 - 11s/epoch - 136us/sample
Epoch 41/90
84077/84077 - 11s - loss: 1.8752e-04 - val_loss: 1.7425e-04 - 11s/epoch - 135us/sample
Epoch 42/90
84077/84077 - 11s - loss: 1.8294e-04 - val_loss: 1.7293e-04 - 11s/epoch - 134us/sample
Epoch 43/90
84077/84077 - 11s - loss: 1.8277e-04 - val_loss: 1.7095e-04 - 11s/epoch - 135us/sample
Epoch 44/90
84077/84077 - 11s - loss: 1.8203e-04 - val_loss: 1.7551e-04 - 11s/epoch - 134us/sample
Epoch 45/90
84077/84077 - 11s - loss: 1.8054e-04 - val_loss: 1.7189e-04 - 11s/epoch - 135us/sample
Epoch 46/90
84077/84077 - 11s - loss: 1.8005e-04 - val_loss: 1.7169e-04 - 11s/epoch - 134us/sample
Epoch 47/90
84077/84077 - 11s - loss: 1.7990e-04 - val_loss: 1.6820e-04 - 11s/epoch - 135us/sample
Epoch 48/90
84077/84077 - 11s - loss: 1.7876e-04 - val_loss: 1.7358e-04 - 11s/epoch - 134us/sample
Epoch 49/90
84077/84077 - 11s - loss: 1.8289e-04 - val_loss: 1.7431e-04 - 11s/epoch - 134us/sample
Epoch 50/90
84077/84077 - 11s - loss: 1.8364e-04 - val_loss: 1.7859e-04 - 11s/epoch - 134us/sample
Epoch 51/90
84077/84077 - 11s - loss: 1.8235e-04 - val_loss: 1.7019e-04 - 11s/epoch - 134us/sample
Epoch 52/90
84077/84077 - 11s - loss: 1.7880e-04 - val_loss: 1.6936e-04 - 11s/epoch - 134us/sample
Epoch 53/90
84077/84077 - 11s - loss: 1.7828e-04 - val_loss: 1.7017e-04 - 11s/epoch - 135us/sample
Epoch 54/90
84077/84077 - 11s - loss: 1.7610e-04 - val_loss: 1.6520e-04 - 11s/epoch - 134us/sample
Epoch 55/90
84077/84077 - 11s - loss: 1.7652e-04 - val_loss: 1.6796e-04 - 11s/epoch - 135us/sample
Epoch 56/90
84077/84077 - 11s - loss: 1.7711e-04 - val_loss: 1.6613e-04 - 11s/epoch - 134us/sample
Epoch 57/90
84077/84077 - 11s - loss: 1.7465e-04 - val_loss: 1.6591e-04 - 11s/epoch - 134us/sample
Epoch 58/90
84077/84077 - 11s - loss: 1.7559e-04 - val_loss: 1.6648e-04 - 11s/epoch - 134us/sample
Epoch 59/90
84077/84077 - 11s - loss: 1.7332e-04 - val_loss: 1.6680e-04 - 11s/epoch - 135us/sample
Epoch 60/90
84077/84077 - 11s - loss: 1.7486e-04 - val_loss: 1.6246e-04 - 11s/epoch - 134us/sample
Epoch 61/90
84077/84077 - 11s - loss: 1.7426e-04 - val_loss: 1.6330e-04 - 11s/epoch - 134us/sample
Epoch 62/90
84077/84077 - 11s - loss: 1.7309e-04 - val_loss: 1.6443e-04 - 11s/epoch - 134us/sample
Epoch 63/90
84077/84077 - 11s - loss: 1.7488e-04 - val_loss: 1.6910e-04 - 11s/epoch - 134us/sample
Epoch 64/90
84077/84077 - 11s - loss: 1.7156e-04 - val_loss: 1.6596e-04 - 11s/epoch - 135us/sample
Epoch 65/90
84077/84077 - 11s - loss: 1.7185e-04 - val_loss: 1.6240e-04 - 11s/epoch - 135us/sample
Epoch 66/90
84077/84077 - 11s - loss: 1.7428e-04 - val_loss: 1.6376e-04 - 11s/epoch - 134us/sample
Epoch 67/90
84077/84077 - 11s - loss: 1.7189e-04 - val_loss: 1.6582e-04 - 11s/epoch - 135us/sample
Epoch 68/90
84077/84077 - 11s - loss: 1.7055e-04 - val_loss: 1.6030e-04 - 11s/epoch - 134us/sample
Epoch 69/90
84077/84077 - 11s - loss: 1.7078e-04 - val_loss: 1.6051e-04 - 11s/epoch - 134us/sample
Epoch 70/90
84077/84077 - 11s - loss: 1.6944e-04 - val_loss: 1.6211e-04 - 11s/epoch - 134us/sample
Epoch 71/90
84077/84077 - 11s - loss: 1.7565e-04 - val_loss: 1.7115e-04 - 11s/epoch - 135us/sample
Epoch 72/90
84077/84077 - 11s - loss: 1.6937e-04 - val_loss: 1.6098e-04 - 11s/epoch - 135us/sample
Epoch 73/90
84077/84077 - 11s - loss: 1.6942e-04 - val_loss: 1.6725e-04 - 11s/epoch - 134us/sample
Epoch 74/90
84077/84077 - 11s - loss: 1.6845e-04 - val_loss: 1.5901e-04 - 11s/epoch - 134us/sample
Epoch 75/90
84077/84077 - 11s - loss: 1.6809e-04 - val_loss: 1.6897e-04 - 11s/epoch - 134us/sample
Epoch 76/90
84077/84077 - 11s - loss: 1.6806e-04 - val_loss: 1.6309e-04 - 11s/epoch - 135us/sample
Epoch 77/90
84077/84077 - 11s - loss: 1.6795e-04 - val_loss: 1.6137e-04 - 11s/epoch - 135us/sample
Epoch 78/90
84077/84077 - 11s - loss: 1.7264e-04 - val_loss: 1.6587e-04 - 11s/epoch - 135us/sample
Epoch 79/90
84077/84077 - 11s - loss: 1.6778e-04 - val_loss: 1.6058e-04 - 11s/epoch - 135us/sample
Epoch 80/90
84077/84077 - 11s - loss: 1.6787e-04 - val_loss: 1.6158e-04 - 11s/epoch - 135us/sample
Epoch 81/90
84077/84077 - 11s - loss: 1.6720e-04 - val_loss: 1.6226e-04 - 11s/epoch - 134us/sample
Epoch 82/90
84077/84077 - 11s - loss: 1.6648e-04 - val_loss: 1.5732e-04 - 11s/epoch - 134us/sample
Epoch 83/90
84077/84077 - 11s - loss: 1.6660e-04 - val_loss: 1.5776e-04 - 11s/epoch - 135us/sample
Epoch 84/90
84077/84077 - 11s - loss: 1.6645e-04 - val_loss: 1.5716e-04 - 11s/epoch - 135us/sample
Epoch 85/90
84077/84077 - 11s - loss: 1.6652e-04 - val_loss: 2.0159e-04 - 11s/epoch - 134us/sample
Epoch 86/90
84077/84077 - 11s - loss: 1.6776e-04 - val_loss: 1.5791e-04 - 11s/epoch - 135us/sample
Epoch 87/90
84077/84077 - 11s - loss: 1.6425e-04 - val_loss: 1.5698e-04 - 11s/epoch - 134us/sample
Epoch 88/90
84077/84077 - 11s - loss: 1.6503e-04 - val_loss: 1.5909e-04 - 11s/epoch - 134us/sample
Epoch 89/90
84077/84077 - 11s - loss: 1.6494e-04 - val_loss: 1.5956e-04 - 11s/epoch - 134us/sample
Epoch 90/90
84077/84077 - 11s - loss: 1.6480e-04 - val_loss: 1.5853e-04 - 11s/epoch - 135us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0001585297232614485
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:44:33.507504: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:17186 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.017987079085763035
cosine 0.01776781359462164
MAE: 0.001917374185392066
RMSE: 0.008541690789683734
r2: 0.9432206532910347
RMSE zero-vector: 0.04004287452915337
['1.4custom_VAE', 'mse', 32, 90, 0.0012000000000000001, 0.3, 282, 0.0001647999779365182, 0.0001585297232614485, 0.017987079085763035, 0.01776781359462164, 0.001917374185392066, 0.008541690789683734, 0.9432206532910347, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 90 0.001 8 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 1886)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 22:44:45.058268: W tensorflow/c/c_api.cc:291] Operation '{name:'training_28/Adam/dense_enc0_14/kernel/m/Assign' id:18923 op device:{requested: '', assigned: ''} def:{{{node training_28/Adam/dense_enc0_14/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_28/Adam/dense_enc0_14/kernel/m, training_28/Adam/dense_enc0_14/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:45:23.650034: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:18470 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 44s - loss: 0.0045 - val_loss: 0.0012 - 44s/epoch - 519us/sample
Epoch 2/90
84077/84077 - 38s - loss: 0.0013 - val_loss: 0.0016 - 38s/epoch - 453us/sample
Epoch 3/90
84077/84077 - 38s - loss: 0.0011 - val_loss: 0.0015 - 38s/epoch - 451us/sample
Epoch 4/90
84077/84077 - 38s - loss: 9.5423e-04 - val_loss: 0.0076 - 38s/epoch - 453us/sample
Epoch 5/90
84077/84077 - 38s - loss: 8.6243e-04 - val_loss: 0.0338 - 38s/epoch - 454us/sample
Epoch 6/90
84077/84077 - 38s - loss: 8.1121e-04 - val_loss: 0.0863 - 38s/epoch - 453us/sample
Epoch 7/90
84077/84077 - 38s - loss: 7.8318e-04 - val_loss: 0.1222 - 38s/epoch - 454us/sample
Epoch 8/90
84077/84077 - 38s - loss: 7.6152e-04 - val_loss: 0.1687 - 38s/epoch - 453us/sample
Epoch 9/90
84077/84077 - 38s - loss: 7.4477e-04 - val_loss: 0.2174 - 38s/epoch - 454us/sample
Epoch 10/90
84077/84077 - 38s - loss: 7.3528e-04 - val_loss: 0.2302 - 38s/epoch - 454us/sample
Epoch 11/90
84077/84077 - 38s - loss: 7.2735e-04 - val_loss: 0.2593 - 38s/epoch - 453us/sample
Epoch 12/90
84077/84077 - 38s - loss: 7.2044e-04 - val_loss: 0.2587 - 38s/epoch - 454us/sample
Epoch 13/90
84077/84077 - 38s - loss: 7.1523e-04 - val_loss: 0.3246 - 38s/epoch - 454us/sample
Epoch 14/90
84077/84077 - 38s - loss: 7.0914e-04 - val_loss: 0.3772 - 38s/epoch - 454us/sample
Epoch 15/90
84077/84077 - 38s - loss: 7.0226e-04 - val_loss: 0.3789 - 38s/epoch - 453us/sample
Epoch 16/90
84077/84077 - 38s - loss: 6.9681e-04 - val_loss: 0.4034 - 38s/epoch - 454us/sample
Epoch 17/90
84077/84077 - 38s - loss: 6.9396e-04 - val_loss: 0.4946 - 38s/epoch - 454us/sample
Epoch 18/90
84077/84077 - 38s - loss: 6.8853e-04 - val_loss: 0.3948 - 38s/epoch - 453us/sample
Epoch 19/90
84077/84077 - 38s - loss: 6.8255e-04 - val_loss: 0.5169 - 38s/epoch - 453us/sample
Epoch 20/90
84077/84077 - 38s - loss: 6.7804e-04 - val_loss: 0.5193 - 38s/epoch - 453us/sample
Epoch 21/90
84077/84077 - 38s - loss: 6.7308e-04 - val_loss: 0.4295 - 38s/epoch - 454us/sample
Epoch 22/90
84077/84077 - 38s - loss: 6.6776e-04 - val_loss: 0.4605 - 38s/epoch - 453us/sample
Epoch 23/90
84077/84077 - 38s - loss: 6.6345e-04 - val_loss: 0.4477 - 38s/epoch - 453us/sample
Epoch 24/90
84077/84077 - 38s - loss: 6.5657e-04 - val_loss: 0.4490 - 38s/epoch - 454us/sample
Epoch 25/90
84077/84077 - 38s - loss: 6.5204e-04 - val_loss: 0.4601 - 38s/epoch - 453us/sample
Epoch 26/90
84077/84077 - 38s - loss: 6.5063e-04 - val_loss: 0.4648 - 38s/epoch - 453us/sample
Epoch 27/90
84077/84077 - 38s - loss: 6.4715e-04 - val_loss: 0.5297 - 38s/epoch - 454us/sample
Epoch 28/90
84077/84077 - 38s - loss: 6.4557e-04 - val_loss: 0.4954 - 38s/epoch - 453us/sample
Epoch 29/90
84077/84077 - 38s - loss: 6.4308e-04 - val_loss: 0.5511 - 38s/epoch - 452us/sample
Epoch 30/90
84077/84077 - 38s - loss: 6.4123e-04 - val_loss: 0.4864 - 38s/epoch - 454us/sample
Epoch 31/90
84077/84077 - 38s - loss: 6.3990e-04 - val_loss: 0.5715 - 38s/epoch - 453us/sample
Epoch 32/90
84077/84077 - 38s - loss: 6.3833e-04 - val_loss: 0.4589 - 38s/epoch - 454us/sample
Epoch 33/90
84077/84077 - 38s - loss: 6.3734e-04 - val_loss: 0.4564 - 38s/epoch - 452us/sample
Epoch 34/90
84077/84077 - 38s - loss: 6.3451e-04 - val_loss: 0.4507 - 38s/epoch - 454us/sample
Epoch 35/90
84077/84077 - 38s - loss: 6.3239e-04 - val_loss: 0.4046 - 38s/epoch - 454us/sample
Epoch 36/90
84077/84077 - 38s - loss: 6.2965e-04 - val_loss: 0.4886 - 38s/epoch - 452us/sample
Epoch 37/90
84077/84077 - 38s - loss: 6.2874e-04 - val_loss: 0.4237 - 38s/epoch - 454us/sample
Epoch 38/90
84077/84077 - 38s - loss: 6.2651e-04 - val_loss: 0.4607 - 38s/epoch - 453us/sample
Epoch 39/90
84077/84077 - 38s - loss: 6.2365e-04 - val_loss: 0.5277 - 38s/epoch - 454us/sample
Epoch 40/90
84077/84077 - 38s - loss: 6.2092e-04 - val_loss: 0.5484 - 38s/epoch - 454us/sample
Epoch 41/90
84077/84077 - 38s - loss: 6.2131e-04 - val_loss: 0.4810 - 38s/epoch - 454us/sample
Epoch 42/90
84077/84077 - 38s - loss: 6.1882e-04 - val_loss: 0.4598 - 38s/epoch - 453us/sample
Epoch 43/90
84077/84077 - 38s - loss: 6.1824e-04 - val_loss: 0.4877 - 38s/epoch - 453us/sample
Epoch 44/90
84077/84077 - 38s - loss: 6.1800e-04 - val_loss: 0.4294 - 38s/epoch - 453us/sample
Epoch 45/90
84077/84077 - 38s - loss: 6.1647e-04 - val_loss: 0.4909 - 38s/epoch - 454us/sample
Epoch 46/90
84077/84077 - 38s - loss: 6.1606e-04 - val_loss: 0.5127 - 38s/epoch - 454us/sample
Epoch 47/90
84077/84077 - 38s - loss: 6.1501e-04 - val_loss: 0.4543 - 38s/epoch - 452us/sample
Epoch 48/90
84077/84077 - 38s - loss: 6.1324e-04 - val_loss: 0.4287 - 38s/epoch - 454us/sample
Epoch 49/90
84077/84077 - 38s - loss: 6.1126e-04 - val_loss: 0.5393 - 38s/epoch - 454us/sample
Epoch 50/90
84077/84077 - 38s - loss: 6.1127e-04 - val_loss: 0.4494 - 38s/epoch - 453us/sample
Epoch 51/90
84077/84077 - 38s - loss: 6.0910e-04 - val_loss: 0.4269 - 38s/epoch - 453us/sample
Epoch 52/90
84077/84077 - 38s - loss: 6.0762e-04 - val_loss: 0.4381 - 38s/epoch - 454us/sample
Epoch 53/90
84077/84077 - 38s - loss: 6.0520e-04 - val_loss: 0.5473 - 38s/epoch - 453us/sample
Epoch 54/90
84077/84077 - 38s - loss: 6.0629e-04 - val_loss: 0.4814 - 38s/epoch - 454us/sample
Epoch 55/90
84077/84077 - 38s - loss: 6.0444e-04 - val_loss: 0.4811 - 38s/epoch - 453us/sample
Epoch 56/90
84077/84077 - 38s - loss: 6.0293e-04 - val_loss: 0.6497 - 38s/epoch - 454us/sample
Epoch 57/90
84077/84077 - 38s - loss: 6.0191e-04 - val_loss: 0.5765 - 38s/epoch - 454us/sample
Epoch 58/90
84077/84077 - 38s - loss: 6.0131e-04 - val_loss: 0.4977 - 38s/epoch - 453us/sample
Epoch 59/90
84077/84077 - 38s - loss: 5.9988e-04 - val_loss: 0.5360 - 38s/epoch - 454us/sample
Epoch 60/90
84077/84077 - 38s - loss: 5.9828e-04 - val_loss: 0.5400 - 38s/epoch - 453us/sample
Epoch 61/90
84077/84077 - 38s - loss: 5.9649e-04 - val_loss: 0.5054 - 38s/epoch - 453us/sample
Epoch 62/90
84077/84077 - 38s - loss: 5.9453e-04 - val_loss: 0.4897 - 38s/epoch - 454us/sample
Epoch 63/90
84077/84077 - 38s - loss: 5.9488e-04 - val_loss: 0.4723 - 38s/epoch - 453us/sample
Epoch 64/90
84077/84077 - 38s - loss: 5.9296e-04 - val_loss: 0.4994 - 38s/epoch - 454us/sample
Epoch 65/90
84077/84077 - 38s - loss: 5.9218e-04 - val_loss: 0.4454 - 38s/epoch - 453us/sample
Epoch 66/90
84077/84077 - 38s - loss: 5.9204e-04 - val_loss: 0.3771 - 38s/epoch - 454us/sample
Epoch 67/90
84077/84077 - 38s - loss: 5.9071e-04 - val_loss: 0.3714 - 38s/epoch - 453us/sample
Epoch 68/90
84077/84077 - 38s - loss: 5.8968e-04 - val_loss: 0.3985 - 38s/epoch - 453us/sample
Epoch 69/90
84077/84077 - 38s - loss: 5.8885e-04 - val_loss: 0.3842 - 38s/epoch - 453us/sample
Epoch 70/90
84077/84077 - 38s - loss: 5.8920e-04 - val_loss: 0.3903 - 38s/epoch - 454us/sample
Epoch 71/90
84077/84077 - 38s - loss: 5.8762e-04 - val_loss: 0.3307 - 38s/epoch - 453us/sample
Epoch 72/90
84077/84077 - 38s - loss: 5.8647e-04 - val_loss: 0.3495 - 38s/epoch - 453us/sample
Epoch 73/90
84077/84077 - 38s - loss: 5.8578e-04 - val_loss: 0.3571 - 38s/epoch - 455us/sample
Epoch 74/90
84077/84077 - 38s - loss: 5.8647e-04 - val_loss: 0.3385 - 38s/epoch - 453us/sample
Epoch 75/90
84077/84077 - 38s - loss: 5.8509e-04 - val_loss: 0.3404 - 38s/epoch - 453us/sample
Epoch 76/90
84077/84077 - 38s - loss: 5.8438e-04 - val_loss: 0.3360 - 38s/epoch - 453us/sample
Epoch 77/90
84077/84077 - 38s - loss: 5.8371e-04 - val_loss: 0.3233 - 38s/epoch - 454us/sample
Epoch 78/90
84077/84077 - 38s - loss: 5.8219e-04 - val_loss: 0.2990 - 38s/epoch - 454us/sample
Epoch 79/90
84077/84077 - 38s - loss: 5.8341e-04 - val_loss: 0.3215 - 38s/epoch - 454us/sample
Epoch 80/90
84077/84077 - 38s - loss: 5.8099e-04 - val_loss: 0.3619 - 38s/epoch - 453us/sample
Epoch 81/90
84077/84077 - 38s - loss: 5.8113e-04 - val_loss: 0.3100 - 38s/epoch - 454us/sample
Epoch 82/90
84077/84077 - 38s - loss: 5.8102e-04 - val_loss: 0.2907 - 38s/epoch - 453us/sample
Epoch 83/90
84077/84077 - 38s - loss: 5.7915e-04 - val_loss: 0.3076 - 38s/epoch - 453us/sample
Epoch 84/90
84077/84077 - 38s - loss: 5.7763e-04 - val_loss: 0.2782 - 38s/epoch - 454us/sample
Epoch 85/90
84077/84077 - 38s - loss: 5.7774e-04 - val_loss: 0.2650 - 38s/epoch - 453us/sample
Epoch 86/90
84077/84077 - 38s - loss: 5.7736e-04 - val_loss: 0.2295 - 38s/epoch - 452us/sample
Epoch 87/90
84077/84077 - 38s - loss: 5.7732e-04 - val_loss: 0.2964 - 38s/epoch - 455us/sample
Epoch 88/90
84077/84077 - 38s - loss: 5.7728e-04 - val_loss: 0.2853 - 38s/epoch - 453us/sample
Epoch 89/90
84077/84077 - 38s - loss: 5.7516e-04 - val_loss: 0.2636 - 38s/epoch - 453us/sample
Epoch 90/90
84077/84077 - 38s - loss: 5.7567e-04 - val_loss: 0.2276 - 38s/epoch - 454us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.22758306470189485
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:41:59.852685: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:18441 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.20991706972437768
cosine 0.20798005911258677
MAE: 0.041378375030507994
RMSE: 0.4777953657838482
r2: -177.3309107639052
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 8, 90, 0.001, 0.3, 282, 0.0005756688700436539, 0.22758306470189485, 0.20991706972437768, 0.20798005911258677, 0.041378375030507994, 0.4777953657838482, -177.3309107639052, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 1980)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 23:42:12.008837: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_46/moving_variance/Assign' id:19467 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_46/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_46/moving_variance, batch_normalization_46/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:42:21.979931: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:19732 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0036 - val_loss: 0.0013 - 14s/epoch - 165us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0014 - val_loss: 0.0192 - 8s/epoch - 93us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0012 - val_loss: 6.3056e-04 - 8s/epoch - 93us/sample
Epoch 4/90
84077/84077 - 8s - loss: 6.6213e-04 - val_loss: 7.1089e-04 - 8s/epoch - 93us/sample
Epoch 5/90
84077/84077 - 8s - loss: 7.6049e-04 - val_loss: 4.1641e-04 - 8s/epoch - 93us/sample
Epoch 6/90
84077/84077 - 8s - loss: 4.2676e-04 - val_loss: 4.1904e-04 - 8s/epoch - 93us/sample
Epoch 7/90
84077/84077 - 8s - loss: 4.1918e-04 - val_loss: 3.2987e-04 - 8s/epoch - 93us/sample
Epoch 8/90
84077/84077 - 8s - loss: 4.6212e-04 - val_loss: 3.4274e-04 - 8s/epoch - 93us/sample
Epoch 9/90
84077/84077 - 8s - loss: 3.3086e-04 - val_loss: 2.6387e-04 - 8s/epoch - 93us/sample
Epoch 10/90
84077/84077 - 8s - loss: 2.8364e-04 - val_loss: 2.6539e-04 - 8s/epoch - 93us/sample
Epoch 11/90
84077/84077 - 8s - loss: 2.5855e-04 - val_loss: 2.1679e-04 - 8s/epoch - 93us/sample
Epoch 12/90
84077/84077 - 8s - loss: 2.3977e-04 - val_loss: 2.0109e-04 - 8s/epoch - 93us/sample
Epoch 13/90
84077/84077 - 8s - loss: 2.2300e-04 - val_loss: 1.8883e-04 - 8s/epoch - 93us/sample
Epoch 14/90
84077/84077 - 8s - loss: 2.1185e-04 - val_loss: 1.7992e-04 - 8s/epoch - 94us/sample
Epoch 15/90
84077/84077 - 8s - loss: 2.0211e-04 - val_loss: 1.7110e-04 - 8s/epoch - 93us/sample
Epoch 16/90
84077/84077 - 8s - loss: 1.9086e-04 - val_loss: 1.6174e-04 - 8s/epoch - 93us/sample
Epoch 17/90
84077/84077 - 8s - loss: 1.8210e-04 - val_loss: 1.5689e-04 - 8s/epoch - 93us/sample
Epoch 18/90
84077/84077 - 8s - loss: 1.7483e-04 - val_loss: 1.5113e-04 - 8s/epoch - 93us/sample
Epoch 19/90
84077/84077 - 8s - loss: 1.6825e-04 - val_loss: 1.4497e-04 - 8s/epoch - 93us/sample
Epoch 20/90
84077/84077 - 8s - loss: 1.6306e-04 - val_loss: 1.4564e-04 - 8s/epoch - 94us/sample
Epoch 21/90
84077/84077 - 8s - loss: 1.5834e-04 - val_loss: 1.3738e-04 - 8s/epoch - 93us/sample
Epoch 22/90
84077/84077 - 8s - loss: 1.5326e-04 - val_loss: 1.3100e-04 - 8s/epoch - 93us/sample
Epoch 23/90
84077/84077 - 8s - loss: 1.4994e-04 - val_loss: 1.3092e-04 - 8s/epoch - 93us/sample
Epoch 24/90
84077/84077 - 8s - loss: 1.4709e-04 - val_loss: 1.2917e-04 - 8s/epoch - 93us/sample
Epoch 25/90
84077/84077 - 8s - loss: 1.4448e-04 - val_loss: 1.2591e-04 - 8s/epoch - 93us/sample
Epoch 26/90
84077/84077 - 8s - loss: 1.4197e-04 - val_loss: 1.2421e-04 - 8s/epoch - 93us/sample
Epoch 27/90
84077/84077 - 8s - loss: 1.4027e-04 - val_loss: 1.2257e-04 - 8s/epoch - 93us/sample
Epoch 28/90
84077/84077 - 8s - loss: 1.3811e-04 - val_loss: 1.2203e-04 - 8s/epoch - 94us/sample
Epoch 29/90
84077/84077 - 8s - loss: 1.3633e-04 - val_loss: 1.2111e-04 - 8s/epoch - 93us/sample
Epoch 30/90
84077/84077 - 8s - loss: 1.3491e-04 - val_loss: 1.1931e-04 - 8s/epoch - 93us/sample
Epoch 31/90
84077/84077 - 8s - loss: 1.3300e-04 - val_loss: 1.1819e-04 - 8s/epoch - 94us/sample
Epoch 32/90
84077/84077 - 8s - loss: 1.3206e-04 - val_loss: 1.1599e-04 - 8s/epoch - 93us/sample
Epoch 33/90
84077/84077 - 8s - loss: 1.3055e-04 - val_loss: 1.1556e-04 - 8s/epoch - 93us/sample
Epoch 34/90
84077/84077 - 8s - loss: 1.2939e-04 - val_loss: 1.1462e-04 - 8s/epoch - 93us/sample
Epoch 35/90
84077/84077 - 8s - loss: 1.2819e-04 - val_loss: 1.1460e-04 - 8s/epoch - 94us/sample
Epoch 36/90
84077/84077 - 8s - loss: 1.2719e-04 - val_loss: 1.1508e-04 - 8s/epoch - 93us/sample
Epoch 37/90
84077/84077 - 8s - loss: 1.2656e-04 - val_loss: 1.1295e-04 - 8s/epoch - 93us/sample
Epoch 38/90
84077/84077 - 8s - loss: 1.2790e-04 - val_loss: 1.1220e-04 - 8s/epoch - 93us/sample
Epoch 39/90
84077/84077 - 8s - loss: 1.2511e-04 - val_loss: 1.1268e-04 - 8s/epoch - 93us/sample
Epoch 40/90
84077/84077 - 8s - loss: 1.2459e-04 - val_loss: 1.1278e-04 - 8s/epoch - 93us/sample
Epoch 41/90
84077/84077 - 8s - loss: 1.2370e-04 - val_loss: 1.1184e-04 - 8s/epoch - 93us/sample
Epoch 42/90
84077/84077 - 8s - loss: 1.2397e-04 - val_loss: 1.1062e-04 - 8s/epoch - 94us/sample
Epoch 43/90
84077/84077 - 8s - loss: 1.2263e-04 - val_loss: 1.1030e-04 - 8s/epoch - 94us/sample
Epoch 44/90
84077/84077 - 8s - loss: 1.2147e-04 - val_loss: 1.1065e-04 - 8s/epoch - 93us/sample
Epoch 45/90
84077/84077 - 8s - loss: 1.2094e-04 - val_loss: 1.0915e-04 - 8s/epoch - 93us/sample
Epoch 46/90
84077/84077 - 8s - loss: 1.2059e-04 - val_loss: 1.0877e-04 - 8s/epoch - 93us/sample
Epoch 47/90
84077/84077 - 8s - loss: 1.2023e-04 - val_loss: 1.0831e-04 - 8s/epoch - 93us/sample
Epoch 48/90
84077/84077 - 8s - loss: 1.2042e-04 - val_loss: 1.0814e-04 - 8s/epoch - 94us/sample
Epoch 49/90
84077/84077 - 8s - loss: 1.1937e-04 - val_loss: 1.0867e-04 - 8s/epoch - 94us/sample
Epoch 50/90
84077/84077 - 8s - loss: 1.1897e-04 - val_loss: 1.0961e-04 - 8s/epoch - 93us/sample
Epoch 51/90
84077/84077 - 8s - loss: 1.1832e-04 - val_loss: 1.0735e-04 - 8s/epoch - 93us/sample
Epoch 52/90
84077/84077 - 8s - loss: 1.1853e-04 - val_loss: 1.0814e-04 - 8s/epoch - 93us/sample
Epoch 53/90
84077/84077 - 8s - loss: 1.1765e-04 - val_loss: 1.0712e-04 - 8s/epoch - 93us/sample
Epoch 54/90
84077/84077 - 8s - loss: 1.1725e-04 - val_loss: 1.1087e-04 - 8s/epoch - 93us/sample
Epoch 55/90
84077/84077 - 8s - loss: 1.1720e-04 - val_loss: 1.0637e-04 - 8s/epoch - 94us/sample
Epoch 56/90
84077/84077 - 8s - loss: 1.1630e-04 - val_loss: 1.0572e-04 - 8s/epoch - 93us/sample
Epoch 57/90
84077/84077 - 8s - loss: 1.1609e-04 - val_loss: 1.0579e-04 - 8s/epoch - 93us/sample
Epoch 58/90
84077/84077 - 8s - loss: 1.1536e-04 - val_loss: 1.0500e-04 - 8s/epoch - 93us/sample
Epoch 59/90
84077/84077 - 8s - loss: 1.1541e-04 - val_loss: 1.0561e-04 - 8s/epoch - 93us/sample
Epoch 60/90
84077/84077 - 8s - loss: 1.1473e-04 - val_loss: 1.0591e-04 - 8s/epoch - 93us/sample
Epoch 61/90
84077/84077 - 8s - loss: 1.1440e-04 - val_loss: 1.0608e-04 - 8s/epoch - 94us/sample
Epoch 62/90
84077/84077 - 8s - loss: 1.1428e-04 - val_loss: 1.0510e-04 - 8s/epoch - 93us/sample
Epoch 63/90
84077/84077 - 8s - loss: 1.1401e-04 - val_loss: 1.0409e-04 - 8s/epoch - 93us/sample
Epoch 64/90
84077/84077 - 8s - loss: 1.1423e-04 - val_loss: 1.0438e-04 - 8s/epoch - 93us/sample
Epoch 65/90
84077/84077 - 8s - loss: 1.1379e-04 - val_loss: 1.0368e-04 - 8s/epoch - 93us/sample
Epoch 66/90
84077/84077 - 8s - loss: 1.1327e-04 - val_loss: 1.0463e-04 - 8s/epoch - 93us/sample
Epoch 67/90
84077/84077 - 8s - loss: 1.1308e-04 - val_loss: 1.0408e-04 - 8s/epoch - 93us/sample
Epoch 68/90
84077/84077 - 8s - loss: 1.1309e-04 - val_loss: 1.0333e-04 - 8s/epoch - 94us/sample
Epoch 69/90
84077/84077 - 8s - loss: 1.1272e-04 - val_loss: 1.0293e-04 - 8s/epoch - 93us/sample
Epoch 70/90
84077/84077 - 8s - loss: 1.1215e-04 - val_loss: 1.0472e-04 - 8s/epoch - 93us/sample
Epoch 71/90
84077/84077 - 8s - loss: 1.1184e-04 - val_loss: 1.0314e-04 - 8s/epoch - 93us/sample
Epoch 72/90
84077/84077 - 8s - loss: 1.1168e-04 - val_loss: 1.0224e-04 - 8s/epoch - 93us/sample
Epoch 73/90
84077/84077 - 8s - loss: 1.1154e-04 - val_loss: 1.0229e-04 - 8s/epoch - 93us/sample
Epoch 74/90
84077/84077 - 8s - loss: 1.1084e-04 - val_loss: 1.0299e-04 - 8s/epoch - 94us/sample
Epoch 75/90
84077/84077 - 8s - loss: 1.1136e-04 - val_loss: 1.0210e-04 - 8s/epoch - 94us/sample
Epoch 76/90
84077/84077 - 8s - loss: 1.1072e-04 - val_loss: 1.0193e-04 - 8s/epoch - 93us/sample
Epoch 77/90
84077/84077 - 8s - loss: 1.1102e-04 - val_loss: 1.0192e-04 - 8s/epoch - 93us/sample
Epoch 78/90
84077/84077 - 8s - loss: 1.1064e-04 - val_loss: 1.0230e-04 - 8s/epoch - 93us/sample
Epoch 79/90
84077/84077 - 8s - loss: 1.1009e-04 - val_loss: 1.0223e-04 - 8s/epoch - 93us/sample
Epoch 80/90
84077/84077 - 8s - loss: 1.0978e-04 - val_loss: 1.0174e-04 - 8s/epoch - 93us/sample
Epoch 81/90
84077/84077 - 8s - loss: 1.1012e-04 - val_loss: 1.0164e-04 - 8s/epoch - 93us/sample
Epoch 82/90
84077/84077 - 8s - loss: 1.0945e-04 - val_loss: 1.0137e-04 - 8s/epoch - 94us/sample
Epoch 83/90
84077/84077 - 8s - loss: 1.0962e-04 - val_loss: 1.0192e-04 - 8s/epoch - 94us/sample
Epoch 84/90
84077/84077 - 8s - loss: 1.0948e-04 - val_loss: 1.0176e-04 - 8s/epoch - 93us/sample
Epoch 85/90
84077/84077 - 8s - loss: 1.0960e-04 - val_loss: 1.0111e-04 - 8s/epoch - 93us/sample
Epoch 86/90
84077/84077 - 8s - loss: 1.1021e-04 - val_loss: 1.0197e-04 - 8s/epoch - 93us/sample
Epoch 87/90
84077/84077 - 8s - loss: 1.0873e-04 - val_loss: 1.0174e-04 - 8s/epoch - 93us/sample
Epoch 88/90
84077/84077 - 8s - loss: 1.0863e-04 - val_loss: 1.0079e-04 - 8s/epoch - 93us/sample
Epoch 89/90
84077/84077 - 8s - loss: 1.0857e-04 - val_loss: 1.0069e-04 - 8s/epoch - 94us/sample
Epoch 90/90
84077/84077 - 8s - loss: 1.0963e-04 - val_loss: 9.9932e-05 - 8s/epoch - 94us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 9.993242152288006e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:54:01.713589: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:19696 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.024857338431269055
cosine 0.024556722591092643
MAE: 0.0022858648535178396
RMSE: 0.009177903283960063
r2: 0.9343320970883602
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'logcosh', 64, 90, 0.001, 0.3, 282, 0.00010962741395533319, 9.993242152288006e-05, 0.024857338431269055, 0.024556722591092643, 0.0022858648535178396, 0.009177903283960063, 0.9343320970883602, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 595.8845894037719
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 595.8845894037719.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [595.8845894037719]
[1.5 90 0.001 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1414)         1334816     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 1414)        5656        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 1414)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          399030      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          399030      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1821097     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,959,629
Trainable params: 3,953,409
Non-trainable params: 6,220
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-14 23:54:14.279758: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/bottleneck_zlog_16/kernel/v/Assign' id:21647 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/bottleneck_zlog_16/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/bottleneck_zlog_16/kernel/v, training_32/Adam/bottleneck_zlog_16/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:54:24.371420: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:21017 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 14s - loss: 0.0961 - val_loss: 9.3211e-04 - 14s/epoch - 168us/sample
Epoch 2/90
84077/84077 - 8s - loss: 8.1229e-04 - val_loss: 7.3604e-04 - 8s/epoch - 94us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0013 - val_loss: 6.4989e-04 - 8s/epoch - 94us/sample
Epoch 4/90
84077/84077 - 8s - loss: 6.2377e-04 - val_loss: 6.5398e-04 - 8s/epoch - 94us/sample
Epoch 5/90
84077/84077 - 8s - loss: 5.8454e-04 - val_loss: 4.5226e-04 - 8s/epoch - 94us/sample
Epoch 6/90
84077/84077 - 8s - loss: 4.2314e-04 - val_loss: 3.5161e-04 - 8s/epoch - 95us/sample
Epoch 7/90
84077/84077 - 8s - loss: 3.7304e-04 - val_loss: 3.1229e-04 - 8s/epoch - 94us/sample
Epoch 8/90
84077/84077 - 8s - loss: 3.3452e-04 - val_loss: 2.7756e-04 - 8s/epoch - 94us/sample
Epoch 9/90
84077/84077 - 8s - loss: 3.0709e-04 - val_loss: 2.7002e-04 - 8s/epoch - 94us/sample
Epoch 10/90
84077/84077 - 8s - loss: 2.7357e-04 - val_loss: 2.3566e-04 - 8s/epoch - 94us/sample
Epoch 11/90
84077/84077 - 8s - loss: 2.5935e-04 - val_loss: 2.1129e-04 - 8s/epoch - 94us/sample
Epoch 12/90
84077/84077 - 8s - loss: 2.3067e-04 - val_loss: 1.9650e-04 - 8s/epoch - 94us/sample
Epoch 13/90
84077/84077 - 8s - loss: 2.1290e-04 - val_loss: 1.7987e-04 - 8s/epoch - 94us/sample
Epoch 14/90
84077/84077 - 8s - loss: 2.0074e-04 - val_loss: 1.7160e-04 - 8s/epoch - 95us/sample
Epoch 15/90
84077/84077 - 8s - loss: 1.8807e-04 - val_loss: 1.6148e-04 - 8s/epoch - 94us/sample
Epoch 16/90
84077/84077 - 8s - loss: 1.7943e-04 - val_loss: 1.5283e-04 - 8s/epoch - 94us/sample
Epoch 17/90
84077/84077 - 8s - loss: 1.7060e-04 - val_loss: 1.4525e-04 - 8s/epoch - 94us/sample
Epoch 18/90
84077/84077 - 8s - loss: 1.6528e-04 - val_loss: 1.4190e-04 - 8s/epoch - 94us/sample
Epoch 19/90
84077/84077 - 8s - loss: 1.6016e-04 - val_loss: 1.3789e-04 - 8s/epoch - 94us/sample
Epoch 20/90
84077/84077 - 8s - loss: 1.5511e-04 - val_loss: 1.3360e-04 - 8s/epoch - 95us/sample
Epoch 21/90
84077/84077 - 8s - loss: 1.5074e-04 - val_loss: 1.3177e-04 - 8s/epoch - 95us/sample
Epoch 22/90
84077/84077 - 8s - loss: 1.4713e-04 - val_loss: 1.2963e-04 - 8s/epoch - 94us/sample
Epoch 23/90
84077/84077 - 8s - loss: 1.4386e-04 - val_loss: 1.2701e-04 - 8s/epoch - 94us/sample
Epoch 24/90
84077/84077 - 8s - loss: 1.4181e-04 - val_loss: 1.2471e-04 - 8s/epoch - 94us/sample
Epoch 25/90
84077/84077 - 8s - loss: 1.3954e-04 - val_loss: 1.2444e-04 - 8s/epoch - 94us/sample
Epoch 26/90
84077/84077 - 8s - loss: 1.3790e-04 - val_loss: 1.2145e-04 - 8s/epoch - 94us/sample
Epoch 27/90
84077/84077 - 8s - loss: 1.3567e-04 - val_loss: 1.2059e-04 - 8s/epoch - 94us/sample
Epoch 28/90
84077/84077 - 8s - loss: 1.3400e-04 - val_loss: 1.2040e-04 - 8s/epoch - 94us/sample
Epoch 29/90
84077/84077 - 8s - loss: 1.3305e-04 - val_loss: 1.1906e-04 - 8s/epoch - 94us/sample
Epoch 30/90
84077/84077 - 8s - loss: 1.3115e-04 - val_loss: 1.1764e-04 - 8s/epoch - 94us/sample
Epoch 31/90
84077/84077 - 8s - loss: 1.3048e-04 - val_loss: 1.1772e-04 - 8s/epoch - 94us/sample
Epoch 32/90
84077/84077 - 8s - loss: 1.2859e-04 - val_loss: 1.1687e-04 - 8s/epoch - 94us/sample
Epoch 33/90
84077/84077 - 8s - loss: 1.2836e-04 - val_loss: 1.1574e-04 - 8s/epoch - 94us/sample
Epoch 34/90
84077/84077 - 8s - loss: 1.2678e-04 - val_loss: 1.1527e-04 - 8s/epoch - 95us/sample
Epoch 35/90
84077/84077 - 8s - loss: 1.2640e-04 - val_loss: 1.1367e-04 - 8s/epoch - 95us/sample
Epoch 36/90
84077/84077 - 8s - loss: 1.2510e-04 - val_loss: 1.1375e-04 - 8s/epoch - 94us/sample
Epoch 37/90
84077/84077 - 8s - loss: 1.2535e-04 - val_loss: 1.1343e-04 - 8s/epoch - 94us/sample
Epoch 38/90
84077/84077 - 8s - loss: 1.2350e-04 - val_loss: 1.1440e-04 - 8s/epoch - 94us/sample
Epoch 39/90
84077/84077 - 8s - loss: 1.2315e-04 - val_loss: 1.1205e-04 - 8s/epoch - 94us/sample
Epoch 40/90
84077/84077 - 8s - loss: 1.2277e-04 - val_loss: 1.1610e-04 - 8s/epoch - 94us/sample
Epoch 41/90
84077/84077 - 8s - loss: 1.2181e-04 - val_loss: 1.1138e-04 - 8s/epoch - 95us/sample
Epoch 42/90
84077/84077 - 8s - loss: 1.2105e-04 - val_loss: 1.1107e-04 - 8s/epoch - 95us/sample
Epoch 43/90
84077/84077 - 8s - loss: 1.2194e-04 - val_loss: 1.1008e-04 - 8s/epoch - 94us/sample
Epoch 44/90
84077/84077 - 8s - loss: 1.2161e-04 - val_loss: 1.1032e-04 - 8s/epoch - 94us/sample
Epoch 45/90
84077/84077 - 8s - loss: 1.2009e-04 - val_loss: 1.1152e-04 - 8s/epoch - 94us/sample
Epoch 46/90
84077/84077 - 8s - loss: 1.1895e-04 - val_loss: 1.0945e-04 - 8s/epoch - 94us/sample
Epoch 47/90
84077/84077 - 8s - loss: 1.1869e-04 - val_loss: 1.0914e-04 - 8s/epoch - 95us/sample
Epoch 48/90
84077/84077 - 8s - loss: 1.1804e-04 - val_loss: 1.0926e-04 - 8s/epoch - 95us/sample
Epoch 49/90
84077/84077 - 8s - loss: 1.1767e-04 - val_loss: 1.0752e-04 - 8s/epoch - 94us/sample
Epoch 50/90
84077/84077 - 8s - loss: 1.1741e-04 - val_loss: 1.0751e-04 - 8s/epoch - 94us/sample
Epoch 51/90
84077/84077 - 8s - loss: 1.1706e-04 - val_loss: 1.0836e-04 - 8s/epoch - 94us/sample
Epoch 52/90
84077/84077 - 8s - loss: 1.1659e-04 - val_loss: 1.0718e-04 - 8s/epoch - 94us/sample
Epoch 53/90
84077/84077 - 8s - loss: 1.1595e-04 - val_loss: 1.0699e-04 - 8s/epoch - 94us/sample
Epoch 54/90
84077/84077 - 8s - loss: 1.1561e-04 - val_loss: 1.0593e-04 - 8s/epoch - 94us/sample
Epoch 55/90
84077/84077 - 8s - loss: 1.1542e-04 - val_loss: 1.0650e-04 - 8s/epoch - 95us/sample
Epoch 56/90
84077/84077 - 8s - loss: 1.1469e-04 - val_loss: 1.0696e-04 - 8s/epoch - 95us/sample
Epoch 57/90
84077/84077 - 8s - loss: 1.1426e-04 - val_loss: 1.0551e-04 - 8s/epoch - 94us/sample
Epoch 58/90
84077/84077 - 8s - loss: 1.1420e-04 - val_loss: 1.0608e-04 - 8s/epoch - 94us/sample
Epoch 59/90
84077/84077 - 8s - loss: 1.1396e-04 - val_loss: 1.0500e-04 - 8s/epoch - 94us/sample
Epoch 60/90
84077/84077 - 8s - loss: 1.1374e-04 - val_loss: 1.0550e-04 - 8s/epoch - 94us/sample
Epoch 61/90
84077/84077 - 8s - loss: 1.1352e-04 - val_loss: 1.0484e-04 - 8s/epoch - 94us/sample
Epoch 62/90
84077/84077 - 8s - loss: 1.1310e-04 - val_loss: 1.0470e-04 - 8s/epoch - 95us/sample
Epoch 63/90
84077/84077 - 8s - loss: 1.1256e-04 - val_loss: 1.0372e-04 - 8s/epoch - 94us/sample
Epoch 64/90
84077/84077 - 8s - loss: 1.1248e-04 - val_loss: 1.0316e-04 - 8s/epoch - 94us/sample
Epoch 65/90
84077/84077 - 8s - loss: 1.1305e-04 - val_loss: 1.0360e-04 - 8s/epoch - 95us/sample
Epoch 66/90
84077/84077 - 8s - loss: 1.1352e-04 - val_loss: 1.0468e-04 - 8s/epoch - 94us/sample
Epoch 67/90
84077/84077 - 8s - loss: 1.1151e-04 - val_loss: 1.0329e-04 - 8s/epoch - 94us/sample
Epoch 68/90
84077/84077 - 8s - loss: 1.1145e-04 - val_loss: 1.0266e-04 - 8s/epoch - 94us/sample
Epoch 69/90
84077/84077 - 8s - loss: 1.1126e-04 - val_loss: 1.0247e-04 - 8s/epoch - 95us/sample
Epoch 70/90
84077/84077 - 8s - loss: 1.1078e-04 - val_loss: 1.0272e-04 - 8s/epoch - 94us/sample
Epoch 71/90
84077/84077 - 8s - loss: 1.1100e-04 - val_loss: 1.0289e-04 - 8s/epoch - 94us/sample
Epoch 72/90
84077/84077 - 8s - loss: 1.1103e-04 - val_loss: 1.0443e-04 - 8s/epoch - 94us/sample
Epoch 73/90
84077/84077 - 8s - loss: 1.1022e-04 - val_loss: 1.0196e-04 - 8s/epoch - 94us/sample
Epoch 74/90
84077/84077 - 8s - loss: 1.1017e-04 - val_loss: 1.0140e-04 - 8s/epoch - 94us/sample
Epoch 75/90
84077/84077 - 8s - loss: 1.0982e-04 - val_loss: 1.0171e-04 - 8s/epoch - 95us/sample
Epoch 76/90
84077/84077 - 8s - loss: 1.0978e-04 - val_loss: 1.0186e-04 - 8s/epoch - 95us/sample
Epoch 77/90
84077/84077 - 8s - loss: 1.0908e-04 - val_loss: 1.0094e-04 - 8s/epoch - 94us/sample
Epoch 78/90
84077/84077 - 8s - loss: 1.0935e-04 - val_loss: 1.0096e-04 - 8s/epoch - 94us/sample
Epoch 79/90
84077/84077 - 8s - loss: 1.0919e-04 - val_loss: 1.0176e-04 - 8s/epoch - 94us/sample
Epoch 80/90
84077/84077 - 8s - loss: 1.0909e-04 - val_loss: 1.0168e-04 - 8s/epoch - 94us/sample
Epoch 81/90
84077/84077 - 8s - loss: 1.0848e-04 - val_loss: 1.0130e-04 - 8s/epoch - 95us/sample
Epoch 82/90
84077/84077 - 8s - loss: 1.0820e-04 - val_loss: 1.0072e-04 - 8s/epoch - 95us/sample
Epoch 83/90
84077/84077 - 8s - loss: 1.0852e-04 - val_loss: 1.0069e-04 - 8s/epoch - 94us/sample
Epoch 84/90
84077/84077 - 8s - loss: 1.0807e-04 - val_loss: 1.0044e-04 - 8s/epoch - 94us/sample
Epoch 85/90
84077/84077 - 8s - loss: 1.0849e-04 - val_loss: 9.9742e-05 - 8s/epoch - 94us/sample
Epoch 86/90
84077/84077 - 8s - loss: 1.0809e-04 - val_loss: 9.9344e-05 - 8s/epoch - 94us/sample
Epoch 87/90
84077/84077 - 8s - loss: 1.0782e-04 - val_loss: 9.9754e-05 - 8s/epoch - 94us/sample
Epoch 88/90
84077/84077 - 8s - loss: 1.0792e-04 - val_loss: 1.0013e-04 - 8s/epoch - 95us/sample
Epoch 89/90
84077/84077 - 8s - loss: 1.1489e-04 - val_loss: 1.0673e-04 - 8s/epoch - 95us/sample
Epoch 90/90
84077/84077 - 8s - loss: 1.0912e-04 - val_loss: 1.0166e-04 - 8s/epoch - 94us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00010166295975154803
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:06:11.588797: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:20981 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.024482267671734104
cosine 0.024198315726601183
MAE: 0.0021961540226389815
RMSE: 0.00891549921336232
r2: 0.9381935281063958
RMSE zero-vector: 0.04004287452915337
['1.5custom_VAE', 'logcosh', 64, 90, 0.001, 0.3, 282, 0.00010911935390989618, 0.00010166295975154803, 0.024482267671734104, 0.024198315726601183, 0.0021961540226389815, 0.00891549921336232, 0.9381935281063958, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.0012000000000000001 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 2074)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          585150      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          585150      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2632897     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,769,349
Trainable params: 5,760,489
Non-trainable params: 8,860
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 00:06:24.656811: W tensorflow/c/c_api.cc:291] Operation '{name:'training_34/Adam/batch_normalization_53/gamma/v/Assign' id:22953 op device:{requested: '', assigned: ''} def:{{{node training_34/Adam/batch_normalization_53/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_34/Adam/batch_normalization_53/gamma/v, training_34/Adam/batch_normalization_53/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:06:34.966390: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:22295 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0087 - val_loss: 0.0020 - 15s/epoch - 175us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0015 - val_loss: 0.0015 - 8s/epoch - 96us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0019 - val_loss: 0.0012 - 8s/epoch - 95us/sample
Epoch 4/90
84077/84077 - 8s - loss: 0.0013 - val_loss: 0.0012 - 8s/epoch - 96us/sample
Epoch 5/90
84077/84077 - 8s - loss: 9.4368e-04 - val_loss: 6.9168e-04 - 8s/epoch - 95us/sample
Epoch 6/90
84077/84077 - 8s - loss: 0.0044 - val_loss: 6.3024e-04 - 8s/epoch - 95us/sample
Epoch 7/90
84077/84077 - 8s - loss: 6.9821e-04 - val_loss: 5.8636e-04 - 8s/epoch - 95us/sample
Epoch 8/90
84077/84077 - 8s - loss: 6.0261e-04 - val_loss: 4.3727e-04 - 8s/epoch - 95us/sample
Epoch 9/90
84077/84077 - 8s - loss: 4.7522e-04 - val_loss: 3.6414e-04 - 8s/epoch - 95us/sample
Epoch 10/90
84077/84077 - 8s - loss: 4.3845e-04 - val_loss: 3.4590e-04 - 8s/epoch - 95us/sample
Epoch 11/90
84077/84077 - 8s - loss: 3.7457e-04 - val_loss: 3.1305e-04 - 8s/epoch - 96us/sample
Epoch 12/90
84077/84077 - 8s - loss: 3.3554e-04 - val_loss: 2.7866e-04 - 8s/epoch - 95us/sample
Epoch 13/90
84077/84077 - 8s - loss: 3.1602e-04 - val_loss: 2.5529e-04 - 8s/epoch - 96us/sample
Epoch 14/90
84077/84077 - 8s - loss: 2.9194e-04 - val_loss: 2.4116e-04 - 8s/epoch - 95us/sample
Epoch 15/90
84077/84077 - 8s - loss: 2.7612e-04 - val_loss: 2.3298e-04 - 8s/epoch - 95us/sample
Epoch 16/90
84077/84077 - 8s - loss: 2.6210e-04 - val_loss: 2.2323e-04 - 8s/epoch - 95us/sample
Epoch 17/90
84077/84077 - 8s - loss: 2.5144e-04 - val_loss: 2.1725e-04 - 8s/epoch - 96us/sample
Epoch 18/90
84077/84077 - 8s - loss: 2.4170e-04 - val_loss: 2.2082e-04 - 8s/epoch - 95us/sample
Epoch 19/90
84077/84077 - 8s - loss: 2.3234e-04 - val_loss: 2.0748e-04 - 8s/epoch - 96us/sample
Epoch 20/90
84077/84077 - 8s - loss: 2.2544e-04 - val_loss: 1.9442e-04 - 8s/epoch - 95us/sample
Epoch 21/90
84077/84077 - 8s - loss: 2.1910e-04 - val_loss: 1.9025e-04 - 8s/epoch - 95us/sample
Epoch 22/90
84077/84077 - 8s - loss: 2.1201e-04 - val_loss: 1.8456e-04 - 8s/epoch - 96us/sample
Epoch 23/90
84077/84077 - 8s - loss: 2.0775e-04 - val_loss: 1.8072e-04 - 8s/epoch - 96us/sample
Epoch 24/90
84077/84077 - 8s - loss: 2.0374e-04 - val_loss: 1.7738e-04 - 8s/epoch - 96us/sample
Epoch 25/90
84077/84077 - 8s - loss: 1.9817e-04 - val_loss: 1.7120e-04 - 8s/epoch - 96us/sample
Epoch 26/90
84077/84077 - 8s - loss: 1.9805e-04 - val_loss: 1.7523e-04 - 8s/epoch - 95us/sample
Epoch 27/90
84077/84077 - 8s - loss: 1.9146e-04 - val_loss: 1.7100e-04 - 8s/epoch - 95us/sample
Epoch 28/90
84077/84077 - 8s - loss: 1.8941e-04 - val_loss: 1.6827e-04 - 8s/epoch - 96us/sample
Epoch 29/90
84077/84077 - 8s - loss: 1.8619e-04 - val_loss: 1.6581e-04 - 8s/epoch - 96us/sample
Epoch 30/90
84077/84077 - 8s - loss: 1.8431e-04 - val_loss: 1.6172e-04 - 8s/epoch - 96us/sample
Epoch 31/90
84077/84077 - 8s - loss: 1.8267e-04 - val_loss: 1.6469e-04 - 8s/epoch - 95us/sample
Epoch 32/90
84077/84077 - 8s - loss: 1.7947e-04 - val_loss: 1.5843e-04 - 8s/epoch - 95us/sample
Epoch 33/90
84077/84077 - 8s - loss: 1.7756e-04 - val_loss: 1.5972e-04 - 8s/epoch - 95us/sample
Epoch 34/90
84077/84077 - 8s - loss: 1.7661e-04 - val_loss: 1.5632e-04 - 8s/epoch - 96us/sample
Epoch 35/90
84077/84077 - 8s - loss: 1.7338e-04 - val_loss: 1.5742e-04 - 8s/epoch - 96us/sample
Epoch 36/90
84077/84077 - 8s - loss: 1.7877e-04 - val_loss: 1.5770e-04 - 8s/epoch - 96us/sample
Epoch 37/90
84077/84077 - 8s - loss: 1.7163e-04 - val_loss: 1.5417e-04 - 8s/epoch - 96us/sample
Epoch 38/90
84077/84077 - 8s - loss: 1.6997e-04 - val_loss: 1.5290e-04 - 8s/epoch - 96us/sample
Epoch 39/90
84077/84077 - 8s - loss: 1.6842e-04 - val_loss: 1.5093e-04 - 8s/epoch - 95us/sample
Epoch 40/90
84077/84077 - 8s - loss: 1.6744e-04 - val_loss: 1.5125e-04 - 8s/epoch - 95us/sample
Epoch 41/90
84077/84077 - 8s - loss: 1.6778e-04 - val_loss: 1.4970e-04 - 8s/epoch - 96us/sample
Epoch 42/90
84077/84077 - 8s - loss: 1.6461e-04 - val_loss: 1.4949e-04 - 8s/epoch - 96us/sample
Epoch 43/90
84077/84077 - 8s - loss: 1.6442e-04 - val_loss: 1.4956e-04 - 8s/epoch - 96us/sample
Epoch 44/90
84077/84077 - 8s - loss: 1.6250e-04 - val_loss: 1.4995e-04 - 8s/epoch - 95us/sample
Epoch 45/90
84077/84077 - 8s - loss: 1.6188e-04 - val_loss: 1.4905e-04 - 8s/epoch - 95us/sample
Epoch 46/90
84077/84077 - 8s - loss: 1.6180e-04 - val_loss: 1.5003e-04 - 8s/epoch - 96us/sample
Epoch 47/90
84077/84077 - 8s - loss: 1.5999e-04 - val_loss: 1.4639e-04 - 8s/epoch - 96us/sample
Epoch 48/90
84077/84077 - 8s - loss: 1.5941e-04 - val_loss: 1.4734e-04 - 8s/epoch - 96us/sample
Epoch 49/90
84077/84077 - 8s - loss: 1.5840e-04 - val_loss: 1.4605e-04 - 8s/epoch - 96us/sample
Epoch 50/90
84077/84077 - 8s - loss: 1.6142e-04 - val_loss: 1.5405e-04 - 8s/epoch - 96us/sample
Epoch 51/90
84077/84077 - 8s - loss: 1.5914e-04 - val_loss: 1.4671e-04 - 8s/epoch - 95us/sample
Epoch 52/90
84077/84077 - 8s - loss: 1.5695e-04 - val_loss: 1.4286e-04 - 8s/epoch - 96us/sample
Epoch 53/90
84077/84077 - 8s - loss: 1.5835e-04 - val_loss: 1.4373e-04 - 8s/epoch - 97us/sample
Epoch 54/90
84077/84077 - 8s - loss: 1.5541e-04 - val_loss: 1.4258e-04 - 8s/epoch - 96us/sample
Epoch 55/90
84077/84077 - 8s - loss: 1.5450e-04 - val_loss: 1.4618e-04 - 8s/epoch - 96us/sample
Epoch 56/90
84077/84077 - 8s - loss: 1.5550e-04 - val_loss: 1.4586e-04 - 8s/epoch - 95us/sample
Epoch 57/90
84077/84077 - 8s - loss: 1.5397e-04 - val_loss: 1.4383e-04 - 8s/epoch - 95us/sample
Epoch 58/90
84077/84077 - 8s - loss: 1.5326e-04 - val_loss: 1.4231e-04 - 8s/epoch - 95us/sample
Epoch 59/90
84077/84077 - 8s - loss: 1.5233e-04 - val_loss: 1.4082e-04 - 8s/epoch - 96us/sample
Epoch 60/90
84077/84077 - 8s - loss: 1.5189e-04 - val_loss: 1.4040e-04 - 8s/epoch - 95us/sample
Epoch 61/90
84077/84077 - 8s - loss: 1.5215e-04 - val_loss: 1.4920e-04 - 8s/epoch - 96us/sample
Epoch 62/90
84077/84077 - 8s - loss: 1.5210e-04 - val_loss: 1.4114e-04 - 8s/epoch - 96us/sample
Epoch 63/90
84077/84077 - 8s - loss: 1.5013e-04 - val_loss: 1.3740e-04 - 8s/epoch - 95us/sample
Epoch 64/90
84077/84077 - 8s - loss: 1.4999e-04 - val_loss: 1.3844e-04 - 8s/epoch - 96us/sample
Epoch 65/90
84077/84077 - 8s - loss: 1.4978e-04 - val_loss: 1.4031e-04 - 8s/epoch - 96us/sample
Epoch 66/90
84077/84077 - 8s - loss: 1.4992e-04 - val_loss: 1.7513e-04 - 8s/epoch - 96us/sample
Epoch 67/90
84077/84077 - 8s - loss: 1.5030e-04 - val_loss: 1.3936e-04 - 8s/epoch - 96us/sample
Epoch 68/90
84077/84077 - 8s - loss: 1.5185e-04 - val_loss: 1.4351e-04 - 8s/epoch - 95us/sample
Epoch 69/90
84077/84077 - 8s - loss: 1.4903e-04 - val_loss: 1.3649e-04 - 8s/epoch - 95us/sample
Epoch 70/90
84077/84077 - 8s - loss: 1.4834e-04 - val_loss: 1.3914e-04 - 8s/epoch - 96us/sample
Epoch 71/90
84077/84077 - 8s - loss: 1.4750e-04 - val_loss: 1.3673e-04 - 8s/epoch - 96us/sample
Epoch 72/90
84077/84077 - 8s - loss: 1.4683e-04 - val_loss: 1.3609e-04 - 8s/epoch - 96us/sample
Epoch 73/90
84077/84077 - 8s - loss: 1.4669e-04 - val_loss: 1.3704e-04 - 8s/epoch - 96us/sample
Epoch 74/90
84077/84077 - 8s - loss: 1.4674e-04 - val_loss: 1.4201e-04 - 8s/epoch - 95us/sample
Epoch 75/90
84077/84077 - 8s - loss: 1.4643e-04 - val_loss: 1.3763e-04 - 8s/epoch - 95us/sample
Epoch 76/90
84077/84077 - 8s - loss: 1.4774e-04 - val_loss: 1.3438e-04 - 8s/epoch - 95us/sample
Epoch 77/90
84077/84077 - 8s - loss: 1.4686e-04 - val_loss: 1.3502e-04 - 8s/epoch - 96us/sample
Epoch 78/90
84077/84077 - 8s - loss: 1.4511e-04 - val_loss: 1.3512e-04 - 8s/epoch - 96us/sample
Epoch 79/90
84077/84077 - 8s - loss: 1.4399e-04 - val_loss: 1.3454e-04 - 8s/epoch - 96us/sample
Epoch 80/90
84077/84077 - 8s - loss: 1.4625e-04 - val_loss: 1.3466e-04 - 8s/epoch - 96us/sample
Epoch 81/90
84077/84077 - 8s - loss: 1.4479e-04 - val_loss: 1.3513e-04 - 8s/epoch - 95us/sample
Epoch 82/90
84077/84077 - 8s - loss: 1.4322e-04 - val_loss: 1.3494e-04 - 8s/epoch - 95us/sample
Epoch 83/90
84077/84077 - 8s - loss: 1.4971e-04 - val_loss: 1.6449e-04 - 8s/epoch - 96us/sample
Epoch 84/90
84077/84077 - 8s - loss: 1.4683e-04 - val_loss: 1.3519e-04 - 8s/epoch - 96us/sample
Epoch 85/90
84077/84077 - 8s - loss: 1.4317e-04 - val_loss: 1.3567e-04 - 8s/epoch - 96us/sample
Epoch 86/90
84077/84077 - 8s - loss: 1.4328e-04 - val_loss: 1.3811e-04 - 8s/epoch - 96us/sample
Epoch 87/90
84077/84077 - 8s - loss: 1.4249e-04 - val_loss: 1.3448e-04 - 8s/epoch - 95us/sample
Epoch 88/90
84077/84077 - 8s - loss: 1.4280e-04 - val_loss: 1.3341e-04 - 8s/epoch - 96us/sample
Epoch 89/90
84077/84077 - 8s - loss: 1.4201e-04 - val_loss: 1.3423e-04 - 8s/epoch - 97us/sample
Epoch 90/90
84077/84077 - 8s - loss: 1.4151e-04 - val_loss: 1.3287e-04 - 8s/epoch - 96us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0001328707150266437
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:18:32.567236: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:22266 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.015684371349506537
cosine 0.015496098491460088
MAE: 0.0018058970842646772
RMSE: 0.007303867932088825
r2: 0.9590594640493902
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 90, 0.0012000000000000001, 0.3, 282, 0.00014151435209083652, 0.0001328707150266437, 0.015684371349506537, 0.015496098491460088, 0.0018058970842646772, 0.007303867932088825, 0.9590594640493902, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.001 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 2074)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          585150      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          585150      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2632897     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,769,349
Trainable params: 5,760,489
Non-trainable params: 8,860
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 00:18:46.103670: W tensorflow/c/c_api.cc:291] Operation '{name:'training_36/Adam/beta_1/Assign' id:23984 op device:{requested: '', assigned: ''} def:{{{node training_36/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_36/Adam/beta_1, training_36/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:18:56.567369: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_18/mul' id:23553 op device:{requested: '', assigned: ''} def:{{{node loss_18/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_18/mul/x, loss_18/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 15s - loss: 0.0110 - val_loss: 0.0017 - 15s/epoch - 179us/sample
Epoch 2/90
84077/84077 - 8s - loss: 0.0016 - val_loss: 0.0016 - 8s/epoch - 95us/sample
Epoch 3/90
84077/84077 - 8s - loss: 0.0516 - val_loss: 0.0297 - 8s/epoch - 93us/sample
Epoch 4/90
84077/84077 - 8s - loss: 0.0025 - val_loss: 0.0012 - 8s/epoch - 94us/sample
Epoch 5/90
84077/84077 - 8s - loss: 0.0011 - val_loss: 8.7559e-04 - 8s/epoch - 93us/sample
Epoch 6/90
84077/84077 - 8s - loss: 9.1800e-04 - val_loss: 8.7047e-04 - 8s/epoch - 93us/sample
Epoch 7/90
84077/84077 - 8s - loss: 7.8311e-04 - val_loss: 7.5341e-04 - 8s/epoch - 93us/sample
Epoch 8/90
84077/84077 - 8s - loss: 9.4398e-04 - val_loss: 8.1718e-04 - 8s/epoch - 93us/sample
Epoch 9/90
84077/84077 - 8s - loss: 6.1468e-04 - val_loss: 5.1571e-04 - 8s/epoch - 93us/sample
Epoch 10/90
84077/84077 - 8s - loss: 9.6976e-04 - val_loss: 5.4352e-04 - 8s/epoch - 94us/sample
Epoch 11/90
84077/84077 - 8s - loss: 5.3542e-04 - val_loss: 4.6742e-04 - 8s/epoch - 94us/sample
Epoch 12/90
84077/84077 - 8s - loss: 4.8193e-04 - val_loss: 7.0524e-04 - 8s/epoch - 94us/sample
Epoch 13/90
84077/84077 - 8s - loss: 0.0011 - val_loss: 0.0018 - 8s/epoch - 94us/sample
Epoch 14/90
84077/84077 - 8s - loss: 6.7517e-04 - val_loss: 3.8527e-04 - 8s/epoch - 93us/sample
Epoch 15/90
84077/84077 - 8s - loss: 4.6025e-04 - val_loss: 3.4233e-04 - 8s/epoch - 94us/sample
Epoch 16/90
84077/84077 - 8s - loss: 3.6548e-04 - val_loss: 3.0560e-04 - 8s/epoch - 94us/sample
Epoch 17/90
84077/84077 - 8s - loss: 3.4599e-04 - val_loss: 2.9181e-04 - 8s/epoch - 94us/sample
Epoch 18/90
84077/84077 - 8s - loss: 3.1842e-04 - val_loss: 2.7858e-04 - 8s/epoch - 94us/sample
Epoch 19/90
84077/84077 - 8s - loss: 3.1462e-04 - val_loss: 2.6164e-04 - 8s/epoch - 94us/sample
Epoch 20/90
84077/84077 - 8s - loss: 3.4141e-04 - val_loss: 2.5070e-04 - 8s/epoch - 94us/sample
Epoch 21/90
84077/84077 - 8s - loss: 2.7541e-04 - val_loss: 2.4269e-04 - 8s/epoch - 94us/sample
Epoch 22/90
84077/84077 - 8s - loss: 2.6368e-04 - val_loss: 2.3716e-04 - 8s/epoch - 94us/sample
Epoch 23/90
84077/84077 - 8s - loss: 2.5370e-04 - val_loss: 2.2238e-04 - 8s/epoch - 94us/sample
Epoch 24/90
84077/84077 - 8s - loss: 2.4391e-04 - val_loss: 2.2040e-04 - 8s/epoch - 94us/sample
Epoch 25/90
84077/84077 - 8s - loss: 2.4620e-04 - val_loss: 2.1690e-04 - 8s/epoch - 94us/sample
Epoch 26/90
84077/84077 - 8s - loss: 2.3237e-04 - val_loss: 2.0868e-04 - 8s/epoch - 94us/sample
Epoch 27/90
84077/84077 - 8s - loss: 2.2515e-04 - val_loss: 2.0504e-04 - 8s/epoch - 94us/sample
Epoch 28/90
84077/84077 - 8s - loss: 2.1966e-04 - val_loss: 1.9813e-04 - 8s/epoch - 94us/sample
Epoch 29/90
84077/84077 - 8s - loss: 2.1477e-04 - val_loss: 1.9753e-04 - 8s/epoch - 94us/sample
Epoch 30/90
84077/84077 - 8s - loss: 2.1305e-04 - val_loss: 2.1033e-04 - 8s/epoch - 95us/sample
Epoch 31/90
84077/84077 - 8s - loss: 2.0969e-04 - val_loss: 1.8982e-04 - 8s/epoch - 94us/sample
Epoch 32/90
84077/84077 - 8s - loss: 2.0615e-04 - val_loss: 1.9253e-04 - 8s/epoch - 94us/sample
Epoch 33/90
84077/84077 - 8s - loss: 2.0294e-04 - val_loss: 1.8841e-04 - 8s/epoch - 94us/sample
Epoch 34/90
84077/84077 - 8s - loss: 1.9984e-04 - val_loss: 1.8813e-04 - 8s/epoch - 94us/sample
Epoch 35/90
84077/84077 - 8s - loss: 1.9744e-04 - val_loss: 1.8659e-04 - 8s/epoch - 94us/sample
Epoch 36/90
84077/84077 - 8s - loss: 1.9518e-04 - val_loss: 1.8394e-04 - 8s/epoch - 94us/sample
Epoch 37/90
84077/84077 - 8s - loss: 1.9346e-04 - val_loss: 1.8055e-04 - 8s/epoch - 95us/sample
Epoch 38/90
84077/84077 - 8s - loss: 1.9187e-04 - val_loss: 1.8162e-04 - 8s/epoch - 94us/sample
Epoch 39/90
84077/84077 - 8s - loss: 1.9051e-04 - val_loss: 1.8087e-04 - 8s/epoch - 94us/sample
Epoch 40/90
84077/84077 - 8s - loss: 1.8817e-04 - val_loss: 1.7678e-04 - 8s/epoch - 94us/sample
Epoch 41/90
84077/84077 - 8s - loss: 1.8702e-04 - val_loss: 1.7674e-04 - 8s/epoch - 94us/sample
Epoch 42/90
84077/84077 - 8s - loss: 1.8566e-04 - val_loss: 1.7654e-04 - 8s/epoch - 94us/sample
Epoch 43/90
84077/84077 - 8s - loss: 1.8438e-04 - val_loss: 1.7425e-04 - 8s/epoch - 94us/sample
Epoch 44/90
84077/84077 - 8s - loss: 1.8258e-04 - val_loss: 1.7153e-04 - 8s/epoch - 94us/sample
Epoch 45/90
84077/84077 - 8s - loss: 1.8122e-04 - val_loss: 1.7454e-04 - 8s/epoch - 94us/sample
Epoch 46/90
84077/84077 - 8s - loss: 1.8017e-04 - val_loss: 1.7295e-04 - 8s/epoch - 94us/sample
Epoch 47/90
84077/84077 - 8s - loss: 1.8019e-04 - val_loss: 1.7036e-04 - 8s/epoch - 94us/sample
Epoch 48/90
84077/84077 - 8s - loss: 1.7819e-04 - val_loss: 1.7141e-04 - 8s/epoch - 94us/sample
Epoch 49/90
84077/84077 - 8s - loss: 1.7787e-04 - val_loss: 1.7188e-04 - 8s/epoch - 94us/sample
Epoch 50/90
84077/84077 - 8s - loss: 1.7620e-04 - val_loss: 1.6904e-04 - 8s/epoch - 95us/sample
Epoch 51/90
84077/84077 - 8s - loss: 1.7532e-04 - val_loss: 1.6878e-04 - 8s/epoch - 94us/sample
Epoch 52/90
84077/84077 - 8s - loss: 1.7568e-04 - val_loss: 1.6785e-04 - 8s/epoch - 94us/sample
Epoch 53/90
84077/84077 - 8s - loss: 1.7362e-04 - val_loss: 1.6629e-04 - 8s/epoch - 94us/sample
Epoch 54/90
84077/84077 - 8s - loss: 1.7288e-04 - val_loss: 1.6568e-04 - 8s/epoch - 94us/sample
Epoch 55/90
84077/84077 - 8s - loss: 1.7230e-04 - val_loss: 1.6534e-04 - 8s/epoch - 94us/sample
Epoch 56/90
84077/84077 - 8s - loss: 1.7170e-04 - val_loss: 1.6389e-04 - 8s/epoch - 94us/sample
Epoch 57/90
84077/84077 - 8s - loss: 1.7039e-04 - val_loss: 1.6340e-04 - 8s/epoch - 94us/sample
Epoch 58/90
84077/84077 - 8s - loss: 1.7016e-04 - val_loss: 1.6392e-04 - 8s/epoch - 94us/sample
Epoch 59/90
84077/84077 - 8s - loss: 1.6961e-04 - val_loss: 1.6149e-04 - 8s/epoch - 94us/sample
Epoch 60/90
84077/84077 - 8s - loss: 1.6888e-04 - val_loss: 1.6242e-04 - 8s/epoch - 94us/sample
Epoch 61/90
84077/84077 - 8s - loss: 1.6848e-04 - val_loss: 1.6129e-04 - 8s/epoch - 94us/sample
Epoch 62/90
84077/84077 - 8s - loss: 1.6728e-04 - val_loss: 1.6228e-04 - 8s/epoch - 94us/sample
Epoch 63/90
84077/84077 - 8s - loss: 1.6699e-04 - val_loss: 1.5933e-04 - 8s/epoch - 95us/sample
Epoch 64/90
84077/84077 - 8s - loss: 1.6582e-04 - val_loss: 1.5990e-04 - 8s/epoch - 94us/sample
Epoch 65/90
84077/84077 - 8s - loss: 1.6592e-04 - val_loss: 1.5992e-04 - 8s/epoch - 94us/sample
Epoch 66/90
84077/84077 - 8s - loss: 1.6443e-04 - val_loss: 1.5833e-04 - 8s/epoch - 94us/sample
Epoch 67/90
84077/84077 - 8s - loss: 1.6516e-04 - val_loss: 1.5990e-04 - 8s/epoch - 94us/sample
Epoch 68/90
84077/84077 - 8s - loss: 1.6397e-04 - val_loss: 1.5935e-04 - 8s/epoch - 94us/sample
Epoch 69/90
84077/84077 - 8s - loss: 1.6295e-04 - val_loss: 1.5813e-04 - 8s/epoch - 94us/sample
Epoch 70/90
84077/84077 - 8s - loss: 1.6296e-04 - val_loss: 1.5733e-04 - 8s/epoch - 95us/sample
Epoch 71/90
84077/84077 - 8s - loss: 1.6224e-04 - val_loss: 1.5670e-04 - 8s/epoch - 94us/sample
Epoch 72/90
84077/84077 - 8s - loss: 1.6214e-04 - val_loss: 1.5481e-04 - 8s/epoch - 94us/sample
Epoch 73/90
84077/84077 - 8s - loss: 1.6154e-04 - val_loss: 1.5560e-04 - 8s/epoch - 94us/sample
Epoch 74/90
84077/84077 - 8s - loss: 1.6121e-04 - val_loss: 1.5488e-04 - 8s/epoch - 94us/sample
Epoch 75/90
84077/84077 - 8s - loss: 1.6116e-04 - val_loss: 1.5949e-04 - 8s/epoch - 94us/sample
Epoch 76/90
84077/84077 - 8s - loss: 1.5995e-04 - val_loss: 1.5734e-04 - 8s/epoch - 94us/sample
Epoch 77/90
84077/84077 - 8s - loss: 1.5969e-04 - val_loss: 1.5589e-04 - 8s/epoch - 94us/sample
Epoch 78/90
84077/84077 - 8s - loss: 1.5928e-04 - val_loss: 1.5485e-04 - 8s/epoch - 94us/sample
Epoch 79/90
84077/84077 - 8s - loss: 1.5885e-04 - val_loss: 1.5341e-04 - 8s/epoch - 94us/sample
Epoch 80/90
84077/84077 - 8s - loss: 1.5881e-04 - val_loss: 1.6728e-04 - 8s/epoch - 94us/sample
Epoch 81/90
84077/84077 - 8s - loss: 1.5912e-04 - val_loss: 1.5221e-04 - 8s/epoch - 94us/sample
Epoch 82/90
84077/84077 - 8s - loss: 1.5801e-04 - val_loss: 1.5369e-04 - 8s/epoch - 94us/sample
Epoch 83/90
84077/84077 - 8s - loss: 1.5903e-04 - val_loss: 1.5340e-04 - 8s/epoch - 94us/sample
Epoch 84/90
84077/84077 - 8s - loss: 1.5768e-04 - val_loss: 1.5374e-04 - 8s/epoch - 94us/sample
Epoch 85/90
84077/84077 - 8s - loss: 1.5806e-04 - val_loss: 1.5303e-04 - 8s/epoch - 95us/sample
Epoch 86/90
84077/84077 - 8s - loss: 1.5739e-04 - val_loss: 1.5461e-04 - 8s/epoch - 94us/sample
Epoch 87/90
84077/84077 - 8s - loss: 1.5762e-04 - val_loss: 1.5177e-04 - 8s/epoch - 94us/sample
Epoch 88/90
84077/84077 - 8s - loss: 1.5704e-04 - val_loss: 1.5171e-04 - 8s/epoch - 94us/sample
Epoch 89/90
84077/84077 - 8s - loss: 1.5593e-04 - val_loss: 1.5431e-04 - 8s/epoch - 94us/sample
Epoch 90/90
84077/84077 - 8s - loss: 1.5583e-04 - val_loss: 1.5133e-04 - 8s/epoch - 94us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0001513253284179464
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:30:42.198279: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_18/outputlayer/BiasAdd' id:23524 op device:{requested: '', assigned: ''} def:{{{node decoder_model_18/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_18/outputlayer/MatMul, decoder_model_18/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.018342830973679346
cosine 0.018122939779693932
MAE: 0.0017644644706814017
RMSE: 0.00849971874461566
r2: 0.9437158306919163
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 90, 0.001, 0.3, 282, 0.0001558277221752877, 0.0001513253284179464, 0.018342830973679346, 0.018122939779693932, 0.0017644644706814017, 0.00849971874461566, 0.9437158306919163, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.001 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 1980)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 00:30:56.341117: W tensorflow/c/c_api.cc:291] Operation '{name:'training_38/Adam/batch_normalization_58/gamma/m/Assign' id:25323 op device:{requested: '', assigned: ''} def:{{{node training_38/Adam/batch_normalization_58/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_38/Adam/batch_normalization_58/gamma/m, training_38/Adam/batch_normalization_58/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:31:10.803640: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_19/mul' id:24808 op device:{requested: '', assigned: ''} def:{{{node loss_19/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_19/mul/x, loss_19/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 20s - loss: 0.0064 - val_loss: 0.0020 - 20s/epoch - 232us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0028 - val_loss: 0.0010 - 12s/epoch - 141us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0012 - val_loss: 9.9148e-04 - 12s/epoch - 141us/sample
Epoch 4/85
84077/84077 - 12s - loss: 8.6711e-04 - val_loss: 8.6603e-04 - 12s/epoch - 141us/sample
Epoch 5/85
84077/84077 - 12s - loss: 7.2596e-04 - val_loss: 5.3613e-04 - 12s/epoch - 141us/sample
Epoch 6/85
84077/84077 - 12s - loss: 5.9136e-04 - val_loss: 4.6128e-04 - 12s/epoch - 142us/sample
Epoch 7/85
84077/84077 - 12s - loss: 4.7862e-04 - val_loss: 3.9664e-04 - 12s/epoch - 141us/sample
Epoch 8/85
84077/84077 - 12s - loss: 4.1256e-04 - val_loss: 3.5158e-04 - 12s/epoch - 141us/sample
Epoch 9/85
84077/84077 - 12s - loss: 3.6980e-04 - val_loss: 3.2828e-04 - 12s/epoch - 141us/sample
Epoch 10/85
84077/84077 - 12s - loss: 3.3926e-04 - val_loss: 3.0432e-04 - 12s/epoch - 141us/sample
Epoch 11/85
84077/84077 - 12s - loss: 3.1483e-04 - val_loss: 2.8675e-04 - 12s/epoch - 141us/sample
Epoch 12/85
84077/84077 - 12s - loss: 2.9265e-04 - val_loss: 2.7863e-04 - 12s/epoch - 142us/sample
Epoch 13/85
84077/84077 - 12s - loss: 2.8714e-04 - val_loss: 2.5887e-04 - 12s/epoch - 142us/sample
Epoch 14/85
84077/84077 - 12s - loss: 2.7217e-04 - val_loss: 2.5328e-04 - 12s/epoch - 141us/sample
Epoch 15/85
84077/84077 - 12s - loss: 2.6330e-04 - val_loss: 2.4358e-04 - 12s/epoch - 141us/sample
Epoch 16/85
84077/84077 - 12s - loss: 2.5225e-04 - val_loss: 2.3975e-04 - 12s/epoch - 142us/sample
Epoch 17/85
84077/84077 - 12s - loss: 2.4356e-04 - val_loss: 2.3304e-04 - 12s/epoch - 142us/sample
Epoch 18/85
84077/84077 - 12s - loss: 2.3816e-04 - val_loss: 2.3065e-04 - 12s/epoch - 141us/sample
Epoch 19/85
84077/84077 - 12s - loss: 2.3365e-04 - val_loss: 2.2741e-04 - 12s/epoch - 142us/sample
Epoch 20/85
84077/84077 - 12s - loss: 2.3121e-04 - val_loss: 2.2568e-04 - 12s/epoch - 141us/sample
Epoch 21/85
84077/84077 - 12s - loss: 2.2642e-04 - val_loss: 2.1662e-04 - 12s/epoch - 142us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.2421e-04 - val_loss: 2.1365e-04 - 12s/epoch - 142us/sample
Epoch 23/85
84077/84077 - 12s - loss: 2.1903e-04 - val_loss: 2.1552e-04 - 12s/epoch - 141us/sample
Epoch 24/85
84077/84077 - 12s - loss: 2.1507e-04 - val_loss: 2.0883e-04 - 12s/epoch - 141us/sample
Epoch 25/85
84077/84077 - 12s - loss: 2.1332e-04 - val_loss: 2.0751e-04 - 12s/epoch - 142us/sample
Epoch 26/85
84077/84077 - 12s - loss: 2.0933e-04 - val_loss: 2.0804e-04 - 12s/epoch - 142us/sample
Epoch 27/85
84077/84077 - 12s - loss: 2.0664e-04 - val_loss: 2.0233e-04 - 12s/epoch - 142us/sample
Epoch 28/85
84077/84077 - 12s - loss: 2.0730e-04 - val_loss: 2.0213e-04 - 12s/epoch - 141us/sample
Epoch 29/85
84077/84077 - 12s - loss: 2.0135e-04 - val_loss: 1.9780e-04 - 12s/epoch - 142us/sample
Epoch 30/85
84077/84077 - 12s - loss: 2.0182e-04 - val_loss: 1.9628e-04 - 12s/epoch - 142us/sample
Epoch 31/85
84077/84077 - 12s - loss: 2.0078e-04 - val_loss: 2.0063e-04 - 12s/epoch - 141us/sample
Epoch 32/85
84077/84077 - 12s - loss: 1.9733e-04 - val_loss: 1.9662e-04 - 12s/epoch - 141us/sample
Epoch 33/85
84077/84077 - 12s - loss: 1.9609e-04 - val_loss: 1.9198e-04 - 12s/epoch - 143us/sample
Epoch 34/85
84077/84077 - 12s - loss: 1.9713e-04 - val_loss: 1.9307e-04 - 12s/epoch - 142us/sample
Epoch 35/85
84077/84077 - 12s - loss: 1.9325e-04 - val_loss: 1.9230e-04 - 12s/epoch - 142us/sample
Epoch 36/85
84077/84077 - 12s - loss: 1.9092e-04 - val_loss: 1.8708e-04 - 12s/epoch - 141us/sample
Epoch 37/85
84077/84077 - 12s - loss: 1.9058e-04 - val_loss: 1.8668e-04 - 12s/epoch - 141us/sample
Epoch 38/85
84077/84077 - 12s - loss: 1.9121e-04 - val_loss: 1.8572e-04 - 12s/epoch - 142us/sample
Epoch 39/85
84077/84077 - 12s - loss: 1.9018e-04 - val_loss: 1.8912e-04 - 12s/epoch - 142us/sample
Epoch 40/85
84077/84077 - 12s - loss: 1.9746e-04 - val_loss: 1.9280e-04 - 12s/epoch - 142us/sample
Epoch 41/85
84077/84077 - 12s - loss: 1.8815e-04 - val_loss: 1.8797e-04 - 12s/epoch - 141us/sample
Epoch 42/85
84077/84077 - 12s - loss: 1.8522e-04 - val_loss: 1.8539e-04 - 12s/epoch - 142us/sample
Epoch 43/85
84077/84077 - 12s - loss: 1.8976e-04 - val_loss: 1.8080e-04 - 12s/epoch - 143us/sample
Epoch 44/85
84077/84077 - 12s - loss: 1.8407e-04 - val_loss: 1.8626e-04 - 12s/epoch - 142us/sample
Epoch 45/85
84077/84077 - 12s - loss: 1.8496e-04 - val_loss: 1.8584e-04 - 12s/epoch - 141us/sample
Epoch 46/85
84077/84077 - 12s - loss: 1.8132e-04 - val_loss: 1.7899e-04 - 12s/epoch - 141us/sample
Epoch 47/85
84077/84077 - 12s - loss: 1.8309e-04 - val_loss: 1.7933e-04 - 12s/epoch - 141us/sample
Epoch 48/85
84077/84077 - 12s - loss: 1.8065e-04 - val_loss: 1.8251e-04 - 12s/epoch - 142us/sample
Epoch 49/85
84077/84077 - 12s - loss: 1.8069e-04 - val_loss: 1.7719e-04 - 12s/epoch - 142us/sample
Epoch 50/85
84077/84077 - 12s - loss: 1.7886e-04 - val_loss: 1.7790e-04 - 12s/epoch - 141us/sample
Epoch 51/85
84077/84077 - 12s - loss: 1.8116e-04 - val_loss: 1.7631e-04 - 12s/epoch - 141us/sample
Epoch 52/85
84077/84077 - 12s - loss: 1.7711e-04 - val_loss: 1.7331e-04 - 12s/epoch - 142us/sample
Epoch 53/85
84077/84077 - 12s - loss: 1.7720e-04 - val_loss: 1.7720e-04 - 12s/epoch - 142us/sample
Epoch 54/85
84077/84077 - 12s - loss: 1.7818e-04 - val_loss: 1.7570e-04 - 12s/epoch - 141us/sample
Epoch 55/85
84077/84077 - 12s - loss: 1.7654e-04 - val_loss: 1.7881e-04 - 12s/epoch - 141us/sample
Epoch 56/85
84077/84077 - 12s - loss: 1.7559e-04 - val_loss: 1.7764e-04 - 12s/epoch - 142us/sample
Epoch 57/85
84077/84077 - 12s - loss: 1.7533e-04 - val_loss: 1.7220e-04 - 12s/epoch - 142us/sample
Epoch 58/85
84077/84077 - 12s - loss: 1.7433e-04 - val_loss: 1.7484e-04 - 12s/epoch - 141us/sample
Epoch 59/85
84077/84077 - 12s - loss: 1.8120e-04 - val_loss: 1.6932e-04 - 12s/epoch - 141us/sample
Epoch 60/85
84077/84077 - 12s - loss: 1.7338e-04 - val_loss: 1.6942e-04 - 12s/epoch - 142us/sample
Epoch 61/85
84077/84077 - 12s - loss: 1.7311e-04 - val_loss: 1.7214e-04 - 12s/epoch - 142us/sample
Epoch 62/85
84077/84077 - 12s - loss: 1.7322e-04 - val_loss: 1.6983e-04 - 12s/epoch - 141us/sample
Epoch 63/85
84077/84077 - 12s - loss: 1.7261e-04 - val_loss: 1.7074e-04 - 12s/epoch - 141us/sample
Epoch 64/85
84077/84077 - 12s - loss: 1.7252e-04 - val_loss: 1.7216e-04 - 12s/epoch - 142us/sample
Epoch 65/85
84077/84077 - 12s - loss: 1.7422e-04 - val_loss: 1.6942e-04 - 12s/epoch - 142us/sample
Epoch 66/85
84077/84077 - 12s - loss: 1.7255e-04 - val_loss: 1.6843e-04 - 12s/epoch - 141us/sample
Epoch 67/85
84077/84077 - 12s - loss: 1.7043e-04 - val_loss: 1.8835e-04 - 12s/epoch - 141us/sample
Epoch 68/85
84077/84077 - 12s - loss: 1.7015e-04 - val_loss: 1.6835e-04 - 12s/epoch - 141us/sample
Epoch 69/85
84077/84077 - 12s - loss: 1.6960e-04 - val_loss: 1.7149e-04 - 12s/epoch - 142us/sample
Epoch 70/85
84077/84077 - 12s - loss: 1.6961e-04 - val_loss: 1.6959e-04 - 12s/epoch - 142us/sample
Epoch 71/85
84077/84077 - 12s - loss: 1.7073e-04 - val_loss: 1.6721e-04 - 12s/epoch - 142us/sample
Epoch 72/85
84077/84077 - 12s - loss: 1.6872e-04 - val_loss: 1.6777e-04 - 12s/epoch - 141us/sample
Epoch 73/85
84077/84077 - 12s - loss: 1.7317e-04 - val_loss: 1.7145e-04 - 12s/epoch - 142us/sample
Epoch 74/85
84077/84077 - 12s - loss: 1.6878e-04 - val_loss: 1.6819e-04 - 12s/epoch - 142us/sample
Epoch 75/85
84077/84077 - 12s - loss: 1.6730e-04 - val_loss: 1.6512e-04 - 12s/epoch - 142us/sample
Epoch 76/85
84077/84077 - 12s - loss: 1.7161e-04 - val_loss: 1.6627e-04 - 12s/epoch - 141us/sample
Epoch 77/85
84077/84077 - 12s - loss: 1.6971e-04 - val_loss: 1.7089e-04 - 12s/epoch - 142us/sample
Epoch 78/85
84077/84077 - 12s - loss: 1.7114e-04 - val_loss: 1.6568e-04 - 12s/epoch - 142us/sample
Epoch 79/85
84077/84077 - 12s - loss: 1.7227e-04 - val_loss: 1.6623e-04 - 12s/epoch - 142us/sample
Epoch 80/85
84077/84077 - 12s - loss: 1.7024e-04 - val_loss: 1.6840e-04 - 12s/epoch - 141us/sample
Epoch 81/85
84077/84077 - 12s - loss: 1.6611e-04 - val_loss: 1.6620e-04 - 12s/epoch - 142us/sample
Epoch 82/85
84077/84077 - 12s - loss: 1.6844e-04 - val_loss: 1.6467e-04 - 12s/epoch - 142us/sample
Epoch 83/85
84077/84077 - 12s - loss: 1.6640e-04 - val_loss: 1.6603e-04 - 12s/epoch - 142us/sample
Epoch 84/85
84077/84077 - 12s - loss: 1.6645e-04 - val_loss: 1.6656e-04 - 12s/epoch - 141us/sample
Epoch 85/85
84077/84077 - 12s - loss: 1.6550e-04 - val_loss: 1.6416e-04 - 12s/epoch - 141us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00016415881648196835
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:47:53.457635: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:24779 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02001621807946206
cosine 0.01977690030488619
MAE: 0.001785853499007424
RMSE: 0.008962699228854077
r2: 0.9373862170317853
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 85, 0.001, 0.3, 282, 0.00016550261026614444, 0.00016415881648196835, 0.02001621807946206, 0.01977690030488619, 0.001785853499007424, 0.008962699228854077, 0.9373862170317853, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 85 0.001 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1320)         1246080     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 1320)        5280        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 1320)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          372522      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          372522      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          1705477     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,701,881
Trainable params: 3,696,037
Non-trainable params: 5,844
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 00:48:08.059044: W tensorflow/c/c_api.cc:291] Operation '{name:'training_40/Adam/batch_normalization_62/gamma/m/Assign' id:26634 op device:{requested: '', assigned: ''} def:{{{node training_40/Adam/batch_normalization_62/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_40/Adam/batch_normalization_62/gamma/m, training_40/Adam/batch_normalization_62/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:48:19.019592: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:26070 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 16s - loss: 0.0059 - val_loss: 8.8899e-04 - 16s/epoch - 191us/sample
Epoch 2/85
84077/84077 - 8s - loss: 7.4416e-04 - val_loss: 7.7820e-04 - 8s/epoch - 97us/sample
Epoch 3/85
84077/84077 - 8s - loss: 9.1527e-04 - val_loss: 6.1976e-04 - 8s/epoch - 97us/sample
Epoch 4/85
84077/84077 - 8s - loss: 5.9837e-04 - val_loss: 5.1652e-04 - 8s/epoch - 97us/sample
Epoch 5/85
84077/84077 - 8s - loss: 6.0988e-04 - val_loss: 4.4977e-04 - 8s/epoch - 97us/sample
Epoch 6/85
84077/84077 - 8s - loss: 4.0783e-04 - val_loss: 3.7294e-04 - 8s/epoch - 97us/sample
Epoch 7/85
84077/84077 - 8s - loss: 3.5153e-04 - val_loss: 3.2604e-04 - 8s/epoch - 98us/sample
Epoch 8/85
84077/84077 - 8s - loss: 3.2454e-04 - val_loss: 2.7025e-04 - 8s/epoch - 98us/sample
Epoch 9/85
84077/84077 - 8s - loss: 2.8960e-04 - val_loss: 2.4969e-04 - 8s/epoch - 98us/sample
Epoch 10/85
84077/84077 - 8s - loss: 2.6439e-04 - val_loss: 2.4333e-04 - 8s/epoch - 97us/sample
Epoch 11/85
84077/84077 - 8s - loss: 2.5111e-04 - val_loss: 2.0459e-04 - 8s/epoch - 97us/sample
Epoch 12/85
84077/84077 - 8s - loss: 2.1721e-04 - val_loss: 1.8401e-04 - 8s/epoch - 97us/sample
Epoch 13/85
84077/84077 - 8s - loss: 1.9959e-04 - val_loss: 1.7228e-04 - 8s/epoch - 97us/sample
Epoch 14/85
84077/84077 - 8s - loss: 1.8902e-04 - val_loss: 1.6263e-04 - 8s/epoch - 98us/sample
Epoch 15/85
84077/84077 - 8s - loss: 1.7926e-04 - val_loss: 1.5409e-04 - 8s/epoch - 97us/sample
Epoch 16/85
84077/84077 - 8s - loss: 1.7062e-04 - val_loss: 1.4776e-04 - 8s/epoch - 98us/sample
Epoch 17/85
84077/84077 - 8s - loss: 1.6540e-04 - val_loss: 1.4119e-04 - 8s/epoch - 97us/sample
Epoch 18/85
84077/84077 - 8s - loss: 1.5827e-04 - val_loss: 1.3738e-04 - 8s/epoch - 97us/sample
Epoch 19/85
84077/84077 - 8s - loss: 1.5525e-04 - val_loss: 1.3585e-04 - 8s/epoch - 97us/sample
Epoch 20/85
84077/84077 - 8s - loss: 1.5102e-04 - val_loss: 1.3501e-04 - 8s/epoch - 98us/sample
Epoch 21/85
84077/84077 - 8s - loss: 1.4772e-04 - val_loss: 1.3129e-04 - 8s/epoch - 98us/sample
Epoch 22/85
84077/84077 - 8s - loss: 1.4502e-04 - val_loss: 1.2920e-04 - 8s/epoch - 98us/sample
Epoch 23/85
84077/84077 - 8s - loss: 1.4309e-04 - val_loss: 1.2814e-04 - 8s/epoch - 97us/sample
Epoch 24/85
84077/84077 - 8s - loss: 1.4024e-04 - val_loss: 1.2494e-04 - 8s/epoch - 97us/sample
Epoch 25/85
84077/84077 - 8s - loss: 1.3863e-04 - val_loss: 1.2474e-04 - 8s/epoch - 97us/sample
Epoch 26/85
84077/84077 - 8s - loss: 1.3666e-04 - val_loss: 1.2073e-04 - 8s/epoch - 98us/sample
Epoch 27/85
84077/84077 - 8s - loss: 1.3554e-04 - val_loss: 1.2029e-04 - 8s/epoch - 97us/sample
Epoch 28/85
84077/84077 - 8s - loss: 1.3347e-04 - val_loss: 1.1961e-04 - 8s/epoch - 98us/sample
Epoch 29/85
84077/84077 - 8s - loss: 1.3257e-04 - val_loss: 1.2001e-04 - 8s/epoch - 97us/sample
Epoch 30/85
84077/84077 - 8s - loss: 1.3180e-04 - val_loss: 1.1903e-04 - 8s/epoch - 97us/sample
Epoch 31/85
84077/84077 - 8s - loss: 1.3006e-04 - val_loss: 1.1769e-04 - 8s/epoch - 97us/sample
Epoch 32/85
84077/84077 - 8s - loss: 1.2878e-04 - val_loss: 1.1688e-04 - 8s/epoch - 98us/sample
Epoch 33/85
84077/84077 - 8s - loss: 1.2828e-04 - val_loss: 1.1687e-04 - 8s/epoch - 98us/sample
Epoch 34/85
84077/84077 - 8s - loss: 1.2695e-04 - val_loss: 1.1487e-04 - 8s/epoch - 98us/sample
Epoch 35/85
84077/84077 - 8s - loss: 1.2627e-04 - val_loss: 1.1437e-04 - 8s/epoch - 98us/sample
Epoch 36/85
84077/84077 - 8s - loss: 1.2576e-04 - val_loss: 1.1263e-04 - 8s/epoch - 97us/sample
Epoch 37/85
84077/84077 - 8s - loss: 1.2444e-04 - val_loss: 1.1327e-04 - 8s/epoch - 97us/sample
Epoch 38/85
84077/84077 - 8s - loss: 1.2390e-04 - val_loss: 1.1293e-04 - 8s/epoch - 98us/sample
Epoch 39/85
84077/84077 - 8s - loss: 1.2323e-04 - val_loss: 1.1172e-04 - 8s/epoch - 98us/sample
Epoch 40/85
84077/84077 - 8s - loss: 1.2281e-04 - val_loss: 1.1109e-04 - 8s/epoch - 98us/sample
Epoch 41/85
84077/84077 - 8s - loss: 1.2190e-04 - val_loss: 1.1089e-04 - 8s/epoch - 98us/sample
Epoch 42/85
84077/84077 - 8s - loss: 1.2271e-04 - val_loss: 1.0990e-04 - 8s/epoch - 97us/sample
Epoch 43/85
84077/84077 - 8s - loss: 1.2052e-04 - val_loss: 1.1094e-04 - 8s/epoch - 97us/sample
Epoch 44/85
84077/84077 - 8s - loss: 1.2040e-04 - val_loss: 1.0997e-04 - 8s/epoch - 98us/sample
Epoch 45/85
84077/84077 - 8s - loss: 1.2018e-04 - val_loss: 1.0961e-04 - 8s/epoch - 98us/sample
Epoch 46/85
84077/84077 - 8s - loss: 1.1962e-04 - val_loss: 1.0964e-04 - 8s/epoch - 98us/sample
Epoch 47/85
84077/84077 - 8s - loss: 1.1899e-04 - val_loss: 1.0882e-04 - 8s/epoch - 98us/sample
Epoch 48/85
84077/84077 - 8s - loss: 1.1840e-04 - val_loss: 1.0855e-04 - 8s/epoch - 97us/sample
Epoch 49/85
84077/84077 - 8s - loss: 1.1786e-04 - val_loss: 1.0694e-04 - 8s/epoch - 98us/sample
Epoch 50/85
84077/84077 - 8s - loss: 1.1756e-04 - val_loss: 1.0719e-04 - 8s/epoch - 98us/sample
Epoch 51/85
84077/84077 - 8s - loss: 1.1807e-04 - val_loss: 1.0652e-04 - 8s/epoch - 98us/sample
Epoch 52/85
84077/84077 - 8s - loss: 1.1660e-04 - val_loss: 1.0656e-04 - 8s/epoch - 98us/sample
Epoch 53/85
84077/84077 - 8s - loss: 1.1616e-04 - val_loss: 1.0852e-04 - 8s/epoch - 98us/sample
Epoch 54/85
84077/84077 - 8s - loss: 1.1573e-04 - val_loss: 1.0751e-04 - 8s/epoch - 97us/sample
Epoch 55/85
84077/84077 - 8s - loss: 1.1542e-04 - val_loss: 1.0800e-04 - 8s/epoch - 97us/sample
Epoch 56/85
84077/84077 - 8s - loss: 1.1553e-04 - val_loss: 1.0590e-04 - 8s/epoch - 98us/sample
Epoch 57/85
84077/84077 - 8s - loss: 1.1519e-04 - val_loss: 1.0849e-04 - 8s/epoch - 98us/sample
Epoch 58/85
84077/84077 - 8s - loss: 1.1465e-04 - val_loss: 1.0604e-04 - 8s/epoch - 98us/sample
Epoch 59/85
84077/84077 - 8s - loss: 1.1429e-04 - val_loss: 1.0479e-04 - 8s/epoch - 97us/sample
Epoch 60/85
84077/84077 - 8s - loss: 1.1396e-04 - val_loss: 1.0567e-04 - 8s/epoch - 97us/sample
Epoch 61/85
84077/84077 - 8s - loss: 1.1388e-04 - val_loss: 1.0564e-04 - 8s/epoch - 97us/sample
Epoch 62/85
84077/84077 - 8s - loss: 1.1353e-04 - val_loss: 1.0552e-04 - 8s/epoch - 98us/sample
Epoch 63/85
84077/84077 - 8s - loss: 1.1332e-04 - val_loss: 1.0746e-04 - 8s/epoch - 98us/sample
Epoch 64/85
84077/84077 - 8s - loss: 1.1302e-04 - val_loss: 1.0453e-04 - 8s/epoch - 98us/sample
Epoch 65/85
84077/84077 - 8s - loss: 1.1262e-04 - val_loss: 1.0437e-04 - 8s/epoch - 97us/sample
Epoch 66/85
84077/84077 - 8s - loss: 1.1330e-04 - val_loss: 1.0464e-04 - 8s/epoch - 97us/sample
Epoch 67/85
84077/84077 - 8s - loss: 1.1230e-04 - val_loss: 1.0293e-04 - 8s/epoch - 97us/sample
Epoch 68/85
84077/84077 - 8s - loss: 1.1202e-04 - val_loss: 1.0362e-04 - 8s/epoch - 98us/sample
Epoch 69/85
84077/84077 - 8s - loss: 1.1155e-04 - val_loss: 1.0278e-04 - 8s/epoch - 98us/sample
Epoch 70/85
84077/84077 - 8s - loss: 1.1142e-04 - val_loss: 1.0315e-04 - 8s/epoch - 98us/sample
Epoch 71/85
84077/84077 - 8s - loss: 1.1116e-04 - val_loss: 1.0224e-04 - 8s/epoch - 98us/sample
Epoch 72/85
84077/84077 - 8s - loss: 1.1117e-04 - val_loss: 1.0320e-04 - 8s/epoch - 98us/sample
Epoch 73/85
84077/84077 - 8s - loss: 1.1089e-04 - val_loss: 1.0223e-04 - 8s/epoch - 97us/sample
Epoch 74/85
84077/84077 - 8s - loss: 1.1054e-04 - val_loss: 1.0330e-04 - 8s/epoch - 97us/sample
Epoch 75/85
84077/84077 - 8s - loss: 1.1052e-04 - val_loss: 1.0300e-04 - 8s/epoch - 98us/sample
Epoch 76/85
84077/84077 - 8s - loss: 1.1026e-04 - val_loss: 1.0221e-04 - 8s/epoch - 99us/sample
Epoch 77/85
84077/84077 - 8s - loss: 1.1026e-04 - val_loss: 1.0295e-04 - 8s/epoch - 98us/sample
Epoch 78/85
84077/84077 - 8s - loss: 1.0992e-04 - val_loss: 1.0298e-04 - 8s/epoch - 98us/sample
Epoch 79/85
84077/84077 - 8s - loss: 1.0959e-04 - val_loss: 1.0163e-04 - 8s/epoch - 97us/sample
Epoch 80/85
84077/84077 - 8s - loss: 1.0998e-04 - val_loss: 1.0168e-04 - 8s/epoch - 97us/sample
Epoch 81/85
84077/84077 - 8s - loss: 1.0932e-04 - val_loss: 1.0218e-04 - 8s/epoch - 97us/sample
Epoch 82/85
84077/84077 - 8s - loss: 1.0908e-04 - val_loss: 1.0222e-04 - 8s/epoch - 98us/sample
Epoch 83/85
84077/84077 - 8s - loss: 1.0869e-04 - val_loss: 1.0286e-04 - 8s/epoch - 98us/sample
Epoch 84/85
84077/84077 - 8s - loss: 1.0887e-04 - val_loss: 1.0171e-04 - 8s/epoch - 98us/sample
Epoch 85/85
84077/84077 - 8s - loss: 1.0872e-04 - val_loss: 1.0193e-04 - 8s/epoch - 97us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00010193011161532402
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:59:50.990306: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_20/outputlayer/BiasAdd' id:26034 op device:{requested: '', assigned: ''} def:{{{node decoder_model_20/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_20/outputlayer/MatMul, decoder_model_20/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023539166913429027
cosine 0.02327864794548269
MAE: 0.002373894322913622
RMSE: 0.008886476613937948
r2: 0.9388088495151283
RMSE zero-vector: 0.04004287452915337
['1.4custom_VAE', 'logcosh', 64, 85, 0.001, 0.3, 282, 0.00010871581100605034, 0.00010193011161532402, 0.023539166913429027, 0.02327864794548269, 0.002373894322913622, 0.008886476613937948, 0.9388088495151283, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 64 0] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_63 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_63 (ReLU)                (None, 1980)         0           ['batch_normalization_63[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 01:00:07.398605: W tensorflow/c/c_api.cc:291] Operation '{name:'training_42/Adam/dense_dec1_21/bias/m/Assign' id:27940 op device:{requested: '', assigned: ''} def:{{{node training_42/Adam/dense_dec1_21/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_42/Adam/dense_dec1_21/bias/m, training_42/Adam/dense_dec1_21/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:00:19.549591: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_21/mul' id:27367 op device:{requested: '', assigned: ''} def:{{{node loss_21/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_21/mul/x, loss_21/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0763 - val_loss: 0.0736 - 18s/epoch - 213us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0762 - val_loss: 0.0712 - 9s/epoch - 105us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0687 - val_loss: 0.0679 - 9s/epoch - 104us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0684 - val_loss: 0.0732 - 9s/epoch - 104us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0686 - val_loss: 0.0684 - 9s/epoch - 105us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0690 - val_loss: 0.0742 - 9s/epoch - 105us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0698 - val_loss: 0.0680 - 9s/epoch - 105us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.0672 - val_loss: 0.0675 - 9s/epoch - 105us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 13/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 105us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 18/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 19/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 21/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 22/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 23/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 24/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 25/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 26/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 104us/sample
Epoch 27/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 29/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 30/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 31/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 32/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 33/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 35/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 36/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 37/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 40/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 41/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 42/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 43/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 45/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 46/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 47/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 53/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 54/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 58/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 59/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 60/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 64/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 65/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 69/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 70/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 71/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 72/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 77/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 78/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 83/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 84/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 85/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 86/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 87/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 106us/sample
Epoch 89/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 105us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.06734574904688878
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:13:28.182463: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_21/outputlayer/BiasAdd' id:27319 op device:{requested: '', assigned: ''} def:{{{node decoder_model_21/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_21/outputlayer/MatMul, decoder_model_21/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.2932535028804473
cosine 1.1867506071450495
MAE: 5.360155651068691
RMSE: 6.289779530302896
r2: -29089.899107497637
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'binary_crossentropy', 64, 90, 0.001, 0.3, 282, 0.06683694033418598, 0.06734574904688878, 1.2932535028804473, 1.1867506071450495, 5.360155651068691, 6.289779530302896, -29089.899107497637, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 2
Fitness    = 595.8845894037719
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 595.8845894037719.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [595.8845894037719, 595.8845894037719]
[1.9 85 0.0008 32 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1791)         1690704     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_66 (BatchN  (None, 1791)        7164        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_66 (ReLU)                (None, 1791)         0           ['batch_normalization_66[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          505344      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          505344      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2284807     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 4,993,363
Trainable params: 4,985,635
Non-trainable params: 7,728
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 01:13:45.253740: W tensorflow/c/c_api.cc:291] Operation '{name:'training_44/Adam/batch_normalization_66/beta/m/Assign' id:29159 op device:{requested: '', assigned: ''} def:{{{node training_44/Adam/batch_normalization_66/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_44/Adam/batch_normalization_66/beta/m, training_44/Adam/batch_normalization_66/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:14:00.740426: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_22/mul' id:28685 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0201 - val_loss: 0.0015 - 22s/epoch - 258us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0041 - val_loss: 0.0010 - 12s/epoch - 148us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.2763 - val_loss: 0.0013 - 12s/epoch - 147us/sample
Epoch 4/85
84077/84077 - 12s - loss: 0.0065 - val_loss: 0.0011 - 12s/epoch - 147us/sample
Epoch 5/85
84077/84077 - 12s - loss: 0.0017 - val_loss: 0.0011 - 12s/epoch - 147us/sample
Epoch 6/85
84077/84077 - 12s - loss: 0.0080 - val_loss: 0.0035 - 12s/epoch - 147us/sample
Epoch 7/85
84077/84077 - 12s - loss: 0.0015 - val_loss: 9.0075e-04 - 12s/epoch - 147us/sample
Epoch 8/85
84077/84077 - 12s - loss: 0.0010 - val_loss: 6.7674e-04 - 12s/epoch - 148us/sample
Epoch 9/85
84077/84077 - 12s - loss: 7.4580e-04 - val_loss: 0.0010 - 12s/epoch - 147us/sample
Epoch 10/85
84077/84077 - 12s - loss: 6.6208e-04 - val_loss: 5.7029e-04 - 12s/epoch - 148us/sample
Epoch 11/85
84077/84077 - 12s - loss: 8.4798e-04 - val_loss: 0.0012 - 12s/epoch - 147us/sample
Epoch 12/85
84077/84077 - 12s - loss: 0.0015 - val_loss: 5.7711e-04 - 12s/epoch - 148us/sample
Epoch 13/85
84077/84077 - 12s - loss: 5.7636e-04 - val_loss: 5.2735e-04 - 12s/epoch - 147us/sample
Epoch 14/85
84077/84077 - 12s - loss: 5.1027e-04 - val_loss: 4.4587e-04 - 12s/epoch - 147us/sample
Epoch 15/85
84077/84077 - 12s - loss: 4.8789e-04 - val_loss: 4.2711e-04 - 12s/epoch - 147us/sample
Epoch 16/85
84077/84077 - 12s - loss: 4.6644e-04 - val_loss: 4.2375e-04 - 12s/epoch - 147us/sample
Epoch 17/85
84077/84077 - 12s - loss: 4.3951e-04 - val_loss: 3.8821e-04 - 12s/epoch - 148us/sample
Epoch 18/85
84077/84077 - 12s - loss: 4.0353e-04 - val_loss: 3.7196e-04 - 12s/epoch - 147us/sample
Epoch 19/85
84077/84077 - 12s - loss: 3.9004e-04 - val_loss: 3.6318e-04 - 12s/epoch - 148us/sample
Epoch 20/85
84077/84077 - 12s - loss: 3.7482e-04 - val_loss: 3.7485e-04 - 12s/epoch - 148us/sample
Epoch 21/85
84077/84077 - 12s - loss: 3.6625e-04 - val_loss: 3.4136e-04 - 12s/epoch - 148us/sample
Epoch 22/85
84077/84077 - 12s - loss: 3.5802e-04 - val_loss: 3.4059e-04 - 12s/epoch - 148us/sample
Epoch 23/85
84077/84077 - 12s - loss: 3.5170e-04 - val_loss: 3.3910e-04 - 12s/epoch - 148us/sample
Epoch 24/85
84077/84077 - 12s - loss: 3.4593e-04 - val_loss: 3.2681e-04 - 12s/epoch - 147us/sample
Epoch 25/85
84077/84077 - 12s - loss: 3.4087e-04 - val_loss: 3.2892e-04 - 12s/epoch - 148us/sample
Epoch 26/85
84077/84077 - 12s - loss: 3.3598e-04 - val_loss: 3.1770e-04 - 12s/epoch - 148us/sample
Epoch 27/85
84077/84077 - 12s - loss: 3.3169e-04 - val_loss: 3.2340e-04 - 12s/epoch - 147us/sample
Epoch 28/85
84077/84077 - 12s - loss: 3.2987e-04 - val_loss: 3.1842e-04 - 12s/epoch - 148us/sample
Epoch 29/85
84077/84077 - 12s - loss: 3.2635e-04 - val_loss: 3.1555e-04 - 12s/epoch - 148us/sample
Epoch 30/85
84077/84077 - 12s - loss: 3.2385e-04 - val_loss: 3.1934e-04 - 12s/epoch - 147us/sample
Epoch 31/85
84077/84077 - 12s - loss: 3.2152e-04 - val_loss: 3.1190e-04 - 12s/epoch - 148us/sample
Epoch 32/85
84077/84077 - 12s - loss: 3.1978e-04 - val_loss: 3.1646e-04 - 12s/epoch - 147us/sample
Epoch 33/85
84077/84077 - 12s - loss: 3.1753e-04 - val_loss: 3.0323e-04 - 12s/epoch - 147us/sample
Epoch 34/85
84077/84077 - 12s - loss: 3.1515e-04 - val_loss: 3.0972e-04 - 12s/epoch - 148us/sample
Epoch 35/85
84077/84077 - 12s - loss: 3.1252e-04 - val_loss: 3.0520e-04 - 12s/epoch - 148us/sample
Epoch 36/85
84077/84077 - 12s - loss: 3.1070e-04 - val_loss: 3.0700e-04 - 12s/epoch - 148us/sample
Epoch 37/85
84077/84077 - 12s - loss: 3.0958e-04 - val_loss: 3.0090e-04 - 12s/epoch - 148us/sample
Epoch 38/85
84077/84077 - 12s - loss: 3.0825e-04 - val_loss: 3.0310e-04 - 12s/epoch - 147us/sample
Epoch 39/85
84077/84077 - 12s - loss: 3.0711e-04 - val_loss: 3.0242e-04 - 12s/epoch - 148us/sample
Epoch 40/85
84077/84077 - 12s - loss: 3.0580e-04 - val_loss: 3.0031e-04 - 12s/epoch - 148us/sample
Epoch 41/85
84077/84077 - 12s - loss: 3.0481e-04 - val_loss: 3.0133e-04 - 12s/epoch - 148us/sample
Epoch 42/85
84077/84077 - 12s - loss: 3.0339e-04 - val_loss: 2.9794e-04 - 12s/epoch - 147us/sample
Epoch 43/85
84077/84077 - 12s - loss: 3.0220e-04 - val_loss: 2.9620e-04 - 12s/epoch - 148us/sample
Epoch 44/85
84077/84077 - 13s - loss: 3.0099e-04 - val_loss: 2.9906e-04 - 13s/epoch - 149us/sample
Epoch 45/85
84077/84077 - 12s - loss: 3.0014e-04 - val_loss: 2.9535e-04 - 12s/epoch - 147us/sample
Epoch 46/85
84077/84077 - 12s - loss: 2.9883e-04 - val_loss: 2.9221e-04 - 12s/epoch - 147us/sample
Epoch 47/85
84077/84077 - 12s - loss: 2.9882e-04 - val_loss: 2.9146e-04 - 12s/epoch - 147us/sample
Epoch 48/85
84077/84077 - 12s - loss: 2.9750e-04 - val_loss: 2.9230e-04 - 12s/epoch - 148us/sample
Epoch 49/85
84077/84077 - 12s - loss: 2.9610e-04 - val_loss: 2.9005e-04 - 12s/epoch - 148us/sample
Epoch 50/85
84077/84077 - 12s - loss: 2.9666e-04 - val_loss: 2.9012e-04 - 12s/epoch - 147us/sample
Epoch 51/85
84077/84077 - 12s - loss: 2.9549e-04 - val_loss: 2.8264e-04 - 12s/epoch - 147us/sample
Epoch 52/85
84077/84077 - 12s - loss: 2.9318e-04 - val_loss: 2.9134e-04 - 12s/epoch - 148us/sample
Epoch 53/85
84077/84077 - 12s - loss: 2.9266e-04 - val_loss: 2.8779e-04 - 12s/epoch - 149us/sample
Epoch 54/85
84077/84077 - 12s - loss: 2.9160e-04 - val_loss: 2.8592e-04 - 12s/epoch - 148us/sample
Epoch 55/85
84077/84077 - 12s - loss: 2.9064e-04 - val_loss: 2.8457e-04 - 12s/epoch - 148us/sample
Epoch 56/85
84077/84077 - 12s - loss: 2.9569e-04 - val_loss: 2.8359e-04 - 12s/epoch - 147us/sample
Epoch 57/85
84077/84077 - 12s - loss: 2.9010e-04 - val_loss: 2.8539e-04 - 12s/epoch - 149us/sample
Epoch 58/85
84077/84077 - 12s - loss: 2.8980e-04 - val_loss: 2.8164e-04 - 12s/epoch - 148us/sample
Epoch 59/85
84077/84077 - 12s - loss: 2.8897e-04 - val_loss: 2.8181e-04 - 12s/epoch - 148us/sample
Epoch 60/85
84077/84077 - 12s - loss: 2.8810e-04 - val_loss: 2.8458e-04 - 12s/epoch - 147us/sample
Epoch 61/85
84077/84077 - 12s - loss: 2.8783e-04 - val_loss: 2.8167e-04 - 12s/epoch - 148us/sample
Epoch 62/85
84077/84077 - 12s - loss: 2.8750e-04 - val_loss: 2.7853e-04 - 12s/epoch - 148us/sample
Epoch 63/85
84077/84077 - 12s - loss: 2.8678e-04 - val_loss: 2.8029e-04 - 12s/epoch - 148us/sample
Epoch 64/85
84077/84077 - 12s - loss: 2.8594e-04 - val_loss: 2.8479e-04 - 12s/epoch - 147us/sample
Epoch 65/85
84077/84077 - 12s - loss: 2.8563e-04 - val_loss: 2.8034e-04 - 12s/epoch - 148us/sample
Epoch 66/85
84077/84077 - 12s - loss: 2.8532e-04 - val_loss: 2.8116e-04 - 12s/epoch - 148us/sample
Epoch 67/85
84077/84077 - 12s - loss: 2.8421e-04 - val_loss: 2.8086e-04 - 12s/epoch - 148us/sample
Epoch 68/85
84077/84077 - 12s - loss: 2.8470e-04 - val_loss: 2.8299e-04 - 12s/epoch - 147us/sample
Epoch 69/85
84077/84077 - 12s - loss: 2.8309e-04 - val_loss: 2.7810e-04 - 12s/epoch - 147us/sample
Epoch 70/85
84077/84077 - 12s - loss: 2.8347e-04 - val_loss: 2.7516e-04 - 12s/epoch - 148us/sample
Epoch 71/85
84077/84077 - 12s - loss: 2.8246e-04 - val_loss: 2.7645e-04 - 12s/epoch - 147us/sample
Epoch 72/85
84077/84077 - 12s - loss: 2.8213e-04 - val_loss: 2.7696e-04 - 12s/epoch - 148us/sample
Epoch 73/85
84077/84077 - 12s - loss: 2.8186e-04 - val_loss: 2.7899e-04 - 12s/epoch - 148us/sample
Epoch 74/85
84077/84077 - 12s - loss: 2.8105e-04 - val_loss: 2.7694e-04 - 12s/epoch - 147us/sample
Epoch 75/85
84077/84077 - 12s - loss: 2.8078e-04 - val_loss: 2.7882e-04 - 12s/epoch - 149us/sample
Epoch 76/85
84077/84077 - 12s - loss: 2.8102e-04 - val_loss: 2.7127e-04 - 12s/epoch - 147us/sample
Epoch 77/85
84077/84077 - 12s - loss: 2.8063e-04 - val_loss: 2.7541e-04 - 12s/epoch - 148us/sample
Epoch 78/85
84077/84077 - 12s - loss: 2.8000e-04 - val_loss: 2.7354e-04 - 12s/epoch - 147us/sample
Epoch 79/85
84077/84077 - 12s - loss: 2.7980e-04 - val_loss: 2.7341e-04 - 12s/epoch - 147us/sample
Epoch 80/85
84077/84077 - 12s - loss: 2.7904e-04 - val_loss: 2.7352e-04 - 12s/epoch - 148us/sample
Epoch 81/85
84077/84077 - 12s - loss: 2.7940e-04 - val_loss: 2.7257e-04 - 12s/epoch - 148us/sample
Epoch 82/85
84077/84077 - 12s - loss: 2.7817e-04 - val_loss: 2.7144e-04 - 12s/epoch - 148us/sample
Epoch 83/85
84077/84077 - 12s - loss: 2.7800e-04 - val_loss: 2.7306e-04 - 12s/epoch - 147us/sample
Epoch 84/85
84077/84077 - 12s - loss: 2.7757e-04 - val_loss: 2.6959e-04 - 12s/epoch - 148us/sample
Epoch 85/85
84077/84077 - 12s - loss: 2.7725e-04 - val_loss: 2.6923e-04 - 12s/epoch - 148us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00026923056867816437
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:31:27.123123: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_22/outputlayer/BiasAdd' id:28656 op device:{requested: '', assigned: ''} def:{{{node decoder_model_22/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_22/outputlayer/MatMul, decoder_model_22/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.045195767917696444
cosine 0.0446221579423883
MAE: 0.0028079545807412924
RMSE: 0.013889028256656718
r2: 0.8493302167626057
RMSE zero-vector: 0.04004287452915337
['1.9custom_VAE', 'mse', 32, 85, 0.0008, 0.3, 282, 0.00027724665274381395, 0.00026923056867816437, 0.045195767917696444, 0.0446221579423883, 0.0028079545807412924, 0.013889028256656718, 0.8493302167626057, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0008 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_69 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_69 (ReLU)                (None, 1980)         0           ['batch_normalization_69[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 01:31:44.232277: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_23/kernel/Assign' id:29618 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_23/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_23/kernel, bottleneck_zlog_23/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:31:59.675439: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_23/mul' id:29940 op device:{requested: '', assigned: ''} def:{{{node loss_23/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_23/mul/x, loss_23/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0069 - val_loss: 0.0072 - 22s/epoch - 260us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0019 - val_loss: 0.0011 - 12s/epoch - 148us/sample
Epoch 3/85
84077/84077 - 12s - loss: 0.0010 - val_loss: 8.6694e-04 - 12s/epoch - 147us/sample
Epoch 4/85
84077/84077 - 12s - loss: 7.4126e-04 - val_loss: 5.3004e-04 - 12s/epoch - 147us/sample
Epoch 5/85
84077/84077 - 12s - loss: 5.4955e-04 - val_loss: 4.1186e-04 - 12s/epoch - 147us/sample
Epoch 6/85
84077/84077 - 12s - loss: 4.3583e-04 - val_loss: 3.4161e-04 - 12s/epoch - 147us/sample
Epoch 7/85
84077/84077 - 12s - loss: 3.6225e-04 - val_loss: 2.9438e-04 - 12s/epoch - 147us/sample
Epoch 8/85
84077/84077 - 12s - loss: 3.1879e-04 - val_loss: 2.5988e-04 - 12s/epoch - 148us/sample
Epoch 9/85
84077/84077 - 12s - loss: 2.9076e-04 - val_loss: 2.4815e-04 - 12s/epoch - 147us/sample
Epoch 10/85
84077/84077 - 12s - loss: 2.6944e-04 - val_loss: 2.3253e-04 - 12s/epoch - 147us/sample
Epoch 11/85
84077/84077 - 12s - loss: 2.5576e-04 - val_loss: 2.3411e-04 - 12s/epoch - 147us/sample
Epoch 12/85
84077/84077 - 12s - loss: 2.4386e-04 - val_loss: 2.2138e-04 - 12s/epoch - 147us/sample
Epoch 13/85
84077/84077 - 12s - loss: 2.3754e-04 - val_loss: 2.1401e-04 - 12s/epoch - 147us/sample
Epoch 14/85
84077/84077 - 12s - loss: 2.2768e-04 - val_loss: 2.0952e-04 - 12s/epoch - 148us/sample
Epoch 15/85
84077/84077 - 12s - loss: 2.2798e-04 - val_loss: 3.8731e-04 - 12s/epoch - 148us/sample
Epoch 16/85
84077/84077 - 12s - loss: 2.2884e-04 - val_loss: 1.9929e-04 - 12s/epoch - 147us/sample
Epoch 17/85
84077/84077 - 12s - loss: 2.1403e-04 - val_loss: 1.9772e-04 - 12s/epoch - 147us/sample
Epoch 18/85
84077/84077 - 12s - loss: 2.1020e-04 - val_loss: 1.9677e-04 - 12s/epoch - 147us/sample
Epoch 19/85
84077/84077 - 12s - loss: 2.0698e-04 - val_loss: 1.9132e-04 - 12s/epoch - 148us/sample
Epoch 20/85
84077/84077 - 12s - loss: 2.0454e-04 - val_loss: 1.9003e-04 - 12s/epoch - 147us/sample
Epoch 21/85
84077/84077 - 12s - loss: 2.0122e-04 - val_loss: 1.9402e-04 - 12s/epoch - 147us/sample
Epoch 22/85
84077/84077 - 12s - loss: 2.0096e-04 - val_loss: 1.8648e-04 - 12s/epoch - 147us/sample
Epoch 23/85
84077/84077 - 12s - loss: 1.9763e-04 - val_loss: 1.8551e-04 - 12s/epoch - 148us/sample
Epoch 24/85
84077/84077 - 12s - loss: 1.9648e-04 - val_loss: 1.8089e-04 - 12s/epoch - 147us/sample
Epoch 25/85
84077/84077 - 12s - loss: 1.9345e-04 - val_loss: 1.8423e-04 - 12s/epoch - 148us/sample
Epoch 26/85
84077/84077 - 12s - loss: 1.9148e-04 - val_loss: 1.8094e-04 - 12s/epoch - 147us/sample
Epoch 27/85
84077/84077 - 12s - loss: 1.9085e-04 - val_loss: 1.8542e-04 - 12s/epoch - 147us/sample
Epoch 28/85
84077/84077 - 12s - loss: 1.8980e-04 - val_loss: 1.8572e-04 - 12s/epoch - 148us/sample
Epoch 29/85
84077/84077 - 12s - loss: 1.9506e-04 - val_loss: 1.7744e-04 - 12s/epoch - 148us/sample
Epoch 30/85
84077/84077 - 12s - loss: 1.8606e-04 - val_loss: 1.7404e-04 - 12s/epoch - 147us/sample
Epoch 31/85
84077/84077 - 12s - loss: 1.8674e-04 - val_loss: 1.7600e-04 - 12s/epoch - 147us/sample
Epoch 32/85
84077/84077 - 12s - loss: 1.8405e-04 - val_loss: 1.7383e-04 - 12s/epoch - 147us/sample
Epoch 33/85
84077/84077 - 12s - loss: 1.8193e-04 - val_loss: 1.7169e-04 - 12s/epoch - 148us/sample
Epoch 34/85
84077/84077 - 12s - loss: 1.8193e-04 - val_loss: 1.7203e-04 - 12s/epoch - 147us/sample
Epoch 35/85
84077/84077 - 12s - loss: 1.8165e-04 - val_loss: 1.6924e-04 - 12s/epoch - 147us/sample
Epoch 36/85
84077/84077 - 12s - loss: 1.8052e-04 - val_loss: 1.6848e-04 - 12s/epoch - 147us/sample
Epoch 37/85
84077/84077 - 12s - loss: 1.8339e-04 - val_loss: 1.6677e-04 - 12s/epoch - 147us/sample
Epoch 38/85
84077/84077 - 12s - loss: 1.7914e-04 - val_loss: 1.6775e-04 - 12s/epoch - 148us/sample
Epoch 39/85
84077/84077 - 12s - loss: 1.7671e-04 - val_loss: 1.6549e-04 - 12s/epoch - 148us/sample
Epoch 40/85
84077/84077 - 12s - loss: 1.7634e-04 - val_loss: 1.6716e-04 - 12s/epoch - 148us/sample
Epoch 41/85
84077/84077 - 12s - loss: 1.7746e-04 - val_loss: 1.6604e-04 - 12s/epoch - 147us/sample
Epoch 42/85
84077/84077 - 12s - loss: 1.7520e-04 - val_loss: 1.6585e-04 - 12s/epoch - 147us/sample
Epoch 43/85
84077/84077 - 12s - loss: 1.8158e-04 - val_loss: 1.7150e-04 - 12s/epoch - 148us/sample
Epoch 44/85
84077/84077 - 12s - loss: 1.8012e-04 - val_loss: 1.6297e-04 - 12s/epoch - 148us/sample
Epoch 45/85
84077/84077 - 12s - loss: 1.7904e-04 - val_loss: 1.6289e-04 - 12s/epoch - 147us/sample
Epoch 46/85
84077/84077 - 12s - loss: 1.7373e-04 - val_loss: 1.6062e-04 - 12s/epoch - 147us/sample
Epoch 47/85
84077/84077 - 12s - loss: 1.7321e-04 - val_loss: 1.6059e-04 - 12s/epoch - 147us/sample
Epoch 48/85
84077/84077 - 12s - loss: 1.7408e-04 - val_loss: 1.5771e-04 - 12s/epoch - 148us/sample
Epoch 49/85
84077/84077 - 12s - loss: 1.7219e-04 - val_loss: 1.5846e-04 - 12s/epoch - 147us/sample
Epoch 50/85
84077/84077 - 12s - loss: 1.7140e-04 - val_loss: 1.5870e-04 - 12s/epoch - 147us/sample
Epoch 51/85
84077/84077 - 12s - loss: 1.6935e-04 - val_loss: 1.5583e-04 - 12s/epoch - 147us/sample
Epoch 52/85
84077/84077 - 12s - loss: 1.7068e-04 - val_loss: 1.6015e-04 - 12s/epoch - 147us/sample
Epoch 53/85
84077/84077 - 12s - loss: 1.7022e-04 - val_loss: 1.5511e-04 - 12s/epoch - 148us/sample
Epoch 54/85
84077/84077 - 12s - loss: 1.6879e-04 - val_loss: 1.5790e-04 - 12s/epoch - 147us/sample
Epoch 55/85
84077/84077 - 12s - loss: 1.6855e-04 - val_loss: 1.5551e-04 - 12s/epoch - 147us/sample
Epoch 56/85
84077/84077 - 12s - loss: 1.6677e-04 - val_loss: 1.5563e-04 - 12s/epoch - 147us/sample
Epoch 57/85
84077/84077 - 12s - loss: 1.6823e-04 - val_loss: 1.5429e-04 - 12s/epoch - 148us/sample
Epoch 58/85
84077/84077 - 12s - loss: 1.6624e-04 - val_loss: 1.5450e-04 - 12s/epoch - 148us/sample
Epoch 59/85
84077/84077 - 12s - loss: 1.6652e-04 - val_loss: 1.5326e-04 - 12s/epoch - 148us/sample
Epoch 60/85
84077/84077 - 12s - loss: 1.6636e-04 - val_loss: 1.5618e-04 - 12s/epoch - 147us/sample
Epoch 61/85
84077/84077 - 12s - loss: 1.6682e-04 - val_loss: 1.5795e-04 - 12s/epoch - 147us/sample
Epoch 62/85
84077/84077 - 12s - loss: 1.6474e-04 - val_loss: 1.5326e-04 - 12s/epoch - 148us/sample
Epoch 63/85
84077/84077 - 12s - loss: 1.6529e-04 - val_loss: 1.5443e-04 - 12s/epoch - 148us/sample
Epoch 64/85
84077/84077 - 12s - loss: 1.6680e-04 - val_loss: 1.5441e-04 - 12s/epoch - 147us/sample
Epoch 65/85
84077/84077 - 12s - loss: 1.6413e-04 - val_loss: 1.5042e-04 - 12s/epoch - 147us/sample
Epoch 66/85
84077/84077 - 12s - loss: 1.6424e-04 - val_loss: 1.5560e-04 - 12s/epoch - 147us/sample
Epoch 67/85
84077/84077 - 12s - loss: 1.6508e-04 - val_loss: 1.5539e-04 - 12s/epoch - 148us/sample
Epoch 68/85
84077/84077 - 12s - loss: 1.6917e-04 - val_loss: 1.5117e-04 - 12s/epoch - 148us/sample
Epoch 69/85
84077/84077 - 12s - loss: 1.6368e-04 - val_loss: 1.5030e-04 - 12s/epoch - 147us/sample
Epoch 70/85
84077/84077 - 12s - loss: 1.6296e-04 - val_loss: 1.5461e-04 - 12s/epoch - 147us/sample
Epoch 71/85
84077/84077 - 12s - loss: 1.6316e-04 - val_loss: 1.5225e-04 - 12s/epoch - 148us/sample
Epoch 72/85
84077/84077 - 12s - loss: 1.6224e-04 - val_loss: 1.5043e-04 - 12s/epoch - 148us/sample
Epoch 73/85
84077/84077 - 12s - loss: 1.6349e-04 - val_loss: 1.5098e-04 - 12s/epoch - 147us/sample
Epoch 74/85
84077/84077 - 12s - loss: 1.6115e-04 - val_loss: 1.4763e-04 - 12s/epoch - 147us/sample
Epoch 75/85
84077/84077 - 12s - loss: 1.6089e-04 - val_loss: 1.4978e-04 - 12s/epoch - 147us/sample
Epoch 76/85
84077/84077 - 12s - loss: 1.6086e-04 - val_loss: 1.4759e-04 - 12s/epoch - 147us/sample
Epoch 77/85
84077/84077 - 12s - loss: 1.5994e-04 - val_loss: 1.5458e-04 - 12s/epoch - 148us/sample
Epoch 78/85
84077/84077 - 12s - loss: 1.6110e-04 - val_loss: 1.4932e-04 - 12s/epoch - 147us/sample
Epoch 79/85
84077/84077 - 12s - loss: 1.6020e-04 - val_loss: 1.4896e-04 - 12s/epoch - 147us/sample
Epoch 80/85
84077/84077 - 12s - loss: 1.5992e-04 - val_loss: 1.4845e-04 - 12s/epoch - 147us/sample
Epoch 81/85
84077/84077 - 12s - loss: 1.5976e-04 - val_loss: 1.4920e-04 - 12s/epoch - 147us/sample
Epoch 82/85
84077/84077 - 12s - loss: 1.6786e-04 - val_loss: 1.5134e-04 - 12s/epoch - 148us/sample
Epoch 83/85
84077/84077 - 12s - loss: 1.6004e-04 - val_loss: 1.4915e-04 - 12s/epoch - 147us/sample
Epoch 84/85
84077/84077 - 12s - loss: 1.5877e-04 - val_loss: 1.4973e-04 - 12s/epoch - 148us/sample
Epoch 85/85
84077/84077 - 12s - loss: 1.5940e-04 - val_loss: 1.4631e-04 - 12s/epoch - 147us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00014631327916785785
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:49:23.989795: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_23/outputlayer/BiasAdd' id:29911 op device:{requested: '', assigned: ''} def:{{{node decoder_model_23/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_23/outputlayer/MatMul, decoder_model_23/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.017179384337564073
cosine 0.016970486853319942
MAE: 0.0017474822289824754
RMSE: 0.007834595948731767
r2: 0.9522108737785497
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 85, 0.0008, 0.3, 282, 0.00015939526129050461, 0.00014631327916785785, 0.017179384337564073, 0.016970486853319942, 0.0017474822289824754, 0.007834595948731767, 0.9522108737785497, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0012000000000000001 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_72 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_72 (ReLU)                (None, 1980)         0           ['batch_normalization_72[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 01:49:41.346547: W tensorflow/c/c_api.cc:291] Operation '{name:'training_48/Adam/batch_normalization_72/beta/v/Assign' id:31783 op device:{requested: '', assigned: ''} def:{{{node training_48/Adam/batch_normalization_72/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_48/Adam/batch_normalization_72/beta/v, training_48/Adam/batch_normalization_72/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:49:53.318589: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_24/mul' id:31195 op device:{requested: '', assigned: ''} def:{{{node loss_24/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_24/mul/x, loss_24/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 18s - loss: 0.0093 - val_loss: 0.0017 - 18s/epoch - 218us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0018 - val_loss: 0.0014 - 9s/epoch - 102us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.1370 - val_loss: 0.0090 - 9s/epoch - 102us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0025 - val_loss: 0.0015 - 9s/epoch - 102us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 103us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0016 - 9s/epoch - 103us/sample
Epoch 7/90
84077/84077 - 9s - loss: 9.5522e-04 - val_loss: 7.7052e-04 - 9s/epoch - 103us/sample
Epoch 8/90
84077/84077 - 9s - loss: 7.6880e-04 - val_loss: 6.5772e-04 - 9s/epoch - 103us/sample
Epoch 9/90
84077/84077 - 9s - loss: 6.7791e-04 - val_loss: 5.5641e-04 - 9s/epoch - 103us/sample
Epoch 10/90
84077/84077 - 9s - loss: 6.2380e-04 - val_loss: 5.9937e-04 - 9s/epoch - 102us/sample
Epoch 11/90
84077/84077 - 9s - loss: 5.5710e-04 - val_loss: 7.7460e-04 - 9s/epoch - 103us/sample
Epoch 12/90
84077/84077 - 9s - loss: 7.2931e-04 - val_loss: 6.0977e-04 - 9s/epoch - 103us/sample
Epoch 13/90
84077/84077 - 9s - loss: 5.2685e-04 - val_loss: 3.7479e-04 - 9s/epoch - 104us/sample
Epoch 14/90
84077/84077 - 9s - loss: 4.2395e-04 - val_loss: 4.0224e-04 - 9s/epoch - 103us/sample
Epoch 15/90
84077/84077 - 9s - loss: 3.8177e-04 - val_loss: 3.5788e-04 - 9s/epoch - 103us/sample
Epoch 16/90
84077/84077 - 9s - loss: 3.5955e-04 - val_loss: 3.2327e-04 - 9s/epoch - 103us/sample
Epoch 17/90
84077/84077 - 9s - loss: 3.2855e-04 - val_loss: 2.9129e-04 - 9s/epoch - 103us/sample
Epoch 18/90
84077/84077 - 9s - loss: 3.0869e-04 - val_loss: 2.7301e-04 - 9s/epoch - 104us/sample
Epoch 19/90
84077/84077 - 9s - loss: 2.9702e-04 - val_loss: 2.6221e-04 - 9s/epoch - 103us/sample
Epoch 20/90
84077/84077 - 9s - loss: 2.7899e-04 - val_loss: 2.5148e-04 - 9s/epoch - 103us/sample
Epoch 21/90
84077/84077 - 9s - loss: 2.6702e-04 - val_loss: 2.4828e-04 - 9s/epoch - 103us/sample
Epoch 22/90
84077/84077 - 9s - loss: 2.5679e-04 - val_loss: 2.4279e-04 - 9s/epoch - 103us/sample
Epoch 23/90
84077/84077 - 9s - loss: 2.4790e-04 - val_loss: 2.2726e-04 - 9s/epoch - 103us/sample
Epoch 24/90
84077/84077 - 9s - loss: 2.4036e-04 - val_loss: 2.2605e-04 - 9s/epoch - 104us/sample
Epoch 25/90
84077/84077 - 9s - loss: 2.3437e-04 - val_loss: 2.1259e-04 - 9s/epoch - 103us/sample
Epoch 26/90
84077/84077 - 9s - loss: 2.2836e-04 - val_loss: 2.1019e-04 - 9s/epoch - 103us/sample
Epoch 27/90
84077/84077 - 9s - loss: 2.2264e-04 - val_loss: 2.0414e-04 - 9s/epoch - 103us/sample
Epoch 28/90
84077/84077 - 9s - loss: 2.1898e-04 - val_loss: 2.0203e-04 - 9s/epoch - 103us/sample
Epoch 29/90
84077/84077 - 9s - loss: 2.1444e-04 - val_loss: 2.0282e-04 - 9s/epoch - 103us/sample
Epoch 30/90
84077/84077 - 9s - loss: 2.1186e-04 - val_loss: 1.9694e-04 - 9s/epoch - 103us/sample
Epoch 31/90
84077/84077 - 9s - loss: 2.0777e-04 - val_loss: 1.9619e-04 - 9s/epoch - 104us/sample
Epoch 32/90
84077/84077 - 9s - loss: 2.0532e-04 - val_loss: 1.9559e-04 - 9s/epoch - 103us/sample
Epoch 33/90
84077/84077 - 9s - loss: 2.0229e-04 - val_loss: 1.8931e-04 - 9s/epoch - 103us/sample
Epoch 34/90
84077/84077 - 9s - loss: 2.0001e-04 - val_loss: 1.9077e-04 - 9s/epoch - 103us/sample
Epoch 35/90
84077/84077 - 9s - loss: 1.9717e-04 - val_loss: 1.8601e-04 - 9s/epoch - 103us/sample
Epoch 36/90
84077/84077 - 9s - loss: 1.9631e-04 - val_loss: 1.8854e-04 - 9s/epoch - 103us/sample
Epoch 37/90
84077/84077 - 9s - loss: 1.9412e-04 - val_loss: 1.8502e-04 - 9s/epoch - 104us/sample
Epoch 38/90
84077/84077 - 9s - loss: 1.9236e-04 - val_loss: 1.8731e-04 - 9s/epoch - 103us/sample
Epoch 39/90
84077/84077 - 9s - loss: 1.9091e-04 - val_loss: 1.8765e-04 - 9s/epoch - 103us/sample
Epoch 40/90
84077/84077 - 9s - loss: 1.8882e-04 - val_loss: 1.8272e-04 - 9s/epoch - 103us/sample
Epoch 41/90
84077/84077 - 9s - loss: 1.8736e-04 - val_loss: 1.8343e-04 - 9s/epoch - 103us/sample
Epoch 42/90
84077/84077 - 9s - loss: 1.8668e-04 - val_loss: 1.8187e-04 - 9s/epoch - 103us/sample
Epoch 43/90
84077/84077 - 9s - loss: 1.8491e-04 - val_loss: 1.8007e-04 - 9s/epoch - 103us/sample
Epoch 44/90
84077/84077 - 9s - loss: 1.8390e-04 - val_loss: 1.7921e-04 - 9s/epoch - 104us/sample
Epoch 45/90
84077/84077 - 9s - loss: 1.8278e-04 - val_loss: 1.7388e-04 - 9s/epoch - 103us/sample
Epoch 46/90
84077/84077 - 9s - loss: 1.8167e-04 - val_loss: 1.7683e-04 - 9s/epoch - 103us/sample
Epoch 47/90
84077/84077 - 9s - loss: 1.8033e-04 - val_loss: 1.7796e-04 - 9s/epoch - 103us/sample
Epoch 48/90
84077/84077 - 9s - loss: 1.7967e-04 - val_loss: 1.7641e-04 - 9s/epoch - 103us/sample
Epoch 49/90
84077/84077 - 9s - loss: 1.7942e-04 - val_loss: 1.7725e-04 - 9s/epoch - 104us/sample
Epoch 50/90
84077/84077 - 9s - loss: 1.7785e-04 - val_loss: 1.7528e-04 - 9s/epoch - 104us/sample
Epoch 51/90
84077/84077 - 9s - loss: 1.7679e-04 - val_loss: 1.7371e-04 - 9s/epoch - 103us/sample
Epoch 52/90
84077/84077 - 9s - loss: 1.7652e-04 - val_loss: 1.7441e-04 - 9s/epoch - 103us/sample
Epoch 53/90
84077/84077 - 9s - loss: 1.7532e-04 - val_loss: 1.7300e-04 - 9s/epoch - 103us/sample
Epoch 54/90
84077/84077 - 9s - loss: 1.7441e-04 - val_loss: 1.7253e-04 - 9s/epoch - 103us/sample
Epoch 55/90
84077/84077 - 9s - loss: 1.7428e-04 - val_loss: 1.7221e-04 - 9s/epoch - 103us/sample
Epoch 56/90
84077/84077 - 9s - loss: 1.7348e-04 - val_loss: 1.7342e-04 - 9s/epoch - 103us/sample
Epoch 57/90
84077/84077 - 9s - loss: 1.7224e-04 - val_loss: 1.7137e-04 - 9s/epoch - 104us/sample
Epoch 58/90
84077/84077 - 9s - loss: 1.7211e-04 - val_loss: 1.7126e-04 - 9s/epoch - 103us/sample
Epoch 59/90
84077/84077 - 9s - loss: 1.7170e-04 - val_loss: 1.6850e-04 - 9s/epoch - 103us/sample
Epoch 60/90
84077/84077 - 9s - loss: 1.7068e-04 - val_loss: 1.7017e-04 - 9s/epoch - 103us/sample
Epoch 61/90
84077/84077 - 9s - loss: 1.7036e-04 - val_loss: 1.6910e-04 - 9s/epoch - 103us/sample
Epoch 62/90
84077/84077 - 9s - loss: 1.7026e-04 - val_loss: 1.6801e-04 - 9s/epoch - 103us/sample
Epoch 63/90
84077/84077 - 9s - loss: 1.6945e-04 - val_loss: 1.6868e-04 - 9s/epoch - 103us/sample
Epoch 64/90
84077/84077 - 9s - loss: 1.6886e-04 - val_loss: 1.6763e-04 - 9s/epoch - 104us/sample
Epoch 65/90
84077/84077 - 9s - loss: 1.6842e-04 - val_loss: 1.6722e-04 - 9s/epoch - 103us/sample
Epoch 66/90
84077/84077 - 9s - loss: 1.6821e-04 - val_loss: 1.6834e-04 - 9s/epoch - 103us/sample
Epoch 67/90
84077/84077 - 9s - loss: 1.6727e-04 - val_loss: 1.6619e-04 - 9s/epoch - 103us/sample
Epoch 68/90
84077/84077 - 9s - loss: 1.6727e-04 - val_loss: 1.6802e-04 - 9s/epoch - 103us/sample
Epoch 69/90
84077/84077 - 9s - loss: 1.6681e-04 - val_loss: 1.6806e-04 - 9s/epoch - 103us/sample
Epoch 70/90
84077/84077 - 9s - loss: 1.6595e-04 - val_loss: 1.6533e-04 - 9s/epoch - 103us/sample
Epoch 71/90
84077/84077 - 9s - loss: 1.6632e-04 - val_loss: 1.6503e-04 - 9s/epoch - 104us/sample
Epoch 72/90
84077/84077 - 9s - loss: 1.6527e-04 - val_loss: 1.6571e-04 - 9s/epoch - 103us/sample
Epoch 73/90
84077/84077 - 9s - loss: 1.6610e-04 - val_loss: 1.6431e-04 - 9s/epoch - 103us/sample
Epoch 74/90
84077/84077 - 9s - loss: 1.6504e-04 - val_loss: 1.6623e-04 - 9s/epoch - 103us/sample
Epoch 75/90
84077/84077 - 9s - loss: 1.6404e-04 - val_loss: 1.6424e-04 - 9s/epoch - 103us/sample
Epoch 76/90
84077/84077 - 9s - loss: 1.6429e-04 - val_loss: 1.6265e-04 - 9s/epoch - 103us/sample
Epoch 77/90
84077/84077 - 9s - loss: 1.6383e-04 - val_loss: 1.6362e-04 - 9s/epoch - 104us/sample
Epoch 78/90
84077/84077 - 9s - loss: 1.6372e-04 - val_loss: 1.6472e-04 - 9s/epoch - 103us/sample
Epoch 79/90
84077/84077 - 9s - loss: 1.6334e-04 - val_loss: 1.6485e-04 - 9s/epoch - 103us/sample
Epoch 80/90
84077/84077 - 9s - loss: 1.6280e-04 - val_loss: 1.6232e-04 - 9s/epoch - 103us/sample
Epoch 81/90
84077/84077 - 9s - loss: 1.6296e-04 - val_loss: 1.6160e-04 - 9s/epoch - 103us/sample
Epoch 82/90
84077/84077 - 9s - loss: 1.6220e-04 - val_loss: 1.6241e-04 - 9s/epoch - 103us/sample
Epoch 83/90
84077/84077 - 9s - loss: 1.6181e-04 - val_loss: 1.6101e-04 - 9s/epoch - 104us/sample
Epoch 84/90
84077/84077 - 9s - loss: 1.6188e-04 - val_loss: 1.6235e-04 - 9s/epoch - 103us/sample
Epoch 85/90
84077/84077 - 9s - loss: 1.6210e-04 - val_loss: 1.6203e-04 - 9s/epoch - 103us/sample
Epoch 86/90
84077/84077 - 9s - loss: 1.6096e-04 - val_loss: 1.6113e-04 - 9s/epoch - 103us/sample
Epoch 87/90
84077/84077 - 9s - loss: 1.6084e-04 - val_loss: 1.6033e-04 - 9s/epoch - 103us/sample
Epoch 88/90
84077/84077 - 9s - loss: 1.6093e-04 - val_loss: 1.6044e-04 - 9s/epoch - 103us/sample
Epoch 89/90
84077/84077 - 9s - loss: 1.6074e-04 - val_loss: 1.6103e-04 - 9s/epoch - 103us/sample
Epoch 90/90
84077/84077 - 9s - loss: 1.6024e-04 - val_loss: 1.6020e-04 - 9s/epoch - 104us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0001601995267571875
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:02:47.941710: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_24/outputlayer/BiasAdd' id:31166 op device:{requested: '', assigned: ''} def:{{{node decoder_model_24/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_24/outputlayer/MatMul, decoder_model_24/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.019054359245541948
cosine 0.0188201917668396
MAE: 0.001833234324383011
RMSE: 0.009045081925336866
r2: 0.936285834061861
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.0012000000000000001, 0.3, 282, 0.00016024295823332025, 0.0001601995267571875, 0.019054359245541948, 0.0188201917668396, 0.001833234324383011, 0.009045081925336866, 0.936285834061861, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 90 0.001 64 0] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_75 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_75 (ReLU)                (None, 2074)         0           ['batch_normalization_75[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          585150      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          585150      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2632897     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,769,349
Trainable params: 5,760,489
Non-trainable params: 8,860
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 02:03:05.651767: W tensorflow/c/c_api.cc:291] Operation '{name:'training_50/Adam/batch_normalization_77/gamma/v/Assign' id:33187 op device:{requested: '', assigned: ''} def:{{{node training_50/Adam/batch_normalization_77/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_50/Adam/batch_normalization_77/gamma/v, training_50/Adam/batch_normalization_77/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:03:18.409757: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_25/mul' id:32469 op device:{requested: '', assigned: ''} def:{{{node loss_25/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul/x, loss_25/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 19s - loss: 0.0716 - val_loss: 0.0695 - 19s/epoch - 230us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0685 - val_loss: 0.0679 - 9s/epoch - 108us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0678 - val_loss: 0.0677 - 9s/epoch - 108us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0674 - 9s/epoch - 108us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 107us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 108us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 108us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 108us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 108us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 13/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 18/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 19/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 21/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 22/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 23/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 24/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 25/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 26/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 27/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 29/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 30/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 31/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 32/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 33/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 35/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 36/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 37/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 40/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 41/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 42/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 43/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 45/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 46/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 47/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 53/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 54/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 58/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 59/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 60/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 64/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 65/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 69/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 70/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 71/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 72/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 77/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 78/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 107us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 83/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 84/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 85/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 86/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 87/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
Epoch 89/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 108us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.06734575127360908
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:16:49.939934: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_25/outputlayer/BiasAdd' id:32421 op device:{requested: '', assigned: ''} def:{{{node decoder_model_25/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_25/outputlayer/MatMul, decoder_model_25/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1177489966465441
cosine 1.1326406569400085
MAE: 5.7356040224039075
RMSE: 5.99315662011478
r2: -26320.828134387986
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'binary_crossentropy', 64, 90, 0.001, 0.3, 282, 0.06683692937431697, 0.06734575127360908, 1.1177489966465441, 1.1326406569400085, 5.7356040224039075, 5.99315662011478, -26320.828134387986, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 32 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_78 (BatchN  (None, 1886)        7544        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_78 (ReLU)                (None, 1886)         0           ['batch_normalization_78[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 02:17:08.037539: W tensorflow/c/c_api.cc:291] Operation '{name:'training_52/Adam/batch_normalization_78/beta/m/Assign' id:34261 op device:{requested: '', assigned: ''} def:{{{node training_52/Adam/batch_normalization_78/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_52/Adam/batch_normalization_78/beta/m, training_52/Adam/batch_normalization_78/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:17:24.217400: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_26/mul' id:33787 op device:{requested: '', assigned: ''} def:{{{node loss_26/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_26/mul/x, loss_26/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0104 - val_loss: 0.0041 - 23s/epoch - 276us/sample
Epoch 2/85
84077/84077 - 13s - loss: 0.2961 - val_loss: 0.0029 - 13s/epoch - 151us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0065 - val_loss: 8.5853e-04 - 13s/epoch - 150us/sample
Epoch 4/85
84077/84077 - 13s - loss: 0.0012 - val_loss: 0.0015 - 13s/epoch - 150us/sample
Epoch 5/85
84077/84077 - 13s - loss: 0.0012 - val_loss: 7.9538e-04 - 13s/epoch - 150us/sample
Epoch 6/85
84077/84077 - 13s - loss: 8.5012e-04 - val_loss: 0.0019 - 13s/epoch - 150us/sample
Epoch 7/85
84077/84077 - 13s - loss: 0.5536 - val_loss: 0.0255 - 13s/epoch - 151us/sample
Epoch 8/85
84077/84077 - 13s - loss: 6.4554e-04 - val_loss: 5.8207e-04 - 13s/epoch - 151us/sample
Epoch 9/85
84077/84077 - 13s - loss: 6.0254e-04 - val_loss: 4.8153e-04 - 13s/epoch - 150us/sample
Epoch 10/85
84077/84077 - 13s - loss: 5.7283e-04 - val_loss: 0.0014 - 13s/epoch - 150us/sample
Epoch 11/85
84077/84077 - 13s - loss: 4.9271e-04 - val_loss: 4.1596e-04 - 13s/epoch - 151us/sample
Epoch 12/85
84077/84077 - 13s - loss: 4.4193e-04 - val_loss: 3.9974e-04 - 13s/epoch - 150us/sample
Epoch 13/85
84077/84077 - 13s - loss: 4.3241e-04 - val_loss: 3.8613e-04 - 13s/epoch - 151us/sample
Epoch 14/85
84077/84077 - 13s - loss: 3.8815e-04 - val_loss: 3.7212e-04 - 13s/epoch - 150us/sample
Epoch 15/85
84077/84077 - 13s - loss: 3.6984e-04 - val_loss: 3.6813e-04 - 13s/epoch - 151us/sample
Epoch 16/85
84077/84077 - 13s - loss: 3.5426e-04 - val_loss: 3.5579e-04 - 13s/epoch - 151us/sample
Epoch 17/85
84077/84077 - 13s - loss: 3.4136e-04 - val_loss: 3.4441e-04 - 13s/epoch - 150us/sample
Epoch 18/85
84077/84077 - 13s - loss: 3.2868e-04 - val_loss: 3.2781e-04 - 13s/epoch - 150us/sample
Epoch 19/85
84077/84077 - 13s - loss: 3.1855e-04 - val_loss: 3.2090e-04 - 13s/epoch - 151us/sample
Epoch 20/85
84077/84077 - 13s - loss: 3.0964e-04 - val_loss: 3.1475e-04 - 13s/epoch - 151us/sample
Epoch 21/85
84077/84077 - 13s - loss: 3.0216e-04 - val_loss: 3.0745e-04 - 13s/epoch - 150us/sample
Epoch 22/85
84077/84077 - 13s - loss: 2.9624e-04 - val_loss: 3.0605e-04 - 13s/epoch - 150us/sample
Epoch 23/85
84077/84077 - 13s - loss: 2.9199e-04 - val_loss: 3.0645e-04 - 13s/epoch - 150us/sample
Epoch 24/85
84077/84077 - 13s - loss: 2.8812e-04 - val_loss: 3.0190e-04 - 13s/epoch - 151us/sample
Epoch 25/85
84077/84077 - 13s - loss: 2.8442e-04 - val_loss: 2.9919e-04 - 13s/epoch - 151us/sample
Epoch 26/85
84077/84077 - 13s - loss: 2.8197e-04 - val_loss: 2.9948e-04 - 13s/epoch - 150us/sample
Epoch 27/85
84077/84077 - 13s - loss: 2.7891e-04 - val_loss: 2.9096e-04 - 13s/epoch - 150us/sample
Epoch 28/85
84077/84077 - 13s - loss: 2.7651e-04 - val_loss: 2.8958e-04 - 13s/epoch - 151us/sample
Epoch 29/85
84077/84077 - 13s - loss: 2.7428e-04 - val_loss: 2.8736e-04 - 13s/epoch - 151us/sample
Epoch 30/85
84077/84077 - 13s - loss: 2.7270e-04 - val_loss: 2.8708e-04 - 13s/epoch - 150us/sample
Epoch 31/85
84077/84077 - 13s - loss: 2.7068e-04 - val_loss: 2.8780e-04 - 13s/epoch - 151us/sample
Epoch 32/85
84077/84077 - 13s - loss: 2.6931e-04 - val_loss: 2.8123e-04 - 13s/epoch - 152us/sample
Epoch 33/85
84077/84077 - 13s - loss: 2.6745e-04 - val_loss: 2.8778e-04 - 13s/epoch - 151us/sample
Epoch 34/85
84077/84077 - 13s - loss: 2.6585e-04 - val_loss: 2.7569e-04 - 13s/epoch - 150us/sample
Epoch 35/85
84077/84077 - 13s - loss: 2.6432e-04 - val_loss: 2.7636e-04 - 13s/epoch - 151us/sample
Epoch 36/85
84077/84077 - 13s - loss: 2.6265e-04 - val_loss: 2.7524e-04 - 13s/epoch - 152us/sample
Epoch 37/85
84077/84077 - 13s - loss: 2.6110e-04 - val_loss: 2.6939e-04 - 13s/epoch - 151us/sample
Epoch 38/85
84077/84077 - 13s - loss: 2.6028e-04 - val_loss: 2.7044e-04 - 13s/epoch - 150us/sample
Epoch 39/85
84077/84077 - 13s - loss: 2.5860e-04 - val_loss: 2.7487e-04 - 13s/epoch - 151us/sample
Epoch 40/85
84077/84077 - 13s - loss: 2.5770e-04 - val_loss: 2.6823e-04 - 13s/epoch - 151us/sample
Epoch 41/85
84077/84077 - 13s - loss: 2.5707e-04 - val_loss: 2.7024e-04 - 13s/epoch - 152us/sample
Epoch 42/85
84077/84077 - 13s - loss: 2.5490e-04 - val_loss: 2.6747e-04 - 13s/epoch - 150us/sample
Epoch 43/85
84077/84077 - 13s - loss: 2.5345e-04 - val_loss: 2.6172e-04 - 13s/epoch - 150us/sample
Epoch 44/85
84077/84077 - 13s - loss: 2.5274e-04 - val_loss: 2.6433e-04 - 13s/epoch - 151us/sample
Epoch 45/85
84077/84077 - 13s - loss: 2.5125e-04 - val_loss: 2.6537e-04 - 13s/epoch - 151us/sample
Epoch 46/85
84077/84077 - 13s - loss: 2.5037e-04 - val_loss: 2.6440e-04 - 13s/epoch - 152us/sample
Epoch 47/85
84077/84077 - 13s - loss: 2.4921e-04 - val_loss: 2.6093e-04 - 13s/epoch - 150us/sample
Epoch 48/85
84077/84077 - 13s - loss: 2.4839e-04 - val_loss: 2.6973e-04 - 13s/epoch - 151us/sample
Epoch 49/85
84077/84077 - 13s - loss: 2.4766e-04 - val_loss: 2.5830e-04 - 13s/epoch - 151us/sample
Epoch 50/85
84077/84077 - 13s - loss: 2.4713e-04 - val_loss: 2.6065e-04 - 13s/epoch - 151us/sample
Epoch 51/85
84077/84077 - 13s - loss: 2.4611e-04 - val_loss: 2.6013e-04 - 13s/epoch - 150us/sample
Epoch 52/85
84077/84077 - 13s - loss: 2.4558e-04 - val_loss: 2.6027e-04 - 13s/epoch - 150us/sample
Epoch 53/85
84077/84077 - 13s - loss: 2.4542e-04 - val_loss: 2.5869e-04 - 13s/epoch - 151us/sample
Epoch 54/85
84077/84077 - 13s - loss: 2.4434e-04 - val_loss: 2.6075e-04 - 13s/epoch - 151us/sample
Epoch 55/85
84077/84077 - 13s - loss: 2.4381e-04 - val_loss: 2.5873e-04 - 13s/epoch - 151us/sample
Epoch 56/85
84077/84077 - 13s - loss: 2.4348e-04 - val_loss: 2.5750e-04 - 13s/epoch - 150us/sample
Epoch 57/85
84077/84077 - 13s - loss: 2.4260e-04 - val_loss: 2.5895e-04 - 13s/epoch - 150us/sample
Epoch 58/85
84077/84077 - 13s - loss: 2.4210e-04 - val_loss: 2.5328e-04 - 13s/epoch - 152us/sample
Epoch 59/85
84077/84077 - 13s - loss: 2.4164e-04 - val_loss: 2.5898e-04 - 13s/epoch - 151us/sample
Epoch 60/85
84077/84077 - 13s - loss: 2.4077e-04 - val_loss: 2.5588e-04 - 13s/epoch - 151us/sample
Epoch 61/85
84077/84077 - 13s - loss: 2.3996e-04 - val_loss: 2.5440e-04 - 13s/epoch - 150us/sample
Epoch 62/85
84077/84077 - 13s - loss: 2.3934e-04 - val_loss: 2.5583e-04 - 13s/epoch - 152us/sample
Epoch 63/85
84077/84077 - 13s - loss: 2.3953e-04 - val_loss: 2.5256e-04 - 13s/epoch - 151us/sample
Epoch 64/85
84077/84077 - 13s - loss: 2.3803e-04 - val_loss: 2.5446e-04 - 13s/epoch - 150us/sample
Epoch 65/85
84077/84077 - 13s - loss: 2.3810e-04 - val_loss: 2.5305e-04 - 13s/epoch - 151us/sample
Epoch 66/85
84077/84077 - 13s - loss: 2.3765e-04 - val_loss: 2.5365e-04 - 13s/epoch - 151us/sample
Epoch 67/85
84077/84077 - 13s - loss: 2.3727e-04 - val_loss: 2.5289e-04 - 13s/epoch - 151us/sample
Epoch 68/85
84077/84077 - 13s - loss: 2.3688e-04 - val_loss: 2.5916e-04 - 13s/epoch - 150us/sample
Epoch 69/85
84077/84077 - 13s - loss: 2.3667e-04 - val_loss: 2.4646e-04 - 13s/epoch - 151us/sample
Epoch 70/85
84077/84077 - 13s - loss: 2.3577e-04 - val_loss: 2.5217e-04 - 13s/epoch - 151us/sample
Epoch 71/85
84077/84077 - 13s - loss: 2.3570e-04 - val_loss: 2.5059e-04 - 13s/epoch - 151us/sample
Epoch 72/85
84077/84077 - 13s - loss: 2.3520e-04 - val_loss: 2.5071e-04 - 13s/epoch - 151us/sample
Epoch 73/85
84077/84077 - 13s - loss: 2.3457e-04 - val_loss: 2.5200e-04 - 13s/epoch - 150us/sample
Epoch 74/85
84077/84077 - 13s - loss: 2.3502e-04 - val_loss: 2.5226e-04 - 13s/epoch - 151us/sample
Epoch 75/85
84077/84077 - 13s - loss: 2.3451e-04 - val_loss: 2.5159e-04 - 13s/epoch - 152us/sample
Epoch 76/85
84077/84077 - 13s - loss: 2.3433e-04 - val_loss: 2.5223e-04 - 13s/epoch - 151us/sample
Epoch 77/85
84077/84077 - 13s - loss: 2.3443e-04 - val_loss: 2.5128e-04 - 13s/epoch - 151us/sample
Epoch 78/85
84077/84077 - 13s - loss: 2.3375e-04 - val_loss: 2.4674e-04 - 13s/epoch - 151us/sample
Epoch 79/85
84077/84077 - 13s - loss: 2.3331e-04 - val_loss: 2.4826e-04 - 13s/epoch - 151us/sample
Epoch 80/85
84077/84077 - 13s - loss: 2.3324e-04 - val_loss: 2.5569e-04 - 13s/epoch - 151us/sample
Epoch 81/85
84077/84077 - 13s - loss: 2.3249e-04 - val_loss: 2.4990e-04 - 13s/epoch - 152us/sample
Epoch 82/85
84077/84077 - 13s - loss: 2.3222e-04 - val_loss: 2.4718e-04 - 13s/epoch - 151us/sample
Epoch 83/85
84077/84077 - 13s - loss: 2.3228e-04 - val_loss: 2.4686e-04 - 13s/epoch - 151us/sample
Epoch 84/85
84077/84077 - 13s - loss: 2.3155e-04 - val_loss: 2.4540e-04 - 13s/epoch - 152us/sample
Epoch 85/85
84077/84077 - 13s - loss: 2.3099e-04 - val_loss: 2.5095e-04 - 13s/epoch - 151us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00025094576126945094
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:35:12.620041: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_26/outputlayer/BiasAdd' id:33758 op device:{requested: '', assigned: ''} def:{{{node decoder_model_26/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_26/outputlayer/MatMul, decoder_model_26/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03509214966594498
cosine 0.03466054289295935
MAE: 0.0025943451236968176
RMSE: 0.01307865391573148
r2: 0.8664412094900222
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 85, 0.001, 0.3, 282, 0.00023098600810252657, 0.00025094576126945094, 0.03509214966594498, 0.03466054289295935, 0.0025943451236968176, 0.01307865391573148, 0.8664412094900222, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0008 8 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_81 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_81 (ReLU)                (None, 1980)         0           ['batch_normalization_81[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 02:35:31.167036: W tensorflow/c/c_api.cc:291] Operation '{name:'training_54/Adam/beta_2/Assign' id:35478 op device:{requested: '', assigned: ''} def:{{{node training_54/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_54/Adam/beta_2, training_54/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:36:15.206813: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_27/mul' id:35042 op device:{requested: '', assigned: ''} def:{{{node loss_27/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_27/mul/x, loss_27/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 53s - loss: 0.0048 - val_loss: 0.0016 - 53s/epoch - 629us/sample
Epoch 2/85
84077/84077 - 41s - loss: 0.0013 - val_loss: 0.0012 - 41s/epoch - 492us/sample
Epoch 3/85
84077/84077 - 41s - loss: 0.0011 - val_loss: 0.0012 - 41s/epoch - 492us/sample
Epoch 4/85
84077/84077 - 42s - loss: 9.8813e-04 - val_loss: 0.0016 - 42s/epoch - 494us/sample
Epoch 5/85
84077/84077 - 42s - loss: 9.2893e-04 - val_loss: 0.0052 - 42s/epoch - 494us/sample
Epoch 6/85
84077/84077 - 41s - loss: 8.6110e-04 - val_loss: 0.0172 - 41s/epoch - 492us/sample
Epoch 7/85
84077/84077 - 42s - loss: 8.1591e-04 - val_loss: 0.0359 - 42s/epoch - 494us/sample
Epoch 8/85
84077/84077 - 42s - loss: 7.8819e-04 - val_loss: 0.0749 - 42s/epoch - 494us/sample
Epoch 9/85
84077/84077 - 41s - loss: 7.6191e-04 - val_loss: 0.1475 - 41s/epoch - 493us/sample
Epoch 10/85
84077/84077 - 42s - loss: 7.4767e-04 - val_loss: 0.1538 - 42s/epoch - 494us/sample
Epoch 11/85
84077/84077 - 41s - loss: 7.3952e-04 - val_loss: 0.3570 - 41s/epoch - 492us/sample
Epoch 12/85
84077/84077 - 42s - loss: 7.3068e-04 - val_loss: 0.3927 - 42s/epoch - 494us/sample
Epoch 13/85
84077/84077 - 41s - loss: 7.2043e-04 - val_loss: 0.3509 - 41s/epoch - 493us/sample
Epoch 14/85
84077/84077 - 41s - loss: 7.1393e-04 - val_loss: 0.4875 - 41s/epoch - 493us/sample
Epoch 15/85
84077/84077 - 42s - loss: 7.0891e-04 - val_loss: 0.4115 - 42s/epoch - 494us/sample
Epoch 16/85
84077/84077 - 41s - loss: 7.0416e-04 - val_loss: 0.5041 - 41s/epoch - 493us/sample
Epoch 17/85
84077/84077 - 41s - loss: 7.0088e-04 - val_loss: 0.4813 - 41s/epoch - 493us/sample
Epoch 18/85
84077/84077 - 42s - loss: 6.9670e-04 - val_loss: 0.6538 - 42s/epoch - 494us/sample
Epoch 19/85
84077/84077 - 41s - loss: 6.9370e-04 - val_loss: 0.9862 - 41s/epoch - 492us/sample
Epoch 20/85
84077/84077 - 41s - loss: 6.8957e-04 - val_loss: 0.5077 - 41s/epoch - 493us/sample
Epoch 21/85
84077/84077 - 42s - loss: 6.8816e-04 - val_loss: 0.6321 - 42s/epoch - 494us/sample
Epoch 22/85
84077/84077 - 41s - loss: 6.8513e-04 - val_loss: 0.6675 - 41s/epoch - 494us/sample
Epoch 23/85
84077/84077 - 41s - loss: 6.8130e-04 - val_loss: 0.8903 - 41s/epoch - 493us/sample
Epoch 24/85
84077/84077 - 42s - loss: 6.7942e-04 - val_loss: 0.7903 - 42s/epoch - 494us/sample
Epoch 25/85
84077/84077 - 41s - loss: 6.7724e-04 - val_loss: 0.9045 - 41s/epoch - 493us/sample
Epoch 26/85
84077/84077 - 41s - loss: 6.7395e-04 - val_loss: 0.7731 - 41s/epoch - 493us/sample
Epoch 27/85
84077/84077 - 41s - loss: 6.7259e-04 - val_loss: 0.7553 - 41s/epoch - 493us/sample
Epoch 28/85
84077/84077 - 41s - loss: 6.6954e-04 - val_loss: 0.9743 - 41s/epoch - 493us/sample
Epoch 29/85
84077/84077 - 41s - loss: 6.6700e-04 - val_loss: 0.5814 - 41s/epoch - 494us/sample
Epoch 30/85
84077/84077 - 41s - loss: 6.6626e-04 - val_loss: 0.7654 - 41s/epoch - 493us/sample
Epoch 31/85
84077/84077 - 42s - loss: 6.6428e-04 - val_loss: 0.7255 - 42s/epoch - 494us/sample
Epoch 32/85
84077/84077 - 41s - loss: 6.6350e-04 - val_loss: 0.7371 - 41s/epoch - 493us/sample
Epoch 33/85
84077/84077 - 41s - loss: 6.6091e-04 - val_loss: 0.8407 - 41s/epoch - 493us/sample
Epoch 34/85
84077/84077 - 42s - loss: 6.6026e-04 - val_loss: 0.7317 - 42s/epoch - 494us/sample
Epoch 35/85
84077/84077 - 41s - loss: 6.5963e-04 - val_loss: 0.9648 - 41s/epoch - 492us/sample
Epoch 36/85
84077/84077 - 42s - loss: 6.5889e-04 - val_loss: 0.6432 - 42s/epoch - 494us/sample
Epoch 37/85
84077/84077 - 41s - loss: 6.5750e-04 - val_loss: 0.8613 - 41s/epoch - 493us/sample
Epoch 38/85
84077/84077 - 41s - loss: 6.5570e-04 - val_loss: 0.7042 - 41s/epoch - 493us/sample
Epoch 39/85
84077/84077 - 41s - loss: 6.5560e-04 - val_loss: 0.8233 - 41s/epoch - 493us/sample
Epoch 40/85
84077/84077 - 41s - loss: 6.5418e-04 - val_loss: 0.7021 - 41s/epoch - 493us/sample
Epoch 41/85
84077/84077 - 42s - loss: 6.5234e-04 - val_loss: 0.8638 - 42s/epoch - 494us/sample
Epoch 42/85
84077/84077 - 41s - loss: 6.5173e-04 - val_loss: 0.6396 - 41s/epoch - 493us/sample
Epoch 43/85
84077/84077 - 41s - loss: 6.5147e-04 - val_loss: 0.7718 - 41s/epoch - 493us/sample
Epoch 44/85
84077/84077 - 42s - loss: 6.5062e-04 - val_loss: 0.7126 - 42s/epoch - 494us/sample
Epoch 45/85
84077/84077 - 41s - loss: 6.4879e-04 - val_loss: 0.7422 - 41s/epoch - 494us/sample
Epoch 46/85
84077/84077 - 41s - loss: 6.4753e-04 - val_loss: 0.7705 - 41s/epoch - 492us/sample
Epoch 47/85
84077/84077 - 42s - loss: 6.4722e-04 - val_loss: 0.7575 - 42s/epoch - 494us/sample
Epoch 48/85
84077/84077 - 41s - loss: 6.4635e-04 - val_loss: 0.5480 - 41s/epoch - 493us/sample
Epoch 49/85
84077/84077 - 41s - loss: 6.4546e-04 - val_loss: 0.6876 - 41s/epoch - 492us/sample
Epoch 50/85
84077/84077 - 41s - loss: 6.4447e-04 - val_loss: 0.5817 - 41s/epoch - 493us/sample
Epoch 51/85
84077/84077 - 41s - loss: 6.4410e-04 - val_loss: 1.1093 - 41s/epoch - 493us/sample
Epoch 52/85
84077/84077 - 42s - loss: 6.4290e-04 - val_loss: 0.7699 - 42s/epoch - 494us/sample
Epoch 53/85
84077/84077 - 42s - loss: 6.4198e-04 - val_loss: 1.1329 - 42s/epoch - 494us/sample
Epoch 54/85
84077/84077 - 41s - loss: 6.4189e-04 - val_loss: 1.0227 - 41s/epoch - 493us/sample
Epoch 55/85
84077/84077 - 42s - loss: 6.4073e-04 - val_loss: 0.4737 - 42s/epoch - 494us/sample
Epoch 56/85
84077/84077 - 41s - loss: 6.4070e-04 - val_loss: 0.7313 - 41s/epoch - 493us/sample
Epoch 57/85
84077/84077 - 41s - loss: 6.3993e-04 - val_loss: 0.9507 - 41s/epoch - 492us/sample
Epoch 58/85
84077/84077 - 41s - loss: 6.3945e-04 - val_loss: 0.6061 - 41s/epoch - 494us/sample
Epoch 59/85
84077/84077 - 41s - loss: 6.3836e-04 - val_loss: 0.9264 - 41s/epoch - 492us/sample
Epoch 60/85
84077/84077 - 42s - loss: 6.3736e-04 - val_loss: 0.8601 - 42s/epoch - 494us/sample
Epoch 61/85
84077/84077 - 42s - loss: 6.3769e-04 - val_loss: 0.4912 - 42s/epoch - 494us/sample
Epoch 62/85
84077/84077 - 41s - loss: 6.3641e-04 - val_loss: 0.8158 - 41s/epoch - 492us/sample
Epoch 63/85
84077/84077 - 42s - loss: 6.3583e-04 - val_loss: 0.8997 - 42s/epoch - 494us/sample
Epoch 64/85
84077/84077 - 41s - loss: 6.3642e-04 - val_loss: 0.9251 - 41s/epoch - 494us/sample
Epoch 65/85
84077/84077 - 41s - loss: 6.3380e-04 - val_loss: 0.7102 - 41s/epoch - 493us/sample
Epoch 66/85
84077/84077 - 42s - loss: 6.3381e-04 - val_loss: 0.8955 - 42s/epoch - 494us/sample
Epoch 67/85
84077/84077 - 41s - loss: 6.3202e-04 - val_loss: 0.7270 - 41s/epoch - 493us/sample
Epoch 68/85
84077/84077 - 41s - loss: 6.3061e-04 - val_loss: 0.5474 - 41s/epoch - 493us/sample
Epoch 69/85
84077/84077 - 41s - loss: 6.2925e-04 - val_loss: 0.7550 - 41s/epoch - 493us/sample
Epoch 70/85
84077/84077 - 41s - loss: 6.2987e-04 - val_loss: 0.5180 - 41s/epoch - 493us/sample
Epoch 71/85
84077/84077 - 42s - loss: 6.2866e-04 - val_loss: 0.5488 - 42s/epoch - 494us/sample
Epoch 72/85
84077/84077 - 42s - loss: 6.2686e-04 - val_loss: 0.7732 - 42s/epoch - 494us/sample
Epoch 73/85
84077/84077 - 41s - loss: 6.2627e-04 - val_loss: 0.7289 - 41s/epoch - 493us/sample
Epoch 74/85
84077/84077 - 41s - loss: 6.2529e-04 - val_loss: 0.6733 - 41s/epoch - 493us/sample
Epoch 75/85
84077/84077 - 41s - loss: 6.2572e-04 - val_loss: 0.8503 - 41s/epoch - 492us/sample
Epoch 76/85
84077/84077 - 42s - loss: 6.2398e-04 - val_loss: 0.6860 - 42s/epoch - 494us/sample
Epoch 77/85
84077/84077 - 42s - loss: 6.2360e-04 - val_loss: 0.7393 - 42s/epoch - 494us/sample
Epoch 78/85
84077/84077 - 41s - loss: 6.2460e-04 - val_loss: 0.6466 - 41s/epoch - 492us/sample
Epoch 79/85
84077/84077 - 42s - loss: 6.2365e-04 - val_loss: 0.6532 - 42s/epoch - 494us/sample
Epoch 80/85
84077/84077 - 42s - loss: 6.2297e-04 - val_loss: 0.5933 - 42s/epoch - 494us/sample
Epoch 81/85
84077/84077 - 41s - loss: 6.2351e-04 - val_loss: 0.9731 - 41s/epoch - 493us/sample
Epoch 82/85
84077/84077 - 42s - loss: 6.2291e-04 - val_loss: 0.8170 - 42s/epoch - 494us/sample
Epoch 83/85
84077/84077 - 41s - loss: 6.2221e-04 - val_loss: 0.6575 - 41s/epoch - 493us/sample
Epoch 84/85
84077/84077 - 41s - loss: 6.2108e-04 - val_loss: 0.7232 - 41s/epoch - 493us/sample
Epoch 85/85
84077/84077 - 42s - loss: 6.2104e-04 - val_loss: 0.6007 - 42s/epoch - 494us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.6007111940880075
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:34:24.206951: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_27/outputlayer/BiasAdd' id:35013 op device:{requested: '', assigned: ''} def:{{{node decoder_model_27/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_27/outputlayer/MatMul, decoder_model_27/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.20842863921294102
cosine 0.20650747394665547
MAE: 0.04475342955373315
RMSE: 0.7753772363480431
r2: -468.65568291083457
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 8, 85, 0.0008, 0.3, 282, 0.0006210414864332855, 0.6007111940880075, 0.20842863921294102, 0.20650747394665547, 0.04475342955373315, 0.7753772363480431, -468.65568291083457, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0012000000000000001 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_84 (BatchN  (None, 1980)        7920        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_84 (ReLU)                (None, 1980)         0           ['batch_normalization_84[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 03:34:44.373745: W tensorflow/c/c_api.cc:291] Operation '{name:'training_56/Adam/bottleneck_zlog_28/bias/v/Assign' id:36909 op device:{requested: '', assigned: ''} def:{{{node training_56/Adam/bottleneck_zlog_28/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_56/Adam/bottleneck_zlog_28/bias/v, training_56/Adam/bottleneck_zlog_28/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:34:57.689993: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_28/mul' id:36297 op device:{requested: '', assigned: ''} def:{{{node loss_28/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_28/mul/x, loss_28/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0092 - val_loss: 0.0017 - 21s/epoch - 247us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0025 - val_loss: 0.0012 - 9s/epoch - 106us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0014 - 9s/epoch - 106us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0011 - 9s/epoch - 106us/sample
Epoch 5/85
84077/84077 - 9s - loss: 9.5436e-04 - val_loss: 7.2674e-04 - 9s/epoch - 106us/sample
Epoch 6/85
84077/84077 - 9s - loss: 7.5823e-04 - val_loss: 6.1659e-04 - 9s/epoch - 106us/sample
Epoch 7/85
84077/84077 - 9s - loss: 6.3877e-04 - val_loss: 5.4673e-04 - 9s/epoch - 107us/sample
Epoch 8/85
84077/84077 - 9s - loss: 5.7080e-04 - val_loss: 8.1771e-04 - 9s/epoch - 106us/sample
Epoch 9/85
84077/84077 - 9s - loss: 4.9053e-04 - val_loss: 3.7127e-04 - 9s/epoch - 106us/sample
Epoch 10/85
84077/84077 - 9s - loss: 4.1088e-04 - val_loss: 3.1670e-04 - 9s/epoch - 107us/sample
Epoch 11/85
84077/84077 - 9s - loss: 3.5535e-04 - val_loss: 2.8807e-04 - 9s/epoch - 106us/sample
Epoch 12/85
84077/84077 - 9s - loss: 3.5073e-04 - val_loss: 3.1248e-04 - 9s/epoch - 106us/sample
Epoch 13/85
84077/84077 - 9s - loss: 3.0466e-04 - val_loss: 2.5511e-04 - 9s/epoch - 106us/sample
Epoch 14/85
84077/84077 - 9s - loss: 2.8127e-04 - val_loss: 2.3738e-04 - 9s/epoch - 106us/sample
Epoch 15/85
84077/84077 - 9s - loss: 2.7095e-04 - val_loss: 2.2922e-04 - 9s/epoch - 106us/sample
Epoch 16/85
84077/84077 - 9s - loss: 2.5326e-04 - val_loss: 2.1656e-04 - 9s/epoch - 107us/sample
Epoch 17/85
84077/84077 - 9s - loss: 2.4562e-04 - val_loss: 2.1189e-04 - 9s/epoch - 107us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.3421e-04 - val_loss: 2.0330e-04 - 9s/epoch - 106us/sample
Epoch 19/85
84077/84077 - 9s - loss: 2.2462e-04 - val_loss: 1.9553e-04 - 9s/epoch - 107us/sample
Epoch 20/85
84077/84077 - 9s - loss: 2.5348e-04 - val_loss: 1.9389e-04 - 9s/epoch - 106us/sample
Epoch 21/85
84077/84077 - 9s - loss: 2.1791e-04 - val_loss: 1.8864e-04 - 9s/epoch - 106us/sample
Epoch 22/85
84077/84077 - 9s - loss: 2.1172e-04 - val_loss: 1.9061e-04 - 9s/epoch - 106us/sample
Epoch 23/85
84077/84077 - 9s - loss: 2.0875e-04 - val_loss: 1.8242e-04 - 9s/epoch - 106us/sample
Epoch 24/85
84077/84077 - 9s - loss: 2.0198e-04 - val_loss: 1.8147e-04 - 9s/epoch - 107us/sample
Epoch 25/85
84077/84077 - 9s - loss: 1.9703e-04 - val_loss: 1.7860e-04 - 9s/epoch - 107us/sample
Epoch 26/85
84077/84077 - 9s - loss: 1.9538e-04 - val_loss: 1.7494e-04 - 9s/epoch - 107us/sample
Epoch 27/85
84077/84077 - 9s - loss: 1.9037e-04 - val_loss: 1.7234e-04 - 9s/epoch - 106us/sample
Epoch 28/85
84077/84077 - 9s - loss: 1.8863e-04 - val_loss: 1.7103e-04 - 9s/epoch - 106us/sample
Epoch 29/85
84077/84077 - 9s - loss: 1.8585e-04 - val_loss: 1.6861e-04 - 9s/epoch - 106us/sample
Epoch 30/85
84077/84077 - 9s - loss: 1.8366e-04 - val_loss: 1.7484e-04 - 9s/epoch - 107us/sample
Epoch 31/85
84077/84077 - 9s - loss: 1.9669e-04 - val_loss: 1.6991e-04 - 9s/epoch - 107us/sample
Epoch 32/85
84077/84077 - 9s - loss: 1.8047e-04 - val_loss: 1.6584e-04 - 9s/epoch - 107us/sample
Epoch 33/85
84077/84077 - 9s - loss: 1.8133e-04 - val_loss: 1.6549e-04 - 9s/epoch - 107us/sample
Epoch 34/85
84077/84077 - 9s - loss: 1.7703e-04 - val_loss: 1.6790e-04 - 9s/epoch - 106us/sample
Epoch 35/85
84077/84077 - 9s - loss: 1.7693e-04 - val_loss: 1.6279e-04 - 9s/epoch - 106us/sample
Epoch 36/85
84077/84077 - 9s - loss: 1.7458e-04 - val_loss: 1.5921e-04 - 9s/epoch - 106us/sample
Epoch 37/85
84077/84077 - 9s - loss: 1.7406e-04 - val_loss: 1.5801e-04 - 9s/epoch - 107us/sample
Epoch 38/85
84077/84077 - 9s - loss: 1.7003e-04 - val_loss: 1.5590e-04 - 9s/epoch - 107us/sample
Epoch 39/85
84077/84077 - 9s - loss: 1.6933e-04 - val_loss: 1.5871e-04 - 9s/epoch - 107us/sample
Epoch 40/85
84077/84077 - 9s - loss: 1.6921e-04 - val_loss: 1.5660e-04 - 9s/epoch - 107us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.6831e-04 - val_loss: 1.5512e-04 - 9s/epoch - 107us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.6765e-04 - val_loss: 1.5194e-04 - 9s/epoch - 106us/sample
Epoch 43/85
84077/84077 - 9s - loss: 1.6695e-04 - val_loss: 1.5304e-04 - 9s/epoch - 106us/sample
Epoch 44/85
84077/84077 - 9s - loss: 1.6523e-04 - val_loss: 1.5381e-04 - 9s/epoch - 107us/sample
Epoch 45/85
84077/84077 - 9s - loss: 1.6435e-04 - val_loss: 1.5583e-04 - 9s/epoch - 106us/sample
Epoch 46/85
84077/84077 - 9s - loss: 1.6454e-04 - val_loss: 1.5207e-04 - 9s/epoch - 107us/sample
Epoch 47/85
84077/84077 - 9s - loss: 1.6212e-04 - val_loss: 1.5070e-04 - 9s/epoch - 107us/sample
Epoch 48/85
84077/84077 - 9s - loss: 1.6182e-04 - val_loss: 1.5589e-04 - 9s/epoch - 107us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.6075e-04 - val_loss: 1.5320e-04 - 9s/epoch - 107us/sample
Epoch 50/85
84077/84077 - 9s - loss: 1.5936e-04 - val_loss: 1.4794e-04 - 9s/epoch - 107us/sample
Epoch 51/85
84077/84077 - 9s - loss: 1.6049e-04 - val_loss: 1.4924e-04 - 9s/epoch - 107us/sample
Epoch 52/85
84077/84077 - 9s - loss: 1.5833e-04 - val_loss: 1.4508e-04 - 9s/epoch - 107us/sample
Epoch 53/85
84077/84077 - 9s - loss: 1.5706e-04 - val_loss: 1.4545e-04 - 9s/epoch - 107us/sample
Epoch 54/85
84077/84077 - 9s - loss: 1.6035e-04 - val_loss: 1.4548e-04 - 9s/epoch - 108us/sample
Epoch 55/85
84077/84077 - 9s - loss: 1.5702e-04 - val_loss: 1.4442e-04 - 9s/epoch - 106us/sample
Epoch 56/85
84077/84077 - 9s - loss: 1.5883e-04 - val_loss: 1.4505e-04 - 9s/epoch - 106us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.5564e-04 - val_loss: 1.4625e-04 - 9s/epoch - 106us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.5460e-04 - val_loss: 1.4819e-04 - 9s/epoch - 107us/sample
Epoch 59/85
84077/84077 - 9s - loss: 1.5440e-04 - val_loss: 1.4242e-04 - 9s/epoch - 107us/sample
Epoch 60/85
84077/84077 - 9s - loss: 1.5402e-04 - val_loss: 1.4367e-04 - 9s/epoch - 107us/sample
Epoch 61/85
84077/84077 - 9s - loss: 1.5244e-04 - val_loss: 1.4224e-04 - 9s/epoch - 107us/sample
Epoch 62/85
84077/84077 - 9s - loss: 1.5329e-04 - val_loss: 1.4534e-04 - 9s/epoch - 107us/sample
Epoch 63/85
84077/84077 - 9s - loss: 1.5306e-04 - val_loss: 1.4164e-04 - 9s/epoch - 107us/sample
Epoch 64/85
84077/84077 - 9s - loss: 1.5699e-04 - val_loss: 1.5227e-04 - 9s/epoch - 107us/sample
Epoch 65/85
84077/84077 - 9s - loss: 1.5359e-04 - val_loss: 1.3961e-04 - 9s/epoch - 106us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.5085e-04 - val_loss: 1.3863e-04 - 9s/epoch - 107us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.5113e-04 - val_loss: 1.4148e-04 - 9s/epoch - 107us/sample
Epoch 68/85
84077/84077 - 9s - loss: 1.5094e-04 - val_loss: 1.4098e-04 - 9s/epoch - 107us/sample
Epoch 69/85
84077/84077 - 9s - loss: 1.4975e-04 - val_loss: 1.3762e-04 - 9s/epoch - 107us/sample
Epoch 70/85
84077/84077 - 9s - loss: 1.4939e-04 - val_loss: 1.3939e-04 - 9s/epoch - 107us/sample
Epoch 71/85
84077/84077 - 9s - loss: 1.4995e-04 - val_loss: 1.4337e-04 - 9s/epoch - 107us/sample
Epoch 72/85
84077/84077 - 9s - loss: 1.4988e-04 - val_loss: 1.4071e-04 - 9s/epoch - 107us/sample
Epoch 73/85
84077/84077 - 9s - loss: 1.4969e-04 - val_loss: 1.4042e-04 - 9s/epoch - 107us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.4796e-04 - val_loss: 1.3922e-04 - 9s/epoch - 107us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.4734e-04 - val_loss: 1.3635e-04 - 9s/epoch - 107us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.4843e-04 - val_loss: 1.3937e-04 - 9s/epoch - 107us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.4799e-04 - val_loss: 1.3805e-04 - 9s/epoch - 107us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.4587e-04 - val_loss: 1.3763e-04 - 9s/epoch - 107us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.4675e-04 - val_loss: 1.3620e-04 - 9s/epoch - 106us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.4666e-04 - val_loss: 1.3984e-04 - 9s/epoch - 107us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.4647e-04 - val_loss: 1.3581e-04 - 9s/epoch - 107us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.4500e-04 - val_loss: 1.3923e-04 - 9s/epoch - 107us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.4845e-04 - val_loss: 1.7008e-04 - 9s/epoch - 107us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.4809e-04 - val_loss: 1.3846e-04 - 9s/epoch - 107us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.4609e-04 - val_loss: 1.3851e-04 - 9s/epoch - 107us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00013851488791777008
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:47:35.014184: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_28/outputlayer/BiasAdd' id:36268 op device:{requested: '', assigned: ''} def:{{{node decoder_model_28/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_28/outputlayer/MatMul, decoder_model_28/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.015554887803526993
cosine 0.01536615706569996
MAE: 0.001786869710807498
RMSE: 0.007476134827713365
r2: 0.9572808189336707
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.3, 282, 0.00014608840574932707, 0.00013851488791777008, 0.015554887803526993, 0.01536615706569996, 0.001786869710807498, 0.007476134827713365, 0.9572808189336707, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 90 0.001 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_87 (BatchN  (None, 2168)        8672        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_87 (ReLU)                (None, 2168)         0           ['batch_normalization_87[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          611658      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          611658      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2748517     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,027,097
Trainable params: 6,017,861
Non-trainable params: 9,236
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 03:47:55.612195: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_89/gamma/Assign' id:37361 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_89/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_89/gamma, batch_normalization_89/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:48:09.106211: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_29/mul' id:37552 op device:{requested: '', assigned: ''} def:{{{node loss_29/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_29/mul/x, loss_29/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 21s - loss: 0.0987 - val_loss: 0.0017 - 21s/epoch - 252us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 107us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0014 - val_loss: 0.0017 - 9s/epoch - 107us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0666 - val_loss: 0.0023 - 9s/epoch - 106us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0015 - val_loss: 0.0012 - 9s/epoch - 107us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0010 - val_loss: 8.2420e-04 - 9s/epoch - 107us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0010 - val_loss: 0.0014 - 9s/epoch - 107us/sample
Epoch 8/90
84077/84077 - 9s - loss: 8.3485e-04 - val_loss: 7.2268e-04 - 9s/epoch - 107us/sample
Epoch 9/90
84077/84077 - 9s - loss: 6.9458e-04 - val_loss: 5.4658e-04 - 9s/epoch - 108us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0019 - val_loss: 8.8929e-04 - 9s/epoch - 108us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 0.0012 - 9s/epoch - 107us/sample
Epoch 12/90
84077/84077 - 9s - loss: 6.3118e-04 - val_loss: 4.6370e-04 - 9s/epoch - 107us/sample
Epoch 13/90
84077/84077 - 9s - loss: 7.7887e-04 - val_loss: 5.0182e-04 - 9s/epoch - 107us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0026 - val_loss: 0.0035 - 9s/epoch - 107us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0015 - val_loss: 7.5876e-04 - 9s/epoch - 107us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0016 - val_loss: 6.7169e-04 - 9s/epoch - 108us/sample
Epoch 17/90
84077/84077 - 9s - loss: 6.0187e-04 - val_loss: 6.1062e-04 - 9s/epoch - 108us/sample
Epoch 18/90
84077/84077 - 9s - loss: 8.2299e-04 - val_loss: 0.0014 - 9s/epoch - 107us/sample
Epoch 19/90
84077/84077 - 9s - loss: 6.4079 - val_loss: 0.0138 - 9s/epoch - 108us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0014 - val_loss: 4.5961e-04 - 9s/epoch - 107us/sample
Epoch 21/90
84077/84077 - 9s - loss: 4.5386e-04 - val_loss: 3.6419e-04 - 9s/epoch - 107us/sample
Epoch 22/90
84077/84077 - 9s - loss: 4.0570e-04 - val_loss: 3.4456e-04 - 9s/epoch - 107us/sample
Epoch 23/90
84077/84077 - 9s - loss: 3.7691e-04 - val_loss: 3.3615e-04 - 9s/epoch - 107us/sample
Epoch 24/90
84077/84077 - 9s - loss: 3.5693e-04 - val_loss: 5.4504e-04 - 9s/epoch - 108us/sample
Epoch 25/90
84077/84077 - 9s - loss: 3.4398e-04 - val_loss: 3.0706e-04 - 9s/epoch - 108us/sample
Epoch 26/90
84077/84077 - 9s - loss: 4.3422e-04 - val_loss: 5.1998e-04 - 9s/epoch - 107us/sample
Epoch 27/90
84077/84077 - 9s - loss: 3.1875e-04 - val_loss: 2.7345e-04 - 9s/epoch - 107us/sample
Epoch 28/90
84077/84077 - 9s - loss: 3.1305e-04 - val_loss: 4.3537e-04 - 9s/epoch - 107us/sample
Epoch 29/90
84077/84077 - 9s - loss: 2.9908e-04 - val_loss: 2.6124e-04 - 9s/epoch - 107us/sample
Epoch 30/90
84077/84077 - 9s - loss: 2.8145e-04 - val_loss: 2.6332e-04 - 9s/epoch - 107us/sample
Epoch 31/90
84077/84077 - 9s - loss: 2.9357e-04 - val_loss: 2.5195e-04 - 9s/epoch - 108us/sample
Epoch 32/90
84077/84077 - 9s - loss: 2.7012e-04 - val_loss: 2.3893e-04 - 9s/epoch - 108us/sample
Epoch 33/90
84077/84077 - 9s - loss: 2.7167e-04 - val_loss: 2.3458e-04 - 9s/epoch - 107us/sample
Epoch 34/90
84077/84077 - 9s - loss: 2.5497e-04 - val_loss: 2.2555e-04 - 9s/epoch - 108us/sample
Epoch 35/90
84077/84077 - 9s - loss: 2.4932e-04 - val_loss: 2.2573e-04 - 9s/epoch - 107us/sample
Epoch 36/90
84077/84077 - 9s - loss: 2.7452e-04 - val_loss: 2.2052e-04 - 9s/epoch - 107us/sample
Epoch 37/90
84077/84077 - 9s - loss: 2.4174e-04 - val_loss: 2.2736e-04 - 9s/epoch - 107us/sample
Epoch 38/90
84077/84077 - 9s - loss: 2.4357e-04 - val_loss: 2.1309e-04 - 9s/epoch - 108us/sample
Epoch 39/90
84077/84077 - 9s - loss: 2.3084e-04 - val_loss: 2.1029e-04 - 9s/epoch - 108us/sample
Epoch 40/90
84077/84077 - 9s - loss: 2.2951e-04 - val_loss: 2.0911e-04 - 9s/epoch - 107us/sample
Epoch 41/90
84077/84077 - 9s - loss: 2.2578e-04 - val_loss: 2.1303e-04 - 9s/epoch - 108us/sample
Epoch 42/90
84077/84077 - 9s - loss: 2.2187e-04 - val_loss: 2.0015e-04 - 9s/epoch - 107us/sample
Epoch 43/90
84077/84077 - 9s - loss: 2.1961e-04 - val_loss: 1.9897e-04 - 9s/epoch - 107us/sample
Epoch 44/90
84077/84077 - 9s - loss: 2.1533e-04 - val_loss: 1.9606e-04 - 9s/epoch - 107us/sample
Epoch 45/90
84077/84077 - 9s - loss: 2.1133e-04 - val_loss: 1.9483e-04 - 9s/epoch - 108us/sample
Epoch 46/90
84077/84077 - 9s - loss: 2.0976e-04 - val_loss: 1.9384e-04 - 9s/epoch - 108us/sample
Epoch 47/90
84077/84077 - 9s - loss: 2.0640e-04 - val_loss: 1.9738e-04 - 9s/epoch - 108us/sample
Epoch 48/90
84077/84077 - 9s - loss: 2.0405e-04 - val_loss: 1.9034e-04 - 9s/epoch - 107us/sample
Epoch 49/90
84077/84077 - 9s - loss: 2.0143e-04 - val_loss: 1.8650e-04 - 9s/epoch - 107us/sample
Epoch 50/90
84077/84077 - 9s - loss: 2.0025e-04 - val_loss: 1.8705e-04 - 9s/epoch - 107us/sample
Epoch 51/90
84077/84077 - 9s - loss: 1.9924e-04 - val_loss: 1.8560e-04 - 9s/epoch - 107us/sample
Epoch 52/90
84077/84077 - 9s - loss: 1.9716e-04 - val_loss: 1.8317e-04 - 9s/epoch - 108us/sample
Epoch 53/90
84077/84077 - 9s - loss: 1.9652e-04 - val_loss: 1.8402e-04 - 9s/epoch - 108us/sample
Epoch 54/90
84077/84077 - 9s - loss: 1.9502e-04 - val_loss: 1.8116e-04 - 9s/epoch - 108us/sample
Epoch 55/90
84077/84077 - 9s - loss: 1.9323e-04 - val_loss: 1.8095e-04 - 9s/epoch - 107us/sample
Epoch 56/90
84077/84077 - 9s - loss: 1.9170e-04 - val_loss: 1.7985e-04 - 9s/epoch - 108us/sample
Epoch 57/90
84077/84077 - 9s - loss: 1.9081e-04 - val_loss: 1.7821e-04 - 9s/epoch - 107us/sample
Epoch 58/90
84077/84077 - 9s - loss: 1.9015e-04 - val_loss: 1.8030e-04 - 9s/epoch - 107us/sample
Epoch 59/90
84077/84077 - 9s - loss: 1.8827e-04 - val_loss: 1.7733e-04 - 9s/epoch - 107us/sample
Epoch 60/90
84077/84077 - 9s - loss: 1.8812e-04 - val_loss: 1.7790e-04 - 9s/epoch - 108us/sample
Epoch 61/90
84077/84077 - 9s - loss: 1.8716e-04 - val_loss: 1.7592e-04 - 9s/epoch - 108us/sample
Epoch 62/90
84077/84077 - 9s - loss: 1.8604e-04 - val_loss: 1.7600e-04 - 9s/epoch - 108us/sample
Epoch 63/90
84077/84077 - 9s - loss: 1.8510e-04 - val_loss: 1.7507e-04 - 9s/epoch - 108us/sample
Epoch 64/90
84077/84077 - 9s - loss: 1.8392e-04 - val_loss: 1.7437e-04 - 9s/epoch - 108us/sample
Epoch 65/90
84077/84077 - 9s - loss: 1.8328e-04 - val_loss: 1.7479e-04 - 9s/epoch - 107us/sample
Epoch 66/90
84077/84077 - 9s - loss: 1.8277e-04 - val_loss: 1.7396e-04 - 9s/epoch - 107us/sample
Epoch 67/90
84077/84077 - 9s - loss: 1.8203e-04 - val_loss: 1.7441e-04 - 9s/epoch - 108us/sample
Epoch 68/90
84077/84077 - 9s - loss: 1.8130e-04 - val_loss: 1.7461e-04 - 9s/epoch - 108us/sample
Epoch 69/90
84077/84077 - 9s - loss: 1.8180e-04 - val_loss: 1.7823e-04 - 9s/epoch - 108us/sample
Epoch 70/90
84077/84077 - 9s - loss: 1.8035e-04 - val_loss: 1.7165e-04 - 9s/epoch - 107us/sample
Epoch 71/90
84077/84077 - 9s - loss: 1.7929e-04 - val_loss: 1.7207e-04 - 9s/epoch - 108us/sample
Epoch 72/90
84077/84077 - 9s - loss: 1.7922e-04 - val_loss: 1.7007e-04 - 9s/epoch - 107us/sample
Epoch 73/90
84077/84077 - 9s - loss: 1.7847e-04 - val_loss: 1.7094e-04 - 9s/epoch - 107us/sample
Epoch 74/90
84077/84077 - 9s - loss: 1.7849e-04 - val_loss: 1.6965e-04 - 9s/epoch - 107us/sample
Epoch 75/90
84077/84077 - 9s - loss: 1.7737e-04 - val_loss: 1.6986e-04 - 9s/epoch - 108us/sample
Epoch 76/90
84077/84077 - 9s - loss: 1.7679e-04 - val_loss: 1.6959e-04 - 9s/epoch - 109us/sample
Epoch 77/90
84077/84077 - 9s - loss: 1.7588e-04 - val_loss: 1.6780e-04 - 9s/epoch - 107us/sample
Epoch 78/90
84077/84077 - 9s - loss: 1.7577e-04 - val_loss: 1.6800e-04 - 9s/epoch - 108us/sample
Epoch 79/90
84077/84077 - 9s - loss: 1.7514e-04 - val_loss: 1.6903e-04 - 9s/epoch - 108us/sample
Epoch 80/90
84077/84077 - 9s - loss: 1.7547e-04 - val_loss: 1.6699e-04 - 9s/epoch - 107us/sample
Epoch 81/90
84077/84077 - 9s - loss: 1.7394e-04 - val_loss: 1.6736e-04 - 9s/epoch - 107us/sample
Epoch 82/90
84077/84077 - 9s - loss: 1.7342e-04 - val_loss: 1.6468e-04 - 9s/epoch - 108us/sample
Epoch 83/90
84077/84077 - 9s - loss: 1.7269e-04 - val_loss: 1.6645e-04 - 9s/epoch - 108us/sample
Epoch 84/90
84077/84077 - 9s - loss: 1.7245e-04 - val_loss: 1.6675e-04 - 9s/epoch - 108us/sample
Epoch 85/90
84077/84077 - 9s - loss: 1.7212e-04 - val_loss: 1.6570e-04 - 9s/epoch - 108us/sample
Epoch 86/90
84077/84077 - 9s - loss: 1.7256e-04 - val_loss: 1.6573e-04 - 9s/epoch - 107us/sample
Epoch 87/90
84077/84077 - 9s - loss: 1.7169e-04 - val_loss: 1.6527e-04 - 9s/epoch - 107us/sample
Epoch 88/90
84077/84077 - 9s - loss: 1.7118e-04 - val_loss: 1.6422e-04 - 9s/epoch - 108us/sample
Epoch 89/90
84077/84077 - 9s - loss: 1.7029e-04 - val_loss: 1.6391e-04 - 9s/epoch - 108us/sample
Epoch 90/90
84077/84077 - 9s - loss: 1.7018e-04 - val_loss: 1.6503e-04 - 9s/epoch - 109us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0001650255171475831
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:01:37.825115: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_29/outputlayer/BiasAdd' id:37523 op device:{requested: '', assigned: ''} def:{{{node decoder_model_29/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_29/outputlayer/MatMul, decoder_model_29/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.021729102436326874
cosine 0.02146458974873963
MAE: 0.00183365216206272
RMSE: 0.009227600469893803
r2: 0.9336341044934432
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'mse', 64, 90, 0.001, 0.3, 282, 0.00017018035470455628, 0.0001650255171475831, 0.021729102436326874, 0.02146458974873963, 0.00183365216206272, 0.009227600469893803, 0.9336341044934432, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 3
Fitness    = 595.8845894037719
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 595.8845894037719.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [595.8845894037719, 595.8845894037719, 595.8845894037719]
[2.4000000000000004 90 0.001 64 0] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2263)         2136272     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_90 (BatchN  (None, 2263)        9052        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_90 (ReLU)                (None, 2263)         0           ['batch_normalization_90[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          638448      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          638448      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2865367     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,287,587
Trainable params: 6,277,971
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 04:01:58.319353: W tensorflow/c/c_api.cc:291] Operation '{name:'training_60/Adam/batch_normalization_92/gamma/v/Assign' id:39550 op device:{requested: '', assigned: ''} def:{{{node training_60/Adam/batch_normalization_92/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_60/Adam/batch_normalization_92/gamma/v, training_60/Adam/batch_normalization_92/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:02:12.321891: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_30/mul' id:38829 op device:{requested: '', assigned: ''} def:{{{node loss_30/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_30/mul/x, loss_30/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.0734 - val_loss: 0.0707 - 22s/epoch - 261us/sample
Epoch 2/90
84077/84077 - 9s - loss: 1.1090 - val_loss: 0.0712 - 9s/epoch - 110us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0676 - val_loss: 0.0676 - 9s/epoch - 110us/sample
Epoch 4/90
84077/84077 - 9s - loss: 0.0670 - val_loss: 0.0675 - 9s/epoch - 109us/sample
Epoch 5/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 6/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0682 - 9s/epoch - 109us/sample
Epoch 7/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 8/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 9/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 109us/sample
Epoch 10/90
84077/84077 - 9s - loss: 0.0669 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 11/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 12/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0674 - 9s/epoch - 110us/sample
Epoch 13/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 14/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 15/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 16/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 17/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 18/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 19/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 20/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 21/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 22/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 23/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 24/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 25/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 26/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 27/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 28/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 29/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 30/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 31/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 32/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 33/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 34/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 35/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 36/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 37/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 38/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 39/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 40/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 41/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 42/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 43/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 44/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 45/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 46/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 47/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 48/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 49/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 50/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 51/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 52/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 53/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 54/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 55/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 56/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 57/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 58/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 59/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 60/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 61/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 62/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 63/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 64/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 65/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 66/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 67/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 68/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 69/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 70/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 71/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 72/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 73/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 74/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 75/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 76/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 77/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 78/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 79/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 80/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 81/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 82/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 83/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 84/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 85/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 109us/sample
Epoch 86/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 87/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 88/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 111us/sample
Epoch 89/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
Epoch 90/90
84077/84077 - 9s - loss: 0.0668 - val_loss: 0.0673 - 9s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0673457394254154
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:15:58.002812: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_30/outputlayer/BiasAdd' id:38781 op device:{requested: '', assigned: ''} def:{{{node decoder_model_30/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_30/outputlayer/MatMul, decoder_model_30/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 1.1743803357732039
cosine 1.143534206978659
MAE: 6.143679449154667
RMSE: 6.340547184621969
r2: -29477.034511745136
RMSE zero-vector: 0.04004287452915337
['2.4000000000000004custom_VAE', 'binary_crossentropy', 64, 90, 0.001, 0.3, 282, 0.06683692109083311, 0.0673457394254154, 1.1743803357732039, 1.143534206978659, 6.143679449154667, 6.340547184621969, -29477.034511745136, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4000000000000004 90 0.0012000000000000001 8 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2263)         2136272     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_93 (BatchN  (None, 2263)        9052        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_93 (ReLU)                (None, 2263)         0           ['batch_normalization_93[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          638448      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          638448      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2865367     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,287,587
Trainable params: 6,277,971
Non-trainable params: 9,616
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 04:16:19.010400: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_94/gamma/Assign' id:39877 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_94/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_94/gamma, batch_normalization_94/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:17:05.533851: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_31/mul' id:40150 op device:{requested: '', assigned: ''} def:{{{node loss_31/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_31/mul/x, loss_31/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 56s - loss: 0.0057 - val_loss: 0.0021 - 56s/epoch - 671us/sample
Epoch 2/90
84077/84077 - 43s - loss: 0.0012 - val_loss: 0.0014 - 43s/epoch - 516us/sample
Epoch 3/90
84077/84077 - 43s - loss: 9.0583e-04 - val_loss: 0.0105 - 43s/epoch - 515us/sample
Epoch 4/90
84077/84077 - 43s - loss: 8.0322e-04 - val_loss: 0.0469 - 43s/epoch - 516us/sample
Epoch 5/90
84077/84077 - 43s - loss: 7.5358e-04 - val_loss: 0.1008 - 43s/epoch - 516us/sample
Epoch 6/90
84077/84077 - 43s - loss: 7.2050e-04 - val_loss: 0.1097 - 43s/epoch - 516us/sample
Epoch 7/90
84077/84077 - 43s - loss: 6.9959e-04 - val_loss: 0.2030 - 43s/epoch - 517us/sample
Epoch 8/90
84077/84077 - 43s - loss: 6.8681e-04 - val_loss: 0.2028 - 43s/epoch - 516us/sample
Epoch 9/90
84077/84077 - 43s - loss: 6.7370e-04 - val_loss: 0.1927 - 43s/epoch - 517us/sample
Epoch 10/90
84077/84077 - 43s - loss: 6.6437e-04 - val_loss: 0.2439 - 43s/epoch - 516us/sample
Epoch 11/90
84077/84077 - 43s - loss: 6.5805e-04 - val_loss: 0.2795 - 43s/epoch - 517us/sample
Epoch 12/90
84077/84077 - 43s - loss: 6.5300e-04 - val_loss: 0.2761 - 43s/epoch - 517us/sample
Epoch 13/90
84077/84077 - 43s - loss: 6.4816e-04 - val_loss: 0.3051 - 43s/epoch - 517us/sample
Epoch 14/90
84077/84077 - 43s - loss: 6.4452e-04 - val_loss: 0.2502 - 43s/epoch - 517us/sample
Epoch 15/90
84077/84077 - 43s - loss: 6.4114e-04 - val_loss: 0.2593 - 43s/epoch - 516us/sample
Epoch 16/90
84077/84077 - 43s - loss: 6.3745e-04 - val_loss: 0.2639 - 43s/epoch - 516us/sample
Epoch 17/90
84077/84077 - 43s - loss: 6.3509e-04 - val_loss: 0.2946 - 43s/epoch - 517us/sample
Epoch 18/90
84077/84077 - 43s - loss: 6.3349e-04 - val_loss: 0.2602 - 43s/epoch - 517us/sample
Epoch 19/90
84077/84077 - 43s - loss: 6.2957e-04 - val_loss: 0.2948 - 43s/epoch - 516us/sample
Epoch 20/90
84077/84077 - 43s - loss: 6.2759e-04 - val_loss: 0.3040 - 43s/epoch - 515us/sample
Epoch 21/90
84077/84077 - 43s - loss: 6.2416e-04 - val_loss: 0.3022 - 43s/epoch - 516us/sample
Epoch 22/90
84077/84077 - 43s - loss: 6.2168e-04 - val_loss: 0.2797 - 43s/epoch - 517us/sample
Epoch 23/90
84077/84077 - 43s - loss: 6.2011e-04 - val_loss: 0.3087 - 43s/epoch - 516us/sample
Epoch 24/90
84077/84077 - 43s - loss: 6.1724e-04 - val_loss: 0.3590 - 43s/epoch - 516us/sample
Epoch 25/90
84077/84077 - 43s - loss: 6.1594e-04 - val_loss: 0.3257 - 43s/epoch - 516us/sample
Epoch 26/90
84077/84077 - 43s - loss: 6.1505e-04 - val_loss: 0.2927 - 43s/epoch - 516us/sample
Epoch 27/90
84077/84077 - 43s - loss: 6.1382e-04 - val_loss: 0.3769 - 43s/epoch - 516us/sample
Epoch 28/90
84077/84077 - 43s - loss: 6.1022e-04 - val_loss: 0.3833 - 43s/epoch - 516us/sample
Epoch 29/90
84077/84077 - 43s - loss: 6.0927e-04 - val_loss: 0.3662 - 43s/epoch - 516us/sample
Epoch 30/90
84077/84077 - 43s - loss: 6.0928e-04 - val_loss: 0.3677 - 43s/epoch - 517us/sample
Epoch 31/90
84077/84077 - 43s - loss: 6.0716e-04 - val_loss: 0.3602 - 43s/epoch - 516us/sample
Epoch 32/90
84077/84077 - 43s - loss: 6.0685e-04 - val_loss: 0.3633 - 43s/epoch - 515us/sample
Epoch 33/90
84077/84077 - 43s - loss: 6.0515e-04 - val_loss: 0.2981 - 43s/epoch - 517us/sample
Epoch 34/90
84077/84077 - 43s - loss: 6.0196e-04 - val_loss: 0.3567 - 43s/epoch - 517us/sample
Epoch 35/90
84077/84077 - 43s - loss: 6.0210e-04 - val_loss: 0.3921 - 43s/epoch - 515us/sample
Epoch 36/90
84077/84077 - 43s - loss: 5.9969e-04 - val_loss: 0.3650 - 43s/epoch - 516us/sample
Epoch 37/90
84077/84077 - 43s - loss: 5.9941e-04 - val_loss: 0.3328 - 43s/epoch - 517us/sample
Epoch 38/90
84077/84077 - 43s - loss: 5.9745e-04 - val_loss: 0.3505 - 43s/epoch - 516us/sample
Epoch 39/90
84077/84077 - 43s - loss: 5.9761e-04 - val_loss: 0.3677 - 43s/epoch - 515us/sample
Epoch 40/90
84077/84077 - 43s - loss: 5.9563e-04 - val_loss: 0.3950 - 43s/epoch - 517us/sample
Epoch 41/90
84077/84077 - 43s - loss: 5.9271e-04 - val_loss: 0.3487 - 43s/epoch - 516us/sample
Epoch 42/90
84077/84077 - 43s - loss: 5.9207e-04 - val_loss: 0.3441 - 43s/epoch - 516us/sample
Epoch 43/90
84077/84077 - 43s - loss: 5.9171e-04 - val_loss: 0.3412 - 43s/epoch - 517us/sample
Epoch 44/90
84077/84077 - 43s - loss: 5.8988e-04 - val_loss: 0.4161 - 43s/epoch - 516us/sample
Epoch 45/90
84077/84077 - 43s - loss: 5.8876e-04 - val_loss: 0.4008 - 43s/epoch - 516us/sample
Epoch 46/90
84077/84077 - 43s - loss: 5.8628e-04 - val_loss: 0.3750 - 43s/epoch - 517us/sample
Epoch 47/90
84077/84077 - 43s - loss: 5.8530e-04 - val_loss: 0.3914 - 43s/epoch - 516us/sample
Epoch 48/90
84077/84077 - 43s - loss: 5.8443e-04 - val_loss: 0.3651 - 43s/epoch - 515us/sample
Epoch 49/90
84077/84077 - 43s - loss: 5.8400e-04 - val_loss: 0.3733 - 43s/epoch - 517us/sample
Epoch 50/90
84077/84077 - 43s - loss: 5.8314e-04 - val_loss: 0.3626 - 43s/epoch - 516us/sample
Epoch 51/90
84077/84077 - 43s - loss: 5.8129e-04 - val_loss: 0.3627 - 43s/epoch - 517us/sample
Epoch 52/90
84077/84077 - 43s - loss: 5.8096e-04 - val_loss: 0.3399 - 43s/epoch - 516us/sample
Epoch 53/90
84077/84077 - 43s - loss: 5.8177e-04 - val_loss: 0.3583 - 43s/epoch - 517us/sample
Epoch 54/90
84077/84077 - 43s - loss: 5.7994e-04 - val_loss: 0.3515 - 43s/epoch - 515us/sample
Epoch 55/90
84077/84077 - 43s - loss: 5.7903e-04 - val_loss: 0.3883 - 43s/epoch - 517us/sample
Epoch 56/90
84077/84077 - 43s - loss: 5.7904e-04 - val_loss: 0.4284 - 43s/epoch - 517us/sample
Epoch 57/90
84077/84077 - 43s - loss: 5.7829e-04 - val_loss: 0.3058 - 43s/epoch - 515us/sample
Epoch 58/90
84077/84077 - 43s - loss: 5.7804e-04 - val_loss: 0.3126 - 43s/epoch - 517us/sample
Epoch 59/90
84077/84077 - 43s - loss: 5.7730e-04 - val_loss: 0.3187 - 43s/epoch - 517us/sample
Epoch 60/90
84077/84077 - 43s - loss: 5.7615e-04 - val_loss: 0.3176 - 43s/epoch - 517us/sample
Epoch 61/90
84077/84077 - 43s - loss: 5.7622e-04 - val_loss: 0.2886 - 43s/epoch - 517us/sample
Epoch 62/90
84077/84077 - 43s - loss: 5.7586e-04 - val_loss: 0.3364 - 43s/epoch - 516us/sample
Epoch 63/90
84077/84077 - 43s - loss: 5.7505e-04 - val_loss: 0.3541 - 43s/epoch - 517us/sample
Epoch 64/90
84077/84077 - 43s - loss: 5.7562e-04 - val_loss: 0.3000 - 43s/epoch - 516us/sample
Epoch 65/90
84077/84077 - 43s - loss: 5.7317e-04 - val_loss: 0.3536 - 43s/epoch - 516us/sample
Epoch 66/90
84077/84077 - 44s - loss: 5.7459e-04 - val_loss: 0.3722 - 44s/epoch - 518us/sample
Epoch 67/90
84077/84077 - 43s - loss: 5.7255e-04 - val_loss: 0.2998 - 43s/epoch - 517us/sample
Epoch 68/90
84077/84077 - 43s - loss: 5.7433e-04 - val_loss: 0.2963 - 43s/epoch - 517us/sample
Epoch 69/90
84077/84077 - 44s - loss: 5.7250e-04 - val_loss: 0.3108 - 44s/epoch - 518us/sample
Epoch 70/90
84077/84077 - 43s - loss: 5.7222e-04 - val_loss: 0.2498 - 43s/epoch - 516us/sample
Epoch 71/90
84077/84077 - 43s - loss: 5.7213e-04 - val_loss: 0.2685 - 43s/epoch - 517us/sample
Epoch 72/90
84077/84077 - 43s - loss: 5.7092e-04 - val_loss: 0.3151 - 43s/epoch - 517us/sample
Epoch 73/90
84077/84077 - 44s - loss: 5.7140e-04 - val_loss: 0.2786 - 44s/epoch - 517us/sample
Epoch 74/90
84077/84077 - 43s - loss: 5.7093e-04 - val_loss: 0.3064 - 43s/epoch - 516us/sample
Epoch 75/90
84077/84077 - 43s - loss: 5.7005e-04 - val_loss: 0.2917 - 43s/epoch - 517us/sample
Epoch 76/90
84077/84077 - 43s - loss: 5.6976e-04 - val_loss: 0.3047 - 43s/epoch - 517us/sample
Epoch 77/90
84077/84077 - 44s - loss: 5.6938e-04 - val_loss: 0.2584 - 44s/epoch - 518us/sample
Epoch 78/90
84077/84077 - 43s - loss: 5.6998e-04 - val_loss: 0.2941 - 43s/epoch - 517us/sample
Epoch 79/90
84077/84077 - 43s - loss: 5.6882e-04 - val_loss: 0.2998 - 43s/epoch - 516us/sample
Epoch 80/90
84077/84077 - 43s - loss: 5.6831e-04 - val_loss: 0.2940 - 43s/epoch - 517us/sample
Epoch 81/90
84077/84077 - 43s - loss: 5.6757e-04 - val_loss: 0.2449 - 43s/epoch - 517us/sample
Epoch 82/90
84077/84077 - 43s - loss: 5.6755e-04 - val_loss: 0.3038 - 43s/epoch - 516us/sample
Epoch 83/90
84077/84077 - 43s - loss: 5.6771e-04 - val_loss: 0.2867 - 43s/epoch - 516us/sample
Epoch 84/90
84077/84077 - 43s - loss: 5.6653e-04 - val_loss: 0.2828 - 43s/epoch - 517us/sample
Epoch 85/90
84077/84077 - 43s - loss: 5.6710e-04 - val_loss: 0.3477 - 43s/epoch - 517us/sample
Epoch 86/90
84077/84077 - 43s - loss: 5.6606e-04 - val_loss: 0.2531 - 43s/epoch - 516us/sample
Epoch 87/90
84077/84077 - 43s - loss: 5.6569e-04 - val_loss: 0.2706 - 43s/epoch - 516us/sample
Epoch 88/90
84077/84077 - 43s - loss: 5.6626e-04 - val_loss: 0.3020 - 43s/epoch - 517us/sample
Epoch 89/90
84077/84077 - 43s - loss: 5.6551e-04 - val_loss: 0.2750 - 43s/epoch - 517us/sample
Epoch 90/90
84077/84077 - 43s - loss: 5.6504e-04 - val_loss: 0.2811 - 43s/epoch - 515us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.2811404850353095
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:21:35.507599: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_31/outputlayer/BiasAdd' id:40121 op device:{requested: '', assigned: ''} def:{{{node decoder_model_31/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_31/outputlayer/MatMul, decoder_model_31/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.18551700687931597
cosine 0.18371282342232703
MAE: 0.04692575577552342
RMSE: 0.5285125738594406
r2: -217.19465250264682
RMSE zero-vector: 0.04004287452915337
['2.4000000000000004custom_VAE', 'mse', 8, 90, 0.0012000000000000001, 0.3, 282, 0.0005650351819132967, 0.2811404850353095, 0.18551700687931597, 0.18371282342232703, 0.04692575577552342, 0.5285125738594406, -217.19465250264682, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_96 (BatchN  (None, 2074)        8296        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_96 (ReLU)                (None, 2074)         0           ['batch_normalization_96[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          585150      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          585150      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2632897     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,769,349
Trainable params: 5,760,489
Non-trainable params: 8,860
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 05:21:56.877607: W tensorflow/c/c_api.cc:291] Operation '{name:'training_64/Adam/dense_dec1_32/bias/v/Assign' id:42029 op device:{requested: '', assigned: ''} def:{{{node training_64/Adam/dense_dec1_32/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_64/Adam/dense_dec1_32/bias/v, training_64/Adam/dense_dec1_32/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:22:10.802389: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_32/mul' id:41405 op device:{requested: '', assigned: ''} def:{{{node loss_32/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_32/mul/x, loss_32/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 22s - loss: 0.3243 - val_loss: 0.0020 - 22s/epoch - 265us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0020 - val_loss: 0.0012 - 9s/epoch - 111us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0021 - val_loss: 0.0015 - 9s/epoch - 110us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 9.3203e-04 - 9s/epoch - 110us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 8.9608e-04 - 9s/epoch - 110us/sample
Epoch 6/85
84077/84077 - 9s - loss: 8.1006e-04 - val_loss: 7.2108e-04 - 9s/epoch - 109us/sample
Epoch 7/85
84077/84077 - 9s - loss: 9.1594e-04 - val_loss: 6.3081e-04 - 9s/epoch - 109us/sample
Epoch 8/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 7.0755e-04 - 9s/epoch - 109us/sample
Epoch 9/85
84077/84077 - 9s - loss: 0.0021 - val_loss: 0.0022 - 9s/epoch - 109us/sample
Epoch 10/85
84077/84077 - 9s - loss: 6.8633e-04 - val_loss: 4.5409e-04 - 9s/epoch - 110us/sample
Epoch 11/85
84077/84077 - 9s - loss: 5.0499e-04 - val_loss: 4.2070e-04 - 9s/epoch - 110us/sample
Epoch 12/85
84077/84077 - 9s - loss: 4.6730e-04 - val_loss: 4.0119e-04 - 9s/epoch - 110us/sample
Epoch 13/85
84077/84077 - 9s - loss: 4.5195e-04 - val_loss: 3.3628e-04 - 9s/epoch - 110us/sample
Epoch 14/85
84077/84077 - 9s - loss: 3.7570e-04 - val_loss: 3.1011e-04 - 9s/epoch - 109us/sample
Epoch 15/85
84077/84077 - 9s - loss: 3.4765e-04 - val_loss: 3.0350e-04 - 9s/epoch - 109us/sample
Epoch 16/85
84077/84077 - 9s - loss: 3.2610e-04 - val_loss: 3.2123e-04 - 9s/epoch - 109us/sample
Epoch 17/85
84077/84077 - 9s - loss: 3.1597e-04 - val_loss: 2.7900e-04 - 9s/epoch - 110us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.9694e-04 - val_loss: 2.4699e-04 - 9s/epoch - 110us/sample
Epoch 19/85
84077/84077 - 9s - loss: 2.8182e-04 - val_loss: 2.5301e-04 - 9s/epoch - 111us/sample
Epoch 20/85
84077/84077 - 9s - loss: 2.7017e-04 - val_loss: 2.3138e-04 - 9s/epoch - 110us/sample
Epoch 21/85
84077/84077 - 9s - loss: 2.5921e-04 - val_loss: 2.2387e-04 - 9s/epoch - 110us/sample
Epoch 22/85
84077/84077 - 9s - loss: 2.4943e-04 - val_loss: 2.1460e-04 - 9s/epoch - 110us/sample
Epoch 23/85
84077/84077 - 9s - loss: 2.4374e-04 - val_loss: 2.0791e-04 - 9s/epoch - 110us/sample
Epoch 24/85
84077/84077 - 9s - loss: 2.3439e-04 - val_loss: 2.0186e-04 - 9s/epoch - 110us/sample
Epoch 25/85
84077/84077 - 9s - loss: 2.2953e-04 - val_loss: 2.0341e-04 - 9s/epoch - 110us/sample
Epoch 26/85
84077/84077 - 9s - loss: 2.2344e-04 - val_loss: 1.9636e-04 - 9s/epoch - 111us/sample
Epoch 27/85
84077/84077 - 9s - loss: 2.2043e-04 - val_loss: 1.9371e-04 - 9s/epoch - 110us/sample
Epoch 28/85
84077/84077 - 9s - loss: 2.1578e-04 - val_loss: 1.8982e-04 - 9s/epoch - 110us/sample
Epoch 29/85
84077/84077 - 9s - loss: 2.1196e-04 - val_loss: 1.8717e-04 - 9s/epoch - 110us/sample
Epoch 30/85
84077/84077 - 9s - loss: 2.0923e-04 - val_loss: 1.8739e-04 - 9s/epoch - 110us/sample
Epoch 31/85
84077/84077 - 9s - loss: 2.0725e-04 - val_loss: 1.8451e-04 - 9s/epoch - 110us/sample
Epoch 32/85
84077/84077 - 9s - loss: 2.0329e-04 - val_loss: 1.8167e-04 - 9s/epoch - 110us/sample
Epoch 33/85
84077/84077 - 9s - loss: 2.0046e-04 - val_loss: 1.8229e-04 - 9s/epoch - 111us/sample
Epoch 34/85
84077/84077 - 9s - loss: 1.9916e-04 - val_loss: 1.8104e-04 - 9s/epoch - 110us/sample
Epoch 35/85
84077/84077 - 9s - loss: 1.9712e-04 - val_loss: 1.7787e-04 - 9s/epoch - 110us/sample
Epoch 36/85
84077/84077 - 9s - loss: 1.9393e-04 - val_loss: 1.8310e-04 - 9s/epoch - 110us/sample
Epoch 37/85
84077/84077 - 9s - loss: 1.9333e-04 - val_loss: 1.7408e-04 - 9s/epoch - 110us/sample
Epoch 38/85
84077/84077 - 9s - loss: 1.9087e-04 - val_loss: 1.7347e-04 - 9s/epoch - 110us/sample
Epoch 39/85
84077/84077 - 9s - loss: 1.8836e-04 - val_loss: 1.7267e-04 - 9s/epoch - 110us/sample
Epoch 40/85
84077/84077 - 9s - loss: 1.8746e-04 - val_loss: 1.7126e-04 - 9s/epoch - 111us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.8773e-04 - val_loss: 1.7147e-04 - 9s/epoch - 110us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.8555e-04 - val_loss: 1.6949e-04 - 9s/epoch - 110us/sample
Epoch 43/85
84077/84077 - 9s - loss: 1.8590e-04 - val_loss: 1.6858e-04 - 9s/epoch - 110us/sample
Epoch 44/85
84077/84077 - 9s - loss: 1.8531e-04 - val_loss: 1.6859e-04 - 9s/epoch - 110us/sample
Epoch 45/85
84077/84077 - 9s - loss: 1.8206e-04 - val_loss: 1.6685e-04 - 9s/epoch - 110us/sample
Epoch 46/85
84077/84077 - 9s - loss: 1.7945e-04 - val_loss: 1.6512e-04 - 9s/epoch - 110us/sample
Epoch 47/85
84077/84077 - 9s - loss: 1.7979e-04 - val_loss: 1.6460e-04 - 9s/epoch - 111us/sample
Epoch 48/85
84077/84077 - 9s - loss: 1.7776e-04 - val_loss: 1.6361e-04 - 9s/epoch - 111us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.7594e-04 - val_loss: 1.6374e-04 - 9s/epoch - 110us/sample
Epoch 50/85
84077/84077 - 9s - loss: 1.7609e-04 - val_loss: 1.6529e-04 - 9s/epoch - 110us/sample
Epoch 51/85
84077/84077 - 9s - loss: 1.7591e-04 - val_loss: 1.6243e-04 - 9s/epoch - 110us/sample
Epoch 52/85
84077/84077 - 9s - loss: 1.7515e-04 - val_loss: 1.6190e-04 - 9s/epoch - 110us/sample
Epoch 53/85
84077/84077 - 9s - loss: 1.7387e-04 - val_loss: 1.5979e-04 - 9s/epoch - 110us/sample
Epoch 54/85
84077/84077 - 9s - loss: 1.7293e-04 - val_loss: 1.6118e-04 - 9s/epoch - 110us/sample
Epoch 55/85
84077/84077 - 10s - loss: 1.7195e-04 - val_loss: 1.5955e-04 - 10s/epoch - 115us/sample
Epoch 56/85
84077/84077 - 10s - loss: 1.7177e-04 - val_loss: 1.5841e-04 - 10s/epoch - 113us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.7139e-04 - val_loss: 1.5756e-04 - 9s/epoch - 110us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.6941e-04 - val_loss: 1.5888e-04 - 9s/epoch - 110us/sample
Epoch 59/85
84077/84077 - 9s - loss: 1.6883e-04 - val_loss: 1.5712e-04 - 9s/epoch - 110us/sample
Epoch 60/85
84077/84077 - 9s - loss: 1.6877e-04 - val_loss: 1.5773e-04 - 9s/epoch - 110us/sample
Epoch 61/85
84077/84077 - 9s - loss: 1.6828e-04 - val_loss: 1.5703e-04 - 9s/epoch - 110us/sample
Epoch 62/85
84077/84077 - 9s - loss: 1.6918e-04 - val_loss: 1.5662e-04 - 9s/epoch - 111us/sample
Epoch 63/85
84077/84077 - 9s - loss: 1.6835e-04 - val_loss: 1.5750e-04 - 9s/epoch - 110us/sample
Epoch 64/85
84077/84077 - 9s - loss: 1.6686e-04 - val_loss: 1.5391e-04 - 9s/epoch - 111us/sample
Epoch 65/85
84077/84077 - 9s - loss: 1.6543e-04 - val_loss: 1.5507e-04 - 9s/epoch - 110us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.6508e-04 - val_loss: 1.5485e-04 - 9s/epoch - 110us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.6510e-04 - val_loss: 1.5375e-04 - 9s/epoch - 110us/sample
Epoch 68/85
84077/84077 - 9s - loss: 1.6419e-04 - val_loss: 1.5406e-04 - 9s/epoch - 110us/sample
Epoch 69/85
84077/84077 - 9s - loss: 1.6343e-04 - val_loss: 1.5450e-04 - 9s/epoch - 111us/sample
Epoch 70/85
84077/84077 - 9s - loss: 1.6283e-04 - val_loss: 1.5218e-04 - 9s/epoch - 110us/sample
Epoch 71/85
84077/84077 - 9s - loss: 1.6330e-04 - val_loss: 1.5151e-04 - 9s/epoch - 111us/sample
Epoch 72/85
84077/84077 - 9s - loss: 1.6224e-04 - val_loss: 1.5220e-04 - 9s/epoch - 110us/sample
Epoch 73/85
84077/84077 - 9s - loss: 1.6175e-04 - val_loss: 1.5122e-04 - 9s/epoch - 110us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.6170e-04 - val_loss: 1.5119e-04 - 9s/epoch - 110us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.6072e-04 - val_loss: 1.5682e-04 - 9s/epoch - 111us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.6054e-04 - val_loss: 1.5174e-04 - 9s/epoch - 111us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.5985e-04 - val_loss: 1.4922e-04 - 9s/epoch - 110us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.5944e-04 - val_loss: 1.4938e-04 - 9s/epoch - 110us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.5907e-04 - val_loss: 1.4746e-04 - 9s/epoch - 110us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.5914e-04 - val_loss: 1.4919e-04 - 9s/epoch - 110us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.5830e-04 - val_loss: 1.4685e-04 - 9s/epoch - 110us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.5809e-04 - val_loss: 1.4780e-04 - 9s/epoch - 111us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.5766e-04 - val_loss: 1.4715e-04 - 9s/epoch - 111us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.5769e-04 - val_loss: 1.4603e-04 - 9s/epoch - 110us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.5693e-04 - val_loss: 1.4531e-04 - 9s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0001453074833398234
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:35:13.314480: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_32/outputlayer/BiasAdd' id:41376 op device:{requested: '', assigned: ''} def:{{{node decoder_model_32/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_32/outputlayer/MatMul, decoder_model_32/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01806195603779887
cosine 0.01783940628535095
MAE: 0.0021155167552610694
RMSE: 0.007951577999486072
r2: 0.950888369802555
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 85, 0.001, 0.3, 282, 0.0001569297067950231, 0.0001453074833398234, 0.01806195603779887, 0.01783940628535095, 0.0021155167552610694, 0.007951577999486072, 0.950888369802555, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 90 0.001 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_99 (BatchN  (None, 2168)        8672        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_99 (ReLU)                (None, 2168)         0           ['batch_normalization_99[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          611658      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          611658      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2748517     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,027,097
Trainable params: 6,017,861
Non-trainable params: 9,236
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 05:35:35.219098: W tensorflow/c/c_api.cc:291] Operation '{name:'training_66/Adam/outputlayer_33/kernel/m/Assign' id:43245 op device:{requested: '', assigned: ''} def:{{{node training_66/Adam/outputlayer_33/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_66/Adam/outputlayer_33/kernel/m, training_66/Adam/outputlayer_33/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:35:49.608173: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_33/mul' id:42667 op device:{requested: '', assigned: ''} def:{{{node loss_33/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_33/mul/x, loss_33/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 23s - loss: 0.0046 - val_loss: 0.0013 - 23s/epoch - 273us/sample
Epoch 2/90
84077/84077 - 9s - loss: 0.0014 - val_loss: 8.7343e-04 - 9s/epoch - 109us/sample
Epoch 3/90
84077/84077 - 9s - loss: 0.0011 - val_loss: 7.2639e-04 - 9s/epoch - 109us/sample
Epoch 4/90
84077/84077 - 9s - loss: 6.4288e-04 - val_loss: 6.4016e-04 - 9s/epoch - 109us/sample
Epoch 5/90
84077/84077 - 9s - loss: 8.1242e-04 - val_loss: 4.9311e-04 - 9s/epoch - 109us/sample
Epoch 6/90
84077/84077 - 9s - loss: 4.9092e-04 - val_loss: 4.4010e-04 - 9s/epoch - 110us/sample
Epoch 7/90
84077/84077 - 9s - loss: 4.0068e-04 - val_loss: 3.6839e-04 - 9s/epoch - 110us/sample
Epoch 8/90
84077/84077 - 9s - loss: 3.4654e-04 - val_loss: 4.1860e-04 - 9s/epoch - 109us/sample
Epoch 9/90
84077/84077 - 9s - loss: 3.0000e-04 - val_loss: 2.5783e-04 - 9s/epoch - 109us/sample
Epoch 10/90
84077/84077 - 9s - loss: 2.7128e-04 - val_loss: 2.2863e-04 - 9s/epoch - 109us/sample
Epoch 11/90
84077/84077 - 9s - loss: 2.4530e-04 - val_loss: 1.9783e-04 - 9s/epoch - 109us/sample
Epoch 12/90
84077/84077 - 9s - loss: 2.1964e-04 - val_loss: 1.7856e-04 - 9s/epoch - 109us/sample
Epoch 13/90
84077/84077 - 9s - loss: 1.9965e-04 - val_loss: 1.6864e-04 - 9s/epoch - 109us/sample
Epoch 14/90
84077/84077 - 9s - loss: 1.8911e-04 - val_loss: 1.5152e-04 - 9s/epoch - 110us/sample
Epoch 15/90
84077/84077 - 9s - loss: 1.7656e-04 - val_loss: 1.5119e-04 - 9s/epoch - 110us/sample
Epoch 16/90
84077/84077 - 9s - loss: 1.6706e-04 - val_loss: 1.4092e-04 - 9s/epoch - 109us/sample
Epoch 17/90
84077/84077 - 9s - loss: 1.6181e-04 - val_loss: 1.3801e-04 - 9s/epoch - 109us/sample
Epoch 18/90
84077/84077 - 9s - loss: 1.5627e-04 - val_loss: 1.3494e-04 - 9s/epoch - 109us/sample
Epoch 19/90
84077/84077 - 9s - loss: 1.5237e-04 - val_loss: 1.3200e-04 - 9s/epoch - 109us/sample
Epoch 20/90
84077/84077 - 9s - loss: 1.4929e-04 - val_loss: 1.2739e-04 - 9s/epoch - 110us/sample
Epoch 21/90
84077/84077 - 9s - loss: 1.4573e-04 - val_loss: 1.2582e-04 - 9s/epoch - 110us/sample
Epoch 22/90
84077/84077 - 9s - loss: 1.4375e-04 - val_loss: 1.2514e-04 - 9s/epoch - 109us/sample
Epoch 23/90
84077/84077 - 9s - loss: 1.4047e-04 - val_loss: 1.2196e-04 - 9s/epoch - 109us/sample
Epoch 24/90
84077/84077 - 9s - loss: 1.3949e-04 - val_loss: 1.2177e-04 - 9s/epoch - 109us/sample
Epoch 25/90
84077/84077 - 9s - loss: 1.3697e-04 - val_loss: 1.2113e-04 - 9s/epoch - 109us/sample
Epoch 26/90
84077/84077 - 9s - loss: 1.3520e-04 - val_loss: 1.1846e-04 - 9s/epoch - 109us/sample
Epoch 27/90
84077/84077 - 9s - loss: 1.3336e-04 - val_loss: 1.1737e-04 - 9s/epoch - 109us/sample
Epoch 28/90
84077/84077 - 9s - loss: 1.3178e-04 - val_loss: 1.1512e-04 - 9s/epoch - 110us/sample
Epoch 29/90
84077/84077 - 9s - loss: 1.3186e-04 - val_loss: 1.1503e-04 - 9s/epoch - 110us/sample
Epoch 30/90
84077/84077 - 9s - loss: 1.2936e-04 - val_loss: 1.1542e-04 - 9s/epoch - 109us/sample
Epoch 31/90
84077/84077 - 9s - loss: 1.2805e-04 - val_loss: 1.1350e-04 - 9s/epoch - 109us/sample
Epoch 32/90
84077/84077 - 9s - loss: 1.2701e-04 - val_loss: 1.1263e-04 - 9s/epoch - 109us/sample
Epoch 33/90
84077/84077 - 9s - loss: 1.2633e-04 - val_loss: 1.1249e-04 - 9s/epoch - 109us/sample
Epoch 34/90
84077/84077 - 9s - loss: 1.2517e-04 - val_loss: 1.1208e-04 - 9s/epoch - 109us/sample
Epoch 35/90
84077/84077 - 9s - loss: 1.2448e-04 - val_loss: 1.1060e-04 - 9s/epoch - 109us/sample
Epoch 36/90
84077/84077 - 9s - loss: 1.2363e-04 - val_loss: 1.0992e-04 - 9s/epoch - 110us/sample
Epoch 37/90
84077/84077 - 9s - loss: 1.2281e-04 - val_loss: 1.1068e-04 - 9s/epoch - 110us/sample
Epoch 38/90
84077/84077 - 9s - loss: 1.2249e-04 - val_loss: 1.0970e-04 - 9s/epoch - 110us/sample
Epoch 39/90
84077/84077 - 9s - loss: 1.2115e-04 - val_loss: 1.0897e-04 - 9s/epoch - 109us/sample
Epoch 40/90
84077/84077 - 9s - loss: 1.2068e-04 - val_loss: 1.0836e-04 - 9s/epoch - 109us/sample
Epoch 41/90
84077/84077 - 9s - loss: 1.2004e-04 - val_loss: 1.0722e-04 - 9s/epoch - 109us/sample
Epoch 42/90
84077/84077 - 9s - loss: 1.1932e-04 - val_loss: 1.0589e-04 - 9s/epoch - 109us/sample
Epoch 43/90
84077/84077 - 9s - loss: 1.1889e-04 - val_loss: 1.0441e-04 - 9s/epoch - 110us/sample
Epoch 44/90
84077/84077 - 9s - loss: 1.1804e-04 - val_loss: 1.0535e-04 - 9s/epoch - 110us/sample
Epoch 45/90
84077/84077 - 9s - loss: 1.1716e-04 - val_loss: 1.0495e-04 - 9s/epoch - 109us/sample
Epoch 46/90
84077/84077 - 9s - loss: 1.1692e-04 - val_loss: 1.0560e-04 - 9s/epoch - 109us/sample
Epoch 47/90
84077/84077 - 9s - loss: 1.1684e-04 - val_loss: 1.0502e-04 - 9s/epoch - 109us/sample
Epoch 48/90
84077/84077 - 9s - loss: 1.1614e-04 - val_loss: 1.0358e-04 - 9s/epoch - 109us/sample
Epoch 49/90
84077/84077 - 9s - loss: 1.1563e-04 - val_loss: 1.0379e-04 - 9s/epoch - 109us/sample
Epoch 50/90
84077/84077 - 9s - loss: 1.1521e-04 - val_loss: 1.0445e-04 - 9s/epoch - 109us/sample
Epoch 51/90
84077/84077 - 9s - loss: 1.1499e-04 - val_loss: 1.0257e-04 - 9s/epoch - 110us/sample
Epoch 52/90
84077/84077 - 9s - loss: 1.1444e-04 - val_loss: 1.0386e-04 - 9s/epoch - 110us/sample
Epoch 53/90
84077/84077 - 9s - loss: 1.1421e-04 - val_loss: 1.0302e-04 - 9s/epoch - 109us/sample
Epoch 54/90
84077/84077 - 9s - loss: 1.1387e-04 - val_loss: 1.0310e-04 - 9s/epoch - 109us/sample
Epoch 55/90
84077/84077 - 9s - loss: 1.1357e-04 - val_loss: 1.0259e-04 - 9s/epoch - 109us/sample
Epoch 56/90
84077/84077 - 9s - loss: 1.1333e-04 - val_loss: 1.0124e-04 - 9s/epoch - 109us/sample
Epoch 57/90
84077/84077 - 9s - loss: 1.1277e-04 - val_loss: 1.0125e-04 - 9s/epoch - 109us/sample
Epoch 58/90
84077/84077 - 9s - loss: 1.1230e-04 - val_loss: 1.0106e-04 - 9s/epoch - 109us/sample
Epoch 59/90
84077/84077 - 9s - loss: 1.1196e-04 - val_loss: 1.0148e-04 - 9s/epoch - 110us/sample
Epoch 60/90
84077/84077 - 9s - loss: 1.1154e-04 - val_loss: 1.0108e-04 - 9s/epoch - 110us/sample
Epoch 61/90
84077/84077 - 9s - loss: 1.1170e-04 - val_loss: 1.0166e-04 - 9s/epoch - 109us/sample
Epoch 62/90
84077/84077 - 9s - loss: 1.1123e-04 - val_loss: 1.0199e-04 - 9s/epoch - 109us/sample
Epoch 63/90
84077/84077 - 9s - loss: 1.1085e-04 - val_loss: 1.0078e-04 - 9s/epoch - 109us/sample
Epoch 64/90
84077/84077 - 9s - loss: 1.1066e-04 - val_loss: 1.0055e-04 - 9s/epoch - 109us/sample
Epoch 65/90
84077/84077 - 9s - loss: 1.1035e-04 - val_loss: 9.9319e-05 - 9s/epoch - 109us/sample
Epoch 66/90
84077/84077 - 9s - loss: 1.1033e-04 - val_loss: 9.9457e-05 - 9s/epoch - 109us/sample
Epoch 67/90
84077/84077 - 9s - loss: 1.1005e-04 - val_loss: 9.9465e-05 - 9s/epoch - 110us/sample
Epoch 68/90
84077/84077 - 9s - loss: 1.0960e-04 - val_loss: 1.0018e-04 - 9s/epoch - 110us/sample
Epoch 69/90
84077/84077 - 9s - loss: 1.0979e-04 - val_loss: 9.8773e-05 - 9s/epoch - 109us/sample
Epoch 70/90
84077/84077 - 9s - loss: 1.0909e-04 - val_loss: 1.0091e-04 - 9s/epoch - 109us/sample
Epoch 71/90
84077/84077 - 9s - loss: 1.0926e-04 - val_loss: 9.9986e-05 - 9s/epoch - 109us/sample
Epoch 72/90
84077/84077 - 9s - loss: 1.0879e-04 - val_loss: 9.9657e-05 - 9s/epoch - 109us/sample
Epoch 73/90
84077/84077 - 9s - loss: 1.0838e-04 - val_loss: 9.8841e-05 - 9s/epoch - 109us/sample
Epoch 74/90
84077/84077 - 9s - loss: 1.0846e-04 - val_loss: 9.8808e-05 - 9s/epoch - 109us/sample
Epoch 75/90
84077/84077 - 9s - loss: 1.0965e-04 - val_loss: 9.8458e-05 - 9s/epoch - 110us/sample
Epoch 76/90
84077/84077 - 9s - loss: 1.0816e-04 - val_loss: 9.9079e-05 - 9s/epoch - 109us/sample
Epoch 77/90
84077/84077 - 9s - loss: 1.0776e-04 - val_loss: 9.8483e-05 - 9s/epoch - 110us/sample
Epoch 78/90
84077/84077 - 9s - loss: 1.0773e-04 - val_loss: 9.9284e-05 - 9s/epoch - 109us/sample
Epoch 79/90
84077/84077 - 9s - loss: 1.0760e-04 - val_loss: 9.8403e-05 - 9s/epoch - 109us/sample
Epoch 80/90
84077/84077 - 9s - loss: 1.0740e-04 - val_loss: 1.0059e-04 - 9s/epoch - 109us/sample
Epoch 81/90
84077/84077 - 9s - loss: 1.0730e-04 - val_loss: 9.7987e-05 - 9s/epoch - 109us/sample
Epoch 82/90
84077/84077 - 9s - loss: 1.0692e-04 - val_loss: 9.8243e-05 - 9s/epoch - 110us/sample
Epoch 83/90
84077/84077 - 9s - loss: 1.0710e-04 - val_loss: 9.7182e-05 - 9s/epoch - 110us/sample
Epoch 84/90
84077/84077 - 9s - loss: 1.0669e-04 - val_loss: 9.8792e-05 - 9s/epoch - 110us/sample
Epoch 85/90
84077/84077 - 9s - loss: 1.0654e-04 - val_loss: 9.7751e-05 - 9s/epoch - 110us/sample
Epoch 86/90
84077/84077 - 9s - loss: 1.0624e-04 - val_loss: 9.6484e-05 - 9s/epoch - 109us/sample
Epoch 87/90
84077/84077 - 9s - loss: 1.0599e-04 - val_loss: 9.7160e-05 - 9s/epoch - 109us/sample
Epoch 88/90
84077/84077 - 9s - loss: 1.0591e-04 - val_loss: 9.8004e-05 - 9s/epoch - 109us/sample
Epoch 89/90
84077/84077 - 9s - loss: 1.0594e-04 - val_loss: 9.6735e-05 - 9s/epoch - 109us/sample
Epoch 90/90
84077/84077 - 9s - loss: 1.0560e-04 - val_loss: 9.6662e-05 - 9s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 9.666203886289065e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:49:32.495810: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_33/outputlayer/BiasAdd' id:42631 op device:{requested: '', assigned: ''} def:{{{node decoder_model_33/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_33/outputlayer/MatMul, decoder_model_33/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023023304126202408
cosine 0.022744452181704403
MAE: 0.0023549542793615303
RMSE: 0.008453452814079939
r2: 0.9446223448140592
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'logcosh', 64, 90, 0.001, 0.3, 282, 0.00010559691282101513, 9.666203886289065e-05, 0.023023304126202408, 0.022744452181704403, 0.0023549542793615303, 0.008453452814079939, 0.9446223448140592, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0012000000000000001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_102 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_102 (ReLU)               (None, 1980)         0           ['batch_normalization_102[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 05:49:54.892555: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_103/moving_mean/Assign' id:43682 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_103/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_103/moving_mean, batch_normalization_103/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:50:13.116809: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_34/mul' id:43945 op device:{requested: '', assigned: ''} def:{{{node loss_34/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_34/mul/x, loss_34/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0072 - val_loss: 0.0025 - 27s/epoch - 325us/sample
Epoch 2/85
84077/84077 - 14s - loss: 0.0017 - val_loss: 0.0011 - 14s/epoch - 161us/sample
Epoch 3/85
84077/84077 - 13s - loss: 0.0013 - val_loss: 9.8242e-04 - 13s/epoch - 160us/sample
Epoch 4/85
84077/84077 - 13s - loss: 9.2699e-04 - val_loss: 6.6319e-04 - 13s/epoch - 159us/sample
Epoch 5/85
84077/84077 - 13s - loss: 8.6047e-04 - val_loss: 5.6322e-04 - 13s/epoch - 160us/sample
Epoch 6/85
84077/84077 - 13s - loss: 6.0705e-04 - val_loss: 4.9588e-04 - 13s/epoch - 159us/sample
Epoch 7/85
84077/84077 - 13s - loss: 5.2157e-04 - val_loss: 4.5742e-04 - 13s/epoch - 161us/sample
Epoch 8/85
84077/84077 - 14s - loss: 4.6491e-04 - val_loss: 3.8272e-04 - 14s/epoch - 161us/sample
Epoch 9/85
84077/84077 - 13s - loss: 3.9701e-04 - val_loss: 3.5502e-04 - 13s/epoch - 160us/sample
Epoch 10/85
84077/84077 - 13s - loss: 3.6208e-04 - val_loss: 3.4553e-04 - 13s/epoch - 160us/sample
Epoch 11/85
84077/84077 - 13s - loss: 3.3442e-04 - val_loss: 3.3602e-04 - 13s/epoch - 160us/sample
Epoch 12/85
84077/84077 - 13s - loss: 3.1637e-04 - val_loss: 3.3608e-04 - 13s/epoch - 160us/sample
Epoch 13/85
84077/84077 - 14s - loss: 3.0135e-04 - val_loss: 3.0755e-04 - 14s/epoch - 161us/sample
Epoch 14/85
84077/84077 - 13s - loss: 2.8971e-04 - val_loss: 3.0406e-04 - 13s/epoch - 160us/sample
Epoch 15/85
84077/84077 - 13s - loss: 2.7895e-04 - val_loss: 2.8939e-04 - 13s/epoch - 160us/sample
Epoch 16/85
84077/84077 - 13s - loss: 2.7057e-04 - val_loss: 2.7676e-04 - 13s/epoch - 160us/sample
Epoch 17/85
84077/84077 - 13s - loss: 2.6259e-04 - val_loss: 2.8016e-04 - 13s/epoch - 161us/sample
Epoch 18/85
84077/84077 - 14s - loss: 2.5653e-04 - val_loss: 2.6232e-04 - 14s/epoch - 161us/sample
Epoch 19/85
84077/84077 - 13s - loss: 2.5174e-04 - val_loss: 2.8587e-04 - 13s/epoch - 160us/sample
Epoch 20/85
84077/84077 - 13s - loss: 2.4671e-04 - val_loss: 2.7867e-04 - 13s/epoch - 160us/sample
Epoch 21/85
84077/84077 - 13s - loss: 2.4225e-04 - val_loss: 2.6541e-04 - 13s/epoch - 160us/sample
Epoch 22/85
84077/84077 - 13s - loss: 2.3931e-04 - val_loss: 2.6611e-04 - 13s/epoch - 160us/sample
Epoch 23/85
84077/84077 - 14s - loss: 2.3631e-04 - val_loss: 2.6264e-04 - 14s/epoch - 161us/sample
Epoch 24/85
84077/84077 - 13s - loss: 2.3408e-04 - val_loss: 2.5891e-04 - 13s/epoch - 160us/sample
Epoch 25/85
84077/84077 - 13s - loss: 2.3202e-04 - val_loss: 2.6607e-04 - 13s/epoch - 160us/sample
Epoch 26/85
84077/84077 - 13s - loss: 2.3000e-04 - val_loss: 2.5085e-04 - 13s/epoch - 160us/sample
Epoch 27/85
84077/84077 - 13s - loss: 2.2717e-04 - val_loss: 2.5430e-04 - 13s/epoch - 160us/sample
Epoch 28/85
84077/84077 - 14s - loss: 2.2388e-04 - val_loss: 2.4297e-04 - 14s/epoch - 161us/sample
Epoch 29/85
84077/84077 - 14s - loss: 2.2194e-04 - val_loss: 2.5556e-04 - 14s/epoch - 161us/sample
Epoch 30/85
84077/84077 - 13s - loss: 2.2116e-04 - val_loss: 2.4407e-04 - 13s/epoch - 160us/sample
Epoch 31/85
84077/84077 - 13s - loss: 2.1949e-04 - val_loss: 2.4126e-04 - 13s/epoch - 160us/sample
Epoch 32/85
84077/84077 - 14s - loss: 2.1763e-04 - val_loss: 2.4319e-04 - 14s/epoch - 162us/sample
Epoch 33/85
84077/84077 - 13s - loss: 2.1668e-04 - val_loss: 2.4451e-04 - 13s/epoch - 160us/sample
Epoch 34/85
84077/84077 - 13s - loss: 2.1535e-04 - val_loss: 2.4762e-04 - 13s/epoch - 160us/sample
Epoch 35/85
84077/84077 - 13s - loss: 2.1329e-04 - val_loss: 2.4350e-04 - 13s/epoch - 160us/sample
Epoch 36/85
84077/84077 - 13s - loss: 2.1273e-04 - val_loss: 2.3965e-04 - 13s/epoch - 160us/sample
Epoch 37/85
84077/84077 - 14s - loss: 2.1228e-04 - val_loss: 2.3612e-04 - 14s/epoch - 162us/sample
Epoch 38/85
84077/84077 - 14s - loss: 2.1043e-04 - val_loss: 2.4364e-04 - 14s/epoch - 161us/sample
Epoch 39/85
84077/84077 - 13s - loss: 2.0951e-04 - val_loss: 2.3146e-04 - 13s/epoch - 160us/sample
Epoch 40/85
84077/84077 - 13s - loss: 2.0894e-04 - val_loss: 2.4132e-04 - 13s/epoch - 160us/sample
Epoch 41/85
84077/84077 - 13s - loss: 2.0752e-04 - val_loss: 2.3808e-04 - 13s/epoch - 160us/sample
Epoch 42/85
84077/84077 - 14s - loss: 2.0704e-04 - val_loss: 2.3570e-04 - 14s/epoch - 161us/sample
Epoch 43/85
84077/84077 - 14s - loss: 2.0651e-04 - val_loss: 2.4065e-04 - 14s/epoch - 161us/sample
Epoch 44/85
84077/84077 - 13s - loss: 2.0562e-04 - val_loss: 2.3204e-04 - 13s/epoch - 160us/sample
Epoch 45/85
84077/84077 - 13s - loss: 2.0457e-04 - val_loss: 2.2936e-04 - 13s/epoch - 160us/sample
Epoch 46/85
84077/84077 - 14s - loss: 2.0387e-04 - val_loss: 2.3555e-04 - 14s/epoch - 162us/sample
Epoch 47/85
84077/84077 - 14s - loss: 2.0282e-04 - val_loss: 2.2486e-04 - 14s/epoch - 161us/sample
Epoch 48/85
84077/84077 - 14s - loss: 2.0296e-04 - val_loss: 2.2728e-04 - 14s/epoch - 161us/sample
Epoch 49/85
84077/84077 - 13s - loss: 2.0178e-04 - val_loss: 2.3585e-04 - 13s/epoch - 160us/sample
Epoch 50/85
84077/84077 - 13s - loss: 2.0110e-04 - val_loss: 2.3445e-04 - 13s/epoch - 161us/sample
Epoch 51/85
84077/84077 - 14s - loss: 2.0065e-04 - val_loss: 2.2849e-04 - 14s/epoch - 161us/sample
Epoch 52/85
84077/84077 - 14s - loss: 1.9990e-04 - val_loss: 2.3159e-04 - 14s/epoch - 161us/sample
Epoch 53/85
84077/84077 - 14s - loss: 1.9963e-04 - val_loss: 2.3222e-04 - 14s/epoch - 161us/sample
Epoch 54/85
84077/84077 - 13s - loss: 1.9921e-04 - val_loss: 2.2904e-04 - 13s/epoch - 160us/sample
Epoch 55/85
84077/84077 - 13s - loss: 1.9824e-04 - val_loss: 2.2812e-04 - 13s/epoch - 160us/sample
Epoch 56/85
84077/84077 - 14s - loss: 1.9782e-04 - val_loss: 2.2290e-04 - 14s/epoch - 161us/sample
Epoch 57/85
84077/84077 - 14s - loss: 1.9727e-04 - val_loss: 2.2703e-04 - 14s/epoch - 161us/sample
Epoch 58/85
84077/84077 - 14s - loss: 1.9787e-04 - val_loss: 2.2601e-04 - 14s/epoch - 161us/sample
Epoch 59/85
84077/84077 - 13s - loss: 1.9714e-04 - val_loss: 2.1692e-04 - 13s/epoch - 160us/sample
Epoch 60/85
84077/84077 - 14s - loss: 1.9580e-04 - val_loss: 2.2532e-04 - 14s/epoch - 161us/sample
Epoch 61/85
84077/84077 - 14s - loss: 1.9542e-04 - val_loss: 2.2746e-04 - 14s/epoch - 162us/sample
Epoch 62/85
84077/84077 - 14s - loss: 1.9553e-04 - val_loss: 2.2511e-04 - 14s/epoch - 161us/sample
Epoch 63/85
84077/84077 - 13s - loss: 1.9570e-04 - val_loss: 2.2601e-04 - 13s/epoch - 160us/sample
Epoch 64/85
84077/84077 - 13s - loss: 1.9470e-04 - val_loss: 2.2465e-04 - 13s/epoch - 160us/sample
Epoch 65/85
84077/84077 - 14s - loss: 1.9349e-04 - val_loss: 2.2080e-04 - 14s/epoch - 162us/sample
Epoch 66/85
84077/84077 - 14s - loss: 1.9397e-04 - val_loss: 2.2102e-04 - 14s/epoch - 161us/sample
Epoch 67/85
84077/84077 - 13s - loss: 1.9319e-04 - val_loss: 2.2761e-04 - 13s/epoch - 160us/sample
Epoch 68/85
84077/84077 - 13s - loss: 1.9279e-04 - val_loss: 2.2816e-04 - 13s/epoch - 160us/sample
Epoch 69/85
84077/84077 - 14s - loss: 1.9186e-04 - val_loss: 2.2011e-04 - 14s/epoch - 161us/sample
Epoch 70/85
84077/84077 - 14s - loss: 1.9205e-04 - val_loss: 2.2688e-04 - 14s/epoch - 161us/sample
Epoch 71/85
84077/84077 - 14s - loss: 1.9149e-04 - val_loss: 2.2365e-04 - 14s/epoch - 161us/sample
Epoch 72/85
84077/84077 - 13s - loss: 1.9120e-04 - val_loss: 2.2408e-04 - 13s/epoch - 160us/sample
Epoch 73/85
84077/84077 - 14s - loss: 1.9133e-04 - val_loss: 2.2162e-04 - 14s/epoch - 161us/sample
Epoch 74/85
84077/84077 - 14s - loss: 1.9083e-04 - val_loss: 2.1849e-04 - 14s/epoch - 162us/sample
Epoch 75/85
84077/84077 - 14s - loss: 1.9022e-04 - val_loss: 2.1761e-04 - 14s/epoch - 161us/sample
Epoch 76/85
84077/84077 - 14s - loss: 1.9028e-04 - val_loss: 2.2106e-04 - 14s/epoch - 161us/sample
Epoch 77/85
84077/84077 - 13s - loss: 1.8999e-04 - val_loss: 2.2123e-04 - 13s/epoch - 160us/sample
Epoch 78/85
84077/84077 - 14s - loss: 1.8945e-04 - val_loss: 2.1715e-04 - 14s/epoch - 161us/sample
Epoch 79/85
84077/84077 - 14s - loss: 1.8883e-04 - val_loss: 2.1800e-04 - 14s/epoch - 162us/sample
Epoch 80/85
84077/84077 - 14s - loss: 1.8970e-04 - val_loss: 2.1472e-04 - 14s/epoch - 161us/sample
Epoch 81/85
84077/84077 - 13s - loss: 1.8936e-04 - val_loss: 2.1556e-04 - 13s/epoch - 160us/sample
Epoch 82/85
84077/84077 - 13s - loss: 1.8862e-04 - val_loss: 2.1600e-04 - 13s/epoch - 160us/sample
Epoch 83/85
84077/84077 - 14s - loss: 1.8814e-04 - val_loss: 2.1641e-04 - 14s/epoch - 161us/sample
Epoch 84/85
84077/84077 - 14s - loss: 1.8802e-04 - val_loss: 2.2121e-04 - 14s/epoch - 162us/sample
Epoch 85/85
84077/84077 - 14s - loss: 1.8764e-04 - val_loss: 2.1254e-04 - 14s/epoch - 161us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0002125365925753458
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:09:12.421529: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_34/outputlayer/BiasAdd' id:43916 op device:{requested: '', assigned: ''} def:{{{node decoder_model_34/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_34/outputlayer/MatMul, decoder_model_34/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.026313006014901344
cosine 0.02600006269385441
MAE: 0.0022501601305288275
RMSE: 0.011592916420122674
r2: 0.8951479981678853
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 85, 0.0012000000000000001, 0.3, 282, 0.00018764094945812192, 0.0002125365925753458, 0.026313006014901344, 0.02600006269385441, 0.0022501601305288275, 0.011592916420122674, 0.8951479981678853, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 4
Fitness    = 595.8845894037719
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 595.8845894037719.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [595.8845894037719, 595.8845894037719, 595.8845894037719, 595.8845894037719]
[2.1 85 0.001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_105 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_105 (ReLU)               (None, 1980)         0           ['batch_normalization_105[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 06:09:35.412526: W tensorflow/c/c_api.cc:291] Operation '{name:'training_70/Adam/batch_normalization_106/beta/v/Assign' id:45834 op device:{requested: '', assigned: ''} def:{{{node training_70/Adam/batch_normalization_106/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_70/Adam/batch_normalization_106/beta/v, training_70/Adam/batch_normalization_106/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:09:50.187523: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_35/mul' id:45200 op device:{requested: '', assigned: ''} def:{{{node loss_35/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_35/mul/x, loss_35/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 24s - loss: 0.0079 - val_loss: 0.0018 - 24s/epoch - 285us/sample
Epoch 2/85
84077/84077 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 111us/sample
Epoch 3/85
84077/84077 - 9s - loss: 0.0055 - val_loss: 0.0013 - 9s/epoch - 111us/sample
Epoch 4/85
84077/84077 - 9s - loss: 0.0012 - val_loss: 8.5038e-04 - 9s/epoch - 111us/sample
Epoch 5/85
84077/84077 - 9s - loss: 0.0010 - val_loss: 7.3580e-04 - 9s/epoch - 111us/sample
Epoch 6/85
84077/84077 - 9s - loss: 7.8411e-04 - val_loss: 7.4467e-04 - 9s/epoch - 111us/sample
Epoch 7/85
84077/84077 - 9s - loss: 6.2412e-04 - val_loss: 5.1112e-04 - 9s/epoch - 111us/sample
Epoch 8/85
84077/84077 - 9s - loss: 5.6430e-04 - val_loss: 4.4516e-04 - 9s/epoch - 112us/sample
Epoch 9/85
84077/84077 - 9s - loss: 4.7339e-04 - val_loss: 3.5958e-04 - 9s/epoch - 111us/sample
Epoch 10/85
84077/84077 - 9s - loss: 4.1693e-04 - val_loss: 3.3610e-04 - 9s/epoch - 111us/sample
Epoch 11/85
84077/84077 - 9s - loss: 3.5921e-04 - val_loss: 2.9882e-04 - 9s/epoch - 111us/sample
Epoch 12/85
84077/84077 - 9s - loss: 3.4783e-04 - val_loss: 2.6937e-04 - 9s/epoch - 111us/sample
Epoch 13/85
84077/84077 - 9s - loss: 3.0738e-04 - val_loss: 2.4476e-04 - 9s/epoch - 111us/sample
Epoch 14/85
84077/84077 - 9s - loss: 2.8620e-04 - val_loss: 2.3193e-04 - 9s/epoch - 111us/sample
Epoch 15/85
84077/84077 - 9s - loss: 2.6960e-04 - val_loss: 2.2194e-04 - 9s/epoch - 112us/sample
Epoch 16/85
84077/84077 - 9s - loss: 2.5616e-04 - val_loss: 2.1896e-04 - 9s/epoch - 111us/sample
Epoch 17/85
84077/84077 - 9s - loss: 2.4574e-04 - val_loss: 2.5848e-04 - 9s/epoch - 111us/sample
Epoch 18/85
84077/84077 - 9s - loss: 2.3648e-04 - val_loss: 1.9925e-04 - 9s/epoch - 111us/sample
Epoch 19/85
84077/84077 - 9s - loss: 2.2571e-04 - val_loss: 1.9405e-04 - 9s/epoch - 111us/sample
Epoch 20/85
84077/84077 - 9s - loss: 2.1894e-04 - val_loss: 1.9304e-04 - 9s/epoch - 111us/sample
Epoch 21/85
84077/84077 - 9s - loss: 2.1334e-04 - val_loss: 1.8548e-04 - 9s/epoch - 111us/sample
Epoch 22/85
84077/84077 - 9s - loss: 2.0707e-04 - val_loss: 1.8092e-04 - 9s/epoch - 111us/sample
Epoch 23/85
84077/84077 - 9s - loss: 2.0165e-04 - val_loss: 1.8170e-04 - 9s/epoch - 112us/sample
Epoch 24/85
84077/84077 - 9s - loss: 1.9961e-04 - val_loss: 1.7813e-04 - 9s/epoch - 111us/sample
Epoch 25/85
84077/84077 - 9s - loss: 1.9466e-04 - val_loss: 1.7337e-04 - 9s/epoch - 112us/sample
Epoch 26/85
84077/84077 - 9s - loss: 1.9170e-04 - val_loss: 1.7347e-04 - 9s/epoch - 111us/sample
Epoch 27/85
84077/84077 - 9s - loss: 1.8765e-04 - val_loss: 1.6885e-04 - 9s/epoch - 111us/sample
Epoch 28/85
84077/84077 - 9s - loss: 1.8618e-04 - val_loss: 1.6844e-04 - 9s/epoch - 111us/sample
Epoch 29/85
84077/84077 - 9s - loss: 1.8344e-04 - val_loss: 1.6468e-04 - 9s/epoch - 111us/sample
Epoch 30/85
84077/84077 - 9s - loss: 1.8064e-04 - val_loss: 1.6317e-04 - 9s/epoch - 111us/sample
Epoch 31/85
84077/84077 - 9s - loss: 1.7921e-04 - val_loss: 1.6174e-04 - 9s/epoch - 112us/sample
Epoch 32/85
84077/84077 - 9s - loss: 1.7702e-04 - val_loss: 1.6017e-04 - 9s/epoch - 111us/sample
Epoch 33/85
84077/84077 - 9s - loss: 1.7573e-04 - val_loss: 1.6077e-04 - 9s/epoch - 111us/sample
Epoch 34/85
84077/84077 - 9s - loss: 1.7775e-04 - val_loss: 1.6069e-04 - 9s/epoch - 111us/sample
Epoch 35/85
84077/84077 - 9s - loss: 1.7237e-04 - val_loss: 1.5531e-04 - 9s/epoch - 111us/sample
Epoch 36/85
84077/84077 - 9s - loss: 1.7120e-04 - val_loss: 1.6022e-04 - 9s/epoch - 111us/sample
Epoch 37/85
84077/84077 - 9s - loss: 1.6901e-04 - val_loss: 1.5474e-04 - 9s/epoch - 111us/sample
Epoch 38/85
84077/84077 - 9s - loss: 1.6791e-04 - val_loss: 1.5381e-04 - 9s/epoch - 112us/sample
Epoch 39/85
84077/84077 - 9s - loss: 1.6678e-04 - val_loss: 1.5230e-04 - 9s/epoch - 112us/sample
Epoch 40/85
84077/84077 - 9s - loss: 1.6623e-04 - val_loss: 1.5342e-04 - 9s/epoch - 111us/sample
Epoch 41/85
84077/84077 - 9s - loss: 1.6531e-04 - val_loss: 1.5231e-04 - 9s/epoch - 111us/sample
Epoch 42/85
84077/84077 - 9s - loss: 1.6466e-04 - val_loss: 1.5220e-04 - 9s/epoch - 111us/sample
Epoch 43/85
84077/84077 - 9s - loss: 1.6270e-04 - val_loss: 1.5011e-04 - 9s/epoch - 111us/sample
Epoch 44/85
84077/84077 - 9s - loss: 1.6144e-04 - val_loss: 1.5106e-04 - 9s/epoch - 111us/sample
Epoch 45/85
84077/84077 - 9s - loss: 1.6071e-04 - val_loss: 1.4934e-04 - 9s/epoch - 112us/sample
Epoch 46/85
84077/84077 - 9s - loss: 1.6067e-04 - val_loss: 1.4991e-04 - 9s/epoch - 111us/sample
Epoch 47/85
84077/84077 - 9s - loss: 1.5840e-04 - val_loss: 1.4876e-04 - 9s/epoch - 111us/sample
Epoch 48/85
84077/84077 - 9s - loss: 1.6055e-04 - val_loss: 1.4583e-04 - 9s/epoch - 111us/sample
Epoch 49/85
84077/84077 - 9s - loss: 1.5791e-04 - val_loss: 1.4877e-04 - 9s/epoch - 111us/sample
Epoch 50/85
84077/84077 - 9s - loss: 1.5724e-04 - val_loss: 1.4795e-04 - 9s/epoch - 111us/sample
Epoch 51/85
84077/84077 - 9s - loss: 1.5735e-04 - val_loss: 1.4472e-04 - 9s/epoch - 111us/sample
Epoch 52/85
84077/84077 - 9s - loss: 1.5538e-04 - val_loss: 1.4572e-04 - 9s/epoch - 112us/sample
Epoch 53/85
84077/84077 - 9s - loss: 1.5466e-04 - val_loss: 1.4559e-04 - 9s/epoch - 112us/sample
Epoch 54/85
84077/84077 - 9s - loss: 1.5443e-04 - val_loss: 1.4240e-04 - 9s/epoch - 111us/sample
Epoch 55/85
84077/84077 - 9s - loss: 1.5438e-04 - val_loss: 1.4453e-04 - 9s/epoch - 111us/sample
Epoch 56/85
84077/84077 - 9s - loss: 1.5297e-04 - val_loss: 1.4147e-04 - 9s/epoch - 111us/sample
Epoch 57/85
84077/84077 - 9s - loss: 1.5231e-04 - val_loss: 1.4595e-04 - 9s/epoch - 111us/sample
Epoch 58/85
84077/84077 - 9s - loss: 1.5216e-04 - val_loss: 1.4448e-04 - 9s/epoch - 111us/sample
Epoch 59/85
84077/84077 - 9s - loss: 1.5146e-04 - val_loss: 1.4041e-04 - 9s/epoch - 112us/sample
Epoch 60/85
84077/84077 - 9s - loss: 1.5104e-04 - val_loss: 1.4228e-04 - 9s/epoch - 112us/sample
Epoch 61/85
84077/84077 - 9s - loss: 1.5138e-04 - val_loss: 1.4084e-04 - 9s/epoch - 112us/sample
Epoch 62/85
84077/84077 - 9s - loss: 1.5145e-04 - val_loss: 1.4240e-04 - 9s/epoch - 111us/sample
Epoch 63/85
84077/84077 - 9s - loss: 1.4985e-04 - val_loss: 1.4126e-04 - 9s/epoch - 111us/sample
Epoch 64/85
84077/84077 - 9s - loss: 1.4899e-04 - val_loss: 1.3971e-04 - 9s/epoch - 111us/sample
Epoch 65/85
84077/84077 - 9s - loss: 1.4933e-04 - val_loss: 1.3972e-04 - 9s/epoch - 111us/sample
Epoch 66/85
84077/84077 - 9s - loss: 1.4797e-04 - val_loss: 1.3808e-04 - 9s/epoch - 111us/sample
Epoch 67/85
84077/84077 - 9s - loss: 1.4857e-04 - val_loss: 1.3688e-04 - 9s/epoch - 111us/sample
Epoch 68/85
84077/84077 - 9s - loss: 1.4738e-04 - val_loss: 1.3816e-04 - 9s/epoch - 112us/sample
Epoch 69/85
84077/84077 - 9s - loss: 1.4652e-04 - val_loss: 1.3799e-04 - 9s/epoch - 111us/sample
Epoch 70/85
84077/84077 - 9s - loss: 1.4675e-04 - val_loss: 1.3690e-04 - 9s/epoch - 111us/sample
Epoch 71/85
84077/84077 - 9s - loss: 1.5098e-04 - val_loss: 1.3973e-04 - 9s/epoch - 111us/sample
Epoch 72/85
84077/84077 - 9s - loss: 1.4624e-04 - val_loss: 1.3872e-04 - 9s/epoch - 111us/sample
Epoch 73/85
84077/84077 - 9s - loss: 1.4879e-04 - val_loss: 1.3936e-04 - 9s/epoch - 111us/sample
Epoch 74/85
84077/84077 - 9s - loss: 1.4570e-04 - val_loss: 1.3753e-04 - 9s/epoch - 111us/sample
Epoch 75/85
84077/84077 - 9s - loss: 1.4481e-04 - val_loss: 1.3559e-04 - 9s/epoch - 112us/sample
Epoch 76/85
84077/84077 - 9s - loss: 1.4474e-04 - val_loss: 1.3461e-04 - 9s/epoch - 111us/sample
Epoch 77/85
84077/84077 - 9s - loss: 1.4427e-04 - val_loss: 1.3518e-04 - 9s/epoch - 111us/sample
Epoch 78/85
84077/84077 - 9s - loss: 1.4480e-04 - val_loss: 1.3621e-04 - 9s/epoch - 111us/sample
Epoch 79/85
84077/84077 - 9s - loss: 1.4356e-04 - val_loss: 1.3500e-04 - 9s/epoch - 111us/sample
Epoch 80/85
84077/84077 - 9s - loss: 1.4318e-04 - val_loss: 1.3539e-04 - 9s/epoch - 111us/sample
Epoch 81/85
84077/84077 - 9s - loss: 1.4280e-04 - val_loss: 1.3294e-04 - 9s/epoch - 111us/sample
Epoch 82/85
84077/84077 - 9s - loss: 1.4240e-04 - val_loss: 1.3424e-04 - 9s/epoch - 113us/sample
Epoch 83/85
84077/84077 - 9s - loss: 1.4187e-04 - val_loss: 1.3230e-04 - 9s/epoch - 111us/sample
Epoch 84/85
84077/84077 - 9s - loss: 1.4219e-04 - val_loss: 1.3522e-04 - 9s/epoch - 112us/sample
Epoch 85/85
84077/84077 - 9s - loss: 1.4204e-04 - val_loss: 1.3372e-04 - 9s/epoch - 111us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00013372294488736813
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:23:00.990915: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_35/outputlayer/BiasAdd' id:45171 op device:{requested: '', assigned: ''} def:{{{node decoder_model_35/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_35/outputlayer/MatMul, decoder_model_35/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.015484845550984866
cosine 0.0152977548415974
MAE: 0.0018851767623073174
RMSE: 0.007281863190763504
r2: 0.9592029144875317
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 85, 0.001, 0.3, 282, 0.00014204231514979989, 0.00013372294488736813, 0.015484845550984866, 0.0152977548415974, 0.0018851767623073174, 0.007281863190763504, 0.9592029144875317, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0008 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_108 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_108 (ReLU)               (None, 2074)         0           ['batch_normalization_108[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          585150      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          585150      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2632897     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,769,349
Trainable params: 5,760,489
Non-trainable params: 8,860
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 06:23:24.482024: W tensorflow/c/c_api.cc:291] Operation '{name:'training_72/Adam/dense_enc0_36/bias/v/Assign' id:47029 op device:{requested: '', assigned: ''} def:{{{node training_72/Adam/dense_enc0_36/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_72/Adam/dense_enc0_36/bias/v, training_72/Adam/dense_enc0_36/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:23:39.650177: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_36/mul' id:46455 op device:{requested: '', assigned: ''} def:{{{node loss_36/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_36/mul/x, loss_36/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 25s - loss: 50757.2812 - val_loss: 0.0019 - 25s/epoch - 293us/sample
Epoch 2/85
84077/84077 - 10s - loss: 0.0015 - val_loss: 0.0013 - 10s/epoch - 115us/sample
Epoch 3/85
84077/84077 - 10s - loss: 0.0029 - val_loss: 0.0020 - 10s/epoch - 115us/sample
Epoch 4/85
84077/84077 - 10s - loss: 0.0015 - val_loss: 9.1582e-04 - 10s/epoch - 115us/sample
Epoch 5/85
84077/84077 - 10s - loss: 0.0932 - val_loss: 0.0010 - 10s/epoch - 115us/sample
Epoch 6/85
84077/84077 - 10s - loss: 0.0012 - val_loss: 0.0016 - 10s/epoch - 116us/sample
Epoch 7/85
84077/84077 - 10s - loss: 36.0426 - val_loss: 8.6343e-04 - 10s/epoch - 116us/sample
Epoch 8/85
84077/84077 - 10s - loss: 7.0127e-04 - val_loss: 5.7404e-04 - 10s/epoch - 116us/sample
Epoch 9/85
84077/84077 - 10s - loss: 7.2609e-04 - val_loss: 9.1916e-04 - 10s/epoch - 116us/sample
Epoch 10/85
84077/84077 - 10s - loss: 6.3511e-04 - val_loss: 8.9252e-04 - 10s/epoch - 115us/sample
Epoch 11/85
84077/84077 - 10s - loss: 6.5221e-04 - val_loss: 0.0011 - 10s/epoch - 115us/sample
Epoch 12/85
84077/84077 - 10s - loss: 6.3794e-04 - val_loss: 4.2057e-04 - 10s/epoch - 116us/sample
Epoch 13/85
84077/84077 - 10s - loss: 4.7178e-04 - val_loss: 3.8535e-04 - 10s/epoch - 116us/sample
Epoch 14/85
84077/84077 - 10s - loss: 3.9672e-04 - val_loss: 3.5865e-04 - 10s/epoch - 117us/sample
Epoch 15/85
84077/84077 - 10s - loss: 3.7491e-04 - val_loss: 3.7451e-04 - 10s/epoch - 116us/sample
Epoch 16/85
84077/84077 - 10s - loss: 3.5755e-04 - val_loss: 3.0350e-04 - 10s/epoch - 115us/sample
Epoch 17/85
84077/84077 - 10s - loss: 3.2562e-04 - val_loss: 2.8790e-04 - 10s/epoch - 115us/sample
Epoch 18/85
84077/84077 - 10s - loss: 3.0867e-04 - val_loss: 2.7430e-04 - 10s/epoch - 115us/sample
Epoch 19/85
84077/84077 - 10s - loss: 2.9483e-04 - val_loss: 2.6598e-04 - 10s/epoch - 116us/sample
Epoch 20/85
84077/84077 - 10s - loss: 2.8141e-04 - val_loss: 2.5805e-04 - 10s/epoch - 116us/sample
Epoch 21/85
84077/84077 - 10s - loss: 2.7257e-04 - val_loss: 2.4613e-04 - 10s/epoch - 117us/sample
Epoch 22/85
84077/84077 - 10s - loss: 2.6200e-04 - val_loss: 2.4030e-04 - 10s/epoch - 116us/sample
Epoch 23/85
84077/84077 - 10s - loss: 2.5327e-04 - val_loss: 2.3489e-04 - 10s/epoch - 115us/sample
Epoch 24/85
84077/84077 - 10s - loss: 2.4756e-04 - val_loss: 2.2657e-04 - 10s/epoch - 115us/sample
Epoch 25/85
84077/84077 - 10s - loss: 2.3990e-04 - val_loss: 2.2050e-04 - 10s/epoch - 116us/sample
Epoch 26/85
84077/84077 - 10s - loss: 2.3420e-04 - val_loss: 2.2171e-04 - 10s/epoch - 116us/sample
Epoch 27/85
84077/84077 - 10s - loss: 2.3322e-04 - val_loss: 2.1775e-04 - 10s/epoch - 117us/sample
Epoch 28/85
84077/84077 - 10s - loss: 2.2484e-04 - val_loss: 2.0993e-04 - 10s/epoch - 116us/sample
Epoch 29/85
84077/84077 - 10s - loss: 2.2006e-04 - val_loss: 2.1134e-04 - 10s/epoch - 116us/sample
Epoch 30/85
84077/84077 - 10s - loss: 2.1619e-04 - val_loss: 2.0715e-04 - 10s/epoch - 116us/sample
Epoch 31/85
84077/84077 - 10s - loss: 2.1398e-04 - val_loss: 2.0165e-04 - 10s/epoch - 116us/sample
Epoch 32/85
84077/84077 - 10s - loss: 2.0932e-04 - val_loss: 2.0293e-04 - 10s/epoch - 116us/sample
Epoch 33/85
84077/84077 - 10s - loss: 2.0731e-04 - val_loss: 2.0276e-04 - 10s/epoch - 116us/sample
Epoch 34/85
84077/84077 - 10s - loss: 2.0321e-04 - val_loss: 1.9088e-04 - 10s/epoch - 116us/sample
Epoch 35/85
84077/84077 - 10s - loss: 2.0079e-04 - val_loss: 1.8859e-04 - 10s/epoch - 116us/sample
Epoch 36/85
84077/84077 - 10s - loss: 1.9729e-04 - val_loss: 1.8813e-04 - 10s/epoch - 115us/sample
Epoch 37/85
84077/84077 - 10s - loss: 1.9436e-04 - val_loss: 1.8641e-04 - 10s/epoch - 116us/sample
Epoch 38/85
84077/84077 - 10s - loss: 1.9237e-04 - val_loss: 1.8293e-04 - 10s/epoch - 116us/sample
Epoch 39/85
84077/84077 - 10s - loss: 1.8907e-04 - val_loss: 1.7913e-04 - 10s/epoch - 116us/sample
Epoch 40/85
84077/84077 - 10s - loss: 1.8683e-04 - val_loss: 1.7813e-04 - 10s/epoch - 117us/sample
Epoch 41/85
84077/84077 - 10s - loss: 1.8385e-04 - val_loss: 1.7259e-04 - 10s/epoch - 117us/sample
Epoch 42/85
84077/84077 - 10s - loss: 1.8223e-04 - val_loss: 1.7227e-04 - 10s/epoch - 116us/sample
Epoch 43/85
84077/84077 - 10s - loss: 1.8013e-04 - val_loss: 1.7028e-04 - 10s/epoch - 116us/sample
Epoch 44/85
84077/84077 - 10s - loss: 1.7764e-04 - val_loss: 1.7076e-04 - 10s/epoch - 116us/sample
Epoch 45/85
84077/84077 - 10s - loss: 1.7662e-04 - val_loss: 1.6954e-04 - 10s/epoch - 116us/sample
Epoch 46/85
84077/84077 - 10s - loss: 1.7525e-04 - val_loss: 1.6826e-04 - 10s/epoch - 117us/sample
Epoch 47/85
84077/84077 - 10s - loss: 1.7407e-04 - val_loss: 1.6594e-04 - 10s/epoch - 116us/sample
Epoch 48/85
84077/84077 - 10s - loss: 1.7278e-04 - val_loss: 1.6713e-04 - 10s/epoch - 116us/sample
Epoch 49/85
84077/84077 - 10s - loss: 1.7198e-04 - val_loss: 1.6437e-04 - 10s/epoch - 116us/sample
Epoch 50/85
84077/84077 - 10s - loss: 1.7037e-04 - val_loss: 1.6206e-04 - 10s/epoch - 116us/sample
Epoch 51/85
84077/84077 - 10s - loss: 1.6960e-04 - val_loss: 1.6466e-04 - 10s/epoch - 116us/sample
Epoch 52/85
84077/84077 - 10s - loss: 1.6843e-04 - val_loss: 1.6206e-04 - 10s/epoch - 116us/sample
Epoch 53/85
84077/84077 - 10s - loss: 1.6838e-04 - val_loss: 1.6215e-04 - 10s/epoch - 117us/sample
Epoch 54/85
84077/84077 - 10s - loss: 1.6747e-04 - val_loss: 1.5966e-04 - 10s/epoch - 116us/sample
Epoch 55/85
84077/84077 - 10s - loss: 1.6666e-04 - val_loss: 1.5871e-04 - 10s/epoch - 116us/sample
Epoch 56/85
84077/84077 - 10s - loss: 1.6566e-04 - val_loss: 1.5957e-04 - 10s/epoch - 116us/sample
Epoch 57/85
84077/84077 - 10s - loss: 1.6473e-04 - val_loss: 1.6121e-04 - 10s/epoch - 116us/sample
Epoch 58/85
84077/84077 - 10s - loss: 1.6476e-04 - val_loss: 1.6001e-04 - 10s/epoch - 116us/sample
Epoch 59/85
84077/84077 - 10s - loss: 1.6344e-04 - val_loss: 1.6001e-04 - 10s/epoch - 116us/sample
Epoch 60/85
84077/84077 - 10s - loss: 1.6336e-04 - val_loss: 1.5737e-04 - 10s/epoch - 117us/sample
Epoch 61/85
84077/84077 - 10s - loss: 1.6232e-04 - val_loss: 1.5826e-04 - 10s/epoch - 116us/sample
Epoch 62/85
84077/84077 - 10s - loss: 1.6260e-04 - val_loss: 1.5687e-04 - 10s/epoch - 116us/sample
Epoch 63/85
84077/84077 - 10s - loss: 1.6125e-04 - val_loss: 1.5585e-04 - 10s/epoch - 116us/sample
Epoch 64/85
84077/84077 - 10s - loss: 1.6059e-04 - val_loss: 1.5582e-04 - 10s/epoch - 116us/sample
Epoch 65/85
84077/84077 - 10s - loss: 1.6033e-04 - val_loss: 1.5821e-04 - 10s/epoch - 116us/sample
Epoch 66/85
84077/84077 - 10s - loss: 1.6006e-04 - val_loss: 1.5537e-04 - 10s/epoch - 117us/sample
Epoch 67/85
84077/84077 - 10s - loss: 1.5885e-04 - val_loss: 1.5522e-04 - 10s/epoch - 116us/sample
Epoch 68/85
84077/84077 - 10s - loss: 1.5891e-04 - val_loss: 1.5364e-04 - 10s/epoch - 116us/sample
Epoch 69/85
84077/84077 - 10s - loss: 1.5874e-04 - val_loss: 1.5515e-04 - 10s/epoch - 116us/sample
Epoch 70/85
84077/84077 - 10s - loss: 1.5815e-04 - val_loss: 1.5285e-04 - 10s/epoch - 116us/sample
Epoch 71/85
84077/84077 - 10s - loss: 1.5761e-04 - val_loss: 1.5338e-04 - 10s/epoch - 116us/sample
Epoch 72/85
84077/84077 - 10s - loss: 1.5705e-04 - val_loss: 1.5683e-04 - 10s/epoch - 116us/sample
Epoch 73/85
84077/84077 - 10s - loss: 1.5674e-04 - val_loss: 1.5235e-04 - 10s/epoch - 117us/sample
Epoch 74/85
84077/84077 - 10s - loss: 1.5575e-04 - val_loss: 1.5062e-04 - 10s/epoch - 116us/sample
Epoch 75/85
84077/84077 - 10s - loss: 1.5576e-04 - val_loss: 1.5226e-04 - 10s/epoch - 116us/sample
Epoch 76/85
84077/84077 - 10s - loss: 1.5592e-04 - val_loss: 1.5210e-04 - 10s/epoch - 116us/sample
Epoch 77/85
84077/84077 - 10s - loss: 1.5543e-04 - val_loss: 1.4965e-04 - 10s/epoch - 116us/sample
Epoch 78/85
84077/84077 - 10s - loss: 1.5472e-04 - val_loss: 1.5328e-04 - 10s/epoch - 116us/sample
Epoch 79/85
84077/84077 - 10s - loss: 1.5427e-04 - val_loss: 1.4910e-04 - 10s/epoch - 117us/sample
Epoch 80/85
84077/84077 - 10s - loss: 1.5429e-04 - val_loss: 1.5215e-04 - 10s/epoch - 117us/sample
Epoch 81/85
84077/84077 - 10s - loss: 1.5351e-04 - val_loss: 1.5023e-04 - 10s/epoch - 116us/sample
Epoch 82/85
84077/84077 - 10s - loss: 1.5378e-04 - val_loss: 1.4868e-04 - 10s/epoch - 116us/sample
Epoch 83/85
84077/84077 - 10s - loss: 1.5305e-04 - val_loss: 1.5540e-04 - 10s/epoch - 116us/sample
Epoch 84/85
84077/84077 - 10s - loss: 1.5275e-04 - val_loss: 1.5024e-04 - 10s/epoch - 116us/sample
Epoch 85/85
84077/84077 - 10s - loss: 1.5232e-04 - val_loss: 1.4778e-04 - 10s/epoch - 116us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00014778219090056971
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:37:23.536483: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_36/outputlayer/BiasAdd' id:46426 op device:{requested: '', assigned: ''} def:{{{node decoder_model_36/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_36/outputlayer/MatMul, decoder_model_36/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.017931480903848045
cosine 0.017713311007866633
MAE: 0.0017591168851633977
RMSE: 0.008398525997593154
r2: 0.9450900585200174
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 85, 0.0008, 0.3, 282, 0.00015231594991720236, 0.00014778219090056971, 0.017931480903848045, 0.017713311007866633, 0.0017591168851633977, 0.008398525997593154, 0.9450900585200174, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 80 0.0008 32 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_111 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_111 (ReLU)               (None, 1886)         0           ['batch_normalization_111[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/80
2023-02-15 06:37:47.412226: W tensorflow/c/c_api.cc:291] Operation '{name:'training_74/Adam/dense_dec1_37/kernel/v/Assign' id:48329 op device:{requested: '', assigned: ''} def:{{{node training_74/Adam/dense_dec1_37/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_74/Adam/dense_dec1_37/kernel/v, training_74/Adam/dense_dec1_37/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:38:06.315948: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_37/mul' id:47710 op device:{requested: '', assigned: ''} def:{{{node loss_37/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_37/mul/x, loss_37/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 29s - loss: 0.0059 - val_loss: 0.0017 - 29s/epoch - 343us/sample
Epoch 2/80
84077/84077 - 15s - loss: 0.0019 - val_loss: 0.0022 - 15s/epoch - 179us/sample
Epoch 3/80
84077/84077 - 14s - loss: 0.0012 - val_loss: 8.1878e-04 - 14s/epoch - 163us/sample
Epoch 4/80
84077/84077 - 14s - loss: 8.0777e-04 - val_loss: 6.7861e-04 - 14s/epoch - 163us/sample
Epoch 5/80
84077/84077 - 14s - loss: 6.6539e-04 - val_loss: 5.3863e-04 - 14s/epoch - 163us/sample
Epoch 6/80
84077/84077 - 14s - loss: 5.4877e-04 - val_loss: 4.4129e-04 - 14s/epoch - 164us/sample
Epoch 7/80
84077/84077 - 14s - loss: 4.5866e-04 - val_loss: 4.0466e-04 - 14s/epoch - 164us/sample
Epoch 8/80
84077/84077 - 14s - loss: 4.0385e-04 - val_loss: 3.7515e-04 - 14s/epoch - 163us/sample
Epoch 9/80
84077/84077 - 14s - loss: 3.6436e-04 - val_loss: 3.5090e-04 - 14s/epoch - 163us/sample
Epoch 10/80
84077/84077 - 14s - loss: 3.3370e-04 - val_loss: 3.2508e-04 - 14s/epoch - 163us/sample
Epoch 11/80
84077/84077 - 14s - loss: 3.1162e-04 - val_loss: 3.1748e-04 - 14s/epoch - 164us/sample
Epoch 12/80
84077/84077 - 14s - loss: 2.9477e-04 - val_loss: 3.0476e-04 - 14s/epoch - 164us/sample
Epoch 13/80
84077/84077 - 14s - loss: 2.8291e-04 - val_loss: 3.0721e-04 - 14s/epoch - 163us/sample
Epoch 14/80
84077/84077 - 14s - loss: 2.7249e-04 - val_loss: 2.9066e-04 - 14s/epoch - 163us/sample
Epoch 15/80
84077/84077 - 14s - loss: 2.6517e-04 - val_loss: 2.8136e-04 - 14s/epoch - 164us/sample
Epoch 16/80
84077/84077 - 14s - loss: 2.5801e-04 - val_loss: 2.8256e-04 - 14s/epoch - 165us/sample
Epoch 17/80
84077/84077 - 14s - loss: 2.5159e-04 - val_loss: 2.7143e-04 - 14s/epoch - 163us/sample
Epoch 18/80
84077/84077 - 14s - loss: 2.4647e-04 - val_loss: 2.6874e-04 - 14s/epoch - 163us/sample
Epoch 19/80
84077/84077 - 14s - loss: 2.4249e-04 - val_loss: 2.5008e-04 - 14s/epoch - 163us/sample
Epoch 20/80
84077/84077 - 14s - loss: 2.3756e-04 - val_loss: 2.5841e-04 - 14s/epoch - 165us/sample
Epoch 21/80
84077/84077 - 14s - loss: 2.3448e-04 - val_loss: 2.6097e-04 - 14s/epoch - 165us/sample
Epoch 22/80
84077/84077 - 14s - loss: 2.3246e-04 - val_loss: 2.5351e-04 - 14s/epoch - 164us/sample
Epoch 23/80
84077/84077 - 14s - loss: 2.2804e-04 - val_loss: 2.5950e-04 - 14s/epoch - 163us/sample
Epoch 24/80
84077/84077 - 14s - loss: 2.2530e-04 - val_loss: 2.5146e-04 - 14s/epoch - 164us/sample
Epoch 25/80
84077/84077 - 14s - loss: 2.2341e-04 - val_loss: 2.3825e-04 - 14s/epoch - 164us/sample
Epoch 26/80
84077/84077 - 14s - loss: 2.2017e-04 - val_loss: 2.3932e-04 - 14s/epoch - 165us/sample
Epoch 27/80
84077/84077 - 14s - loss: 2.1869e-04 - val_loss: 2.4290e-04 - 14s/epoch - 164us/sample
Epoch 28/80
84077/84077 - 14s - loss: 2.1669e-04 - val_loss: 2.3935e-04 - 14s/epoch - 164us/sample
Epoch 29/80
84077/84077 - 14s - loss: 2.1594e-04 - val_loss: 2.3706e-04 - 14s/epoch - 163us/sample
Epoch 30/80
84077/84077 - 14s - loss: 2.1298e-04 - val_loss: 2.3968e-04 - 14s/epoch - 164us/sample
Epoch 31/80
84077/84077 - 14s - loss: 2.1227e-04 - val_loss: 2.3984e-04 - 14s/epoch - 165us/sample
Epoch 32/80
84077/84077 - 14s - loss: 2.0993e-04 - val_loss: 2.3504e-04 - 14s/epoch - 165us/sample
Epoch 33/80
84077/84077 - 14s - loss: 2.0863e-04 - val_loss: 2.3311e-04 - 14s/epoch - 164us/sample
Epoch 34/80
84077/84077 - 14s - loss: 2.0852e-04 - val_loss: 2.2935e-04 - 14s/epoch - 164us/sample
Epoch 35/80
84077/84077 - 14s - loss: 2.0814e-04 - val_loss: 2.2122e-04 - 14s/epoch - 164us/sample
Epoch 36/80
84077/84077 - 14s - loss: 2.0429e-04 - val_loss: 2.2393e-04 - 14s/epoch - 165us/sample
Epoch 37/80
84077/84077 - 14s - loss: 2.0465e-04 - val_loss: 2.3205e-04 - 14s/epoch - 165us/sample
Epoch 38/80
84077/84077 - 14s - loss: 2.0344e-04 - val_loss: 2.2135e-04 - 14s/epoch - 164us/sample
Epoch 39/80
84077/84077 - 14s - loss: 2.0220e-04 - val_loss: 2.2729e-04 - 14s/epoch - 164us/sample
Epoch 40/80
84077/84077 - 14s - loss: 2.0056e-04 - val_loss: 2.2451e-04 - 14s/epoch - 164us/sample
Epoch 41/80
84077/84077 - 14s - loss: 1.9976e-04 - val_loss: 2.2104e-04 - 14s/epoch - 165us/sample
Epoch 42/80
84077/84077 - 14s - loss: 1.9866e-04 - val_loss: 2.1604e-04 - 14s/epoch - 165us/sample
Epoch 43/80
84077/84077 - 14s - loss: 1.9690e-04 - val_loss: 2.1687e-04 - 14s/epoch - 164us/sample
Epoch 44/80
84077/84077 - 14s - loss: 1.9757e-04 - val_loss: 2.1183e-04 - 14s/epoch - 163us/sample
Epoch 45/80
84077/84077 - 14s - loss: 1.9533e-04 - val_loss: 2.1659e-04 - 14s/epoch - 164us/sample
Epoch 46/80
84077/84077 - 14s - loss: 1.9519e-04 - val_loss: 2.1653e-04 - 14s/epoch - 165us/sample
Epoch 47/80
84077/84077 - 14s - loss: 1.9399e-04 - val_loss: 2.1546e-04 - 14s/epoch - 165us/sample
Epoch 48/80
84077/84077 - 14s - loss: 1.9382e-04 - val_loss: 2.1554e-04 - 14s/epoch - 164us/sample
Epoch 49/80
84077/84077 - 14s - loss: 1.9492e-04 - val_loss: 2.1373e-04 - 14s/epoch - 163us/sample
Epoch 50/80
84077/84077 - 14s - loss: 1.9227e-04 - val_loss: 2.1464e-04 - 14s/epoch - 164us/sample
Epoch 51/80
84077/84077 - 14s - loss: 1.9335e-04 - val_loss: 2.1552e-04 - 14s/epoch - 165us/sample
Epoch 52/80
84077/84077 - 14s - loss: 1.9082e-04 - val_loss: 2.0474e-04 - 14s/epoch - 165us/sample
Epoch 53/80
84077/84077 - 14s - loss: 1.9125e-04 - val_loss: 2.1427e-04 - 14s/epoch - 164us/sample
Epoch 54/80
84077/84077 - 14s - loss: 1.9056e-04 - val_loss: 2.0516e-04 - 14s/epoch - 164us/sample
Epoch 55/80
84077/84077 - 14s - loss: 1.8917e-04 - val_loss: 2.1009e-04 - 14s/epoch - 164us/sample
Epoch 56/80
84077/84077 - 14s - loss: 1.8896e-04 - val_loss: 2.0979e-04 - 14s/epoch - 165us/sample
Epoch 57/80
84077/84077 - 14s - loss: 1.8781e-04 - val_loss: 2.0622e-04 - 14s/epoch - 165us/sample
Epoch 58/80
84077/84077 - 14s - loss: 1.8731e-04 - val_loss: 2.0449e-04 - 14s/epoch - 165us/sample
Epoch 59/80
84077/84077 - 14s - loss: 1.8714e-04 - val_loss: 2.0878e-04 - 14s/epoch - 164us/sample
Epoch 60/80
84077/84077 - 14s - loss: 1.8727e-04 - val_loss: 2.0822e-04 - 14s/epoch - 165us/sample
Epoch 61/80
84077/84077 - 14s - loss: 1.8659e-04 - val_loss: 2.0612e-04 - 14s/epoch - 166us/sample
Epoch 62/80
84077/84077 - 14s - loss: 1.8655e-04 - val_loss: 2.1297e-04 - 14s/epoch - 164us/sample
Epoch 63/80
84077/84077 - 14s - loss: 1.8511e-04 - val_loss: 2.0597e-04 - 14s/epoch - 164us/sample
Epoch 64/80
84077/84077 - 14s - loss: 1.8552e-04 - val_loss: 2.0480e-04 - 14s/epoch - 164us/sample
Epoch 65/80
84077/84077 - 14s - loss: 1.8415e-04 - val_loss: 2.0502e-04 - 14s/epoch - 165us/sample
Epoch 66/80
84077/84077 - 14s - loss: 1.8438e-04 - val_loss: 2.0197e-04 - 14s/epoch - 165us/sample
Epoch 67/80
84077/84077 - 14s - loss: 1.8386e-04 - val_loss: 2.0522e-04 - 14s/epoch - 165us/sample
Epoch 68/80
84077/84077 - 14s - loss: 1.8346e-04 - val_loss: 2.0206e-04 - 14s/epoch - 164us/sample
Epoch 69/80
84077/84077 - 14s - loss: 1.8236e-04 - val_loss: 1.9904e-04 - 14s/epoch - 164us/sample
Epoch 70/80
84077/84077 - 14s - loss: 1.8319e-04 - val_loss: 1.9537e-04 - 14s/epoch - 165us/sample
Epoch 71/80
84077/84077 - 14s - loss: 1.8147e-04 - val_loss: 2.0089e-04 - 14s/epoch - 165us/sample
Epoch 72/80
84077/84077 - 14s - loss: 1.8120e-04 - val_loss: 1.9373e-04 - 14s/epoch - 164us/sample
Epoch 73/80
84077/84077 - 14s - loss: 1.8046e-04 - val_loss: 1.9662e-04 - 14s/epoch - 164us/sample
Epoch 74/80
84077/84077 - 14s - loss: 1.7935e-04 - val_loss: 2.0098e-04 - 14s/epoch - 164us/sample
Epoch 75/80
84077/84077 - 14s - loss: 1.8084e-04 - val_loss: 1.9435e-04 - 14s/epoch - 165us/sample
Epoch 76/80
84077/84077 - 14s - loss: 1.7995e-04 - val_loss: 1.9764e-04 - 14s/epoch - 165us/sample
Epoch 77/80
84077/84077 - 14s - loss: 1.8003e-04 - val_loss: 1.9860e-04 - 14s/epoch - 165us/sample
Epoch 78/80
84077/84077 - 14s - loss: 1.7877e-04 - val_loss: 1.9355e-04 - 14s/epoch - 165us/sample
Epoch 79/80
84077/84077 - 14s - loss: 1.7906e-04 - val_loss: 1.9419e-04 - 14s/epoch - 164us/sample
Epoch 80/80
84077/84077 - 14s - loss: 1.7806e-04 - val_loss: 1.9756e-04 - 14s/epoch - 165us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00019756145055978129
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:56:23.784162: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_37/outputlayer/BiasAdd' id:47681 op device:{requested: '', assigned: ''} def:{{{node decoder_model_37/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_37/outputlayer/MatMul, decoder_model_37/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02252406278650321
cosine 0.022264175171687127
MAE: 0.0021455893341103985
RMSE: 0.010691037945759652
r2: 0.9108414769688318
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'mse', 32, 80, 0.0008, 0.3, 282, 0.00017805746906137657, 0.00019756145055978129, 0.02252406278650321, 0.022264175171687127, 0.0021455893341103985, 0.010691037945759652, 0.9108414769688318, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0012000000000000001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_114 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_114 (ReLU)               (None, 2074)         0           ['batch_normalization_114[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          585150      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          585150      ['re_lu_114[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2632897     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,769,349
Trainable params: 5,760,489
Non-trainable params: 8,860
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 06:56:49.601027: W tensorflow/c/c_api.cc:291] Operation '{name:'training_76/Adam/learning_rate/Assign' id:49411 op device:{requested: '', assigned: ''} def:{{{node training_76/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_76/Adam/learning_rate, training_76/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:57:05.314974: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_38/mul' id:48965 op device:{requested: '', assigned: ''} def:{{{node loss_38/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_38/mul/x, loss_38/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 26s - loss: 0.0076 - val_loss: 0.0019 - 26s/epoch - 307us/sample
Epoch 2/85
84077/84077 - 10s - loss: 0.0015 - val_loss: 0.0016 - 10s/epoch - 119us/sample
Epoch 3/85
84077/84077 - 10s - loss: 0.0018 - val_loss: 9.6746e-04 - 10s/epoch - 119us/sample
Epoch 4/85
84077/84077 - 10s - loss: 0.0010 - val_loss: 7.4472e-04 - 10s/epoch - 118us/sample
Epoch 5/85
84077/84077 - 10s - loss: 7.7676e-04 - val_loss: 6.1349e-04 - 10s/epoch - 118us/sample
Epoch 6/85
84077/84077 - 10s - loss: 0.0015 - val_loss: 9.9734e-04 - 10s/epoch - 118us/sample
Epoch 7/85
84077/84077 - 10s - loss: 0.0013 - val_loss: 7.2346e-04 - 10s/epoch - 118us/sample
Epoch 8/85
84077/84077 - 10s - loss: 7.9231e-04 - val_loss: 6.1221e-04 - 10s/epoch - 119us/sample
Epoch 9/85
84077/84077 - 10s - loss: 6.2855e-04 - val_loss: 5.2104e-04 - 10s/epoch - 119us/sample
Epoch 10/85
84077/84077 - 10s - loss: 5.4473e-04 - val_loss: 4.5128e-04 - 10s/epoch - 120us/sample
Epoch 11/85
84077/84077 - 10s - loss: 4.9700e-04 - val_loss: 4.0943e-04 - 10s/epoch - 119us/sample
Epoch 12/85
84077/84077 - 10s - loss: 4.6493e-04 - val_loss: 3.6928e-04 - 10s/epoch - 119us/sample
Epoch 13/85
84077/84077 - 10s - loss: 4.1541e-04 - val_loss: 3.5094e-04 - 10s/epoch - 118us/sample
Epoch 14/85
84077/84077 - 10s - loss: 3.9008e-04 - val_loss: 3.1828e-04 - 10s/epoch - 118us/sample
Epoch 15/85
84077/84077 - 10s - loss: 3.5854e-04 - val_loss: 2.9554e-04 - 10s/epoch - 119us/sample
Epoch 16/85
84077/84077 - 10s - loss: 3.3590e-04 - val_loss: 2.7733e-04 - 10s/epoch - 120us/sample
Epoch 17/85
84077/84077 - 10s - loss: 3.4203e-04 - val_loss: 2.7646e-04 - 10s/epoch - 120us/sample
Epoch 18/85
84077/84077 - 10s - loss: 3.1322e-04 - val_loss: 2.6833e-04 - 10s/epoch - 119us/sample
Epoch 19/85
84077/84077 - 10s - loss: 2.9085e-04 - val_loss: 2.4656e-04 - 10s/epoch - 118us/sample
Epoch 20/85
84077/84077 - 10s - loss: 2.7532e-04 - val_loss: 2.3415e-04 - 10s/epoch - 119us/sample
Epoch 21/85
84077/84077 - 10s - loss: 2.6769e-04 - val_loss: 2.2450e-04 - 10s/epoch - 119us/sample
Epoch 22/85
84077/84077 - 10s - loss: 2.6060e-04 - val_loss: 2.2294e-04 - 10s/epoch - 120us/sample
Epoch 23/85
84077/84077 - 10s - loss: 2.5004e-04 - val_loss: 2.1918e-04 - 10s/epoch - 120us/sample
Epoch 24/85
84077/84077 - 10s - loss: 2.4553e-04 - val_loss: 2.1141e-04 - 10s/epoch - 119us/sample
Epoch 25/85
84077/84077 - 10s - loss: 2.3917e-04 - val_loss: 2.0453e-04 - 10s/epoch - 118us/sample
Epoch 26/85
84077/84077 - 10s - loss: 2.3444e-04 - val_loss: 2.0356e-04 - 10s/epoch - 119us/sample
Epoch 27/85
84077/84077 - 10s - loss: 2.2908e-04 - val_loss: 1.9844e-04 - 10s/epoch - 119us/sample
Epoch 28/85
84077/84077 - 10s - loss: 2.2431e-04 - val_loss: 1.9381e-04 - 10s/epoch - 120us/sample
Epoch 29/85
84077/84077 - 10s - loss: 2.1864e-04 - val_loss: 1.9242e-04 - 10s/epoch - 120us/sample
Epoch 30/85
84077/84077 - 10s - loss: 2.2056e-04 - val_loss: 1.8955e-04 - 10s/epoch - 120us/sample
Epoch 31/85
84077/84077 - 10s - loss: 2.1361e-04 - val_loss: 1.8788e-04 - 10s/epoch - 119us/sample
Epoch 32/85
84077/84077 - 10s - loss: 2.1155e-04 - val_loss: 1.8622e-04 - 10s/epoch - 119us/sample
Epoch 33/85
84077/84077 - 10s - loss: 2.0840e-04 - val_loss: 1.8150e-04 - 10s/epoch - 119us/sample
Epoch 34/85
84077/84077 - 10s - loss: 2.0567e-04 - val_loss: 1.8208e-04 - 10s/epoch - 119us/sample
Epoch 35/85
84077/84077 - 10s - loss: 2.0273e-04 - val_loss: 1.7985e-04 - 10s/epoch - 120us/sample
Epoch 36/85
84077/84077 - 10s - loss: 2.0069e-04 - val_loss: 1.8087e-04 - 10s/epoch - 120us/sample
Epoch 37/85
84077/84077 - 10s - loss: 1.9871e-04 - val_loss: 1.7747e-04 - 10s/epoch - 119us/sample
Epoch 38/85
84077/84077 - 10s - loss: 1.9908e-04 - val_loss: 1.7531e-04 - 10s/epoch - 119us/sample
Epoch 39/85
84077/84077 - 10s - loss: 1.9591e-04 - val_loss: 1.7427e-04 - 10s/epoch - 119us/sample
Epoch 40/85
84077/84077 - 10s - loss: 1.9367e-04 - val_loss: 1.7310e-04 - 10s/epoch - 119us/sample
Epoch 41/85
84077/84077 - 10s - loss: 1.9109e-04 - val_loss: 1.7277e-04 - 10s/epoch - 120us/sample
Epoch 42/85
84077/84077 - 10s - loss: 1.9181e-04 - val_loss: 1.7207e-04 - 10s/epoch - 120us/sample
Epoch 43/85
84077/84077 - 10s - loss: 1.8899e-04 - val_loss: 1.6952e-04 - 10s/epoch - 119us/sample
Epoch 44/85
84077/84077 - 10s - loss: 1.8703e-04 - val_loss: 1.6919e-04 - 10s/epoch - 119us/sample
Epoch 45/85
84077/84077 - 10s - loss: 1.8666e-04 - val_loss: 1.7055e-04 - 10s/epoch - 119us/sample
Epoch 46/85
84077/84077 - 10s - loss: 1.8664e-04 - val_loss: 1.6698e-04 - 10s/epoch - 119us/sample
Epoch 47/85
84077/84077 - 10s - loss: 1.8255e-04 - val_loss: 1.6656e-04 - 10s/epoch - 120us/sample
Epoch 48/85
84077/84077 - 10s - loss: 1.8447e-04 - val_loss: 1.7070e-04 - 10s/epoch - 120us/sample
Epoch 49/85
84077/84077 - 10s - loss: 1.8246e-04 - val_loss: 1.6654e-04 - 10s/epoch - 119us/sample
Epoch 50/85
84077/84077 - 10s - loss: 1.8094e-04 - val_loss: 1.6492e-04 - 10s/epoch - 119us/sample
Epoch 51/85
84077/84077 - 10s - loss: 1.7879e-04 - val_loss: 1.6329e-04 - 10s/epoch - 119us/sample
Epoch 52/85
84077/84077 - 10s - loss: 1.7746e-04 - val_loss: 1.6290e-04 - 10s/epoch - 119us/sample
Epoch 53/85
84077/84077 - 10s - loss: 1.7732e-04 - val_loss: 1.6367e-04 - 10s/epoch - 120us/sample
Epoch 54/85
84077/84077 - 10s - loss: 1.7612e-04 - val_loss: 1.6170e-04 - 10s/epoch - 120us/sample
Epoch 55/85
84077/84077 - 10s - loss: 1.7694e-04 - val_loss: 1.6131e-04 - 10s/epoch - 120us/sample
Epoch 56/85
84077/84077 - 10s - loss: 1.7485e-04 - val_loss: 1.5982e-04 - 10s/epoch - 119us/sample
Epoch 57/85
84077/84077 - 10s - loss: 1.7399e-04 - val_loss: 1.5986e-04 - 10s/epoch - 119us/sample
Epoch 58/85
84077/84077 - 10s - loss: 1.7253e-04 - val_loss: 1.6048e-04 - 10s/epoch - 119us/sample
Epoch 59/85
84077/84077 - 10s - loss: 1.7295e-04 - val_loss: 1.5653e-04 - 10s/epoch - 119us/sample
Epoch 60/85
84077/84077 - 10s - loss: 1.7146e-04 - val_loss: 1.5826e-04 - 10s/epoch - 120us/sample
Epoch 61/85
84077/84077 - 10s - loss: 1.7004e-04 - val_loss: 1.5724e-04 - 10s/epoch - 120us/sample
Epoch 62/85
84077/84077 - 10s - loss: 1.7034e-04 - val_loss: 1.5676e-04 - 10s/epoch - 119us/sample
Epoch 63/85
84077/84077 - 10s - loss: 1.6956e-04 - val_loss: 1.5533e-04 - 10s/epoch - 119us/sample
Epoch 64/85
84077/84077 - 10s - loss: 1.6845e-04 - val_loss: 1.5451e-04 - 10s/epoch - 119us/sample
Epoch 65/85
84077/84077 - 10s - loss: 1.7483e-04 - val_loss: 1.5630e-04 - 10s/epoch - 119us/sample
Epoch 66/85
84077/84077 - 10s - loss: 1.6744e-04 - val_loss: 1.5547e-04 - 10s/epoch - 120us/sample
Epoch 67/85
84077/84077 - 10s - loss: 1.6654e-04 - val_loss: 1.5709e-04 - 10s/epoch - 120us/sample
Epoch 68/85
84077/84077 - 10s - loss: 1.6661e-04 - val_loss: 1.5479e-04 - 10s/epoch - 119us/sample
Epoch 69/85
84077/84077 - 10s - loss: 1.6604e-04 - val_loss: 1.5258e-04 - 10s/epoch - 119us/sample
Epoch 70/85
84077/84077 - 10s - loss: 1.6505e-04 - val_loss: 1.5216e-04 - 10s/epoch - 119us/sample
Epoch 71/85
84077/84077 - 10s - loss: 1.6458e-04 - val_loss: 1.5298e-04 - 10s/epoch - 119us/sample
Epoch 72/85
84077/84077 - 10s - loss: 1.6384e-04 - val_loss: 1.5146e-04 - 10s/epoch - 120us/sample
Epoch 73/85
84077/84077 - 10s - loss: 1.6337e-04 - val_loss: 1.5257e-04 - 10s/epoch - 120us/sample
Epoch 74/85
84077/84077 - 10s - loss: 1.6397e-04 - val_loss: 1.5226e-04 - 10s/epoch - 120us/sample
Epoch 75/85
84077/84077 - 10s - loss: 1.6319e-04 - val_loss: 1.5229e-04 - 10s/epoch - 119us/sample
Epoch 76/85
84077/84077 - 10s - loss: 1.6214e-04 - val_loss: 1.5166e-04 - 10s/epoch - 119us/sample
Epoch 77/85
84077/84077 - 10s - loss: 1.6101e-04 - val_loss: 1.5095e-04 - 10s/epoch - 119us/sample
Epoch 78/85
84077/84077 - 10s - loss: 1.6091e-04 - val_loss: 1.5124e-04 - 10s/epoch - 119us/sample
Epoch 79/85
84077/84077 - 10s - loss: 1.6027e-04 - val_loss: 1.5267e-04 - 10s/epoch - 120us/sample
Epoch 80/85
84077/84077 - 10s - loss: 1.6132e-04 - val_loss: 1.4956e-04 - 10s/epoch - 121us/sample
Epoch 81/85
84077/84077 - 10s - loss: 1.5924e-04 - val_loss: 1.4851e-04 - 10s/epoch - 120us/sample
Epoch 82/85
84077/84077 - 10s - loss: 1.5836e-04 - val_loss: 1.4912e-04 - 10s/epoch - 119us/sample
Epoch 83/85
84077/84077 - 10s - loss: 1.5834e-04 - val_loss: 1.4991e-04 - 10s/epoch - 119us/sample
Epoch 84/85
84077/84077 - 10s - loss: 1.5880e-04 - val_loss: 1.4770e-04 - 10s/epoch - 119us/sample
Epoch 85/85
84077/84077 - 10s - loss: 1.5724e-04 - val_loss: 1.4691e-04 - 10s/epoch - 119us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00014690961811502707
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:11:13.175885: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_38/outputlayer/BiasAdd' id:48936 op device:{requested: '', assigned: ''} def:{{{node decoder_model_38/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_38/outputlayer/MatMul, decoder_model_38/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01844441063664095
cosine 0.018222452533665726
MAE: 0.0018451047279782371
RMSE: 0.008133771024549968
r2: 0.9484743751786974
RMSE zero-vector: 0.04004287452915337
['2.2custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.3, 282, 0.00015724137355534981, 0.00014690961811502707, 0.01844441063664095, 0.018222452533665726, 0.0018451047279782371, 0.008133771024549968, 0.9484743751786974, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 32 1] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_117 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_117 (ReLU)               (None, 1980)         0           ['batch_normalization_117[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_117[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 07:11:39.219670: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_39/kernel/Assign' id:50011 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_39/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_39/kernel, dense_dec0_39/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:11:59.261033: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_39/mul' id:50220 op device:{requested: '', assigned: ''} def:{{{node loss_39/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_39/mul/x, loss_39/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 31s - loss: 0.0079 - val_loss: 0.0073 - 31s/epoch - 364us/sample
Epoch 2/90
84077/84077 - 14s - loss: 0.0016 - val_loss: 0.0010 - 14s/epoch - 171us/sample
Epoch 3/90
84077/84077 - 14s - loss: 0.0012 - val_loss: 8.2548e-04 - 14s/epoch - 172us/sample
Epoch 4/90
84077/84077 - 14s - loss: 0.0011 - val_loss: 0.0018 - 14s/epoch - 170us/sample
Epoch 5/90
84077/84077 - 14s - loss: 7.3301e-04 - val_loss: 5.6157e-04 - 14s/epoch - 170us/sample
Epoch 6/90
84077/84077 - 14s - loss: 5.8134e-04 - val_loss: 4.9666e-04 - 14s/epoch - 169us/sample
Epoch 7/90
84077/84077 - 14s - loss: 4.9451e-04 - val_loss: 4.4354e-04 - 14s/epoch - 170us/sample
Epoch 8/90
84077/84077 - 14s - loss: 4.3094e-04 - val_loss: 4.0183e-04 - 14s/epoch - 170us/sample
Epoch 9/90
84077/84077 - 14s - loss: 3.8783e-04 - val_loss: 3.7172e-04 - 14s/epoch - 171us/sample
Epoch 10/90
84077/84077 - 14s - loss: 3.5432e-04 - val_loss: 3.6160e-04 - 14s/epoch - 170us/sample
Epoch 11/90
84077/84077 - 14s - loss: 3.3186e-04 - val_loss: 3.5372e-04 - 14s/epoch - 169us/sample
Epoch 12/90
84077/84077 - 14s - loss: 3.1246e-04 - val_loss: 3.1854e-04 - 14s/epoch - 170us/sample
Epoch 13/90
84077/84077 - 14s - loss: 2.9831e-04 - val_loss: 3.1880e-04 - 14s/epoch - 172us/sample
Epoch 14/90
84077/84077 - 14s - loss: 2.8578e-04 - val_loss: 2.9830e-04 - 14s/epoch - 170us/sample
Epoch 15/90
84077/84077 - 14s - loss: 2.7725e-04 - val_loss: 3.0368e-04 - 14s/epoch - 170us/sample
Epoch 16/90
84077/84077 - 14s - loss: 2.6979e-04 - val_loss: 2.9056e-04 - 14s/epoch - 170us/sample
Epoch 17/90
84077/84077 - 14s - loss: 2.6190e-04 - val_loss: 2.8263e-04 - 14s/epoch - 171us/sample
Epoch 18/90
84077/84077 - 14s - loss: 2.5509e-04 - val_loss: 2.7783e-04 - 14s/epoch - 171us/sample
Epoch 19/90
84077/84077 - 14s - loss: 2.5033e-04 - val_loss: 2.7731e-04 - 14s/epoch - 170us/sample
Epoch 20/90
84077/84077 - 14s - loss: 2.4601e-04 - val_loss: 2.6969e-04 - 14s/epoch - 170us/sample
Epoch 21/90
84077/84077 - 14s - loss: 2.4074e-04 - val_loss: 2.6150e-04 - 14s/epoch - 170us/sample
Epoch 22/90
84077/84077 - 14s - loss: 2.3789e-04 - val_loss: 2.5834e-04 - 14s/epoch - 172us/sample
Epoch 23/90
84077/84077 - 14s - loss: 2.3590e-04 - val_loss: 2.6363e-04 - 14s/epoch - 170us/sample
Epoch 24/90
84077/84077 - 14s - loss: 2.3233e-04 - val_loss: 2.5359e-04 - 14s/epoch - 170us/sample
Epoch 25/90
84077/84077 - 14s - loss: 2.2945e-04 - val_loss: 2.4672e-04 - 14s/epoch - 170us/sample
Epoch 26/90
84077/84077 - 14s - loss: 2.2758e-04 - val_loss: 2.6312e-04 - 14s/epoch - 171us/sample
Epoch 27/90
84077/84077 - 14s - loss: 2.2556e-04 - val_loss: 2.4874e-04 - 14s/epoch - 171us/sample
Epoch 28/90
84077/84077 - 14s - loss: 2.2316e-04 - val_loss: 2.5028e-04 - 14s/epoch - 170us/sample
Epoch 29/90
84077/84077 - 14s - loss: 2.2117e-04 - val_loss: 2.4365e-04 - 14s/epoch - 170us/sample
Epoch 30/90
84077/84077 - 14s - loss: 2.2086e-04 - val_loss: 2.4321e-04 - 14s/epoch - 171us/sample
Epoch 31/90
84077/84077 - 14s - loss: 2.1751e-04 - val_loss: 2.4205e-04 - 14s/epoch - 171us/sample
Epoch 32/90
84077/84077 - 14s - loss: 2.1645e-04 - val_loss: 2.4300e-04 - 14s/epoch - 171us/sample
Epoch 33/90
84077/84077 - 14s - loss: 2.1573e-04 - val_loss: 2.4380e-04 - 14s/epoch - 170us/sample
Epoch 34/90
84077/84077 - 14s - loss: 2.1446e-04 - val_loss: 2.5216e-04 - 14s/epoch - 171us/sample
Epoch 35/90
84077/84077 - 14s - loss: 2.1312e-04 - val_loss: 2.3514e-04 - 14s/epoch - 171us/sample
Epoch 36/90
84077/84077 - 14s - loss: 2.1235e-04 - val_loss: 2.4189e-04 - 14s/epoch - 170us/sample
Epoch 37/90
84077/84077 - 14s - loss: 2.1098e-04 - val_loss: 2.2908e-04 - 14s/epoch - 171us/sample
Epoch 38/90
84077/84077 - 14s - loss: 2.1018e-04 - val_loss: 2.3572e-04 - 14s/epoch - 170us/sample
Epoch 39/90
84077/84077 - 14s - loss: 2.0961e-04 - val_loss: 2.2959e-04 - 14s/epoch - 171us/sample
Epoch 40/90
84077/84077 - 14s - loss: 2.0791e-04 - val_loss: 2.2757e-04 - 14s/epoch - 171us/sample
Epoch 41/90
84077/84077 - 14s - loss: 2.0741e-04 - val_loss: 2.3004e-04 - 14s/epoch - 171us/sample
Epoch 42/90
84077/84077 - 14s - loss: 2.0623e-04 - val_loss: 2.2748e-04 - 14s/epoch - 170us/sample
Epoch 43/90
84077/84077 - 14s - loss: 2.0454e-04 - val_loss: 2.3067e-04 - 14s/epoch - 170us/sample
Epoch 44/90
84077/84077 - 14s - loss: 2.0496e-04 - val_loss: 2.3387e-04 - 14s/epoch - 171us/sample
Epoch 45/90
84077/84077 - 14s - loss: 2.0437e-04 - val_loss: 2.3456e-04 - 14s/epoch - 171us/sample
Epoch 46/90
84077/84077 - 14s - loss: 2.0351e-04 - val_loss: 2.2640e-04 - 14s/epoch - 171us/sample
Epoch 47/90
84077/84077 - 14s - loss: 2.0204e-04 - val_loss: 2.2613e-04 - 14s/epoch - 170us/sample
Epoch 48/90
84077/84077 - 14s - loss: 2.0127e-04 - val_loss: 2.2628e-04 - 14s/epoch - 171us/sample
Epoch 49/90
84077/84077 - 14s - loss: 2.0085e-04 - val_loss: 2.2732e-04 - 14s/epoch - 171us/sample
Epoch 50/90
84077/84077 - 14s - loss: 2.0060e-04 - val_loss: 2.2089e-04 - 14s/epoch - 171us/sample
Epoch 51/90
84077/84077 - 14s - loss: 1.9999e-04 - val_loss: 2.2872e-04 - 14s/epoch - 171us/sample
Epoch 52/90
84077/84077 - 14s - loss: 1.9867e-04 - val_loss: 2.1896e-04 - 14s/epoch - 171us/sample
Epoch 53/90
84077/84077 - 14s - loss: 1.9880e-04 - val_loss: 2.1799e-04 - 14s/epoch - 171us/sample
Epoch 54/90
84077/84077 - 14s - loss: 1.9805e-04 - val_loss: 2.2151e-04 - 14s/epoch - 171us/sample
Epoch 55/90
84077/84077 - 14s - loss: 1.9727e-04 - val_loss: 2.2569e-04 - 14s/epoch - 171us/sample
Epoch 56/90
84077/84077 - 14s - loss: 1.9621e-04 - val_loss: 2.1959e-04 - 14s/epoch - 170us/sample
Epoch 57/90
84077/84077 - 14s - loss: 1.9631e-04 - val_loss: 2.2282e-04 - 14s/epoch - 170us/sample
Epoch 58/90
84077/84077 - 14s - loss: 1.9579e-04 - val_loss: 2.1684e-04 - 14s/epoch - 172us/sample
Epoch 59/90
84077/84077 - 14s - loss: 1.9488e-04 - val_loss: 2.1910e-04 - 14s/epoch - 171us/sample
Epoch 60/90
84077/84077 - 14s - loss: 1.9452e-04 - val_loss: 2.1013e-04 - 14s/epoch - 170us/sample
Epoch 61/90
84077/84077 - 14s - loss: 1.9408e-04 - val_loss: 2.2108e-04 - 14s/epoch - 170us/sample
Epoch 62/90
84077/84077 - 14s - loss: 1.9365e-04 - val_loss: 2.2169e-04 - 14s/epoch - 171us/sample
Epoch 63/90
84077/84077 - 14s - loss: 1.9252e-04 - val_loss: 2.0975e-04 - 14s/epoch - 172us/sample
Epoch 64/90
84077/84077 - 14s - loss: 1.9313e-04 - val_loss: 2.1933e-04 - 14s/epoch - 171us/sample
Epoch 65/90
84077/84077 - 14s - loss: 1.9188e-04 - val_loss: 2.1042e-04 - 14s/epoch - 171us/sample
Epoch 66/90
84077/84077 - 14s - loss: 1.9107e-04 - val_loss: 2.0898e-04 - 14s/epoch - 170us/sample
Epoch 67/90
84077/84077 - 14s - loss: 1.9164e-04 - val_loss: 2.1819e-04 - 14s/epoch - 171us/sample
Epoch 68/90
84077/84077 - 14s - loss: 1.9027e-04 - val_loss: 2.0715e-04 - 14s/epoch - 171us/sample
Epoch 69/90
84077/84077 - 14s - loss: 1.9049e-04 - val_loss: 2.1266e-04 - 14s/epoch - 171us/sample
Epoch 70/90
84077/84077 - 14s - loss: 1.8980e-04 - val_loss: 2.1545e-04 - 14s/epoch - 171us/sample
Epoch 71/90
84077/84077 - 14s - loss: 1.8964e-04 - val_loss: 2.1487e-04 - 14s/epoch - 171us/sample
Epoch 72/90
84077/84077 - 14s - loss: 1.8874e-04 - val_loss: 2.1533e-04 - 14s/epoch - 171us/sample
Epoch 73/90
84077/84077 - 14s - loss: 1.8829e-04 - val_loss: 2.0809e-04 - 14s/epoch - 171us/sample
Epoch 74/90
84077/84077 - 14s - loss: 1.8858e-04 - val_loss: 2.0977e-04 - 14s/epoch - 172us/sample
Epoch 75/90
84077/84077 - 14s - loss: 1.8934e-04 - val_loss: 2.0669e-04 - 14s/epoch - 171us/sample
Epoch 76/90
84077/84077 - 14s - loss: 1.8785e-04 - val_loss: 2.1143e-04 - 14s/epoch - 171us/sample
Epoch 77/90
84077/84077 - 14s - loss: 1.8712e-04 - val_loss: 2.1530e-04 - 14s/epoch - 171us/sample
Epoch 78/90
84077/84077 - 14s - loss: 1.8853e-04 - val_loss: 2.0937e-04 - 14s/epoch - 172us/sample
Epoch 79/90
84077/84077 - 14s - loss: 1.8711e-04 - val_loss: 2.0575e-04 - 14s/epoch - 171us/sample
Epoch 80/90
84077/84077 - 14s - loss: 1.8629e-04 - val_loss: 2.1071e-04 - 14s/epoch - 171us/sample
Epoch 81/90
84077/84077 - 14s - loss: 1.8625e-04 - val_loss: 2.0705e-04 - 14s/epoch - 171us/sample
Epoch 82/90
84077/84077 - 14s - loss: 1.8636e-04 - val_loss: 2.1025e-04 - 14s/epoch - 172us/sample
Epoch 83/90
84077/84077 - 14s - loss: 1.8580e-04 - val_loss: 2.1027e-04 - 14s/epoch - 171us/sample
Epoch 84/90
84077/84077 - 14s - loss: 1.8537e-04 - val_loss: 2.0482e-04 - 14s/epoch - 171us/sample
Epoch 85/90
84077/84077 - 14s - loss: 1.8525e-04 - val_loss: 2.0806e-04 - 14s/epoch - 171us/sample
Epoch 86/90
84077/84077 - 14s - loss: 1.8489e-04 - val_loss: 2.0681e-04 - 14s/epoch - 172us/sample
Epoch 87/90
84077/84077 - 14s - loss: 1.8466e-04 - val_loss: 2.1000e-04 - 14s/epoch - 172us/sample
Epoch 88/90
84077/84077 - 14s - loss: 1.8366e-04 - val_loss: 2.0198e-04 - 14s/epoch - 172us/sample
Epoch 89/90
84077/84077 - 14s - loss: 1.8407e-04 - val_loss: 2.0527e-04 - 14s/epoch - 171us/sample
Epoch 90/90
84077/84077 - 14s - loss: 1.8427e-04 - val_loss: 2.0995e-04 - 14s/epoch - 171us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00020994944677700045
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:33:22.826754: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_39/outputlayer/BiasAdd' id:50191 op device:{requested: '', assigned: ''} def:{{{node decoder_model_39/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_39/outputlayer/MatMul, decoder_model_39/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023520376826465902
cosine 0.023252382435806556
MAE: 0.002182700175483396
RMSE: 0.011188258395804599
r2: 0.9023261349084506
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 90, 0.001, 0.3, 282, 0.00018427479999895391, 0.00020994944677700045, 0.023520376826465902, 0.023252382435806556, 0.002182700175483396, 0.011188258395804599, 0.9023261349084506, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0008 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_120 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_120 (ReLU)               (None, 1980)         0           ['batch_normalization_120[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_120[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 07:33:49.065371: W tensorflow/c/c_api.cc:291] Operation '{name:'training_80/Adam/outputlayer_40/kernel/v/Assign' id:52144 op device:{requested: '', assigned: ''} def:{{{node training_80/Adam/outputlayer_40/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_80/Adam/outputlayer_40/kernel/v, training_80/Adam/outputlayer_40/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:34:05.260825: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_40/mul' id:51475 op device:{requested: '', assigned: ''} def:{{{node loss_40/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_40/mul/x, loss_40/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0075 - val_loss: 0.0017 - 27s/epoch - 318us/sample
Epoch 2/90
84077/84077 - 10s - loss: 0.0016 - val_loss: 0.0012 - 10s/epoch - 124us/sample
Epoch 3/90
84077/84077 - 10s - loss: 0.0015 - val_loss: 0.0902 - 10s/epoch - 124us/sample
Epoch 4/90
84077/84077 - 10s - loss: 0.0022 - val_loss: 9.7325e-04 - 10s/epoch - 124us/sample
Epoch 5/90
84077/84077 - 10s - loss: 9.2142e-04 - val_loss: 7.7351e-04 - 10s/epoch - 121us/sample
Epoch 6/90
84077/84077 - 10s - loss: 8.7271e-04 - val_loss: 9.2876e-04 - 10s/epoch - 121us/sample
Epoch 7/90
84077/84077 - 10s - loss: 7.4358e-04 - val_loss: 6.2165e-04 - 10s/epoch - 121us/sample
Epoch 8/90
84077/84077 - 10s - loss: 6.6572e-04 - val_loss: 5.1662e-04 - 10s/epoch - 121us/sample
Epoch 9/90
84077/84077 - 10s - loss: 5.3376e-04 - val_loss: 4.3777e-04 - 10s/epoch - 121us/sample
Epoch 10/90
84077/84077 - 10s - loss: 4.6862e-04 - val_loss: 3.6834e-04 - 10s/epoch - 122us/sample
Epoch 11/90
84077/84077 - 10s - loss: 4.1356e-04 - val_loss: 3.2830e-04 - 10s/epoch - 122us/sample
Epoch 12/90
84077/84077 - 10s - loss: 3.7995e-04 - val_loss: 3.2311e-04 - 10s/epoch - 121us/sample
Epoch 13/90
84077/84077 - 10s - loss: 3.4707e-04 - val_loss: 2.9213e-04 - 10s/epoch - 121us/sample
Epoch 14/90
84077/84077 - 10s - loss: 3.2169e-04 - val_loss: 2.6017e-04 - 10s/epoch - 121us/sample
Epoch 15/90
84077/84077 - 10s - loss: 3.0011e-04 - val_loss: 2.4627e-04 - 10s/epoch - 121us/sample
Epoch 16/90
84077/84077 - 10s - loss: 2.8650e-04 - val_loss: 2.3748e-04 - 10s/epoch - 122us/sample
Epoch 17/90
84077/84077 - 10s - loss: 2.7236e-04 - val_loss: 2.3534e-04 - 10s/epoch - 123us/sample
Epoch 18/90
84077/84077 - 10s - loss: 2.6192e-04 - val_loss: 2.1821e-04 - 10s/epoch - 121us/sample
Epoch 19/90
84077/84077 - 10s - loss: 2.5307e-04 - val_loss: 2.1007e-04 - 10s/epoch - 121us/sample
Epoch 20/90
84077/84077 - 10s - loss: 2.4477e-04 - val_loss: 2.0437e-04 - 10s/epoch - 121us/sample
Epoch 21/90
84077/84077 - 10s - loss: 2.3481e-04 - val_loss: 2.0117e-04 - 10s/epoch - 121us/sample
Epoch 22/90
84077/84077 - 10s - loss: 2.3100e-04 - val_loss: 1.9887e-04 - 10s/epoch - 121us/sample
Epoch 23/90
84077/84077 - 10s - loss: 2.2715e-04 - val_loss: 1.9725e-04 - 10s/epoch - 122us/sample
Epoch 24/90
84077/84077 - 10s - loss: 2.2253e-04 - val_loss: 1.9464e-04 - 10s/epoch - 123us/sample
Epoch 25/90
84077/84077 - 10s - loss: 2.1916e-04 - val_loss: 1.9105e-04 - 10s/epoch - 121us/sample
Epoch 26/90
84077/84077 - 10s - loss: 2.1435e-04 - val_loss: 1.8696e-04 - 10s/epoch - 121us/sample
Epoch 27/90
84077/84077 - 10s - loss: 2.1395e-04 - val_loss: 1.8409e-04 - 10s/epoch - 121us/sample
Epoch 28/90
84077/84077 - 10s - loss: 2.0796e-04 - val_loss: 1.8056e-04 - 10s/epoch - 121us/sample
Epoch 29/90
84077/84077 - 10s - loss: 2.0530e-04 - val_loss: 1.8003e-04 - 10s/epoch - 122us/sample
Epoch 30/90
84077/84077 - 10s - loss: 2.0404e-04 - val_loss: 1.7609e-04 - 10s/epoch - 122us/sample
Epoch 31/90
84077/84077 - 10s - loss: 2.0047e-04 - val_loss: 1.7612e-04 - 10s/epoch - 123us/sample
Epoch 32/90
84077/84077 - 10s - loss: 2.0018e-04 - val_loss: 1.7360e-04 - 10s/epoch - 122us/sample
Epoch 33/90
84077/84077 - 10s - loss: 1.9686e-04 - val_loss: 1.7247e-04 - 10s/epoch - 121us/sample
Epoch 34/90
84077/84077 - 10s - loss: 1.9469e-04 - val_loss: 1.6930e-04 - 10s/epoch - 121us/sample
Epoch 35/90
84077/84077 - 10s - loss: 1.9200e-04 - val_loss: 1.6728e-04 - 10s/epoch - 121us/sample
Epoch 36/90
84077/84077 - 10s - loss: 1.9113e-04 - val_loss: 1.6834e-04 - 10s/epoch - 121us/sample
Epoch 37/90
84077/84077 - 10s - loss: 1.9022e-04 - val_loss: 1.6584e-04 - 10s/epoch - 123us/sample
Epoch 38/90
84077/84077 - 10s - loss: 1.8841e-04 - val_loss: 1.6528e-04 - 10s/epoch - 122us/sample
Epoch 39/90
84077/84077 - 10s - loss: 1.8738e-04 - val_loss: 1.6372e-04 - 10s/epoch - 122us/sample
Epoch 40/90
84077/84077 - 10s - loss: 1.8477e-04 - val_loss: 1.6319e-04 - 10s/epoch - 121us/sample
Epoch 41/90
84077/84077 - 10s - loss: 1.8440e-04 - val_loss: 1.6297e-04 - 10s/epoch - 121us/sample
Epoch 42/90
84077/84077 - 10s - loss: 1.8296e-04 - val_loss: 1.6058e-04 - 10s/epoch - 121us/sample
Epoch 43/90
84077/84077 - 10s - loss: 1.8214e-04 - val_loss: 1.5989e-04 - 10s/epoch - 122us/sample
Epoch 44/90
84077/84077 - 10s - loss: 1.8045e-04 - val_loss: 1.6115e-04 - 10s/epoch - 123us/sample
Epoch 45/90
84077/84077 - 10s - loss: 1.7959e-04 - val_loss: 1.5791e-04 - 10s/epoch - 122us/sample
Epoch 46/90
84077/84077 - 10s - loss: 1.7851e-04 - val_loss: 1.6065e-04 - 10s/epoch - 122us/sample
Epoch 47/90
84077/84077 - 10s - loss: 1.7723e-04 - val_loss: 1.5764e-04 - 10s/epoch - 121us/sample
Epoch 48/90
84077/84077 - 10s - loss: 1.7699e-04 - val_loss: 1.5617e-04 - 10s/epoch - 121us/sample
Epoch 49/90
84077/84077 - 10s - loss: 1.7584e-04 - val_loss: 1.5753e-04 - 10s/epoch - 121us/sample
Epoch 50/90
84077/84077 - 10s - loss: 1.7544e-04 - val_loss: 1.5539e-04 - 10s/epoch - 122us/sample
Epoch 51/90
84077/84077 - 10s - loss: 1.7410e-04 - val_loss: 1.5467e-04 - 10s/epoch - 123us/sample
Epoch 52/90
84077/84077 - 10s - loss: 1.7376e-04 - val_loss: 1.5650e-04 - 10s/epoch - 122us/sample
Epoch 53/90
84077/84077 - 10s - loss: 1.7246e-04 - val_loss: 1.5326e-04 - 10s/epoch - 122us/sample
Epoch 54/90
84077/84077 - 10s - loss: 1.7269e-04 - val_loss: 1.5262e-04 - 10s/epoch - 121us/sample
Epoch 55/90
84077/84077 - 10s - loss: 1.7016e-04 - val_loss: 1.5197e-04 - 10s/epoch - 121us/sample
Epoch 56/90
84077/84077 - 10s - loss: 1.7050e-04 - val_loss: 1.5190e-04 - 10s/epoch - 122us/sample
Epoch 57/90
84077/84077 - 10s - loss: 1.7058e-04 - val_loss: 1.5229e-04 - 10s/epoch - 122us/sample
Epoch 58/90
84077/84077 - 10s - loss: 1.6867e-04 - val_loss: 1.5122e-04 - 10s/epoch - 123us/sample
Epoch 59/90
84077/84077 - 10s - loss: 1.6852e-04 - val_loss: 1.5033e-04 - 10s/epoch - 121us/sample
Epoch 60/90
84077/84077 - 10s - loss: 1.6736e-04 - val_loss: 1.5091e-04 - 10s/epoch - 122us/sample
Epoch 61/90
84077/84077 - 10s - loss: 1.6689e-04 - val_loss: 1.4986e-04 - 10s/epoch - 121us/sample
Epoch 62/90
84077/84077 - 10s - loss: 1.6688e-04 - val_loss: 1.4970e-04 - 10s/epoch - 121us/sample
Epoch 63/90
84077/84077 - 10s - loss: 1.6617e-04 - val_loss: 1.4945e-04 - 10s/epoch - 122us/sample
Epoch 64/90
84077/84077 - 10s - loss: 1.6587e-04 - val_loss: 1.5215e-04 - 10s/epoch - 122us/sample
Epoch 65/90
84077/84077 - 10s - loss: 1.6437e-04 - val_loss: 1.4814e-04 - 10s/epoch - 123us/sample
Epoch 66/90
84077/84077 - 10s - loss: 1.6451e-04 - val_loss: 1.4878e-04 - 10s/epoch - 122us/sample
Epoch 67/90
84077/84077 - 10s - loss: 1.6421e-04 - val_loss: 1.4738e-04 - 10s/epoch - 121us/sample
Epoch 68/90
84077/84077 - 10s - loss: 1.6349e-04 - val_loss: 1.4891e-04 - 10s/epoch - 121us/sample
Epoch 69/90
84077/84077 - 10s - loss: 1.6390e-04 - val_loss: 1.4641e-04 - 10s/epoch - 121us/sample
Epoch 70/90
84077/84077 - 10s - loss: 1.6276e-04 - val_loss: 1.4720e-04 - 10s/epoch - 122us/sample
Epoch 71/90
84077/84077 - 10s - loss: 1.6310e-04 - val_loss: 1.4973e-04 - 10s/epoch - 122us/sample
Epoch 72/90
84077/84077 - 10s - loss: 1.6213e-04 - val_loss: 1.4567e-04 - 10s/epoch - 123us/sample
Epoch 73/90
84077/84077 - 10s - loss: 1.6140e-04 - val_loss: 1.4497e-04 - 10s/epoch - 122us/sample
Epoch 74/90
84077/84077 - 10s - loss: 1.6159e-04 - val_loss: 1.4615e-04 - 10s/epoch - 122us/sample
Epoch 75/90
84077/84077 - 10s - loss: 1.6034e-04 - val_loss: 1.4433e-04 - 10s/epoch - 121us/sample
Epoch 76/90
84077/84077 - 10s - loss: 1.5983e-04 - val_loss: 1.4474e-04 - 10s/epoch - 121us/sample
Epoch 77/90
84077/84077 - 10s - loss: 1.5969e-04 - val_loss: 1.4423e-04 - 10s/epoch - 122us/sample
Epoch 78/90
84077/84077 - 10s - loss: 1.5963e-04 - val_loss: 1.4242e-04 - 10s/epoch - 122us/sample
Epoch 79/90
84077/84077 - 10s - loss: 1.6063e-04 - val_loss: 1.4452e-04 - 10s/epoch - 123us/sample
Epoch 80/90
84077/84077 - 10s - loss: 1.5875e-04 - val_loss: 1.4267e-04 - 10s/epoch - 122us/sample
Epoch 81/90
84077/84077 - 10s - loss: 1.5940e-04 - val_loss: 1.4561e-04 - 10s/epoch - 121us/sample
Epoch 82/90
84077/84077 - 10s - loss: 1.5905e-04 - val_loss: 1.4561e-04 - 10s/epoch - 121us/sample
Epoch 83/90
84077/84077 - 10s - loss: 1.5740e-04 - val_loss: 1.4171e-04 - 10s/epoch - 121us/sample
Epoch 84/90
84077/84077 - 10s - loss: 1.5876e-04 - val_loss: 1.4154e-04 - 10s/epoch - 122us/sample
Epoch 85/90
84077/84077 - 10s - loss: 1.5752e-04 - val_loss: 1.4263e-04 - 10s/epoch - 122us/sample
Epoch 86/90
84077/84077 - 10s - loss: 1.5701e-04 - val_loss: 1.4015e-04 - 10s/epoch - 123us/sample
Epoch 87/90
84077/84077 - 10s - loss: 1.5645e-04 - val_loss: 1.4138e-04 - 10s/epoch - 122us/sample
Epoch 88/90
84077/84077 - 10s - loss: 1.5550e-04 - val_loss: 1.4151e-04 - 10s/epoch - 121us/sample
Epoch 89/90
84077/84077 - 10s - loss: 1.5591e-04 - val_loss: 1.4059e-04 - 10s/epoch - 121us/sample
Epoch 90/90
84077/84077 - 10s - loss: 1.5617e-04 - val_loss: 1.4201e-04 - 10s/epoch - 121us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00014200809867384167
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:49:21.593824: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_40/outputlayer/BiasAdd' id:51446 op device:{requested: '', assigned: ''} def:{{{node decoder_model_40/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_40/outputlayer/MatMul, decoder_model_40/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01741991866713541
cosine 0.01720866371526521
MAE: 0.0022884150893566738
RMSE: 0.0076681136413507735
r2: 0.9544237096798117
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 64, 90, 0.0008, 0.3, 282, 0.0001561681730632092, 0.00014200809867384167, 0.01741991866713541, 0.01720866371526521, 0.0022884150893566738, 0.0076681136413507735, 0.9544237096798117, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0012000000000000001 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1886)         1780384     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_123 (Batch  (None, 1886)        7544        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_123 (ReLU)               (None, 1886)         0           ['batch_normalization_123[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          532134      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          532134      ['re_lu_123[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2401657     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,253,853
Trainable params: 5,245,745
Non-trainable params: 8,108
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 07:49:48.534466: W tensorflow/c/c_api.cc:291] Operation '{name:'training_82/Adam/batch_normalization_124/gamma/m/Assign' id:53275 op device:{requested: '', assigned: ''} def:{{{node training_82/Adam/batch_normalization_124/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_82/Adam/batch_normalization_124/gamma/m, training_82/Adam/batch_normalization_124/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:50:05.239349: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_41/mul' id:52737 op device:{requested: '', assigned: ''} def:{{{node loss_41/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_41/mul/x, loss_41/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 27s - loss: 0.0036 - val_loss: 9.8685e-04 - 27s/epoch - 327us/sample
Epoch 2/85
84077/84077 - 11s - loss: 0.0022 - val_loss: 7.6768e-04 - 11s/epoch - 127us/sample
Epoch 3/85
84077/84077 - 11s - loss: 7.2629e-04 - val_loss: 6.2001e-04 - 11s/epoch - 128us/sample
Epoch 4/85
84077/84077 - 11s - loss: 6.8776e-04 - val_loss: 0.0029 - 11s/epoch - 128us/sample
Epoch 5/85
84077/84077 - 11s - loss: 7.2595e-04 - val_loss: 5.1338e-04 - 11s/epoch - 131us/sample
Epoch 6/85
84077/84077 - 10s - loss: 4.4140e-04 - val_loss: 3.6557e-04 - 10s/epoch - 125us/sample
Epoch 7/85
84077/84077 - 10s - loss: 3.5747e-04 - val_loss: 3.1009e-04 - 10s/epoch - 124us/sample
Epoch 8/85
84077/84077 - 10s - loss: 3.2646e-04 - val_loss: 2.7387e-04 - 10s/epoch - 124us/sample
Epoch 9/85
84077/84077 - 10s - loss: 2.9140e-04 - val_loss: 2.4956e-04 - 10s/epoch - 124us/sample
Epoch 10/85
84077/84077 - 10s - loss: 2.6698e-04 - val_loss: 2.1785e-04 - 10s/epoch - 125us/sample
Epoch 11/85
84077/84077 - 11s - loss: 2.3820e-04 - val_loss: 1.9889e-04 - 11s/epoch - 125us/sample
Epoch 12/85
84077/84077 - 10s - loss: 2.1554e-04 - val_loss: 1.8222e-04 - 10s/epoch - 124us/sample
Epoch 13/85
84077/84077 - 10s - loss: 1.9616e-04 - val_loss: 1.6371e-04 - 10s/epoch - 124us/sample
Epoch 14/85
84077/84077 - 10s - loss: 1.8175e-04 - val_loss: 1.5830e-04 - 10s/epoch - 124us/sample
Epoch 15/85
84077/84077 - 10s - loss: 1.7144e-04 - val_loss: 1.5131e-04 - 10s/epoch - 124us/sample
Epoch 16/85
84077/84077 - 10s - loss: 1.6684e-04 - val_loss: 1.4189e-04 - 10s/epoch - 124us/sample
Epoch 17/85
84077/84077 - 11s - loss: 1.5951e-04 - val_loss: 1.3953e-04 - 11s/epoch - 125us/sample
Epoch 18/85
84077/84077 - 11s - loss: 1.5407e-04 - val_loss: 1.3756e-04 - 11s/epoch - 125us/sample
Epoch 19/85
84077/84077 - 10s - loss: 1.5045e-04 - val_loss: 1.3077e-04 - 10s/epoch - 125us/sample
Epoch 20/85
84077/84077 - 10s - loss: 1.4670e-04 - val_loss: 1.3069e-04 - 10s/epoch - 124us/sample
Epoch 21/85
84077/84077 - 10s - loss: 1.4333e-04 - val_loss: 1.2699e-04 - 10s/epoch - 124us/sample
Epoch 22/85
84077/84077 - 10s - loss: 1.4120e-04 - val_loss: 1.2533e-04 - 10s/epoch - 124us/sample
Epoch 23/85
84077/84077 - 11s - loss: 1.3897e-04 - val_loss: 1.2241e-04 - 11s/epoch - 125us/sample
Epoch 24/85
84077/84077 - 11s - loss: 1.3619e-04 - val_loss: 1.2068e-04 - 11s/epoch - 125us/sample
Epoch 25/85
84077/84077 - 10s - loss: 1.3467e-04 - val_loss: 1.2050e-04 - 10s/epoch - 125us/sample
Epoch 26/85
84077/84077 - 10s - loss: 1.3309e-04 - val_loss: 1.1930e-04 - 10s/epoch - 124us/sample
Epoch 27/85
84077/84077 - 10s - loss: 1.3167e-04 - val_loss: 1.1950e-04 - 10s/epoch - 124us/sample
Epoch 28/85
84077/84077 - 10s - loss: 1.3050e-04 - val_loss: 1.1701e-04 - 10s/epoch - 124us/sample
Epoch 29/85
84077/84077 - 10s - loss: 1.2922e-04 - val_loss: 1.1672e-04 - 10s/epoch - 125us/sample
Epoch 30/85
84077/84077 - 11s - loss: 1.2769e-04 - val_loss: 1.1516e-04 - 11s/epoch - 125us/sample
Epoch 31/85
84077/84077 - 10s - loss: 1.2638e-04 - val_loss: 1.1405e-04 - 10s/epoch - 124us/sample
Epoch 32/85
84077/84077 - 10s - loss: 1.2568e-04 - val_loss: 1.1424e-04 - 10s/epoch - 124us/sample
Epoch 33/85
84077/84077 - 10s - loss: 1.2498e-04 - val_loss: 1.1357e-04 - 10s/epoch - 124us/sample
Epoch 34/85
84077/84077 - 10s - loss: 1.2353e-04 - val_loss: 1.1088e-04 - 10s/epoch - 124us/sample
Epoch 35/85
84077/84077 - 10s - loss: 1.2342e-04 - val_loss: 1.1077e-04 - 10s/epoch - 125us/sample
Epoch 36/85
84077/84077 - 11s - loss: 1.2203e-04 - val_loss: 1.1087e-04 - 11s/epoch - 126us/sample
Epoch 37/85
84077/84077 - 10s - loss: 1.2155e-04 - val_loss: 1.1015e-04 - 10s/epoch - 124us/sample
Epoch 38/85
84077/84077 - 10s - loss: 1.2084e-04 - val_loss: 1.0933e-04 - 10s/epoch - 124us/sample
Epoch 39/85
84077/84077 - 10s - loss: 1.1992e-04 - val_loss: 1.1068e-04 - 10s/epoch - 124us/sample
Epoch 40/85
84077/84077 - 10s - loss: 1.1916e-04 - val_loss: 1.0825e-04 - 10s/epoch - 124us/sample
Epoch 41/85
84077/84077 - 10s - loss: 1.1871e-04 - val_loss: 1.0801e-04 - 10s/epoch - 124us/sample
Epoch 42/85
84077/84077 - 10s - loss: 1.1778e-04 - val_loss: 1.0749e-04 - 10s/epoch - 125us/sample
Epoch 43/85
84077/84077 - 11s - loss: 1.1751e-04 - val_loss: 1.0660e-04 - 11s/epoch - 125us/sample
Epoch 44/85
84077/84077 - 10s - loss: 1.1739e-04 - val_loss: 1.0788e-04 - 10s/epoch - 124us/sample
Epoch 45/85
84077/84077 - 10s - loss: 1.1675e-04 - val_loss: 1.0605e-04 - 10s/epoch - 124us/sample
Epoch 46/85
84077/84077 - 10s - loss: 1.1577e-04 - val_loss: 1.0590e-04 - 10s/epoch - 124us/sample
Epoch 47/85
84077/84077 - 10s - loss: 1.1510e-04 - val_loss: 1.0525e-04 - 10s/epoch - 124us/sample
Epoch 48/85
84077/84077 - 10s - loss: 1.1498e-04 - val_loss: 1.0507e-04 - 10s/epoch - 124us/sample
Epoch 49/85
84077/84077 - 11s - loss: 1.1446e-04 - val_loss: 1.0423e-04 - 11s/epoch - 125us/sample
Epoch 50/85
84077/84077 - 11s - loss: 1.1398e-04 - val_loss: 1.0460e-04 - 11s/epoch - 125us/sample
Epoch 51/85
84077/84077 - 11s - loss: 1.1385e-04 - val_loss: 1.0443e-04 - 11s/epoch - 125us/sample
Epoch 52/85
84077/84077 - 10s - loss: 1.1644e-04 - val_loss: 1.0527e-04 - 10s/epoch - 124us/sample
Epoch 53/85
84077/84077 - 10s - loss: 1.1306e-04 - val_loss: 1.0418e-04 - 10s/epoch - 124us/sample
Epoch 54/85
84077/84077 - 10s - loss: 1.1257e-04 - val_loss: 1.0456e-04 - 10s/epoch - 124us/sample
Epoch 55/85
84077/84077 - 11s - loss: 1.1231e-04 - val_loss: 1.0426e-04 - 11s/epoch - 125us/sample
Epoch 56/85
84077/84077 - 11s - loss: 1.1206e-04 - val_loss: 1.0364e-04 - 11s/epoch - 125us/sample
Epoch 57/85
84077/84077 - 11s - loss: 1.1790e-04 - val_loss: 1.0448e-04 - 11s/epoch - 125us/sample
Epoch 58/85
84077/84077 - 10s - loss: 1.1233e-04 - val_loss: 1.0425e-04 - 10s/epoch - 124us/sample
Epoch 59/85
84077/84077 - 10s - loss: 1.1115e-04 - val_loss: 1.0197e-04 - 10s/epoch - 124us/sample
Epoch 60/85
84077/84077 - 10s - loss: 1.1100e-04 - val_loss: 1.0232e-04 - 10s/epoch - 124us/sample
Epoch 61/85
84077/84077 - 10s - loss: 1.1089e-04 - val_loss: 1.0147e-04 - 10s/epoch - 125us/sample
Epoch 62/85
84077/84077 - 11s - loss: 1.1065e-04 - val_loss: 1.0358e-04 - 11s/epoch - 126us/sample
Epoch 63/85
84077/84077 - 10s - loss: 1.1020e-04 - val_loss: 1.0127e-04 - 10s/epoch - 125us/sample
Epoch 64/85
84077/84077 - 10s - loss: 1.1005e-04 - val_loss: 1.0195e-04 - 10s/epoch - 125us/sample
Epoch 65/85
84077/84077 - 10s - loss: 1.0949e-04 - val_loss: 1.0146e-04 - 10s/epoch - 124us/sample
Epoch 66/85
84077/84077 - 10s - loss: 1.0893e-04 - val_loss: 1.0093e-04 - 10s/epoch - 124us/sample
Epoch 67/85
84077/84077 - 10s - loss: 1.0906e-04 - val_loss: 1.0098e-04 - 10s/epoch - 125us/sample
Epoch 68/85
84077/84077 - 11s - loss: 1.0884e-04 - val_loss: 9.9575e-05 - 11s/epoch - 126us/sample
Epoch 69/85
84077/84077 - 10s - loss: 1.0831e-04 - val_loss: 1.0007e-04 - 10s/epoch - 125us/sample
Epoch 70/85
84077/84077 - 10s - loss: 1.0884e-04 - val_loss: 1.0172e-04 - 10s/epoch - 125us/sample
Epoch 71/85
84077/84077 - 10s - loss: 1.0825e-04 - val_loss: 1.0051e-04 - 10s/epoch - 124us/sample
Epoch 72/85
84077/84077 - 10s - loss: 1.0752e-04 - val_loss: 9.9109e-05 - 10s/epoch - 124us/sample
Epoch 73/85
84077/84077 - 10s - loss: 1.0753e-04 - val_loss: 9.9639e-05 - 10s/epoch - 125us/sample
Epoch 74/85
84077/84077 - 11s - loss: 1.0734e-04 - val_loss: 9.9214e-05 - 11s/epoch - 125us/sample
Epoch 75/85
84077/84077 - 10s - loss: 1.0700e-04 - val_loss: 9.9945e-05 - 10s/epoch - 125us/sample
Epoch 76/85
84077/84077 - 10s - loss: 1.0691e-04 - val_loss: 9.8545e-05 - 10s/epoch - 125us/sample
Epoch 77/85
84077/84077 - 10s - loss: 1.0652e-04 - val_loss: 1.0050e-04 - 10s/epoch - 124us/sample
Epoch 78/85
84077/84077 - 10s - loss: 1.0680e-04 - val_loss: 9.9003e-05 - 10s/epoch - 124us/sample
Epoch 79/85
84077/84077 - 10s - loss: 1.0608e-04 - val_loss: 9.8358e-05 - 10s/epoch - 125us/sample
Epoch 80/85
84077/84077 - 11s - loss: 1.0634e-04 - val_loss: 9.8800e-05 - 11s/epoch - 125us/sample
Epoch 81/85
84077/84077 - 11s - loss: 1.0593e-04 - val_loss: 9.9600e-05 - 11s/epoch - 126us/sample
Epoch 82/85
84077/84077 - 10s - loss: 1.0614e-04 - val_loss: 9.8395e-05 - 10s/epoch - 124us/sample
Epoch 83/85
84077/84077 - 10s - loss: 1.0550e-04 - val_loss: 9.7821e-05 - 10s/epoch - 125us/sample
Epoch 84/85
84077/84077 - 10s - loss: 1.0570e-04 - val_loss: 9.8390e-05 - 10s/epoch - 124us/sample
Epoch 85/85
84077/84077 - 10s - loss: 1.0523e-04 - val_loss: 9.7648e-05 - 10s/epoch - 124us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 9.76482789621017e-05
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:04:51.927359: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_41/outputlayer/BiasAdd' id:52701 op device:{requested: '', assigned: ''} def:{{{node decoder_model_41/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_41/outputlayer/MatMul, decoder_model_41/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023735604459569997
cosine 0.02344375756541238
MAE: 0.0022862655495554567
RMSE: 0.00867867645154511
r2: 0.9413817168799189
RMSE zero-vector: 0.04004287452915337
['2.0custom_VAE', 'logcosh', 64, 85, 0.0012000000000000001, 0.3, 282, 0.00010522788220723141, 9.76482789621017e-05, 0.023735604459569997, 0.02344375756541238, 0.0022862655495554567, 0.00867867645154511, 0.9413817168799189, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0012000000000000001 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_126 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_126 (ReLU)               (None, 1980)         0           ['batch_normalization_126[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_126[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 08:05:19.587417: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_127/moving_mean/Assign' id:53752 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_127/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_127/moving_mean, batch_normalization_127/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:06:11.903489: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_42/mul' id:54015 op device:{requested: '', assigned: ''} def:{{{node loss_42/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_42/mul/x, loss_42/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 66s - loss: 0.0049 - val_loss: 0.0015 - 66s/epoch - 780us/sample
Epoch 2/85
84077/84077 - 48s - loss: 0.0016 - val_loss: 0.0012 - 48s/epoch - 568us/sample
Epoch 3/85
84077/84077 - 47s - loss: 0.0013 - val_loss: 0.0011 - 47s/epoch - 561us/sample
Epoch 4/85
84077/84077 - 47s - loss: 0.0011 - val_loss: 0.0018 - 47s/epoch - 560us/sample
Epoch 5/85
84077/84077 - 47s - loss: 9.9614e-04 - val_loss: 0.0098 - 47s/epoch - 564us/sample
Epoch 6/85
84077/84077 - 47s - loss: 9.3721e-04 - val_loss: 0.0372 - 47s/epoch - 562us/sample
Epoch 7/85
84077/84077 - 47s - loss: 9.0052e-04 - val_loss: 0.0848 - 47s/epoch - 564us/sample
Epoch 8/85
84077/84077 - 47s - loss: 8.6656e-04 - val_loss: 0.0698 - 47s/epoch - 563us/sample
Epoch 9/85
84077/84077 - 47s - loss: 8.3672e-04 - val_loss: 0.1594 - 47s/epoch - 562us/sample
Epoch 10/85
84077/84077 - 48s - loss: 8.1357e-04 - val_loss: 0.1637 - 48s/epoch - 566us/sample
Epoch 11/85
84077/84077 - 47s - loss: 7.9749e-04 - val_loss: 0.1912 - 47s/epoch - 565us/sample
Epoch 12/85
84077/84077 - 47s - loss: 7.9174e-04 - val_loss: 0.1407 - 47s/epoch - 563us/sample
Epoch 13/85
84077/84077 - 47s - loss: 7.8432e-04 - val_loss: 0.1733 - 47s/epoch - 563us/sample
Epoch 14/85
84077/84077 - 47s - loss: 7.7989e-04 - val_loss: 0.2759 - 47s/epoch - 564us/sample
Epoch 15/85
84077/84077 - 47s - loss: 7.7606e-04 - val_loss: 0.1798 - 47s/epoch - 565us/sample
Epoch 16/85
84077/84077 - 47s - loss: 7.7143e-04 - val_loss: 0.1449 - 47s/epoch - 564us/sample
Epoch 17/85
84077/84077 - 47s - loss: 7.6643e-04 - val_loss: 0.1894 - 47s/epoch - 562us/sample
Epoch 18/85
84077/84077 - 47s - loss: 7.5884e-04 - val_loss: 0.1986 - 47s/epoch - 564us/sample
Epoch 19/85
84077/84077 - 47s - loss: 7.5299e-04 - val_loss: 0.3213 - 47s/epoch - 561us/sample
Epoch 20/85
84077/84077 - 47s - loss: 7.4942e-04 - val_loss: 0.2929 - 47s/epoch - 563us/sample
Epoch 21/85
84077/84077 - 47s - loss: 7.4662e-04 - val_loss: 0.2978 - 47s/epoch - 562us/sample
Epoch 22/85
84077/84077 - 47s - loss: 7.4506e-04 - val_loss: 0.3368 - 47s/epoch - 561us/sample
Epoch 23/85
84077/84077 - 47s - loss: 7.4217e-04 - val_loss: 0.2435 - 47s/epoch - 563us/sample
Epoch 24/85
84077/84077 - 47s - loss: 7.4041e-04 - val_loss: 0.3004 - 47s/epoch - 563us/sample
Epoch 25/85
84077/84077 - 47s - loss: 7.3913e-04 - val_loss: 0.3023 - 47s/epoch - 562us/sample
Epoch 26/85
84077/84077 - 47s - loss: 7.3760e-04 - val_loss: 0.2860 - 47s/epoch - 563us/sample
Epoch 27/85
84077/84077 - 47s - loss: 7.3412e-04 - val_loss: 0.2905 - 47s/epoch - 562us/sample
Epoch 28/85
84077/84077 - 47s - loss: 7.3340e-04 - val_loss: 0.2783 - 47s/epoch - 563us/sample
Epoch 29/85
84077/84077 - 47s - loss: 7.3053e-04 - val_loss: 0.3069 - 47s/epoch - 563us/sample
Epoch 30/85
84077/84077 - 47s - loss: 7.2894e-04 - val_loss: 0.2941 - 47s/epoch - 562us/sample
Epoch 31/85
84077/84077 - 47s - loss: 7.2721e-04 - val_loss: 0.2111 - 47s/epoch - 563us/sample
Epoch 32/85
84077/84077 - 47s - loss: 7.2557e-04 - val_loss: 0.2603 - 47s/epoch - 564us/sample
Epoch 33/85
84077/84077 - 47s - loss: 7.2473e-04 - val_loss: 0.2379 - 47s/epoch - 561us/sample
Epoch 34/85
84077/84077 - 47s - loss: 7.2195e-04 - val_loss: 0.2721 - 47s/epoch - 564us/sample
Epoch 35/85
84077/84077 - 47s - loss: 7.2061e-04 - val_loss: 0.3042 - 47s/epoch - 561us/sample
Epoch 36/85
84077/84077 - 47s - loss: 7.1967e-04 - val_loss: 0.3153 - 47s/epoch - 563us/sample
Epoch 37/85
84077/84077 - 47s - loss: 7.1955e-04 - val_loss: 0.2288 - 47s/epoch - 563us/sample
Epoch 38/85
84077/84077 - 47s - loss: 7.1643e-04 - val_loss: 0.3127 - 47s/epoch - 563us/sample
Epoch 39/85
84077/84077 - 47s - loss: 7.1517e-04 - val_loss: 0.2777 - 47s/epoch - 561us/sample
Epoch 40/85
84077/84077 - 47s - loss: 7.1308e-04 - val_loss: 0.2268 - 47s/epoch - 565us/sample
Epoch 41/85
84077/84077 - 47s - loss: 7.1331e-04 - val_loss: 0.3184 - 47s/epoch - 565us/sample
Epoch 42/85
84077/84077 - 47s - loss: 7.1167e-04 - val_loss: 0.2230 - 47s/epoch - 564us/sample
Epoch 43/85
84077/84077 - 47s - loss: 7.1103e-04 - val_loss: 0.2101 - 47s/epoch - 561us/sample
Epoch 44/85
84077/84077 - 47s - loss: 7.0895e-04 - val_loss: 0.2569 - 47s/epoch - 564us/sample
Epoch 45/85
84077/84077 - 47s - loss: 7.0869e-04 - val_loss: 0.2699 - 47s/epoch - 562us/sample
Epoch 46/85
84077/84077 - 47s - loss: 7.0739e-04 - val_loss: 0.2978 - 47s/epoch - 561us/sample
Epoch 47/85
84077/84077 - 47s - loss: 7.0606e-04 - val_loss: 0.1651 - 47s/epoch - 562us/sample
Epoch 48/85
84077/84077 - 47s - loss: 7.0441e-04 - val_loss: 0.2625 - 47s/epoch - 564us/sample
Epoch 49/85
84077/84077 - 47s - loss: 7.0457e-04 - val_loss: 0.2389 - 47s/epoch - 563us/sample
Epoch 50/85
84077/84077 - 47s - loss: 7.0238e-04 - val_loss: 0.2125 - 47s/epoch - 564us/sample
Epoch 51/85
84077/84077 - 47s - loss: 7.0264e-04 - val_loss: 0.1962 - 47s/epoch - 564us/sample
Epoch 52/85
84077/84077 - 47s - loss: 7.0189e-04 - val_loss: 0.1979 - 47s/epoch - 562us/sample
Epoch 53/85
84077/84077 - 47s - loss: 7.0086e-04 - val_loss: 0.2929 - 47s/epoch - 564us/sample
Epoch 54/85
84077/84077 - 47s - loss: 7.0154e-04 - val_loss: 0.2313 - 47s/epoch - 563us/sample
Epoch 55/85
84077/84077 - 47s - loss: 6.9995e-04 - val_loss: 0.1962 - 47s/epoch - 564us/sample
Epoch 56/85
84077/84077 - 47s - loss: 6.9975e-04 - val_loss: 0.2647 - 47s/epoch - 564us/sample
Epoch 57/85
84077/84077 - 47s - loss: 6.9899e-04 - val_loss: 0.2604 - 47s/epoch - 563us/sample
Epoch 58/85
84077/84077 - 47s - loss: 6.9816e-04 - val_loss: 0.1753 - 47s/epoch - 562us/sample
Epoch 59/85
84077/84077 - 47s - loss: 6.9807e-04 - val_loss: 0.3056 - 47s/epoch - 563us/sample
Epoch 60/85
84077/84077 - 47s - loss: 6.9751e-04 - val_loss: 0.1787 - 47s/epoch - 561us/sample
Epoch 61/85
84077/84077 - 47s - loss: 6.9812e-04 - val_loss: 0.1561 - 47s/epoch - 563us/sample
Epoch 62/85
84077/84077 - 47s - loss: 6.9718e-04 - val_loss: 0.1934 - 47s/epoch - 562us/sample
Epoch 63/85
84077/84077 - 47s - loss: 6.9576e-04 - val_loss: 0.1926 - 47s/epoch - 564us/sample
Epoch 64/85
84077/84077 - 47s - loss: 6.9570e-04 - val_loss: 0.2310 - 47s/epoch - 562us/sample
Epoch 65/85
84077/84077 - 47s - loss: 6.9444e-04 - val_loss: 0.2030 - 47s/epoch - 562us/sample
Epoch 66/85
84077/84077 - 47s - loss: 6.9451e-04 - val_loss: 0.1343 - 47s/epoch - 565us/sample
Epoch 67/85
84077/84077 - 47s - loss: 6.9298e-04 - val_loss: 0.1579 - 47s/epoch - 561us/sample
Epoch 68/85
84077/84077 - 47s - loss: 6.9273e-04 - val_loss: 0.2195 - 47s/epoch - 564us/sample
Epoch 69/85
84077/84077 - 47s - loss: 6.9123e-04 - val_loss: 0.1982 - 47s/epoch - 562us/sample
Epoch 70/85
84077/84077 - 47s - loss: 6.9257e-04 - val_loss: 0.1453 - 47s/epoch - 562us/sample
Epoch 71/85
84077/84077 - 47s - loss: 6.9149e-04 - val_loss: 0.1640 - 47s/epoch - 564us/sample
Epoch 72/85
84077/84077 - 47s - loss: 6.9113e-04 - val_loss: 0.1643 - 47s/epoch - 561us/sample
Epoch 73/85
84077/84077 - 48s - loss: 6.8932e-04 - val_loss: 0.2183 - 48s/epoch - 566us/sample
Epoch 74/85
84077/84077 - 47s - loss: 6.8775e-04 - val_loss: 0.1703 - 47s/epoch - 564us/sample
Epoch 75/85
84077/84077 - 47s - loss: 6.8713e-04 - val_loss: 0.2346 - 47s/epoch - 561us/sample
Epoch 76/85
84077/84077 - 47s - loss: 6.8619e-04 - val_loss: 0.2848 - 47s/epoch - 563us/sample
Epoch 77/85
84077/84077 - 47s - loss: 6.8663e-04 - val_loss: 0.1779 - 47s/epoch - 564us/sample
Epoch 78/85
84077/84077 - 47s - loss: 6.8664e-04 - val_loss: 0.1361 - 47s/epoch - 561us/sample
Epoch 79/85
84077/84077 - 47s - loss: 6.8527e-04 - val_loss: 0.1856 - 47s/epoch - 564us/sample
Epoch 80/85
84077/84077 - 47s - loss: 6.8477e-04 - val_loss: 0.1310 - 47s/epoch - 561us/sample
Epoch 81/85
84077/84077 - 48s - loss: 6.8416e-04 - val_loss: 0.1590 - 48s/epoch - 566us/sample
Epoch 82/85
84077/84077 - 47s - loss: 6.8400e-04 - val_loss: 0.2204 - 47s/epoch - 565us/sample
Epoch 83/85
84077/84077 - 48s - loss: 6.8340e-04 - val_loss: 0.2622 - 48s/epoch - 566us/sample
Epoch 84/85
84077/84077 - 47s - loss: 6.8268e-04 - val_loss: 0.1736 - 47s/epoch - 561us/sample
Epoch 85/85
84077/84077 - 47s - loss: 6.8146e-04 - val_loss: 0.1909 - 47s/epoch - 563us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.19088164452380196
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:12:35.983618: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_42/outputlayer/BiasAdd' id:53986 op device:{requested: '', assigned: ''} def:{{{node decoder_model_42/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_42/outputlayer/MatMul, decoder_model_42/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2047517262721638
cosine 0.20247743054725229
MAE: 0.028307502613762015
RMSE: 0.4377331082955841
r2: -148.6667448621472
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 8, 85, 0.0012000000000000001, 0.3, 282, 0.000681457272837661, 0.19088164452380196, 0.2047517262721638, 0.20247743054725229, 0.028307502613762015, 0.4377331082955841, -148.6667448621472, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 5
Fitness    = 595.8845894037719
Last generation's best solutions = [2.0 90 0.001 64 1] with fitness 595.8845894037719.
Best solutions :  [array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object), array([2.0, 90, 0.001, 64, 1], dtype=object)]
Best solutions fitness :  [595.8845894037719, 595.8845894037719, 595.8845894037719, 595.8845894037719, 595.8845894037719]
[2.3000000000000003 85 0.0012000000000000001 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_129 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_129 (ReLU)               (None, 2168)         0           ['batch_normalization_129[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          611658      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          611658      ['re_lu_129[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2748517     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,027,097
Trainable params: 6,017,861
Non-trainable params: 9,236
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 09:13:04.595620: W tensorflow/c/c_api.cc:291] Operation '{name:'training_86/Adam/batch_normalization_131/beta/m/Assign' id:55818 op device:{requested: '', assigned: ''} def:{{{node training_86/Adam/batch_normalization_131/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_86/Adam/batch_normalization_131/beta/m, training_86/Adam/batch_normalization_131/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:13:21.781300: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_43/mul' id:55270 op device:{requested: '', assigned: ''} def:{{{node loss_43/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_43/mul/x, loss_43/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 29s - loss: 0.0342 - val_loss: 0.0014 - 29s/epoch - 339us/sample
Epoch 2/85
84077/84077 - 11s - loss: 0.0036 - val_loss: 0.0025 - 11s/epoch - 130us/sample
Epoch 3/85
84077/84077 - 11s - loss: 0.0015 - val_loss: 0.0047 - 11s/epoch - 131us/sample
Epoch 4/85
84077/84077 - 11s - loss: 0.0012 - val_loss: 9.3786e-04 - 11s/epoch - 135us/sample
Epoch 5/85
84077/84077 - 11s - loss: 8.7183e-04 - val_loss: 0.0018 - 11s/epoch - 129us/sample
Epoch 6/85
84077/84077 - 11s - loss: 8.1791e-04 - val_loss: 7.1871e-04 - 11s/epoch - 128us/sample
Epoch 7/85
84077/84077 - 11s - loss: 6.7771e-04 - val_loss: 5.4214e-04 - 11s/epoch - 127us/sample
Epoch 8/85
84077/84077 - 11s - loss: 5.5480e-04 - val_loss: 4.8496e-04 - 11s/epoch - 128us/sample
Epoch 9/85
84077/84077 - 11s - loss: 4.9053e-04 - val_loss: 4.1879e-04 - 11s/epoch - 128us/sample
Epoch 10/85
84077/84077 - 11s - loss: 4.0135e-04 - val_loss: 3.4958e-04 - 11s/epoch - 130us/sample
Epoch 11/85
84077/84077 - 11s - loss: 3.5299e-04 - val_loss: 2.8922e-04 - 11s/epoch - 128us/sample
Epoch 12/85
84077/84077 - 11s - loss: 3.2040e-04 - val_loss: 2.7727e-04 - 11s/epoch - 127us/sample
Epoch 13/85
84077/84077 - 11s - loss: 3.0148e-04 - val_loss: 2.8413e-04 - 11s/epoch - 127us/sample
Epoch 14/85
84077/84077 - 11s - loss: 2.8245e-04 - val_loss: 2.4685e-04 - 11s/epoch - 128us/sample
Epoch 15/85
84077/84077 - 11s - loss: 2.6912e-04 - val_loss: 2.3959e-04 - 11s/epoch - 129us/sample
Epoch 16/85
84077/84077 - 11s - loss: 2.6910e-04 - val_loss: 2.2085e-04 - 11s/epoch - 130us/sample
Epoch 17/85
84077/84077 - 11s - loss: 2.5205e-04 - val_loss: 2.1007e-04 - 11s/epoch - 128us/sample
Epoch 18/85
84077/84077 - 11s - loss: 2.3326e-04 - val_loss: 2.0684e-04 - 11s/epoch - 127us/sample
Epoch 19/85
84077/84077 - 11s - loss: 2.2548e-04 - val_loss: 1.9581e-04 - 11s/epoch - 127us/sample
Epoch 20/85
84077/84077 - 11s - loss: 2.1679e-04 - val_loss: 1.9531e-04 - 11s/epoch - 128us/sample
Epoch 21/85
84077/84077 - 11s - loss: 2.1147e-04 - val_loss: 1.8847e-04 - 11s/epoch - 128us/sample
Epoch 22/85
84077/84077 - 11s - loss: 2.0578e-04 - val_loss: 1.8563e-04 - 11s/epoch - 130us/sample
Epoch 23/85
84077/84077 - 11s - loss: 2.0791e-04 - val_loss: 1.8090e-04 - 11s/epoch - 128us/sample
Epoch 24/85
84077/84077 - 11s - loss: 1.9807e-04 - val_loss: 1.7710e-04 - 11s/epoch - 127us/sample
Epoch 25/85
84077/84077 - 11s - loss: 1.9670e-04 - val_loss: 1.7695e-04 - 11s/epoch - 127us/sample
Epoch 26/85
84077/84077 - 11s - loss: 1.9216e-04 - val_loss: 1.7661e-04 - 11s/epoch - 128us/sample
Epoch 27/85
84077/84077 - 11s - loss: 1.8891e-04 - val_loss: 1.7110e-04 - 11s/epoch - 128us/sample
Epoch 28/85
84077/84077 - 11s - loss: 1.8670e-04 - val_loss: 1.6946e-04 - 11s/epoch - 130us/sample
Epoch 29/85
84077/84077 - 11s - loss: 2.0008e-04 - val_loss: 1.7082e-04 - 11s/epoch - 128us/sample
Epoch 30/85
84077/84077 - 11s - loss: 1.8476e-04 - val_loss: 1.6512e-04 - 11s/epoch - 128us/sample
Epoch 31/85
84077/84077 - 11s - loss: 1.8190e-04 - val_loss: 1.6232e-04 - 11s/epoch - 127us/sample
Epoch 32/85
84077/84077 - 11s - loss: 1.7913e-04 - val_loss: 1.6314e-04 - 11s/epoch - 128us/sample
Epoch 33/85
84077/84077 - 11s - loss: 1.7791e-04 - val_loss: 1.6099e-04 - 11s/epoch - 128us/sample
Epoch 34/85
84077/84077 - 11s - loss: 1.7805e-04 - val_loss: 1.6111e-04 - 11s/epoch - 129us/sample
Epoch 35/85
84077/84077 - 11s - loss: 1.7518e-04 - val_loss: 1.5861e-04 - 11s/epoch - 129us/sample
Epoch 36/85
84077/84077 - 11s - loss: 1.7406e-04 - val_loss: 1.6059e-04 - 11s/epoch - 127us/sample
Epoch 37/85
84077/84077 - 11s - loss: 1.7189e-04 - val_loss: 1.5553e-04 - 11s/epoch - 128us/sample
Epoch 38/85
84077/84077 - 11s - loss: 1.7293e-04 - val_loss: 1.5480e-04 - 11s/epoch - 128us/sample
Epoch 39/85
84077/84077 - 11s - loss: 1.7184e-04 - val_loss: 1.5571e-04 - 11s/epoch - 128us/sample
Epoch 40/85
84077/84077 - 11s - loss: 1.6855e-04 - val_loss: 1.5268e-04 - 11s/epoch - 130us/sample
Epoch 41/85
84077/84077 - 11s - loss: 1.6785e-04 - val_loss: 1.5444e-04 - 11s/epoch - 128us/sample
Epoch 42/85
84077/84077 - 11s - loss: 1.6801e-04 - val_loss: 1.5116e-04 - 11s/epoch - 128us/sample
Epoch 43/85
84077/84077 - 11s - loss: 1.6624e-04 - val_loss: 1.5129e-04 - 11s/epoch - 128us/sample
Epoch 44/85
84077/84077 - 11s - loss: 1.6609e-04 - val_loss: 1.5098e-04 - 11s/epoch - 128us/sample
Epoch 45/85
84077/84077 - 11s - loss: 1.9137e-04 - val_loss: 1.7702e-04 - 11s/epoch - 128us/sample
Epoch 46/85
84077/84077 - 11s - loss: 1.7459e-04 - val_loss: 1.5290e-04 - 11s/epoch - 130us/sample
Epoch 47/85
84077/84077 - 11s - loss: 1.6603e-04 - val_loss: 1.5054e-04 - 11s/epoch - 128us/sample
Epoch 48/85
84077/84077 - 11s - loss: 1.6363e-04 - val_loss: 1.4995e-04 - 11s/epoch - 128us/sample
Epoch 49/85
84077/84077 - 11s - loss: 1.6696e-04 - val_loss: 1.4900e-04 - 11s/epoch - 127us/sample
Epoch 50/85
84077/84077 - 11s - loss: 1.6216e-04 - val_loss: 1.4717e-04 - 11s/epoch - 128us/sample
Epoch 51/85
84077/84077 - 11s - loss: 1.6107e-04 - val_loss: 1.4719e-04 - 11s/epoch - 128us/sample
Epoch 52/85
84077/84077 - 11s - loss: 1.6039e-04 - val_loss: 1.4796e-04 - 11s/epoch - 129us/sample
Epoch 53/85
84077/84077 - 11s - loss: 1.5981e-04 - val_loss: 1.4703e-04 - 11s/epoch - 129us/sample
Epoch 54/85
84077/84077 - 11s - loss: 1.5833e-04 - val_loss: 1.4790e-04 - 11s/epoch - 128us/sample
Epoch 55/85
84077/84077 - 11s - loss: 1.5935e-04 - val_loss: 1.4457e-04 - 11s/epoch - 128us/sample
Epoch 56/85
84077/84077 - 11s - loss: 1.6314e-04 - val_loss: 1.4781e-04 - 11s/epoch - 128us/sample
Epoch 57/85
84077/84077 - 11s - loss: 1.5804e-04 - val_loss: 1.4490e-04 - 11s/epoch - 128us/sample
Epoch 58/85
84077/84077 - 11s - loss: 1.5658e-04 - val_loss: 1.4397e-04 - 11s/epoch - 129us/sample
Epoch 59/85
84077/84077 - 11s - loss: 1.5559e-04 - val_loss: 1.4533e-04 - 11s/epoch - 129us/sample
Epoch 60/85
84077/84077 - 11s - loss: 1.5566e-04 - val_loss: 1.4185e-04 - 11s/epoch - 128us/sample
Epoch 61/85
84077/84077 - 11s - loss: 1.5484e-04 - val_loss: 1.4328e-04 - 11s/epoch - 128us/sample
Epoch 62/85
84077/84077 - 11s - loss: 1.5419e-04 - val_loss: 1.4201e-04 - 11s/epoch - 128us/sample
Epoch 63/85
84077/84077 - 11s - loss: 1.5685e-04 - val_loss: 1.4127e-04 - 11s/epoch - 128us/sample
Epoch 64/85
84077/84077 - 11s - loss: 1.5353e-04 - val_loss: 1.4371e-04 - 11s/epoch - 129us/sample
Epoch 65/85
84077/84077 - 11s - loss: 1.5376e-04 - val_loss: 1.4105e-04 - 11s/epoch - 130us/sample
Epoch 66/85
84077/84077 - 11s - loss: 1.5201e-04 - val_loss: 1.3972e-04 - 11s/epoch - 128us/sample
Epoch 67/85
84077/84077 - 11s - loss: 1.6247e-04 - val_loss: 1.3878e-04 - 11s/epoch - 128us/sample
Epoch 68/85
84077/84077 - 11s - loss: 1.5312e-04 - val_loss: 1.4274e-04 - 11s/epoch - 128us/sample
Epoch 69/85
84077/84077 - 11s - loss: 1.5126e-04 - val_loss: 1.4057e-04 - 11s/epoch - 128us/sample
Epoch 70/85
84077/84077 - 11s - loss: 1.5105e-04 - val_loss: 1.4002e-04 - 11s/epoch - 129us/sample
Epoch 71/85
84077/84077 - 11s - loss: 1.5115e-04 - val_loss: 1.4137e-04 - 11s/epoch - 130us/sample
Epoch 72/85
84077/84077 - 11s - loss: 1.5061e-04 - val_loss: 1.3881e-04 - 11s/epoch - 128us/sample
Epoch 73/85
84077/84077 - 11s - loss: 1.4954e-04 - val_loss: 1.3656e-04 - 11s/epoch - 128us/sample
Epoch 74/85
84077/84077 - 11s - loss: 1.5069e-04 - val_loss: 1.3817e-04 - 11s/epoch - 128us/sample
Epoch 75/85
84077/84077 - 11s - loss: 1.4952e-04 - val_loss: 1.3792e-04 - 11s/epoch - 128us/sample
Epoch 76/85
84077/84077 - 11s - loss: 1.4908e-04 - val_loss: 1.3819e-04 - 11s/epoch - 130us/sample
Epoch 77/85
84077/84077 - 11s - loss: 1.4862e-04 - val_loss: 1.3799e-04 - 11s/epoch - 128us/sample
Epoch 78/85
84077/84077 - 11s - loss: 1.4909e-04 - val_loss: 1.3672e-04 - 11s/epoch - 128us/sample
Epoch 79/85
84077/84077 - 11s - loss: 1.4788e-04 - val_loss: 1.3587e-04 - 11s/epoch - 128us/sample
Epoch 80/85
84077/84077 - 11s - loss: 1.4894e-04 - val_loss: 1.3794e-04 - 11s/epoch - 128us/sample
Epoch 81/85
84077/84077 - 11s - loss: 1.4767e-04 - val_loss: 1.3625e-04 - 11s/epoch - 128us/sample
Epoch 82/85
84077/84077 - 11s - loss: 1.5205e-04 - val_loss: 1.3832e-04 - 11s/epoch - 129us/sample
Epoch 83/85
84077/84077 - 11s - loss: 1.4867e-04 - val_loss: 1.3688e-04 - 11s/epoch - 129us/sample
Epoch 84/85
84077/84077 - 11s - loss: 1.4671e-04 - val_loss: 1.3693e-04 - 11s/epoch - 128us/sample
Epoch 85/85
84077/84077 - 11s - loss: 1.4670e-04 - val_loss: 1.3658e-04 - 11s/epoch - 128us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.00013658109586299589
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:28:34.426141: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_43/outputlayer/BiasAdd' id:55241 op device:{requested: '', assigned: ''} def:{{{node decoder_model_43/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_43/outputlayer/MatMul, decoder_model_43/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01593080371358728
cosine 0.015735994188352035
MAE: 0.0020794014277818406
RMSE: 0.007339703146592209
r2: 0.9583477208200225
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.3, 282, 0.0001467048098335581, 0.00013658109586299589, 0.01593080371358728, 0.015735994188352035, 0.0020794014277818406, 0.007339703146592209, 0.9583477208200225, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 85 0.0008 8 1] 4
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2168)         2046592     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_132 (Batch  (None, 2168)        8672        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_132 (ReLU)               (None, 2168)         0           ['batch_normalization_132[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          611658      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          611658      ['re_lu_132[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2748517     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,027,097
Trainable params: 6,017,861
Non-trainable params: 9,236
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 09:29:03.829168: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_44/bias/Assign' id:56243 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_44/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_44/bias, dense_dec1_44/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:29:56.285106: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_44/mul' id:56525 op device:{requested: '', assigned: ''} def:{{{node loss_44/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_44/mul/x, loss_44/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 66s - loss: 0.0528 - val_loss: 77.1295 - 66s/epoch - 789us/sample
Epoch 2/85
84077/84077 - 48s - loss: 0.0028 - val_loss: 0.0012 - 48s/epoch - 567us/sample
Epoch 3/85
84077/84077 - 47s - loss: 0.0013 - val_loss: 0.0011 - 47s/epoch - 565us/sample
Epoch 4/85
84077/84077 - 47s - loss: 0.0010 - val_loss: 0.0050 - 47s/epoch - 563us/sample
Epoch 5/85
84077/84077 - 47s - loss: 9.8063e-04 - val_loss: 0.0040 - 47s/epoch - 564us/sample
Epoch 6/85
84077/84077 - 48s - loss: 9.0306e-04 - val_loss: 0.0106 - 48s/epoch - 565us/sample
Epoch 7/85
84077/84077 - 47s - loss: 8.4716e-04 - val_loss: 0.0195 - 47s/epoch - 562us/sample
Epoch 8/85
84077/84077 - 48s - loss: 8.2277e-04 - val_loss: 0.0244 - 48s/epoch - 566us/sample
Epoch 9/85
84077/84077 - 48s - loss: 8.0861e-04 - val_loss: 0.0323 - 48s/epoch - 567us/sample
Epoch 10/85
84077/84077 - 48s - loss: 7.9669e-04 - val_loss: 0.0379 - 48s/epoch - 566us/sample
Epoch 11/85
84077/84077 - 47s - loss: 7.8263e-04 - val_loss: 0.0386 - 47s/epoch - 564us/sample
Epoch 12/85
84077/84077 - 48s - loss: 7.7182e-04 - val_loss: 0.0544 - 48s/epoch - 567us/sample
Epoch 13/85
84077/84077 - 48s - loss: 7.6133e-04 - val_loss: 0.0583 - 48s/epoch - 567us/sample
Epoch 14/85
84077/84077 - 48s - loss: 7.5069e-04 - val_loss: 0.0643 - 48s/epoch - 567us/sample
Epoch 15/85
84077/84077 - 47s - loss: 7.4402e-04 - val_loss: 0.0699 - 47s/epoch - 563us/sample
Epoch 16/85
84077/84077 - 48s - loss: 7.3794e-04 - val_loss: 0.0868 - 48s/epoch - 566us/sample
Epoch 17/85
84077/84077 - 47s - loss: 7.3395e-04 - val_loss: 0.0723 - 47s/epoch - 563us/sample
Epoch 18/85
84077/84077 - 48s - loss: 7.2913e-04 - val_loss: 0.1058 - 48s/epoch - 566us/sample
Epoch 19/85
84077/84077 - 48s - loss: 7.2500e-04 - val_loss: 0.0926 - 48s/epoch - 567us/sample
Epoch 20/85
84077/84077 - 48s - loss: 7.2336e-04 - val_loss: 0.1010 - 48s/epoch - 566us/sample
Epoch 21/85
84077/84077 - 47s - loss: 7.2048e-04 - val_loss: 0.1132 - 47s/epoch - 563us/sample
Epoch 22/85
84077/84077 - 48s - loss: 7.1646e-04 - val_loss: 0.1440 - 48s/epoch - 566us/sample
Epoch 23/85
84077/84077 - 48s - loss: 7.1422e-04 - val_loss: 0.1417 - 48s/epoch - 566us/sample
Epoch 24/85
84077/84077 - 47s - loss: 7.1188e-04 - val_loss: 0.1559 - 47s/epoch - 563us/sample
Epoch 25/85
84077/84077 - 48s - loss: 7.0915e-04 - val_loss: 0.1607 - 48s/epoch - 566us/sample
Epoch 26/85
84077/84077 - 48s - loss: 7.0715e-04 - val_loss: 0.1763 - 48s/epoch - 566us/sample
Epoch 27/85
84077/84077 - 47s - loss: 7.0506e-04 - val_loss: 0.1754 - 47s/epoch - 562us/sample
Epoch 28/85
84077/84077 - 48s - loss: 7.0260e-04 - val_loss: 0.1617 - 48s/epoch - 566us/sample
Epoch 29/85
84077/84077 - 48s - loss: 6.9988e-04 - val_loss: 0.1594 - 48s/epoch - 566us/sample
Epoch 30/85
84077/84077 - 47s - loss: 6.9795e-04 - val_loss: 0.1949 - 47s/epoch - 563us/sample
Epoch 31/85
84077/84077 - 48s - loss: 6.9547e-04 - val_loss: 0.1737 - 48s/epoch - 566us/sample
Epoch 32/85
84077/84077 - 47s - loss: 6.9270e-04 - val_loss: 0.2041 - 47s/epoch - 564us/sample
Epoch 33/85
84077/84077 - 48s - loss: 6.8928e-04 - val_loss: 0.1741 - 48s/epoch - 565us/sample
Epoch 34/85
84077/84077 - 48s - loss: 6.8699e-04 - val_loss: 0.1808 - 48s/epoch - 566us/sample
Epoch 35/85
84077/84077 - 48s - loss: 6.8399e-04 - val_loss: 0.1890 - 48s/epoch - 566us/sample
Epoch 36/85
84077/84077 - 47s - loss: 6.8128e-04 - val_loss: 0.1871 - 47s/epoch - 562us/sample
Epoch 37/85
84077/84077 - 48s - loss: 6.8066e-04 - val_loss: 0.2290 - 48s/epoch - 566us/sample
Epoch 38/85
84077/84077 - 48s - loss: 6.7799e-04 - val_loss: 0.1970 - 48s/epoch - 566us/sample
Epoch 39/85
84077/84077 - 47s - loss: 6.7617e-04 - val_loss: 0.2484 - 47s/epoch - 563us/sample
Epoch 40/85
84077/84077 - 48s - loss: 6.7301e-04 - val_loss: 0.1827 - 48s/epoch - 566us/sample
Epoch 41/85
84077/84077 - 48s - loss: 6.7204e-04 - val_loss: 0.2087 - 48s/epoch - 566us/sample
Epoch 42/85
84077/84077 - 48s - loss: 6.7098e-04 - val_loss: 0.1576 - 48s/epoch - 566us/sample
Epoch 43/85
84077/84077 - 47s - loss: 6.6971e-04 - val_loss: 0.2031 - 47s/epoch - 562us/sample
Epoch 44/85
84077/84077 - 48s - loss: 6.6753e-04 - val_loss: 0.2731 - 48s/epoch - 565us/sample
Epoch 45/85
84077/84077 - 48s - loss: 6.6627e-04 - val_loss: 0.2069 - 48s/epoch - 566us/sample
Epoch 46/85
84077/84077 - 48s - loss: 6.6532e-04 - val_loss: 0.2025 - 48s/epoch - 566us/sample
Epoch 47/85
84077/84077 - 47s - loss: 6.6507e-04 - val_loss: 0.2013 - 47s/epoch - 563us/sample
Epoch 48/85
84077/84077 - 48s - loss: 6.6442e-04 - val_loss: 0.1819 - 48s/epoch - 566us/sample
Epoch 49/85
84077/84077 - 48s - loss: 6.6292e-04 - val_loss: 0.1805 - 48s/epoch - 566us/sample
Epoch 50/85
84077/84077 - 48s - loss: 6.6381e-04 - val_loss: 0.2047 - 48s/epoch - 566us/sample
Epoch 51/85
84077/84077 - 48s - loss: 6.6070e-04 - val_loss: 0.1835 - 48s/epoch - 566us/sample
Epoch 52/85
84077/84077 - 47s - loss: 6.6025e-04 - val_loss: 0.1741 - 47s/epoch - 563us/sample
Epoch 53/85
84077/84077 - 48s - loss: 6.5993e-04 - val_loss: 0.1827 - 48s/epoch - 566us/sample
Epoch 54/85
84077/84077 - 48s - loss: 6.5897e-04 - val_loss: 0.1666 - 48s/epoch - 566us/sample
Epoch 55/85
84077/84077 - 48s - loss: 6.5899e-04 - val_loss: 0.1767 - 48s/epoch - 566us/sample
Epoch 56/85
84077/84077 - 47s - loss: 6.5831e-04 - val_loss: 0.1798 - 47s/epoch - 563us/sample
Epoch 57/85
84077/84077 - 48s - loss: 6.5737e-04 - val_loss: 0.2048 - 48s/epoch - 566us/sample
Epoch 58/85
84077/84077 - 48s - loss: 6.5573e-04 - val_loss: 0.1813 - 48s/epoch - 566us/sample
Epoch 59/85
84077/84077 - 48s - loss: 6.5672e-04 - val_loss: 0.1531 - 48s/epoch - 566us/sample
Epoch 60/85
84077/84077 - 47s - loss: 6.5543e-04 - val_loss: 0.1792 - 47s/epoch - 562us/sample
Epoch 61/85
84077/84077 - 48s - loss: 6.5364e-04 - val_loss: 0.1692 - 48s/epoch - 566us/sample
Epoch 62/85
84077/84077 - 48s - loss: 6.5429e-04 - val_loss: 0.1696 - 48s/epoch - 566us/sample
Epoch 63/85
84077/84077 - 47s - loss: 6.5389e-04 - val_loss: 0.1576 - 47s/epoch - 563us/sample
Epoch 64/85
84077/84077 - 48s - loss: 6.5410e-04 - val_loss: 0.1316 - 48s/epoch - 566us/sample
Epoch 65/85
84077/84077 - 48s - loss: 6.5269e-04 - val_loss: 0.1429 - 48s/epoch - 566us/sample
Epoch 66/85
84077/84077 - 47s - loss: 6.5214e-04 - val_loss: 0.1669 - 47s/epoch - 564us/sample
Epoch 67/85
84077/84077 - 47s - loss: 6.5164e-04 - val_loss: 0.1447 - 47s/epoch - 564us/sample
Epoch 68/85
84077/84077 - 48s - loss: 6.4929e-04 - val_loss: 0.1611 - 48s/epoch - 566us/sample
Epoch 69/85
84077/84077 - 47s - loss: 6.5030e-04 - val_loss: 0.1223 - 47s/epoch - 563us/sample
Epoch 70/85
84077/84077 - 48s - loss: 6.4957e-04 - val_loss: 0.1638 - 48s/epoch - 566us/sample
Epoch 71/85
84077/84077 - 48s - loss: 6.4837e-04 - val_loss: 0.1773 - 48s/epoch - 566us/sample
Epoch 72/85
84077/84077 - 47s - loss: 6.4850e-04 - val_loss: 0.1406 - 47s/epoch - 563us/sample
Epoch 73/85
84077/84077 - 48s - loss: 6.4699e-04 - val_loss: 0.1582 - 48s/epoch - 566us/sample
Epoch 74/85
84077/84077 - 48s - loss: 6.4755e-04 - val_loss: 0.1381 - 48s/epoch - 566us/sample
Epoch 75/85
84077/84077 - 48s - loss: 6.4640e-04 - val_loss: 0.1584 - 48s/epoch - 566us/sample
Epoch 76/85
84077/84077 - 47s - loss: 6.4584e-04 - val_loss: 0.1538 - 47s/epoch - 564us/sample
Epoch 77/85
84077/84077 - 48s - loss: 6.4622e-04 - val_loss: 0.1338 - 48s/epoch - 566us/sample
Epoch 78/85
84077/84077 - 48s - loss: 6.4423e-04 - val_loss: 0.1427 - 48s/epoch - 566us/sample
Epoch 79/85
84077/84077 - 48s - loss: 6.4390e-04 - val_loss: 0.1788 - 48s/epoch - 566us/sample
Epoch 80/85
84077/84077 - 47s - loss: 6.4417e-04 - val_loss: 0.1731 - 47s/epoch - 562us/sample
Epoch 81/85
84077/84077 - 48s - loss: 6.4368e-04 - val_loss: 0.1563 - 48s/epoch - 566us/sample
Epoch 82/85
84077/84077 - 48s - loss: 6.4351e-04 - val_loss: 0.1935 - 48s/epoch - 566us/sample
Epoch 83/85
84077/84077 - 47s - loss: 6.4172e-04 - val_loss: 0.1461 - 47s/epoch - 564us/sample
Epoch 84/85
84077/84077 - 48s - loss: 6.3984e-04 - val_loss: 0.1630 - 48s/epoch - 565us/sample
Epoch 85/85
84077/84077 - 48s - loss: 6.3885e-04 - val_loss: 0.1583 - 48s/epoch - 567us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.1583349266619935
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:36:35.282645: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_44/outputlayer/BiasAdd' id:56496 op device:{requested: '', assigned: ''} def:{{{node decoder_model_44/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_44/outputlayer/MatMul, decoder_model_44/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.19544511786612692
cosine 0.19334321721386152
MAE: 0.022954641671059713
RMSE: 0.40015762549746114
r2: -124.07074833055535
RMSE zero-vector: 0.04004287452915337
['2.3000000000000003custom_VAE', 'mse', 8, 85, 0.0008, 0.3, 282, 0.0006388513401015409, 0.1583349266619935, 0.19544511786612692, 0.19334321721386152, 0.022954641671059713, 0.40015762549746114, -124.07074833055535, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.0008 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1980)         1869120     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_135 (Batch  (None, 1980)        7920        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_135 (ReLU)               (None, 1980)         0           ['batch_normalization_135[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          558642      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          558642      ['re_lu_135[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2517277     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,511,601
Trainable params: 5,503,117
Non-trainable params: 8,484
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/90
2023-02-15 10:37:05.293468: W tensorflow/c/c_api.cc:291] Operation '{name:'training_90/Adam/dense_dec1_45/bias/v/Assign' id:58404 op device:{requested: '', assigned: ''} def:{{{node training_90/Adam/dense_dec1_45/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_90/Adam/dense_dec1_45/bias/v, training_90/Adam/dense_dec1_45/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:37:27.440855: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_45/mul' id:57780 op device:{requested: '', assigned: ''} def:{{{node loss_45/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_45/mul/x, loss_45/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 34s - loss: 0.0082 - val_loss: 0.0137 - 34s/epoch - 410us/sample
Epoch 2/90
84077/84077 - 15s - loss: 0.0024 - val_loss: 0.0011 - 15s/epoch - 182us/sample
Epoch 3/90
84077/84077 - 15s - loss: 0.0011 - val_loss: 8.8975e-04 - 15s/epoch - 183us/sample
Epoch 4/90
84077/84077 - 16s - loss: 0.0015 - val_loss: 6.8354e-04 - 16s/epoch - 186us/sample
Epoch 5/90
84077/84077 - 15s - loss: 0.0011 - val_loss: 9.1418e-04 - 15s/epoch - 183us/sample
Epoch 6/90
84077/84077 - 15s - loss: 7.5947e-04 - val_loss: 5.4029e-04 - 15s/epoch - 182us/sample
Epoch 7/90
84077/84077 - 15s - loss: 5.9737e-04 - val_loss: 4.7280e-04 - 15s/epoch - 182us/sample
Epoch 8/90
84077/84077 - 15s - loss: 5.3718e-04 - val_loss: 4.3664e-04 - 15s/epoch - 183us/sample
Epoch 9/90
84077/84077 - 16s - loss: 4.4726e-04 - val_loss: 3.8416e-04 - 16s/epoch - 186us/sample
Epoch 10/90
84077/84077 - 15s - loss: 4.0445e-04 - val_loss: 3.6093e-04 - 15s/epoch - 183us/sample
Epoch 11/90
84077/84077 - 15s - loss: 3.6588e-04 - val_loss: 3.4786e-04 - 15s/epoch - 183us/sample
Epoch 12/90
84077/84077 - 15s - loss: 3.4195e-04 - val_loss: 3.3227e-04 - 15s/epoch - 183us/sample
Epoch 13/90
84077/84077 - 16s - loss: 3.2371e-04 - val_loss: 3.1742e-04 - 16s/epoch - 185us/sample
Epoch 14/90
84077/84077 - 15s - loss: 3.0624e-04 - val_loss: 3.1253e-04 - 15s/epoch - 183us/sample
Epoch 15/90
84077/84077 - 15s - loss: 2.9238e-04 - val_loss: 3.0045e-04 - 15s/epoch - 183us/sample
Epoch 16/90
84077/84077 - 15s - loss: 2.8388e-04 - val_loss: 2.9238e-04 - 15s/epoch - 183us/sample
Epoch 17/90
84077/84077 - 16s - loss: 2.7476e-04 - val_loss: 2.8926e-04 - 16s/epoch - 186us/sample
Epoch 18/90
84077/84077 - 15s - loss: 2.6754e-04 - val_loss: 2.9031e-04 - 15s/epoch - 183us/sample
Epoch 19/90
84077/84077 - 15s - loss: 2.6066e-04 - val_loss: 2.7456e-04 - 15s/epoch - 182us/sample
Epoch 20/90
84077/84077 - 15s - loss: 2.5598e-04 - val_loss: 2.7843e-04 - 15s/epoch - 183us/sample
Epoch 21/90
84077/84077 - 16s - loss: 2.5197e-04 - val_loss: 2.7099e-04 - 16s/epoch - 184us/sample
Epoch 22/90
84077/84077 - 15s - loss: 2.4757e-04 - val_loss: 2.6869e-04 - 15s/epoch - 184us/sample
Epoch 23/90
84077/84077 - 15s - loss: 2.4462e-04 - val_loss: 2.6811e-04 - 15s/epoch - 182us/sample
Epoch 24/90
84077/84077 - 15s - loss: 2.4086e-04 - val_loss: 2.6392e-04 - 15s/epoch - 182us/sample
Epoch 25/90
84077/84077 - 15s - loss: 2.3836e-04 - val_loss: 2.5701e-04 - 15s/epoch - 183us/sample
Epoch 26/90
84077/84077 - 16s - loss: 2.3608e-04 - val_loss: 2.5317e-04 - 16s/epoch - 186us/sample
Epoch 27/90
84077/84077 - 15s - loss: 2.3352e-04 - val_loss: 2.6111e-04 - 15s/epoch - 183us/sample
Epoch 28/90
84077/84077 - 15s - loss: 2.3104e-04 - val_loss: 2.5669e-04 - 15s/epoch - 182us/sample
Epoch 29/90
84077/84077 - 15s - loss: 2.2934e-04 - val_loss: 2.5941e-04 - 15s/epoch - 183us/sample
Epoch 30/90
84077/84077 - 16s - loss: 2.2733e-04 - val_loss: 2.5308e-04 - 16s/epoch - 185us/sample
Epoch 31/90
84077/84077 - 15s - loss: 2.2588e-04 - val_loss: 2.5441e-04 - 15s/epoch - 182us/sample
Epoch 32/90
84077/84077 - 15s - loss: 2.2438e-04 - val_loss: 2.4703e-04 - 15s/epoch - 182us/sample
Epoch 33/90
84077/84077 - 15s - loss: 2.2245e-04 - val_loss: 2.5727e-04 - 15s/epoch - 183us/sample
Epoch 34/90
84077/84077 - 16s - loss: 2.2181e-04 - val_loss: 2.5222e-04 - 16s/epoch - 185us/sample
Epoch 35/90
84077/84077 - 15s - loss: 2.2023e-04 - val_loss: 2.4731e-04 - 15s/epoch - 184us/sample
Epoch 36/90
84077/84077 - 15s - loss: 2.1822e-04 - val_loss: 2.5198e-04 - 15s/epoch - 182us/sample
Epoch 37/90
84077/84077 - 15s - loss: 2.1835e-04 - val_loss: 2.4751e-04 - 15s/epoch - 183us/sample
Epoch 38/90
84077/84077 - 15s - loss: 2.1694e-04 - val_loss: 2.5096e-04 - 15s/epoch - 183us/sample
Epoch 39/90
84077/84077 - 16s - loss: 2.1549e-04 - val_loss: 2.4100e-04 - 16s/epoch - 185us/sample
Epoch 40/90
84077/84077 - 15s - loss: 2.1476e-04 - val_loss: 2.4801e-04 - 15s/epoch - 182us/sample
Epoch 41/90
84077/84077 - 15s - loss: 2.1369e-04 - val_loss: 2.4246e-04 - 15s/epoch - 182us/sample
Epoch 42/90
84077/84077 - 15s - loss: 2.1314e-04 - val_loss: 2.3834e-04 - 15s/epoch - 183us/sample
Epoch 43/90
84077/84077 - 16s - loss: 2.1256e-04 - val_loss: 2.4219e-04 - 16s/epoch - 185us/sample
Epoch 44/90
84077/84077 - 15s - loss: 2.1131e-04 - val_loss: 2.3851e-04 - 15s/epoch - 183us/sample
Epoch 45/90
84077/84077 - 15s - loss: 2.1070e-04 - val_loss: 2.3365e-04 - 15s/epoch - 182us/sample
Epoch 46/90
84077/84077 - 15s - loss: 2.1010e-04 - val_loss: 2.4253e-04 - 15s/epoch - 182us/sample
Epoch 47/90
84077/84077 - 16s - loss: 2.0978e-04 - val_loss: 2.3721e-04 - 16s/epoch - 186us/sample
Epoch 48/90
84077/84077 - 15s - loss: 2.0846e-04 - val_loss: 2.3560e-04 - 15s/epoch - 183us/sample
Epoch 49/90
84077/84077 - 15s - loss: 2.0868e-04 - val_loss: 2.3195e-04 - 15s/epoch - 182us/sample
Epoch 50/90
84077/84077 - 15s - loss: 2.0759e-04 - val_loss: 2.3394e-04 - 15s/epoch - 182us/sample
Epoch 51/90
84077/84077 - 15s - loss: 2.0689e-04 - val_loss: 2.3769e-04 - 15s/epoch - 183us/sample
Epoch 52/90
84077/84077 - 16s - loss: 2.0576e-04 - val_loss: 2.3068e-04 - 16s/epoch - 184us/sample
Epoch 53/90
84077/84077 - 15s - loss: 2.0541e-04 - val_loss: 2.2903e-04 - 15s/epoch - 183us/sample
Epoch 54/90
84077/84077 - 15s - loss: 2.0475e-04 - val_loss: 2.3244e-04 - 15s/epoch - 182us/sample
Epoch 55/90
84077/84077 - 15s - loss: 2.0403e-04 - val_loss: 2.3506e-04 - 15s/epoch - 182us/sample
Epoch 56/90
84077/84077 - 16s - loss: 2.0325e-04 - val_loss: 2.2824e-04 - 16s/epoch - 184us/sample
Epoch 57/90
84077/84077 - 15s - loss: 2.0349e-04 - val_loss: 2.3118e-04 - 15s/epoch - 184us/sample
Epoch 58/90
84077/84077 - 15s - loss: 2.0226e-04 - val_loss: 2.3178e-04 - 15s/epoch - 182us/sample
Epoch 59/90
84077/84077 - 15s - loss: 2.0254e-04 - val_loss: 2.2512e-04 - 15s/epoch - 182us/sample
Epoch 60/90
84077/84077 - 15s - loss: 2.0141e-04 - val_loss: 2.2947e-04 - 15s/epoch - 182us/sample
Epoch 61/90
84077/84077 - 16s - loss: 2.0119e-04 - val_loss: 2.2783e-04 - 16s/epoch - 186us/sample
Epoch 62/90
84077/84077 - 15s - loss: 2.0049e-04 - val_loss: 2.2912e-04 - 15s/epoch - 182us/sample
Epoch 63/90
84077/84077 - 15s - loss: 2.0021e-04 - val_loss: 2.1959e-04 - 15s/epoch - 182us/sample
Epoch 64/90
84077/84077 - 15s - loss: 1.9962e-04 - val_loss: 2.2868e-04 - 15s/epoch - 182us/sample
Epoch 65/90
84077/84077 - 16s - loss: 1.9963e-04 - val_loss: 2.2348e-04 - 16s/epoch - 185us/sample
Epoch 66/90
84077/84077 - 15s - loss: 1.9903e-04 - val_loss: 2.1764e-04 - 15s/epoch - 183us/sample
Epoch 67/90
84077/84077 - 15s - loss: 1.9856e-04 - val_loss: 2.3316e-04 - 15s/epoch - 182us/sample
Epoch 68/90
84077/84077 - 15s - loss: 1.9844e-04 - val_loss: 2.2581e-04 - 15s/epoch - 183us/sample
Epoch 69/90
84077/84077 - 15s - loss: 1.9762e-04 - val_loss: 2.1900e-04 - 15s/epoch - 183us/sample
Epoch 70/90
84077/84077 - 16s - loss: 1.9743e-04 - val_loss: 2.1809e-04 - 16s/epoch - 185us/sample
Epoch 71/90
84077/84077 - 15s - loss: 1.9704e-04 - val_loss: 2.2219e-04 - 15s/epoch - 182us/sample
Epoch 72/90
84077/84077 - 15s - loss: 1.9690e-04 - val_loss: 2.2904e-04 - 15s/epoch - 182us/sample
Epoch 73/90
84077/84077 - 15s - loss: 1.9658e-04 - val_loss: 2.2465e-04 - 15s/epoch - 183us/sample
Epoch 74/90
84077/84077 - 16s - loss: 1.9544e-04 - val_loss: 2.2107e-04 - 16s/epoch - 185us/sample
Epoch 75/90
84077/84077 - 15s - loss: 1.9566e-04 - val_loss: 2.2469e-04 - 15s/epoch - 183us/sample
Epoch 76/90
84077/84077 - 15s - loss: 1.9527e-04 - val_loss: 2.2336e-04 - 15s/epoch - 182us/sample
Epoch 77/90
84077/84077 - 15s - loss: 1.9497e-04 - val_loss: 2.1464e-04 - 15s/epoch - 183us/sample
Epoch 78/90
84077/84077 - 16s - loss: 1.9440e-04 - val_loss: 2.2034e-04 - 16s/epoch - 186us/sample
Epoch 79/90
84077/84077 - 15s - loss: 1.9435e-04 - val_loss: 2.1890e-04 - 15s/epoch - 183us/sample
Epoch 80/90
84077/84077 - 15s - loss: 1.9442e-04 - val_loss: 2.2190e-04 - 15s/epoch - 182us/sample
Epoch 81/90
84077/84077 - 15s - loss: 1.9307e-04 - val_loss: 2.1906e-04 - 15s/epoch - 182us/sample
Epoch 82/90
84077/84077 - 16s - loss: 1.9358e-04 - val_loss: 2.2079e-04 - 16s/epoch - 184us/sample
Epoch 83/90
84077/84077 - 15s - loss: 1.9301e-04 - val_loss: 2.1351e-04 - 15s/epoch - 184us/sample
Epoch 84/90
84077/84077 - 15s - loss: 1.9296e-04 - val_loss: 2.2045e-04 - 15s/epoch - 182us/sample
Epoch 85/90
84077/84077 - 15s - loss: 1.9267e-04 - val_loss: 2.1235e-04 - 15s/epoch - 182us/sample
Epoch 86/90
84077/84077 - 15s - loss: 1.9208e-04 - val_loss: 2.2216e-04 - 15s/epoch - 183us/sample
Epoch 87/90
84077/84077 - 16s - loss: 1.9150e-04 - val_loss: 2.1820e-04 - 16s/epoch - 186us/sample
Epoch 88/90
84077/84077 - 15s - loss: 1.9121e-04 - val_loss: 2.1732e-04 - 15s/epoch - 183us/sample
Epoch 89/90
84077/84077 - 15s - loss: 1.9136e-04 - val_loss: 2.1885e-04 - 15s/epoch - 182us/sample
Epoch 90/90
84077/84077 - 15s - loss: 1.9099e-04 - val_loss: 2.1650e-04 - 15s/epoch - 183us/sample
COMPRESSED VECTOR SIZE: 282
Loss in the autoencoder: 0.0002165028611351095
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 11:00:24.956547: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_45/outputlayer/BiasAdd' id:57751 op device:{requested: '', assigned: ''} def:{{{node decoder_model_45/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_45/outputlayer/MatMul, decoder_model_45/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.025568908505385932
cosine 0.02526619703990495
MAE: 0.002254812941724762
RMSE: 0.011589952417622595
r2: 0.8952405283722396
RMSE zero-vector: 0.04004287452915337
['2.1custom_VAE', 'mse', 32, 90, 0.0008, 0.3, 282, 0.0001909879809623429, 0.0002165028611351095, 0.025568908505385932, 0.02526619703990495, 0.002254812941724762, 0.011589952417622595, 0.8952405283722396, 0.04004287452915337] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 85 0.0012000000000000001 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (93419, 943)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 943)]        0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2074)         1957856     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_138 (Batch  (None, 2074)        8296        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_138 (ReLU)               (None, 2074)         0           ['batch_normalization_138[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 282)          585150      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 282)          585150      ['re_lu_138[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 282)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 943)          2632897     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,769,349
Trainable params: 5,760,489
Non-trainable params: 8,860
__________________________________________________________________________________________________
Train on 84077 samples, validate on 9342 samples
Epoch 1/85
2023-02-15 11:00:55.541183: W tensorflow/c/c_api.cc:291] Operation '{name:'training_92/Adam/bottleneck_zmean_46/bias/v/Assign' id:59665 op device:{requested: '', assigned: ''} def:{{{node training_92/Adam/bottleneck_zmean_46/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_92/Adam/bottleneck_zmean_46/bias/v, training_92/Adam/bottleneck_zmean_46/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 11:01:14.050691: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_46/mul' id:59042 op device:{requested: '', assigned: ''} def:{{{node loss_46/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_46/mul/x, loss_46/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
84077/84077 - 31s - loss: 0.0092 - val_loss: 0.0015 - 31s/epoch - 371us/sample
Epoch 2/85
84077/84077 - 12s - loss: 0.0011 - val_loss: 9.6855e-04 - 12s/epoch - 138us/sample
Epoch 3/85
84077/84077 - 11s - loss: 0.0012 - val_loss: 8.4973e-04 - 11s/epoch - 135us/sample
Epoch 4/85
84077/84077 - 11s - loss: 0.0011 - val_loss: 6.7842e-04 - 11s/epoch - 134us/sample
Epoch 5/85
84077/84077 - 11s - loss: 5.8324e-04 - val_loss: 5.4702e-04 - 11s/epoch - 134us/sample
Epoch 6/85
84077/84077 - 11s - loss: 4.9267e-04 - val_loss: 5.2815e-04 - 11s/epoch - 135us/sample
Epoch 7/85
84077/84077 - 11s - loss: 4.1978e-04 - val_loss: 3.7576e-04 - 11s/epoch - 135us/sample
Epoch 8/85
84077/84077 - 11s - loss: 3.4295e-04 - val_loss: 3.1050e-04 - 11s/epoch - 137us/sample
Epoch 9/85
84077/84077 - 11s - loss: 3.0849e-04 - val_loss: 2.4996e-04 - 11s/epoch - 137us/sample
Epoch 10/85
84077/84077 - 11s - loss: 2.7150e-04 - val_loss: 2.3248e-04 - 11s/epoch - 136us/sample
Epoch 11/85
84077/84077 - 11s - loss: 2.4135e-04 - val_loss: 2.1420e-04 - 11s/epoch - 135us/sample
Epoch 12/85
84077/84077 - 11s - loss: 2.2462e-04 - val_loss: 1.9379e-04 - 11s/epoch - 135us/sample
Epoch 13/85
84077/84077 - 12s - loss: 2.0035e-04 - val_loss: 1.8149e-04 - 12s/epoch - 137us/sample
Epoch 14/85
84077/84077 - 12s - loss: 1.8710e-04 - val_loss: 1.6300e-04 - 12s/epoch - 137us/sample
Epoch 15/85
84077/84077 - 11s - loss: 1.7680e-04 - val_loss: 1.5258e-04 - 11s/epoch - 135us/sample
Epoch 16/85
84077/84077 - 11s - loss: 1.6644e-04 - val_loss: 1.4610e-04 - 11s/epoch - 135us/sample
Epoch 17/85
84077/84077 - 11s - loss: 1.6002e-04 - val_loss: 1.4082e-04 - 11s/epoch - 136us/sample
Epoch 18/85
84077/84077 - 12s - loss: 1.5455e-04 - val_loss: 1.3564e-04 - 12s/epoch - 139us/sample
Epoch 19/85
84077/84077 - 11s - loss: 1.4979e-04 - val_loss: 1.3131e-04 - 11s/epoch - 136us/sample
Epoch 20/85
84077/84077 - 11s - loss: 1.4594e-04 - val_loss: 1.2950e-04 - 11s/epoch - 135us/sample
Epoch 21/85
84077/84077 - 11s - loss: 1.4252e-04 - val_loss: 1.2601e-04 - 11s/epoch - 136us/sample
Epoch 22/85
84077/84077 - 12s - loss: 1.3966e-04 - val_loss: 1.2380e-04 - 12s/epoch - 138us/sample
Epoch 23/85
84077/84077 - 11s - loss: 1.3728e-04 - val_loss: 1.2176e-04 - 11s/epoch - 137us/sample
Epoch 24/85
84077/84077 - 11s - loss: 1.3495e-04 - val_loss: 1.2227e-04 - 11s/epoch - 135us/sample
Epoch 25/85
84077/84077 - 11s - loss: 1.3389e-04 - val_loss: 1.2081e-04 - 11s/epoch - 136us/sample
Epoch 26/85
84077/84077 - 11s - loss: 1.3194e-04 - val_loss: 1.1742e-04 - 11s/epoch - 136us/sample
Epoch 27/85
84077/84077 - 12s - loss: 1.3036e-04 - val_loss: 1.1758e-04 - 12s/epoch - 138us/sample
Epoch 28/85
slurmstepd: error: *** JOB 36005771 ON mb-rom102 CANCELLED AT 2023-02-15T11:06:21 ***
