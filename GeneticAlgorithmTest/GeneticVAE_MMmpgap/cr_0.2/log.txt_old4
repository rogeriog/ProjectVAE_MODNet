start
Fri Feb 17 18:24:35 CET 2023
2023-02-17 18:24:36.667250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-17 18:24:36.778482: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-17 18:25:05,353 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fdcf0b64d90> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 150 0.002 256 1]
 [1.5 120 0.002 256 2]
 [1.5 60 0.0005 256 1]
 [2.5 90 0.002 64 2]
 [2.0 30 0.0005 128 1]
 [2.5 150 0.001 256 2]
 [1.5 30 0.0005 64 2]
 [2.0 210 0.001 256 1]
 [1.0 180 0.001 128 2]
 [2.5 30 0.002 256 1]]
[2.0 150 0.002 256 1] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-17 18:25:07.603888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-17 18:25:07.974624: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-17 18:25:07.974737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20633 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:a2:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/150
2023-02-17 18:25:10.678412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-02-17 18:25:10.699919: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55595532f370 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-17 18:25:10.699938: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6
2023-02-17 18:25:10.704590: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-17 18:25:10.816697: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
374/374 - 4s - loss: 0.0387 - val_loss: 0.0265 - 4s/epoch - 12ms/step
Epoch 2/150
374/374 - 1s - loss: 0.0166 - val_loss: 0.0221 - 1s/epoch - 4ms/step
Epoch 3/150
374/374 - 1s - loss: 0.0153 - val_loss: 0.0193 - 1s/epoch - 4ms/step
Epoch 4/150
374/374 - 1s - loss: 0.0144 - val_loss: 0.0197 - 1s/epoch - 4ms/step
Epoch 5/150
374/374 - 1s - loss: 0.0141 - val_loss: 0.0192 - 1s/epoch - 4ms/step
Epoch 6/150
374/374 - 1s - loss: 0.0137 - val_loss: 0.0316 - 1s/epoch - 4ms/step
Epoch 7/150
374/374 - 1s - loss: 0.0136 - val_loss: 0.0148 - 1s/epoch - 4ms/step
Epoch 8/150
374/374 - 1s - loss: 0.0127 - val_loss: 0.0894 - 1s/epoch - 4ms/step
Epoch 9/150
374/374 - 1s - loss: 0.0138 - val_loss: 0.0193 - 1s/epoch - 4ms/step
Epoch 10/150
374/374 - 1s - loss: 0.0124 - val_loss: 0.0177 - 1s/epoch - 4ms/step
Epoch 11/150
374/374 - 1s - loss: 0.0124 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 12/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 13/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0190 - 1s/epoch - 4ms/step
Epoch 14/150
374/374 - 1s - loss: 0.0122 - val_loss: 0.0132 - 1s/epoch - 4ms/step
Epoch 15/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 16/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 17/150
374/374 - 1s - loss: 0.0131 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 18/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 19/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0145 - 1s/epoch - 4ms/step
Epoch 20/150
374/374 - 1s - loss: 0.0151 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 21/150
374/374 - 1s - loss: 0.0161 - val_loss: 0.0145 - 1s/epoch - 4ms/step
Epoch 22/150
374/374 - 1s - loss: 0.0227 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 23/150
374/374 - 1s - loss: 0.0120 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 24/150
374/374 - 1s - loss: 0.0122 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 25/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 26/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 27/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 28/150
374/374 - 1s - loss: 0.0120 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 29/150
374/374 - 1s - loss: 0.0144 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 30/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 31/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 32/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 33/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 34/150
374/374 - 1s - loss: 0.0176 - val_loss: 0.0192 - 1s/epoch - 4ms/step
Epoch 35/150
374/374 - 1s - loss: 0.0340 - val_loss: 0.0143 - 1s/epoch - 4ms/step
Epoch 36/150
374/374 - 1s - loss: 0.0123 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 37/150
374/374 - 1s - loss: 0.0125 - val_loss: 0.0159 - 1s/epoch - 4ms/step
Epoch 38/150
374/374 - 1s - loss: 0.0141 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 39/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 40/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 41/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 42/150
374/374 - 1s - loss: 0.0134 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 43/150
374/374 - 1s - loss: 0.0120 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 44/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 45/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 46/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 47/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0174 - 1s/epoch - 4ms/step
Epoch 48/150
374/374 - 1s - loss: 0.0209 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 49/150
374/374 - 1s - loss: 0.0145 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 50/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 51/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 52/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 53/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 54/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0135 - 1s/epoch - 4ms/step
Epoch 55/150
374/374 - 1s - loss: 0.0178 - val_loss: 0.0149 - 1s/epoch - 4ms/step
Epoch 56/150
374/374 - 1s - loss: 0.0288 - val_loss: 0.0133 - 1s/epoch - 4ms/step
Epoch 57/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 58/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 59/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 60/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0223 - 1s/epoch - 4ms/step
Epoch 61/150
374/374 - 1s - loss: 0.0387 - val_loss: 0.0143 - 1s/epoch - 4ms/step
Epoch 62/150
374/374 - 1s - loss: 0.0122 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 63/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 64/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 65/150
374/374 - 1s - loss: 0.0128 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 66/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 67/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 68/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 69/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 70/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 71/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 72/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 73/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 74/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 75/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 76/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 77/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 78/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 79/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 80/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 81/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 82/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0232 - 1s/epoch - 4ms/step
Epoch 83/150
374/374 - 1s - loss: 0.0117 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 84/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 85/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 86/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 87/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 88/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 89/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 90/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 91/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 92/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 93/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 94/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 95/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 96/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 97/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0139 - 1s/epoch - 4ms/step
Epoch 98/150
374/374 - 1s - loss: 0.0121 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 99/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 100/150
374/374 - 1s - loss: 0.0156 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 101/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 102/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 103/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 104/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 105/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 106/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 107/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 108/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 109/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 110/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 111/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 112/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 113/150
374/374 - 1s - loss: 0.0187 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 114/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 115/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 116/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 117/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 118/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 119/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0147 - 1s/epoch - 4ms/step
Epoch 120/150
374/374 - 1s - loss: 0.0191 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 121/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 122/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 123/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 124/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 125/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 126/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 127/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 128/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 129/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 130/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 131/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 132/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 133/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 134/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 135/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 136/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 137/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 138/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 139/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 140/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 141/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 142/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 143/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 144/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0219 - 1s/epoch - 4ms/step
Epoch 145/150
374/374 - 1s - loss: 0.0255 - val_loss: 0.0126 - 1s/epoch - 4ms/step
Epoch 146/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 147/150
374/374 - 1s - loss: 0.0117 - val_loss: 0.0144 - 1s/epoch - 4ms/step
Epoch 148/150
374/374 - 1s - loss: 0.0391 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 149/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 150/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0109 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.010935111902654171
  1/332 [..............................] - ETA: 45s 45/332 [===>..........................] - ETA: 0s  88/332 [======>.......................] - ETA: 0s131/332 [==========>...................] - ETA: 0s174/332 [==============>...............] - ETA: 0s217/332 [==================>...........] - ETA: 0s261/332 [======================>.......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08262621152189782
cosine 0.06476983517868942
MAE: 0.037764255
RMSE: 0.082431175
r2: 0.559198646985889
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 256, 150, 0.002, 0.2, 252, 0.010955142788589, 0.010935111902654171, 0.08262621152189782, 0.06476983517868942, 0.037764254957437515, 0.08243117481470108, 0.559198646985889, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 120 0.002 256 2] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/120
374/374 - 4s - loss: 0.0185 - val_loss: 0.0243 - 4s/epoch - 10ms/step
Epoch 2/120
374/374 - 1s - loss: 0.0085 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 3/120
374/374 - 1s - loss: 0.0080 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 4/120
374/374 - 1s - loss: 0.0077 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 5/120
374/374 - 1s - loss: 0.0076 - val_loss: 0.0085 - 1s/epoch - 4ms/step
Epoch 6/120
374/374 - 1s - loss: 0.0075 - val_loss: 0.0084 - 1s/epoch - 4ms/step
Epoch 7/120
374/374 - 1s - loss: 0.0074 - val_loss: 0.0078 - 1s/epoch - 4ms/step
Epoch 8/120
374/374 - 1s - loss: 0.0073 - val_loss: 0.0142 - 1s/epoch - 4ms/step
Epoch 9/120
374/374 - 1s - loss: 0.0073 - val_loss: 0.0075 - 1s/epoch - 4ms/step
Epoch 10/120
374/374 - 1s - loss: 0.0070 - val_loss: 0.0073 - 1s/epoch - 4ms/step
Epoch 11/120
374/374 - 1s - loss: 0.0070 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 12/120
374/374 - 1s - loss: 0.0069 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 13/120
374/374 - 1s - loss: 0.0069 - val_loss: 0.0072 - 1s/epoch - 4ms/step
Epoch 14/120
374/374 - 1s - loss: 0.0068 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 15/120
374/374 - 1s - loss: 0.0068 - val_loss: 0.0073 - 1s/epoch - 4ms/step
Epoch 16/120
374/374 - 1s - loss: 0.0068 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 17/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 18/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 19/120
374/374 - 1s - loss: 0.0066 - val_loss: 0.0073 - 1s/epoch - 4ms/step
Epoch 20/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 21/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 22/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 23/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 24/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 25/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 26/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 27/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 28/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 29/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 30/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 31/120
374/374 - 1s - loss: 0.0069 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 32/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 33/120
374/374 - 1s - loss: 0.0085 - val_loss: 0.0076 - 1s/epoch - 4ms/step
Epoch 34/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 35/120
374/374 - 1s - loss: 0.0070 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 36/120
374/374 - 1s - loss: 0.0076 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 37/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 38/120
374/374 - 1s - loss: 0.0079 - val_loss: 0.0081 - 1s/epoch - 4ms/step
Epoch 39/120
374/374 - 1s - loss: 0.0083 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 40/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 41/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 42/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 43/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 44/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 45/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 46/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 47/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 48/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 49/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 50/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0080 - 1s/epoch - 4ms/step
Epoch 51/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 52/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 53/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0073 - 1s/epoch - 4ms/step
Epoch 54/120
374/374 - 1s - loss: 0.0079 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 55/120
374/374 - 1s - loss: 0.0076 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 56/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 57/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 58/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 59/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 60/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 61/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 62/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 63/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 64/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 65/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 66/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0082 - 1s/epoch - 4ms/step
Epoch 67/120
374/374 - 1s - loss: 0.0072 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 68/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 69/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 70/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 71/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 72/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 73/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 74/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 75/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 76/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 77/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 78/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 79/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 80/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 81/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 82/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 83/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 84/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 85/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 86/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 87/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 88/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 89/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 90/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 91/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 92/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 93/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 94/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 95/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 96/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 97/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 98/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 99/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 100/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 101/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 102/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 103/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 104/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 105/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 106/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 107/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 108/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 109/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 110/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 111/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 112/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 113/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 114/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 115/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 116/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 117/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 118/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 119/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 120/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00588339613750577
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.11431125596676779
cosine 0.0894841182088115
MAE: 0.04455008
RMSE: 0.096266285
r2: 0.3988147457359235
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 256, 120, 0.002, 0.2, 252, 0.005923296790570021, 0.00588339613750577, 0.11431125596676779, 0.0894841182088115, 0.044550079852342606, 0.09626628458499908, 0.3988147457359235, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 60 0.0005 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.5_cr0.2_bs256_ep60_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 35s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.07974397015041376
cosine 0.06252585863712101
MAE: 0.038352575
RMSE: 0.081144325
r2: 0.5728557841522939
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.5custom_VAE', 'mse', 256, 60, 0.0005, 0.2, 252, '--', '--', 0.07974397015041376, 0.06252585863712101, 0.03835257515311241, 0.08114432543516159, 0.5728557841522939, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 90 0.002 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.5_cr0.2_bs64_ep90_loss_logcosh_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 33s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.12176138260905535
cosine 0.09528264943110103
MAE: 0.046112616
RMSE: 0.099352874
r2: 0.35964485055882417
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.5custom_VAE', 'logcosh', 64, 90, 0.002, 0.2, 252, '--', '--', 0.12176138260905535, 0.09528264943110103, 0.04611261561512947, 0.0993528738617897, 0.35964485055882417, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 30 0.0005 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/30
747/747 - 5s - loss: 0.0301 - val_loss: 0.0196 - 5s/epoch - 7ms/step
Epoch 2/30
747/747 - 3s - loss: 0.0157 - val_loss: 0.0163 - 3s/epoch - 4ms/step
Epoch 3/30
747/747 - 3s - loss: 0.0144 - val_loss: 0.0138 - 3s/epoch - 4ms/step
Epoch 4/30
747/747 - 3s - loss: 0.0131 - val_loss: 0.0144 - 3s/epoch - 4ms/step
Epoch 5/30
747/747 - 3s - loss: 0.0124 - val_loss: 0.0463 - 3s/epoch - 4ms/step
Epoch 6/30
747/747 - 3s - loss: 0.0129 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 7/30
747/747 - 3s - loss: 0.0116 - val_loss: 0.0128 - 3s/epoch - 4ms/step
Epoch 8/30
747/747 - 3s - loss: 0.0117 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 9/30
747/747 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 10/30
747/747 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 11/30
747/747 - 3s - loss: 0.0112 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 12/30
747/747 - 3s - loss: 0.0110 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 13/30
747/747 - 3s - loss: 0.0109 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 14/30
747/747 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 15/30
747/747 - 3s - loss: 0.0107 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 16/30
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 17/30
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 18/30
747/747 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 19/30
747/747 - 3s - loss: 0.0103 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 20/30
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 21/30
747/747 - 3s - loss: 0.0107 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 22/30
747/747 - 3s - loss: 0.0109 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 23/30
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 24/30
747/747 - 3s - loss: 0.0107 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 25/30
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 26/30
747/747 - 3s - loss: 0.0102 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 27/30
747/747 - 3s - loss: 0.0102 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 28/30
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 29/30
747/747 - 3s - loss: 0.0103 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 30/30
747/747 - 3s - loss: 0.0103 - val_loss: 0.0099 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00994982197880745
  1/332 [..............................] - ETA: 34s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s173/332 [==============>...............] - ETA: 0s216/332 [==================>...........] - ETA: 0s259/332 [======================>.......] - ETA: 0s302/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.08589917277552882
cosine 0.06736844569118887
MAE: 0.039602824
RMSE: 0.08399621
r2: 0.5423036667421159
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 128, 30, 0.0005, 0.2, 252, 0.010338075459003448, 0.00994982197880745, 0.08589917277552882, 0.06736844569118887, 0.03960282355546951, 0.08399620652198792, 0.5423036667421159, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 150 0.001 256 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0287 - val_loss: 0.0448 - 4s/epoch - 10ms/step
Epoch 2/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0122 - 2s/epoch - 4ms/step
Epoch 3/150
374/374 - 2s - loss: 0.0088 - val_loss: 0.0133 - 2s/epoch - 4ms/step
Epoch 4/150
374/374 - 2s - loss: 0.0086 - val_loss: 0.0135 - 2s/epoch - 4ms/step
Epoch 5/150
374/374 - 2s - loss: 0.0081 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 6/150
374/374 - 2s - loss: 0.0114 - val_loss: 15.9372 - 2s/epoch - 4ms/step
Epoch 7/150
374/374 - 2s - loss: 0.0073 - val_loss: 0.0268 - 2s/epoch - 4ms/step
Epoch 8/150
374/374 - 2s - loss: 0.0073 - val_loss: 0.0115 - 2s/epoch - 4ms/step
Epoch 9/150
374/374 - 2s - loss: 0.0072 - val_loss: 0.0073 - 2s/epoch - 4ms/step
Epoch 10/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0072 - 2s/epoch - 4ms/step
Epoch 11/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0212 - 2s/epoch - 4ms/step
Epoch 12/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0076 - 2s/epoch - 4ms/step
Epoch 13/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0329 - 2s/epoch - 4ms/step
Epoch 14/150
374/374 - 2s - loss: 0.0077 - val_loss: 0.0066 - 2s/epoch - 4ms/step
Epoch 15/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0087 - 2s/epoch - 4ms/step
Epoch 16/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0074 - 2s/epoch - 4ms/step
Epoch 17/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0095 - 2s/epoch - 4ms/step
Epoch 18/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0069 - 2s/epoch - 4ms/step
Epoch 19/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0206 - 2s/epoch - 4ms/step
Epoch 20/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0369 - 2s/epoch - 4ms/step
Epoch 21/150
374/374 - 2s - loss: 0.0090 - val_loss: 0.0642 - 2s/epoch - 4ms/step
Epoch 22/150
374/374 - 2s - loss: 0.0143 - val_loss: 0.0448 - 2s/epoch - 4ms/step
Epoch 23/150
374/374 - 2s - loss: 0.0144 - val_loss: 0.0081 - 2s/epoch - 4ms/step
Epoch 24/150
374/374 - 2s - loss: 0.0120 - val_loss: 0.0893 - 2s/epoch - 4ms/step
Epoch 25/150
374/374 - 2s - loss: 0.0081 - val_loss: 0.0842 - 2s/epoch - 4ms/step
Epoch 26/150
374/374 - 2s - loss: 0.0522 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 27/150
374/374 - 2s - loss: 0.0076 - val_loss: 0.0071 - 2s/epoch - 4ms/step
Epoch 28/150
374/374 - 2s - loss: 0.0071 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 29/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 30/150
374/374 - 2s - loss: 0.0069 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 31/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 32/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0265 - 2s/epoch - 4ms/step
Epoch 33/150
374/374 - 2s - loss: 0.0151 - val_loss: 0.0069 - 2s/epoch - 4ms/step
Epoch 34/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0547 - 2s/epoch - 4ms/step
Epoch 35/150
374/374 - 2s - loss: 0.0328 - val_loss: 0.0088 - 2s/epoch - 4ms/step
Epoch 36/150
374/374 - 2s - loss: 0.0285 - val_loss: 0.0419 - 2s/epoch - 4ms/step
Epoch 37/150
374/374 - 2s - loss: 0.0488 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 38/150
374/374 - 2s - loss: 0.0081 - val_loss: 0.0078 - 2s/epoch - 4ms/step
Epoch 39/150
374/374 - 2s - loss: 0.0076 - val_loss: 0.0074 - 2s/epoch - 4ms/step
Epoch 40/150
374/374 - 2s - loss: 0.0073 - val_loss: 0.0073 - 2s/epoch - 4ms/step
Epoch 41/150
374/374 - 2s - loss: 0.0072 - val_loss: 0.0071 - 2s/epoch - 4ms/step
Epoch 42/150
374/374 - 2s - loss: 0.0070 - val_loss: 0.0070 - 2s/epoch - 4ms/step
Epoch 43/150
374/374 - 2s - loss: 0.0081 - val_loss: 0.0121 - 2s/epoch - 4ms/step
Epoch 44/150
374/374 - 2s - loss: 0.0071 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 45/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 46/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 47/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 48/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 4ms/step
Epoch 49/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 50/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.1582 - 2s/epoch - 4ms/step
Epoch 51/150
374/374 - 2s - loss: 0.0790 - val_loss: 0.0153 - 2s/epoch - 4ms/step
Epoch 52/150
374/374 - 2s - loss: 0.0082 - val_loss: 0.0076 - 2s/epoch - 4ms/step
Epoch 53/150
374/374 - 2s - loss: 0.0075 - val_loss: 0.0072 - 2s/epoch - 4ms/step
Epoch 54/150
374/374 - 2s - loss: 0.0072 - val_loss: 0.0071 - 2s/epoch - 4ms/step
Epoch 55/150
374/374 - 2s - loss: 0.0070 - val_loss: 0.0069 - 2s/epoch - 4ms/step
Epoch 56/150
374/374 - 2s - loss: 0.0069 - val_loss: 0.0076 - 2s/epoch - 4ms/step
Epoch 57/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 58/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 59/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0671 - 2s/epoch - 4ms/step
Epoch 60/150
374/374 - 2s - loss: 0.0564 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 61/150
374/374 - 2s - loss: 0.0082 - val_loss: 0.0076 - 2s/epoch - 4ms/step
Epoch 62/150
374/374 - 2s - loss: 0.0073 - val_loss: 0.0070 - 2s/epoch - 4ms/step
Epoch 63/150
374/374 - 2s - loss: 0.0070 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 64/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 65/150
374/374 - 2s - loss: 0.0069 - val_loss: 0.0287 - 2s/epoch - 4ms/step
Epoch 66/150
374/374 - 2s - loss: 0.0299 - val_loss: 0.0080 - 2s/epoch - 4ms/step
Epoch 67/150
374/374 - 2s - loss: 0.0075 - val_loss: 0.0072 - 2s/epoch - 4ms/step
Epoch 68/150
374/374 - 2s - loss: 0.0073 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 69/150
374/374 - 2s - loss: 0.0093 - val_loss: 0.0070 - 2s/epoch - 4ms/step
Epoch 70/150
374/374 - 2s - loss: 0.0069 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 71/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 72/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0151 - 2s/epoch - 4ms/step
Epoch 73/150
374/374 - 2s - loss: 0.0125 - val_loss: 0.0069 - 2s/epoch - 4ms/step
Epoch 74/150
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 75/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 4ms/step
Epoch 76/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 77/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 78/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 79/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 80/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 81/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 82/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 83/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 84/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 85/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0075 - 2s/epoch - 4ms/step
Epoch 86/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 87/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 88/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 89/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 90/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 91/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 92/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0788 - 2s/epoch - 4ms/step
Epoch 93/150
374/374 - 2s - loss: 0.0456 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 94/150
374/374 - 2s - loss: 0.0071 - val_loss: 0.0068 - 2s/epoch - 4ms/step
Epoch 95/150
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 4ms/step
Epoch 96/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 97/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 98/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 99/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0066 - 2s/epoch - 4ms/step
Epoch 100/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 101/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 102/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 103/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0296 - 2s/epoch - 4ms/step
Epoch 104/150
374/374 - 2s - loss: 0.0135 - val_loss: 0.0075 - 2s/epoch - 4ms/step
Epoch 105/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 106/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 107/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 108/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 109/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 110/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 111/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 112/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 113/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 114/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 115/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 116/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 117/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 118/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 119/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 4ms/step
Epoch 120/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 121/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 122/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0089 - 2s/epoch - 4ms/step
Epoch 123/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 124/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 125/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 126/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 127/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0066 - 2s/epoch - 4ms/step
Epoch 128/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 129/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 4ms/step
Epoch 130/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0290 - 2s/epoch - 4ms/step
Epoch 131/150
374/374 - 2s - loss: 0.0149 - val_loss: 0.0066 - 2s/epoch - 4ms/step
Epoch 132/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0177 - 2s/epoch - 4ms/step
Epoch 133/150
374/374 - 2s - loss: 0.0158 - val_loss: 0.0064 - 2s/epoch - 4ms/step
Epoch 134/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 135/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 136/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 137/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 138/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0307 - 2s/epoch - 4ms/step
Epoch 139/150
374/374 - 2s - loss: 0.0197 - val_loss: 0.0066 - 2s/epoch - 4ms/step
Epoch 140/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 141/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 4ms/step
Epoch 142/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 143/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 144/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 145/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0065 - 2s/epoch - 4ms/step
Epoch 146/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0061 - 2s/epoch - 4ms/step
Epoch 147/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 4ms/step
Epoch 148/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 4ms/step
Epoch 149/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 4ms/step
Epoch 150/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0061098807491362095
  1/332 [..............................] - ETA: 40s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12308475159493384
cosine 0.09637219183318263
MAE: 0.04640082
RMSE: 0.099682875
r2: 0.35538380381249884
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 256, 150, 0.001, 0.2, 252, 0.006108190398663282, 0.0061098807491362095, 0.12308475159493384, 0.09637219183318263, 0.04640081897377968, 0.09968287497758865, 0.35538380381249884, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/30
1493/1493 - 7s - loss: 0.0128 - val_loss: 0.0093 - 7s/epoch - 5ms/step
Epoch 2/30
1493/1493 - 5s - loss: 0.0077 - val_loss: 0.0074 - 5s/epoch - 3ms/step
Epoch 3/30
1493/1493 - 5s - loss: 0.0071 - val_loss: 0.0069 - 5s/epoch - 3ms/step
Epoch 4/30
1493/1493 - 5s - loss: 0.0066 - val_loss: 0.0067 - 5s/epoch - 3ms/step
Epoch 5/30
1493/1493 - 5s - loss: 0.0064 - val_loss: 0.0063 - 5s/epoch - 3ms/step
Epoch 6/30
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 7/30
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 8/30
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 9/30
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 10/30
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 11/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 12/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 13/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 14/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 15/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 16/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 17/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 18/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 19/30
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 20/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 21/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 22/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 23/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 24/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 25/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 26/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 27/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 28/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 29/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 30/30
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.006028133910149336
  1/332 [..............................] - ETA: 35s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s256/332 [======================>.......] - ETA: 0s299/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.1238222331022075
cosine 0.09689789236468097
MAE: 0.04659531
RMSE: 0.10005339
r2: 0.3505828283877862
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 64, 30, 0.0005, 0.2, 252, 0.00608805101364851, 0.006028133910149336, 0.1238222331022075, 0.09689789236468097, 0.04659530892968178, 0.10005339235067368, 0.3505828283877862, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 210 0.001 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/210
374/374 - 4s - loss: 0.0385 - val_loss: 0.0193 - 4s/epoch - 10ms/step
Epoch 2/210
374/374 - 1s - loss: 0.0160 - val_loss: 0.0191 - 1s/epoch - 4ms/step
Epoch 3/210
374/374 - 1s - loss: 0.0153 - val_loss: 0.0178 - 1s/epoch - 4ms/step
Epoch 4/210
374/374 - 1s - loss: 0.0143 - val_loss: 0.0199 - 1s/epoch - 4ms/step
Epoch 5/210
374/374 - 1s - loss: 0.0140 - val_loss: 0.0148 - 1s/epoch - 4ms/step
Epoch 6/210
374/374 - 1s - loss: 0.0135 - val_loss: 0.0260 - 1s/epoch - 4ms/step
Epoch 7/210
374/374 - 1s - loss: 0.0133 - val_loss: 0.0162 - 1s/epoch - 4ms/step
Epoch 8/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0138 - 1s/epoch - 4ms/step
Epoch 9/210
374/374 - 1s - loss: 0.0123 - val_loss: 0.0242 - 1s/epoch - 4ms/step
Epoch 10/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.2331 - 1s/epoch - 4ms/step
Epoch 11/210
374/374 - 1s - loss: 0.0195 - val_loss: 0.0158 - 1s/epoch - 4ms/step
Epoch 12/210
374/374 - 1s - loss: 0.0124 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 13/210
374/374 - 1s - loss: 0.0116 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 14/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 15/210
374/374 - 1s - loss: 0.0118 - val_loss: 0.0168 - 1s/epoch - 4ms/step
Epoch 16/210
374/374 - 1s - loss: 0.0127 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 17/210
374/374 - 1s - loss: 0.0112 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 18/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 19/210
374/374 - 1s - loss: 0.0113 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 20/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 21/210
374/374 - 1s - loss: 0.0138 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 22/210
374/374 - 1s - loss: 0.0147 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 23/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 24/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 25/210
374/374 - 1s - loss: 0.0196 - val_loss: 0.0132 - 1s/epoch - 4ms/step
Epoch 26/210
374/374 - 1s - loss: 0.0219 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 27/210
374/374 - 1s - loss: 0.0118 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 28/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 29/210
374/374 - 1s - loss: 0.0123 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 30/210
374/374 - 1s - loss: 0.0130 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 31/210
374/374 - 1s - loss: 0.0149 - val_loss: 0.0126 - 1s/epoch - 4ms/step
Epoch 32/210
374/374 - 1s - loss: 0.0155 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 33/210
374/374 - 1s - loss: 0.0120 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 34/210
374/374 - 1s - loss: 0.0147 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 35/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 36/210
374/374 - 1s - loss: 0.0147 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 37/210
374/374 - 1s - loss: 0.0238 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 38/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 39/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 40/210
374/374 - 1s - loss: 0.0137 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 41/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0142 - 1s/epoch - 4ms/step
Epoch 42/210
374/374 - 1s - loss: 0.0166 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 43/210
374/374 - 1s - loss: 0.0116 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 44/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 45/210
374/374 - 1s - loss: 0.0118 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 46/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 47/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0259 - 1s/epoch - 4ms/step
Epoch 48/210
374/374 - 1s - loss: 0.0335 - val_loss: 0.0130 - 1s/epoch - 4ms/step
Epoch 49/210
374/374 - 1s - loss: 0.0147 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 50/210
374/374 - 1s - loss: 0.0116 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 51/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0150 - 1s/epoch - 4ms/step
Epoch 52/210
374/374 - 1s - loss: 0.0247 - val_loss: 0.0130 - 1s/epoch - 4ms/step
Epoch 53/210
374/374 - 1s - loss: 0.0170 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 54/210
374/374 - 1s - loss: 0.0131 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 55/210
374/374 - 1s - loss: 0.0174 - val_loss: 0.0151 - 1s/epoch - 4ms/step
Epoch 56/210
374/374 - 1s - loss: 0.0262 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 57/210
374/374 - 1s - loss: 0.0137 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 58/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 59/210
374/374 - 1s - loss: 0.0116 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 60/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 61/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 62/210
374/374 - 1s - loss: 0.0113 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 63/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 64/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 65/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 66/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 67/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 68/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 69/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 70/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0153 - 1s/epoch - 4ms/step
Epoch 71/210
374/374 - 1s - loss: 0.0193 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 72/210
374/374 - 1s - loss: 0.0120 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 73/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0214 - 1s/epoch - 4ms/step
Epoch 74/210
374/374 - 1s - loss: 0.0235 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 75/210
374/374 - 1s - loss: 0.0115 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 76/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 77/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 78/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 79/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 80/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 81/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 82/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 83/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 84/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 85/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 86/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 87/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 88/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 89/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 90/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 91/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 92/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 93/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 94/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 95/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 96/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 97/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 98/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 99/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 100/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0158 - 1s/epoch - 4ms/step
Epoch 101/210
374/374 - 1s - loss: 0.0130 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 102/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 103/210
374/374 - 1s - loss: 0.0112 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 104/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 105/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 106/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 107/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 108/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 109/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 110/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 111/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 112/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 113/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 114/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 115/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 116/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 117/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 118/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 119/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 120/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 121/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 122/210
374/374 - 1s - loss: 0.0125 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 123/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 124/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 125/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 126/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 127/210
374/374 - 1s - loss: 0.0123 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 128/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 129/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 130/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 131/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 132/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 133/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 134/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 135/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 136/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 137/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 138/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 139/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 140/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 141/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 142/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 143/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 144/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 145/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 146/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 147/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 148/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 149/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 150/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 151/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 152/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 153/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 154/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 155/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 156/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 157/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 158/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 159/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 160/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 161/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 162/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 163/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 164/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 165/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 166/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 167/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 168/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 169/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 170/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 171/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 172/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 173/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 174/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 175/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 176/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 177/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 178/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 179/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 180/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 181/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 182/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 183/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 184/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 185/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 186/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 187/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 188/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 189/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 190/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 191/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 192/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 193/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 194/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 195/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 196/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 197/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 198/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 199/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 200/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 201/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 202/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 203/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 204/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 205/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 206/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 207/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 208/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 209/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 210/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009317313320934772
  1/332 [..............................] - ETA: 38s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s168/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s297/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07325502214591727
cosine 0.05750138968892067
MAE: 0.03552876
RMSE: 0.07779591
r2: 0.6073790325365095
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 256, 210, 0.001, 0.2, 252, 0.009428114630281925, 0.009317313320934772, 0.07325502214591727, 0.05750138968892067, 0.0355287604033947, 0.07779590785503387, 0.6073790325365095, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 180 0.001 128 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 5s - loss: 0.0141 - val_loss: 0.0090 - 5s/epoch - 7ms/step
Epoch 2/180
747/747 - 3s - loss: 0.0077 - val_loss: 0.0090 - 3s/epoch - 4ms/step
Epoch 3/180
747/747 - 3s - loss: 0.0075 - val_loss: 0.0076 - 3s/epoch - 4ms/step
Epoch 4/180
747/747 - 3s - loss: 0.0073 - val_loss: 0.0079 - 3s/epoch - 4ms/step
Epoch 5/180
747/747 - 3s - loss: 0.0071 - val_loss: 0.0073 - 3s/epoch - 4ms/step
Epoch 6/180
747/747 - 3s - loss: 0.0070 - val_loss: 0.0069 - 3s/epoch - 4ms/step
Epoch 7/180
747/747 - 3s - loss: 0.0069 - val_loss: 0.0069 - 3s/epoch - 4ms/step
Epoch 8/180
747/747 - 3s - loss: 0.0068 - val_loss: 0.0068 - 3s/epoch - 4ms/step
Epoch 9/180
747/747 - 3s - loss: 0.0068 - val_loss: 0.0067 - 3s/epoch - 4ms/step
Epoch 10/180
747/747 - 3s - loss: 0.0067 - val_loss: 0.0067 - 3s/epoch - 4ms/step
Epoch 11/180
747/747 - 3s - loss: 0.0067 - val_loss: 0.0065 - 3s/epoch - 4ms/step
Epoch 12/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 13/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 14/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 15/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 16/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 17/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 18/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 19/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 20/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 21/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 22/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 23/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 24/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 25/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 26/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 27/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 28/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 29/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 30/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 31/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 32/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 33/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 34/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 35/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 36/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 37/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 38/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 39/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 40/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 41/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 42/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 43/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 44/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 45/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 46/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 47/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 48/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 49/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 50/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 51/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 52/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 53/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 54/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 55/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 56/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 57/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 58/180
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 59/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 60/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 61/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 62/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 63/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 64/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 65/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 66/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 67/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 68/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 69/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 70/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 71/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 72/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 73/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 74/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 75/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 76/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 77/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 78/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 79/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 80/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 81/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 82/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 83/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 84/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 85/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 86/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 87/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 88/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 89/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 90/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 91/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 92/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 93/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 94/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 95/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 96/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 97/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 98/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 99/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 100/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 101/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 102/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 103/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 104/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 105/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 106/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 107/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 108/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 109/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 110/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 111/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 112/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 113/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 114/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 115/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 116/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 117/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 118/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 119/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 120/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 121/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 122/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 123/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 124/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 125/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 126/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 127/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 128/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 129/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 130/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 131/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 132/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 133/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 134/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 135/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 136/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 137/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 138/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 139/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 140/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 141/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 142/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 143/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 144/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 145/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 146/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 147/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 148/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 149/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 4ms/step
Epoch 150/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 151/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 152/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 153/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 154/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 155/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 156/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 157/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 158/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 159/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 160/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 161/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 162/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 163/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 164/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 165/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 166/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 167/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 168/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 169/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 170/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 171/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 172/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 173/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 174/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 175/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 176/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 177/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 178/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 179/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
Epoch 180/180
747/747 - 3s - loss: 0.0059 - val_loss: 0.0058 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005832950584590435
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.11139889610766286
cosine 0.08713096710777804
MAE: 0.043746218
RMSE: 0.095111385
r2: 0.4131525681948502
RMSE zero-vector: 0.23411466903540806
['1.0custom_VAE', 'logcosh', 128, 180, 0.001, 0.2, 252, 0.005887418985366821, 0.005832950584590435, 0.11139889610766286, 0.08713096710777804, 0.043746218085289, 0.09511138498783112, 0.4131525681948502, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 30 0.002 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/30
374/374 - 4s - loss: 0.0423 - val_loss: 0.0211 - 4s/epoch - 10ms/step
Epoch 2/30
374/374 - 2s - loss: 0.0167 - val_loss: 0.0262 - 2s/epoch - 4ms/step
Epoch 3/30
374/374 - 2s - loss: 0.0161 - val_loss: 0.0234 - 2s/epoch - 4ms/step
Epoch 4/30
374/374 - 2s - loss: 0.0144 - val_loss: 0.0231 - 2s/epoch - 4ms/step
Epoch 5/30
374/374 - 2s - loss: 0.0142 - val_loss: 0.0233 - 2s/epoch - 4ms/step
Epoch 6/30
374/374 - 2s - loss: 0.0136 - val_loss: 0.0222 - 2s/epoch - 4ms/step
Epoch 7/30
374/374 - 2s - loss: 0.0130 - val_loss: 0.0256 - 2s/epoch - 4ms/step
Epoch 8/30
374/374 - 2s - loss: 0.0133 - val_loss: 0.0151 - 2s/epoch - 4ms/step
Epoch 9/30
374/374 - 2s - loss: 0.0121 - val_loss: 0.0128 - 2s/epoch - 4ms/step
Epoch 10/30
374/374 - 2s - loss: 0.0119 - val_loss: 0.1391 - 2s/epoch - 4ms/step
Epoch 11/30
374/374 - 2s - loss: 0.0177 - val_loss: 0.0130 - 2s/epoch - 4ms/step
Epoch 12/30
374/374 - 2s - loss: 0.0119 - val_loss: 0.0126 - 2s/epoch - 4ms/step
Epoch 13/30
374/374 - 2s - loss: 0.0118 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 14/30
374/374 - 2s - loss: 0.0118 - val_loss: 0.0181 - 2s/epoch - 4ms/step
Epoch 15/30
374/374 - 2s - loss: 0.0131 - val_loss: 0.0140 - 2s/epoch - 4ms/step
Epoch 16/30
374/374 - 2s - loss: 0.0164 - val_loss: 0.0192 - 2s/epoch - 4ms/step
Epoch 17/30
374/374 - 2s - loss: 0.0308 - val_loss: 0.0177 - 2s/epoch - 4ms/step
Epoch 18/30
374/374 - 2s - loss: 0.0549 - val_loss: 0.0241 - 2s/epoch - 4ms/step
Epoch 19/30
374/374 - 2s - loss: 0.1224 - val_loss: 0.0261 - 2s/epoch - 4ms/step
Epoch 20/30
374/374 - 2s - loss: 0.0358 - val_loss: 0.0206 - 2s/epoch - 4ms/step
Epoch 21/30
374/374 - 2s - loss: 0.0206 - val_loss: 0.0200 - 2s/epoch - 4ms/step
Epoch 22/30
374/374 - 2s - loss: 0.0193 - val_loss: 0.0173 - 2s/epoch - 4ms/step
Epoch 23/30
374/374 - 2s - loss: 0.0174 - val_loss: 0.0159 - 2s/epoch - 4ms/step
Epoch 24/30
374/374 - 2s - loss: 0.0163 - val_loss: 0.0152 - 2s/epoch - 4ms/step
Epoch 25/30
374/374 - 2s - loss: 0.0312 - val_loss: 0.0292 - 2s/epoch - 4ms/step
Epoch 26/30
374/374 - 2s - loss: 0.1516 - val_loss: 0.0326 - 2s/epoch - 4ms/step
Epoch 27/30
374/374 - 2s - loss: 0.0338 - val_loss: 0.0297 - 2s/epoch - 4ms/step
Epoch 28/30
374/374 - 2s - loss: 0.0283 - val_loss: 0.0254 - 2s/epoch - 4ms/step
Epoch 29/30
374/374 - 2s - loss: 0.0814 - val_loss: 0.0282 - 2s/epoch - 4ms/step
Epoch 30/30
374/374 - 2s - loss: 0.0247 - val_loss: 0.0209 - 2s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.02088792435824871
  1/332 [..............................] - ETA: 32s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.12527507113884676
cosine 0.09819225622625383
MAE: 0.04821105
RMSE: 0.10049531
r2: 0.3448343849412501
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'mse', 256, 30, 0.002, 0.2, 252, 0.024692408740520477, 0.02088792435824871, 0.12527507113884676, 0.09819225622625383, 0.048211049288511276, 0.10049530863761902, 0.3448343849412501, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 115 0.002 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          446040      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          446040      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2756677     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,893,618
Trainable params: 5,886,038
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/115
374/374 - 4s - loss: 0.0345 - val_loss: 0.0179 - 4s/epoch - 10ms/step
Epoch 2/115
374/374 - 1s - loss: 0.0162 - val_loss: 0.0172 - 1s/epoch - 4ms/step
Epoch 3/115
374/374 - 1s - loss: 0.0147 - val_loss: 0.0171 - 1s/epoch - 4ms/step
Epoch 4/115
374/374 - 1s - loss: 0.0142 - val_loss: 0.0168 - 1s/epoch - 4ms/step
Epoch 5/115
374/374 - 1s - loss: 0.0139 - val_loss: 0.0171 - 1s/epoch - 4ms/step
Epoch 6/115
374/374 - 1s - loss: 0.0138 - val_loss: 0.0324 - 1s/epoch - 4ms/step
Epoch 7/115
374/374 - 1s - loss: 0.0135 - val_loss: 0.0142 - 1s/epoch - 4ms/step
Epoch 8/115
374/374 - 1s - loss: 0.0130 - val_loss: 0.0282 - 1s/epoch - 4ms/step
Epoch 9/115
374/374 - 1s - loss: 0.0130 - val_loss: 0.0141 - 1s/epoch - 4ms/step
Epoch 10/115
374/374 - 1s - loss: 0.0121 - val_loss: 0.0240 - 1s/epoch - 4ms/step
Epoch 11/115
374/374 - 1s - loss: 0.0120 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 12/115
374/374 - 1s - loss: 0.0116 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 13/115
374/374 - 1s - loss: 0.0115 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 14/115
374/374 - 1s - loss: 0.0113 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 15/115
374/374 - 1s - loss: 0.0111 - val_loss: 0.0142 - 1s/epoch - 4ms/step
Epoch 16/115
374/374 - 1s - loss: 0.0113 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 17/115
374/374 - 1s - loss: 0.0109 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 18/115
374/374 - 1s - loss: 0.0109 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 19/115
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 20/115
374/374 - 1s - loss: 0.0105 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 21/115
374/374 - 1s - loss: 0.0112 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 22/115
374/374 - 1s - loss: 0.0133 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 23/115
374/374 - 1s - loss: 0.0113 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 24/115
374/374 - 1s - loss: 0.0110 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 25/115
374/374 - 1s - loss: 0.0118 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 26/115
374/374 - 1s - loss: 0.0108 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 27/115
374/374 - 1s - loss: 0.0123 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 28/115
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 29/115
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 30/115
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 31/115
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 32/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 33/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 34/115
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 35/115
374/374 - 1s - loss: 0.0107 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 36/115
374/374 - 1s - loss: 0.0142 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 37/115
374/374 - 1s - loss: 0.0129 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 38/115
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 39/115
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 40/115
374/374 - 1s - loss: 0.0107 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 41/115
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 42/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 43/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 44/115
374/374 - 1s - loss: 0.0125 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 45/115
374/374 - 1s - loss: 0.0104 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 46/115
374/374 - 1s - loss: 0.0132 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 47/115
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 48/115
374/374 - 1s - loss: 0.0106 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 49/115
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 50/115
374/374 - 1s - loss: 0.0104 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 51/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 52/115
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 53/115
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 54/115
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 55/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 56/115
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 57/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 58/115
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 59/115
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 60/115
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 61/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 62/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 63/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 64/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 65/115
374/374 - 1s - loss: 0.0100 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 66/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 67/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 68/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 69/115
374/374 - 1s - loss: 0.0102 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 70/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 71/115
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 72/115
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 73/115
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 74/115
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 75/115
374/374 - 1s - loss: 0.0097 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 76/115
374/374 - 1s - loss: 0.0099 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 77/115
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 78/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 79/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 80/115
374/374 - 1s - loss: 0.0097 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 81/115
374/374 - 1s - loss: 0.0099 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 82/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 83/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 84/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 85/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 86/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 87/115
374/374 - 1s - loss: 0.0101 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 88/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 89/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 90/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 91/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 92/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 93/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 94/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 95/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 96/115
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 97/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 98/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 99/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 100/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 101/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 102/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 103/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 104/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 105/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 106/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 107/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 108/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 109/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 110/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 111/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 112/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 113/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 114/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 115/115
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009353666566312313
  1/332 [..............................] - ETA: 35s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07480812847299752
cosine 0.05870787958311576
MAE: 0.03617046
RMSE: 0.07867229
r2: 0.5984839529002687
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 256, 115, 0.002, 0.2, 252, 0.009455528110265732, 0.009353666566312313, 0.07480812847299752, 0.05870787958311576, 0.036170460283756256, 0.07867228984832764, 0.5984839529002687, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 25 0.0005 128 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/25
747/747 - 5s - loss: 0.0160 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 2/25
747/747 - 3s - loss: 0.0086 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 3/25
747/747 - 3s - loss: 0.0079 - val_loss: 0.0188 - 3s/epoch - 4ms/step
Epoch 4/25
747/747 - 3s - loss: 0.0075 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 5/25
747/747 - 3s - loss: 0.0071 - val_loss: 0.0134 - 3s/epoch - 4ms/step
Epoch 6/25
747/747 - 3s - loss: 0.0068 - val_loss: 0.0080 - 3s/epoch - 4ms/step
Epoch 7/25
747/747 - 3s - loss: 0.0066 - val_loss: 0.0064 - 3s/epoch - 4ms/step
Epoch 8/25
747/747 - 3s - loss: 0.0064 - val_loss: 0.0069 - 3s/epoch - 4ms/step
Epoch 9/25
747/747 - 3s - loss: 0.0064 - val_loss: 0.0175 - 3s/epoch - 4ms/step
Epoch 10/25
747/747 - 3s - loss: 0.0068 - val_loss: 0.0064 - 3s/epoch - 4ms/step
Epoch 11/25
747/747 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 12/25
747/747 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 13/25
747/747 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 14/25
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 15/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 16/25
747/747 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 17/25
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 18/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 19/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 20/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 21/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 22/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 23/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 24/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 25/25
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.006132901646196842
  1/332 [..............................] - ETA: 33s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.13223397866020642
cosine 0.1034964083378332
MAE: 0.049007345
RMSE: 0.1032095
r2: 0.3089664562098079
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'logcosh', 128, 25, 0.0005, 0.2, 252, 0.006204226985573769, 0.006132901646196842, 0.13223397866020642, 0.1034964083378332, 0.049007344990968704, 0.10320950299501419, 0.3089664562098079, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 120 0.0018 256 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/120
374/374 - 4s - loss: 0.0202 - val_loss: 0.0131 - 4s/epoch - 10ms/step
Epoch 2/120
374/374 - 1s - loss: 0.0086 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 3/120
374/374 - 1s - loss: 0.0079 - val_loss: 0.0086 - 1s/epoch - 4ms/step
Epoch 4/120
374/374 - 1s - loss: 0.0077 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 5/120
374/374 - 1s - loss: 0.0076 - val_loss: 0.0082 - 1s/epoch - 4ms/step
Epoch 6/120
374/374 - 1s - loss: 0.0075 - val_loss: 0.0183 - 1s/epoch - 4ms/step
Epoch 7/120
374/374 - 1s - loss: 0.0075 - val_loss: 0.0081 - 1s/epoch - 4ms/step
Epoch 8/120
374/374 - 1s - loss: 0.0073 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 9/120
374/374 - 1s - loss: 0.0072 - val_loss: 0.0077 - 1s/epoch - 4ms/step
Epoch 10/120
374/374 - 1s - loss: 0.0071 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 11/120
374/374 - 1s - loss: 0.0070 - val_loss: 0.0075 - 1s/epoch - 4ms/step
Epoch 12/120
374/374 - 1s - loss: 0.0069 - val_loss: 0.0071 - 1s/epoch - 4ms/step
Epoch 13/120
374/374 - 1s - loss: 0.0068 - val_loss: 0.0221 - 1s/epoch - 4ms/step
Epoch 14/120
374/374 - 1s - loss: 0.0070 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 15/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 16/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 17/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 18/120
374/374 - 1s - loss: 0.0066 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 19/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 20/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 21/120
374/374 - 1s - loss: 0.0066 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 22/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 23/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 24/120
374/374 - 1s - loss: 0.0066 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 25/120
374/374 - 1s - loss: 0.0071 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 26/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 27/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 28/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 29/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 30/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 31/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 32/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 33/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 34/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 35/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 36/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 37/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 38/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 39/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 40/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 41/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 42/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 43/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 44/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 45/120
374/374 - 1s - loss: 0.0073 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 46/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 47/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 48/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 49/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 50/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 51/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 52/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 53/120
374/374 - 1s - loss: 0.0074 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 54/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 55/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 56/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 57/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 58/120
374/374 - 1s - loss: 0.0064 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 59/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 60/120
374/374 - 1s - loss: 0.0068 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 61/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 62/120
374/374 - 1s - loss: 0.0070 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 63/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 64/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 65/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 66/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 67/120
374/374 - 1s - loss: 0.0066 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 68/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 69/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 70/120
374/374 - 1s - loss: 0.0067 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 71/120
374/374 - 1s - loss: 0.0063 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 72/120
374/374 - 1s - loss: 0.0065 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 73/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 74/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 75/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 76/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 77/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 78/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 79/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 80/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 81/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 82/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 83/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 84/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 85/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 86/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 87/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 88/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 89/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 90/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 91/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 92/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 93/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 94/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 95/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 96/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 97/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 98/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 99/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 100/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 101/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 102/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 103/120
374/374 - 1s - loss: 0.0061 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 104/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 105/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 106/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 107/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 108/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 109/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 110/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 111/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 112/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 113/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 114/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 115/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 116/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 117/120
374/374 - 1s - loss: 0.0062 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 118/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 119/120
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 120/120
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005902983248233795
  1/332 [..............................] - ETA: 35s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s171/332 [==============>...............] - ETA: 0s214/332 [==================>...........] - ETA: 0s257/332 [======================>.......] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.11620272657824515
cosine 0.09094816904628766
MAE: 0.044802617
RMSE: 0.09702377
r2: 0.3893163032640997
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 256, 120, 0.0018, 0.2, 252, 0.005949476268142462, 0.005902983248233795, 0.11620272657824515, 0.09094816904628766, 0.04480261728167534, 0.09702377021312714, 0.3893163032640997, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 150 0.0022 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0391 - val_loss: 0.0209 - 4s/epoch - 10ms/step
Epoch 2/150
374/374 - 1s - loss: 0.0164 - val_loss: 0.0258 - 1s/epoch - 4ms/step
Epoch 3/150
374/374 - 1s - loss: 0.0150 - val_loss: 0.0211 - 1s/epoch - 4ms/step
Epoch 4/150
374/374 - 1s - loss: 0.0144 - val_loss: 0.0212 - 1s/epoch - 4ms/step
Epoch 5/150
374/374 - 1s - loss: 0.0140 - val_loss: 0.0154 - 1s/epoch - 4ms/step
Epoch 6/150
374/374 - 1s - loss: 0.0135 - val_loss: 0.0160 - 1s/epoch - 4ms/step
Epoch 7/150
374/374 - 1s - loss: 0.0133 - val_loss: 0.0154 - 1s/epoch - 4ms/step
Epoch 8/150
374/374 - 1s - loss: 0.0130 - val_loss: 0.0145 - 1s/epoch - 4ms/step
Epoch 9/150
374/374 - 1s - loss: 0.0125 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 10/150
374/374 - 1s - loss: 0.0121 - val_loss: 0.0153 - 1s/epoch - 4ms/step
Epoch 11/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0187 - 1s/epoch - 4ms/step
Epoch 12/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 13/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 14/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 15/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0189 - 1s/epoch - 4ms/step
Epoch 16/150
374/374 - 1s - loss: 0.0131 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 17/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0152 - 1s/epoch - 4ms/step
Epoch 18/150
374/374 - 1s - loss: 0.0173 - val_loss: 0.0145 - 1s/epoch - 4ms/step
Epoch 19/150
374/374 - 1s - loss: 0.0192 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 20/150
374/374 - 1s - loss: 0.0123 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 21/150
374/374 - 1s - loss: 0.0174 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 22/150
374/374 - 1s - loss: 0.0117 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 23/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 24/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 25/150
374/374 - 1s - loss: 0.0136 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 26/150
374/374 - 1s - loss: 0.0134 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 27/150
374/374 - 1s - loss: 0.0155 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 28/150
374/374 - 1s - loss: 0.0126 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 29/150
374/374 - 1s - loss: 0.0142 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 30/150
374/374 - 1s - loss: 0.0126 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 31/150
374/374 - 1s - loss: 0.0175 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 32/150
374/374 - 1s - loss: 0.0137 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 33/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 34/150
374/374 - 1s - loss: 0.0171 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 35/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 36/150
374/374 - 1s - loss: 0.0134 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 37/150
374/374 - 1s - loss: 0.0117 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 38/150
374/374 - 1s - loss: 0.0161 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 39/150
374/374 - 1s - loss: 0.0117 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 40/150
374/374 - 1s - loss: 0.0172 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 41/150
374/374 - 1s - loss: 0.0143 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 42/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 43/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 44/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 45/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 46/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 47/150
374/374 - 1s - loss: 0.0187 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 48/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 49/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0130 - 1s/epoch - 4ms/step
Epoch 50/150
374/374 - 1s - loss: 0.0145 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 51/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0136 - 1s/epoch - 4ms/step
Epoch 52/150
374/374 - 1s - loss: 0.0157 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 53/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 54/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 55/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 56/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 57/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 58/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 59/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 60/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 61/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 62/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 63/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 64/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 65/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 66/150
374/374 - 1s - loss: 0.0130 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 67/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 68/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 69/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 70/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 71/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 72/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 73/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 74/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 75/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 76/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 77/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 78/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 79/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 80/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 81/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 82/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 83/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 84/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 85/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 86/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 87/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 88/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 89/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 90/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 91/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 92/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 93/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 94/150
374/374 - 1s - loss: 0.0142 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 95/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 96/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 97/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 98/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 99/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 100/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 101/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 102/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 103/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 104/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 105/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 106/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 107/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 108/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 109/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 110/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 111/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 112/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 113/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 114/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 115/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 116/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 117/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 118/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 119/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 120/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 121/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 122/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 123/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 124/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 125/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 126/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 127/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 128/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 129/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 130/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 131/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 132/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 133/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 134/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 135/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 136/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 137/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 138/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 139/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 140/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 141/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 142/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 143/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 144/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 145/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 146/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 147/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 148/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 149/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 150/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009465661831200123
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s297/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07697293586408332
cosine 0.060403632636884454
MAE: 0.036521014
RMSE: 0.079648465
r2: 0.5884575251304532
RMSE zero-vector: 0.23411466903540806
['1.9custom_VAE', 'mse', 256, 150, 0.0022, 0.2, 252, 0.009565231390297413, 0.009465661831200123, 0.07697293586408332, 0.060403632636884454, 0.03652101382613182, 0.0796484649181366, 0.5884575251304532, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 150 0.0022 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0397 - val_loss: 0.0188 - 4s/epoch - 10ms/step
Epoch 2/150
374/374 - 2s - loss: 0.0161 - val_loss: 0.0261 - 2s/epoch - 4ms/step
Epoch 3/150
374/374 - 2s - loss: 0.0152 - val_loss: 0.0990 - 2s/epoch - 4ms/step
Epoch 4/150
374/374 - 2s - loss: 0.0160 - val_loss: 0.0467 - 2s/epoch - 4ms/step
Epoch 5/150
374/374 - 2s - loss: 0.0145 - val_loss: 0.0158 - 2s/epoch - 4ms/step
Epoch 6/150
374/374 - 2s - loss: 0.0135 - val_loss: 0.0160 - 2s/epoch - 4ms/step
Epoch 7/150
374/374 - 2s - loss: 0.0132 - val_loss: 0.0156 - 2s/epoch - 4ms/step
Epoch 8/150
374/374 - 2s - loss: 0.0129 - val_loss: 0.0302 - 2s/epoch - 4ms/step
Epoch 9/150
374/374 - 2s - loss: 0.0127 - val_loss: 0.0311 - 2s/epoch - 4ms/step
Epoch 10/150
374/374 - 2s - loss: 0.0132 - val_loss: 0.0135 - 2s/epoch - 4ms/step
Epoch 11/150
374/374 - 2s - loss: 0.0118 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 12/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 4ms/step
Epoch 13/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0238 - 2s/epoch - 4ms/step
Epoch 14/150
374/374 - 2s - loss: 0.0130 - val_loss: 0.0117 - 2s/epoch - 4ms/step
Epoch 15/150
374/374 - 2s - loss: 0.0116 - val_loss: 0.0160 - 2s/epoch - 4ms/step
Epoch 16/150
374/374 - 2s - loss: 0.0134 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 17/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0121 - 2s/epoch - 4ms/step
Epoch 18/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0126 - 2s/epoch - 4ms/step
Epoch 19/150
374/374 - 2s - loss: 0.0133 - val_loss: 0.0131 - 2s/epoch - 4ms/step
Epoch 20/150
374/374 - 2s - loss: 0.0175 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 21/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 22/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 23/150
374/374 - 2s - loss: 0.0158 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 24/150
374/374 - 2s - loss: 0.0117 - val_loss: 0.0136 - 2s/epoch - 4ms/step
Epoch 25/150
374/374 - 2s - loss: 0.0194 - val_loss: 0.0129 - 2s/epoch - 4ms/step
Epoch 26/150
374/374 - 2s - loss: 0.0148 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 27/150
374/374 - 2s - loss: 0.0151 - val_loss: 0.0123 - 2s/epoch - 4ms/step
Epoch 28/150
374/374 - 2s - loss: 0.0146 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 29/150
374/374 - 2s - loss: 0.0126 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 30/150
374/374 - 2s - loss: 0.0142 - val_loss: 0.0129 - 2s/epoch - 4ms/step
Epoch 31/150
374/374 - 2s - loss: 0.0141 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 32/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0122 - 2s/epoch - 4ms/step
Epoch 33/150
374/374 - 2s - loss: 0.0145 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 34/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0126 - 2s/epoch - 4ms/step
Epoch 35/150
374/374 - 2s - loss: 0.0183 - val_loss: 0.0125 - 2s/epoch - 4ms/step
Epoch 36/150
374/374 - 2s - loss: 0.0151 - val_loss: 0.0123 - 2s/epoch - 4ms/step
Epoch 37/150
374/374 - 2s - loss: 0.0139 - val_loss: 0.0120 - 2s/epoch - 4ms/step
Epoch 38/150
374/374 - 2s - loss: 0.0128 - val_loss: 0.0117 - 2s/epoch - 4ms/step
Epoch 39/150
374/374 - 2s - loss: 0.0132 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 40/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 41/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0134 - 2s/epoch - 4ms/step
Epoch 42/150
374/374 - 2s - loss: 0.0193 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 43/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0125 - 2s/epoch - 4ms/step
Epoch 44/150
374/374 - 2s - loss: 0.0191 - val_loss: 0.0117 - 2s/epoch - 4ms/step
Epoch 45/150
374/374 - 2s - loss: 0.0117 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 46/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 47/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 48/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 49/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 50/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 51/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 52/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 53/150
374/374 - 2s - loss: 0.0118 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 54/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 55/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0125 - 2s/epoch - 4ms/step
Epoch 56/150
374/374 - 2s - loss: 0.0148 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 57/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 58/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 59/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 60/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 61/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0146 - 2s/epoch - 4ms/step
Epoch 62/150
374/374 - 2s - loss: 0.0153 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 63/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 64/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0125 - 2s/epoch - 4ms/step
Epoch 65/150
374/374 - 2s - loss: 0.0136 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 66/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 67/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 68/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0127 - 2s/epoch - 4ms/step
Epoch 69/150
374/374 - 2s - loss: 0.0156 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 70/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 71/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0135 - 2s/epoch - 4ms/step
Epoch 72/150
374/374 - 2s - loss: 0.0210 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 73/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 74/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 75/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 76/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 77/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 78/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 79/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 80/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0115 - 2s/epoch - 4ms/step
Epoch 81/150
374/374 - 2s - loss: 0.0123 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 82/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 83/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 84/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 85/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 86/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 87/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 88/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 89/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 90/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 91/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 92/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 93/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 94/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0134 - 2s/epoch - 4ms/step
Epoch 95/150
374/374 - 2s - loss: 0.0160 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 96/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 97/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0129 - 2s/epoch - 4ms/step
Epoch 98/150
374/374 - 2s - loss: 0.0151 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 99/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 100/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 101/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 102/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 103/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 104/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 105/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 106/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 107/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 108/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 109/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 110/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 111/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 112/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 113/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 114/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 115/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 116/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 117/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 118/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0115 - 2s/epoch - 4ms/step
Epoch 119/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0143 - 2s/epoch - 4ms/step
Epoch 120/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 121/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 122/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 123/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 124/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0127 - 2s/epoch - 4ms/step
Epoch 125/150
374/374 - 2s - loss: 0.0116 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 126/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 127/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 128/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 129/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 130/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 131/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 132/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 133/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 134/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 135/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 136/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 137/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 138/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 139/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 140/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 141/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 142/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 143/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 144/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 145/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 146/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 147/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 148/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 149/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 150/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0097 - 2s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00968714989721775
  1/332 [..............................] - ETA: 36s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07882585822187906
cosine 0.061844934237214516
MAE: 0.03690862
RMSE: 0.08058153
r2: 0.5787587225003017
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 256, 150, 0.0022, 0.2, 252, 0.009695922024548054, 0.00968714989721775, 0.07882585822187906, 0.061844934237214516, 0.03690861910581589, 0.08058153092861176, 0.5787587225003017, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 210 0.0008 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/210
374/374 - 4s - loss: 0.0402 - val_loss: 0.0260 - 4s/epoch - 10ms/step
Epoch 2/210
374/374 - 1s - loss: 0.0160 - val_loss: 0.0184 - 1s/epoch - 4ms/step
Epoch 3/210
374/374 - 1s - loss: 0.0149 - val_loss: 0.0193 - 1s/epoch - 4ms/step
Epoch 4/210
374/374 - 1s - loss: 0.0143 - val_loss: 0.0210 - 1s/epoch - 4ms/step
Epoch 5/210
374/374 - 1s - loss: 0.0146 - val_loss: 0.0163 - 1s/epoch - 4ms/step
Epoch 6/210
374/374 - 1s - loss: 0.0134 - val_loss: 0.0161 - 1s/epoch - 4ms/step
Epoch 7/210
374/374 - 1s - loss: 0.0131 - val_loss: 0.0149 - 1s/epoch - 4ms/step
Epoch 8/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0259 - 1s/epoch - 4ms/step
Epoch 9/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0258 - 1s/epoch - 4ms/step
Epoch 10/210
374/374 - 1s - loss: 0.0123 - val_loss: 0.0687 - 1s/epoch - 4ms/step
Epoch 11/210
374/374 - 1s - loss: 0.0168 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 12/210
374/374 - 1s - loss: 0.0118 - val_loss: 0.0200 - 1s/epoch - 4ms/step
Epoch 13/210
374/374 - 1s - loss: 0.0131 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 14/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0138 - 1s/epoch - 4ms/step
Epoch 15/210
374/374 - 1s - loss: 0.0125 - val_loss: 0.0144 - 1s/epoch - 4ms/step
Epoch 16/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.0133 - 1s/epoch - 4ms/step
Epoch 17/210
374/374 - 1s - loss: 0.0124 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 18/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 19/210
374/374 - 1s - loss: 0.0113 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 20/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 21/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 22/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 23/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 24/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 25/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 26/210
374/374 - 1s - loss: 0.0118 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 27/210
374/374 - 1s - loss: 0.0159 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 28/210
374/374 - 1s - loss: 0.0155 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 29/210
374/374 - 1s - loss: 0.0142 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 30/210
374/374 - 1s - loss: 0.0280 - val_loss: 0.0143 - 1s/epoch - 4ms/step
Epoch 31/210
374/374 - 1s - loss: 0.0294 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 32/210
374/374 - 1s - loss: 0.0131 - val_loss: 0.0139 - 1s/epoch - 4ms/step
Epoch 33/210
374/374 - 1s - loss: 0.0259 - val_loss: 0.0130 - 1s/epoch - 4ms/step
Epoch 34/210
374/374 - 1s - loss: 0.0130 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 35/210
374/374 - 1s - loss: 0.0123 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 36/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 37/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 38/210
374/374 - 1s - loss: 0.0115 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 39/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 40/210
374/374 - 1s - loss: 0.0113 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 41/210
374/374 - 1s - loss: 0.0112 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 42/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0130 - 1s/epoch - 4ms/step
Epoch 43/210
374/374 - 1s - loss: 0.0137 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 44/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 45/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 46/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0141 - 1s/epoch - 4ms/step
Epoch 47/210
374/374 - 1s - loss: 0.0143 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 48/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 49/210
374/374 - 1s - loss: 0.0157 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 50/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 51/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 52/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 53/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 54/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 55/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 56/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 57/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 58/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 59/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 60/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 61/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 62/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 63/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 64/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 65/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0180 - 1s/epoch - 4ms/step
Epoch 66/210
374/374 - 1s - loss: 0.0152 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 67/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 68/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 69/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 70/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 71/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0154 - 1s/epoch - 4ms/step
Epoch 72/210
374/374 - 1s - loss: 0.0187 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 73/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 74/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 75/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 76/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 77/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 78/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 79/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 80/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 81/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 82/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 83/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 84/210
374/374 - 1s - loss: 0.0184 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 85/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 86/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 87/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 88/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 89/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 90/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 91/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 92/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 93/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 94/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 95/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 96/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 97/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 98/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 99/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 100/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 101/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 102/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0139 - 1s/epoch - 4ms/step
Epoch 103/210
374/374 - 1s - loss: 0.0140 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 104/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 105/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 106/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 107/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 108/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 109/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 110/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 111/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 112/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 113/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 114/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 115/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 116/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 117/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 118/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 119/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 120/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 121/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 122/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 123/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 124/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 125/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 126/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 127/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 128/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 129/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 130/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 131/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 132/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 133/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 134/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 135/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 136/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 137/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 138/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 139/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 140/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 141/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 142/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 143/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 144/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 145/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 146/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 147/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 148/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 149/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 150/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 151/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 152/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 153/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 154/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 155/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 156/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 157/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 158/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 159/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 160/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 161/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 162/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 163/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 164/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 165/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 166/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 167/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 168/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 169/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 170/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 171/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 172/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 173/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 174/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 175/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 176/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 177/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 178/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 179/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 180/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 181/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 182/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 183/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 184/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 185/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 186/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 187/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 188/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 189/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 190/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 191/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 192/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 193/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 194/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 195/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 196/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 197/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 198/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 199/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 200/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 201/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 202/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 203/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 204/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 205/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 206/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 207/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 208/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 209/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 210/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009231454692780972
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07133550575172232
cosine 0.05594724260026268
MAE: 0.034955163
RMSE: 0.076876946
r2: 0.6165999769715528
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 256, 210, 0.0008, 0.2, 252, 0.009315770119428635, 0.009231454692780972, 0.07133550575172232, 0.05594724260026268, 0.034955162554979324, 0.07687694579362869, 0.6165999769715528, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 205 0.002 256 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/205
374/374 - 4s - loss: 0.0209 - val_loss: 0.0128 - 4s/epoch - 10ms/step
Epoch 2/205
374/374 - 1s - loss: 0.0088 - val_loss: 0.0171 - 1s/epoch - 4ms/step
Epoch 3/205
374/374 - 1s - loss: 0.0091 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 4/205
374/374 - 1s - loss: 0.0078 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 5/205
374/374 - 1s - loss: 0.0077 - val_loss: 0.0087 - 1s/epoch - 4ms/step
Epoch 6/205
374/374 - 1s - loss: 0.0075 - val_loss: 0.0085 - 1s/epoch - 4ms/step
Epoch 7/205
374/374 - 1s - loss: 0.0074 - val_loss: 0.0273 - 1s/epoch - 4ms/step
Epoch 8/205
374/374 - 1s - loss: 0.0075 - val_loss: 0.0335 - 1s/epoch - 4ms/step
Epoch 9/205
374/374 - 1s - loss: 0.0078 - val_loss: 0.0078 - 1s/epoch - 4ms/step
Epoch 10/205
374/374 - 1s - loss: 0.0068 - val_loss: 0.0071 - 1s/epoch - 4ms/step
Epoch 11/205
374/374 - 1s - loss: 0.0066 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 12/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 13/205
374/374 - 1s - loss: 0.0066 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 14/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 15/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 16/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 17/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0268 - 1s/epoch - 4ms/step
Epoch 18/205
374/374 - 1s - loss: 0.0073 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 19/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 20/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 21/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 22/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 23/205
374/374 - 1s - loss: 0.0066 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 24/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 25/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 26/205
374/374 - 1s - loss: 0.0079 - val_loss: 0.0075 - 1s/epoch - 4ms/step
Epoch 27/205
374/374 - 1s - loss: 0.0092 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 28/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 29/205
374/374 - 1s - loss: 0.0068 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 30/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 31/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 32/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 33/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 34/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 35/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 36/205
374/374 - 1s - loss: 0.0067 - val_loss: 0.0074 - 1s/epoch - 4ms/step
Epoch 37/205
374/374 - 1s - loss: 0.0100 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 38/205
374/374 - 1s - loss: 0.0099 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 39/205
374/374 - 1s - loss: 0.0102 - val_loss: 0.0083 - 1s/epoch - 4ms/step
Epoch 40/205
374/374 - 1s - loss: 0.0200 - val_loss: 0.0076 - 1s/epoch - 4ms/step
Epoch 41/205
374/374 - 1s - loss: 0.0081 - val_loss: 0.0071 - 1s/epoch - 4ms/step
Epoch 42/205
374/374 - 1s - loss: 0.0077 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 43/205
374/374 - 1s - loss: 0.0069 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 44/205
374/374 - 1s - loss: 0.0070 - val_loss: 0.0126 - 1s/epoch - 4ms/step
Epoch 45/205
374/374 - 1s - loss: 0.0292 - val_loss: 0.0090 - 1s/epoch - 4ms/step
Epoch 46/205
374/374 - 1s - loss: 0.0075 - val_loss: 0.0073 - 1s/epoch - 4ms/step
Epoch 47/205
374/374 - 1s - loss: 0.0083 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 48/205
374/374 - 1s - loss: 0.0070 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 49/205
374/374 - 1s - loss: 0.0069 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 50/205
374/374 - 1s - loss: 0.0068 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 51/205
374/374 - 1s - loss: 0.0067 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 52/205
374/374 - 1s - loss: 0.0066 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 53/205
374/374 - 1s - loss: 0.0070 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 54/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 55/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 56/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 57/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 58/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 59/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 60/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 61/205
374/374 - 1s - loss: 0.0143 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 62/205
374/374 - 1s - loss: 0.0069 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 63/205
374/374 - 1s - loss: 0.0066 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 64/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 65/205
374/374 - 1s - loss: 0.0067 - val_loss: 0.0252 - 1s/epoch - 4ms/step
Epoch 66/205
374/374 - 1s - loss: 0.0270 - val_loss: 0.0078 - 1s/epoch - 4ms/step
Epoch 67/205
374/374 - 1s - loss: 0.0070 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 68/205
374/374 - 1s - loss: 0.0067 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 69/205
374/374 - 1s - loss: 0.0066 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 70/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 71/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 72/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 73/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 74/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 75/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 76/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 77/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 78/205
374/374 - 1s - loss: 0.0091 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 79/205
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 80/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 81/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 82/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 83/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 84/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 85/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 86/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 87/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 88/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 89/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 90/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 91/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 92/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 93/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 94/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 95/205
374/374 - 1s - loss: 0.0068 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 96/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 97/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 98/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 99/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 100/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 101/205
374/374 - 1s - loss: 0.0063 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 102/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0075 - 1s/epoch - 4ms/step
Epoch 103/205
374/374 - 1s - loss: 0.0069 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 104/205
374/374 - 1s - loss: 0.0071 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 105/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 106/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 107/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 108/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 109/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 110/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 111/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0088 - 1s/epoch - 4ms/step
Epoch 112/205
374/374 - 1s - loss: 0.0077 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 113/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 114/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 115/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 116/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 117/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 118/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 119/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 120/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 121/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 122/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 123/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 124/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 125/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 126/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 127/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 128/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 129/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 130/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 131/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 132/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 133/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 134/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 135/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 136/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 137/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 138/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 139/205
374/374 - 1s - loss: 0.0065 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 140/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 141/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 142/205
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 143/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 144/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 145/205
374/374 - 1s - loss: 0.0062 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 146/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 147/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 148/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 149/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 150/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 151/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 152/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 153/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 154/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 155/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 156/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 157/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 158/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 159/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 160/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 161/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 162/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 163/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 164/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 165/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 166/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 167/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 168/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 169/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 170/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 171/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 172/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 173/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 174/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 175/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 176/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 177/205
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 178/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 179/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 180/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 181/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 182/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 183/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 184/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 185/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 186/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 187/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 188/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 189/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 190/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 191/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 192/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 193/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 194/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 195/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 196/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 197/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 198/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 199/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 200/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 201/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 202/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 203/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 204/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 205/205
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0058455923572182655
  1/332 [..............................] - ETA: 38s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.1147832679627214
cosine 0.089778148455077
MAE: 0.04437441
RMSE: 0.096488915
r2: 0.3960304788072739
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'logcosh', 256, 205, 0.002, 0.2, 252, 0.005908710416406393, 0.0058455923572182655, 0.1147832679627214, 0.089778148455077, 0.044374410063028336, 0.09648891538381577, 0.3960304788072739, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[1.7999999999999998 150 0.0022 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2275)         2877875     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2275)        9100        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2275)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3526303     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,560,382
Trainable params: 7,550,778
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0373 - val_loss: 0.0182 - 4s/epoch - 11ms/step
Epoch 2/150
374/374 - 1s - loss: 0.0167 - val_loss: 0.0213 - 1s/epoch - 4ms/step
Epoch 3/150
374/374 - 1s - loss: 0.0154 - val_loss: 0.0169 - 1s/epoch - 4ms/step
Epoch 4/150
374/374 - 1s - loss: 0.0147 - val_loss: 0.0186 - 1s/epoch - 4ms/step
Epoch 5/150
374/374 - 1s - loss: 0.0141 - val_loss: 0.0186 - 1s/epoch - 4ms/step
Epoch 6/150
374/374 - 1s - loss: 0.0138 - val_loss: 0.0254 - 1s/epoch - 4ms/step
Epoch 7/150
374/374 - 1s - loss: 0.0135 - val_loss: 0.0147 - 1s/epoch - 4ms/step
Epoch 8/150
374/374 - 1s - loss: 0.0131 - val_loss: 0.0150 - 1s/epoch - 4ms/step
Epoch 9/150
374/374 - 1s - loss: 0.0129 - val_loss: 0.0187 - 1s/epoch - 4ms/step
Epoch 10/150
374/374 - 1s - loss: 0.0127 - val_loss: 0.1601 - 1s/epoch - 4ms/step
Epoch 11/150
374/374 - 1s - loss: 0.0180 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 12/150
374/374 - 1s - loss: 0.0120 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 13/150
374/374 - 1s - loss: 0.0121 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 14/150
374/374 - 1s - loss: 0.0117 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 15/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 16/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 17/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 18/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 19/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 20/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 21/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 22/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 23/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 24/150
374/374 - 1s - loss: 0.0121 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 25/150
374/374 - 1s - loss: 0.0146 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 26/150
374/374 - 1s - loss: 0.0128 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 27/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 28/150
374/374 - 1s - loss: 0.0128 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 29/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 30/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 31/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 32/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 33/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 34/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 35/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 36/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 37/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 38/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 39/150
374/374 - 1s - loss: 0.0137 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 40/150
374/374 - 1s - loss: 0.0141 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 41/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 42/150
374/374 - 1s - loss: 0.0212 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 43/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 44/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 45/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 46/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 47/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 48/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 49/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 50/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 51/150
374/374 - 1s - loss: 0.0127 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 52/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 53/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 54/150
374/374 - 1s - loss: 0.0160 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 55/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 56/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 57/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 58/150
374/374 - 1s - loss: 0.0157 - val_loss: 0.0156 - 1s/epoch - 4ms/step
Epoch 59/150
374/374 - 1s - loss: 0.0215 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 60/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 61/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 62/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 63/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 64/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 65/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 66/150
374/374 - 1s - loss: 0.0137 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 67/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 68/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 69/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 70/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 71/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 72/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 73/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 74/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 75/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 76/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 77/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 78/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 79/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 80/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 81/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 82/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 83/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 84/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 85/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 86/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 87/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 88/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 89/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 90/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 91/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 92/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 93/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 94/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 95/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 96/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 97/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 98/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 99/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 100/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 101/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 102/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 103/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 104/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 105/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 106/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 107/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 108/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 109/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 110/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 111/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 112/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 113/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 114/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 115/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 116/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 117/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 118/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 119/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 120/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 121/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 122/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 123/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 124/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 125/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 126/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 127/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 128/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 129/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 130/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 131/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 132/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 133/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 134/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 135/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 136/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 137/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 138/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 139/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 140/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 141/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 142/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 143/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 144/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 145/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 146/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 147/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 148/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 149/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 150/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009383328258991241
  1/332 [..............................] - ETA: 36s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s297/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07449271764972729
cosine 0.05846647288028223
MAE: 0.035683047
RMSE: 0.07842844
r2: 0.6009686156256604
RMSE zero-vector: 0.23411466903540806
['1.7999999999999998custom_VAE', 'mse', 256, 150, 0.0022, 0.2, 252, 0.009500117041170597, 0.009383328258991241, 0.07449271764972729, 0.05846647288028223, 0.0356830470263958, 0.07842843979597092, 0.6009686156256604, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 145 0.0022 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/145
747/747 - 5s - loss: 0.0304 - val_loss: 0.0194 - 5s/epoch - 7ms/step
Epoch 2/145
747/747 - 3s - loss: 0.0156 - val_loss: 0.0285 - 3s/epoch - 4ms/step
Epoch 3/145
747/747 - 3s - loss: 0.0146 - val_loss: 0.0144 - 3s/epoch - 4ms/step
Epoch 4/145
747/747 - 3s - loss: 0.0133 - val_loss: 0.0450 - 3s/epoch - 4ms/step
Epoch 5/145
747/747 - 3s - loss: 0.0134 - val_loss: 0.0438 - 3s/epoch - 4ms/step
Epoch 6/145
747/747 - 3s - loss: 0.0138 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 7/145
747/747 - 3s - loss: 0.0119 - val_loss: 0.0135 - 3s/epoch - 4ms/step
Epoch 8/145
747/747 - 3s - loss: 0.0119 - val_loss: 0.0133 - 3s/epoch - 4ms/step
Epoch 9/145
747/747 - 3s - loss: 0.0119 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 10/145
747/747 - 3s - loss: 0.0113 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 11/145
747/747 - 3s - loss: 0.0115 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 12/145
747/747 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 13/145
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 14/145
747/747 - 3s - loss: 0.0108 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 15/145
747/747 - 3s - loss: 0.0107 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 16/145
747/747 - 3s - loss: 0.0109 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 17/145
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 18/145
747/747 - 3s - loss: 0.0110 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 19/145
747/747 - 3s - loss: 0.0122 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 20/145
747/747 - 3s - loss: 0.0108 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 21/145
747/747 - 3s - loss: 0.0116 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 22/145
747/747 - 3s - loss: 0.0116 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 23/145
747/747 - 3s - loss: 0.0106 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 24/145
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 25/145
747/747 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 26/145
747/747 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 27/145
747/747 - 3s - loss: 0.0104 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 28/145
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 29/145
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 30/145
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 31/145
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 32/145
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 33/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 34/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 35/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 36/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 37/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 38/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 39/145
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 40/145
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 41/145
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 42/145
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 43/145
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 44/145
747/747 - 3s - loss: 0.0098 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 45/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 46/145
747/747 - 3s - loss: 0.0100 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 47/145
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 48/145
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 49/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 50/145
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 51/145
747/747 - 3s - loss: 0.0099 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 52/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 53/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 54/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 55/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 56/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 57/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 58/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 59/145
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 60/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 61/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 62/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 63/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 64/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 65/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 66/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 67/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 68/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 69/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 70/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 71/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 72/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 73/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 74/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 75/145
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 76/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 77/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 78/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 79/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 80/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 81/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 82/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 83/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 84/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 85/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 86/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 87/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 88/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 89/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 90/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 91/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 92/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 93/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 94/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 95/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 96/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 97/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 98/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 99/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 100/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 101/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 102/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 103/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 104/145
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 105/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 106/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 107/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 108/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 109/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 110/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 111/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 112/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 113/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 114/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 115/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 116/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 117/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 118/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 119/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 120/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 121/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 122/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 123/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 124/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 125/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 126/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 127/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 128/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 129/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 130/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 131/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 132/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 133/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 134/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 135/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 136/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 137/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 138/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 139/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 140/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 141/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 142/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 143/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 144/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 145/145
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009193881414830685
  1/332 [..............................] - ETA: 40s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0709023407965483
cosine 0.05565757582176868
MAE: 0.03506236
RMSE: 0.07661553
r2: 0.6192029891150342
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 128, 145, 0.0022, 0.2, 252, 0.009382654912769794, 0.009193881414830685, 0.0709023407965483, 0.05565757582176868, 0.035062361508607864, 0.07661552727222443, 0.6192029891150342, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 150 0.002 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0417 - val_loss: 0.0229 - 4s/epoch - 10ms/step
Epoch 2/150
374/374 - 2s - loss: 0.0159 - val_loss: 0.0652 - 2s/epoch - 4ms/step
Epoch 3/150
374/374 - 2s - loss: 0.0163 - val_loss: 0.0275 - 2s/epoch - 4ms/step
Epoch 4/150
374/374 - 2s - loss: 0.0147 - val_loss: 0.0231 - 2s/epoch - 4ms/step
Epoch 5/150
374/374 - 2s - loss: 0.0143 - val_loss: 0.0503 - 2s/epoch - 4ms/step
Epoch 6/150
374/374 - 2s - loss: 0.0144 - val_loss: 0.0181 - 2s/epoch - 4ms/step
Epoch 7/150
374/374 - 2s - loss: 0.0136 - val_loss: 0.0377 - 2s/epoch - 4ms/step
Epoch 8/150
374/374 - 2s - loss: 0.0134 - val_loss: 0.0162 - 2s/epoch - 4ms/step
Epoch 9/150
374/374 - 2s - loss: 0.0127 - val_loss: 0.0152 - 2s/epoch - 4ms/step
Epoch 10/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0130 - 2s/epoch - 4ms/step
Epoch 11/150
374/374 - 2s - loss: 0.0120 - val_loss: 0.0291 - 2s/epoch - 4ms/step
Epoch 12/150
374/374 - 2s - loss: 0.0123 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 13/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0212 - 2s/epoch - 4ms/step
Epoch 14/150
374/374 - 2s - loss: 0.0117 - val_loss: 0.0146 - 2s/epoch - 4ms/step
Epoch 15/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0114 - 2s/epoch - 4ms/step
Epoch 16/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 17/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 18/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 19/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 20/150
374/374 - 2s - loss: 0.0130 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 21/150
374/374 - 2s - loss: 0.0198 - val_loss: 0.0156 - 2s/epoch - 4ms/step
Epoch 22/150
374/374 - 2s - loss: 0.0236 - val_loss: 0.0154 - 2s/epoch - 4ms/step
Epoch 23/150
374/374 - 2s - loss: 0.0120 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 24/150
374/374 - 2s - loss: 0.0865 - val_loss: 0.0166 - 2s/epoch - 4ms/step
Epoch 25/150
374/374 - 2s - loss: 0.0598 - val_loss: 0.0187 - 2s/epoch - 4ms/step
Epoch 26/150
374/374 - 2s - loss: 0.0278 - val_loss: 0.0156 - 2s/epoch - 4ms/step
Epoch 27/150
374/374 - 2s - loss: 0.0151 - val_loss: 0.0140 - 2s/epoch - 4ms/step
Epoch 28/150
374/374 - 2s - loss: 0.0143 - val_loss: 0.0136 - 2s/epoch - 4ms/step
Epoch 29/150
374/374 - 2s - loss: 0.0144 - val_loss: 0.0129 - 2s/epoch - 4ms/step
Epoch 30/150
374/374 - 2s - loss: 0.0132 - val_loss: 0.0125 - 2s/epoch - 4ms/step
Epoch 31/150
374/374 - 2s - loss: 0.0128 - val_loss: 0.0122 - 2s/epoch - 4ms/step
Epoch 32/150
374/374 - 2s - loss: 0.0125 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 33/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0118 - 2s/epoch - 4ms/step
Epoch 34/150
374/374 - 2s - loss: 0.0180 - val_loss: 0.0126 - 2s/epoch - 4ms/step
Epoch 35/150
374/374 - 2s - loss: 0.0127 - val_loss: 0.0120 - 2s/epoch - 4ms/step
Epoch 36/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0118 - 2s/epoch - 4ms/step
Epoch 37/150
374/374 - 2s - loss: 0.0120 - val_loss: 0.0115 - 2s/epoch - 4ms/step
Epoch 38/150
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 4ms/step
Epoch 39/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 40/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 41/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 42/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 43/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 44/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0157 - 2s/epoch - 4ms/step
Epoch 45/150
374/374 - 2s - loss: 0.0154 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 46/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 47/150
374/374 - 2s - loss: 0.0152 - val_loss: 0.0230 - 2s/epoch - 4ms/step
Epoch 48/150
374/374 - 2s - loss: 0.0423 - val_loss: 0.0142 - 2s/epoch - 4ms/step
Epoch 49/150
374/374 - 2s - loss: 0.0152 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 50/150
374/374 - 2s - loss: 0.0129 - val_loss: 0.0120 - 2s/epoch - 4ms/step
Epoch 51/150
374/374 - 2s - loss: 0.0120 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 52/150
374/374 - 2s - loss: 0.0133 - val_loss: 0.0129 - 2s/epoch - 4ms/step
Epoch 53/150
374/374 - 2s - loss: 0.0137 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 54/150
374/374 - 2s - loss: 0.0116 - val_loss: 0.0121 - 2s/epoch - 4ms/step
Epoch 55/150
374/374 - 2s - loss: 0.0134 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 56/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 57/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0136 - 2s/epoch - 4ms/step
Epoch 58/150
374/374 - 2s - loss: 0.0187 - val_loss: 0.0118 - 2s/epoch - 4ms/step
Epoch 59/150
374/374 - 2s - loss: 0.0116 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 60/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 61/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 62/150
374/374 - 2s - loss: 0.0118 - val_loss: 0.0239 - 2s/epoch - 4ms/step
Epoch 63/150
374/374 - 2s - loss: 0.0700 - val_loss: 0.0143 - 2s/epoch - 4ms/step
Epoch 64/150
374/374 - 2s - loss: 0.0134 - val_loss: 0.0255 - 2s/epoch - 4ms/step
Epoch 65/150
374/374 - 2s - loss: 0.0462 - val_loss: 0.0145 - 2s/epoch - 4ms/step
Epoch 66/150
374/374 - 2s - loss: 0.0135 - val_loss: 0.0136 - 2s/epoch - 4ms/step
Epoch 67/150
374/374 - 2s - loss: 0.0127 - val_loss: 0.0133 - 2s/epoch - 4ms/step
Epoch 68/150
374/374 - 2s - loss: 0.0124 - val_loss: 0.0120 - 2s/epoch - 4ms/step
Epoch 69/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 4ms/step
Epoch 70/150
374/374 - 2s - loss: 0.0119 - val_loss: 0.0115 - 2s/epoch - 4ms/step
Epoch 71/150
374/374 - 2s - loss: 0.0143 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 72/150
374/374 - 2s - loss: 0.0118 - val_loss: 0.0118 - 2s/epoch - 4ms/step
Epoch 73/150
374/374 - 2s - loss: 0.0124 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 74/150
374/374 - 2s - loss: 0.0117 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 75/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 76/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0133 - 2s/epoch - 4ms/step
Epoch 77/150
374/374 - 2s - loss: 0.0128 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 78/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 79/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 80/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 4ms/step
Epoch 81/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 82/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0152 - 2s/epoch - 4ms/step
Epoch 83/150
374/374 - 2s - loss: 0.0137 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 84/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 85/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 86/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 87/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 88/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 89/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 90/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0151 - 2s/epoch - 4ms/step
Epoch 91/150
374/374 - 2s - loss: 0.0116 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 92/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 93/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 94/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 95/150
374/374 - 2s - loss: 0.0134 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 96/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 97/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 98/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 99/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 100/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0116 - 2s/epoch - 4ms/step
Epoch 101/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 102/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 103/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 104/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 105/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 106/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 107/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 108/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 109/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 110/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0121 - 2s/epoch - 4ms/step
Epoch 111/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 112/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 113/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 114/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 115/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0135 - 2s/epoch - 4ms/step
Epoch 116/150
374/374 - 2s - loss: 0.0153 - val_loss: 0.0176 - 2s/epoch - 4ms/step
Epoch 117/150
374/374 - 2s - loss: 0.0249 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 118/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 119/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 120/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 121/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 122/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 123/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0139 - 2s/epoch - 4ms/step
Epoch 124/150
374/374 - 2s - loss: 0.0339 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 125/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 126/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 127/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 128/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0177 - 2s/epoch - 4ms/step
Epoch 129/150
374/374 - 2s - loss: 0.0312 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 130/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 131/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 132/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 133/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 134/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 135/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 136/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 137/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 138/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 139/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 140/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 141/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 142/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 143/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 144/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 145/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0162 - 2s/epoch - 4ms/step
Epoch 146/150
374/374 - 2s - loss: 0.0116 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 147/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0107 - 2s/epoch - 4ms/step
Epoch 148/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 149/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 150/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009783081710338593
  1/332 [..............................] - ETA: 36s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07832592350556812
cosine 0.061461304954389694
MAE: 0.036580373
RMSE: 0.080302455
r2: 0.5816714149781398
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 256, 150, 0.002, 0.2, 252, 0.009917168878018856, 0.009783081710338593, 0.07832592350556812, 0.061461304954389694, 0.03658037260174751, 0.08030245453119278, 0.5816714149781398, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 150 0.0022 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0411 - val_loss: 0.0190 - 4s/epoch - 10ms/step
Epoch 2/150
374/374 - 1s - loss: 0.0166 - val_loss: 0.0229 - 1s/epoch - 4ms/step
Epoch 3/150
374/374 - 1s - loss: 0.0154 - val_loss: 0.0185 - 1s/epoch - 4ms/step
Epoch 4/150
374/374 - 1s - loss: 0.0152 - val_loss: 0.0298 - 1s/epoch - 4ms/step
Epoch 5/150
374/374 - 1s - loss: 0.0145 - val_loss: 0.0439 - 1s/epoch - 4ms/step
Epoch 6/150
374/374 - 1s - loss: 0.0141 - val_loss: 0.0160 - 1s/epoch - 4ms/step
Epoch 7/150
374/374 - 1s - loss: 0.0130 - val_loss: 0.0156 - 1s/epoch - 4ms/step
Epoch 8/150
374/374 - 1s - loss: 0.0126 - val_loss: 0.0154 - 1s/epoch - 4ms/step
Epoch 9/150
374/374 - 1s - loss: 0.0123 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 10/150
374/374 - 1s - loss: 0.0120 - val_loss: 0.4379 - 1s/epoch - 4ms/step
Epoch 11/150
374/374 - 1s - loss: 0.0192 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 12/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0145 - 1s/epoch - 4ms/step
Epoch 13/150
374/374 - 1s - loss: 0.0123 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 14/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0233 - 1s/epoch - 4ms/step
Epoch 15/150
374/374 - 1s - loss: 0.0124 - val_loss: 0.0306 - 1s/epoch - 4ms/step
Epoch 16/150
374/374 - 1s - loss: 0.0155 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 17/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 18/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 19/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 20/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 21/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0140 - 1s/epoch - 4ms/step
Epoch 22/150
374/374 - 1s - loss: 0.0127 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 23/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 24/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 25/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 26/150
374/374 - 1s - loss: 0.0126 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 27/150
374/374 - 1s - loss: 0.0126 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 28/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 29/150
374/374 - 1s - loss: 0.0120 - val_loss: 0.0132 - 1s/epoch - 4ms/step
Epoch 30/150
374/374 - 1s - loss: 0.0158 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 31/150
374/374 - 1s - loss: 0.0179 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 32/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 33/150
374/374 - 1s - loss: 0.0165 - val_loss: 0.0148 - 1s/epoch - 4ms/step
Epoch 34/150
374/374 - 1s - loss: 0.0424 - val_loss: 0.0136 - 1s/epoch - 4ms/step
Epoch 35/150
374/374 - 1s - loss: 0.0121 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 36/150
374/374 - 1s - loss: 0.0166 - val_loss: 0.0190 - 1s/epoch - 4ms/step
Epoch 37/150
374/374 - 1s - loss: 0.0648 - val_loss: 0.0135 - 1s/epoch - 4ms/step
Epoch 38/150
374/374 - 1s - loss: 0.0126 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 39/150
374/374 - 1s - loss: 0.0121 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 40/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 41/150
374/374 - 1s - loss: 0.0117 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 42/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 43/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 44/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 45/150
374/374 - 1s - loss: 0.0130 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 46/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 47/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 48/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0220 - 1s/epoch - 4ms/step
Epoch 49/150
374/374 - 1s - loss: 0.0374 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 50/150
374/374 - 1s - loss: 0.0123 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 51/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 52/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 53/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 54/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 55/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 56/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 57/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 58/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 59/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 60/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 61/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0186 - 1s/epoch - 4ms/step
Epoch 62/150
374/374 - 1s - loss: 0.0162 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 63/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 64/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 65/150
374/374 - 1s - loss: 0.0246 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 66/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 67/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 68/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 69/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 70/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 71/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 72/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 73/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 74/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 75/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 76/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 77/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 78/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 79/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 80/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 81/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 82/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0189 - 1s/epoch - 4ms/step
Epoch 83/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 84/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 85/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 86/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 87/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 88/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 89/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 90/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 91/150
374/374 - 1s - loss: 0.0119 - val_loss: 0.0149 - 1s/epoch - 4ms/step
Epoch 92/150
374/374 - 1s - loss: 0.0192 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 93/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 94/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 95/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 96/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 97/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0169 - 1s/epoch - 4ms/step
Epoch 98/150
374/374 - 1s - loss: 0.0185 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 99/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 100/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 101/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 102/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 103/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 104/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 105/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 106/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 107/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 108/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 109/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 110/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 111/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 112/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 113/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 114/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 115/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 116/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 117/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 118/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 119/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0144 - 1s/epoch - 4ms/step
Epoch 120/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0175 - 1s/epoch - 4ms/step
Epoch 121/150
374/374 - 1s - loss: 0.0214 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 122/150
374/374 - 1s - loss: 0.0160 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 123/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 124/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 125/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 126/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 127/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0177 - 1s/epoch - 4ms/step
Epoch 128/150
374/374 - 1s - loss: 0.0216 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 129/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 130/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0213 - 1s/epoch - 4ms/step
Epoch 131/150
374/374 - 1s - loss: 0.0305 - val_loss: 0.0211 - 1s/epoch - 4ms/step
Epoch 132/150
374/374 - 1s - loss: 0.0284 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 133/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 134/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 135/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 136/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 137/150
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 138/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 139/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 140/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 141/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 142/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 143/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 144/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 145/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 146/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 147/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 148/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 149/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0213 - 1s/epoch - 4ms/step
Epoch 150/150
374/374 - 1s - loss: 0.0142 - val_loss: 0.0105 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.010451255366206169
  1/332 [..............................] - ETA: 38s 42/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s213/332 [==================>...........] - ETA: 0s256/332 [======================>.......] - ETA: 0s299/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08016467102284797
cosine 0.0629042454746324
MAE: 0.037231855
RMSE: 0.081176855
r2: 0.5725115519773001
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'mse', 256, 150, 0.0022, 0.2, 252, 0.014190866611897945, 0.010451255366206169, 0.08016467102284797, 0.0629042454746324, 0.03723185509443283, 0.08117685467004776, 0.5725115519773001, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 150 0.0022 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-17 19:37:43.845393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 447873024/23676715008
2023-02-17 19:37:43.845434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22184537428
MaxInUse:                  22258275001
NumAllocs:                   144856035
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:37:43.845496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:37:43.845500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 110
2023-02-17 19:37:43.845504: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 92
2023-02-17 19:37:43.845506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 378
2023-02-17 19:37:43.845509: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:37:43.845512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 88
2023-02-17 19:37:43.845515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:37:43.845518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:37:43.845520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:37:43.845523: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:37:43.845526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:37:43.845528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 66
2023-02-17 19:37:43.845531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 22
2023-02-17 19:37:43.845534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:37:43.845536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 66
2023-02-17 19:37:43.845539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:37:43.845542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:37:43.845544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:37:43.845547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:37:43.845550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:37:43.845552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:37:43.845555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 27
2023-02-17 19:37:43.845558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 9
2023-02-17 19:37:43.845560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:37:43.845563: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:37:43.845573: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:37:43.845575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:37:43.845578: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:37:43.845581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:37:43.845583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:37:43.845586: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 18
2023-02-17 19:37:43.845589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 6
2023-02-17 19:37:43.845592: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:37:43.845594: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:37:43.845597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 150, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 150 0.0022 256 1]) is not valid.
[2.1 150 0.002 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.1_cr0.2_bs256_ep150_loss_mse_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 34s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.07832592350556812
cosine 0.061461304954389694
MAE: 0.036580373
RMSE: 0.080302455
r2: 0.5816714149781398
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.1custom_VAE', 'mse', 256, 150, 0.002, 0.2, 252, '--', '--', 0.07832592350556812, 0.061461304954389694, 0.03658037260174751, 0.08030245453119278, 0.5816714149781398, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[2.1 210 0.001 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:37:56.807707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 313655296/23676715008
2023-02-17 19:37:56.807745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22360624660
MaxInUse:                  22486356241
NumAllocs:                   144860958
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:37:56.807810: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:37:56.807816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:37:56.807819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:37:56.807822: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 409
2023-02-17 19:37:56.807825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:37:56.807828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 93
2023-02-17 19:37:56.807832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:37:56.807835: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:37:56.807838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:37:56.807840: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:37:56.807843: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:37:56.807846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 108
2023-02-17 19:37:56.807849: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 22
2023-02-17 19:37:56.807852: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:37:56.807855: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 71
2023-02-17 19:37:56.807858: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:37:56.807861: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:37:56.807864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:37:56.807874: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:37:56.807877: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:37:56.807880: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:37:56.807883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 42
2023-02-17 19:37:56.807886: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 9
2023-02-17 19:37:56.807889: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:37:56.807892: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:37:56.807895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:37:56.807897: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:37:56.807900: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:37:56.807903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:37:56.807906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:37:56.807909: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 28
2023-02-17 19:37:56.807912: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 6
2023-02-17 19:37:56.807915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:37:56.807918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:37:56.807921: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 256, 210, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 210 0.001 256 1]) is not valid.
[2.1 210 0.0022 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:37:58.545420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 280100864/23676715008
2023-02-17 19:37:58.545457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22360624660
MaxInUse:                  22486356241
NumAllocs:                   144861014
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:37:58.545520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:37:58.545526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:37:58.545529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:37:58.545532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 409
2023-02-17 19:37:58.545535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:37:58.545538: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 93
2023-02-17 19:37:58.545541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:37:58.545544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:37:58.545547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:37:58.545550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:37:58.545553: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:37:58.545556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 108
2023-02-17 19:37:58.545559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 22
2023-02-17 19:37:58.545562: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:37:58.545565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 71
2023-02-17 19:37:58.545568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:37:58.545578: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:37:58.545582: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:37:58.545584: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:37:58.545587: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:37:58.545590: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:37:58.545593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 42
2023-02-17 19:37:58.545596: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 9
2023-02-17 19:37:58.545599: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:37:58.545602: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:37:58.545605: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:37:58.545608: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:37:58.545611: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:37:58.545614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:37:58.545617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:37:58.545620: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 28
2023-02-17 19:37:58.545622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 6
2023-02-17 19:37:58.545625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:37:58.545628: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:37:58.545630: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 64, 210, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 210 0.0022 64 1]) is not valid.
[2.1 145 0.0022 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:00.264387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 280100864/23676715008
2023-02-17 19:38:00.264425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22395859892
MaxInUse:                  22486356241
NumAllocs:                   144861070
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:00.264491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:00.264497: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:00.264500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:00.264502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 416
2023-02-17 19:38:00.264505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:00.264508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 94
2023-02-17 19:38:00.264511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:00.264513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:00.264516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:00.264519: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:00.264522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:38:00.264524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 118
2023-02-17 19:38:00.264527: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 22
2023-02-17 19:38:00.264530: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:00.264539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 72
2023-02-17 19:38:00.264542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:00.264545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:00.264548: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:00.264550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:00.264553: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:00.264556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:38:00.264558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 45
2023-02-17 19:38:00.264561: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 9
2023-02-17 19:38:00.264564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:00.264566: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:00.264569: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:00.264572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:00.264575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:00.264577: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:00.264580: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:38:00.264583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 30
2023-02-17 19:38:00.264585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 6
2023-02-17 19:38:00.264588: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:00.264591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:00.264593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 256, 145, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 145 0.0022 256 1]) is not valid.
[2.1 145 0.0022 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:02.051841: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 246546432/23676715008
2023-02-17 19:38:02.051875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22395859892
MaxInUse:                  22486356241
NumAllocs:                   144861126
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:02.051945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:02.051950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:02.051954: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:02.051957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 416
2023-02-17 19:38:02.051960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:02.051964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 94
2023-02-17 19:38:02.051967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:02.051970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:02.051973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:02.051976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:02.051979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:38:02.051982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 118
2023-02-17 19:38:02.051991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 22
2023-02-17 19:38:02.051994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:02.051997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 72
2023-02-17 19:38:02.052000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:02.052003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:02.052006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:02.052009: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:02.052012: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:02.052015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:38:02.052017: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 45
2023-02-17 19:38:02.052020: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 9
2023-02-17 19:38:02.052023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:02.052026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:02.052029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:02.052032: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:02.052035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:02.052038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:02.052041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:38:02.052044: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 30
2023-02-17 19:38:02.052047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 6
2023-02-17 19:38:02.052050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:02.052053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:02.052055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 64, 145, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 145 0.0022 64 1]) is not valid.
[2.1 150 0.0024000000000000002 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:03.769348: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 246546432/23676715008
2023-02-17 19:38:03.769385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22431095124
MaxInUse:                  22486356241
NumAllocs:                   144861182
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:03.769450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:03.769455: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:03.769459: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:03.769462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 423
2023-02-17 19:38:03.769465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:03.769468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 95
2023-02-17 19:38:03.769471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:03.769474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:03.769477: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:03.769480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:03.769490: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:38:03.769493: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 128
2023-02-17 19:38:03.769496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 22
2023-02-17 19:38:03.769499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:03.769502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 73
2023-02-17 19:38:03.769505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:03.769508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:03.769511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:03.769514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:03.769517: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:03.769520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:38:03.769523: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 48
2023-02-17 19:38:03.769526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 9
2023-02-17 19:38:03.769529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:03.769532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:03.769534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:03.769537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:03.769540: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:03.769543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:03.769546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:38:03.769549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 32
2023-02-17 19:38:03.769552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 6
2023-02-17 19:38:03.769555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:03.769558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:03.769561: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 256, 150, 0.0024000000000000002, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 150 0.0024000000000000002 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_3.pkl
[2.2 145 0.0022 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
2023-02-17 19:38:11.951326: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 179437568/23676715008
2023-02-17 19:38:11.951364: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22432755300
MaxInUse:                  22496096852
NumAllocs:                   144861238
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:11.951428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:11.951433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:11.951437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:11.951440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 423
2023-02-17 19:38:11.951443: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:11.951446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 95
2023-02-17 19:38:11.951449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:11.951459: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:11.951462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:11.951465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:11.951468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:38:11.951471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 118
2023-02-17 19:38:11.951474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:11.951477: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:11.951480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 73
2023-02-17 19:38:11.951483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:11.951486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:11.951489: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:11.951492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:11.951495: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:11.951498: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:38:11.951500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 45
2023-02-17 19:38:11.951503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:11.951506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:11.951509: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:11.951512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:11.951515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:11.951518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:11.951521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:11.951524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:38:11.951527: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 30
2023-02-17 19:38:11.951529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:11.951532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:11.951535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:11.951538: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.2custom_VAE', 'mse', 128, 145, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.2 145 0.0022 128 1]) is not valid.
[2.2 145 0.0008 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
2023-02-17 19:38:13.673078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 179437568/23676715008
2023-02-17 19:38:13.673116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22469650708
MaxInUse:                  22497757028
NumAllocs:                   144861294
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:13.673185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:13.673190: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:13.673193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:13.673196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 430
2023-02-17 19:38:13.673199: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:13.673209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 96
2023-02-17 19:38:13.673212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:13.673215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:13.673217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:13.673220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:13.673223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:38:13.673225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 118
2023-02-17 19:38:13.673228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 42
2023-02-17 19:38:13.673231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:13.673234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 74
2023-02-17 19:38:13.673236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:13.673239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:13.673242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:13.673244: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:13.673247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:13.673250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:38:13.673253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 45
2023-02-17 19:38:13.673255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 15
2023-02-17 19:38:13.673258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:13.673261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:13.673263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:13.673266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:13.673269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:13.673271: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:13.673274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:38:13.673277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 30
2023-02-17 19:38:13.673279: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 10
2023-02-17 19:38:13.673282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:13.673285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:13.673287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.2custom_VAE', 'mse', 256, 145, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.2 145 0.0008 256 1]) is not valid.
[2.1 210 0.0008 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:15.403158: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 145883136/23676715008
2023-02-17 19:38:15.403194: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22467990532
MaxInUse:                  22531718148
NumAllocs:                   144861350
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:15.403259: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:15.403264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:15.403268: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:15.403277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 430
2023-02-17 19:38:15.403280: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:15.403283: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 96
2023-02-17 19:38:15.403285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:15.403288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:15.403291: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:15.403293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:15.403296: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:38:15.403299: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 128
2023-02-17 19:38:15.403301: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:15.403304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:15.403307: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 74
2023-02-17 19:38:15.403310: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:15.403312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:15.403315: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:15.403318: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:15.403320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:15.403323: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:38:15.403326: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 48
2023-02-17 19:38:15.403328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:15.403331: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:15.403334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:15.403336: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:15.403339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:15.403342: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:15.403344: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:15.403347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:38:15.403350: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 32
2023-02-17 19:38:15.403352: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:15.403355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:15.403358: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:15.403360: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 256, 210, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 210 0.0008 256 1]) is not valid.
[2.1 210 0.0008 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:17.119604: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 145883136/23676715008
2023-02-17 19:38:17.119641: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22503225764
MaxInUse:                  22531718148
NumAllocs:                   144861406
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:17.119708: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:17.119721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:17.119725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:17.119727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 437
2023-02-17 19:38:17.119730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:17.119733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 97
2023-02-17 19:38:17.119736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:17.119739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:17.119741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:17.119744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:17.119747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-17 19:38:17.119749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 138
2023-02-17 19:38:17.119752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:17.119755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:17.119758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 75
2023-02-17 19:38:17.119760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:17.119763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:17.119766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:17.119769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:17.119771: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:17.119774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-17 19:38:17.119777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 51
2023-02-17 19:38:17.119779: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:17.119782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:17.119785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:17.119788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:17.119790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:17.119793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:17.119796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:17.119798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-17 19:38:17.119801: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 34
2023-02-17 19:38:17.119804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:17.119807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:17.119809: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:17.119812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 256, 210, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 210 0.0008 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_4.pkl
[2.0 210 0.0022 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-17 19:38:24.648436: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 145883136/23676715008
2023-02-17 19:38:24.648473: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22501565588
MaxInUse:                  22562358916
NumAllocs:                   144861462
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:24.648541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:24.648546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:24.648549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:24.648552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 437
2023-02-17 19:38:24.648555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:24.648557: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 97
2023-02-17 19:38:24.648560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:24.648563: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:24.648565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:24.648568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:24.648571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 152
2023-02-17 19:38:24.648574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 128
2023-02-17 19:38:24.648576: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:24.648579: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:24.648582: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 75
2023-02-17 19:38:24.648584: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:24.648587: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:24.648590: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:24.648593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:24.648595: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:24.648598: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 60
2023-02-17 19:38:24.648601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 48
2023-02-17 19:38:24.648603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:24.648606: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:24.648609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:24.648611: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:24.648614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:24.648617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:24.648620: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:24.648622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 40
2023-02-17 19:38:24.648625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 32
2023-02-17 19:38:24.648628: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:24.648630: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:24.648633: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:24.648636: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 64, 210, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 210 0.0022 64 1]) is not valid.
[2.0 205 0.0006000000000000001 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-17 19:38:26.386131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 145883136/23676715008
2023-02-17 19:38:26.386167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22535140644
MaxInUse:                  22562358916
NumAllocs:                   144861518
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:26.386236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:26.386241: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:26.386244: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:26.386247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 444
2023-02-17 19:38:26.386250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:26.386253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 98
2023-02-17 19:38:26.386255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:26.386258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:26.386261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:26.386263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:26.386266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 162
2023-02-17 19:38:26.386269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 128
2023-02-17 19:38:26.386271: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:26.386274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:26.386277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 76
2023-02-17 19:38:26.386280: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:26.386282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:26.386285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:26.386288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:26.386290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:26.386293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 63
2023-02-17 19:38:26.386296: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 48
2023-02-17 19:38:26.386298: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:26.386301: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:26.386304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:26.386307: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:26.386309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:26.386312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:26.386315: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:26.386317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 42
2023-02-17 19:38:26.386320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 32
2023-02-17 19:38:26.386323: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:26.386325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:26.386330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:26.386333: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 205, 0.0006000000000000001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 205 0.0006000000000000001 256 1]) is not valid.
[2.1 205 0.0008 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:28.108174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 112328704/23676715008
2023-02-17 19:38:28.108216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22536800820
MaxInUse:                  22597208084
NumAllocs:                   144861574
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:28.108281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:28.108287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:28.108290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:28.108292: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 444
2023-02-17 19:38:28.108295: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:28.108298: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 98
2023-02-17 19:38:28.108301: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:28.108303: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:28.108306: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:28.108308: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:28.108311: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 152
2023-02-17 19:38:28.108314: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 138
2023-02-17 19:38:28.108316: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:28.108319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:28.108322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 76
2023-02-17 19:38:28.108325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:28.108327: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:28.108330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:28.108333: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:28.108335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:28.108338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 60
2023-02-17 19:38:28.108341: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 51
2023-02-17 19:38:28.108343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:28.108346: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:28.108349: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:28.108351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:28.108354: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:28.108357: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:28.108359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:28.108362: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 40
2023-02-17 19:38:28.108364: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 34
2023-02-17 19:38:28.108369: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:28.108372: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:28.108375: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:28.108377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 256, 205, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 205 0.0008 256 1]) is not valid.
[1.9 210 0.0008 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
2023-02-17 19:38:29.828532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 112328704/23676715008
2023-02-17 19:38:29.828570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22568702524
MaxInUse:                  22597208084
NumAllocs:                   144861630
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:29.828634: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:29.828639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:29.828642: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:29.828645: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 451
2023-02-17 19:38:29.828648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:29.828651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 99
2023-02-17 19:38:29.828653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:29.828656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:29.828659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:29.828661: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 32
2023-02-17 19:38:29.828664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 152
2023-02-17 19:38:29.828667: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 138
2023-02-17 19:38:29.828669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:29.828672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:29.828675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 77
2023-02-17 19:38:29.828681: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:29.828684: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:29.828686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:29.828689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:29.828692: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 12
2023-02-17 19:38:29.828695: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 60
2023-02-17 19:38:29.828697: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 51
2023-02-17 19:38:29.828700: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:29.828703: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:29.828705: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:29.828708: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:29.828711: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:29.828713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:29.828716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 8
2023-02-17 19:38:29.828725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 40
2023-02-17 19:38:29.828728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 34
2023-02-17 19:38:29.828731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:29.828734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:29.828736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:29.828739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.9custom_VAE', 'mse', 64, 210, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.9 210 0.0008 64 1]) is not valid.
[2.0 205 0.001 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-17 19:38:31.555991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 78774272/23676715008
2023-02-17 19:38:31.556030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22570375876
MaxInUse:                  22627835676
NumAllocs:                   144861686
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:31.556099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:31.556105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:31.556108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:31.556110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 451
2023-02-17 19:38:31.556113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:31.556116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 99
2023-02-17 19:38:31.556119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:31.556122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:31.556124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:31.556127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:31.556130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 162
2023-02-17 19:38:31.556133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 138
2023-02-17 19:38:31.556135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:31.556138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:31.556141: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 77
2023-02-17 19:38:31.556143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:31.556146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:31.556149: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:31.556152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:31.556154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:31.556157: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 63
2023-02-17 19:38:31.556160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 51
2023-02-17 19:38:31.556162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:31.556165: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:31.556168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:31.556170: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:31.556173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:31.556183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:31.556186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:31.556189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 42
2023-02-17 19:38:31.556192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 34
2023-02-17 19:38:31.556194: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:31.556197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:31.556200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:31.556202: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 205, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 205 0.001 256 1]) is not valid.
[2.1 145 0.0008 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:33.317778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 78774272/23676715008
2023-02-17 19:38:33.317815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22605611108
MaxInUse:                  22632443316
NumAllocs:                   144861742
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:33.317882: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:33.317887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:33.317890: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:33.317893: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 458
2023-02-17 19:38:33.317896: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:33.317899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 100
2023-02-17 19:38:33.317901: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:33.317904: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:33.317907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:33.317910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:33.317912: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 162
2023-02-17 19:38:33.317915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 148
2023-02-17 19:38:33.317918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:33.317920: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:33.317923: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 78
2023-02-17 19:38:33.317926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:33.317929: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:33.317931: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:33.317934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:33.317937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:33.317939: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 63
2023-02-17 19:38:33.317942: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-17 19:38:33.317945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:33.317947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:33.317950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:33.317960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:33.317963: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:33.317965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:33.317968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:33.317971: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 42
2023-02-17 19:38:33.317973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-17 19:38:33.317976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:33.317979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:33.317982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:33.317984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 64, 145, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 145 0.0008 64 1]) is not valid.
[2.0 205 0.0008 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-17 19:38:35.048604: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 45219840/23676715008
2023-02-17 19:38:35.048641: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22603950932
MaxInUse:                  22664744260
NumAllocs:                   144861798
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:35.048717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:35.048722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:35.048725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:35.048728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 458
2023-02-17 19:38:35.048731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:35.048734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 100
2023-02-17 19:38:35.048737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:35.048739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:35.048742: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:35.048745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:35.048748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 172
2023-02-17 19:38:35.048750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 138
2023-02-17 19:38:35.048753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:35.048756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:35.048758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 78
2023-02-17 19:38:35.048761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:35.048764: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:35.048767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:35.048769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:35.048772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:35.048775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 66
2023-02-17 19:38:35.048777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 51
2023-02-17 19:38:35.048780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:35.048789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:35.048792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:35.048795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:35.048798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:35.048800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:35.048803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:35.048806: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 44
2023-02-17 19:38:35.048808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 34
2023-02-17 19:38:35.048811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:35.048814: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:35.048817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:35.048819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 205, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 205 0.0008 256 1]) is not valid.
[2.0 205 0.001 256 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-17 19:38:36.770390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 45219840/23676715008
2023-02-17 19:38:36.770427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22637525988
MaxInUse:                  22664744260
NumAllocs:                   144861854
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:36.770494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:36.770499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:36.770502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:36.770505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 465
2023-02-17 19:38:36.770507: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:36.770510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 101
2023-02-17 19:38:36.770513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:36.770516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:36.770518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:36.770521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:36.770524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 182
2023-02-17 19:38:36.770526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 138
2023-02-17 19:38:36.770529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:36.770532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:36.770534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 79
2023-02-17 19:38:36.770537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:36.770540: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:36.770542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:36.770545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:36.770548: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:36.770550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 69
2023-02-17 19:38:36.770561: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 51
2023-02-17 19:38:36.770564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:36.770567: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:36.770569: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:36.770572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:36.770575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:36.770577: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:36.770580: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:36.770583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 46
2023-02-17 19:38:36.770585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 34
2023-02-17 19:38:36.770588: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:36.770591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:36.770593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:36.770596: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'logcosh', 256, 205, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 205 0.001 256 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_5.pkl
[2.1 145 0.0022 128 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-17 19:38:44.204625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 11665408/23676715008
2023-02-17 19:38:44.204657: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22639186164
MaxInUse:                  22699593428
NumAllocs:                   144861910
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:44.204730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:44.204735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:44.204738: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:44.204741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 465
2023-02-17 19:38:44.204744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:44.204746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 101
2023-02-17 19:38:44.204749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:44.204752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:44.204755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:44.204758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:44.204761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 172
2023-02-17 19:38:44.204763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 148
2023-02-17 19:38:44.204766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:44.204769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:44.204772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 79
2023-02-17 19:38:44.204774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:44.204777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:44.204780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:44.204782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:44.204792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:44.204795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 66
2023-02-17 19:38:44.204798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-17 19:38:44.204800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:44.204803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:44.204806: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:44.204808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:44.204811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:44.204814: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:44.204816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:44.204819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 44
2023-02-17 19:38:44.204822: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-17 19:38:44.204824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:44.204827: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:44.204830: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:44.204832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'logcosh', 128, 145, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 145 0.0022 128 2]) is not valid.
[2.0 145 0.0022 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-17 19:38:45.927970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 11665408/23676715008
2023-02-17 19:38:45.928007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22672761220
MaxInUse:                  22699593428
NumAllocs:                   144861966
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:45.928072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:45.928077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:45.928080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-17 19:38:45.928083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 472
2023-02-17 19:38:45.928086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:45.928088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 102
2023-02-17 19:38:45.928091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:45.928094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:45.928097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:45.928099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:45.928102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 182
2023-02-17 19:38:45.928105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 148
2023-02-17 19:38:45.928108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:45.928110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:45.928113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 80
2023-02-17 19:38:45.928116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:45.928118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:45.928129: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:45.928132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:45.928135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:45.928137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 69
2023-02-17 19:38:45.928140: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-17 19:38:45.928143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:45.928145: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:45.928148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:45.928151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:45.928153: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:45.928156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:45.928159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 6
2023-02-17 19:38:45.928161: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 46
2023-02-17 19:38:45.928164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-17 19:38:45.928167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:45.928169: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:45.928172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:45.928175: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 128, 145, 0.0022, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 145 0.0022 128 1]) is not valid.
[1.9 210 0.0008 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-17 19:38:47.279524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 12139456 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 11665408/23676715008
2023-02-17 19:38:47.279553: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22697040156
MaxInUse:                  22699593428
NumAllocs:                   144861971
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-17 19:38:47.279617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-17 19:38:47.279622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 114
2023-02-17 19:38:47.279625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 95
2023-02-17 19:38:47.279628: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-17 19:38:47.279637: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 472
2023-02-17 19:38:47.279640: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-17 19:38:47.279643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 102
2023-02-17 19:38:47.279646: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-17 19:38:47.279648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-17 19:38:47.279651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-17 19:38:47.279654: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 22
2023-02-17 19:38:47.279656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 182
2023-02-17 19:38:47.279659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 148
2023-02-17 19:38:47.279662: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-17 19:38:47.279664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 76
2023-02-17 19:38:47.279667: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 80
2023-02-17 19:38:47.279670: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 9
2023-02-17 19:38:47.279672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-17 19:38:47.279675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 39
2023-02-17 19:38:47.279683: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-17 19:38:47.279685: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 9
2023-02-17 19:38:47.279688: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 69
2023-02-17 19:38:47.279691: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-17 19:38:47.279693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 12
2023-02-17 19:38:47.279696: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 30
2023-02-17 19:38:47.279699: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 6
2023-02-17 19:38:47.279701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-17 19:38:47.279704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-17 19:38:47.279707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-17 19:38:47.279709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 8
2023-02-17 19:38:47.279712: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 46
2023-02-17 19:38:47.279715: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-17 19:38:47.279717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-17 19:38:47.279720: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 20
2023-02-17 19:38:47.279723: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 32
2023-02-17 19:38:47.279725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
2023-02-17 19:38:47.279745: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory
Traceback (most recent call last):
  File "genetic.py", line 20, in <module>
    genetic_hypertune_autoencoder(prefix_name = 'geneticVAE_MMmp_gap',
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.2/../../genetic_hypertune.py", line 206, in genetic_hypertune_autoencoder
    ga_instance.run()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1409, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.2/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 352, in train_autoencoder
    model = create_autoencoder(n_inputs=n_inputs, layers_structure=layers_structure, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 545, in create_autoencoder
    e = Dense(neurons_per_layer[i], name=f'dense_enc{i}')(input_layer)
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2]
Fri Feb 17 19:39:02 CET 2023
done
