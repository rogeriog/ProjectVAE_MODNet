start
Sat Feb 18 15:33:46 CET 2023
2023-02-18 15:33:48.328139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-18 15:33:48.485075: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-18 15:34:39,080 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f2db3fb4fd0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
[2.0 210 0.0022 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-18 15:34:48.204764: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-18 15:34:48.725220: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-18 15:34:48.728676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:37:00.0, compute capability: 7.0
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/210
2023-02-18 15:34:52.848023: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f2a5c016480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-18 15:34:52.848060: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2023-02-18 15:34:52.853349: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-18 15:34:53.864094: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1493/1493 - 14s - loss: 0.0256 - val_loss: 0.0166 - 14s/epoch - 9ms/step
Epoch 2/210
1493/1493 - 8s - loss: 0.0146 - val_loss: 0.0140 - 8s/epoch - 5ms/step
Epoch 3/210
1493/1493 - 8s - loss: 0.0131 - val_loss: 0.0130 - 8s/epoch - 5ms/step
Epoch 4/210
1493/1493 - 8s - loss: 0.0122 - val_loss: 0.0116 - 8s/epoch - 5ms/step
Epoch 5/210
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 6/210
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 7/210
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 8/210
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 9/210
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 10/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 11/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 12/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 13/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 14/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 15/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 16/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 17/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 18/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 19/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 20/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 21/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 22/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 23/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 24/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 25/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 26/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 27/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 28/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 29/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 30/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 31/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 32/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 33/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 34/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 35/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 36/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 37/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 38/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 39/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 40/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 41/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 42/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 43/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 44/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 45/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 46/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 47/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 48/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 49/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 50/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 51/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 52/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 53/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 54/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 55/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 56/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 57/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 58/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 59/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 60/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 61/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 62/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 63/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 64/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 65/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 66/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 67/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 68/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 69/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 70/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 71/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 72/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 73/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 74/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 75/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 76/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 77/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 78/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 79/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 80/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 81/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 82/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 83/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 84/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 85/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 86/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 87/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 88/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 89/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 90/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 91/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 92/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 93/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 94/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 95/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 96/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 97/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 98/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 99/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 100/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 101/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 102/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 103/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 104/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 105/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 106/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 107/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 108/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 109/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 110/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 111/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 112/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 113/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 114/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 115/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 116/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 117/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 118/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 119/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 120/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 121/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 122/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 123/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 124/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 125/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 126/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 127/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 128/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 129/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 130/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 131/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 132/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 133/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 134/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 135/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 136/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 137/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 138/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 139/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 140/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 141/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 142/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 143/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 144/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 145/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 146/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 147/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 148/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 149/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 150/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 151/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 152/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 153/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 154/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 155/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 156/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 157/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 158/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 159/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 160/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 161/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 162/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 163/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 164/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 165/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 166/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0092 - 9s/epoch - 6ms/step
Epoch 167/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 168/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0092 - 9s/epoch - 6ms/step
Epoch 169/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 170/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 171/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 172/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 173/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 174/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 175/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 176/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 177/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 178/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 179/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 180/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 181/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 182/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 183/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 184/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 185/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 186/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 187/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 188/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 189/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 190/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 191/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 192/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 193/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 194/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 195/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 196/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 197/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 198/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 199/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 200/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 201/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 202/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 203/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 204/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 205/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0092 - 9s/epoch - 6ms/step
Epoch 206/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0092 - 9s/epoch - 6ms/step
Epoch 207/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0092 - 9s/epoch - 6ms/step
Epoch 208/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 209/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 210/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009242949075996876
  1/332 [..............................] - ETA: 56s 33/332 [=>............................] - ETA: 0s  65/332 [====>.........................] - ETA: 0s 97/332 [=======>......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s162/332 [=============>................] - ETA: 0s195/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s261/332 [======================>.......] - ETA: 0s294/332 [=========================>....] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0715861239614821
cosine 0.05622537289751371
MAE: 0.035088383
RMSE: 0.076992154
r2: 0.6154498898967923
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 64, 210, 0.0022, 0.2, 252, 0.009462160058319569, 0.009242949075996876, 0.0715861239614821, 0.05622537289751371, 0.03508838266134262, 0.07699215412139893, 0.6154498898967923, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 205 0.0006000000000000001 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/205
374/374 - 6s - loss: 0.0398 - val_loss: 0.0206 - 6s/epoch - 15ms/step
Epoch 2/205
374/374 - 2s - loss: 0.0157 - val_loss: 0.0202 - 2s/epoch - 6ms/step
Epoch 3/205
374/374 - 2s - loss: 0.0157 - val_loss: 0.0177 - 2s/epoch - 6ms/step
Epoch 4/205
374/374 - 2s - loss: 0.0143 - val_loss: 0.0207 - 2s/epoch - 6ms/step
Epoch 5/205
374/374 - 2s - loss: 0.0140 - val_loss: 0.0152 - 2s/epoch - 6ms/step
Epoch 6/205
374/374 - 2s - loss: 0.0135 - val_loss: 0.0201 - 2s/epoch - 6ms/step
Epoch 7/205
374/374 - 2s - loss: 0.0133 - val_loss: 0.0159 - 2s/epoch - 6ms/step
Epoch 8/205
374/374 - 2s - loss: 0.0129 - val_loss: 0.0156 - 2s/epoch - 6ms/step
Epoch 9/205
374/374 - 2s - loss: 0.0125 - val_loss: 0.0291 - 2s/epoch - 6ms/step
Epoch 10/205
374/374 - 2s - loss: 0.0126 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 11/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0211 - 2s/epoch - 6ms/step
Epoch 12/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 13/205
374/374 - 2s - loss: 0.0112 - val_loss: 0.0140 - 2s/epoch - 6ms/step
Epoch 14/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0143 - 2s/epoch - 6ms/step
Epoch 15/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 16/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 17/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 18/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 19/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 20/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 21/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 22/205
374/374 - 2s - loss: 0.0139 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 23/205
374/374 - 2s - loss: 0.0139 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 24/205
374/374 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 25/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 26/205
374/374 - 2s - loss: 0.0150 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 27/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 28/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 29/205
374/374 - 2s - loss: 0.0129 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 30/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 31/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 32/205
374/374 - 2s - loss: 0.0127 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 33/205
374/374 - 2s - loss: 0.0202 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 34/205
374/374 - 2s - loss: 0.0138 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 35/205
374/374 - 2s - loss: 0.0365 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 36/205
374/374 - 2s - loss: 0.0124 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 37/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 38/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 39/205
374/374 - 2s - loss: 0.0130 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 40/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 41/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 42/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 43/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 44/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 45/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0149 - 2s/epoch - 6ms/step
Epoch 46/205
374/374 - 2s - loss: 0.0216 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 47/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 48/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 49/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 50/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0176 - 2s/epoch - 6ms/step
Epoch 51/205
374/374 - 2s - loss: 0.0204 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 52/205
374/374 - 2s - loss: 0.0112 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 53/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 54/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 55/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 56/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 57/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 58/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 59/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 60/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 61/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 62/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 63/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 64/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 65/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 66/205
374/374 - 2s - loss: 0.0130 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 67/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0136 - 2s/epoch - 6ms/step
Epoch 68/205
374/374 - 2s - loss: 0.0140 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 69/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 70/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 71/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 72/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 73/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 74/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 75/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 76/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 77/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 78/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 79/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 80/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 81/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 82/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 83/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 84/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 85/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 86/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 87/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 88/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 89/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 90/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 91/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 92/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 93/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 94/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 95/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 96/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 97/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 98/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 99/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 100/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 101/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 102/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 103/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 104/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 105/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 106/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 107/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 108/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 109/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 110/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 111/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 112/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 113/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 114/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 115/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 116/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0103 - 2s/epoch - 7ms/step
Epoch 117/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 118/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 119/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 120/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 121/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 122/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 123/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 124/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 125/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 126/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 127/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 128/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 129/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 130/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 131/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 132/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 133/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 134/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 135/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 136/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 137/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 138/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 139/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 140/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 141/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 142/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 143/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 144/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 145/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 146/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 147/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 148/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 149/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 150/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 151/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 152/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 153/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 154/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 155/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 156/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 157/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 158/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 159/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 160/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 161/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 162/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 163/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 164/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 165/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 166/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 167/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 168/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 169/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 170/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 171/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 172/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 173/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 174/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 175/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 176/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 177/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 178/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 179/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 180/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 181/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 182/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 183/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 184/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 185/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 186/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 187/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 188/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 189/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 190/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 191/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 192/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 193/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 194/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 195/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 196/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 197/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 198/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 199/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 200/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 201/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 202/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 203/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 204/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 205/205
374/374 - 2s - loss: 0.0093 - val_loss: 0.0092 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009219290688633919
  1/332 [..............................] - ETA: 47s 33/332 [=>............................] - ETA: 0s  65/332 [====>.........................] - ETA: 0s 97/332 [=======>......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s163/332 [=============>................] - ETA: 0s196/332 [================>.............] - ETA: 0s229/332 [===================>..........] - ETA: 0s262/332 [======================>.......] - ETA: 0s295/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07203986118570437
cosine 0.05653042888533517
MAE: 0.035089992
RMSE: 0.07719708
r2: 0.6133999768630631
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 256, 205, 0.0006000000000000001, 0.2, 252, 0.009326406754553318, 0.009219290688633919, 0.07203986118570437, 0.05653042888533517, 0.035089991986751556, 0.07719708234071732, 0.6133999768630631, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 205 0.0008 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/205
374/374 - 7s - loss: 0.0402 - val_loss: 0.0209 - 7s/epoch - 18ms/step
Epoch 2/205
374/374 - 2s - loss: 0.0160 - val_loss: 0.0234 - 2s/epoch - 6ms/step
Epoch 3/205
374/374 - 2s - loss: 0.0154 - val_loss: 0.0164 - 2s/epoch - 6ms/step
Epoch 4/205
374/374 - 2s - loss: 0.0142 - val_loss: 0.0205 - 2s/epoch - 6ms/step
Epoch 5/205
374/374 - 2s - loss: 0.0140 - val_loss: 0.0177 - 2s/epoch - 6ms/step
Epoch 6/205
374/374 - 2s - loss: 0.0134 - val_loss: 0.0151 - 2s/epoch - 6ms/step
Epoch 7/205
374/374 - 2s - loss: 0.0128 - val_loss: 0.0187 - 2s/epoch - 6ms/step
Epoch 8/205
374/374 - 2s - loss: 0.0126 - val_loss: 0.0261 - 2s/epoch - 6ms/step
Epoch 9/205
374/374 - 2s - loss: 0.0128 - val_loss: 0.0241 - 2s/epoch - 6ms/step
Epoch 10/205
374/374 - 2s - loss: 0.0124 - val_loss: 0.0200 - 2s/epoch - 6ms/step
Epoch 11/205
374/374 - 2s - loss: 0.0125 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 12/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0194 - 2s/epoch - 6ms/step
Epoch 13/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0172 - 2s/epoch - 6ms/step
Epoch 14/205
374/374 - 2s - loss: 0.0130 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 15/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 16/205
374/374 - 2s - loss: 0.0136 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 17/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 18/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 19/205
374/374 - 2s - loss: 0.0132 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 20/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 21/205
374/374 - 2s - loss: 0.0140 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 22/205
374/374 - 2s - loss: 0.0194 - val_loss: 0.0150 - 2s/epoch - 6ms/step
Epoch 23/205
374/374 - 2s - loss: 0.0147 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 24/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 25/205
374/374 - 2s - loss: 0.0125 - val_loss: 0.0146 - 2s/epoch - 6ms/step
Epoch 26/205
374/374 - 2s - loss: 0.0279 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 27/205
374/374 - 2s - loss: 0.0123 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 28/205
374/374 - 2s - loss: 0.0123 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 29/205
374/374 - 2s - loss: 0.0178 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 30/205
374/374 - 2s - loss: 0.0119 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 31/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 32/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 33/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 34/205
374/374 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 35/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 36/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 37/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 38/205
374/374 - 2s - loss: 0.0135 - val_loss: 0.0146 - 2s/epoch - 6ms/step
Epoch 39/205
374/374 - 2s - loss: 0.0190 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 40/205
374/374 - 2s - loss: 0.0119 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 41/205
374/374 - 2s - loss: 0.0147 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 42/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 43/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 7ms/step
Epoch 44/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 45/205
374/374 - 2s - loss: 0.0139 - val_loss: 0.0146 - 2s/epoch - 6ms/step
Epoch 46/205
374/374 - 2s - loss: 0.0292 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 47/205
374/374 - 2s - loss: 0.0149 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 48/205
374/374 - 2s - loss: 0.0218 - val_loss: 0.0188 - 2s/epoch - 6ms/step
Epoch 49/205
374/374 - 2s - loss: 0.0573 - val_loss: 0.0142 - 2s/epoch - 6ms/step
Epoch 50/205
374/374 - 2s - loss: 0.0138 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 51/205
374/374 - 2s - loss: 0.0130 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 52/205
374/374 - 2s - loss: 0.0126 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 53/205
374/374 - 2s - loss: 0.0381 - val_loss: 0.0358 - 2s/epoch - 6ms/step
Epoch 54/205
374/374 - 2s - loss: 0.0149 - val_loss: 0.0142 - 2s/epoch - 6ms/step
Epoch 55/205
374/374 - 2s - loss: 0.0177 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 56/205
374/374 - 2s - loss: 0.0137 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 57/205
374/374 - 2s - loss: 0.0131 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 58/205
374/374 - 2s - loss: 0.0128 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 59/205
374/374 - 2s - loss: 0.0126 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 60/205
374/374 - 2s - loss: 0.0124 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 61/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 62/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 63/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 64/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 65/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 66/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 67/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 68/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 69/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 70/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 71/205
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 72/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 73/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 74/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 75/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 76/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 77/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 78/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 79/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0181 - 2s/epoch - 6ms/step
Epoch 80/205
374/374 - 2s - loss: 0.0278 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 81/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 82/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 83/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 84/205
374/374 - 2s - loss: 0.0142 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 85/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 86/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 87/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 88/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 89/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 90/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 91/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0187 - 2s/epoch - 7ms/step
Epoch 92/205
374/374 - 2s - loss: 0.0389 - val_loss: 0.0120 - 2s/epoch - 7ms/step
Epoch 93/205
374/374 - 2s - loss: 0.0119 - val_loss: 0.0114 - 2s/epoch - 7ms/step
Epoch 94/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 95/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 96/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 97/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0148 - 2s/epoch - 6ms/step
Epoch 98/205
374/374 - 2s - loss: 0.0210 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 99/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 100/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 101/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 102/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 103/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 104/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 105/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 106/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 107/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 108/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 109/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 110/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 111/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 112/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 113/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 114/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 115/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 116/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 117/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 118/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0232 - 2s/epoch - 6ms/step
Epoch 119/205
374/374 - 2s - loss: 0.0145 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 120/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 121/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 122/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0198 - 2s/epoch - 6ms/step
Epoch 123/205
374/374 - 2s - loss: 0.0193 - val_loss: 0.0107 - 2s/epoch - 7ms/step
Epoch 124/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 125/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 126/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 127/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 128/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 129/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 130/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 131/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 132/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 133/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 134/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0214 - 2s/epoch - 6ms/step
Epoch 135/205
374/374 - 2s - loss: 0.0193 - val_loss: 0.0170 - 2s/epoch - 6ms/step
Epoch 136/205
374/374 - 2s - loss: 0.0241 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 137/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 138/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 139/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 140/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 141/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 142/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 143/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 144/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 145/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 146/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 147/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 148/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 149/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 150/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 151/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 152/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 153/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 154/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 155/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 156/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 157/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 158/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 159/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 160/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 161/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 162/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 163/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 164/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 165/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 166/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 167/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 168/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0140 - 2s/epoch - 6ms/step
Epoch 169/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 170/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 171/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 172/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 173/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 174/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 175/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 176/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 177/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 178/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 179/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 180/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 181/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 182/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 183/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 184/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 185/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 186/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 187/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 188/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 189/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 190/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 191/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 192/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 193/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 194/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 195/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 196/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 197/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 198/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 199/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 200/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 201/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 202/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 203/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 204/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 205/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0096 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009628641419112682
  1/332 [..............................] - ETA: 49s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s159/332 [=============>................] - ETA: 0s191/332 [================>.............] - ETA: 0s223/332 [===================>..........] - ETA: 0s255/332 [======================>.......] - ETA: 0s287/332 [========================>.....] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07973233002943225
cosine 0.06255331509098933
MAE: 0.03724886
RMSE: 0.08101627
r2: 0.5742010743638054
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 256, 205, 0.0008, 0.2, 252, 0.010652868077158928, 0.009628641419112682, 0.07973233002943225, 0.06255331509098933, 0.03724886104464531, 0.08101627230644226, 0.5742010743638054, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 210 0.0008 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/210
1493/1493 - 13s - loss: 0.0253 - val_loss: 0.0176 - 13s/epoch - 8ms/step
Epoch 2/210
1493/1493 - 8s - loss: 0.0149 - val_loss: 0.0170 - 8s/epoch - 5ms/step
Epoch 3/210
1493/1493 - 8s - loss: 0.0132 - val_loss: 0.0124 - 8s/epoch - 5ms/step
Epoch 4/210
1493/1493 - 8s - loss: 0.0121 - val_loss: 0.0115 - 8s/epoch - 5ms/step
Epoch 5/210
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 6/210
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 7/210
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 8/210
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 9/210
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 10/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 11/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 12/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 13/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 14/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 15/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 16/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 17/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 18/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 19/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 20/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 21/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 22/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 23/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 24/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 25/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 26/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 27/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 28/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 29/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 30/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 31/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 32/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 33/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 34/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 35/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 36/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 37/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 38/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 39/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 40/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 41/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 42/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 43/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 44/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 45/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 46/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 47/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 48/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 49/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 50/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 51/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 52/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 53/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 54/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 55/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 56/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 57/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 58/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 59/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 60/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 61/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 62/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 63/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 64/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 65/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 66/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 67/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 68/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 69/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 70/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 71/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 72/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 73/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 74/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 75/210
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 76/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 77/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 78/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 79/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 80/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 81/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 82/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 84/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 85/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 86/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 87/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 88/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 89/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 90/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 91/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 92/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 93/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 94/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 95/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 96/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 97/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 98/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 99/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 100/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 101/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 102/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 103/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 104/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 105/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 106/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 107/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 108/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 109/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 110/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 111/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 112/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 113/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 114/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 115/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 116/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 117/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 118/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 119/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 120/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 121/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 122/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 123/210
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 124/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 125/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 126/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 127/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 128/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 129/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 130/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 131/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 132/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 133/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 134/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 135/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 136/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 137/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 138/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 139/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 140/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 141/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 142/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 143/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 144/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 145/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 146/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 147/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 148/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 149/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 150/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 151/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 152/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 153/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 154/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 155/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 156/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 157/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 158/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 159/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 160/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 161/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 162/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 163/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 164/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 165/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 166/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 167/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 168/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 169/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 170/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 171/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 172/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 173/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 174/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 175/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 176/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 177/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 178/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 179/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 180/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 181/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 182/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 183/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 184/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 185/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 186/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 187/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 188/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 189/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 190/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 191/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 192/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 193/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 194/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 195/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 196/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 197/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 198/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 199/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 200/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 201/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 202/210
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0092 - 9s/epoch - 6ms/step
Epoch 203/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 204/210
1493/1493 - 8s - loss: 0.0094 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 205/210
1493/1493 - 8s - loss: 0.0094 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 206/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 207/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 208/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 209/210
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 210/210
1493/1493 - 8s - loss: 0.0094 - val_loss: 0.0092 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009241055697202682
  1/332 [..............................] - ETA: 52s 31/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s247/332 [=====================>........] - ETA: 0s279/332 [========================>.....] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07078641538479802
cosine 0.05559436446129489
MAE: 0.034772925
RMSE: 0.07654044
r2: 0.6199488855676052
RMSE zero-vector: 0.23411466903540806
['1.9custom_VAE', 'mse', 64, 210, 0.0008, 0.2, 252, 0.009446474723517895, 0.009241055697202682, 0.07078641538479802, 0.05559436446129489, 0.034772925078868866, 0.07654044032096863, 0.6199488855676052, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 205 0.001 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/205
374/374 - 6s - loss: 0.0404 - val_loss: 0.0183 - 6s/epoch - 16ms/step
Epoch 2/205
374/374 - 2s - loss: 0.0164 - val_loss: 0.0207 - 2s/epoch - 6ms/step
Epoch 3/205
374/374 - 2s - loss: 0.0151 - val_loss: 0.0177 - 2s/epoch - 6ms/step
Epoch 4/205
374/374 - 2s - loss: 0.0144 - val_loss: 0.0516 - 2s/epoch - 6ms/step
Epoch 5/205
374/374 - 2s - loss: 0.0144 - val_loss: 0.0155 - 2s/epoch - 6ms/step
Epoch 6/205
374/374 - 2s - loss: 0.0130 - val_loss: 0.0425 - 2s/epoch - 6ms/step
Epoch 7/205
374/374 - 2s - loss: 0.0135 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 8/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.1327 - 2s/epoch - 6ms/step
Epoch 9/205
374/374 - 2s - loss: 0.0156 - val_loss: 0.0140 - 2s/epoch - 6ms/step
Epoch 10/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0211 - 2s/epoch - 6ms/step
Epoch 11/205
374/374 - 2s - loss: 0.0134 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 12/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0202 - 2s/epoch - 6ms/step
Epoch 13/205
374/374 - 2s - loss: 0.0134 - val_loss: 0.0148 - 2s/epoch - 6ms/step
Epoch 14/205
374/374 - 2s - loss: 0.0145 - val_loss: 0.0163 - 2s/epoch - 6ms/step
Epoch 15/205
374/374 - 2s - loss: 0.0162 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 16/205
374/374 - 2s - loss: 0.0142 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 17/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 18/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 19/205
374/374 - 2s - loss: 0.0148 - val_loss: 0.0120 - 2s/epoch - 7ms/step
Epoch 20/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 21/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 22/205
374/374 - 2s - loss: 0.0137 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 23/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 24/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 25/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 26/205
374/374 - 2s - loss: 0.0184 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 27/205
374/374 - 2s - loss: 0.0157 - val_loss: 0.0160 - 2s/epoch - 6ms/step
Epoch 28/205
374/374 - 2s - loss: 0.0372 - val_loss: 0.0159 - 2s/epoch - 6ms/step
Epoch 29/205
374/374 - 2s - loss: 0.0233 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 30/205
374/374 - 2s - loss: 0.0134 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 31/205
374/374 - 2s - loss: 0.0125 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 32/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 33/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 34/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 35/205
374/374 - 2s - loss: 0.0123 - val_loss: 0.0139 - 2s/epoch - 6ms/step
Epoch 36/205
374/374 - 2s - loss: 0.0208 - val_loss: 0.0161 - 2s/epoch - 6ms/step
Epoch 37/205
374/374 - 2s - loss: 0.0428 - val_loss: 0.0140 - 2s/epoch - 6ms/step
Epoch 38/205
374/374 - 2s - loss: 0.0131 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 39/205
374/374 - 2s - loss: 0.0150 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 40/205
374/374 - 2s - loss: 0.0125 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 41/205
374/374 - 2s - loss: 0.0123 - val_loss: 0.0144 - 2s/epoch - 7ms/step
Epoch 42/205
374/374 - 2s - loss: 0.0232 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 43/205
374/374 - 2s - loss: 0.0130 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 44/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 45/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 46/205
374/374 - 2s - loss: 0.0133 - val_loss: 0.0141 - 2s/epoch - 6ms/step
Epoch 47/205
374/374 - 2s - loss: 0.0210 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 48/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 49/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 50/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 51/205
374/374 - 2s - loss: 0.0185 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 52/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 53/205
374/374 - 2s - loss: 0.0151 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 54/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 55/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0152 - 2s/epoch - 6ms/step
Epoch 56/205
374/374 - 2s - loss: 0.0211 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 57/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 58/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 59/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 60/205
374/374 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 61/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 62/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 63/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 7ms/step
Epoch 64/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 65/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 66/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 67/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 68/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 69/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 70/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 71/205
374/374 - 2s - loss: 0.0131 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 72/205
374/374 - 2s - loss: 0.0119 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 73/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 74/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 75/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 76/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 77/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 78/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 79/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 80/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 81/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 82/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 83/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 84/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 7ms/step
Epoch 85/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 86/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 87/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 88/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 89/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 90/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 91/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 92/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 93/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 94/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 95/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 96/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 97/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 98/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 99/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 100/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 101/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 102/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 103/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 104/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 105/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 106/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 107/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 108/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 109/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 110/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 111/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 112/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 113/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 114/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 115/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 116/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 117/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 118/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 119/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 120/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 121/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 122/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 123/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 124/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 125/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 126/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 127/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 128/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 129/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 130/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 131/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 132/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 133/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 134/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 135/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 136/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 137/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 138/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 139/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 140/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 141/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 142/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 143/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 144/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 145/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 146/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 147/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 148/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 149/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 150/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 151/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 152/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 153/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 154/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 155/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 156/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 157/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 158/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 159/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 160/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 161/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 162/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 163/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 164/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 165/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 166/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 167/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 168/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 169/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 170/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 171/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 172/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 173/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 174/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 175/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 176/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 177/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 178/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 179/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 180/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 181/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 182/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 183/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 184/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 185/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 186/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 187/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 188/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 189/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 190/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 191/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 192/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 193/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 194/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 195/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 196/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 197/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 198/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 199/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 200/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 201/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 202/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 203/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 204/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 205/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009271693415939808
  1/332 [..............................] - ETA: 50s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 95/332 [=======>......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s157/332 [=============>................] - ETA: 0s188/332 [===============>..............] - ETA: 0s219/332 [==================>...........] - ETA: 0s250/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07281853962946604
cosine 0.057137078421995675
MAE: 0.03521478
RMSE: 0.07759271
r2: 0.6094273318058409
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 256, 205, 0.001, 0.2, 252, 0.009357503615319729, 0.009271693415939808, 0.07281853962946604, 0.057137078421995675, 0.035214781761169434, 0.07759270817041397, 0.6094273318058409, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 145 0.0008 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 12s - loss: 0.0259 - val_loss: 0.0216 - 12s/epoch - 8ms/step
Epoch 2/145
1493/1493 - 8s - loss: 0.0147 - val_loss: 0.0169 - 8s/epoch - 6ms/step
Epoch 3/145
1493/1493 - 8s - loss: 0.0129 - val_loss: 0.0119 - 8s/epoch - 6ms/step
Epoch 4/145
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0119 - 8s/epoch - 6ms/step
Epoch 5/145
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0114 - 8s/epoch - 6ms/step
Epoch 6/145
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 7/145
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 9/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0106 - 8s/epoch - 6ms/step
Epoch 10/145
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 11/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 12/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 13/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 16/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 17/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 18/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 19/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 21/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 22/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 23/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 24/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 25/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 27/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 28/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 29/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 30/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 31/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 33/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 34/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 35/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 36/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 37/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 38/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 39/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 40/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 44/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 45/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 46/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 47/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 49/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 50/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 51/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 52/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 53/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 56/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 57/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 58/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 59/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 60/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 61/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 62/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 63/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 64/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 65/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 66/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 69/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 70/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 71/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 72/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 73/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 74/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 75/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 76/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 86/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 87/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 90/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 92/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 93/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 102/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 103/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 104/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 110/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 111/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 112/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 117/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 119/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 121/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 125/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 127/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 128/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 131/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 132/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 133/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 134/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 135/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 145/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009337530471384525
  1/332 [..............................] - ETA: 51s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s160/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s224/332 [===================>..........] - ETA: 0s256/332 [======================>.......] - ETA: 0s288/332 [=========================>....] - ETA: 0s320/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07339599052892735
cosine 0.057675844969005345
MAE: 0.035715528
RMSE: 0.07794464
r2: 0.605876393232613
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 64, 145, 0.0008, 0.2, 252, 0.009579145349562168, 0.009337530471384525, 0.07339599052892735, 0.057675844969005345, 0.03571552783250809, 0.07794463634490967, 0.605876393232613, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 205 0.0008 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/205
374/374 - 6s - loss: 0.0396 - val_loss: 0.0181 - 6s/epoch - 16ms/step
Epoch 2/205
374/374 - 2s - loss: 0.0159 - val_loss: 0.0297 - 2s/epoch - 6ms/step
Epoch 3/205
374/374 - 2s - loss: 0.0157 - val_loss: 0.0201 - 2s/epoch - 7ms/step
Epoch 4/205
374/374 - 2s - loss: 0.0147 - val_loss: 0.0198 - 2s/epoch - 6ms/step
Epoch 5/205
374/374 - 2s - loss: 0.0147 - val_loss: 0.0147 - 2s/epoch - 6ms/step
Epoch 6/205
374/374 - 2s - loss: 0.0136 - val_loss: 0.0162 - 2s/epoch - 6ms/step
Epoch 7/205
374/374 - 2s - loss: 0.0133 - val_loss: 0.0341 - 2s/epoch - 6ms/step
Epoch 8/205
374/374 - 2s - loss: 0.0132 - val_loss: 0.0147 - 2s/epoch - 6ms/step
Epoch 9/205
374/374 - 2s - loss: 0.0123 - val_loss: 0.0173 - 2s/epoch - 6ms/step
Epoch 10/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 11/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0136 - 2s/epoch - 6ms/step
Epoch 12/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 13/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0621 - 2s/epoch - 6ms/step
Epoch 14/205
374/374 - 2s - loss: 0.0144 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 15/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0177 - 2s/epoch - 7ms/step
Epoch 16/205
374/374 - 2s - loss: 0.0137 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 17/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 18/205
374/374 - 2s - loss: 0.0135 - val_loss: 0.0137 - 2s/epoch - 7ms/step
Epoch 19/205
374/374 - 2s - loss: 0.0142 - val_loss: 0.0114 - 2s/epoch - 7ms/step
Epoch 20/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0124 - 2s/epoch - 7ms/step
Epoch 21/205
374/374 - 2s - loss: 0.0144 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 22/205
374/374 - 2s - loss: 0.0153 - val_loss: 0.0143 - 2s/epoch - 7ms/step
Epoch 23/205
374/374 - 2s - loss: 0.0272 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 24/205
374/374 - 2s - loss: 0.0267 - val_loss: 0.0149 - 2s/epoch - 6ms/step
Epoch 25/205
374/374 - 2s - loss: 0.0305 - val_loss: 0.0159 - 2s/epoch - 7ms/step
Epoch 26/205
374/374 - 2s - loss: 0.0243 - val_loss: 0.0190 - 2s/epoch - 6ms/step
Epoch 27/205
374/374 - 2s - loss: 0.0696 - val_loss: 0.0214 - 2s/epoch - 6ms/step
Epoch 28/205
374/374 - 2s - loss: 0.0167 - val_loss: 0.0166 - 2s/epoch - 6ms/step
Epoch 29/205
374/374 - 2s - loss: 0.0167 - val_loss: 0.0148 - 2s/epoch - 6ms/step
Epoch 30/205
374/374 - 2s - loss: 0.0173 - val_loss: 0.0144 - 2s/epoch - 6ms/step
Epoch 31/205
374/374 - 2s - loss: 0.0161 - val_loss: 0.0159 - 2s/epoch - 6ms/step
Epoch 32/205
374/374 - 2s - loss: 0.0325 - val_loss: 0.0176 - 2s/epoch - 7ms/step
Epoch 33/205
374/374 - 2s - loss: 0.0205 - val_loss: 0.0159 - 2s/epoch - 7ms/step
Epoch 34/205
374/374 - 2s - loss: 0.0153 - val_loss: 0.0152 - 2s/epoch - 6ms/step
Epoch 35/205
374/374 - 2s - loss: 0.0145 - val_loss: 0.0139 - 2s/epoch - 6ms/step
Epoch 36/205
374/374 - 2s - loss: 0.0140 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 37/205
374/374 - 2s - loss: 0.0137 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 38/205
374/374 - 2s - loss: 0.0134 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 39/205
374/374 - 2s - loss: 0.0131 - val_loss: 0.0125 - 2s/epoch - 7ms/step
Epoch 40/205
374/374 - 2s - loss: 0.0139 - val_loss: 0.0266 - 2s/epoch - 6ms/step
Epoch 41/205
374/374 - 3s - loss: 0.0537 - val_loss: 0.0169 - 3s/epoch - 7ms/step
Epoch 42/205
374/374 - 3s - loss: 0.0179 - val_loss: 0.0144 - 3s/epoch - 7ms/step
Epoch 43/205
374/374 - 2s - loss: 0.0143 - val_loss: 0.0137 - 2s/epoch - 7ms/step
Epoch 44/205
374/374 - 2s - loss: 0.0138 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 45/205
374/374 - 2s - loss: 0.0133 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 46/205
374/374 - 2s - loss: 0.0129 - val_loss: 0.0125 - 2s/epoch - 7ms/step
Epoch 47/205
374/374 - 2s - loss: 0.0127 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 48/205
374/374 - 2s - loss: 0.0124 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 49/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 50/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0118 - 2s/epoch - 7ms/step
Epoch 51/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 52/205
374/374 - 2s - loss: 0.0118 - val_loss: 0.0124 - 2s/epoch - 7ms/step
Epoch 53/205
374/374 - 2s - loss: 0.0126 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 54/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 55/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0171 - 2s/epoch - 7ms/step
Epoch 56/205
374/374 - 2s - loss: 0.0369 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 57/205
374/374 - 2s - loss: 0.0125 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 58/205
374/374 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 59/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 60/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0176 - 2s/epoch - 6ms/step
Epoch 61/205
374/374 - 2s - loss: 0.0186 - val_loss: 0.0204 - 2s/epoch - 6ms/step
Epoch 62/205
374/374 - 2s - loss: 0.0135 - val_loss: 0.0116 - 2s/epoch - 7ms/step
Epoch 63/205
374/374 - 3s - loss: 0.0119 - val_loss: 0.0226 - 3s/epoch - 7ms/step
Epoch 64/205
374/374 - 2s - loss: 0.0302 - val_loss: 0.0129 - 2s/epoch - 7ms/step
Epoch 65/205
374/374 - 2s - loss: 0.0121 - val_loss: 0.0117 - 2s/epoch - 7ms/step
Epoch 66/205
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 67/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 68/205
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 69/205
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 70/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 71/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 72/205
374/374 - 2s - loss: 0.0115 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 73/205
374/374 - 2s - loss: 0.0109 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 74/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 75/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 76/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 77/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 78/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 79/205
374/374 - 2s - loss: 0.0107 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 80/205
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 81/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 82/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 83/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 84/205
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 85/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0106 - 2s/epoch - 7ms/step
Epoch 86/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 87/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 88/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 89/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 90/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 91/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 92/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 93/205
374/374 - 3s - loss: 0.0100 - val_loss: 0.0132 - 3s/epoch - 7ms/step
Epoch 94/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 95/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0165 - 2s/epoch - 6ms/step
Epoch 96/205
374/374 - 2s - loss: 0.0197 - val_loss: 0.0140 - 2s/epoch - 6ms/step
Epoch 97/205
374/374 - 2s - loss: 0.0234 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 98/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 99/205
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 100/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 101/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 102/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 103/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 104/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 105/205
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 106/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 107/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 108/205
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 109/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 110/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 111/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 112/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 113/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 114/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 115/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 116/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0215 - 2s/epoch - 6ms/step
Epoch 117/205
374/374 - 2s - loss: 0.0120 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 118/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 119/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 120/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 121/205
374/374 - 2s - loss: 0.0104 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 122/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 123/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 124/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 125/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 126/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 127/205
374/374 - 2s - loss: 0.0131 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 128/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 129/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 130/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 131/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 132/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 133/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 134/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 135/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 136/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 137/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 138/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 139/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 140/205
374/374 - 2s - loss: 0.0099 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 141/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 142/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 143/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 144/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 145/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 146/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 147/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 148/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 149/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 150/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 151/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 152/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 153/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 154/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 155/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 156/205
374/374 - 2s - loss: 0.0106 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 157/205
374/374 - 2s - loss: 0.0111 - val_loss: 0.0103 - 2s/epoch - 7ms/step
Epoch 158/205
374/374 - 2s - loss: 0.0100 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 159/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 160/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 161/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 162/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 163/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 164/205
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 165/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 166/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 167/205
374/374 - 2s - loss: 0.0098 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 168/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 169/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 170/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 171/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 172/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 173/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 174/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 175/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 176/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 177/205
374/374 - 2s - loss: 0.0103 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 178/205
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 179/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 180/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 181/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 182/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 183/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 184/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 185/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 186/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 187/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 188/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 189/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 190/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 191/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 192/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 193/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 194/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 195/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 196/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 197/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 198/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 199/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 200/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 201/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 202/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 203/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 204/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 205/205
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0093041080981493
  1/332 [..............................] - ETA: 50s 25/332 [=>............................] - ETA: 0s  56/332 [====>.........................] - ETA: 0s 87/332 [======>.......................] - ETA: 0s118/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s180/332 [===============>..............] - ETA: 0s211/332 [==================>...........] - ETA: 0s243/332 [====================>.........] - ETA: 0s274/332 [=======================>......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07346863339206115
cosine 0.05768217360955385
MAE: 0.035650708
RMSE: 0.07792954
r2: 0.606028965733221
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 256, 205, 0.0008, 0.2, 252, 0.009411417879164219, 0.0093041080981493, 0.07346863339206115, 0.05768217360955385, 0.03565070778131485, 0.0779295414686203, 0.606028965733221, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 205 0.001 256 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/205
374/374 - 5s - loss: 0.0211 - val_loss: 0.0103 - 5s/epoch - 15ms/step
Epoch 2/205
374/374 - 2s - loss: 0.0086 - val_loss: 0.0141 - 2s/epoch - 6ms/step
Epoch 3/205
374/374 - 2s - loss: 0.0082 - val_loss: 0.0091 - 2s/epoch - 6ms/step
Epoch 4/205
374/374 - 2s - loss: 0.0080 - val_loss: 0.0284 - 2s/epoch - 6ms/step
Epoch 5/205
374/374 - 2s - loss: 0.0077 - val_loss: 0.0176 - 2s/epoch - 6ms/step
Epoch 6/205
374/374 - 2s - loss: 0.0076 - val_loss: 0.0192 - 2s/epoch - 6ms/step
Epoch 7/205
374/374 - 2s - loss: 0.0075 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 8/205
374/374 - 2s - loss: 0.0072 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 9/205
374/374 - 2s - loss: 0.0073 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 10/205
374/374 - 2s - loss: 0.0068 - val_loss: 0.0083 - 2s/epoch - 6ms/step
Epoch 11/205
374/374 - 2s - loss: 0.0067 - val_loss: 0.0143 - 2s/epoch - 6ms/step
Epoch 12/205
374/374 - 2s - loss: 0.0086 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 13/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 14/205
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 15/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 16/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0333 - 2s/epoch - 6ms/step
Epoch 17/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 18/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0077 - 2s/epoch - 6ms/step
Epoch 19/205
374/374 - 2s - loss: 0.0076 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 20/205
374/374 - 2s - loss: 0.0116 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 21/205
374/374 - 2s - loss: 0.0161 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 22/205
374/374 - 2s - loss: 0.0076 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 23/205
374/374 - 2s - loss: 0.0250 - val_loss: 0.0169 - 2s/epoch - 6ms/step
Epoch 24/205
374/374 - 2s - loss: 0.0355 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 25/205
374/374 - 2s - loss: 0.0095 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 26/205
374/374 - 2s - loss: 0.0141 - val_loss: 0.0086 - 2s/epoch - 6ms/step
Epoch 27/205
374/374 - 2s - loss: 0.0088 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 28/205
374/374 - 2s - loss: 0.0077 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 29/205
374/374 - 2s - loss: 0.0074 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 30/205
374/374 - 2s - loss: 0.0072 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 31/205
374/374 - 2s - loss: 0.0071 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 32/205
374/374 - 2s - loss: 0.0073 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 33/205
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 34/205
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 35/205
374/374 - 2s - loss: 0.0068 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 36/205
374/374 - 2s - loss: 0.0188 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 37/205
374/374 - 2s - loss: 0.0072 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 38/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 39/205
374/374 - 2s - loss: 0.0068 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 40/205
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 41/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 42/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0080 - 2s/epoch - 6ms/step
Epoch 43/205
374/374 - 2s - loss: 0.0089 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 44/205
374/374 - 2s - loss: 0.0067 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 45/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 46/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 47/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 48/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0083 - 2s/epoch - 6ms/step
Epoch 49/205
374/374 - 2s - loss: 0.0077 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 50/205
374/374 - 2s - loss: 0.0196 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 51/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 52/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 53/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 54/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 55/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 56/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 57/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 58/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 59/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 60/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 61/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 62/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 63/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 64/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 65/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 66/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 67/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 68/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 69/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 70/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 71/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 72/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 73/205
374/374 - 2s - loss: 0.0119 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 74/205
374/374 - 2s - loss: 0.0091 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 75/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 76/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 77/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 78/205
374/374 - 2s - loss: 0.0073 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 79/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 80/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 81/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 82/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 83/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 84/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 85/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0080 - 2s/epoch - 6ms/step
Epoch 86/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 87/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 88/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 89/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 90/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 91/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 92/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 93/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 94/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 95/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 96/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 97/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 98/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 99/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 100/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 101/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 102/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0087 - 2s/epoch - 6ms/step
Epoch 103/205
374/374 - 2s - loss: 0.0070 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 104/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 105/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 106/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 107/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 108/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 109/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 110/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 111/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 112/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0089 - 2s/epoch - 6ms/step
Epoch 113/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 114/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 115/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 116/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 117/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 118/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 119/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 120/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 121/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 122/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 123/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 124/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 125/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 126/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 127/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 128/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 129/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 130/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 131/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 132/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 133/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 134/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 135/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 136/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 137/205
374/374 - 2s - loss: 0.0071 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 138/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 139/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 140/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 141/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 142/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 143/205
374/374 - 2s - loss: 0.0076 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 144/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 145/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 146/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 147/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 148/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 149/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 150/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 151/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 152/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 153/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 154/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 155/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 156/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 157/205
374/374 - 2s - loss: 0.0071 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 158/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 159/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 160/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 161/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 162/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 163/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 164/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 165/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 166/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 167/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 168/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 169/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 170/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 171/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 172/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 173/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 174/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 175/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 176/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 177/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 178/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 179/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 180/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 181/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 182/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 183/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 184/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 185/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 186/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 187/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 188/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 189/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 190/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 191/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 192/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 193/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 194/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 195/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 196/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 197/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 198/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 199/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 200/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 201/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 202/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 203/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 204/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 205/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005925014615058899
  1/332 [..............................] - ETA: 51s 32/332 [=>............................] - ETA: 0s  63/332 [====>.........................] - ETA: 0s 94/332 [=======>......................] - ETA: 0s125/332 [==========>...................] - ETA: 0s156/332 [=============>................] - ETA: 0s187/332 [===============>..............] - ETA: 0s218/332 [==================>...........] - ETA: 0s249/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s314/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11944566127313024
cosine 0.09348562428895546
MAE: 0.04559187
RMSE: 0.09829353
r2: 0.37322724247102007
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'logcosh', 256, 205, 0.001, 0.2, 252, 0.005958561785519123, 0.005925014615058899, 0.11944566127313024, 0.09348562428895546, 0.045591868460178375, 0.09829352796077728, 0.37322724247102007, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 145 0.0022 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/145
747/747 - 8s - loss: 0.0309 - val_loss: 0.0249 - 8s/epoch - 11ms/step
Epoch 2/145
747/747 - 4s - loss: 0.0152 - val_loss: 0.0169 - 4s/epoch - 6ms/step
Epoch 3/145
747/747 - 4s - loss: 0.0141 - val_loss: 0.0138 - 4s/epoch - 6ms/step
Epoch 4/145
747/747 - 4s - loss: 0.0130 - val_loss: 0.0146 - 4s/epoch - 6ms/step
Epoch 5/145
747/747 - 4s - loss: 0.0125 - val_loss: 0.0182 - 4s/epoch - 6ms/step
Epoch 6/145
747/747 - 4s - loss: 0.0121 - val_loss: 0.0350 - 4s/epoch - 6ms/step
Epoch 7/145
747/747 - 4s - loss: 0.0124 - val_loss: 0.0119 - 4s/epoch - 6ms/step
Epoch 8/145
747/747 - 4s - loss: 0.0118 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 9/145
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 10/145
747/747 - 4s - loss: 0.0111 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 11/145
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 6ms/step
Epoch 12/145
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 13/145
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 14/145
747/747 - 4s - loss: 0.0112 - val_loss: 0.0108 - 4s/epoch - 6ms/step
Epoch 15/145
747/747 - 4s - loss: 0.0110 - val_loss: 0.0114 - 4s/epoch - 6ms/step
Epoch 16/145
747/747 - 4s - loss: 0.0126 - val_loss: 0.0114 - 4s/epoch - 6ms/step
Epoch 17/145
747/747 - 4s - loss: 0.0118 - val_loss: 0.0107 - 4s/epoch - 6ms/step
Epoch 18/145
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 6ms/step
Epoch 19/145
747/747 - 4s - loss: 0.0108 - val_loss: 0.0115 - 4s/epoch - 6ms/step
Epoch 20/145
747/747 - 4s - loss: 0.0118 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 21/145
747/747 - 4s - loss: 0.0107 - val_loss: 0.0115 - 4s/epoch - 6ms/step
Epoch 22/145
747/747 - 4s - loss: 0.0121 - val_loss: 0.0108 - 4s/epoch - 6ms/step
Epoch 23/145
747/747 - 4s - loss: 0.0110 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 24/145
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 25/145
747/747 - 4s - loss: 0.0105 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 26/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0115 - 4s/epoch - 6ms/step
Epoch 27/145
747/747 - 4s - loss: 0.0117 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 28/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 29/145
747/747 - 4s - loss: 0.0110 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 30/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 31/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 32/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0107 - 4s/epoch - 6ms/step
Epoch 33/145
747/747 - 4s - loss: 0.0106 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 34/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 35/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 36/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 37/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 38/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 39/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 40/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 41/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 42/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 43/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 44/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 45/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 46/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 47/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 48/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 49/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 50/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 51/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 52/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 53/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 54/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 55/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 56/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 57/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 58/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 59/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 60/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 61/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 62/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 63/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 64/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 65/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 66/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 67/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 68/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 69/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 70/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 71/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 72/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 73/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 74/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 75/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 76/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 77/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 78/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 79/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 80/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 81/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 82/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 83/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 84/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 85/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 86/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 87/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 88/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 89/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 90/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 91/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 92/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 93/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 94/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 95/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 96/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 97/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 98/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 99/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 100/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 101/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 102/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 103/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 104/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 105/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 106/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 107/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 108/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 109/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 110/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 111/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 112/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 113/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 114/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 115/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 116/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 117/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 118/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 119/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 120/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 121/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 122/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 123/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 124/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 125/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 126/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 127/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 128/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 129/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 130/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 131/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 132/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 133/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 134/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 135/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 136/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 137/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 138/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 139/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 140/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 141/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 142/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 143/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 144/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 145/145
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00921714212745428
  1/332 [..............................] - ETA: 51s 31/332 [=>............................] - ETA: 0s  58/332 [====>.........................] - ETA: 0s 87/332 [======>.......................] - ETA: 0s117/332 [=========>....................] - ETA: 0s147/332 [============>.................] - ETA: 0s176/332 [==============>...............] - ETA: 0s205/332 [=================>............] - ETA: 0s234/332 [====================>.........] - ETA: 0s263/332 [======================>.......] - ETA: 0s294/332 [=========================>....] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07102664769302437
cosine 0.0557720082304049
MAE: 0.03485192
RMSE: 0.07665302
r2: 0.6188301918558183
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 128, 145, 0.0022, 0.2, 252, 0.009395228698849678, 0.00921714212745428, 0.07102664769302437, 0.0557720082304049, 0.03485191985964775, 0.07665301859378815, 0.6188301918558183, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 145 0.0008 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 14s - loss: 0.0263 - val_loss: 0.0175 - 14s/epoch - 9ms/step
Epoch 2/145
1493/1493 - 8s - loss: 0.0150 - val_loss: 0.0139 - 8s/epoch - 5ms/step
Epoch 3/145
1493/1493 - 8s - loss: 0.0134 - val_loss: 0.0126 - 8s/epoch - 5ms/step
Epoch 4/145
1493/1493 - 8s - loss: 0.0124 - val_loss: 0.0117 - 8s/epoch - 5ms/step
Epoch 5/145
1493/1493 - 8s - loss: 0.0118 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 6/145
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 7/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 9/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 10/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 11/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 12/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 13/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 16/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 17/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 18/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 19/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 21/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 22/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 23/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 24/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 25/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 27/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 28/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 29/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 30/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 31/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 32/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 33/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 34/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 35/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 36/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 37/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 38/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 39/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 40/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 44/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 45/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 46/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 47/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 49/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 50/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 51/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 52/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 53/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 56/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 57/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 58/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 59/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 60/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 61/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 62/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 63/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 64/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 65/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 66/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 67/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 69/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 70/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 71/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 72/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 73/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 74/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 75/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 76/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 86/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 87/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 90/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 92/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 93/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 99/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 102/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 103/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 104/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 106/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 109/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 110/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 111/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 112/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 113/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 117/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 119/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 120/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 121/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 124/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 125/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 126/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 127/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 128/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 131/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 132/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 133/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 134/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 135/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 136/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 137/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 145/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009322920814156532
  1/332 [..............................] - ETA: 53s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s124/332 [==========>...................] - ETA: 0s156/332 [=============>................] - ETA: 0s188/332 [===============>..............] - ETA: 0s220/332 [==================>...........] - ETA: 0s252/332 [=====================>........] - ETA: 0s284/332 [========================>.....] - ETA: 0s317/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0736997449764251
cosine 0.057898218755087584
MAE: 0.03568202
RMSE: 0.07812883
r2: 0.604011406706131
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'mse', 64, 145, 0.0008, 0.2, 252, 0.009572751820087433, 0.009322920814156532, 0.0736997449764251, 0.057898218755087584, 0.03568201884627342, 0.07812882959842682, 0.604011406706131, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 145 0.0008 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/145
374/374 - 6s - loss: 0.0397 - val_loss: 0.0514 - 6s/epoch - 16ms/step
Epoch 2/145
374/374 - 3s - loss: 0.0173 - val_loss: 0.0188 - 3s/epoch - 7ms/step
Epoch 3/145
374/374 - 2s - loss: 0.0156 - val_loss: 0.0220 - 2s/epoch - 7ms/step
Epoch 4/145
374/374 - 2s - loss: 0.0146 - val_loss: 0.0204 - 2s/epoch - 7ms/step
Epoch 5/145
374/374 - 2s - loss: 0.0141 - val_loss: 0.0174 - 2s/epoch - 6ms/step
Epoch 6/145
374/374 - 2s - loss: 0.0138 - val_loss: 0.0175 - 2s/epoch - 6ms/step
Epoch 7/145
374/374 - 2s - loss: 0.0136 - val_loss: 0.0187 - 2s/epoch - 7ms/step
Epoch 8/145
374/374 - 2s - loss: 0.0131 - val_loss: 0.0154 - 2s/epoch - 7ms/step
Epoch 9/145
374/374 - 2s - loss: 0.0126 - val_loss: 0.0139 - 2s/epoch - 7ms/step
Epoch 10/145
374/374 - 2s - loss: 0.0123 - val_loss: 0.0368 - 2s/epoch - 6ms/step
Epoch 11/145
374/374 - 2s - loss: 0.0130 - val_loss: 0.0240 - 2s/epoch - 7ms/step
Epoch 12/145
374/374 - 2s - loss: 0.0129 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 13/145
374/374 - 2s - loss: 0.0115 - val_loss: 0.0116 - 2s/epoch - 7ms/step
Epoch 14/145
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 7ms/step
Epoch 15/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 16/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 17/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 18/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 19/145
374/374 - 2s - loss: 0.0114 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 20/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 21/145
374/374 - 2s - loss: 0.0112 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 22/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0124 - 2s/epoch - 7ms/step
Epoch 23/145
374/374 - 3s - loss: 0.0149 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 24/145
374/374 - 3s - loss: 0.0176 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 25/145
374/374 - 2s - loss: 0.0115 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 26/145
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 7ms/step
Epoch 27/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0132 - 3s/epoch - 7ms/step
Epoch 28/145
374/374 - 3s - loss: 0.0160 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 29/145
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 7ms/step
Epoch 30/145
374/374 - 2s - loss: 0.0130 - val_loss: 0.0108 - 2s/epoch - 7ms/step
Epoch 31/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 32/145
374/374 - 2s - loss: 0.0109 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 33/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 34/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 35/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 36/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0149 - 2s/epoch - 6ms/step
Epoch 37/145
374/374 - 2s - loss: 0.0252 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 38/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 39/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 40/145
374/374 - 2s - loss: 0.0112 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 41/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 42/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 43/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 44/145
374/374 - 2s - loss: 0.0121 - val_loss: 0.0109 - 2s/epoch - 7ms/step
Epoch 45/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 46/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0108 - 2s/epoch - 7ms/step
Epoch 47/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 48/145
374/374 - 2s - loss: 0.0171 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 49/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0162 - 2s/epoch - 7ms/step
Epoch 50/145
374/374 - 2s - loss: 0.0229 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 51/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 7ms/step
Epoch 52/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 53/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 54/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 55/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 56/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 57/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 58/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 59/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 60/145
374/374 - 2s - loss: 0.0134 - val_loss: 0.0149 - 2s/epoch - 7ms/step
Epoch 61/145
374/374 - 2s - loss: 0.0304 - val_loss: 0.0137 - 2s/epoch - 7ms/step
Epoch 62/145
374/374 - 2s - loss: 0.0227 - val_loss: 0.0124 - 2s/epoch - 7ms/step
Epoch 63/145
374/374 - 2s - loss: 0.0124 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 64/145
374/374 - 2s - loss: 0.0113 - val_loss: 0.0109 - 2s/epoch - 7ms/step
Epoch 65/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 66/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 67/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 68/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 69/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 70/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 71/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 72/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 73/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 74/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 75/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 76/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 77/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0147 - 2s/epoch - 7ms/step
Epoch 78/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 79/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 80/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 81/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0181 - 2s/epoch - 6ms/step
Epoch 82/145
374/374 - 2s - loss: 0.0163 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 83/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 84/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0223 - 2s/epoch - 7ms/step
Epoch 85/145
374/374 - 2s - loss: 0.0242 - val_loss: 0.0117 - 2s/epoch - 7ms/step
Epoch 86/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 87/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 88/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 89/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 90/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 91/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0149 - 3s/epoch - 7ms/step
Epoch 92/145
374/374 - 2s - loss: 0.0174 - val_loss: 0.0117 - 2s/epoch - 7ms/step
Epoch 93/145
374/374 - 2s - loss: 0.0126 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 94/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 95/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 96/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0242 - 2s/epoch - 6ms/step
Epoch 97/145
374/374 - 2s - loss: 0.0317 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 98/145
374/374 - 2s - loss: 0.0113 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 99/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 100/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 101/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 102/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0114 - 2s/epoch - 7ms/step
Epoch 103/145
374/374 - 2s - loss: 0.0109 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 104/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 105/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 106/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 107/145
374/374 - 2s - loss: 0.0112 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 108/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 109/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 110/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 111/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 112/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 113/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 114/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 115/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 116/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 117/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 118/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 119/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 120/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 121/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 122/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 123/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 124/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 125/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 126/145
374/374 - 3s - loss: 0.0097 - val_loss: 0.0258 - 3s/epoch - 7ms/step
Epoch 127/145
374/374 - 3s - loss: 0.0192 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 128/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 129/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 130/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 131/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 132/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 133/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 134/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0187 - 2s/epoch - 7ms/step
Epoch 135/145
374/374 - 2s - loss: 0.0204 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 136/145
374/374 - 2s - loss: 0.0269 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 137/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 138/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 139/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 140/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 141/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 142/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 143/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 144/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 145/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009831564500927925
  1/332 [..............................] - ETA: 46s 32/332 [=>............................] - ETA: 0s  65/332 [====>.........................] - ETA: 0s 98/332 [=======>......................] - ETA: 0s131/332 [==========>...................] - ETA: 0s164/332 [=============>................] - ETA: 0s197/332 [================>.............] - ETA: 0s230/332 [===================>..........] - ETA: 0s262/332 [======================>.......] - ETA: 0s295/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07691334162709221
cosine 0.06036106703463834
MAE: 0.036663644
RMSE: 0.07962719
r2: 0.5886772254743725
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 256, 145, 0.0008, 0.2, 252, 0.009907183237373829, 0.009831564500927925, 0.07691334162709221, 0.06036106703463834, 0.03666364401578903, 0.07962719351053238, 0.5886772254743725, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 140 0.0022 128 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/140
747/747 - 7s - loss: 0.0164 - val_loss: 0.0100 - 7s/epoch - 10ms/step
Epoch 2/140
747/747 - 4s - loss: 0.0086 - val_loss: 0.0211 - 4s/epoch - 6ms/step
Epoch 3/140
747/747 - 5s - loss: 0.0081 - val_loss: 0.0087 - 5s/epoch - 6ms/step
Epoch 4/140
747/747 - 4s - loss: 0.0073 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 5/140
747/747 - 4s - loss: 0.0070 - val_loss: 0.0202 - 4s/epoch - 6ms/step
Epoch 6/140
747/747 - 4s - loss: 0.0068 - val_loss: 0.0071 - 4s/epoch - 6ms/step
Epoch 7/140
747/747 - 4s - loss: 0.0065 - val_loss: 0.0179 - 4s/epoch - 6ms/step
Epoch 8/140
747/747 - 4s - loss: 0.0066 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 9/140
747/747 - 4s - loss: 0.0064 - val_loss: 0.0086 - 4s/epoch - 6ms/step
Epoch 10/140
747/747 - 4s - loss: 0.0065 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 11/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 12/140
747/747 - 4s - loss: 0.0065 - val_loss: 0.0064 - 4s/epoch - 6ms/step
Epoch 13/140
747/747 - 4s - loss: 0.0067 - val_loss: 0.0065 - 4s/epoch - 6ms/step
Epoch 14/140
747/747 - 4s - loss: 0.0071 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 15/140
747/747 - 5s - loss: 0.0063 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 16/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 17/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 18/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 19/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 20/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 21/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 22/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 23/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 24/140
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 25/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 26/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 27/140
747/747 - 4s - loss: 0.0063 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 28/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 29/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 30/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 31/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 32/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 33/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 34/140
747/747 - 4s - loss: 0.0064 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 35/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 36/140
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 37/140
747/747 - 4s - loss: 0.0065 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 38/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 39/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 40/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 41/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 42/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 43/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 44/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 45/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 46/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 47/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 48/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 49/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 50/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 51/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 52/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 53/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 54/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 55/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 56/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 57/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 58/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 59/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 60/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 61/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 62/140
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 63/140
747/747 - 4s - loss: 0.0062 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 64/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 65/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 66/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 67/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 68/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 69/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 70/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 71/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 72/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 73/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 74/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 75/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 76/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 77/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 78/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 79/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 80/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 81/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 82/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 83/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 84/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 85/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 86/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 87/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 88/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 89/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 90/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 91/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 92/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 93/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 94/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 95/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 96/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 97/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 98/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 99/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 100/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 101/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 102/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 103/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 104/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 105/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 106/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 107/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 108/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 109/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 110/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 111/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 112/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 113/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 114/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 115/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 116/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 117/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 118/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 119/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 120/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 121/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 122/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 123/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 124/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 125/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 126/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 127/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 128/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 129/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 130/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 131/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 132/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 133/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 134/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 135/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 136/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 137/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 138/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 139/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 140/140
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005890214815735817
  1/332 [..............................] - ETA: 56s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s160/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s224/332 [===================>..........] - ETA: 0s256/332 [======================>.......] - ETA: 0s288/332 [=========================>....] - ETA: 0s320/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11795530380885465
cosine 0.0922618941275794
MAE: 0.045171436
RMSE: 0.09778243
r2: 0.379728406137643
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'logcosh', 128, 140, 0.0022, 0.2, 252, 0.005959685891866684, 0.005890214815735817, 0.11795530380885465, 0.0922618941275794, 0.04517143592238426, 0.0977824330329895, 0.379728406137643, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 145 0.001 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 12s - loss: 0.0260 - val_loss: 0.0227 - 12s/epoch - 8ms/step
Epoch 2/145
1493/1493 - 8s - loss: 0.0152 - val_loss: 0.0158 - 8s/epoch - 5ms/step
Epoch 3/145
1493/1493 - 8s - loss: 0.0129 - val_loss: 0.0121 - 8s/epoch - 5ms/step
Epoch 4/145
1493/1493 - 8s - loss: 0.0122 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 5/145
1493/1493 - 8s - loss: 0.0122 - val_loss: 0.0116 - 8s/epoch - 5ms/step
Epoch 6/145
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0113 - 8s/epoch - 5ms/step
Epoch 7/145
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 9/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 10/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 11/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 12/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 13/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 16/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 17/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 18/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 19/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 20/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 21/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 22/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 23/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 24/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 25/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 26/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 27/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 28/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 29/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 30/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 31/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 33/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 34/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 35/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 36/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 37/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 38/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 39/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 40/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 41/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 44/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 45/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 46/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 47/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 48/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 49/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 50/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 51/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 52/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 53/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 54/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 56/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 57/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 58/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 59/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 60/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 61/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 62/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 63/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 64/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 65/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 66/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 68/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 69/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 70/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 71/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 72/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 73/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 74/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 75/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 76/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 79/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 80/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 81/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 82/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 84/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 85/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 86/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 87/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 88/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 89/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 90/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 92/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 93/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 94/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 96/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 97/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 98/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 99/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 102/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 103/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 104/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 105/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 106/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 107/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 108/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 109/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 110/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 111/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 112/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 116/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 117/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 119/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 121/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 123/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 125/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 127/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 128/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 131/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 132/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 133/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 134/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 135/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 138/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 139/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 140/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 141/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 144/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 145/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009312358684837818
  1/332 [..............................] - ETA: 52s 30/332 [=>............................] - ETA: 0s  60/332 [====>.........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s120/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s179/332 [===============>..............] - ETA: 0s208/332 [=================>............] - ETA: 0s238/332 [====================>.........] - ETA: 0s267/332 [=======================>......] - ETA: 0s296/332 [=========================>....] - ETA: 0s326/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07187840373975306
cosine 0.056425759090687075
MAE: 0.03531403
RMSE: 0.07718528
r2: 0.6135182302090166
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 64, 145, 0.001, 0.2, 252, 0.009559527970850468, 0.009312358684837818, 0.07187840373975306, 0.056425759090687075, 0.035314030945301056, 0.07718528062105179, 0.6135182302090166, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 205 0.0008 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
Epoch 1/205
1493/1493 - 12s - loss: 0.0261 - val_loss: 0.0170 - 12s/epoch - 8ms/step
Epoch 2/205
1493/1493 - 9s - loss: 0.0147 - val_loss: 0.0676 - 9s/epoch - 6ms/step
Epoch 3/205
1493/1493 - 8s - loss: 0.0139 - val_loss: 0.0120 - 8s/epoch - 6ms/step
Epoch 4/205
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0116 - 8s/epoch - 6ms/step
Epoch 5/205
1493/1493 - 8s - loss: 0.0118 - val_loss: 0.0114 - 8s/epoch - 6ms/step
Epoch 6/205
1493/1493 - 9s - loss: 0.0116 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 7/205
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 8/205
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 9/205
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 10/205
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 11/205
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 12/205
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 13/205
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 14/205
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 15/205
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 16/205
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 17/205
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 18/205
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 19/205
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 20/205
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 21/205
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 22/205
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 23/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 24/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 25/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 26/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 27/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 28/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 29/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 30/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 31/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 32/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 33/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 34/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 35/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 36/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 37/205
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 38/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 39/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 40/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 41/205
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 42/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 43/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 44/205
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 45/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 46/205
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 47/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 48/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 49/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 50/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 51/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 52/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 53/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 54/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 55/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 56/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 57/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 58/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 59/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 60/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 61/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 62/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 63/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 64/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 65/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 66/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 67/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 68/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 69/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 70/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 71/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 72/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 73/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 74/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 75/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 76/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 77/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 78/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 79/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 80/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 81/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 82/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 83/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 84/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 85/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 86/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 87/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 88/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 89/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 90/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 91/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 92/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 93/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 94/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 95/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 96/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 97/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 98/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 99/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 100/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 101/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 102/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 103/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 104/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 105/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 106/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 107/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 108/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 109/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 110/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 111/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 112/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 113/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 114/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 115/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 116/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 117/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 118/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 119/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 120/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 121/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 122/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 123/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 124/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 125/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 126/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 127/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 128/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 129/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 130/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 131/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 132/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 133/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 134/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 135/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 136/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 137/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 138/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 139/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 140/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 141/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 142/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 143/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 144/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 145/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 146/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 147/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 148/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 149/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 150/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 151/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 152/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 153/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 154/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 155/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 156/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 157/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 158/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 159/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 160/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 161/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 162/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 163/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 164/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 165/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 166/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 167/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 168/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 169/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 170/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 171/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 172/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 173/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 174/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 175/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 176/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 177/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 178/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 179/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 180/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 181/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 182/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 183/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 184/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 185/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 186/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 187/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 188/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 189/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 190/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 191/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 192/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 193/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 194/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 195/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 196/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 197/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 198/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 199/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 200/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 201/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 202/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 203/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 204/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 205/205
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009288999252021313
  1/332 [..............................] - ETA: 53s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s124/332 [==========>...................] - ETA: 0s155/332 [=============>................] - ETA: 0s187/332 [===============>..............] - ETA: 0s211/332 [==================>...........] - ETA: 0s242/332 [====================>.........] - ETA: 0s272/332 [=======================>......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07238673452741462
cosine 0.05683830683181028
MAE: 0.035266995
RMSE: 0.07738955
r2: 0.6114697781526793
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'mse', 64, 205, 0.0008, 0.2, 252, 0.00952320545911789, 0.009288999252021313, 0.07238673452741462, 0.05683830683181028, 0.035266995429992676, 0.07738955318927765, 0.6114697781526793, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 205 0.001 256 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/205
374/374 - 6s - loss: 0.0205 - val_loss: 0.0138 - 6s/epoch - 16ms/step
Epoch 2/205
374/374 - 2s - loss: 0.0085 - val_loss: 0.0213 - 2s/epoch - 6ms/step
Epoch 3/205
374/374 - 2s - loss: 0.0085 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 4/205
374/374 - 2s - loss: 0.0078 - val_loss: 0.0089 - 2s/epoch - 6ms/step
Epoch 5/205
374/374 - 2s - loss: 0.0076 - val_loss: 0.0168 - 2s/epoch - 6ms/step
Epoch 6/205
374/374 - 2s - loss: 0.0075 - val_loss: 0.0091 - 2s/epoch - 6ms/step
Epoch 7/205
374/374 - 2s - loss: 0.0074 - val_loss: 0.0086 - 2s/epoch - 6ms/step
Epoch 8/205
374/374 - 2s - loss: 0.0072 - val_loss: 0.0090 - 2s/epoch - 6ms/step
Epoch 9/205
374/374 - 2s - loss: 0.0071 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 10/205
374/374 - 2s - loss: 0.0068 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 11/205
374/374 - 2s - loss: 0.0067 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 12/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0082 - 2s/epoch - 6ms/step
Epoch 13/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0193 - 2s/epoch - 6ms/step
Epoch 14/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 15/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 16/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 17/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 18/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0132 - 2s/epoch - 7ms/step
Epoch 19/205
374/374 - 2s - loss: 0.0071 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 20/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 21/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 22/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 23/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 24/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 25/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 26/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 27/205
374/374 - 2s - loss: 0.0075 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 28/205
374/374 - 2s - loss: 0.0082 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 29/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 30/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 31/205
374/374 - 2s - loss: 0.0066 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 32/205
374/374 - 2s - loss: 0.0114 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 33/205
374/374 - 2s - loss: 0.0074 - val_loss: 0.0076 - 2s/epoch - 7ms/step
Epoch 34/205
374/374 - 2s - loss: 0.0084 - val_loss: 0.0066 - 2s/epoch - 7ms/step
Epoch 35/205
374/374 - 3s - loss: 0.0065 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 36/205
374/374 - 3s - loss: 0.0072 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 37/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 38/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 39/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 40/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 41/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 42/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 43/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 44/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 45/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 46/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 47/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 48/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 49/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 50/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 51/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 52/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 53/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 54/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 55/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 56/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 57/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 58/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0083 - 2s/epoch - 6ms/step
Epoch 59/205
374/374 - 2s - loss: 0.0122 - val_loss: 0.0086 - 2s/epoch - 6ms/step
Epoch 60/205
374/374 - 2s - loss: 0.0070 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 61/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 62/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 63/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 64/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 65/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 66/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 67/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 68/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 69/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 70/205
374/374 - 2s - loss: 0.0110 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 71/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 72/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 73/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 74/205
374/374 - 3s - loss: 0.0084 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 75/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0072 - 2s/epoch - 7ms/step
Epoch 76/205
374/374 - 2s - loss: 0.0074 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 77/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0085 - 2s/epoch - 7ms/step
Epoch 78/205
374/374 - 2s - loss: 0.0082 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 79/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 80/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 81/205
374/374 - 2s - loss: 0.0064 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 82/205
374/374 - 2s - loss: 0.0077 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 83/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 84/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 85/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 86/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 87/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 88/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 89/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 90/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 91/205
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 92/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 93/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0083 - 2s/epoch - 6ms/step
Epoch 94/205
374/374 - 2s - loss: 0.0071 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 95/205
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 96/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 97/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 98/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 99/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 100/205
374/374 - 2s - loss: 0.0069 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 101/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 102/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 103/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 104/205
374/374 - 2s - loss: 0.0065 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 105/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 106/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 107/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 108/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 109/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 110/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 111/205
374/374 - 2s - loss: 0.0067 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 112/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 113/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 114/205
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 115/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 116/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 117/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 118/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 119/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 120/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 121/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 122/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 123/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 124/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 125/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 126/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0070 - 2s/epoch - 7ms/step
Epoch 127/205
374/374 - 2s - loss: 0.0063 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 128/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 129/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 130/205
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 131/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 132/205
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 133/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 134/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 135/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 136/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 137/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 138/205
374/374 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 139/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 140/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 141/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 142/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 143/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 144/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 145/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 146/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 147/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 148/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 149/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 150/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 151/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 152/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 153/205
374/374 - 3s - loss: 0.0059 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 154/205
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 155/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 156/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 157/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 158/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 159/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 160/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 161/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 162/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 163/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 164/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 165/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 166/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 167/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 168/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 169/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 170/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 171/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 172/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 173/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 174/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 175/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 176/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 177/205
374/374 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 178/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 179/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 180/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 181/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 182/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 183/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 184/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 185/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 186/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 187/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 188/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 189/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 7ms/step
Epoch 190/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 191/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 192/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 193/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 194/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 195/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 196/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 197/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 7ms/step
Epoch 198/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 199/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 200/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 201/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 202/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 203/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 204/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
Epoch 205/205
374/374 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00583870941773057
  1/332 [..............................] - ETA: 47s 33/332 [=>............................] - ETA: 0s  66/332 [====>.........................] - ETA: 0s 99/332 [=======>......................] - ETA: 0s133/332 [===========>..................] - ETA: 0s165/332 [=============>................] - ETA: 0s196/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s260/332 [======================>.......] - ETA: 0s293/332 [=========================>....] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11306999180616184
cosine 0.08840864833305342
MAE: 0.044090997
RMSE: 0.09580906
r2: 0.4045115860577477
RMSE zero-vector: 0.23411466903540806
['1.9custom_VAE', 'logcosh', 256, 205, 0.001, 0.2, 252, 0.0058813318610191345, 0.00583870941773057, 0.11306999180616184, 0.08840864833305342, 0.04409099742770195, 0.09580905735492706, 0.4045115860577477, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[2.0 145 0.0008 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/145
747/747 - 7s - loss: 0.0309 - val_loss: 0.0186 - 7s/epoch - 10ms/step
Epoch 2/145
747/747 - 4s - loss: 0.0159 - val_loss: 0.0195 - 4s/epoch - 5ms/step
Epoch 3/145
747/747 - 5s - loss: 0.0148 - val_loss: 0.0157 - 5s/epoch - 6ms/step
Epoch 4/145
747/747 - 4s - loss: 0.0135 - val_loss: 0.0139 - 4s/epoch - 6ms/step
Epoch 5/145
747/747 - 4s - loss: 0.0127 - val_loss: 0.0127 - 4s/epoch - 5ms/step
Epoch 6/145
747/747 - 4s - loss: 0.0122 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 7/145
747/747 - 4s - loss: 0.0119 - val_loss: 0.0165 - 4s/epoch - 6ms/step
Epoch 8/145
747/747 - 4s - loss: 0.0117 - val_loss: 0.0141 - 4s/epoch - 5ms/step
Epoch 9/145
747/747 - 4s - loss: 0.0123 - val_loss: 0.0147 - 4s/epoch - 5ms/step
Epoch 10/145
747/747 - 4s - loss: 0.0131 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 11/145
747/747 - 4s - loss: 0.0113 - val_loss: 0.0107 - 4s/epoch - 6ms/step
Epoch 12/145
747/747 - 4s - loss: 0.0109 - val_loss: 0.0118 - 4s/epoch - 6ms/step
Epoch 13/145
747/747 - 4s - loss: 0.0125 - val_loss: 0.0108 - 4s/epoch - 6ms/step
Epoch 14/145
747/747 - 4s - loss: 0.0113 - val_loss: 0.0116 - 4s/epoch - 6ms/step
Epoch 15/145
747/747 - 4s - loss: 0.0117 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 16/145
747/747 - 4s - loss: 0.0118 - val_loss: 0.0119 - 4s/epoch - 6ms/step
Epoch 17/145
747/747 - 4s - loss: 0.0139 - val_loss: 0.0121 - 4s/epoch - 6ms/step
Epoch 18/145
747/747 - 4s - loss: 0.0117 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 19/145
747/747 - 4s - loss: 0.0111 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 20/145
747/747 - 5s - loss: 0.0124 - val_loss: 0.0119 - 5s/epoch - 6ms/step
Epoch 21/145
747/747 - 4s - loss: 0.0137 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 22/145
747/747 - 4s - loss: 0.0121 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 23/145
747/747 - 5s - loss: 0.0109 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 24/145
747/747 - 4s - loss: 0.0108 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 25/145
747/747 - 4s - loss: 0.0107 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 26/145
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 27/145
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 28/145
747/747 - 4s - loss: 0.0105 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 29/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 30/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 31/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 32/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0119 - 4s/epoch - 5ms/step
Epoch 33/145
747/747 - 4s - loss: 0.0116 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 34/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 35/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 36/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0107 - 4s/epoch - 6ms/step
Epoch 37/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 38/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 39/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 40/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 41/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 42/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 43/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 44/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 45/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 46/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 47/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 48/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 49/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 50/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 51/145
747/747 - 4s - loss: 0.0106 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 52/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 53/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 54/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 55/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 56/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 57/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 58/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 59/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 60/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 61/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 62/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 63/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 64/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 65/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 66/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 67/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 68/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 69/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 70/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 71/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 72/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 73/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 74/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 75/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 76/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 77/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 78/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 79/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 80/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 81/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 82/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 83/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 84/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 85/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 86/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 7ms/step
Epoch 87/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 88/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 89/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 90/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 91/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 92/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 93/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 94/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 95/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 96/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 97/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 98/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 99/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 100/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 101/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 102/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 103/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 104/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 105/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 106/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 107/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 108/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 109/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 110/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 111/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 112/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 113/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 114/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 115/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 116/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 117/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 118/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 119/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 120/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 7ms/step
Epoch 121/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 122/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 123/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 124/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 125/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 126/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 127/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 128/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 129/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 130/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 131/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 132/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 133/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 134/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 135/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 136/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 137/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 138/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 139/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 140/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 141/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 142/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 143/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 144/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 145/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009254906326532364
  1/332 [..............................] - ETA: 54s 30/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s119/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s178/332 [===============>..............] - ETA: 0s206/332 [=================>............] - ETA: 0s237/332 [====================>.........] - ETA: 0s267/332 [=======================>......] - ETA: 0s295/332 [=========================>....] - ETA: 0s323/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07279293885601272
cosine 0.05715361432426164
MAE: 0.03548775
RMSE: 0.07763039
r2: 0.6090478330043321
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 128, 145, 0.0008, 0.2, 252, 0.009452402591705322, 0.009254906326532364, 0.07279293885601272, 0.05715361432426164, 0.03548774868249893, 0.07763039320707321, 0.6090478330043321, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 205 0.0006000000000000001 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
Epoch 1/205
1493/1493 - 13s - loss: 0.0141 - val_loss: 0.0162 - 13s/epoch - 9ms/step
Epoch 2/205
1493/1493 - 8s - loss: 0.0080 - val_loss: 0.0075 - 8s/epoch - 5ms/step
Epoch 3/205
1493/1493 - 8s - loss: 0.0072 - val_loss: 0.0070 - 8s/epoch - 6ms/step
Epoch 4/205
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0068 - 8s/epoch - 6ms/step
Epoch 5/205
1493/1493 - 8s - loss: 0.0065 - val_loss: 0.0063 - 8s/epoch - 5ms/step
Epoch 6/205
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0063 - 8s/epoch - 5ms/step
Epoch 7/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0063 - 8s/epoch - 5ms/step
Epoch 8/205
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 9/205
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 10/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 11/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 12/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 13/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 14/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 15/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 16/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 17/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 18/205
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 19/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 20/205
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 21/205
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 22/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 23/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 24/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 25/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 26/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 27/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 28/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 29/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 30/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 31/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 32/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 33/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 34/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 35/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 36/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 37/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 38/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 39/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 40/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 41/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 42/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 43/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 44/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 45/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 46/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 47/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 48/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 49/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 50/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 51/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 52/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 53/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 54/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 55/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 56/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 57/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 58/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 59/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 60/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 61/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 62/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 63/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 64/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 65/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 66/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 67/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 68/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 69/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 70/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 71/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 72/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 73/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 74/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 75/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 76/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 77/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 78/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 79/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 80/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 81/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 82/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 83/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 84/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 85/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 86/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 87/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 88/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 89/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 90/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 91/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 92/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 93/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 94/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 95/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 96/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 97/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 98/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 99/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 100/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 101/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 102/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 103/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 104/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 105/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 106/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 107/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 108/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 109/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 110/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 111/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 112/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 113/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 114/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 115/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 116/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 117/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 118/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 119/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 120/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 121/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 122/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 123/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 124/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 125/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 126/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 127/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 128/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 129/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 130/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 131/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 132/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 133/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 134/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 135/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 136/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 137/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 138/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 139/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 140/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 141/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 142/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 143/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 144/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 145/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 146/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 147/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 148/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 149/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 150/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 151/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 152/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 153/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 154/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 155/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 156/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 157/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 158/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 159/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 160/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 161/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 162/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 163/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 164/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 165/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 166/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 167/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 168/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 169/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 170/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 171/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 172/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 173/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 174/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 175/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 176/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 177/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 178/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 179/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 180/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 181/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 182/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 183/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 184/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 185/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 186/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 187/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 188/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 189/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 190/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 191/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 192/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 193/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 194/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 195/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 196/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 197/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 198/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 199/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 200/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 201/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 202/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 203/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 204/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 205/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005904252640902996
  1/332 [..............................] - ETA: 52s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s248/332 [=====================>........] - ETA: 0s280/332 [========================>.....] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11543236904848848
cosine 0.09034059652226671
MAE: 0.044708677
RMSE: 0.09683353
r2: 0.39170851242560295
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'logcosh', 64, 205, 0.0006000000000000001, 0.2, 252, 0.005951965227723122, 0.005904252640902996, 0.11543236904848848, 0.09034059652226671, 0.044708676636219025, 0.09683352708816528, 0.39170851242560295, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 200 0.0008 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
Epoch 1/200
1493/1493 - 12s - loss: 0.0140 - val_loss: 0.0677 - 12s/epoch - 8ms/step
Epoch 2/200
1493/1493 - 8s - loss: 0.0083 - val_loss: 0.0079 - 8s/epoch - 6ms/step
Epoch 3/200
1493/1493 - 9s - loss: 0.0070 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 4/200
1493/1493 - 8s - loss: 0.0066 - val_loss: 0.0078 - 8s/epoch - 6ms/step
Epoch 5/200
1493/1493 - 8s - loss: 0.0065 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 6/200
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0064 - 8s/epoch - 6ms/step
Epoch 7/200
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 8/200
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 9/200
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 10/200
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 11/200
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 12/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 13/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 14/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 15/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 16/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 17/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 18/200
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 19/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 20/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 21/200
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 22/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 23/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 24/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 25/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 26/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 27/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 28/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 29/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 30/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 31/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 32/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 33/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 34/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 35/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 36/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 37/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 38/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 39/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 40/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 41/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 42/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 43/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 44/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 45/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 46/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 47/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 48/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 49/200
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 50/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 51/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 52/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 53/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 54/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 55/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 56/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 57/200
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 58/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 59/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 60/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 61/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 62/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 63/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 64/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 65/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 66/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 67/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 68/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 69/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 70/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 71/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 72/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 73/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 74/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 75/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 76/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 77/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 78/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 79/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 80/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 81/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 82/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 83/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 84/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 85/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 86/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 87/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 88/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 89/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 90/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 91/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 92/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 93/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 94/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 95/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 96/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 97/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 98/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 99/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 100/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 101/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 102/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 103/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 104/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 105/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 106/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 107/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 108/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 109/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 110/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 111/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 112/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 113/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 114/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 115/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 116/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 117/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 118/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 119/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 120/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 121/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 122/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 123/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 124/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 125/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 126/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 127/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 128/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 129/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 130/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 131/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 132/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 133/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 134/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 135/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 136/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 137/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 138/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 139/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 140/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 141/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 142/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 143/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 144/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 145/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 146/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 147/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 148/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 149/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 150/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 151/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 152/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 153/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 154/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 155/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 156/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 157/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 158/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 159/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 160/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 161/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 162/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 163/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 164/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 165/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 166/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 167/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 168/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 169/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 170/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 171/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 172/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 173/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 174/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 175/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 176/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 177/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 178/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 179/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 180/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 181/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 182/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 183/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 184/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 185/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 186/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 187/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 188/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 189/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 190/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 191/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 192/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 193/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 194/200
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 195/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 196/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 197/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 198/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 199/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 200/200
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005915312096476555
  1/332 [..............................] - ETA: 4:43 30/332 [=>............................] - ETA: 0s   60/332 [====>.........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s119/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s179/332 [===============>..............] - ETA: 0s210/332 [=================>............] - ETA: 0s241/332 [====================>.........] - ETA: 0s270/332 [=======================>......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11597047885476172
cosine 0.09074758644199263
MAE: 0.044808038
RMSE: 0.0969742
r2: 0.389939875747924
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'logcosh', 64, 200, 0.0008, 0.2, 252, 0.005960850976407528, 0.005915312096476555, 0.11597047885476172, 0.09074758644199263, 0.0448080375790596, 0.0969742015004158, 0.389939875747924, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 145 0.001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2907)         3677355     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2907)        11628       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2907)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4487575     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,642,190
Trainable params: 9,630,058
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 15s - loss: 0.0260 - val_loss: 0.0201 - 15s/epoch - 10ms/step
Epoch 2/145
1493/1493 - 8s - loss: 0.0148 - val_loss: 0.0151 - 8s/epoch - 6ms/step
Epoch 3/145
1493/1493 - 8s - loss: 0.0131 - val_loss: 0.0122 - 8s/epoch - 6ms/step
Epoch 4/145
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0114 - 9s/epoch - 6ms/step
Epoch 5/145
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0112 - 8s/epoch - 6ms/step
Epoch 6/145
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0110 - 8s/epoch - 6ms/step
Epoch 7/145
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 9/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 10/145
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 11/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 12/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 13/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 16/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 17/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 18/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 19/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 21/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 22/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 23/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 24/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 25/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 27/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 28/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 29/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 30/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 31/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 33/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 34/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 35/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 36/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 37/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 38/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 39/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 40/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 44/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 45/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 46/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 47/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 49/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 50/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 51/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 52/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 53/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 56/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 57/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 58/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 59/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 60/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 61/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 62/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 63/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 64/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 65/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 66/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 69/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 70/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 71/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 72/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 73/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 74/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 75/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 76/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 86/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 87/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 90/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 92/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 93/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 102/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 103/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 104/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 110/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 111/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 112/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 117/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 119/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 121/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 125/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 127/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 128/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 131/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 132/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 133/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 134/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 135/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 145/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009325094521045685
  1/332 [..............................] - ETA: 56s 30/332 [=>............................] - ETA: 0s  58/332 [====>.........................] - ETA: 0s 88/332 [======>.......................] - ETA: 0s118/332 [=========>....................] - ETA: 0s146/332 [============>.................] - ETA: 0s174/332 [==============>...............] - ETA: 0s203/332 [=================>............] - ETA: 0s233/332 [====================>.........] - ETA: 0s263/332 [======================>.......] - ETA: 0s295/332 [=========================>....] - ETA: 0s326/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0727483429101742
cosine 0.05711883367163884
MAE: 0.035516635
RMSE: 0.07769095
r2: 0.6084377759392934
RMSE zero-vector: 0.23411466903540806
['2.3000000000000003custom_VAE', 'mse', 64, 145, 0.001, 0.2, 252, 0.009516239166259766, 0.009325094521045685, 0.0727483429101742, 0.05711883367163884, 0.035516634583473206, 0.07769095152616501, 0.6084377759392934, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 205 0.0008 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/205
1493/1493 - 13s - loss: 0.0139 - val_loss: 0.0106 - 13s/epoch - 9ms/step
Epoch 2/205
1493/1493 - 8s - loss: 0.0081 - val_loss: 0.0247 - 8s/epoch - 5ms/step
Epoch 3/205
1493/1493 - 8s - loss: 0.0070 - val_loss: 0.0137 - 8s/epoch - 6ms/step
Epoch 4/205
1493/1493 - 8s - loss: 0.0066 - val_loss: 0.0064 - 8s/epoch - 6ms/step
Epoch 5/205
1493/1493 - 9s - loss: 0.0064 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 6/205
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 7/205
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 8/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 9/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 10/205
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 11/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 12/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 13/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 14/205
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 15/205
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 16/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 17/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 18/205
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 19/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 20/205
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 21/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 22/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 23/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 24/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 25/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 26/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 27/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 28/205
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 29/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 30/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 31/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 32/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 33/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 34/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 35/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 36/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 37/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 38/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 39/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 40/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 41/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 42/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 43/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 44/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 45/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 46/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 47/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 48/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 49/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 50/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 51/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 52/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 53/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 54/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 55/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 56/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 57/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 58/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 59/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 60/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 61/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 62/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 63/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 64/205
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 65/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 66/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 67/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 68/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 69/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 70/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 71/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 72/205
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 73/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 74/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 75/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 76/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 77/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 78/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 79/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 80/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 81/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 82/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 83/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 84/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 85/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 86/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 87/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 88/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 89/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 90/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 91/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 92/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 93/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 94/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 95/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 96/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 97/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 98/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 99/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 100/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 101/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 102/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 103/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 104/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 105/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 106/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 107/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 108/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 109/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 110/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 111/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 112/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 113/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 114/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 115/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 116/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 117/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 118/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 119/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 120/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 121/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 122/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 123/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 124/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 125/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 126/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 127/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 128/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 129/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 130/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 131/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 132/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 133/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 134/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 135/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 136/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 137/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 138/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 139/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 140/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 141/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 142/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 143/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 144/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 145/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 146/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 147/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 148/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 149/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 150/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 151/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 152/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 153/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 154/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 155/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 156/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 157/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 158/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 159/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 160/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 161/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 162/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 163/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 164/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 165/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 166/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 167/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 168/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 169/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 170/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 171/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 172/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 173/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 174/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 175/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 176/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 177/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 178/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 179/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 180/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 181/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 182/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 183/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 184/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 185/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 186/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 187/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 188/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 189/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 190/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 191/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 5ms/step
Epoch 192/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 193/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 194/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 195/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 196/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 197/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 198/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 199/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
Epoch 200/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 201/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 202/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 203/205
1493/1493 - 8s - loss: 0.0060 - val_loss: 0.0059 - 8s/epoch - 6ms/step
Epoch 204/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 205/205
1493/1493 - 9s - loss: 0.0060 - val_loss: 0.0059 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005943573545664549
  1/332 [..............................] - ETA: 1:00 32/332 [=>............................] - ETA: 0s   64/332 [====>.........................] - ETA: 0s 95/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s152/332 [============>.................] - ETA: 0s181/332 [===============>..............] - ETA: 0s211/332 [==================>...........] - ETA: 0s240/332 [====================>.........] - ETA: 0s271/332 [=======================>......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12010152220916982
cosine 0.09399857050488232
MAE: 0.045820307
RMSE: 0.09866418
r2: 0.3684916244317473
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'logcosh', 64, 205, 0.0008, 0.2, 252, 0.005990760400891304, 0.005943573545664549, 0.12010152220916982, 0.09399857050488232, 0.04582030698657036, 0.09866417944431305, 0.3684916244317473, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 145 0.002 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/145
747/747 - 8s - loss: 0.0308 - val_loss: 0.0184 - 8s/epoch - 10ms/step
Epoch 2/145
747/747 - 5s - loss: 0.0154 - val_loss: 0.0169 - 5s/epoch - 6ms/step
Epoch 3/145
747/747 - 4s - loss: 0.0142 - val_loss: 0.0173 - 4s/epoch - 6ms/step
Epoch 4/145
747/747 - 4s - loss: 0.0136 - val_loss: 0.0144 - 4s/epoch - 6ms/step
Epoch 5/145
747/747 - 4s - loss: 0.0127 - val_loss: 0.0292 - 4s/epoch - 6ms/step
Epoch 6/145
747/747 - 4s - loss: 0.0123 - val_loss: 0.0123 - 4s/epoch - 6ms/step
Epoch 7/145
747/747 - 4s - loss: 0.0117 - val_loss: 0.0155 - 4s/epoch - 6ms/step
Epoch 8/145
747/747 - 4s - loss: 0.0120 - val_loss: 0.0135 - 4s/epoch - 6ms/step
Epoch 9/145
747/747 - 5s - loss: 0.0120 - val_loss: 0.0109 - 5s/epoch - 7ms/step
Epoch 10/145
747/747 - 4s - loss: 0.0110 - val_loss: 0.0117 - 4s/epoch - 6ms/step
Epoch 11/145
747/747 - 5s - loss: 0.0117 - val_loss: 0.0123 - 5s/epoch - 6ms/step
Epoch 12/145
747/747 - 5s - loss: 0.0123 - val_loss: 0.0131 - 5s/epoch - 6ms/step
Epoch 13/145
747/747 - 5s - loss: 0.0126 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 14/145
747/747 - 4s - loss: 0.0113 - val_loss: 0.0120 - 4s/epoch - 6ms/step
Epoch 15/145
747/747 - 4s - loss: 0.0132 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 16/145
747/747 - 4s - loss: 0.0111 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 17/145
747/747 - 4s - loss: 0.0109 - val_loss: 0.0113 - 4s/epoch - 6ms/step
Epoch 18/145
747/747 - 5s - loss: 0.0119 - val_loss: 0.0122 - 5s/epoch - 6ms/step
Epoch 19/145
747/747 - 4s - loss: 0.0124 - val_loss: 0.0107 - 4s/epoch - 6ms/step
Epoch 20/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 21/145
747/747 - 4s - loss: 0.0111 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 22/145
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 23/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 24/145
747/747 - 4s - loss: 0.0115 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 25/145
747/747 - 4s - loss: 0.0105 - val_loss: 0.0120 - 4s/epoch - 6ms/step
Epoch 26/145
747/747 - 4s - loss: 0.0120 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 27/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 28/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 29/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0119 - 5s/epoch - 6ms/step
Epoch 30/145
747/747 - 4s - loss: 0.0123 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 31/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 32/145
747/747 - 5s - loss: 0.0113 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 33/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 34/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 35/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 36/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 37/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 38/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 39/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 40/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0115 - 5s/epoch - 6ms/step
Epoch 41/145
747/747 - 4s - loss: 0.0117 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 42/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 43/145
747/747 - 5s - loss: 0.0111 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 44/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 45/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 46/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 47/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 48/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 49/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 50/145
747/747 - 4s - loss: 0.0103 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 51/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 52/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 53/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 54/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 55/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 56/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 57/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 58/145
747/747 - 4s - loss: 0.0100 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 59/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 60/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 61/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 62/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 63/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 64/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 65/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 66/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 67/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 68/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 69/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 70/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 71/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 72/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 73/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 74/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 75/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 76/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 77/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 78/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 79/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 80/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 81/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 82/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 7ms/step
Epoch 83/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 84/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 85/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 7ms/step
Epoch 86/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 87/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 88/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 89/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 90/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 91/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 92/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 93/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 94/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 95/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 96/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 97/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 98/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 99/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 100/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 101/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 102/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 103/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 104/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 105/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 106/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 107/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 108/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 109/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 110/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 111/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 112/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 113/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 114/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 115/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 116/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 117/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 118/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 7ms/step
Epoch 119/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 120/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 121/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 122/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 123/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 124/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 125/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 126/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 127/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 128/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 129/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 130/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 131/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 132/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 133/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 134/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 135/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 136/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 137/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 138/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 139/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 140/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 141/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 142/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 143/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 144/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 145/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009275061078369617
  1/332 [..............................] - ETA: 50s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s159/332 [=============>................] - ETA: 0s191/332 [================>.............] - ETA: 0s223/332 [===================>..........] - ETA: 0s255/332 [======================>.......] - ETA: 0s287/332 [========================>.....] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07320804978221307
cosine 0.05745231264431972
MAE: 0.03547156
RMSE: 0.0778004
r2: 0.6073336174713769
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 128, 145, 0.002, 0.2, 252, 0.00946896430104971, 0.009275061078369617, 0.07320804978221307, 0.05745231264431972, 0.035471558570861816, 0.07780040055513382, 0.6073336174713769, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 205 0.0008 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2907)         3677355     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2907)        11628       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2907)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4487575     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,642,190
Trainable params: 9,630,058
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/205
1493/1493 - 11s - loss: 0.0262 - val_loss: 0.0178 - 11s/epoch - 8ms/step
Epoch 2/205
1493/1493 - 8s - loss: 0.0149 - val_loss: 0.0351 - 8s/epoch - 6ms/step
Epoch 3/205
1493/1493 - 8s - loss: 0.0135 - val_loss: 0.0120 - 8s/epoch - 6ms/step
Epoch 4/205
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0121 - 9s/epoch - 6ms/step
Epoch 5/205
1493/1493 - 9s - loss: 0.0122 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 6/205
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 7/205
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 8/205
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 9/205
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 10/205
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 11/205
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 12/205
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 13/205
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 14/205
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 15/205
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 16/205
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 17/205
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 18/205
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 19/205
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 20/205
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 21/205
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 22/205
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 23/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 24/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 25/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 26/205
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 27/205
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 28/205
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 29/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 30/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 31/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 32/205
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 33/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 34/205
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 35/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 36/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 37/205
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 38/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 39/205
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 40/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 41/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 42/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 43/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 44/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 45/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 46/205
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 47/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 48/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 49/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 50/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 51/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 52/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 53/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 54/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 55/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 56/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 57/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 58/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 59/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 60/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 61/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 62/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 63/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 64/205
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 65/205
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 66/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 67/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 68/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 69/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 70/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 71/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 72/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 73/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 74/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 75/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 76/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 77/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 78/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 79/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 80/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 81/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 82/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 84/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 85/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 86/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 87/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 88/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 89/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 90/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 91/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 92/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 93/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 94/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 95/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 96/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 97/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 98/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 99/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 100/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 101/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 102/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 103/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 104/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 105/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 106/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 107/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 108/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 109/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 110/205
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 111/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 112/205
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 113/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 114/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 115/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 116/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 117/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 118/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 119/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 120/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 121/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 122/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 123/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 124/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 125/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 126/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 127/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 128/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 129/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 130/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 131/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 132/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 133/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 134/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 135/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 136/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 137/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 138/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 139/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 140/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 141/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 142/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 143/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 144/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 145/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 146/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 147/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 148/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 149/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 150/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 151/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 152/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 153/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 154/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 155/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 156/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 157/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 158/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 159/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 160/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 161/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 162/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 163/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 164/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 165/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 166/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 167/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 168/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 169/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 170/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 171/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 172/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 173/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 174/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 175/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 176/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 177/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 178/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 179/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 180/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 181/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 182/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 183/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 184/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 185/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 186/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 187/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 188/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 189/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 190/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 191/205
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 192/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 193/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 194/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 195/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 196/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 197/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 198/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 199/205
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 200/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 201/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 202/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 203/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 204/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 205/205
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009327240288257599
  1/332 [..............................] - ETA: 55s 32/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s120/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s177/332 [==============>...............] - ETA: 0s209/332 [=================>............] - ETA: 0s239/332 [====================>.........] - ETA: 0s269/332 [=======================>......] - ETA: 0s298/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07358324727695752
cosine 0.057772283043180196
MAE: 0.03559962
RMSE: 0.07801097
r2: 0.6052051888954901
RMSE zero-vector: 0.23411466903540806
['2.3000000000000003custom_VAE', 'mse', 64, 205, 0.0008, 0.2, 252, 0.009542996995151043, 0.009327240288257599, 0.07358324727695752, 0.057772283043180196, 0.03559961915016174, 0.07801096886396408, 0.6052051888954901, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 145 0.0008 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2907)         3677355     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2907)        11628       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2907)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4487575     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,642,190
Trainable params: 9,630,058
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 13s - loss: 0.0268 - val_loss: 0.0216 - 13s/epoch - 9ms/step
Epoch 2/145
1493/1493 - 9s - loss: 0.0156 - val_loss: 0.0135 - 9s/epoch - 6ms/step
Epoch 3/145
1493/1493 - 9s - loss: 0.0130 - val_loss: 0.0317 - 9s/epoch - 6ms/step
Epoch 4/145
1493/1493 - 9s - loss: 0.0138 - val_loss: 0.0119 - 9s/epoch - 6ms/step
Epoch 5/145
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0112 - 8s/epoch - 6ms/step
Epoch 6/145
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0110 - 8s/epoch - 6ms/step
Epoch 7/145
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 9/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 10/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 11/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 12/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 13/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 16/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 17/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 18/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 19/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 21/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 22/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 23/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 24/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 25/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 27/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 28/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 29/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 30/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 31/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 33/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 34/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 35/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 36/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 37/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 38/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 39/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 40/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 44/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 45/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 46/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 47/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 49/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 50/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 51/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 52/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 53/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 56/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 57/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 58/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 59/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 60/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 61/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 62/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 63/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 64/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 65/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 66/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 69/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 70/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 71/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 72/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 73/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 74/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 75/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 76/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 86/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 87/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 90/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 92/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 93/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 102/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 103/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 104/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 110/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 111/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 112/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 117/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 119/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 121/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 125/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 127/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 128/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 131/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 132/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 133/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 134/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 135/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 145/145
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009293288923799992
  1/332 [..............................] - ETA: 56s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s120/332 [=========>....................] - ETA: 0s151/332 [============>.................] - ETA: 0s182/332 [===============>..............] - ETA: 0s213/332 [==================>...........] - ETA: 0s244/332 [=====================>........] - ETA: 0s275/332 [=======================>......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07253528437185773
cosine 0.056935861959486155
MAE: 0.03529867
RMSE: 0.07751062
r2: 0.610253313363208
RMSE zero-vector: 0.23411466903540806
['2.3000000000000003custom_VAE', 'mse', 64, 145, 0.0008, 0.2, 252, 0.009538576006889343, 0.009293288923799992, 0.07253528437185773, 0.056935861959486155, 0.0352986715734005, 0.07751061767339706, 0.610253313363208, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[1.9 145 0.0008 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/145
374/374 - 6s - loss: 0.0375 - val_loss: 0.0186 - 6s/epoch - 16ms/step
Epoch 2/145
374/374 - 3s - loss: 0.0165 - val_loss: 0.0350 - 3s/epoch - 7ms/step
Epoch 3/145
374/374 - 2s - loss: 0.0169 - val_loss: 0.0160 - 2s/epoch - 7ms/step
Epoch 4/145
374/374 - 3s - loss: 0.0145 - val_loss: 0.0200 - 3s/epoch - 7ms/step
Epoch 5/145
374/374 - 2s - loss: 0.0141 - val_loss: 0.0353 - 2s/epoch - 7ms/step
Epoch 6/145
374/374 - 2s - loss: 0.0140 - val_loss: 0.0165 - 2s/epoch - 7ms/step
Epoch 7/145
374/374 - 2s - loss: 0.0128 - val_loss: 0.0143 - 2s/epoch - 6ms/step
Epoch 8/145
374/374 - 2s - loss: 0.0124 - val_loss: 0.0415 - 2s/epoch - 7ms/step
Epoch 9/145
374/374 - 3s - loss: 0.0126 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 10/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0160 - 3s/epoch - 7ms/step
Epoch 11/145
374/374 - 2s - loss: 0.0119 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 12/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 13/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0318 - 3s/epoch - 7ms/step
Epoch 14/145
374/374 - 2s - loss: 0.0118 - val_loss: 0.0198 - 2s/epoch - 6ms/step
Epoch 15/145
374/374 - 2s - loss: 0.0117 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 16/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0109 - 2s/epoch - 7ms/step
Epoch 17/145
374/374 - 2s - loss: 0.0109 - val_loss: 0.0134 - 2s/epoch - 7ms/step
Epoch 18/145
374/374 - 2s - loss: 0.0121 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 19/145
374/374 - 2s - loss: 0.0111 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 20/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 21/145
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 22/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 23/145
374/374 - 2s - loss: 0.0120 - val_loss: 0.0131 - 2s/epoch - 7ms/step
Epoch 24/145
374/374 - 2s - loss: 0.0166 - val_loss: 0.0125 - 2s/epoch - 7ms/step
Epoch 25/145
374/374 - 3s - loss: 0.0187 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 26/145
374/374 - 2s - loss: 0.0199 - val_loss: 0.0138 - 2s/epoch - 7ms/step
Epoch 27/145
374/374 - 2s - loss: 0.0265 - val_loss: 0.0129 - 2s/epoch - 7ms/step
Epoch 28/145
374/374 - 2s - loss: 0.0125 - val_loss: 0.0119 - 2s/epoch - 7ms/step
Epoch 29/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 30/145
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 7ms/step
Epoch 31/145
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 32/145
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 7ms/step
Epoch 33/145
374/374 - 2s - loss: 0.0135 - val_loss: 0.0181 - 2s/epoch - 7ms/step
Epoch 34/145
374/374 - 2s - loss: 0.0355 - val_loss: 0.0157 - 2s/epoch - 6ms/step
Epoch 35/145
374/374 - 2s - loss: 0.0140 - val_loss: 0.0155 - 2s/epoch - 6ms/step
Epoch 36/145
374/374 - 2s - loss: 0.0292 - val_loss: 0.0139 - 2s/epoch - 6ms/step
Epoch 37/145
374/374 - 2s - loss: 0.0131 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 38/145
374/374 - 2s - loss: 0.0125 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 39/145
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 40/145
374/374 - 2s - loss: 0.0123 - val_loss: 0.0136 - 2s/epoch - 7ms/step
Epoch 41/145
374/374 - 3s - loss: 0.0164 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 42/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 43/145
374/374 - 2s - loss: 0.0133 - val_loss: 0.0123 - 2s/epoch - 7ms/step
Epoch 44/145
374/374 - 2s - loss: 0.0127 - val_loss: 0.0116 - 2s/epoch - 7ms/step
Epoch 45/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 46/145
374/374 - 2s - loss: 0.0115 - val_loss: 0.0116 - 2s/epoch - 7ms/step
Epoch 47/145
374/374 - 2s - loss: 0.0124 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 48/145
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 49/145
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 50/145
374/374 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 51/145
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 7ms/step
Epoch 52/145
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 53/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 54/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 55/145
374/374 - 2s - loss: 0.0135 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 56/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 7ms/step
Epoch 57/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 58/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 59/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0230 - 2s/epoch - 7ms/step
Epoch 60/145
374/374 - 2s - loss: 0.0247 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 61/145
374/374 - 2s - loss: 0.0126 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 62/145
374/374 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 63/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 64/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 65/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 66/145
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 67/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 68/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 69/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0108 - 2s/epoch - 7ms/step
Epoch 70/145
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 71/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 72/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 73/145
374/374 - 2s - loss: 0.0115 - val_loss: 0.0122 - 2s/epoch - 7ms/step
Epoch 74/145
374/374 - 2s - loss: 0.0106 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 75/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 76/145
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 77/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 78/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 79/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 80/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0130 - 2s/epoch - 7ms/step
Epoch 81/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 82/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 83/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 84/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 85/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 86/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 87/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 88/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 89/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 90/145
374/374 - 2s - loss: 0.0115 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 91/145
374/374 - 2s - loss: 0.0127 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 92/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 93/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 94/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0120 - 2s/epoch - 7ms/step
Epoch 95/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 96/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 97/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 98/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 99/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 100/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 101/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 102/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 103/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 104/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 105/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 106/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 107/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 108/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 109/145
374/374 - 2s - loss: 0.0130 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 110/145
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 111/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 112/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 113/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 114/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 115/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 116/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 117/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0133 - 2s/epoch - 7ms/step
Epoch 118/145
374/374 - 2s - loss: 0.0115 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 119/145
374/374 - 2s - loss: 0.0101 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 120/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 121/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 122/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 123/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 124/145
374/374 - 2s - loss: 0.0146 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 125/145
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 126/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 127/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 128/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 129/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 130/145
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 131/145
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 132/145
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 133/145
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 134/145
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 135/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0108 - 2s/epoch - 7ms/step
Epoch 136/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 137/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 138/145
374/374 - 3s - loss: 0.0097 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 139/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 140/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 141/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 142/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 143/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 144/145
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 145/145
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009555457159876823
  1/332 [..............................] - ETA: 49s 30/332 [=>............................] - ETA: 0s  60/332 [====>.........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s119/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s179/332 [===============>..............] - ETA: 0s209/332 [=================>............] - ETA: 0s238/332 [====================>.........] - ETA: 0s265/332 [======================>.......] - ETA: 0s292/332 [=========================>....] - ETA: 0s321/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07803380213006685
cosine 0.061193116450147994
MAE: 0.03700124
RMSE: 0.080242254
r2: 0.582298421506583
RMSE zero-vector: 0.23411466903540806
['1.9custom_VAE', 'mse', 256, 145, 0.0008, 0.2, 252, 0.009651965461671352, 0.009555457159876823, 0.07803380213006685, 0.061193116450147994, 0.037001240998506546, 0.08024225383996964, 0.582298421506583, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4000000000000004 145 0.001 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3033)         3836745     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3033)        12132       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3033)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          764568      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          764568      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4679221     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,057,234
Trainable params: 10,044,598
Non-trainable params: 12,636
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 17s - loss: 0.0267 - val_loss: 0.0162 - 17s/epoch - 11ms/step
Epoch 2/145
1493/1493 - 9s - loss: 0.0149 - val_loss: 0.0213 - 9s/epoch - 6ms/step
Epoch 3/145
1493/1493 - 9s - loss: 0.0131 - val_loss: 0.0120 - 9s/epoch - 6ms/step
Epoch 4/145
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0116 - 9s/epoch - 6ms/step
Epoch 5/145
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0115 - 9s/epoch - 6ms/step
Epoch 6/145
1493/1493 - 9s - loss: 0.0116 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 7/145
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 9/145
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 10/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 11/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 12/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 13/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 16/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 17/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 18/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 19/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 21/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 22/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 23/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 24/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 25/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 27/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 28/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 29/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 30/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 31/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 33/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 34/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 35/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 36/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 37/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 38/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 39/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 40/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 44/145
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 45/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 46/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 47/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 49/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 50/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 51/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 52/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 53/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 56/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 57/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 58/145
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 59/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 60/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 61/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 62/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 63/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 64/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 65/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 66/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 69/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 70/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 71/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 72/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 73/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 74/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 75/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 76/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 86/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 87/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 90/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 92/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 93/145
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 102/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 103/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 104/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 110/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 111/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 112/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 117/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 119/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 121/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 125/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 127/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 128/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 131/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 132/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 133/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 134/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 135/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 145/145
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009324951097369194
  1/332 [..............................] - ETA: 52s 31/332 [=>............................] - ETA: 0s  60/332 [====>.........................] - ETA: 0s 88/332 [======>.......................] - ETA: 0s117/332 [=========>....................] - ETA: 0s146/332 [============>.................] - ETA: 0s175/332 [==============>...............] - ETA: 0s206/332 [=================>............] - ETA: 0s237/332 [====================>.........] - ETA: 0s268/332 [=======================>......] - ETA: 0s298/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07370041210809565
cosine 0.0578461087597032
MAE: 0.035745867
RMSE: 0.0780884
r2: 0.6044210759470509
RMSE zero-vector: 0.23411466903540806
['2.4000000000000004custom_VAE', 'mse', 64, 145, 0.001, 0.2, 252, 0.009571901522576809, 0.009324951097369194, 0.07370041210809565, 0.0578461087597032, 0.03574586659669876, 0.07808840274810791, 0.6044210759470509, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 145 0.0008 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
Epoch 1/145
747/747 - 8s - loss: 0.0319 - val_loss: 0.0190 - 8s/epoch - 11ms/step
Epoch 2/145
747/747 - 5s - loss: 0.0152 - val_loss: 0.0213 - 5s/epoch - 6ms/step
Epoch 3/145
747/747 - 5s - loss: 0.0144 - val_loss: 0.0148 - 5s/epoch - 6ms/step
Epoch 4/145
747/747 - 5s - loss: 0.0133 - val_loss: 0.0171 - 5s/epoch - 6ms/step
Epoch 5/145
747/747 - 5s - loss: 0.0126 - val_loss: 0.0148 - 5s/epoch - 6ms/step
Epoch 6/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 7/145
747/747 - 4s - loss: 0.0116 - val_loss: 0.0126 - 4s/epoch - 6ms/step
Epoch 8/145
747/747 - 4s - loss: 0.0114 - val_loss: 0.0117 - 4s/epoch - 6ms/step
Epoch 9/145
747/747 - 5s - loss: 0.0122 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 10/145
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 11/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 12/145
747/747 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 13/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 14/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 15/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 16/145
747/747 - 4s - loss: 0.0104 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 17/145
747/747 - 5s - loss: 0.0110 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 18/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 19/145
747/747 - 5s - loss: 0.0107 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 20/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 21/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 22/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 23/145
747/747 - 5s - loss: 0.0112 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 24/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 25/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 7ms/step
Epoch 26/145
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 27/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 28/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 29/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 30/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 31/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 32/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 33/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 34/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 35/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 36/145
747/747 - 5s - loss: 0.0107 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 37/145
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 38/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 39/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 40/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 41/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 42/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 43/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 44/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 45/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 46/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 47/145
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 48/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 49/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 50/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 51/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 52/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 53/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 54/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 55/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 56/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 57/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 58/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 59/145
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 60/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 61/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 62/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 63/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 64/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 65/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 66/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 67/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 68/145
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 69/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 70/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 71/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 72/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 73/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 74/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 75/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 76/145
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 77/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 78/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 79/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 80/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 81/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 82/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 83/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 84/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 85/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 86/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 87/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 88/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 89/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 90/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 91/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 92/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 93/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 94/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 95/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 96/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 97/145
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 98/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 99/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 100/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 101/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 102/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 103/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 104/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 105/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 106/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 107/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 108/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 109/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 110/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 111/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 112/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 113/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 114/145
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 115/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 116/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 117/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 118/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 119/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 120/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 121/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 122/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 123/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 124/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 125/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 126/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 127/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 128/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 129/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 130/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 131/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 7ms/step
Epoch 132/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 133/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 134/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 135/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 7ms/step
Epoch 136/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 137/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 138/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 139/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 140/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 141/145
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 142/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 143/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 144/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 145/145
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009279177524149418
  1/332 [..............................] - ETA: 56s 30/332 [=>............................] - ETA: 0s  59/332 [====>.........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s118/332 [=========>....................] - ETA: 0s147/332 [============>.................] - ETA: 0s177/332 [==============>...............] - ETA: 0s207/332 [=================>............] - ETA: 0s237/332 [====================>.........] - ETA: 0s267/332 [=======================>......] - ETA: 0s297/332 [=========================>....] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0741599323308742
cosine 0.05820392250250835
MAE: 0.035658292
RMSE: 0.07826308
r2: 0.6026493676187004
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'mse', 128, 145, 0.0008, 0.2, 252, 0.00945852417498827, 0.009279177524149418, 0.0741599323308742, 0.05820392250250835, 0.03565829247236252, 0.07826308161020279, 0.6026493676187004, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 140 0.0008 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/140
747/747 - 8s - loss: 0.0296 - val_loss: 0.0189 - 8s/epoch - 11ms/step
Epoch 2/140
747/747 - 4s - loss: 0.0154 - val_loss: 0.0248 - 4s/epoch - 6ms/step
Epoch 3/140
747/747 - 4s - loss: 0.0143 - val_loss: 0.0141 - 4s/epoch - 6ms/step
Epoch 4/140
747/747 - 4s - loss: 0.0132 - val_loss: 0.0217 - 4s/epoch - 6ms/step
Epoch 5/140
747/747 - 5s - loss: 0.0127 - val_loss: 0.0125 - 5s/epoch - 6ms/step
Epoch 6/140
747/747 - 5s - loss: 0.0120 - val_loss: 0.0136 - 5s/epoch - 6ms/step
Epoch 7/140
747/747 - 5s - loss: 0.0117 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 8/140
747/747 - 4s - loss: 0.0113 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 9/140
747/747 - 5s - loss: 0.0111 - val_loss: 0.0115 - 5s/epoch - 6ms/step
Epoch 10/140
747/747 - 4s - loss: 0.0114 - val_loss: 0.0114 - 4s/epoch - 6ms/step
Epoch 11/140
747/747 - 4s - loss: 0.0112 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 12/140
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 13/140
747/747 - 4s - loss: 0.0107 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 14/140
747/747 - 4s - loss: 0.0109 - val_loss: 0.0115 - 4s/epoch - 6ms/step
Epoch 15/140
747/747 - 5s - loss: 0.0108 - val_loss: 0.0114 - 5s/epoch - 6ms/step
Epoch 16/140
747/747 - 5s - loss: 0.0115 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 17/140
747/747 - 5s - loss: 0.0107 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 18/140
747/747 - 4s - loss: 0.0111 - val_loss: 0.0113 - 4s/epoch - 6ms/step
Epoch 19/140
747/747 - 5s - loss: 0.0119 - val_loss: 0.0106 - 5s/epoch - 7ms/step
Epoch 20/140
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 21/140
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 22/140
747/747 - 5s - loss: 0.0107 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 23/140
747/747 - 5s - loss: 0.0103 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 24/140
747/747 - 5s - loss: 0.0107 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 25/140
747/747 - 5s - loss: 0.0104 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 26/140
747/747 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 27/140
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 28/140
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 29/140
747/747 - 5s - loss: 0.0101 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 30/140
747/747 - 4s - loss: 0.0102 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 31/140
747/747 - 4s - loss: 0.0100 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 32/140
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 33/140
747/747 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 34/140
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 35/140
747/747 - 5s - loss: 0.0099 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 36/140
747/747 - 5s - loss: 0.0102 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 37/140
747/747 - 5s - loss: 0.0103 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 38/140
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 39/140
747/747 - 4s - loss: 0.0104 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 40/140
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 41/140
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 42/140
747/747 - 5s - loss: 0.0098 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 43/140
747/747 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 44/140
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 45/140
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 46/140
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 47/140
747/747 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 48/140
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 49/140
747/747 - 5s - loss: 0.0097 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 50/140
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 51/140
747/747 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 52/140
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 53/140
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 54/140
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 55/140
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 7ms/step
Epoch 56/140
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 57/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 58/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 59/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 60/140
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 61/140
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 62/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 63/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 64/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 65/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 66/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 67/140
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 68/140
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 69/140
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 70/140
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 71/140
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 72/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 73/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 74/140
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 75/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 76/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 77/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 78/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 79/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 80/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 81/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 82/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 83/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 84/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 85/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 86/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 87/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 88/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 89/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 90/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 7ms/step
Epoch 91/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 92/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 93/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 94/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 95/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 96/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 97/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 98/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 99/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 100/140
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 101/140
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 102/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 103/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 104/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 105/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 106/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 107/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 108/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 109/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 110/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 111/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 112/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 113/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 114/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 115/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 116/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 117/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 118/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 119/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 120/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 121/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 122/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 123/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 124/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 125/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 126/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 127/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 128/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 129/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 130/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 131/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 132/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 133/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 134/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 135/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 136/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 137/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 138/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 139/140
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 140/140
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009189680218696594
  1/332 [..............................] - ETA: 56s 30/332 [=>............................] - ETA: 0s  60/332 [====>.........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s120/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s180/332 [===============>..............] - ETA: 0s210/332 [=================>............] - ETA: 0s240/332 [====================>.........] - ETA: 0s269/332 [=======================>......] - ETA: 0s300/332 [==========================>...] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07032872201742157
cosine 0.05521465035326024
MAE: 0.034875955
RMSE: 0.076319225
r2: 0.6221426480230098
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 128, 140, 0.0008, 0.2, 252, 0.009355969727039337, 0.009189680218696594, 0.07032872201742157, 0.05521465035326024, 0.03487595543265343, 0.07631922513246536, 0.6221426480230098, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.3000000000000003 205 0.0012000000000000001 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2907)         3677355     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2907)        11628       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2907)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          732816      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4487575     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,642,190
Trainable params: 9,630,058
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/205
747/747 - 8s - loss: 0.0318 - val_loss: 0.0193 - 8s/epoch - 11ms/step
Epoch 2/205
747/747 - 4s - loss: 0.0158 - val_loss: 0.0401 - 4s/epoch - 6ms/step
Epoch 3/205
747/747 - 4s - loss: 0.0148 - val_loss: 0.0140 - 4s/epoch - 6ms/step
Epoch 4/205
747/747 - 4s - loss: 0.0131 - val_loss: 0.0200 - 4s/epoch - 6ms/step
Epoch 5/205
747/747 - 5s - loss: 0.0126 - val_loss: 0.0171 - 5s/epoch - 6ms/step
Epoch 6/205
747/747 - 4s - loss: 0.0122 - val_loss: 0.0123 - 4s/epoch - 6ms/step
Epoch 7/205
747/747 - 4s - loss: 0.0117 - val_loss: 0.0127 - 4s/epoch - 6ms/step
Epoch 8/205
747/747 - 4s - loss: 0.0122 - val_loss: 0.0111 - 4s/epoch - 6ms/step
Epoch 9/205
747/747 - 5s - loss: 0.0112 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 10/205
747/747 - 5s - loss: 0.0112 - val_loss: 0.0113 - 5s/epoch - 7ms/step
Epoch 11/205
747/747 - 5s - loss: 0.0113 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 12/205
747/747 - 5s - loss: 0.0109 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 13/205
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 14/205
747/747 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 15/205
747/747 - 4s - loss: 0.0106 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 16/205
747/747 - 4s - loss: 0.0116 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 17/205
747/747 - 4s - loss: 0.0108 - val_loss: 0.0116 - 4s/epoch - 6ms/step
Epoch 18/205
747/747 - 5s - loss: 0.0125 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 19/205
747/747 - 5s - loss: 0.0107 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 20/205
747/747 - 5s - loss: 0.0107 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 21/205
747/747 - 4s - loss: 0.0107 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 22/205
747/747 - 4s - loss: 0.0104 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 23/205
747/747 - 4s - loss: 0.0106 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 24/205
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 25/205
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 26/205
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 27/205
747/747 - 5s - loss: 0.0102 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 28/205
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 29/205
747/747 - 5s - loss: 0.0102 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 30/205
747/747 - 4s - loss: 0.0106 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 31/205
747/747 - 4s - loss: 0.0103 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 32/205
747/747 - 4s - loss: 0.0105 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 33/205
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 34/205
747/747 - 4s - loss: 0.0102 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 35/205
747/747 - 5s - loss: 0.0104 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 36/205
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 37/205
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 38/205
747/747 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 39/205
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 40/205
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 41/205
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 42/205
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 43/205
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 44/205
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 45/205
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 46/205
747/747 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 47/205
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 48/205
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 49/205
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 50/205
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 51/205
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 52/205
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 53/205
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 54/205
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 55/205
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 56/205
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 57/205
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 58/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 59/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 60/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 61/205
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 62/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 63/205
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 64/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 65/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 66/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 67/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 68/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 69/205
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 70/205
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 71/205
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 72/205
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 73/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 74/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 75/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 76/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 77/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 78/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 79/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 80/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 81/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 82/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 83/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 84/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 85/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 7ms/step
Epoch 86/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 87/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 88/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 89/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 90/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 91/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 92/205
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 93/205
747/747 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 94/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 95/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 96/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 97/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 98/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 99/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 100/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 101/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 102/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 103/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 104/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 105/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 106/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 107/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 108/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 109/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 110/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 111/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 112/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 113/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 114/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 115/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 116/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 117/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 118/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 119/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 120/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 121/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 122/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 123/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 124/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 125/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 126/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 127/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 128/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 129/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 130/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 131/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 132/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 133/205
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 134/205
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 135/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 136/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 137/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 138/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 139/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 140/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 141/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 142/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 143/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 144/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 145/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 146/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 147/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 148/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 149/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 150/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 151/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 152/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 153/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 154/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 155/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 156/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 157/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 158/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 159/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 160/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 161/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 162/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 163/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 164/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 165/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 166/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 167/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 168/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 169/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 170/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 171/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 172/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 173/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 174/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 175/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 176/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 177/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 178/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 179/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 180/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 181/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 182/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 183/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 184/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 185/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 186/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 187/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 188/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 189/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 7ms/step
Epoch 190/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 191/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 192/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 193/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 194/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 195/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 196/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 197/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 198/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 199/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 200/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 201/205
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 202/205
747/747 - 5s - loss: 0.0093 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 203/205
747/747 - 5s - loss: 0.0093 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 204/205
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 205/205
747/747 - 5s - loss: 0.0093 - val_loss: 0.0092 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009181451052427292
  1/332 [..............................] - ETA: 53s 31/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 91/332 [=======>......................] - ETA: 0s119/332 [=========>....................] - ETA: 0s147/332 [============>.................] - ETA: 0s175/332 [==============>...............] - ETA: 0s203/332 [=================>............] - ETA: 0s233/332 [====================>.........] - ETA: 0s264/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0716196889365973
cosine 0.05622346680702644
MAE: 0.03498113
RMSE: 0.07702198
r2: 0.6151519086242164
RMSE zero-vector: 0.23411466903540806
['2.3000000000000003custom_VAE', 'mse', 128, 205, 0.0012000000000000001, 0.2, 252, 0.00934133492410183, 0.009181451052427292, 0.0716196889365973, 0.05622346680702644, 0.0349811315536499, 0.07702197879552841, 0.6151519086242164, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 140 0.001 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
Epoch 1/140
1493/1493 - 12s - loss: 0.0257 - val_loss: 0.0168 - 12s/epoch - 8ms/step
Epoch 2/140
1493/1493 - 9s - loss: 0.0148 - val_loss: 0.0937 - 9s/epoch - 6ms/step
Epoch 3/140
1493/1493 - 9s - loss: 0.0145 - val_loss: 0.0120 - 9s/epoch - 6ms/step
Epoch 4/140
1493/1493 - 9s - loss: 0.0121 - val_loss: 0.0117 - 9s/epoch - 6ms/step
Epoch 5/140
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0113 - 8s/epoch - 6ms/step
Epoch 6/140
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 7/140
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 6ms/step
Epoch 8/140
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 9/140
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 10/140
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 11/140
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 12/140
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 13/140
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 14/140
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 15/140
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 16/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 17/140
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 18/140
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 19/140
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 20/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 21/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 22/140
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 23/140
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 24/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 25/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 26/140
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 27/140
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 28/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 29/140
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 30/140
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 31/140
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 32/140
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 33/140
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 34/140
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 35/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 36/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 37/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 38/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 39/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 40/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 41/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 42/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 43/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 44/140
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 45/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 46/140
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 47/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 48/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 49/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 50/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 51/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 52/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 53/140
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 54/140
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 55/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 56/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 57/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 58/140
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 59/140
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 60/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 61/140
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 62/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 63/140
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 64/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 65/140
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 66/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 67/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 68/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 69/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 70/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 71/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 72/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 73/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 74/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 75/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 76/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 77/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 78/140
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 79/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 80/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 81/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 82/140
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 83/140
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 84/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 85/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 86/140
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 87/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 88/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 89/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 90/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 91/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 92/140
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 93/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 94/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 95/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 96/140
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 97/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 98/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 99/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 100/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 101/140
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 102/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 103/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 104/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 105/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 106/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 107/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 108/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 109/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 110/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 111/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 112/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 113/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 114/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 115/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 116/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 117/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 118/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 119/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 120/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 121/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 122/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 123/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 124/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 125/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 126/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 127/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 128/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 129/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 130/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 131/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 132/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 133/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 134/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 135/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 136/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 137/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 138/140
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 139/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 140/140
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009338303469121456
  1/332 [..............................] - ETA: 1:05 25/332 [=>............................] - ETA: 0s   50/332 [===>..........................] - ETA: 0s 71/332 [=====>........................] - ETA: 0s101/332 [========>.....................] - ETA: 0s129/332 [==========>...................] - ETA: 0s158/332 [=============>................] - ETA: 0s187/332 [===============>..............] - ETA: 0s215/332 [==================>...........] - ETA: 0s243/332 [====================>.........] - ETA: 0s273/332 [=======================>......] - ETA: 0s301/332 [==========================>...] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07405945097011662
cosine 0.058130857284411454
MAE: 0.035838295
RMSE: 0.07842264
r2: 0.6010275924630272
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'mse', 64, 140, 0.001, 0.2, 252, 0.009589635767042637, 0.009338303469121456, 0.07405945097011662, 0.058130857284411454, 0.0358382947742939, 0.07842264324426651, 0.6010275924630272, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 145 0.0008 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:50:47.975071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 255328256/34089730048
2023-02-18 23:50:47.975113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32776409505
MaxInUse:                  32903801350
NumAllocs:                   662782237
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:50:47.975187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:50:47.975194: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 119
2023-02-18 23:50:47.975199: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:50:47.975203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 500
2023-02-18 23:50:47.975207: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:50:47.975212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 88
2023-02-18 23:50:47.975221: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:50:47.975226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 208
2023-02-18 23:50:47.975230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:50:47.975234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 132
2023-02-18 23:50:47.975239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:50:47.975243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:50:47.975247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 88
2023-02-18 23:50:47.975252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:50:47.975256: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 84
2023-02-18 23:50:47.975260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:50:47.975264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 54
2023-02-18 23:50:47.975269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:50:47.975273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:50:47.975277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:50:47.975281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 56
2023-02-18 23:50:47.975286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:50:47.975290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 36
2023-02-18 23:50:47.975294: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:50:47.975298: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:50:47.975303: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:50:47.975307: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 145, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 145 0.0008 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_3.pkl
[1.7999999999999998 145 0.0024000000000000002 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2275)         2877875     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2275)        9100        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2275)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3526303     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,560,382
Trainable params: 7,550,778
Non-trainable params: 9,604
__________________________________________________________________________________________________
2023-02-18 23:51:03.037014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 255328256/34089730048
2023-02-18 23:51:03.037061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32773075993
MaxInUse:                  32903801350
NumAllocs:                   662782297
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:03.037143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:03.037150: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:03.037154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:03.037159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 500
2023-02-18 23:51:03.037163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:03.037168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 88
2023-02-18 23:51:03.037172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:03.037176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:03.037180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 198
2023-02-18 23:51:03.037185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:03.037189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 132
2023-02-18 23:51:03.037200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:03.037204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:03.037209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 88
2023-02-18 23:51:03.037213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:03.037218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:03.037222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 81
2023-02-18 23:51:03.037226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:03.037230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 54
2023-02-18 23:51:03.037234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:03.037239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:03.037243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:03.037247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:03.037252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 54
2023-02-18 23:51:03.037256: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:03.037260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 36
2023-02-18 23:51:03.037264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:03.037269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:03.037273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:03.037277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.7999999999999998custom_VAE', 'mse', 128, 145, 0.0024000000000000002, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.7999999999999998 145 0.0024000000000000002 128 1]) is not valid.
[2.0 145 0.0008 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:51:06.313272: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 221773824/34089730048
2023-02-18 23:51:06.313313: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32806651049
MaxInUse:                  32903801350
NumAllocs:                   662782353
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:06.313387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:06.313393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:06.313398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:06.313403: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 507
2023-02-18 23:51:06.313407: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:06.313412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 89
2023-02-18 23:51:06.313416: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:06.313420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:06.313424: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 208
2023-02-18 23:51:06.313429: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:06.313433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 132
2023-02-18 23:51:06.313437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:06.313442: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:06.313452: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 89
2023-02-18 23:51:06.313457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:06.313461: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:06.313465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 84
2023-02-18 23:51:06.313469: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:06.313474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 54
2023-02-18 23:51:06.313478: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:06.313482: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:06.313486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:06.313491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:06.313495: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 56
2023-02-18 23:51:06.313499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:06.313503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 36
2023-02-18 23:51:06.313508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:06.313512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:06.313516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:06.313520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 64, 145, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 145 0.0008 64 1]) is not valid.
[2.2 140 0.0008 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          700812      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4294408     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,223,852
Trainable params: 9,212,228
Non-trainable params: 11,624
__________________________________________________________________________________________________
2023-02-18 23:51:09.697904: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 188219392/34089730048
2023-02-18 23:51:09.697947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32809971401
MaxInUse:                  32903801350
NumAllocs:                   662782409
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:09.698025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:09.698031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:09.698036: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:09.698040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 507
2023-02-18 23:51:09.698045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:09.698049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 89
2023-02-18 23:51:09.698053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:09.698058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:09.698062: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 198
2023-02-18 23:51:09.698066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:09.698070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:09.698075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:09.698079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:09.698083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 89
2023-02-18 23:51:09.698088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:09.698092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:09.698103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 81
2023-02-18 23:51:09.698107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:09.698112: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:09.698116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:09.698120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:09.698125: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:09.698129: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:09.698133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 54
2023-02-18 23:51:09.698138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:09.698142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:09.698146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:09.698150: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:09.698155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:09.698159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.2custom_VAE', 'mse', 64, 140, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.2 140 0.0008 64 1]) is not valid.
[2.0 140 0.001 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:51:12.958863: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 188219392/34089730048
2023-02-18 23:51:12.958905: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32843546457
MaxInUse:                  32903801350
NumAllocs:                   662782465
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:12.958980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:12.958986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:12.958991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:12.958995: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 514
2023-02-18 23:51:12.959000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:12.959004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 90
2023-02-18 23:51:12.959008: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:12.959013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:12.959017: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 208
2023-02-18 23:51:12.959022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:12.959026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:12.959030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:12.959034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:12.959039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 90
2023-02-18 23:51:12.959043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:12.959047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:12.959052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 84
2023-02-18 23:51:12.959056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:12.959060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:12.959070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:12.959075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:12.959079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:12.959083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:12.959088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 56
2023-02-18 23:51:12.959092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:12.959097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:12.959101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:12.959105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:12.959109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:12.959114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 140, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 140 0.001 256 1]) is not valid.
[2.0 145 0.0008 256 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:51:16.220285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 154664960/34089730048
2023-02-18 23:51:16.220331: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32843546457
MaxInUse:                  32903801350
NumAllocs:                   662782521
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:16.220408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:16.220414: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:16.220419: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:16.220423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 514
2023-02-18 23:51:16.220428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:16.220432: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 90
2023-02-18 23:51:16.220436: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:16.220441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:16.220445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 208
2023-02-18 23:51:16.220449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:16.220453: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:16.220458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:16.220462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:16.220466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 90
2023-02-18 23:51:16.220471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:16.220475: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:16.220479: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 84
2023-02-18 23:51:16.220483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:16.220488: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:16.220492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:16.220496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:16.220500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:16.220511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:16.220516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 56
2023-02-18 23:51:16.220520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:16.220525: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:16.220529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:16.220533: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:16.220537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:16.220542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'logcosh', 256, 145, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 145 0.0008 256 2]) is not valid.
[2.1 145 0.0024000000000000002 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-18 23:51:19.468612: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 121110528/34089730048
2023-02-18 23:51:19.468655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32878781689
MaxInUse:                  32905613897
NumAllocs:                   662782577
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:19.468736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:19.468743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:19.468747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:19.468752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 521
2023-02-18 23:51:19.468756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:19.468760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 91
2023-02-18 23:51:19.468765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:19.468769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:19.468774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 208
2023-02-18 23:51:19.468778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 142
2023-02-18 23:51:19.468782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:19.468786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:19.468791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:19.468795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 91
2023-02-18 23:51:19.468799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:19.468803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:19.468808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 84
2023-02-18 23:51:19.468812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 57
2023-02-18 23:51:19.468816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:19.468820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:19.468824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:19.468829: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:19.468833: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:19.468837: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 56
2023-02-18 23:51:19.468841: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 38
2023-02-18 23:51:19.468852: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:19.468856: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:19.468861: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:19.468865: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:19.468869: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 64, 145, 0.0024000000000000002, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 145 0.0024000000000000002 64 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_4.pkl
[2.0 140 0.0008 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:51:30.510919: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 121110528/34089730048
2023-02-18 23:51:30.510964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32877121513
MaxInUse:                  32917126409
NumAllocs:                   662782633
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:30.511040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:30.511046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:30.511051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:30.511055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 521
2023-02-18 23:51:30.511059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:30.511064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 91
2023-02-18 23:51:30.511068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:30.511072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:30.511077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 218
2023-02-18 23:51:30.511081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:30.511085: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:30.511090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:30.511094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:30.511098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 91
2023-02-18 23:51:30.511103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:30.511107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:30.511111: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 87
2023-02-18 23:51:30.511115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:30.511120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:30.511124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:30.511128: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:30.511132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:30.511137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:30.511141: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 58
2023-02-18 23:51:30.511145: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:30.511150: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:30.511154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:30.511164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:30.511169: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:30.511173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 140, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 140 0.0008 256 1]) is not valid.
[2.0 140 0.0008 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:51:33.817803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 121110528/34089730048
2023-02-18 23:51:33.817848: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32910696569
MaxInUse:                  32936254665
NumAllocs:                   662782689
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:33.817926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:33.817933: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:33.817938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:33.817942: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 528
2023-02-18 23:51:33.817947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:33.817951: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 92
2023-02-18 23:51:33.817955: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:33.817959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 66
2023-02-18 23:51:33.817964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 228
2023-02-18 23:51:33.817968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:33.817972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:33.817977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:33.817981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:33.817985: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 92
2023-02-18 23:51:33.817989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:33.817994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 27
2023-02-18 23:51:33.817998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 90
2023-02-18 23:51:33.818002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:33.818006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:33.818011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:33.818015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:33.818019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:33.818024: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 18
2023-02-18 23:51:33.818028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 60
2023-02-18 23:51:33.818032: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:33.818036: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:33.818041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:33.818045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:33.818049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:33.818054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 256, 140, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 140 0.0008 256 1]) is not valid.
[1.9 210 0.001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
2023-02-18 23:51:37.103193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 121110528/34089730048
2023-02-18 23:51:37.103236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32909023217
MaxInUse:                  32947114961
NumAllocs:                   662782745
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:37.103320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:37.103326: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:37.103331: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:37.103335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 528
2023-02-18 23:51:37.103339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:37.103343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 92
2023-02-18 23:51:37.103347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:37.103351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 76
2023-02-18 23:51:37.103355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 218
2023-02-18 23:51:37.103359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:37.103363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:37.103367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:37.103371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:37.103375: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 92
2023-02-18 23:51:37.103379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:37.103383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 30
2023-02-18 23:51:37.103387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 87
2023-02-18 23:51:37.103392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:37.103396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:37.103399: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:37.103404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:37.103408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:37.103412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 20
2023-02-18 23:51:37.103416: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 58
2023-02-18 23:51:37.103420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:37.103424: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:37.103428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:37.103432: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:37.103436: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:37.103440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.9custom_VAE', 'mse', 64, 210, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.9 210 0.001 64 1]) is not valid.
[2.1 140 0.0008 128 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-18 23:51:40.372556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 87556096/34089730048
2023-02-18 23:51:40.372598: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32944258449
MaxInUse:                  32971090657
NumAllocs:                   662782801
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:40.372674: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:40.372687: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:40.372697: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:40.372704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 535
2023-02-18 23:51:40.372709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:40.372714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 93
2023-02-18 23:51:40.372718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:40.372722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 76
2023-02-18 23:51:40.372726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 218
2023-02-18 23:51:40.372731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 142
2023-02-18 23:51:40.372735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:40.372739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:40.372743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:40.372748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 93
2023-02-18 23:51:40.372752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:40.372756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 30
2023-02-18 23:51:40.372760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 87
2023-02-18 23:51:40.372765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 57
2023-02-18 23:51:40.372769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:40.372773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:40.372777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:40.372782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:40.372786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 20
2023-02-18 23:51:40.372790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 58
2023-02-18 23:51:40.372794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 38
2023-02-18 23:51:40.372799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:40.372803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:40.372807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:40.372811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:40.372815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'logcosh', 128, 140, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 140 0.0008 128 2]) is not valid.
[2.0 135 0.0008 128 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:51:43.630632: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 54001664/34089730048
2023-02-18 23:51:43.630675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32942598273
MaxInUse:                  32982603177
NumAllocs:                   662782857
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:43.630758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:43.630765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:43.630770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:43.630782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 535
2023-02-18 23:51:43.630787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:43.630791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 93
2023-02-18 23:51:43.630795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:43.630799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 76
2023-02-18 23:51:43.630804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 228
2023-02-18 23:51:43.630808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:43.630812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:43.630817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:43.630821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:43.630825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 93
2023-02-18 23:51:43.630829: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:43.630834: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 30
2023-02-18 23:51:43.630838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 90
2023-02-18 23:51:43.630842: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:43.630846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:43.630851: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:43.630855: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:43.630859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:43.630864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 20
2023-02-18 23:51:43.630868: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 60
2023-02-18 23:51:43.630872: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:43.630876: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:43.630881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:43.630885: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:43.630889: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:43.630893: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'logcosh', 128, 135, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 135 0.0008 128 2]) is not valid.
[1.7999999999999998 210 0.0008 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2275)         2877875     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2275)        9100        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2275)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3526303     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,560,382
Trainable params: 7,550,778
Non-trainable params: 9,604
__________________________________________________________________________________________________
2023-02-18 23:51:46.957832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 54001664/34089730048
2023-02-18 23:51:46.957874: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32972839801
MaxInUse:                  32995839561
NumAllocs:                   662782913
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:46.957952: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:46.957958: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:46.957963: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:46.957968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 542
2023-02-18 23:51:46.957972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:46.957976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 94
2023-02-18 23:51:46.957987: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 20
2023-02-18 23:51:46.957992: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 76
2023-02-18 23:51:46.957997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 228
2023-02-18 23:51:46.958001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 132
2023-02-18 23:51:46.958005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:46.958009: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:46.958014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:46.958018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 94
2023-02-18 23:51:46.958022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 6
2023-02-18 23:51:46.958027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 30
2023-02-18 23:51:46.958031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 90
2023-02-18 23:51:46.958035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 54
2023-02-18 23:51:46.958040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:46.958044: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:46.958048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:46.958053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 4
2023-02-18 23:51:46.958057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 20
2023-02-18 23:51:46.958061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 60
2023-02-18 23:51:46.958066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 36
2023-02-18 23:51:46.958070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:46.958074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:46.958078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:46.958083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:46.958087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.7999999999999998custom_VAE', 'mse', 64, 210, 0.0008, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.7999999999999998 210 0.0008 64 1]) is not valid.
[2.1 140 0.001 128 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-18 23:51:50.181948: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 54001664/34089730048
2023-02-18 23:51:50.181990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     32977833505
MaxInUse:                  33013095697
NumAllocs:                   662782969
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:51:50.182067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:51:50.182074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:51:50.182078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:51:50.182082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 542
2023-02-18 23:51:50.182086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:51:50.182091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 94
2023-02-18 23:51:50.182095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:51:50.182099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 76
2023-02-18 23:51:50.182103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 228
2023-02-18 23:51:50.182113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 142
2023-02-18 23:51:50.182117: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:51:50.182121: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:51:50.182126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:51:50.182130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 94
2023-02-18 23:51:50.182134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:51:50.182138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 30
2023-02-18 23:51:50.182142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 90
2023-02-18 23:51:50.182146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 57
2023-02-18 23:51:50.182150: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:51:50.182154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:51:50.182158: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:51:50.182162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:51:50.182166: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 20
2023-02-18 23:51:50.182187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 60
2023-02-18 23:51:50.182192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 38
2023-02-18 23:51:50.182196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:51:50.182200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:51:50.182204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:51:50.182208: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:51:50.182213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'mse', 128, 140, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 140 0.001 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_5.pkl
[2.0 145 0.0024000000000000002 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-18 23:52:02.904811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 20447232/34089730048
2023-02-18 23:52:02.904860: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     33011408561
MaxInUse:                  33036966657
NumAllocs:                   662783025
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:52:02.904938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:52:02.904945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 123
2023-02-18 23:52:02.904950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 124
2023-02-18 23:52:02.904955: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 549
2023-02-18 23:52:02.904959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:52:02.904964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 95
2023-02-18 23:52:02.904968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:52:02.904973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 76
2023-02-18 23:52:02.904977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 238
2023-02-18 23:52:02.904981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 142
2023-02-18 23:52:02.904986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:52:02.904997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:52:02.905002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:52:02.905006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 95
2023-02-18 23:52:02.905011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:52:02.905015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 30
2023-02-18 23:52:02.905020: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 93
2023-02-18 23:52:02.905024: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 57
2023-02-18 23:52:02.905028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:52:02.905033: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:52:02.905037: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:52:02.905042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:52:02.905046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 20
2023-02-18 23:52:02.905050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 62
2023-02-18 23:52:02.905055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 38
2023-02-18 23:52:02.905059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:52:02.905063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:52:02.905068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:52:02.905072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:52:02.905076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 128, 145, 0.0024000000000000002, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 145 0.0024000000000000002 128 1]) is not valid.
[2.0 145 0.0024000000000000002 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-18 23:52:05.515934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 12781568 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 20447232/34089730048
2023-02-18 23:52:05.515973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     32475971584
InUse:                     33057760145
MaxInUse:                  33057760149
NumAllocs:                   662783077
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 23:52:05.516051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 23:52:05.516058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 121
2023-02-18 23:52:05.516063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 125
2023-02-18 23:52:05.516074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-18 23:52:05.516079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 556
2023-02-18 23:52:05.516083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 23:52:05.516087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 95
2023-02-18 23:52:05.516092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-18 23:52:05.516096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 76
2023-02-18 23:52:05.516100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 248
2023-02-18 23:52:05.516104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 142
2023-02-18 23:52:05.516109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 142
2023-02-18 23:52:05.516113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11628, 88
2023-02-18 23:52:05.516118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-18 23:52:05.516122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 96
2023-02-18 23:52:05.516126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-18 23:52:05.516130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 30
2023-02-18 23:52:05.516135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 96
2023-02-18 23:52:05.516139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 57
2023-02-18 23:52:05.516143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2802240, 57
2023-02-18 23:52:05.516147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2930256, 36
2023-02-18 23:52:05.516151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-18 23:52:05.516156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-18 23:52:05.516160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 20
2023-02-18 23:52:05.516164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 65
2023-02-18 23:52:05.516168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 38
2023-02-18 23:52:05.516173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 38
2023-02-18 23:52:05.516177: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14697792, 24
2023-02-18 23:52:05.516181: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-18 23:52:05.516186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 31
2023-02-18 23:52:05.516190: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 58
2023-02-18 23:52:05.516210: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory
Traceback (most recent call last):
  File "genetic.py", line 44, in <module>
    ga=continue_run_ga('tmp/ga_instance_generation_5.pkl')
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.2/../../genetic_hypertune.py", line 249, in continue_run_ga
    ga_instance.run()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1409, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.2/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 352, in train_autoencoder
    model = create_autoencoder(n_inputs=n_inputs, layers_structure=layers_structure, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 585, in create_autoencoder
    decoder_output = Dense(n_inputs, activation='linear', name='outputlayer')(d)
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2]
Sat Feb 18 23:52:30 CET 2023
done
