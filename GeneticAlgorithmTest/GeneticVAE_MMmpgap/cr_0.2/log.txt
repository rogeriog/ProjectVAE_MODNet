start
Wed Feb 22 18:58:03 CET 2023
2023-02-22 18:58:04.591181: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 18:58:04.783473: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-22 18:59:03,490 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f70fc9ab040> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.0 90 0.0005 64 2]
 [2.5 120 0.001 32 1]
 [2.0 180 0.0005 128 1]
 [0.5 60 0.0005 16 1]
 [2.0 30 0.001 16 1]
 [2.5 90 0.001 32 1]
 [0.5 180 0.0005 128 1]
 [2.5 150 0.002 16 1]
 [1.0 210 0.0005 128 2]
 [0.5 180 0.0005 64 1]
 [2.0 150 0.001 256 1]
 [2.0 60 0.0005 16 1]
 [1.0 90 0.002 16 2]
 [0.5 180 0.002 128 2]
 [0.5 90 0.002 256 2]]
[1.0 90 0.0005 64 2] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-22 18:59:08.567436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 18:59:09.105165: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-22 18:59:09.106041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22295 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:e1:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/90
2023-02-22 18:59:14.643303: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0082 - 9s/epoch - 6ms/step
Epoch 2/90
1493/1493 - 7s - loss: 0.0077 - val_loss: 0.0077 - 7s/epoch - 4ms/step
Epoch 3/90
1493/1493 - 7s - loss: 0.0072 - val_loss: 0.0071 - 7s/epoch - 5ms/step
Epoch 4/90
1493/1493 - 7s - loss: 0.0069 - val_loss: 0.0068 - 7s/epoch - 5ms/step
Epoch 5/90
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 4ms/step
Epoch 6/90
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 7/90
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0064 - 7s/epoch - 5ms/step
Epoch 8/90
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 9/90
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 10/90
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 11/90
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 4ms/step
Epoch 12/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 13/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 4ms/step
Epoch 14/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 15/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 16/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 17/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 18/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 19/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 4ms/step
Epoch 20/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 21/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 22/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 23/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 24/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 25/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 26/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 27/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 28/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 29/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 30/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 31/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 32/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 33/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 34/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 35/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 36/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 37/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 38/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 39/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 40/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 41/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 42/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 43/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 44/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 45/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 46/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 47/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 48/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 49/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 50/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 51/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 52/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 53/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 54/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 55/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 56/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 57/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 58/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 59/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 60/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 61/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 62/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 63/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 64/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 65/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 66/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 67/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 68/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 69/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 70/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 71/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 72/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 73/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 74/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 75/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 76/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 77/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 78/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 79/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 80/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 81/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 82/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 83/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 84/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 85/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 86/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 87/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 88/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 89/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 90/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005949758924543858
  1/332 [..............................] - ETA: 1:01 36/332 [==>...........................] - ETA: 0s   70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s141/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s290/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.1131415274310148
cosine 0.08637488588741309
MAE: 0.04584317
RMSE: 0.0984895
r2: 0.3712883554719925
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 64, 90, 0.0005, 0.2, 252, 0.006017697509378195, 0.005949758924543858, 0.1131415274310148, 0.08637488588741309, 0.04584316909313202, 0.09848950058221817, 0.3712883554719925, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 120 0.001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/120
2985/2985 - 14s - loss: 0.0250 - val_loss: 0.0139 - 14s/epoch - 5ms/step
Epoch 2/120
2985/2985 - 13s - loss: 0.0130 - val_loss: 0.0119 - 13s/epoch - 4ms/step
Epoch 3/120
2985/2985 - 13s - loss: 0.0121 - val_loss: 0.0117 - 13s/epoch - 4ms/step
Epoch 4/120
2985/2985 - 13s - loss: 0.0118 - val_loss: 0.0114 - 13s/epoch - 4ms/step
Epoch 5/120
2985/2985 - 13s - loss: 0.0115 - val_loss: 0.0113 - 13s/epoch - 4ms/step
Epoch 6/120
2985/2985 - 13s - loss: 0.0113 - val_loss: 0.0109 - 13s/epoch - 4ms/step
Epoch 7/120
2985/2985 - 13s - loss: 0.0111 - val_loss: 0.0108 - 13s/epoch - 4ms/step
Epoch 8/120
2985/2985 - 13s - loss: 0.0110 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 9/120
2985/2985 - 13s - loss: 0.0110 - val_loss: 0.0108 - 13s/epoch - 4ms/step
Epoch 10/120
2985/2985 - 13s - loss: 0.0109 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 11/120
2985/2985 - 13s - loss: 0.0109 - val_loss: 0.0106 - 13s/epoch - 4ms/step
Epoch 12/120
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0110 - 13s/epoch - 4ms/step
Epoch 13/120
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 14/120
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 15/120
2985/2985 - 13s - loss: 0.0107 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 16/120
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0109 - 13s/epoch - 4ms/step
Epoch 17/120
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 18/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 19/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 20/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 21/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 22/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0106 - 13s/epoch - 4ms/step
Epoch 23/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 24/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 25/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 26/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 27/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 28/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 29/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 30/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 31/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 32/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 33/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 34/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 35/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 36/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 37/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 38/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 39/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 40/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 41/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 42/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 43/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 44/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 45/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 46/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 47/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 48/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 49/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 50/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 51/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 52/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 53/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 54/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 55/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 56/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 57/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 58/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 59/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 60/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 61/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 62/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 63/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 64/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0102 - 12s/epoch - 4ms/step
Epoch 65/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 66/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 67/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 68/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 69/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 70/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 71/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 72/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 73/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 74/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 75/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 76/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 77/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 78/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 79/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 80/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 81/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 82/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 83/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 84/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 85/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 86/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 87/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 88/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0102 - 12s/epoch - 4ms/step
Epoch 89/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 90/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 91/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 92/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 93/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 94/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 95/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 96/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 97/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 98/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 99/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 100/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0103 - 12s/epoch - 4ms/step
Epoch 101/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 102/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 103/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 104/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 105/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 106/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 107/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 108/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 109/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 110/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 111/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 112/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 113/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 114/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 115/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 116/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 117/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 118/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 119/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 120/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0105 - 13s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.010518183931708336
  1/332 [..............................] - ETA: 51s 36/332 [==>...........................] - ETA: 0s  73/332 [=====>........................] - ETA: 0s109/332 [========>.....................] - ETA: 0s145/332 [============>.................] - ETA: 0s181/332 [===============>..............] - ETA: 0s217/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s291/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07718394745016295
cosine 0.05923944822896259
MAE: 0.038210344
RMSE: 0.08615505
r2: 0.5189026899140003
RMSE zero-vector: 0.2430644284356365
['2.5custom_VAE', 'mse', 32, 120, 0.001, 0.2, 252, 0.010005587711930275, 0.010518183931708336, 0.07718394745016295, 0.05923944822896259, 0.038210343569517136, 0.08615504950284958, 0.5189026899140003, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 180 0.0005 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 5s - loss: 0.0303 - val_loss: 0.0184 - 5s/epoch - 7ms/step
Epoch 2/180
747/747 - 4s - loss: 0.0160 - val_loss: 0.0260 - 4s/epoch - 5ms/step
Epoch 3/180
747/747 - 4s - loss: 0.0148 - val_loss: 0.0148 - 4s/epoch - 5ms/step
Epoch 4/180
747/747 - 4s - loss: 0.0136 - val_loss: 0.0150 - 4s/epoch - 5ms/step
Epoch 5/180
747/747 - 4s - loss: 0.0129 - val_loss: 0.0145 - 4s/epoch - 5ms/step
Epoch 6/180
747/747 - 4s - loss: 0.0122 - val_loss: 0.0134 - 4s/epoch - 5ms/step
Epoch 7/180
747/747 - 4s - loss: 0.0118 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 8/180
747/747 - 4s - loss: 0.0113 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 9/180
747/747 - 4s - loss: 0.0111 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 10/180
747/747 - 4s - loss: 0.0120 - val_loss: 0.0133 - 4s/epoch - 5ms/step
Epoch 11/180
747/747 - 4s - loss: 0.0139 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 12/180
747/747 - 4s - loss: 0.0135 - val_loss: 0.0161 - 4s/epoch - 5ms/step
Epoch 13/180
747/747 - 4s - loss: 0.0170 - val_loss: 0.0443 - 4s/epoch - 5ms/step
Epoch 14/180
747/747 - 4s - loss: 0.0125 - val_loss: 0.0128 - 4s/epoch - 6ms/step
Epoch 15/180
747/747 - 4s - loss: 0.0134 - val_loss: 0.0144 - 4s/epoch - 5ms/step
Epoch 16/180
747/747 - 4s - loss: 0.0196 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 17/180
747/747 - 4s - loss: 0.0134 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 18/180
747/747 - 4s - loss: 0.0115 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 19/180
747/747 - 4s - loss: 0.0113 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 20/180
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 21/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 22/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 23/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 24/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 25/180
747/747 - 4s - loss: 0.0118 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 26/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 27/180
747/747 - 4s - loss: 0.0119 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 28/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 29/180
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 30/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 31/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 32/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 33/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 34/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 35/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 36/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 37/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 38/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 39/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 40/180
747/747 - 4s - loss: 0.0111 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 41/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 42/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 43/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 44/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 45/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 46/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 47/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 48/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 49/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 50/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 51/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 52/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 53/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 54/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 55/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 56/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 57/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 58/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 59/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 60/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 61/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 62/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 63/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 64/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 65/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 66/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 67/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 68/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 69/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 70/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 71/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 72/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 73/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 74/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 75/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 76/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 77/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 78/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 79/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 80/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 81/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 82/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 83/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 84/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 85/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 86/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 87/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 88/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 89/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 90/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 91/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 92/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 93/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 94/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 95/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 96/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 97/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 98/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 99/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 100/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 101/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 102/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 103/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 104/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 105/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 106/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 107/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 108/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 109/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 110/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 111/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 112/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 113/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 114/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 115/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 116/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 117/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 118/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 119/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 120/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 121/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 122/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 123/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 124/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 125/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 126/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 127/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 128/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 129/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 130/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 131/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 132/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 133/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 134/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 135/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 136/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 137/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 138/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 139/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 140/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 141/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 142/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 143/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 144/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 145/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 146/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 147/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 148/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 149/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 150/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 151/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 152/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 153/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 154/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 155/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 156/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 157/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 158/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 159/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 160/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 161/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 162/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 163/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 164/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 165/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 166/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 167/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 168/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 169/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 170/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 171/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 172/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 173/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 174/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 175/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 176/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 177/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 178/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 179/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 180/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009274762123823166
  1/332 [..............................] - ETA: 51s 37/332 [==>...........................] - ETA: 0s  73/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s141/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s215/332 [==================>...........] - ETA: 0s251/332 [=====================>........] - ETA: 0s288/332 [=========================>....] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0682173476403855
cosine 0.052263215835751387
MAE: 0.0355626
RMSE: 0.07725729
r2: 0.6131430030830872
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 128, 180, 0.0005, 0.2, 252, 0.0094473110511899, 0.009274762123823166, 0.0682173476403855, 0.052263215835751387, 0.03556260094046593, 0.07725729048252106, 0.6131430030830872, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 60 0.0005 16 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/60
5969/5969 - 27s - loss: 0.0170 - val_loss: 0.0129 - 27s/epoch - 5ms/step
Epoch 2/60
5969/5969 - 27s - loss: 0.0129 - val_loss: 0.0123 - 27s/epoch - 5ms/step
Epoch 3/60
5969/5969 - 27s - loss: 0.0121 - val_loss: 0.0118 - 27s/epoch - 4ms/step
Epoch 4/60
5969/5969 - 27s - loss: 0.0117 - val_loss: 0.0120 - 27s/epoch - 5ms/step
Epoch 5/60
5969/5969 - 27s - loss: 0.0115 - val_loss: 0.0123 - 27s/epoch - 4ms/step
Epoch 6/60
5969/5969 - 27s - loss: 0.0114 - val_loss: 0.0122 - 27s/epoch - 5ms/step
Epoch 7/60
5969/5969 - 27s - loss: 0.0113 - val_loss: 0.0129 - 27s/epoch - 5ms/step
Epoch 8/60
5969/5969 - 27s - loss: 0.0113 - val_loss: 0.0128 - 27s/epoch - 5ms/step
Epoch 9/60
5969/5969 - 27s - loss: 0.0112 - val_loss: 0.0135 - 27s/epoch - 4ms/step
Epoch 10/60
5969/5969 - 26s - loss: 0.0112 - val_loss: 0.0122 - 26s/epoch - 4ms/step
Epoch 11/60
5969/5969 - 27s - loss: 0.0112 - val_loss: 0.0132 - 27s/epoch - 4ms/step
Epoch 12/60
5969/5969 - 27s - loss: 0.0112 - val_loss: 0.0130 - 27s/epoch - 4ms/step
Epoch 13/60
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.0127 - 27s/epoch - 4ms/step
Epoch 14/60
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.0125 - 26s/epoch - 4ms/step
Epoch 15/60
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.0125 - 26s/epoch - 4ms/step
Epoch 16/60
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.0133 - 27s/epoch - 4ms/step
Epoch 17/60
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0133 - 27s/epoch - 5ms/step
Epoch 18/60
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0128 - 27s/epoch - 5ms/step
Epoch 19/60
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0132 - 27s/epoch - 5ms/step
Epoch 20/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0149 - 27s/epoch - 5ms/step
Epoch 21/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0139 - 27s/epoch - 4ms/step
Epoch 22/60
5969/5969 - 28s - loss: 0.0109 - val_loss: 0.0138 - 28s/epoch - 5ms/step
Epoch 23/60
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0133 - 26s/epoch - 4ms/step
Epoch 24/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0148 - 27s/epoch - 4ms/step
Epoch 25/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0148 - 27s/epoch - 5ms/step
Epoch 26/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0136 - 27s/epoch - 4ms/step
Epoch 27/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0158 - 27s/epoch - 5ms/step
Epoch 28/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0144 - 27s/epoch - 4ms/step
Epoch 29/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0126 - 27s/epoch - 5ms/step
Epoch 30/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0148 - 27s/epoch - 5ms/step
Epoch 31/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0138 - 27s/epoch - 5ms/step
Epoch 32/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0139 - 27s/epoch - 5ms/step
Epoch 33/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0135 - 27s/epoch - 5ms/step
Epoch 34/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0152 - 27s/epoch - 5ms/step
Epoch 35/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0140 - 27s/epoch - 5ms/step
Epoch 36/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0155 - 27s/epoch - 5ms/step
Epoch 37/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0148 - 27s/epoch - 5ms/step
Epoch 38/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0151 - 27s/epoch - 4ms/step
Epoch 39/60
5969/5969 - 28s - loss: 0.0107 - val_loss: 0.0155 - 28s/epoch - 5ms/step
Epoch 40/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0143 - 27s/epoch - 4ms/step
Epoch 41/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0147 - 26s/epoch - 4ms/step
Epoch 42/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0152 - 27s/epoch - 4ms/step
Epoch 43/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0144 - 27s/epoch - 4ms/step
Epoch 44/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0151 - 26s/epoch - 4ms/step
Epoch 45/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0147 - 26s/epoch - 4ms/step
Epoch 46/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0161 - 27s/epoch - 4ms/step
Epoch 47/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0172 - 26s/epoch - 4ms/step
Epoch 48/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0169 - 26s/epoch - 4ms/step
Epoch 49/60
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0163 - 25s/epoch - 4ms/step
Epoch 50/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0185 - 26s/epoch - 4ms/step
Epoch 51/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0169 - 26s/epoch - 4ms/step
Epoch 52/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0177 - 26s/epoch - 4ms/step
Epoch 53/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0151 - 27s/epoch - 4ms/step
Epoch 54/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0162 - 26s/epoch - 4ms/step
Epoch 55/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0171 - 26s/epoch - 4ms/step
Epoch 56/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0175 - 26s/epoch - 4ms/step
Epoch 57/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0159 - 26s/epoch - 4ms/step
Epoch 58/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0158 - 26s/epoch - 4ms/step
Epoch 59/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0167 - 26s/epoch - 4ms/step
Epoch 60/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0155 - 26s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.015452830120921135
  1/332 [..............................] - ETA: 57s 35/332 [==>...........................] - ETA: 0s  72/332 [=====>........................] - ETA: 0s108/332 [========>.....................] - ETA: 0s144/332 [============>.................] - ETA: 0s180/332 [===============>..............] - ETA: 0s217/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s290/332 [=========================>....] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.09118359852153941
cosine 0.07050176020437846
MAE: 0.0437076
RMSE: 0.11398748
r2: 0.15785209565542874
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 16, 60, 0.0005, 0.2, 252, 0.010654022917151451, 0.015452830120921135, 0.09118359852153941, 0.07050176020437846, 0.043707601726055145, 0.11398748308420181, 0.15785209565542874, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 30 0.001 16 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/30
5969/5969 - 28s - loss: 0.0209 - val_loss: 0.0129 - 28s/epoch - 5ms/step
Epoch 2/30
5969/5969 - 25s - loss: 0.0127 - val_loss: 0.0120 - 25s/epoch - 4ms/step
Epoch 3/30
5969/5969 - 27s - loss: 0.0121 - val_loss: 0.0122 - 27s/epoch - 4ms/step
Epoch 4/30
5969/5969 - 26s - loss: 0.0120 - val_loss: 0.0142 - 26s/epoch - 4ms/step
Epoch 5/30
5969/5969 - 26s - loss: 0.0119 - val_loss: 0.0135 - 26s/epoch - 4ms/step
Epoch 6/30
5969/5969 - 26s - loss: 0.0117 - val_loss: 0.0131 - 26s/epoch - 4ms/step
Epoch 7/30
5969/5969 - 26s - loss: 0.0115 - val_loss: 0.0170 - 26s/epoch - 4ms/step
Epoch 8/30
5969/5969 - 26s - loss: 0.0114 - val_loss: 0.0159 - 26s/epoch - 4ms/step
Epoch 9/30
5969/5969 - 26s - loss: 0.0113 - val_loss: 0.0172 - 26s/epoch - 4ms/step
Epoch 10/30
5969/5969 - 26s - loss: 0.0112 - val_loss: 0.0176 - 26s/epoch - 4ms/step
Epoch 11/30
5969/5969 - 26s - loss: 0.0112 - val_loss: 0.0186 - 26s/epoch - 4ms/step
Epoch 12/30
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.0192 - 26s/epoch - 4ms/step
Epoch 13/30
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.0186 - 26s/epoch - 4ms/step
Epoch 14/30
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.0221 - 26s/epoch - 4ms/step
Epoch 15/30
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.0152 - 26s/epoch - 4ms/step
Epoch 16/30
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0192 - 27s/epoch - 4ms/step
Epoch 17/30
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0179 - 27s/epoch - 5ms/step
Epoch 18/30
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0163 - 26s/epoch - 4ms/step
Epoch 19/30
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0182 - 26s/epoch - 4ms/step
Epoch 20/30
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0184 - 27s/epoch - 4ms/step
Epoch 21/30
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0209 - 26s/epoch - 4ms/step
Epoch 22/30
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0194 - 26s/epoch - 4ms/step
Epoch 23/30
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0193 - 26s/epoch - 4ms/step
Epoch 24/30
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0158 - 26s/epoch - 4ms/step
Epoch 25/30
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0199 - 26s/epoch - 4ms/step
Epoch 26/30
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0187 - 27s/epoch - 4ms/step
Epoch 27/30
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0178 - 27s/epoch - 4ms/step
Epoch 28/30
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0190 - 26s/epoch - 4ms/step
Epoch 29/30
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0155 - 26s/epoch - 4ms/step
Epoch 30/30
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0192 - 26s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.019155601039528847
  1/332 [..............................] - ETA: 58s 36/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s139/332 [===========>..................] - ETA: 0s175/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s247/332 [=====================>........] - ETA: 0s283/332 [========================>.....] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0944882732936449
cosine 0.07324134901901752
MAE: 0.044990297
RMSE: 0.12842566
r2: -0.06900221733776866
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 16, 30, 0.001, 0.2, 252, 0.01079488918185234, 0.019155601039528847, 0.0944882732936449, 0.07324134901901752, 0.04499029740691185, 0.12842565774917603, -0.06900221733776866, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 90 0.001 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/90
2985/2985 - 14s - loss: 0.0253 - val_loss: 0.0141 - 14s/epoch - 5ms/step
Epoch 2/90
2985/2985 - 13s - loss: 0.0132 - val_loss: 0.0122 - 13s/epoch - 4ms/step
Epoch 3/90
2985/2985 - 13s - loss: 0.0122 - val_loss: 0.0116 - 13s/epoch - 4ms/step
Epoch 4/90
2985/2985 - 13s - loss: 0.0118 - val_loss: 0.0115 - 13s/epoch - 5ms/step
Epoch 5/90
2985/2985 - 13s - loss: 0.0116 - val_loss: 0.0116 - 13s/epoch - 4ms/step
Epoch 6/90
2985/2985 - 13s - loss: 0.0114 - val_loss: 0.0113 - 13s/epoch - 4ms/step
Epoch 7/90
2985/2985 - 13s - loss: 0.0111 - val_loss: 0.0110 - 13s/epoch - 4ms/step
Epoch 8/90
2985/2985 - 13s - loss: 0.0109 - val_loss: 0.0108 - 13s/epoch - 4ms/step
Epoch 9/90
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0106 - 13s/epoch - 4ms/step
Epoch 10/90
2985/2985 - 13s - loss: 0.0107 - val_loss: 0.0109 - 13s/epoch - 4ms/step
Epoch 11/90
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 12/90
2985/2985 - 14s - loss: 0.0106 - val_loss: 0.0111 - 14s/epoch - 5ms/step
Epoch 13/90
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0108 - 13s/epoch - 4ms/step
Epoch 14/90
2985/2985 - 14s - loss: 0.0105 - val_loss: 0.0106 - 14s/epoch - 5ms/step
Epoch 15/90
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 16/90
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 17/90
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 18/90
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 19/90
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0106 - 13s/epoch - 4ms/step
Epoch 20/90
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 21/90
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 22/90
2985/2985 - 14s - loss: 0.0103 - val_loss: 0.0103 - 14s/epoch - 5ms/step
Epoch 23/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 24/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 25/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 26/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 27/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 28/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 29/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 30/90
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 31/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 32/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 33/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 34/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 35/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 36/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 37/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 38/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 39/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 40/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 41/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0099 - 13s/epoch - 5ms/step
Epoch 42/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 43/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 44/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 45/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 46/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 47/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 48/90
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 49/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 50/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0098 - 13s/epoch - 4ms/step
Epoch 51/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 52/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 53/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0098 - 13s/epoch - 4ms/step
Epoch 54/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 55/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 56/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0098 - 13s/epoch - 4ms/step
Epoch 57/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 58/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0098 - 13s/epoch - 4ms/step
Epoch 59/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 60/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 61/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 62/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0098 - 13s/epoch - 4ms/step
Epoch 63/90
2985/2985 - 14s - loss: 0.0101 - val_loss: 0.0098 - 14s/epoch - 5ms/step
Epoch 64/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0098 - 13s/epoch - 4ms/step
Epoch 65/90
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0098 - 13s/epoch - 4ms/step
Epoch 66/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 67/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 68/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 69/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 70/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 71/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 72/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 73/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 74/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 75/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 76/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 77/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0097 - 15s/epoch - 5ms/step
Epoch 78/90
2985/2985 - 15s - loss: 0.0101 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 79/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0097 - 15s/epoch - 5ms/step
Epoch 80/90
2985/2985 - 16s - loss: 0.0101 - val_loss: 0.0098 - 16s/epoch - 5ms/step
Epoch 81/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 82/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0097 - 15s/epoch - 5ms/step
Epoch 83/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0097 - 15s/epoch - 5ms/step
Epoch 84/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0097 - 15s/epoch - 5ms/step
Epoch 85/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0097 - 15s/epoch - 5ms/step
Epoch 86/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 87/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 88/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 89/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0098 - 15s/epoch - 5ms/step
Epoch 90/90
2985/2985 - 15s - loss: 0.0100 - val_loss: 0.0098 - 15s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009766057133674622
  1/332 [..............................] - ETA: 55s 28/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 97/332 [=======>......................] - ETA: 0s132/332 [==========>...................] - ETA: 0s167/332 [==============>...............] - ETA: 0s203/332 [=================>............] - ETA: 0s238/332 [====================>.........] - ETA: 0s273/332 [=======================>......] - ETA: 0s308/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07732043497592964
cosine 0.059300774207799965
MAE: 0.0383261
RMSE: 0.08241694
r2: 0.5597451907169024
RMSE zero-vector: 0.2430644284356365
['2.5custom_VAE', 'mse', 32, 90, 0.001, 0.2, 252, 0.010024161078035831, 0.009766057133674622, 0.07732043497592964, 0.059300774207799965, 0.03832609951496124, 0.08241693675518036, 0.5597451907169024, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 180 0.0005 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 6s - loss: 0.0229 - val_loss: 0.0172 - 6s/epoch - 8ms/step
Epoch 2/180
747/747 - 4s - loss: 0.0141 - val_loss: 0.0155 - 4s/epoch - 6ms/step
Epoch 3/180
747/747 - 4s - loss: 0.0136 - val_loss: 0.0136 - 4s/epoch - 6ms/step
Epoch 4/180
747/747 - 4s - loss: 0.0133 - val_loss: 0.0131 - 4s/epoch - 6ms/step
Epoch 5/180
747/747 - 5s - loss: 0.0130 - val_loss: 0.0129 - 5s/epoch - 6ms/step
Epoch 6/180
747/747 - 4s - loss: 0.0127 - val_loss: 0.0126 - 4s/epoch - 6ms/step
Epoch 7/180
747/747 - 5s - loss: 0.0122 - val_loss: 0.0120 - 5s/epoch - 6ms/step
Epoch 8/180
747/747 - 4s - loss: 0.0117 - val_loss: 0.0115 - 4s/epoch - 6ms/step
Epoch 9/180
747/747 - 4s - loss: 0.0113 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 10/180
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 6ms/step
Epoch 11/180
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 12/180
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 13/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 14/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 15/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 16/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 17/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 18/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 5ms/step
Epoch 19/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 20/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 21/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 5ms/step
Epoch 22/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 5ms/step
Epoch 23/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 5ms/step
Epoch 24/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 25/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 26/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 27/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 5ms/step
Epoch 28/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 29/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 30/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 5ms/step
Epoch 31/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 32/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 33/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 34/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 35/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 36/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 37/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 38/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 39/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 40/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 41/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 42/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 43/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 44/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 45/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 46/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 47/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 48/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 49/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 50/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 51/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 52/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 53/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 54/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 55/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 56/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 57/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 58/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 59/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 60/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 61/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 62/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 63/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 64/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 65/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 66/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 67/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 68/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 69/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 70/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 71/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 72/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 73/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 74/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 75/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 76/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 77/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 78/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 79/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 80/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 81/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 82/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 83/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 84/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 85/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 86/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 87/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 88/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 89/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 90/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 91/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 92/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 93/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 94/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 95/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 96/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 97/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 98/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 99/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 100/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 101/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 102/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 103/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 104/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 105/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 106/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 107/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 108/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 109/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 110/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 111/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 112/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 113/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 114/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 115/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 116/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 117/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 118/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 119/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 120/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 121/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 122/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 123/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 124/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 125/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 126/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 127/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 128/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 129/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 130/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 131/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 132/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 133/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 134/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 135/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 136/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 137/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 138/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 139/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 140/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 141/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 142/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 143/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 144/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 145/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 146/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 147/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 148/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 149/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 150/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 151/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 152/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 153/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 154/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 155/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 6ms/step
Epoch 156/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 157/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 158/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 159/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 160/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 161/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 162/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 163/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 164/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 165/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 166/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 167/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 168/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 169/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 170/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 171/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 172/180
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 173/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 174/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 175/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 176/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 177/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 178/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 179/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 180/180
747/747 - 4s - loss: 0.0092 - val_loss: 0.0091 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00908295251429081
  1/332 [..............................] - ETA: 54s 35/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s140/332 [===========>..................] - ETA: 0s175/332 [==============>...............] - ETA: 0s210/332 [=================>............] - ETA: 0s245/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s316/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06403757257143416
cosine 0.04906583197984972
MAE: 0.034285598
RMSE: 0.07487302
r2: 0.6366524905612965
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 128, 180, 0.0005, 0.2, 252, 0.009243393316864967, 0.00908295251429081, 0.06403757257143416, 0.04906583197984972, 0.03428559750318527, 0.07487302273511887, 0.6366524905612965, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 150 0.002 16 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/150
5969/5969 - 26s - loss: 0.0224 - val_loss: 0.0131 - 26s/epoch - 4ms/step
Epoch 2/150
5969/5969 - 25s - loss: 0.0131 - val_loss: 0.0126 - 25s/epoch - 4ms/step
Epoch 3/150
5969/5969 - 25s - loss: 0.0123 - val_loss: 0.0127 - 25s/epoch - 4ms/step
Epoch 4/150
5969/5969 - 26s - loss: 0.0120 - val_loss: 0.0172 - 26s/epoch - 4ms/step
Epoch 5/150
5969/5969 - 26s - loss: 0.0118 - val_loss: 0.0196 - 26s/epoch - 4ms/step
Epoch 6/150
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0212 - 25s/epoch - 4ms/step
Epoch 7/150
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0225 - 25s/epoch - 4ms/step
Epoch 8/150
5969/5969 - 26s - loss: 0.0115 - val_loss: 0.0195 - 26s/epoch - 4ms/step
Epoch 9/150
5969/5969 - 26s - loss: 0.0114 - val_loss: 0.0309 - 26s/epoch - 4ms/step
Epoch 10/150
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0353 - 25s/epoch - 4ms/step
Epoch 11/150
5969/5969 - 26s - loss: 0.0113 - val_loss: 0.0666 - 26s/epoch - 4ms/step
Epoch 12/150
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0659 - 25s/epoch - 4ms/step
Epoch 13/150
5969/5969 - 26s - loss: 0.0112 - val_loss: 0.0520 - 26s/epoch - 4ms/step
Epoch 14/150
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.0382 - 26s/epoch - 4ms/step
Epoch 15/150
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.0427 - 26s/epoch - 4ms/step
Epoch 16/150
5969/5969 - 25s - loss: 0.0110 - val_loss: 0.0334 - 25s/epoch - 4ms/step
Epoch 17/150
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.0427 - 26s/epoch - 4ms/step
Epoch 18/150
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.0308 - 26s/epoch - 4ms/step
Epoch 19/150
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.0479 - 26s/epoch - 4ms/step
Epoch 20/150
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0353 - 26s/epoch - 4ms/step
Epoch 21/150
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0528 - 27s/epoch - 4ms/step
Epoch 22/150
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0665 - 25s/epoch - 4ms/step
Epoch 23/150
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0556 - 25s/epoch - 4ms/step
Epoch 24/150
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0509 - 26s/epoch - 4ms/step
Epoch 25/150
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0437 - 26s/epoch - 4ms/step
Epoch 26/150
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0409 - 25s/epoch - 4ms/step
Epoch 27/150
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0657 - 26s/epoch - 4ms/step
Epoch 28/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0778 - 26s/epoch - 4ms/step
Epoch 29/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0455 - 26s/epoch - 4ms/step
Epoch 30/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0489 - 26s/epoch - 4ms/step
Epoch 31/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0442 - 26s/epoch - 4ms/step
Epoch 32/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0661 - 26s/epoch - 4ms/step
Epoch 33/150
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0481 - 25s/epoch - 4ms/step
Epoch 34/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0598 - 26s/epoch - 4ms/step
Epoch 35/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0582 - 26s/epoch - 4ms/step
Epoch 36/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0195 - 26s/epoch - 4ms/step
Epoch 37/150
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0361 - 25s/epoch - 4ms/step
Epoch 38/150
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0342 - 25s/epoch - 4ms/step
Epoch 39/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0289 - 26s/epoch - 4ms/step
Epoch 40/150
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0273 - 26s/epoch - 4ms/step
Epoch 41/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0388 - 26s/epoch - 4ms/step
Epoch 42/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0265 - 26s/epoch - 4ms/step
Epoch 43/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0493 - 26s/epoch - 4ms/step
Epoch 44/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0320 - 26s/epoch - 4ms/step
Epoch 45/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0340 - 26s/epoch - 4ms/step
Epoch 46/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0468 - 26s/epoch - 4ms/step
Epoch 47/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0217 - 26s/epoch - 4ms/step
Epoch 48/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0492 - 26s/epoch - 4ms/step
Epoch 49/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0276 - 26s/epoch - 4ms/step
Epoch 50/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0339 - 26s/epoch - 4ms/step
Epoch 51/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0204 - 26s/epoch - 4ms/step
Epoch 52/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0346 - 26s/epoch - 4ms/step
Epoch 53/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0219 - 26s/epoch - 4ms/step
Epoch 54/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0239 - 26s/epoch - 4ms/step
Epoch 55/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0354 - 26s/epoch - 4ms/step
Epoch 56/150
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0255 - 25s/epoch - 4ms/step
Epoch 57/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0261 - 26s/epoch - 4ms/step
Epoch 58/150
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0241 - 27s/epoch - 4ms/step
Epoch 59/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0230 - 26s/epoch - 4ms/step
Epoch 60/150
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0235 - 25s/epoch - 4ms/step
Epoch 61/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0255 - 26s/epoch - 4ms/step
Epoch 62/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0261 - 26s/epoch - 4ms/step
Epoch 63/150
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0303 - 26s/epoch - 4ms/step
Epoch 64/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0309 - 26s/epoch - 4ms/step
Epoch 65/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0381 - 26s/epoch - 4ms/step
Epoch 66/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0258 - 25s/epoch - 4ms/step
Epoch 67/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0288 - 25s/epoch - 4ms/step
Epoch 68/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0216 - 27s/epoch - 5ms/step
Epoch 69/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0234 - 27s/epoch - 5ms/step
Epoch 70/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0373 - 27s/epoch - 4ms/step
Epoch 71/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0211 - 27s/epoch - 4ms/step
Epoch 72/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0267 - 26s/epoch - 4ms/step
Epoch 73/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0283 - 27s/epoch - 5ms/step
Epoch 74/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0571 - 27s/epoch - 5ms/step
Epoch 75/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0294 - 27s/epoch - 4ms/step
Epoch 76/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0668 - 27s/epoch - 4ms/step
Epoch 77/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0214 - 26s/epoch - 4ms/step
Epoch 78/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0300 - 27s/epoch - 5ms/step
Epoch 79/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0244 - 27s/epoch - 5ms/step
Epoch 80/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0228 - 27s/epoch - 4ms/step
Epoch 81/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0288 - 27s/epoch - 5ms/step
Epoch 82/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0268 - 26s/epoch - 4ms/step
Epoch 83/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0260 - 27s/epoch - 5ms/step
Epoch 84/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0240 - 27s/epoch - 4ms/step
Epoch 85/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0235 - 26s/epoch - 4ms/step
Epoch 86/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0304 - 27s/epoch - 5ms/step
Epoch 87/150
5969/5969 - 28s - loss: 0.0106 - val_loss: 0.0223 - 28s/epoch - 5ms/step
Epoch 88/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0215 - 27s/epoch - 5ms/step
Epoch 89/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0206 - 26s/epoch - 4ms/step
Epoch 90/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0235 - 27s/epoch - 5ms/step
Epoch 91/150
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.0214 - 29s/epoch - 5ms/step
Epoch 92/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0445 - 26s/epoch - 4ms/step
Epoch 93/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0236 - 25s/epoch - 4ms/step
Epoch 94/150
5969/5969 - 27s - loss: 0.0106 - val_loss: 0.0212 - 27s/epoch - 4ms/step
Epoch 95/150
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.0288 - 29s/epoch - 5ms/step
Epoch 96/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0241 - 26s/epoch - 4ms/step
Epoch 97/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0279 - 26s/epoch - 4ms/step
Epoch 98/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0225 - 25s/epoch - 4ms/step
Epoch 99/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0234 - 25s/epoch - 4ms/step
Epoch 100/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0259 - 25s/epoch - 4ms/step
Epoch 101/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0284 - 25s/epoch - 4ms/step
Epoch 102/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0248 - 25s/epoch - 4ms/step
Epoch 103/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0265 - 26s/epoch - 4ms/step
Epoch 104/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0395 - 25s/epoch - 4ms/step
Epoch 105/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0256 - 25s/epoch - 4ms/step
Epoch 106/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0373 - 26s/epoch - 4ms/step
Epoch 107/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0422 - 25s/epoch - 4ms/step
Epoch 108/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0309 - 26s/epoch - 4ms/step
Epoch 109/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0305 - 25s/epoch - 4ms/step
Epoch 110/150
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0318 - 25s/epoch - 4ms/step
Epoch 111/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0307 - 26s/epoch - 4ms/step
Epoch 112/150
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0257 - 26s/epoch - 4ms/step
Epoch 113/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0307 - 26s/epoch - 4ms/step
Epoch 114/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0287 - 26s/epoch - 4ms/step
Epoch 115/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0295 - 26s/epoch - 4ms/step
Epoch 116/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0244 - 26s/epoch - 4ms/step
Epoch 117/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0293 - 26s/epoch - 4ms/step
Epoch 118/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0252 - 26s/epoch - 4ms/step
Epoch 119/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0249 - 25s/epoch - 4ms/step
Epoch 120/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0228 - 25s/epoch - 4ms/step
Epoch 121/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0237 - 26s/epoch - 4ms/step
Epoch 122/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0265 - 25s/epoch - 4ms/step
Epoch 123/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0233 - 25s/epoch - 4ms/step
Epoch 124/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0227 - 26s/epoch - 4ms/step
Epoch 125/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0256 - 26s/epoch - 4ms/step
Epoch 126/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0321 - 25s/epoch - 4ms/step
Epoch 127/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0341 - 26s/epoch - 4ms/step
Epoch 128/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0242 - 25s/epoch - 4ms/step
Epoch 129/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0251 - 26s/epoch - 4ms/step
Epoch 130/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0277 - 26s/epoch - 4ms/step
Epoch 131/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0280 - 25s/epoch - 4ms/step
Epoch 132/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0289 - 26s/epoch - 4ms/step
Epoch 133/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0315 - 26s/epoch - 4ms/step
Epoch 134/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0283 - 25s/epoch - 4ms/step
Epoch 135/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0247 - 26s/epoch - 4ms/step
Epoch 136/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0261 - 26s/epoch - 4ms/step
Epoch 137/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0252 - 25s/epoch - 4ms/step
Epoch 138/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0261 - 26s/epoch - 4ms/step
Epoch 139/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0285 - 26s/epoch - 4ms/step
Epoch 140/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0305 - 26s/epoch - 4ms/step
Epoch 141/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0382 - 26s/epoch - 4ms/step
Epoch 142/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0336 - 26s/epoch - 4ms/step
Epoch 143/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0306 - 25s/epoch - 4ms/step
Epoch 144/150
5969/5969 - 26s - loss: 0.0105 - val_loss: 0.0255 - 26s/epoch - 4ms/step
Epoch 145/150
5969/5969 - 25s - loss: 0.0105 - val_loss: 0.0264 - 25s/epoch - 4ms/step
Epoch 146/150
5969/5969 - 27s - loss: 0.0105 - val_loss: 0.0381 - 27s/epoch - 5ms/step
Epoch 147/150
5969/5969 - 30s - loss: 0.0105 - val_loss: 0.0249 - 30s/epoch - 5ms/step
Epoch 148/150
5969/5969 - 30s - loss: 0.0105 - val_loss: 0.0334 - 30s/epoch - 5ms/step
Epoch 149/150
5969/5969 - 30s - loss: 0.0105 - val_loss: 0.0297 - 30s/epoch - 5ms/step
Epoch 150/150
5969/5969 - 30s - loss: 0.0105 - val_loss: 0.0382 - 30s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.03822476044297218
  1/332 [..............................] - ETA: 1:18 34/332 [==>...........................] - ETA: 0s   66/332 [====>.........................] - ETA: 0s 98/332 [=======>......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s163/332 [=============>................] - ETA: 0s197/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s261/332 [======================>.......] - ETA: 0s295/332 [=========================>....] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08926723650040556
cosine 0.06918831352710364
MAE: 0.04505044
RMSE: 0.19137268
r2: -1.373750736629949
RMSE zero-vector: 0.2430644284356365
['2.5custom_VAE', 'mse', 16, 150, 0.002, 0.2, 252, 0.010516735725104809, 0.03822476044297218, 0.08926723650040556, 0.06918831352710364, 0.04505043849349022, 0.19137267768383026, -1.373750736629949, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 210 0.0005 128 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/210
747/747 - 6s - loss: 0.0137 - val_loss: 0.0087 - 6s/epoch - 8ms/step
Epoch 2/210
747/747 - 4s - loss: 0.0078 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 3/210
747/747 - 5s - loss: 0.0075 - val_loss: 0.0085 - 5s/epoch - 6ms/step
Epoch 4/210
747/747 - 4s - loss: 0.0074 - val_loss: 0.0076 - 4s/epoch - 6ms/step
Epoch 5/210
747/747 - 4s - loss: 0.0072 - val_loss: 0.0074 - 4s/epoch - 6ms/step
Epoch 6/210
747/747 - 4s - loss: 0.0070 - val_loss: 0.0070 - 4s/epoch - 6ms/step
Epoch 7/210
747/747 - 4s - loss: 0.0069 - val_loss: 0.0069 - 4s/epoch - 6ms/step
Epoch 8/210
747/747 - 4s - loss: 0.0068 - val_loss: 0.0069 - 4s/epoch - 6ms/step
Epoch 9/210
747/747 - 5s - loss: 0.0068 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 10/210
747/747 - 5s - loss: 0.0066 - val_loss: 0.0065 - 5s/epoch - 6ms/step
Epoch 11/210
747/747 - 4s - loss: 0.0064 - val_loss: 0.0064 - 4s/epoch - 6ms/step
Epoch 12/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 13/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 14/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 15/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 16/210
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 17/210
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 18/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 19/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 20/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 21/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 22/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 23/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 24/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 25/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 26/210
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 27/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 28/210
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 29/210
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 30/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 31/210
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 32/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 33/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 34/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 35/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 36/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 37/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 38/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 39/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 40/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 41/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 42/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 43/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 44/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 45/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 46/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 47/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 48/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 49/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 50/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 51/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 52/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 53/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 54/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 55/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 56/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 57/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 58/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 59/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 60/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 61/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 62/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 63/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 64/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 65/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 66/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 67/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 68/210
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 69/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 70/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 71/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 72/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 73/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 74/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 75/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 76/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 77/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 78/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 79/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 80/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 81/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 82/210
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 83/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 84/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 85/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 86/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 87/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 88/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 89/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 90/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 91/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 92/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 93/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 94/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 95/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 96/210
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 97/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 98/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 99/210
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 7ms/step
Epoch 100/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 101/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 102/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 103/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 104/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 105/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 106/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 107/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 108/210
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 109/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 110/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 111/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 112/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 113/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 114/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 115/210
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 116/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 117/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 118/210
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 119/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 120/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 121/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 122/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 123/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 124/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 125/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 126/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 127/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 128/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 129/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 130/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 131/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 132/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 133/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 134/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 135/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 136/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 137/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 138/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 139/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 140/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 141/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 142/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 143/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 144/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 145/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 146/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 147/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 148/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 149/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 150/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 151/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 152/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 153/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 154/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 155/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 156/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 157/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 158/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 159/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 160/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 161/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 162/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 163/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 164/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 165/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 166/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 167/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 168/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 169/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 170/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 171/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 172/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 173/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 174/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 175/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 176/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 177/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 178/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 179/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 180/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 181/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 182/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 183/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 184/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 185/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 186/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 187/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 188/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 189/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 190/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 191/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 192/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 193/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 194/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 195/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 196/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 197/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 198/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 199/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 200/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 201/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 202/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 203/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 204/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 205/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 206/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 207/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 208/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 209/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 210/210
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005816882476210594
  1/332 [..............................] - ETA: 1:04 32/332 [=>............................] - ETA: 0s   63/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s163/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s226/332 [===================>..........] - ETA: 0s258/332 [======================>.......] - ETA: 0s291/332 [=========================>....] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.10338335733929536
cosine 0.07892438108252436
MAE: 0.043581016
RMSE: 0.09437097
r2: 0.4227704993336255
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 128, 210, 0.0005, 0.2, 252, 0.0058728610165417194, 0.005816882476210594, 0.10338335733929536, 0.07892438108252436, 0.04358101636171341, 0.09437096863985062, 0.4227704993336255, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 180 0.0005 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/180
1493/1493 - 10s - loss: 0.0199 - val_loss: 0.0152 - 10s/epoch - 7ms/step
Epoch 2/180
1493/1493 - 8s - loss: 0.0138 - val_loss: 0.0132 - 8s/epoch - 6ms/step
Epoch 3/180
1493/1493 - 9s - loss: 0.0131 - val_loss: 0.0126 - 9s/epoch - 6ms/step
Epoch 4/180
1493/1493 - 8s - loss: 0.0127 - val_loss: 0.0124 - 8s/epoch - 6ms/step
Epoch 5/180
1493/1493 - 8s - loss: 0.0123 - val_loss: 0.0118 - 8s/epoch - 6ms/step
Epoch 6/180
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 7/180
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 8/180
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 9/180
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 10/180
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 11/180
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 12/180
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 13/180
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 14/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 15/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 16/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 17/180
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 18/180
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 19/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 20/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 21/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 22/180
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 23/180
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 24/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 25/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 26/180
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 27/180
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 28/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 29/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 30/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 31/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 32/180
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 33/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 34/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 35/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 36/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 37/180
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 38/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 39/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 6ms/step
Epoch 40/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 41/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 42/180
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 43/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 44/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 45/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 46/180
1493/1493 - 9s - loss: 0.0098 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 47/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 48/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 49/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 50/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 51/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 52/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0095 - 9s/epoch - 6ms/step
Epoch 53/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 54/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 55/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 56/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 57/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 5ms/step
Epoch 58/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 59/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 60/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 61/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0095 - 8s/epoch - 6ms/step
Epoch 62/180
1493/1493 - 9s - loss: 0.0097 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 63/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 64/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 65/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 66/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 67/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 68/180
1493/1493 - 7s - loss: 0.0097 - val_loss: 0.0094 - 7s/epoch - 5ms/step
Epoch 69/180
1493/1493 - 7s - loss: 0.0097 - val_loss: 0.0094 - 7s/epoch - 5ms/step
Epoch 70/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 71/180
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0094 - 9s/epoch - 6ms/step
Epoch 72/180
1493/1493 - 8s - loss: 0.0097 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 73/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 74/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 75/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 76/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 77/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 78/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 79/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 80/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 81/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 82/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 83/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 84/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 85/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 86/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 87/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 88/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 89/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 90/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 91/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 92/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 93/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 94/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 95/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 96/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 97/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 98/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 99/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 100/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 101/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 102/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 103/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 5ms/step
Epoch 104/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 105/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 106/180
1493/1493 - 9s - loss: 0.0096 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 107/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 108/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 109/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 110/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 111/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 112/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 113/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 114/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 115/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 116/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 117/180
1493/1493 - 8s - loss: 0.0096 - val_loss: 0.0094 - 8s/epoch - 6ms/step
Epoch 118/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 119/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 120/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 121/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 122/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 123/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 124/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 125/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 126/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 127/180
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 128/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 129/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 130/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 131/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 132/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 133/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 134/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 135/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 136/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 137/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 138/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 139/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 140/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 141/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 142/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 143/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 144/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 145/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 146/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 147/180
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 148/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 149/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 150/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 151/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 152/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 153/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 154/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 155/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 156/180
1493/1493 - 7s - loss: 0.0095 - val_loss: 0.0093 - 7s/epoch - 5ms/step
Epoch 157/180
1493/1493 - 7s - loss: 0.0095 - val_loss: 0.0093 - 7s/epoch - 5ms/step
Epoch 158/180
1493/1493 - 7s - loss: 0.0095 - val_loss: 0.0093 - 7s/epoch - 5ms/step
Epoch 159/180
1493/1493 - 7s - loss: 0.0095 - val_loss: 0.0093 - 7s/epoch - 5ms/step
Epoch 160/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 161/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 162/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 163/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 164/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 165/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 166/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 167/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 6ms/step
Epoch 168/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 5ms/step
Epoch 169/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 170/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 171/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 172/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 173/180
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 174/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 175/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 176/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 177/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
Epoch 178/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0092 - 8s/epoch - 6ms/step
Epoch 179/180
1493/1493 - 9s - loss: 0.0095 - val_loss: 0.0093 - 9s/epoch - 6ms/step
Epoch 180/180
1493/1493 - 8s - loss: 0.0095 - val_loss: 0.0093 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009262769483029842
  1/332 [..............................] - ETA: 57s 34/332 [==>...........................] - ETA: 0s  68/332 [=====>........................] - ETA: 0s 99/332 [=======>......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s163/332 [=============>................] - ETA: 0s196/332 [================>.............] - ETA: 0s229/332 [===================>..........] - ETA: 0s262/332 [======================>.......] - ETA: 0s294/332 [=========================>....] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.06658221087027082
cosine 0.05101931932576414
MAE: 0.03505131
RMSE: 0.07639152
r2: 0.6217648868284669
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 64, 180, 0.0005, 0.2, 252, 0.009479694999754429, 0.009262769483029842, 0.06658221087027082, 0.05101931932576414, 0.03505130857229233, 0.07639151811599731, 0.6217648868284669, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 150 0.001 256 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0401 - val_loss: 0.0212 - 4s/epoch - 11ms/step
Epoch 2/150
374/374 - 2s - loss: 0.0168 - val_loss: 0.0183 - 2s/epoch - 6ms/step
Epoch 3/150
374/374 - 3s - loss: 0.0152 - val_loss: 0.0409 - 3s/epoch - 7ms/step
Epoch 4/150
374/374 - 2s - loss: 0.0149 - val_loss: 0.0193 - 2s/epoch - 7ms/step
Epoch 5/150
374/374 - 3s - loss: 0.0140 - val_loss: 0.0150 - 3s/epoch - 7ms/step
Epoch 6/150
374/374 - 2s - loss: 0.0132 - val_loss: 0.0156 - 2s/epoch - 7ms/step
Epoch 7/150
374/374 - 2s - loss: 0.0127 - val_loss: 0.0154 - 2s/epoch - 7ms/step
Epoch 8/150
374/374 - 2s - loss: 0.0124 - val_loss: 0.0170 - 2s/epoch - 7ms/step
Epoch 9/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0136 - 2s/epoch - 7ms/step
Epoch 10/150
374/374 - 2s - loss: 0.0118 - val_loss: 0.0877 - 2s/epoch - 7ms/step
Epoch 11/150
374/374 - 3s - loss: 0.0143 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 12/150
374/374 - 3s - loss: 0.0115 - val_loss: 0.0208 - 3s/epoch - 7ms/step
Epoch 13/150
374/374 - 2s - loss: 0.0124 - val_loss: 0.0276 - 2s/epoch - 7ms/step
Epoch 14/150
374/374 - 2s - loss: 0.0141 - val_loss: 0.0171 - 2s/epoch - 6ms/step
Epoch 15/150
374/374 - 3s - loss: 0.0124 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 16/150
374/374 - 3s - loss: 0.0117 - val_loss: 0.0143 - 3s/epoch - 7ms/step
Epoch 17/150
374/374 - 3s - loss: 0.0133 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 18/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 19/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 7ms/step
Epoch 20/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 21/150
374/374 - 3s - loss: 0.0110 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 22/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 23/150
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 7ms/step
Epoch 24/150
374/374 - 3s - loss: 0.0120 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 25/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 26/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 27/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 28/150
374/374 - 3s - loss: 0.0115 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 29/150
374/374 - 3s - loss: 0.0124 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 30/150
374/374 - 2s - loss: 0.0145 - val_loss: 0.0109 - 2s/epoch - 7ms/step
Epoch 31/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 32/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 33/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0106 - 2s/epoch - 7ms/step
Epoch 34/150
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 35/150
374/374 - 3s - loss: 0.0108 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 36/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 37/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 38/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 39/150
374/374 - 2s - loss: 0.0125 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 40/150
374/374 - 3s - loss: 0.0119 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 41/150
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 42/150
374/374 - 3s - loss: 0.0104 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 43/150
374/374 - 2s - loss: 0.0128 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 44/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 45/150
374/374 - 3s - loss: 0.0104 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 46/150
374/374 - 2s - loss: 0.0132 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 47/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 48/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 49/150
374/374 - 3s - loss: 0.0103 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 50/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 51/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 52/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0114 - 2s/epoch - 7ms/step
Epoch 53/150
374/374 - 3s - loss: 0.0136 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 54/150
374/374 - 3s - loss: 0.0105 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 55/150
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 56/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 57/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 58/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 59/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 60/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 61/150
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 62/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 63/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 64/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 65/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 66/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 67/150
374/374 - 2s - loss: 0.0126 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 68/150
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 69/150
374/374 - 3s - loss: 0.0100 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 70/150
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 71/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 72/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 73/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 74/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 75/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 76/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 77/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 78/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 79/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 80/150
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 81/150
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 82/150
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 83/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 84/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 85/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 86/150
374/374 - 3s - loss: 0.0098 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 87/150
374/374 - 3s - loss: 0.0106 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 88/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 89/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 90/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 91/150
374/374 - 3s - loss: 0.0104 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 92/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 93/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 94/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 95/150
374/374 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 96/150
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 97/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0103 - 2s/epoch - 7ms/step
Epoch 98/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 99/150
374/374 - 3s - loss: 0.0098 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 100/150
374/374 - 3s - loss: 0.0101 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 101/150
374/374 - 2s - loss: 0.0099 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 102/150
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 103/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 104/150
374/374 - 3s - loss: 0.0098 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 105/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 106/150
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 107/150
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 108/150
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 109/150
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 110/150
374/374 - 2s - loss: 0.0096 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 111/150
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 112/150
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 113/150
374/374 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 7ms/step
Epoch 114/150
374/374 - 2s - loss: 0.0096 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 115/150
374/374 - 2s - loss: 0.0097 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 116/150
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 117/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 118/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 119/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 120/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 121/150
374/374 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 7ms/step
Epoch 122/150
374/374 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 7ms/step
Epoch 123/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 124/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 125/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 126/150
374/374 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 7ms/step
Epoch 127/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 128/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 129/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 130/150
374/374 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 7ms/step
Epoch 131/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 132/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 133/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 134/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 135/150
374/374 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 7ms/step
Epoch 136/150
374/374 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 7ms/step
Epoch 137/150
374/374 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 7ms/step
Epoch 138/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 139/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 7ms/step
Epoch 140/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 141/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 142/150
374/374 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 7ms/step
Epoch 143/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 144/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 145/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 146/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 147/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 7ms/step
Epoch 148/150
374/374 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 149/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 150/150
374/374 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009294467978179455
  1/332 [..............................] - ETA: 1:00 30/332 [=>............................] - ETA: 0s   61/332 [====>.........................] - ETA: 0s 95/332 [=======>......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s162/332 [=============>................] - ETA: 0s196/332 [================>.............] - ETA: 0s230/332 [===================>..........] - ETA: 0s264/332 [======================>.......] - ETA: 0s297/332 [=========================>....] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.06969882621996096
cosine 0.05337635443721049
MAE: 0.035760205
RMSE: 0.07800928
r2: 0.6055756159700814
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 256, 150, 0.001, 0.2, 252, 0.009423752315342426, 0.009294467978179455, 0.06969882621996096, 0.05337635443721049, 0.03576020523905754, 0.07800927758216858, 0.6055756159700814, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 60 0.0005 16 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/60
5969/5969 - 32s - loss: 0.0214 - val_loss: 0.0129 - 32s/epoch - 5ms/step
Epoch 2/60
5969/5969 - 32s - loss: 0.0131 - val_loss: 0.0127 - 32s/epoch - 5ms/step
Epoch 3/60
5969/5969 - 31s - loss: 0.0126 - val_loss: 0.0126 - 31s/epoch - 5ms/step
Epoch 4/60
5969/5969 - 31s - loss: 0.0121 - val_loss: 0.0152 - 31s/epoch - 5ms/step
Epoch 5/60
5969/5969 - 27s - loss: 0.0120 - val_loss: 0.0182 - 27s/epoch - 4ms/step
Epoch 6/60
5969/5969 - 31s - loss: 0.0118 - val_loss: 0.0184 - 31s/epoch - 5ms/step
Epoch 7/60
5969/5969 - 30s - loss: 0.0115 - val_loss: 0.0184 - 30s/epoch - 5ms/step
Epoch 8/60
5969/5969 - 30s - loss: 0.0115 - val_loss: 0.0199 - 30s/epoch - 5ms/step
Epoch 9/60
5969/5969 - 31s - loss: 0.0114 - val_loss: 0.0181 - 31s/epoch - 5ms/step
Epoch 10/60
5969/5969 - 31s - loss: 0.0113 - val_loss: 0.0221 - 31s/epoch - 5ms/step
Epoch 11/60
5969/5969 - 31s - loss: 0.0112 - val_loss: 0.0273 - 31s/epoch - 5ms/step
Epoch 12/60
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.0272 - 30s/epoch - 5ms/step
Epoch 13/60
5969/5969 - 30s - loss: 0.0111 - val_loss: 0.0210 - 30s/epoch - 5ms/step
Epoch 14/60
5969/5969 - 31s - loss: 0.0111 - val_loss: 0.0211 - 31s/epoch - 5ms/step
Epoch 15/60
5969/5969 - 31s - loss: 0.0111 - val_loss: 0.0242 - 31s/epoch - 5ms/step
Epoch 16/60
5969/5969 - 30s - loss: 0.0111 - val_loss: 0.0399 - 30s/epoch - 5ms/step
Epoch 17/60
5969/5969 - 30s - loss: 0.0110 - val_loss: 0.0383 - 30s/epoch - 5ms/step
Epoch 18/60
5969/5969 - 30s - loss: 0.0110 - val_loss: 0.0345 - 30s/epoch - 5ms/step
Epoch 19/60
5969/5969 - 30s - loss: 0.0110 - val_loss: 0.0400 - 30s/epoch - 5ms/step
Epoch 20/60
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0459 - 30s/epoch - 5ms/step
Epoch 21/60
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0402 - 30s/epoch - 5ms/step
Epoch 22/60
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0440 - 31s/epoch - 5ms/step
Epoch 23/60
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0396 - 31s/epoch - 5ms/step
Epoch 24/60
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0347 - 31s/epoch - 5ms/step
Epoch 25/60
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0399 - 31s/epoch - 5ms/step
Epoch 26/60
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0338 - 31s/epoch - 5ms/step
Epoch 27/60
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0597 - 30s/epoch - 5ms/step
Epoch 28/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0888 - 30s/epoch - 5ms/step
Epoch 29/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0425 - 27s/epoch - 4ms/step
Epoch 30/60
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0407 - 31s/epoch - 5ms/step
Epoch 31/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0649 - 30s/epoch - 5ms/step
Epoch 32/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0646 - 30s/epoch - 5ms/step
Epoch 33/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0520 - 30s/epoch - 5ms/step
Epoch 34/60
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0745 - 31s/epoch - 5ms/step
Epoch 35/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0596 - 30s/epoch - 5ms/step
Epoch 36/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0451 - 30s/epoch - 5ms/step
Epoch 37/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0542 - 30s/epoch - 5ms/step
Epoch 38/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0551 - 30s/epoch - 5ms/step
Epoch 39/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0667 - 30s/epoch - 5ms/step
Epoch 40/60
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.0588 - 30s/epoch - 5ms/step
Epoch 41/60
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.0508 - 30s/epoch - 5ms/step
Epoch 42/60
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0439 - 30s/epoch - 5ms/step
Epoch 43/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0420 - 31s/epoch - 5ms/step
Epoch 44/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0348 - 31s/epoch - 5ms/step
Epoch 45/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0713 - 31s/epoch - 5ms/step
Epoch 46/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0449 - 31s/epoch - 5ms/step
Epoch 47/60
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.0393 - 30s/epoch - 5ms/step
Epoch 48/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0611 - 31s/epoch - 5ms/step
Epoch 49/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0349 - 31s/epoch - 5ms/step
Epoch 50/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0454 - 31s/epoch - 5ms/step
Epoch 51/60
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.0411 - 30s/epoch - 5ms/step
Epoch 52/60
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.0414 - 29s/epoch - 5ms/step
Epoch 53/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0348 - 27s/epoch - 5ms/step
Epoch 54/60
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.0460 - 30s/epoch - 5ms/step
Epoch 55/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0599 - 31s/epoch - 5ms/step
Epoch 56/60
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.0445 - 30s/epoch - 5ms/step
Epoch 57/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0721 - 31s/epoch - 5ms/step
Epoch 58/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0384 - 31s/epoch - 5ms/step
Epoch 59/60
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0566 - 31s/epoch - 5ms/step
Epoch 60/60
5969/5969 - 32s - loss: 0.0107 - val_loss: 0.0389 - 32s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0389169380068779
  1/332 [..............................] - ETA: 1:02 31/332 [=>............................] - ETA: 0s   61/332 [====>.........................] - ETA: 0s 94/332 [=======>......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s159/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s224/332 [===================>..........] - ETA: 0s257/332 [======================>.......] - ETA: 0s290/332 [=========================>....] - ETA: 0s322/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.09398095731550102
cosine 0.0730884004787279
MAE: 0.04598904
RMSE: 0.19916062
r2: -1.5708887814517396
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 16, 60, 0.0005, 0.2, 252, 0.010674074292182922, 0.0389169380068779, 0.09398095731550102, 0.0730884004787279, 0.04598904028534889, 0.1991606205701828, -1.5708887814517396, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 90 0.002 16 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/90
5969/5969 - 32s - loss: 0.0098 - val_loss: 0.0069 - 32s/epoch - 5ms/step
Epoch 2/90
5969/5969 - 33s - loss: 0.0069 - val_loss: 0.0068 - 33s/epoch - 6ms/step
Epoch 3/90
5969/5969 - 32s - loss: 0.0067 - val_loss: 0.0064 - 32s/epoch - 5ms/step
Epoch 4/90
5969/5969 - 32s - loss: 0.0065 - val_loss: 0.0064 - 32s/epoch - 5ms/step
Epoch 5/90
5969/5969 - 32s - loss: 0.0065 - val_loss: 0.0064 - 32s/epoch - 5ms/step
Epoch 6/90
5969/5969 - 32s - loss: 0.0065 - val_loss: 0.0064 - 32s/epoch - 5ms/step
Epoch 7/90
5969/5969 - 32s - loss: 0.0064 - val_loss: 0.0063 - 32s/epoch - 5ms/step
Epoch 8/90
5969/5969 - 33s - loss: 0.0064 - val_loss: 0.0063 - 33s/epoch - 5ms/step
Epoch 9/90
5969/5969 - 33s - loss: 0.0064 - val_loss: 0.0064 - 33s/epoch - 5ms/step
Epoch 10/90
5969/5969 - 32s - loss: 0.0064 - val_loss: 0.0063 - 32s/epoch - 5ms/step
Epoch 11/90
5969/5969 - 32s - loss: 0.0064 - val_loss: 0.0064 - 32s/epoch - 5ms/step
Epoch 12/90
5969/5969 - 32s - loss: 0.0064 - val_loss: 0.0065 - 32s/epoch - 5ms/step
Epoch 13/90
5969/5969 - 32s - loss: 0.0064 - val_loss: 0.0065 - 32s/epoch - 5ms/step
Epoch 14/90
5969/5969 - 31s - loss: 0.0064 - val_loss: 0.0065 - 31s/epoch - 5ms/step
Epoch 15/90
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 16/90
5969/5969 - 31s - loss: 0.0064 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 17/90
5969/5969 - 31s - loss: 0.0064 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 18/90
5969/5969 - 31s - loss: 0.0064 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 19/90
5969/5969 - 31s - loss: 0.0064 - val_loss: 0.0073 - 31s/epoch - 5ms/step
Epoch 20/90
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0066 - 32s/epoch - 5ms/step
Epoch 21/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 22/90
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0073 - 32s/epoch - 5ms/step
Epoch 23/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0072 - 31s/epoch - 5ms/step
Epoch 24/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0073 - 31s/epoch - 5ms/step
Epoch 25/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0066 - 31s/epoch - 5ms/step
Epoch 26/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0075 - 31s/epoch - 5ms/step
Epoch 27/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 28/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 29/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0068 - 31s/epoch - 5ms/step
Epoch 30/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0071 - 31s/epoch - 5ms/step
Epoch 31/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0073 - 31s/epoch - 5ms/step
Epoch 32/90
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0073 - 32s/epoch - 5ms/step
Epoch 33/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 34/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 35/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 36/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0075 - 31s/epoch - 5ms/step
Epoch 37/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0083 - 30s/epoch - 5ms/step
Epoch 38/90
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0072 - 27s/epoch - 5ms/step
Epoch 39/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 40/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0072 - 31s/epoch - 5ms/step
Epoch 41/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0073 - 31s/epoch - 5ms/step
Epoch 42/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 43/90
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0068 - 32s/epoch - 5ms/step
Epoch 44/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0075 - 31s/epoch - 5ms/step
Epoch 45/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 46/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 47/90
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0073 - 32s/epoch - 5ms/step
Epoch 48/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 49/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0073 - 31s/epoch - 5ms/step
Epoch 50/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0082 - 30s/epoch - 5ms/step
Epoch 51/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0079 - 31s/epoch - 5ms/step
Epoch 52/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 53/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 54/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 55/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0077 - 31s/epoch - 5ms/step
Epoch 56/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0070 - 30s/epoch - 5ms/step
Epoch 57/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0070 - 30s/epoch - 5ms/step
Epoch 58/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0067 - 30s/epoch - 5ms/step
Epoch 59/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0071 - 30s/epoch - 5ms/step
Epoch 60/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0071 - 30s/epoch - 5ms/step
Epoch 61/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 62/90
5969/5969 - 28s - loss: 0.0063 - val_loss: 0.0069 - 28s/epoch - 5ms/step
Epoch 63/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0068 - 30s/epoch - 5ms/step
Epoch 64/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0076 - 30s/epoch - 5ms/step
Epoch 65/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0073 - 31s/epoch - 5ms/step
Epoch 66/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0072 - 31s/epoch - 5ms/step
Epoch 67/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 68/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0071 - 30s/epoch - 5ms/step
Epoch 69/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 70/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 71/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 72/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 73/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 74/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0071 - 30s/epoch - 5ms/step
Epoch 75/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0075 - 30s/epoch - 5ms/step
Epoch 76/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 77/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0070 - 30s/epoch - 5ms/step
Epoch 78/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 79/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 80/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0069 - 30s/epoch - 5ms/step
Epoch 81/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0069 - 31s/epoch - 5ms/step
Epoch 82/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 83/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 84/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0071 - 30s/epoch - 5ms/step
Epoch 85/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0070 - 30s/epoch - 5ms/step
Epoch 86/90
5969/5969 - 28s - loss: 0.0063 - val_loss: 0.0071 - 28s/epoch - 5ms/step
Epoch 87/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 88/90
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 89/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 90/90
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.007350422441959381
  1/332 [..............................] - ETA: 1:12 33/332 [=>............................] - ETA: 0s   66/332 [====>.........................] - ETA: 0s 97/332 [=======>......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s158/332 [=============>................] - ETA: 0s189/332 [================>.............] - ETA: 0s220/332 [==================>...........] - ETA: 0s252/332 [=====================>........] - ETA: 0s279/332 [========================>.....] - ETA: 0s309/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12502844677307764
cosine 0.09568255025116938
MAE: 0.050713006
RMSE: 0.13042319
r2: -0.10251778637424772
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 16, 90, 0.002, 0.2, 252, 0.006253218278288841, 0.007350422441959381, 0.12502844677307764, 0.09568255025116938, 0.050713006407022476, 0.1304231882095337, -0.10251778637424772, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 180 0.002 128 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 6s - loss: 0.0127 - val_loss: 0.0078 - 6s/epoch - 9ms/step
Epoch 2/180
747/747 - 4s - loss: 0.0074 - val_loss: 0.0076 - 4s/epoch - 6ms/step
Epoch 3/180
747/747 - 4s - loss: 0.0072 - val_loss: 0.0074 - 4s/epoch - 6ms/step
Epoch 4/180
747/747 - 4s - loss: 0.0071 - val_loss: 0.0072 - 4s/epoch - 6ms/step
Epoch 5/180
747/747 - 5s - loss: 0.0070 - val_loss: 0.0071 - 5s/epoch - 7ms/step
Epoch 6/180
747/747 - 5s - loss: 0.0069 - val_loss: 0.0069 - 5s/epoch - 6ms/step
Epoch 7/180
747/747 - 5s - loss: 0.0069 - val_loss: 0.0069 - 5s/epoch - 6ms/step
Epoch 8/180
747/747 - 5s - loss: 0.0068 - val_loss: 0.0068 - 5s/epoch - 6ms/step
Epoch 9/180
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 10/180
747/747 - 4s - loss: 0.0065 - val_loss: 0.0064 - 4s/epoch - 6ms/step
Epoch 11/180
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 12/180
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 13/180
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 14/180
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 15/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 16/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 17/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 18/180
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 19/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 20/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 21/180
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 22/180
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 23/180
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 24/180
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 7ms/step
Epoch 25/180
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 26/180
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 27/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 28/180
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 29/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 30/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 31/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 32/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 33/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 34/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 35/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 36/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 37/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 38/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 39/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 40/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 41/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 42/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 43/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 44/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 45/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 46/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 47/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 48/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 49/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 50/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 51/180
747/747 - 6s - loss: 0.0060 - val_loss: 0.0059 - 6s/epoch - 8ms/step
Epoch 52/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 53/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 54/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 55/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 56/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 57/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 58/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 59/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 60/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 61/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 62/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 63/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 64/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 65/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 66/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 67/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 68/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 69/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 70/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 71/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 7ms/step
Epoch 72/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 73/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 74/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 75/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 76/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 77/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 78/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 79/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 80/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 81/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 82/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 83/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 84/180
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 85/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 86/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 87/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 88/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 89/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 90/180
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 91/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 92/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 93/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 94/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 95/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 96/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 97/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 98/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 99/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 100/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 101/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 102/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 103/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 104/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 105/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 106/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 107/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 108/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 109/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 110/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 111/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 112/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 113/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 114/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 115/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 116/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 117/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 118/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 119/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 120/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 121/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 122/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 123/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 124/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 125/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 126/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 127/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 128/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 5ms/step
Epoch 129/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 130/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 131/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 132/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 133/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 134/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 7ms/step
Epoch 135/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 136/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 137/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 138/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 139/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 140/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 141/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 142/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 143/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0058 - 5s/epoch - 6ms/step
Epoch 144/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 145/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 146/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 147/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 148/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 149/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 150/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 151/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 152/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 153/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 154/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 155/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 156/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 157/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 158/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 159/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 160/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 161/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 162/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 163/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 164/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 165/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 166/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 167/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 168/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 169/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 170/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 171/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 172/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 173/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 174/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 175/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 176/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0058 - 5s/epoch - 6ms/step
Epoch 177/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 178/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
Epoch 179/180
747/747 - 5s - loss: 0.0059 - val_loss: 0.0058 - 5s/epoch - 6ms/step
Epoch 180/180
747/747 - 4s - loss: 0.0059 - val_loss: 0.0058 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0058372737839818
  1/332 [..............................] - ETA: 56s 25/332 [=>............................] - ETA: 0s  57/332 [====>.........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s121/332 [=========>....................] - ETA: 0s145/332 [============>.................] - ETA: 0s176/332 [==============>...............] - ETA: 0s210/332 [=================>............] - ETA: 0s244/332 [=====================>........] - ETA: 0s277/332 [========================>.....] - ETA: 0s310/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.10448680342906293
cosine 0.07979913600157038
MAE: 0.04383528
RMSE: 0.0947527
r2: 0.41809127235516236
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 128, 180, 0.002, 0.2, 252, 0.005884697660803795, 0.0058372737839818, 0.10448680342906293, 0.07979913600157038, 0.04383527860045433, 0.09475269913673401, 0.41809127235516236, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 90 0.002 256 2] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/90
374/374 - 5s - loss: 0.0164 - val_loss: 0.0083 - 5s/epoch - 13ms/step
Epoch 2/90
374/374 - 3s - loss: 0.0075 - val_loss: 0.0078 - 3s/epoch - 7ms/step
Epoch 3/90
374/374 - 3s - loss: 0.0073 - val_loss: 0.0076 - 3s/epoch - 8ms/step
Epoch 4/90
374/374 - 3s - loss: 0.0072 - val_loss: 0.0080 - 3s/epoch - 7ms/step
Epoch 5/90
374/374 - 2s - loss: 0.0071 - val_loss: 0.0073 - 2s/epoch - 7ms/step
Epoch 6/90
374/374 - 2s - loss: 0.0071 - val_loss: 0.0074 - 2s/epoch - 7ms/step
Epoch 7/90
374/374 - 2s - loss: 0.0070 - val_loss: 0.0074 - 2s/epoch - 7ms/step
Epoch 8/90
374/374 - 3s - loss: 0.0070 - val_loss: 0.0080 - 3s/epoch - 7ms/step
Epoch 9/90
374/374 - 3s - loss: 0.0070 - val_loss: 0.0070 - 3s/epoch - 7ms/step
Epoch 10/90
374/374 - 2s - loss: 0.0069 - val_loss: 0.0072 - 2s/epoch - 7ms/step
Epoch 11/90
374/374 - 3s - loss: 0.0069 - val_loss: 0.0070 - 3s/epoch - 7ms/step
Epoch 12/90
374/374 - 3s - loss: 0.0068 - val_loss: 0.0070 - 3s/epoch - 7ms/step
Epoch 13/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 14/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 15/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 16/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 17/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 18/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 19/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 20/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 21/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 22/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 23/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 24/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 25/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 26/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 27/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 28/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 29/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 30/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 31/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 32/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 33/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 34/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 35/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 36/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 37/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 8ms/step
Epoch 38/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 39/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 40/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 41/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 42/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 43/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 44/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 45/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 46/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 47/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 48/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 49/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 50/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 51/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 52/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 53/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 54/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 55/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 56/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 57/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 58/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 59/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 60/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 61/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 62/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 63/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 64/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 65/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 66/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 67/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 68/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 69/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 70/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 71/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 72/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 73/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 8ms/step
Epoch 74/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 75/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 76/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 77/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 78/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 79/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 80/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 81/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 82/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 83/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 84/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 85/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 86/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 87/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 88/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 89/90
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 90/90
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005904235877096653
  1/332 [..............................] - ETA: 1:04 29/332 [=>............................] - ETA: 0s   61/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s120/332 [=========>....................] - ETA: 0s153/332 [============>.................] - ETA: 0s183/332 [===============>..............] - ETA: 0s214/332 [==================>...........] - ETA: 0s245/332 [=====================>........] - ETA: 0s277/332 [========================>.....] - ETA: 0s310/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11012900801838185
cosine 0.08408532996268439
MAE: 0.045000594
RMSE: 0.09720522
r2: 0.38757814103770444
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 256, 90, 0.002, 0.2, 252, 0.005949666723608971, 0.005904235877096653, 0.11012900801838185, 0.08408532996268439, 0.0450005941092968, 0.09720522165298462, 0.38757814103770444, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 180 0.00030000000000000003 16 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/180
5969/5969 - 33s - loss: 0.0210 - val_loss: 0.0129 - 33s/epoch - 5ms/step
Epoch 2/180
5969/5969 - 33s - loss: 0.0128 - val_loss: 0.0119 - 33s/epoch - 5ms/step
Epoch 3/180
5969/5969 - 33s - loss: 0.0122 - val_loss: 0.0119 - 33s/epoch - 5ms/step
Epoch 4/180
5969/5969 - 32s - loss: 0.0120 - val_loss: 0.0128 - 32s/epoch - 5ms/step
Epoch 5/180
5969/5969 - 32s - loss: 0.0119 - val_loss: 0.0146 - 32s/epoch - 5ms/step
Epoch 6/180
5969/5969 - 32s - loss: 0.0117 - val_loss: 0.0146 - 32s/epoch - 5ms/step
Epoch 7/180
5969/5969 - 32s - loss: 0.0115 - val_loss: 0.0173 - 32s/epoch - 5ms/step
Epoch 8/180
5969/5969 - 30s - loss: 0.0114 - val_loss: 0.0184 - 30s/epoch - 5ms/step
Epoch 9/180
5969/5969 - 32s - loss: 0.0113 - val_loss: 0.0179 - 32s/epoch - 5ms/step
Epoch 10/180
5969/5969 - 32s - loss: 0.0112 - val_loss: 0.0215 - 32s/epoch - 5ms/step
Epoch 11/180
5969/5969 - 31s - loss: 0.0111 - val_loss: 0.0238 - 31s/epoch - 5ms/step
Epoch 12/180
5969/5969 - 32s - loss: 0.0111 - val_loss: 0.0198 - 32s/epoch - 5ms/step
Epoch 13/180
5969/5969 - 31s - loss: 0.0110 - val_loss: 0.0148 - 31s/epoch - 5ms/step
Epoch 14/180
5969/5969 - 30s - loss: 0.0110 - val_loss: 0.0181 - 30s/epoch - 5ms/step
Epoch 15/180
5969/5969 - 31s - loss: 0.0110 - val_loss: 0.0178 - 31s/epoch - 5ms/step
Epoch 16/180
5969/5969 - 32s - loss: 0.0110 - val_loss: 0.0158 - 32s/epoch - 5ms/step
Epoch 17/180
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0158 - 31s/epoch - 5ms/step
Epoch 18/180
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0164 - 31s/epoch - 5ms/step
Epoch 19/180
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0173 - 31s/epoch - 5ms/step
Epoch 20/180
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0170 - 30s/epoch - 5ms/step
Epoch 21/180
5969/5969 - 31s - loss: 0.0109 - val_loss: 0.0172 - 31s/epoch - 5ms/step
Epoch 22/180
5969/5969 - 32s - loss: 0.0108 - val_loss: 0.0184 - 32s/epoch - 5ms/step
Epoch 23/180
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0179 - 31s/epoch - 5ms/step
Epoch 24/180
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0203 - 31s/epoch - 5ms/step
Epoch 25/180
5969/5969 - 32s - loss: 0.0108 - val_loss: 0.0183 - 32s/epoch - 5ms/step
Epoch 26/180
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0202 - 31s/epoch - 5ms/step
Epoch 27/180
5969/5969 - 32s - loss: 0.0108 - val_loss: 0.0188 - 32s/epoch - 5ms/step
Epoch 28/180
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0161 - 31s/epoch - 5ms/step
Epoch 29/180
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0170 - 31s/epoch - 5ms/step
Epoch 30/180
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0236 - 30s/epoch - 5ms/step
Epoch 31/180
5969/5969 - 28s - loss: 0.0107 - val_loss: 0.0205 - 28s/epoch - 5ms/step
Epoch 32/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0191 - 31s/epoch - 5ms/step
Epoch 33/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0199 - 31s/epoch - 5ms/step
Epoch 34/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0176 - 31s/epoch - 5ms/step
Epoch 35/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0236 - 31s/epoch - 5ms/step
Epoch 36/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0192 - 31s/epoch - 5ms/step
Epoch 37/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0166 - 31s/epoch - 5ms/step
Epoch 38/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0226 - 31s/epoch - 5ms/step
Epoch 39/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0186 - 31s/epoch - 5ms/step
Epoch 40/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0140 - 31s/epoch - 5ms/step
Epoch 41/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0188 - 31s/epoch - 5ms/step
Epoch 42/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0184 - 31s/epoch - 5ms/step
Epoch 43/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0167 - 31s/epoch - 5ms/step
Epoch 44/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0163 - 31s/epoch - 5ms/step
Epoch 45/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0159 - 31s/epoch - 5ms/step
Epoch 46/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0192 - 31s/epoch - 5ms/step
Epoch 47/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0176 - 31s/epoch - 5ms/step
Epoch 48/180
5969/5969 - 32s - loss: 0.0107 - val_loss: 0.0163 - 32s/epoch - 5ms/step
Epoch 49/180
5969/5969 - 31s - loss: 0.0106 - val_loss: 0.0191 - 31s/epoch - 5ms/step
Epoch 50/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0196 - 31s/epoch - 5ms/step
Epoch 51/180
5969/5969 - 31s - loss: 0.0107 - val_loss: 0.0211 - 31s/epoch - 5ms/step
Epoch 52/180
5969/5969 - 31s - loss: 0.0106 - val_loss: 0.0174 - 31s/epoch - 5ms/step
Epoch 53/180
5969/5969 - 31s - loss: 0.0106 - val_loss: 0.0181 - 31s/epoch - 5ms/step
Epoch 54/180
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.0152 - 29s/epoch - 5ms/step
Epoch 55/180
5969/5969 - 31s - loss: 0.0106 - val_loss: 0.0198 - 31s/epoch - 5ms/step
Epoch 56/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0202 - 33s/epoch - 6ms/step
Epoch 57/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0214 - 33s/epoch - 6ms/step
Epoch 58/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0220 - 33s/epoch - 6ms/step
Epoch 59/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0220 - 33s/epoch - 6ms/step
Epoch 60/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0220 - 33s/epoch - 6ms/step
Epoch 61/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0193 - 33s/epoch - 6ms/step
Epoch 62/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0183 - 33s/epoch - 6ms/step
Epoch 63/180
5969/5969 - 32s - loss: 0.0106 - val_loss: 0.0187 - 32s/epoch - 5ms/step
Epoch 64/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0214 - 33s/epoch - 5ms/step
Epoch 65/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0175 - 33s/epoch - 5ms/step
Epoch 66/180
5969/5969 - 32s - loss: 0.0106 - val_loss: 0.0225 - 32s/epoch - 5ms/step
Epoch 67/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0157 - 33s/epoch - 6ms/step
Epoch 68/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0163 - 33s/epoch - 6ms/step
Epoch 69/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0180 - 33s/epoch - 5ms/step
Epoch 70/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0173 - 33s/epoch - 6ms/step
Epoch 71/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0198 - 33s/epoch - 6ms/step
Epoch 72/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0177 - 33s/epoch - 5ms/step
Epoch 73/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0184 - 33s/epoch - 6ms/step
Epoch 74/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0174 - 33s/epoch - 5ms/step
Epoch 75/180
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.0197 - 29s/epoch - 5ms/step
Epoch 76/180
5969/5969 - 30s - loss: 0.0106 - val_loss: 0.0191 - 30s/epoch - 5ms/step
Epoch 77/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0167 - 33s/epoch - 6ms/step
Epoch 78/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0160 - 33s/epoch - 6ms/step
Epoch 79/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0175 - 33s/epoch - 6ms/step
Epoch 80/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0200 - 33s/epoch - 5ms/step
Epoch 81/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0230 - 33s/epoch - 5ms/step
Epoch 82/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0154 - 33s/epoch - 6ms/step
Epoch 83/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0180 - 33s/epoch - 6ms/step
Epoch 84/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0223 - 33s/epoch - 5ms/step
Epoch 85/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0186 - 33s/epoch - 5ms/step
Epoch 86/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0255 - 33s/epoch - 6ms/step
Epoch 87/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0148 - 33s/epoch - 6ms/step
Epoch 88/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0184 - 33s/epoch - 6ms/step
Epoch 89/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0193 - 33s/epoch - 5ms/step
Epoch 90/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0196 - 33s/epoch - 6ms/step
Epoch 91/180
5969/5969 - 34s - loss: 0.0106 - val_loss: 0.0186 - 34s/epoch - 6ms/step
Epoch 92/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0212 - 33s/epoch - 6ms/step
Epoch 93/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0231 - 33s/epoch - 6ms/step
Epoch 94/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0244 - 33s/epoch - 6ms/step
Epoch 95/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0177 - 33s/epoch - 5ms/step
Epoch 96/180
5969/5969 - 28s - loss: 0.0105 - val_loss: 0.0173 - 28s/epoch - 5ms/step
Epoch 97/180
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.0200 - 29s/epoch - 5ms/step
Epoch 98/180
5969/5969 - 33s - loss: 0.0106 - val_loss: 0.0189 - 33s/epoch - 5ms/step
Epoch 99/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0225 - 33s/epoch - 6ms/step
Epoch 100/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0182 - 33s/epoch - 6ms/step
Epoch 101/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0210 - 33s/epoch - 5ms/step
Epoch 102/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0218 - 33s/epoch - 5ms/step
Epoch 103/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0196 - 33s/epoch - 6ms/step
Epoch 104/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0212 - 33s/epoch - 6ms/step
Epoch 105/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0187 - 33s/epoch - 5ms/step
Epoch 106/180
5969/5969 - 32s - loss: 0.0105 - val_loss: 0.0201 - 32s/epoch - 5ms/step
Epoch 107/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0191 - 33s/epoch - 5ms/step
Epoch 108/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0194 - 33s/epoch - 5ms/step
Epoch 109/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0187 - 33s/epoch - 6ms/step
Epoch 110/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0156 - 33s/epoch - 6ms/step
Epoch 111/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0165 - 33s/epoch - 6ms/step
Epoch 112/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0171 - 33s/epoch - 6ms/step
Epoch 113/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0168 - 33s/epoch - 6ms/step
Epoch 114/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0177 - 33s/epoch - 6ms/step
Epoch 115/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0169 - 33s/epoch - 6ms/step
Epoch 116/180
5969/5969 - 32s - loss: 0.0105 - val_loss: 0.0188 - 32s/epoch - 5ms/step
Epoch 117/180
5969/5969 - 29s - loss: 0.0105 - val_loss: 0.0177 - 29s/epoch - 5ms/step
Epoch 118/180
5969/5969 - 27s - loss: 0.0105 - val_loss: 0.0182 - 27s/epoch - 5ms/step
Epoch 119/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0180 - 33s/epoch - 6ms/step
Epoch 120/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0204 - 33s/epoch - 6ms/step
Epoch 121/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0196 - 33s/epoch - 5ms/step
Epoch 122/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0171 - 33s/epoch - 6ms/step
Epoch 123/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0182 - 33s/epoch - 6ms/step
Epoch 124/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0197 - 33s/epoch - 6ms/step
Epoch 125/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0197 - 35s/epoch - 6ms/step
Epoch 126/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0239 - 36s/epoch - 6ms/step
Epoch 127/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0197 - 36s/epoch - 6ms/step
Epoch 128/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0186 - 35s/epoch - 6ms/step
Epoch 129/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0177 - 36s/epoch - 6ms/step
Epoch 130/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0224 - 35s/epoch - 6ms/step
Epoch 131/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0225 - 36s/epoch - 6ms/step
Epoch 132/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0188 - 36s/epoch - 6ms/step
Epoch 133/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0216 - 33s/epoch - 5ms/step
Epoch 134/180
5969/5969 - 31s - loss: 0.0105 - val_loss: 0.0229 - 31s/epoch - 5ms/step
Epoch 135/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0196 - 34s/epoch - 6ms/step
Epoch 136/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0196 - 34s/epoch - 6ms/step
Epoch 137/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0218 - 33s/epoch - 6ms/step
Epoch 138/180
5969/5969 - 30s - loss: 0.0105 - val_loss: 0.0225 - 30s/epoch - 5ms/step
Epoch 139/180
5969/5969 - 32s - loss: 0.0105 - val_loss: 0.0252 - 32s/epoch - 5ms/step
Epoch 140/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0231 - 34s/epoch - 6ms/step
Epoch 141/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0173 - 35s/epoch - 6ms/step
Epoch 142/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0238 - 35s/epoch - 6ms/step
Epoch 143/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0183 - 34s/epoch - 6ms/step
Epoch 144/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0183 - 34s/epoch - 6ms/step
Epoch 145/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0192 - 34s/epoch - 6ms/step
Epoch 146/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0232 - 34s/epoch - 6ms/step
Epoch 147/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0205 - 35s/epoch - 6ms/step
Epoch 148/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0254 - 35s/epoch - 6ms/step
Epoch 149/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0200 - 35s/epoch - 6ms/step
Epoch 150/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0211 - 34s/epoch - 6ms/step
Epoch 151/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0281 - 34s/epoch - 6ms/step
Epoch 152/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0223 - 35s/epoch - 6ms/step
Epoch 153/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0211 - 35s/epoch - 6ms/step
Epoch 154/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0213 - 36s/epoch - 6ms/step
Epoch 155/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0211 - 36s/epoch - 6ms/step
Epoch 156/180
5969/5969 - 37s - loss: 0.0105 - val_loss: 0.0183 - 37s/epoch - 6ms/step
Epoch 157/180
5969/5969 - 35s - loss: 0.0105 - val_loss: 0.0191 - 35s/epoch - 6ms/step
Epoch 158/180
5969/5969 - 32s - loss: 0.0105 - val_loss: 0.0191 - 32s/epoch - 5ms/step
Epoch 159/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0175 - 36s/epoch - 6ms/step
Epoch 160/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0251 - 36s/epoch - 6ms/step
Epoch 161/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0220 - 36s/epoch - 6ms/step
Epoch 162/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0203 - 36s/epoch - 6ms/step
Epoch 163/180
5969/5969 - 36s - loss: 0.0105 - val_loss: 0.0201 - 36s/epoch - 6ms/step
Epoch 164/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0242 - 33s/epoch - 6ms/step
Epoch 165/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0181 - 33s/epoch - 5ms/step
Epoch 166/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0199 - 33s/epoch - 6ms/step
Epoch 167/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0204 - 33s/epoch - 6ms/step
Epoch 168/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0184 - 33s/epoch - 6ms/step
Epoch 169/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0184 - 33s/epoch - 6ms/step
Epoch 170/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0241 - 33s/epoch - 6ms/step
Epoch 171/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0232 - 33s/epoch - 6ms/step
Epoch 172/180
5969/5969 - 34s - loss: 0.0105 - val_loss: 0.0210 - 34s/epoch - 6ms/step
Epoch 173/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0222 - 33s/epoch - 6ms/step
Epoch 174/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0180 - 33s/epoch - 6ms/step
Epoch 175/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0215 - 33s/epoch - 6ms/step
Epoch 176/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0200 - 33s/epoch - 6ms/step
Epoch 177/180
5969/5969 - 32s - loss: 0.0105 - val_loss: 0.0263 - 32s/epoch - 5ms/step
Epoch 178/180
5969/5969 - 28s - loss: 0.0105 - val_loss: 0.0234 - 28s/epoch - 5ms/step
Epoch 179/180
5969/5969 - 32s - loss: 0.0105 - val_loss: 0.0215 - 32s/epoch - 5ms/step
Epoch 180/180
5969/5969 - 33s - loss: 0.0105 - val_loss: 0.0209 - 33s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.020898938179016113
  1/332 [..............................] - ETA: 1:12 31/332 [=>............................] - ETA: 0s   61/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s124/332 [==========>...................] - ETA: 0s156/332 [=============>................] - ETA: 0s186/332 [===============>..............] - ETA: 0s217/332 [==================>...........] - ETA: 0s250/332 [=====================>........] - ETA: 0s282/332 [========================>.....] - ETA: 0s314/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0875469169109397
cosine 0.06777771054305448
MAE: 0.04360155
RMSE: 0.13323814
r2: -0.1506231833769875
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 16, 180, 0.00030000000000000003, 0.2, 252, 0.010451999492943287, 0.020898938179016113, 0.0875469169109397, 0.06777771054305448, 0.04360155016183853, 0.13323813676834106, -0.1506231833769875, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 35 0.0008 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/35
1493/1493 - 10s - loss: 0.0137 - val_loss: 0.0102 - 10s/epoch - 7ms/step
Epoch 2/35
1493/1493 - 9s - loss: 0.0082 - val_loss: 0.0077 - 9s/epoch - 6ms/step
Epoch 3/35
1493/1493 - 9s - loss: 0.0071 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 4/35
1493/1493 - 8s - loss: 0.0066 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 5/35
1493/1493 - 9s - loss: 0.0064 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 6/35
1493/1493 - 9s - loss: 0.0064 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 7/35
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 8/35
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 9/35
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 10/35
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 11/35
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 12/35
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 13/35
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 14/35
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 15/35
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 16/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 17/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 18/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 19/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 20/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 21/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 22/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 23/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 24/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 25/35
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 26/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 27/35
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 28/35
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 29/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 30/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 31/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 32/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 33/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 34/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 35/35
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.006024662870913744
  1/332 [..............................] - ETA: 1:08 29/332 [=>............................] - ETA: 0s   61/332 [====>.........................] - ETA: 0s 91/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s155/332 [=============>................] - ETA: 0s185/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s248/332 [=====================>........] - ETA: 0s280/332 [========================>.....] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11787630863350111
cosine 0.0900907297820538
MAE: 0.047294218
RMSE: 0.100391164
r2: 0.34677538957898335
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'logcosh', 64, 35, 0.0008, 0.2, 252, 0.006100210826843977, 0.006024662870913744, 0.11787630863350111, 0.0900907297820538, 0.047294218093156815, 0.10039116442203522, 0.34677538957898335, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 90 0.0007 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/90
374/374 - 4s - loss: 0.0175 - val_loss: 0.0088 - 4s/epoch - 12ms/step
Epoch 2/90
374/374 - 3s - loss: 0.0079 - val_loss: 0.0189 - 3s/epoch - 8ms/step
Epoch 3/90
374/374 - 3s - loss: 0.0077 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 4/90
374/374 - 2s - loss: 0.0075 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 5/90
374/374 - 2s - loss: 0.0074 - val_loss: 0.0077 - 2s/epoch - 6ms/step
Epoch 6/90
374/374 - 3s - loss: 0.0073 - val_loss: 0.0092 - 3s/epoch - 7ms/step
Epoch 7/90
374/374 - 3s - loss: 0.0072 - val_loss: 0.0081 - 3s/epoch - 7ms/step
Epoch 8/90
374/374 - 2s - loss: 0.0072 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 9/90
374/374 - 3s - loss: 0.0071 - val_loss: 0.0077 - 3s/epoch - 7ms/step
Epoch 10/90
374/374 - 3s - loss: 0.0070 - val_loss: 0.0073 - 3s/epoch - 7ms/step
Epoch 11/90
374/374 - 3s - loss: 0.0070 - val_loss: 0.0078 - 3s/epoch - 7ms/step
Epoch 12/90
374/374 - 3s - loss: 0.0069 - val_loss: 0.0070 - 3s/epoch - 7ms/step
Epoch 13/90
374/374 - 2s - loss: 0.0069 - val_loss: 0.0069 - 2s/epoch - 7ms/step
Epoch 14/90
374/374 - 2s - loss: 0.0068 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 15/90
374/374 - 3s - loss: 0.0068 - val_loss: 0.0068 - 3s/epoch - 7ms/step
Epoch 16/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 17/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 18/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 19/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 20/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 21/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0066 - 2s/epoch - 7ms/step
Epoch 22/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0075 - 3s/epoch - 7ms/step
Epoch 23/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 24/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 25/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 26/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 27/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 28/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 29/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 30/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 31/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 32/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 33/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 34/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 35/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 36/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 37/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 38/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 39/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 40/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 41/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 42/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 43/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 44/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 45/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 46/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 47/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 48/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 49/90
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 50/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 51/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 52/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 53/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 54/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 55/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 56/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 57/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 58/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 59/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 60/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 61/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 62/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 63/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 64/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 65/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 66/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 67/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 68/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 69/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 70/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 71/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 72/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 73/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 74/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 75/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 76/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 77/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 78/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 6ms/step
Epoch 79/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 80/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 81/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 82/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 83/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 84/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 85/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 86/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
Epoch 87/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 88/90
374/374 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 89/90
374/374 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 7ms/step
Epoch 90/90
374/374 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0059060403145849705
  1/332 [..............................] - ETA: 1:03 33/332 [=>............................] - ETA: 0s   66/332 [====>.........................] - ETA: 0s101/332 [========>.....................] - ETA: 0s135/332 [===========>..................] - ETA: 0s169/332 [==============>...............] - ETA: 0s203/332 [=================>............] - ETA: 0s238/332 [====================>.........] - ETA: 0s273/332 [=======================>......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.11174968667145814
cosine 0.08534306051301543
MAE: 0.045475047
RMSE: 0.09785389
r2: 0.37937731979064127
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 256, 90, 0.0007, 0.2, 252, 0.0059532467275857925, 0.0059060403145849705, 0.11174968667145814, 0.08534306051301543, 0.04547504708170891, 0.0978538915514946, 0.37937731979064127, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 210 0.0005 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/210
747/747 - 5s - loss: 0.0263 - val_loss: 0.0163 - 5s/epoch - 7ms/step
Epoch 2/210
747/747 - 4s - loss: 0.0145 - val_loss: 0.0145 - 4s/epoch - 5ms/step
Epoch 3/210
747/747 - 4s - loss: 0.0139 - val_loss: 0.0150 - 4s/epoch - 5ms/step
Epoch 4/210
747/747 - 4s - loss: 0.0134 - val_loss: 0.0134 - 4s/epoch - 5ms/step
Epoch 5/210
747/747 - 4s - loss: 0.0130 - val_loss: 0.0134 - 4s/epoch - 5ms/step
Epoch 6/210
747/747 - 4s - loss: 0.0127 - val_loss: 0.0127 - 4s/epoch - 6ms/step
Epoch 7/210
747/747 - 5s - loss: 0.0124 - val_loss: 0.0121 - 5s/epoch - 6ms/step
Epoch 8/210
747/747 - 4s - loss: 0.0120 - val_loss: 0.0117 - 4s/epoch - 6ms/step
Epoch 9/210
747/747 - 5s - loss: 0.0116 - val_loss: 0.0117 - 5s/epoch - 6ms/step
Epoch 10/210
747/747 - 4s - loss: 0.0114 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 11/210
747/747 - 5s - loss: 0.0111 - val_loss: 0.0107 - 5s/epoch - 7ms/step
Epoch 12/210
747/747 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 7ms/step
Epoch 13/210
747/747 - 5s - loss: 0.0106 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 14/210
747/747 - 5s - loss: 0.0106 - val_loss: 0.0108 - 5s/epoch - 7ms/step
Epoch 15/210
747/747 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 16/210
747/747 - 5s - loss: 0.0105 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 17/210
747/747 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 18/210
747/747 - 5s - loss: 0.0102 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 19/210
747/747 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 20/210
747/747 - 5s - loss: 0.0103 - val_loss: 0.0107 - 5s/epoch - 7ms/step
Epoch 21/210
747/747 - 5s - loss: 0.0108 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 22/210
747/747 - 5s - loss: 0.0102 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 23/210
747/747 - 5s - loss: 0.0104 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 24/210
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 25/210
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 6ms/step
Epoch 26/210
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 27/210
747/747 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 28/210
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 29/210
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 30/210
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 31/210
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 32/210
747/747 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 33/210
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 34/210
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 35/210
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 36/210
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 37/210
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 38/210
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 39/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 40/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 41/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 42/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 43/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 44/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 45/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 46/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 47/210
747/747 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 48/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 49/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 50/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0095 - 5s/epoch - 6ms/step
Epoch 51/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 52/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 53/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 54/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 55/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 56/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 57/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 58/210
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 59/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 60/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 61/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 62/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 63/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 64/210
747/747 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 65/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 66/210
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 67/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 68/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 69/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 70/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 71/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 72/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 73/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0094 - 5s/epoch - 6ms/step
Epoch 74/210
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 75/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 76/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 77/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 78/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 79/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 80/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 7ms/step
Epoch 81/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 82/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 83/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 84/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 85/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 86/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 87/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 88/210
747/747 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 7ms/step
Epoch 89/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 90/210
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 91/210
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 92/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 93/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 94/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 95/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 96/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 97/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 98/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 99/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 100/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 101/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 102/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 103/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 104/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 105/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 106/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0093 - 5s/epoch - 6ms/step
Epoch 107/210
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 108/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 109/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 110/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 111/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 112/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 113/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 114/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 115/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 7ms/step
Epoch 116/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 117/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 118/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 119/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 120/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 121/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 122/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 123/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 124/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 125/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 126/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 127/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 128/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 129/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 130/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 131/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 132/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 133/210
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 134/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 135/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 7ms/step
Epoch 136/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 137/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 138/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 139/210
747/747 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 7ms/step
Epoch 140/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 141/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 142/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 143/210
747/747 - 4s - loss: 0.0094 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 144/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 145/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 146/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 147/210
747/747 - 5s - loss: 0.0093 - val_loss: 0.0092 - 5s/epoch - 6ms/step
Epoch 148/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 149/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 6ms/step
Epoch 150/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 151/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 152/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 153/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 154/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 155/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 156/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 157/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 158/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 159/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 160/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 161/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 162/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 163/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 164/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 165/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 166/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 167/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 168/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 169/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 170/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 171/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 172/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 173/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 174/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 175/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 176/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 177/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 178/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 179/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 180/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 181/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 182/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 183/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 184/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 185/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 186/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 187/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 188/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 189/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 190/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 191/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 192/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 193/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 194/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 195/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 196/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0092 - 4s/epoch - 5ms/step
Epoch 197/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 198/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 199/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 200/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 201/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 202/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 203/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 204/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 205/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 206/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 207/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 208/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 209/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 210/210
747/747 - 4s - loss: 0.0093 - val_loss: 0.0091 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009124647825956345
  1/332 [..............................] - ETA: 49s 35/332 [==>...........................] - ETA: 0s  71/332 [=====>........................] - ETA: 0s107/332 [========>.....................] - ETA: 0s142/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s214/332 [==================>...........] - ETA: 0s250/332 [=====================>........] - ETA: 0s287/332 [========================>.....] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06478025137119998
cosine 0.04965239511069108
MAE: 0.03435587
RMSE: 0.07532492
r2: 0.6322531461990103
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'mse', 128, 210, 0.0005, 0.2, 252, 0.00927793886512518, 0.009124647825956345, 0.06478025137119998, 0.04965239511069108, 0.03435587137937546, 0.07532492280006409, 0.6322531461990103, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 30 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/30
5969/5969 - 28s - loss: 0.0209 - val_loss: 0.0130 - 28s/epoch - 5ms/step
Epoch 2/30
5969/5969 - 27s - loss: 0.0131 - val_loss: 0.0126 - 27s/epoch - 5ms/step
Epoch 3/30
5969/5969 - 26s - loss: 0.0125 - val_loss: 0.0120 - 26s/epoch - 4ms/step
Epoch 4/30
5969/5969 - 27s - loss: 0.0120 - val_loss: 0.0145 - 27s/epoch - 5ms/step
Epoch 5/30
5969/5969 - 27s - loss: 0.0118 - val_loss: 0.0156 - 27s/epoch - 5ms/step
Epoch 6/30
5969/5969 - 27s - loss: 0.0117 - val_loss: 0.0146 - 27s/epoch - 4ms/step
Epoch 7/30
5969/5969 - 27s - loss: 0.0116 - val_loss: 0.0160 - 27s/epoch - 5ms/step
Epoch 8/30
5969/5969 - 27s - loss: 0.0115 - val_loss: 0.0147 - 27s/epoch - 5ms/step
Epoch 9/30
5969/5969 - 27s - loss: 0.0114 - val_loss: 0.0188 - 27s/epoch - 5ms/step
Epoch 10/30
5969/5969 - 27s - loss: 0.0113 - val_loss: 0.0229 - 27s/epoch - 5ms/step
Epoch 11/30
5969/5969 - 27s - loss: 0.0112 - val_loss: 0.0209 - 27s/epoch - 5ms/step
Epoch 12/30
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.0333 - 27s/epoch - 5ms/step
Epoch 13/30
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.0215 - 27s/epoch - 4ms/step
Epoch 14/30
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.0266 - 27s/epoch - 5ms/step
Epoch 15/30
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0243 - 27s/epoch - 5ms/step
Epoch 16/30
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0219 - 27s/epoch - 4ms/step
Epoch 17/30
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0223 - 27s/epoch - 5ms/step
Epoch 18/30
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0268 - 27s/epoch - 5ms/step
Epoch 19/30
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0315 - 26s/epoch - 4ms/step
Epoch 20/30
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0288 - 27s/epoch - 5ms/step
Epoch 21/30
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0270 - 27s/epoch - 5ms/step
Epoch 22/30
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0369 - 27s/epoch - 4ms/step
Epoch 23/30
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0430 - 27s/epoch - 5ms/step
Epoch 24/30
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0333 - 27s/epoch - 5ms/step
Epoch 25/30
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0351 - 27s/epoch - 5ms/step
Epoch 26/30
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0417 - 26s/epoch - 4ms/step
Epoch 27/30
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0316 - 27s/epoch - 5ms/step
Epoch 28/30
5969/5969 - 28s - loss: 0.0108 - val_loss: 0.0769 - 28s/epoch - 5ms/step
Epoch 29/30
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0316 - 27s/epoch - 5ms/step
Epoch 30/30
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0355 - 27s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.035511214286088943
  1/332 [..............................] - ETA: 49s 36/332 [==>...........................] - ETA: 0s  71/332 [=====>........................] - ETA: 0s106/332 [========>.....................] - ETA: 0s141/332 [===========>..................] - ETA: 0s177/332 [==============>...............] - ETA: 0s213/332 [==================>...........] - ETA: 0s248/332 [=====================>........] - ETA: 0s283/332 [========================>.....] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.09507137697640788
cosine 0.07360008711404635
MAE: 0.045901723
RMSE: 0.17491557
r2: -0.98305318173689
RMSE zero-vector: 0.2430644284356365
['1.9custom_VAE', 'mse', 16, 30, 0.001, 0.2, 252, 0.010789240710437298, 0.035511214286088943, 0.09507137697640788, 0.07360008711404635, 0.04590172320604324, 0.17491556704044342, -0.98305318173689, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 30 0.0012000000000000001 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/30
1493/1493 - 9s - loss: 0.0135 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 2/30
1493/1493 - 7s - loss: 0.0082 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 3/30
1493/1493 - 7s - loss: 0.0072 - val_loss: 0.0175 - 7s/epoch - 5ms/step
Epoch 4/30
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0064 - 7s/epoch - 5ms/step
Epoch 5/30
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 6/30
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 7/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 8/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 9/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 10/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 11/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 12/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 13/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 14/30
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 15/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 16/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 17/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 18/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 19/30
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 20/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 21/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 22/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 23/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 24/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 25/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 26/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 27/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 28/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 29/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 30/30
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.006128624081611633
  1/332 [..............................] - ETA: 52s 35/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s140/332 [===========>..................] - ETA: 0s175/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s246/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12287206352992729
cosine 0.09387493353158718
MAE: 0.048476756
RMSE: 0.10260781
r2: 0.3176106981731659
RMSE zero-vector: 0.2430644284356365
['1.9custom_VAE', 'logcosh', 64, 30, 0.0012000000000000001, 0.2, 252, 0.006184891797602177, 0.006128624081611633, 0.12287206352992729, 0.09387493353158718, 0.04847675561904907, 0.10260780900716782, 0.3176106981731659, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.9 85 0.0005 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1137)         1438305     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1137)        4548        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1137)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1795405     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,811,810
Trainable params: 3,806,758
Non-trainable params: 5,052
__________________________________________________________________________________________________
Epoch 1/85
1493/1493 - 9s - loss: 0.0117 - val_loss: 0.0083 - 9s/epoch - 6ms/step
Epoch 2/85
1493/1493 - 7s - loss: 0.0076 - val_loss: 0.0074 - 7s/epoch - 5ms/step
Epoch 3/85
1493/1493 - 7s - loss: 0.0071 - val_loss: 0.0073 - 7s/epoch - 5ms/step
Epoch 4/85
1493/1493 - 7s - loss: 0.0069 - val_loss: 0.0068 - 7s/epoch - 5ms/step
Epoch 5/85
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 6/85
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0066 - 7s/epoch - 5ms/step
Epoch 7/85
1493/1493 - 7s - loss: 0.0065 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 8/85
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 9/85
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 10/85
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 11/85
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 12/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 13/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 14/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 15/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 16/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 17/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 18/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 19/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 20/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 21/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 22/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 23/85
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 24/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 25/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 26/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 27/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 28/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 29/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 30/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 31/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 32/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 33/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 34/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 35/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 36/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 37/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 38/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 39/85
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 5ms/step
Epoch 40/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 41/85
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 42/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 43/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 44/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 45/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 46/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 47/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 48/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 49/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 50/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 51/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 52/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 53/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 54/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 55/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 56/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 57/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 58/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 59/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 60/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 61/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 62/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 63/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 64/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 65/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 66/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 67/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 68/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 69/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 70/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 71/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 72/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 73/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 74/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 75/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 76/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 77/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 78/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 79/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 80/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 81/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 82/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 83/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 84/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 85/85
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.0059518152847886086
  1/332 [..............................] - ETA: 53s 35/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s138/332 [===========>..................] - ETA: 0s173/332 [==============>...............] - ETA: 0s209/332 [=================>............] - ETA: 0s244/332 [=====================>........] - ETA: 0s279/332 [========================>.....] - ETA: 0s316/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.10997813767895714
cosine 0.08403976779477443
MAE: 0.045448918
RMSE: 0.09722956
r2: 0.38727131698652684
RMSE zero-vector: 0.2430644284356365
['0.9custom_VAE', 'logcosh', 64, 85, 0.0005, 0.2, 252, 0.005982078146189451, 0.0059518152847886086, 0.10997813767895714, 0.08403976779477443, 0.045448917895555496, 0.09722956269979477, 0.38727131698652684, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 180 0.0007 128 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:01:17.919856: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:17.919949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24487191464
MaxInUse:                  24487191464
NumAllocs:                   647048595
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:17.920013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:17.920021: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 79
2023-02-23 03:01:17.920025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:17.920030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 381
2023-02-23 03:01:17.920034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:17.920038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:17.920042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 22
2023-02-23 03:01:17.920046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 187
2023-02-23 03:01:17.920050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:17.920054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 132
2023-02-23 03:01:17.920058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:17.920062: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 67
2023-02-23 03:01:17.920066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:17.920070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 9
2023-02-23 03:01:17.920074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 48
2023-02-23 03:01:17.920078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:17.920083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 54
2023-02-23 03:01:17.920087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:17.920091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:17.920095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-23 03:01:17.920099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 32
2023-02-23 03:01:17.920103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:17.920107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 36
2023-02-23 03:01:17.920111: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:17.920115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:17.920119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 45
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 180 0.0007 128 1]) is not valid.
[2.0 180 0.001 16 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-23 03:01:20.995653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:20.995735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24037913464
MaxInUse:                  24487191464
NumAllocs:                   647048651
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:20.995803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:20.995811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 79
2023-02-23 03:01:20.995816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:20.995821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 388
2023-02-23 03:01:20.995825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:20.995830: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:20.995834: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 22
2023-02-23 03:01:20.995838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 188
2023-02-23 03:01:20.995842: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:20.995846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:20.995850: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:20.995854: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 68
2023-02-23 03:01:20.995859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:20.995863: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 9
2023-02-23 03:01:20.995867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 48
2023-02-23 03:01:20.995871: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:20.995875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:20.995879: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:20.995883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:20.995887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-23 03:01:20.995891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 32
2023-02-23 03:01:20.995895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:20.995899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:20.995904: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:20.995908: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:20.995912: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 16, 180, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 180 0.001 16 1]) is not valid.
[2.0 85 0.0005 64 2] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-23 03:01:24.087075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:24.087157: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24037913464
MaxInUse:                  24487191464
NumAllocs:                   647048707
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:24.087210: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:24.087218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 79
2023-02-23 03:01:24.087223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:24.087227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 388
2023-02-23 03:01:24.087231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:24.087235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:24.087249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 22
2023-02-23 03:01:24.087253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 188
2023-02-23 03:01:24.087258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:24.087262: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:24.087266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:24.087270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 68
2023-02-23 03:01:24.087274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:24.087278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 9
2023-02-23 03:01:24.087282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 48
2023-02-23 03:01:24.087286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:24.087290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:24.087295: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:24.087305: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:24.087309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-23 03:01:24.087313: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 32
2023-02-23 03:01:24.087317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:24.087321: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:24.087325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:24.087329: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:24.087333: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'logcosh', 64, 85, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 85 0.0005 64 2]) is not valid.
[1.1 90 0.0005 128 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1390)         1758350     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1390)        5560        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1390)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          350532      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          350532      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2180218     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,645,192
Trainable params: 4,639,128
Non-trainable params: 6,064
__________________________________________________________________________________________________
2023-02-23 03:01:27.260592: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:27.260682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24056494248
MaxInUse:                  24487191464
NumAllocs:                   647048767
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:27.260742: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:27.260750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:27.260755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:27.260759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 395
2023-02-23 03:01:27.260763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:27.260767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:27.260772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 22
2023-02-23 03:01:27.260776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 189
2023-02-23 03:01:27.260780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 10
2023-02-23 03:01:27.260784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:27.260788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:27.260792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:27.260796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 69
2023-02-23 03:01:27.260810: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:27.260815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 9
2023-02-23 03:01:27.260819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 48
2023-02-23 03:01:27.260823: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1401120, 3
2023-02-23 03:01:27.260827: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:27.260831: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:27.260835: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:27.260839: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:27.260843: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-23 03:01:27.260847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 32
2023-02-23 03:01:27.260852: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 2
2023-02-23 03:01:27.260856: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:27.260860: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:27.260864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:27.260868: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:27.260872: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1custom_VAE', 'logcosh', 128, 90, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1 90 0.0005 128 2]) is not valid.
[1.0 175 0.0005 128 1] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:01:30.382585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:30.382664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24054834072
MaxInUse:                  24487191464
NumAllocs:                   647048823
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:30.382716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:30.382722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:30.382727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:30.382732: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 395
2023-02-23 03:01:30.382736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:30.382740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:30.382744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 22
2023-02-23 03:01:30.382748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 199
2023-02-23 03:01:30.382753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:30.382757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:30.382761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:30.382765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 69
2023-02-23 03:01:30.382769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:30.382773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 9
2023-02-23 03:01:30.382777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:01:30.382781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:30.382785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:30.382799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:30.382804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:30.382808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-23 03:01:30.382812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:01:30.382816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:30.382820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:30.382825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:30.382829: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:30.382833: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 175 0.0005 128 1]) is not valid.
[2.0 180 0.001 16 1] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
2023-02-23 03:01:33.444239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:33.444322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24088409128
MaxInUse:                  24487191464
NumAllocs:                   647048879
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:33.444376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:33.444383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:33.444388: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:33.444392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 402
2023-02-23 03:01:33.444396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:33.444400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:33.444404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 22
2023-02-23 03:01:33.444409: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 200
2023-02-23 03:01:33.444413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:33.444417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 152
2023-02-23 03:01:33.444421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:33.444425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 70
2023-02-23 03:01:33.444429: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:33.444433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 9
2023-02-23 03:01:33.444437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:01:33.444441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:33.444446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 60
2023-02-23 03:01:33.444451: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:33.444456: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:33.444461: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-23 03:01:33.444465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:01:33.444469: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:33.444474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 40
2023-02-23 03:01:33.444479: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:33.444494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:33.444500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 16, 180, 0.001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 180 0.001 16 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[0.9 85 0.0005 16 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1137)         1438305     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1137)        4548        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1137)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1795405     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,811,810
Trainable params: 3,806,758
Non-trainable params: 5,052
__________________________________________________________________________________________________
2023-02-23 03:01:39.909427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:39.909509: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24070081312
MaxInUse:                  24487191464
NumAllocs:                   647048935
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:39.909565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:39.909572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:39.909577: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:39.909581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 402
2023-02-23 03:01:39.909585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:39.909589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:39.909594: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 32
2023-02-23 03:01:39.909598: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 200
2023-02-23 03:01:39.909602: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:39.909606: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:39.909610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:39.909614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 70
2023-02-23 03:01:39.909618: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:39.909622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 12
2023-02-23 03:01:39.909626: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:01:39.909630: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:39.909635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:39.909639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:39.909643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:39.909647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 8
2023-02-23 03:01:39.909651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:01:39.909655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:39.909659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:39.909663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:39.909667: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:39.909671: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.9custom_VAE', 'mse', 16, 85, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.9 85 0.0005 16 1]) is not valid.
[1.0 210 0.0007 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:01:42.950284: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:42.950372: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24087001904
MaxInUse:                  24487191464
NumAllocs:                   647048991
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:42.950427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:42.950444: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:42.950449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:42.950453: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 409
2023-02-23 03:01:42.950457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:42.950462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:42.950466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 32
2023-02-23 03:01:42.950470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 211
2023-02-23 03:01:42.950474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 44
2023-02-23 03:01:42.950478: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:42.950482: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:42.950486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 71
2023-02-23 03:01:42.950490: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:42.950495: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 12
2023-02-23 03:01:42.950499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 54
2023-02-23 03:01:42.950503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 18
2023-02-23 03:01:42.950507: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:42.950511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:42.950515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:42.950519: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 8
2023-02-23 03:01:42.950524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 03:01:42.950528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 12
2023-02-23 03:01:42.950532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:42.950537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:42.950542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:42.950546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'logcosh', 64, 210, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 210 0.0007 64 2]) is not valid.
[1.9 85 0.0005 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
2023-02-23 03:01:46.170204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:46.170284: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24101983016
MaxInUse:                  24487191464
NumAllocs:                   647049047
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:46.170344: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:46.170351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:46.170356: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:46.170360: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 409
2023-02-23 03:01:46.170365: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:46.170369: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:46.170373: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 32
2023-02-23 03:01:46.170377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 201
2023-02-23 03:01:46.170381: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:01:46.170394: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:46.170399: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:46.170403: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 71
2023-02-23 03:01:46.170407: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:46.170411: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 12
2023-02-23 03:01:46.170415: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:01:46.170419: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:01:46.170423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:46.170427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:46.170431: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:46.170435: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 8
2023-02-23 03:01:46.170440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:01:46.170444: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:01:46.170448: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:46.170452: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:46.170456: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:46.170460: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.9custom_VAE', 'logcosh', 64, 85, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.9 85 0.0005 64 2]) is not valid.
[1.0 85 0.0005 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:01:49.178881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:49.178961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24118903608
MaxInUse:                  24487191464
NumAllocs:                   647049103
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:49.179026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:49.179034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:49.179040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:49.179045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 416
2023-02-23 03:01:49.179050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:49.179055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:49.179060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 32
2023-02-23 03:01:49.179065: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 212
2023-02-23 03:01:49.179070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:01:49.179075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:49.179080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:49.179085: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 72
2023-02-23 03:01:49.179090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:49.179095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 12
2023-02-23 03:01:49.179101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 54
2023-02-23 03:01:49.179105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:01:49.179121: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:49.179127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:49.179132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:49.179137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 8
2023-02-23 03:01:49.179142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 03:01:49.179147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:01:49.179152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:49.179157: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:49.179162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:49.179167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'logcosh', 64, 85, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 85 0.0005 64 2]) is not valid.
[0.9 85 0.0005 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1137)         1438305     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1137)        4548        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1137)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1795405     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,811,810
Trainable params: 3,806,758
Non-trainable params: 5,052
__________________________________________________________________________________________________
2023-02-23 03:01:52.202453: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:52.202533: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24117230256
MaxInUse:                  24487191464
NumAllocs:                   647049159
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:52.202587: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:52.202594: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:52.202599: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:52.202603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 416
2023-02-23 03:01:52.202608: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:52.202614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:52.202618: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:01:52.202622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 202
2023-02-23 03:01:52.202626: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:01:52.202630: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:52.202634: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:52.202638: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 72
2023-02-23 03:01:52.202642: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:52.202647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:01:52.202651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:01:52.202655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:01:52.202659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:52.202663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:52.202667: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:52.202671: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:01:52.202675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:01:52.202679: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:01:52.202683: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:52.202699: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:52.202704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:52.202708: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.9custom_VAE', 'mse', 64, 85, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.9 85 0.0005 64 1]) is not valid.
[1.0 25 0.0014000000000000002 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:01:55.222191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:55.222272: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24134150848
MaxInUse:                  24487191464
NumAllocs:                   647049215
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:55.222331: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:55.222338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 83
2023-02-23 03:01:55.222343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:55.222347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 423
2023-02-23 03:01:55.222352: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:55.222356: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:55.222360: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:01:55.222364: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 213
2023-02-23 03:01:55.222368: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:01:55.222373: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:55.222377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:55.222381: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 73
2023-02-23 03:01:55.222385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:55.222389: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:01:55.222393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 54
2023-02-23 03:01:55.222397: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:01:55.222402: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:55.222406: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:55.222410: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:55.222414: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:01:55.222418: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 03:01:55.222422: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:01:55.222426: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:55.222430: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:55.222434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:55.222440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'logcosh', 64, 25, 0.0014000000000000002, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 25 0.0014000000000000002 64 2]) is not valid.
[1.7999999999999998 35 0.0005 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2275)         2877875     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2275)        9100        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2275)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3526303     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,560,382
Trainable params: 7,550,778
Non-trainable params: 9,604
__________________________________________________________________________________________________
2023-02-23 03:01:58.257200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:01:58.257288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24147471800
MaxInUse:                  24487191464
NumAllocs:                   647049275
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:01:58.257358: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:01:58.257373: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 87
2023-02-23 03:01:58.257379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:01:58.257383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 423
2023-02-23 03:01:58.257387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:01:58.257391: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:01:58.257395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:01:58.257400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 203
2023-02-23 03:01:58.257404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:01:58.257408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:01:58.257412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:01:58.257416: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:01:58.257420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 73
2023-02-23 03:01:58.257424: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:01:58.257428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:01:58.257432: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:01:58.257436: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:01:58.257440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:01:58.257444: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:01:58.257448: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:01:58.257453: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:01:58.257457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:01:58.257461: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:01:58.257465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:01:58.257469: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:01:58.257473: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:01:58.257477: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:01:58.257481: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:01:58.257485: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.7999999999999998custom_VAE', 'logcosh', 64, 35, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.7999999999999998 35 0.0005 64 2]) is not valid.
[0.9 80 0.0005 256 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1137)         1438305     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1137)        4548        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1137)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1795405     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,811,810
Trainable params: 3,806,758
Non-trainable params: 5,052
__________________________________________________________________________________________________
2023-02-23 03:02:01.355784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:01.355864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24162719040
MaxInUse:                  24487191464
NumAllocs:                   647049331
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:01.355924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:01.355930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 87
2023-02-23 03:02:01.355935: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:01.355940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 430
2023-02-23 03:02:01.355955: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:01.355960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:02:01.355964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 52
2023-02-23 03:02:01.355968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 204
2023-02-23 03:02:01.355973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:01.355977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:02:01.355981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:01.355985: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:01.355989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 74
2023-02-23 03:02:01.355994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:02:01.355998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 18
2023-02-23 03:02:01.356002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:01.356006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:01.356010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:02:01.356014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:01.356019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:01.356023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:02:01.356027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 12
2023-02-23 03:02:01.356031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:01.356035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:01.356039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:02:01.356044: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:01.356048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:01.356052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:01.356056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.9custom_VAE', 'logcosh', 256, 80, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.9 80 0.0005 256 2]) is not valid.
[0.8 85 0.00030000000000000003 128 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1011)         1278915     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1011)        4044        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1011)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          255024      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          255024      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1603759     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,396,766
Trainable params: 3,392,218
Non-trainable params: 4,548
__________________________________________________________________________________________________
2023-02-23 03:02:04.462274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:04.462367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24161058880
MaxInUse:                  24487191464
NumAllocs:                   647049391
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:04.462433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:04.462440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:04.462445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:04.462450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 430
2023-02-23 03:02:04.462454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:04.462458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:02:04.462462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 10
2023-02-23 03:02:04.462466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:04.462480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 204
2023-02-23 03:02:04.462485: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:04.462489: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:02:04.462493: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:04.462497: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:04.462501: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 74
2023-02-23 03:02:04.462505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:02:04.462510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 3
2023-02-23 03:02:04.462514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:04.462518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:04.462522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:04.462526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:02:04.462530: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:04.462534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:04.462539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:02:04.462543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 2
2023-02-23 03:02:04.462547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:04.462551: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:04.462555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:04.462559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:02:04.462563: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:04.462568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:04.462572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:04.462576: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.8custom_VAE', 'mse', 128, 85, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.8 85 0.00030000000000000003 128 1]) is not valid.
[1.1 210 0.0012000000000000001 64 2] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1390)         1758350     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1390)        5560        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1390)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          350532      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          350532      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2180218     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,645,192
Trainable params: 4,639,128
Non-trainable params: 6,064
__________________________________________________________________________________________________
2023-02-23 03:02:10.097962: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:10.098041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24179639648
MaxInUse:                  24487191464
NumAllocs:                   647049447
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:10.098102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:10.098109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:10.098114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:10.098118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 437
2023-02-23 03:02:10.098122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:10.098127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:02:10.098131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 10
2023-02-23 03:02:10.098135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:10.098139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 205
2023-02-23 03:02:10.098154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 10
2023-02-23 03:02:10.098160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:10.098164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-23 03:02:10.098168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:10.098172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:10.098176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 75
2023-02-23 03:02:10.098180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:02:10.098184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 3
2023-02-23 03:02:10.098188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:10.098192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:10.098197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1401120, 3
2023-02-23 03:02:10.098201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:10.098205: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-23 03:02:10.098209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:10.098213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:10.098217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:02:10.098221: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 2
2023-02-23 03:02:10.098225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:10.098229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:10.098233: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 2
2023-02-23 03:02:10.098237: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:10.098241: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-23 03:02:10.098245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:10.098249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:10.098253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:10.098258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1custom_VAE', 'logcosh', 64, 210, 0.0012000000000000001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1 210 0.0012000000000000001 64 2]) is not valid.
[1.9 30 0.0012000000000000001 128 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
2023-02-23 03:02:13.197945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:13.198025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24192960584
MaxInUse:                  24487191464
NumAllocs:                   647049503
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:13.198081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:13.198088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:13.198093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:13.198097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 437
2023-02-23 03:02:13.198101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:13.198105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:02:13.198126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 10
2023-02-23 03:02:13.198130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:13.198134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 205
2023-02-23 03:02:13.198138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:13.198142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:13.198147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:13.198151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:13.198155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 75
2023-02-23 03:02:13.198159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:02:13.198163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 3
2023-02-23 03:02:13.198167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:13.198171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:13.198175: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:13.198179: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:13.198183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:13.198187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:13.198191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:02:13.198195: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 2
2023-02-23 03:02:13.198200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:13.198204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:13.198208: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:13.198212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:13.198216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:13.198220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:13.198224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:13.198228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.9custom_VAE', 'logcosh', 128, 30, 0.0012000000000000001, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.9 30 0.0012000000000000001 128 2]) is not valid.
[0.9 85 0.00030000000000000003 64 1] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1137)         1438305     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1137)        4548        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1137)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          286776      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1795405     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,811,810
Trainable params: 3,806,758
Non-trainable params: 5,052
__________________________________________________________________________________________________
2023-02-23 03:02:16.229084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:16.229162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24208207824
MaxInUse:                  24487191464
NumAllocs:                   647049559
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:16.229219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:16.229226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:16.229231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:16.229235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 444
2023-02-23 03:02:16.229239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:16.229243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:02:16.229247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 10
2023-02-23 03:02:16.229260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 52
2023-02-23 03:02:16.229264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 206
2023-02-23 03:02:16.229269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:16.229273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:16.229277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:16.229281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:16.229285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 76
2023-02-23 03:02:16.229289: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:02:16.229293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 3
2023-02-23 03:02:16.229302: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 18
2023-02-23 03:02:16.229307: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:16.229311: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:16.229315: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:16.229319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:16.229323: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:16.229327: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:02:16.229331: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 2
2023-02-23 03:02:16.229335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 12
2023-02-23 03:02:16.229339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:16.229343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:16.229347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:16.229351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:16.229355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:16.229359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:16.229363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.9custom_VAE', 'mse', 64, 85, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.9 85 0.00030000000000000003 64 1]) is not valid.
[0.8 210 0.0007 128 1] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1011)         1278915     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1011)        4044        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1011)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          255024      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          255024      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1603759     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,396,766
Trainable params: 3,392,218
Non-trainable params: 4,548
__________________________________________________________________________________________________
2023-02-23 03:02:19.626582: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:19.626663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24206547648
MaxInUse:                  24487191464
NumAllocs:                   647049615
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:19.626724: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:19.626730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:19.626735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:19.626739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 444
2023-02-23 03:02:19.626744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:19.626748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 110
2023-02-23 03:02:19.626752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:19.626756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:19.626768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 206
2023-02-23 03:02:19.626773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:19.626777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:19.626781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:19.626785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:19.626789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 76
2023-02-23 03:02:19.626794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 45
2023-02-23 03:02:19.626799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:19.626804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:19.626808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:19.626812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:19.626816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:19.626820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:19.626824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:19.626828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 30
2023-02-23 03:02:19.626832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:19.626836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:19.626840: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:19.626844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:19.626849: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:19.626853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:19.626857: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:19.626861: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:19.626865: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.8custom_VAE', 'mse', 128, 210, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.8 210 0.0007 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[0.5 175 0.0007 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:25.939825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:25.939905: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24215141008
MaxInUse:                  24487191464
NumAllocs:                   647049671
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:25.939966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:25.939973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:25.939978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:25.939982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 451
2023-02-23 03:02:25.939986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:25.939991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 120
2023-02-23 03:02:25.939995: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:25.939999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:25.940013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 207
2023-02-23 03:02:25.940018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:25.940022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:25.940026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:25.940030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:25.940034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 77
2023-02-23 03:02:25.940038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 48
2023-02-23 03:02:25.940043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:25.940047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:25.940051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:25.940055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:25.940059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:25.940063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:25.940067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:25.940071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 03:02:25.940075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:25.940079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:25.940084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:25.940088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:25.940092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:25.940096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:25.940100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:25.940104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:25.940108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 175, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0007 128 1]) is not valid.
[0.5 180 0.0005 16 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:28.984651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:28.984728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24215141008
MaxInUse:                  24487191464
NumAllocs:                   647049727
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:28.984788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:28.984795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:28.984799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:28.984804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 451
2023-02-23 03:02:28.984808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:28.984812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 120
2023-02-23 03:02:28.984816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:28.984820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:28.984824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 207
2023-02-23 03:02:28.984828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:28.984842: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:28.984847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:28.984851: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:28.984856: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 77
2023-02-23 03:02:28.984860: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 48
2023-02-23 03:02:28.984864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:28.984869: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:28.984874: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:28.984878: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:28.984882: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:28.984886: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:28.984890: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:28.984895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 03:02:28.984899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:28.984903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:28.984907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:28.984911: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:28.984915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:28.984919: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:28.984923: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:28.984927: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:28.984931: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 16, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 16 2]) is not valid.
[0.5 180 0.00030000000000000003 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:32.062767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:32.062845: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24223734368
MaxInUse:                  24487191464
NumAllocs:                   647049783
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:32.062906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:32.062913: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:32.062918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:32.062922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 458
2023-02-23 03:02:32.062926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:32.062930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 130
2023-02-23 03:02:32.062935: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:32.062939: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:32.062943: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 208
2023-02-23 03:02:32.062947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:32.062961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:32.062966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:32.062970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:32.062974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 78
2023-02-23 03:02:32.062979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 51
2023-02-23 03:02:32.062983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:32.062987: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:32.062991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:32.062995: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:32.062999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:32.063003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:32.063008: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:32.063012: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 03:02:32.063016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:32.063020: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:32.063024: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:32.063028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:32.063032: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:32.063036: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:32.063040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:32.063045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:32.063049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 128 1]) is not valid.
[0.5 180 0.0007 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:35.114564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:35.114646: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24223734368
MaxInUse:                  24487191464
NumAllocs:                   647049839
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:35.114705: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:35.114712: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:35.114716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:35.114721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 458
2023-02-23 03:02:35.114725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:35.114729: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 130
2023-02-23 03:02:35.114733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:35.114737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:35.114741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 208
2023-02-23 03:02:35.114746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:35.114750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:35.114764: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:35.114769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:35.114773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 78
2023-02-23 03:02:35.114777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 51
2023-02-23 03:02:35.114781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:35.114785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:35.114789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:35.114794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:35.114798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:35.114802: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:35.114807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:35.114812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 03:02:35.114817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:35.114823: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:35.114828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:35.114833: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:35.114838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:35.114843: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:35.114848: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:35.114853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:35.114858: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 128 1]) is not valid.
[0.5 180 0.0007 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:38.214819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:38.214898: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24232327728
MaxInUse:                  24487191464
NumAllocs:                   647049895
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:38.214960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:38.214967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 91
2023-02-23 03:02:38.214972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:38.214976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 465
2023-02-23 03:02:38.214980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:38.214984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:02:38.214988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:38.214992: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:38.214996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 209
2023-02-23 03:02:38.215000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:38.215004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:38.215008: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:38.215025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:38.215030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 79
2023-02-23 03:02:38.215034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:02:38.215038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:38.215042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:38.215047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:38.215051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:38.215055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:38.215059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:38.215063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:38.215067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:02:38.215071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:38.215075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:38.215079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:38.215083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:38.215087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:38.215092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:38.215096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:38.215100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:38.215104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 128 1]) is not valid.
[0.4 180 0.0005 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:02:41.310503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:41.310945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24230654392
MaxInUse:                  24487191464
NumAllocs:                   647049955
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:41.311016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:41.311024: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 95
2023-02-23 03:02:41.311028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:41.311033: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 465
2023-02-23 03:02:41.311037: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:41.311041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:02:41.311045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 130
2023-02-23 03:02:41.311049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:41.311053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:41.311057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 209
2023-02-23 03:02:41.311061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:41.311065: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:41.311069: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:41.311074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:41.311088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 79
2023-02-23 03:02:41.311093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:02:41.311097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 51
2023-02-23 03:02:41.311101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:41.311106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:41.311110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:41.311114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:41.311118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:41.311122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:41.311126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:02:41.311130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:41.311134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 03:02:41.311138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:41.311142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:41.311146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:41.311151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:41.311155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:41.311159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:41.311163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:41.311167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:41.311171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 32, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0005 32 1]) is not valid.
[0.5 175 0.0005 128 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:44.431292: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:44.431373: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24239247752
MaxInUse:                  24487191464
NumAllocs:                   647050011
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:44.431434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:44.431441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 95
2023-02-23 03:02:44.431446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:44.431450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 472
2023-02-23 03:02:44.431454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:44.431458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:02:44.431462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:02:44.431467: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:44.431471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:44.431475: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 210
2023-02-23 03:02:44.431479: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:44.431483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:44.431499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:44.431504: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:44.431508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 80
2023-02-23 03:02:44.431512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:02:44.431516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:02:44.431521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:44.431525: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:44.431529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:44.431533: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:44.431537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:44.431541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:44.431545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:02:44.431549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:44.431554: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:02:44.431558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:44.431562: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:44.431566: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:44.431570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:44.431574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:44.431578: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:44.431582: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:44.431586: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:44.431591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0005 128 1]) is not valid.
[0.5 180 0.0005 16 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:47.558808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:47.558891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24239247752
MaxInUse:                  24487191464
NumAllocs:                   647050067
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:47.558968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:47.558975: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 95
2023-02-23 03:02:47.558979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:47.558984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 472
2023-02-23 03:02:47.558988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:47.558992: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:02:47.558996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:02:47.559001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:47.559006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:47.559011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 210
2023-02-23 03:02:47.559024: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:47.559029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:47.559033: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:47.559037: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:47.559041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 80
2023-02-23 03:02:47.559045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:02:47.559049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:02:47.559053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:47.559057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:47.559061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:47.559065: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:47.559070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:47.559074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:47.559078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:02:47.559082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:47.559086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:02:47.559090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:47.559094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:47.559098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:47.559102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:47.559106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:47.559110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:47.559114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:47.559118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:47.559122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 16, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 16 1]) is not valid.
[0.5 180 0.0005 128 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:50.655993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:50.656071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24247841112
MaxInUse:                  24487191464
NumAllocs:                   647050123
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:50.656132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:50.656139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 95
2023-02-23 03:02:50.656144: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:50.656148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 479
2023-02-23 03:02:50.656152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:50.656156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:02:50.656161: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 150
2023-02-23 03:02:50.656165: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:50.656179: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:50.656183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 211
2023-02-23 03:02:50.656187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:50.656192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:50.656196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:50.656200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:50.656204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 81
2023-02-23 03:02:50.656208: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:02:50.656212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-23 03:02:50.656216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:50.656220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:50.656225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:50.656229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:50.656233: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:50.656237: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:50.656241: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:02:50.656245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:50.656249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 03:02:50.656253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:50.656257: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:50.656262: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:50.656266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:50.656270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:50.656274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:50.656278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:50.656282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:50.656286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 128, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 128 2]) is not valid.
[0.6 175 0.0005 128 1] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:02:53.699603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:53.699691: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24249501304
MaxInUse:                  24487191464
NumAllocs:                   647050183
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:53.699760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:53.699768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:02:53.699772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:53.699777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 479
2023-02-23 03:02:53.699781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:53.699785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:02:53.699798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:02:53.699802: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 03:02:53.699806: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:53.699810: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:53.699815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 211
2023-02-23 03:02:53.699819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:53.699823: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:53.699827: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:53.699831: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:53.699835: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 81
2023-02-23 03:02:53.699839: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:02:53.699843: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:02:53.699847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 3
2023-02-23 03:02:53.699851: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:53.699855: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:53.699859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:53.699863: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:53.699867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:53.699872: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:53.699876: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:02:53.699880: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:53.699884: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:02:53.699888: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 03:02:53.699892: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:53.699896: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:53.699900: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:53.699904: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:53.699908: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:53.699912: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:53.699916: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:53.699920: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:53.699924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 128, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 175 0.0005 128 1]) is not valid.
[0.5 180 0.0007 128 1] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:02:56.739504: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:02:56.739576: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24258094664
MaxInUse:                  24487191464
NumAllocs:                   647050239
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:02:56.739637: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:02:56.739653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:02:56.739658: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:02:56.739663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 486
2023-02-23 03:02:56.739667: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:02:56.739671: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:02:56.739675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 150
2023-02-23 03:02:56.739679: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 03:02:56.739683: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:02:56.739687: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:02:56.739691: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 212
2023-02-23 03:02:56.739695: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:02:56.739699: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:02:56.739703: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:02:56.739707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:02:56.739711: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 82
2023-02-23 03:02:56.739716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:02:56.739720: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-23 03:02:56.739724: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 3
2023-02-23 03:02:56.739728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:02:56.739732: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:02:56.739736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:02:56.739740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:02:56.739744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:02:56.739748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:02:56.739752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:02:56.739756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:02:56.739760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 03:02:56.739764: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 03:02:56.739768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:02:56.739772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:02:56.739776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:02:56.739780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:02:56.739784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:02:56.739789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:02:56.739793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:02:56.739797: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:02:56.739803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_3.pkl
[0.6 180 0.0005 16 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:03:02.537930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:02.538018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24259754840
MaxInUse:                  24487191464
NumAllocs:                   647050295
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:02.538087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:02.538094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:02.538099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:02.538103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 486
2023-02-23 03:03:02.538108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:02.538113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:03:02.538118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:02.538123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:02.538127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:02.538132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:02.538137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 212
2023-02-23 03:03:02.538141: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:02.538145: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:02.538149: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:02.538154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:02.538158: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 82
2023-02-23 03:03:02.538162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:03:02.538166: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:02.538170: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:02.538174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:02.538178: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:02.538182: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:03:02.538186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:02.538190: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:02.538195: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:02.538199: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:03:02.538203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:02.538207: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:02.538211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:02.538215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:02.538219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:02.538223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:03:02.538230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:02.538234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:02.538238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:02.538243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:02.538247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:02.538251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 16, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 16 1]) is not valid.
[0.5 205 0.0005 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:03:05.598846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:05.598924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24268348200
MaxInUse:                  24487191464
NumAllocs:                   647050351
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:05.598989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:05.598996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:05.599000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:05.599005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 493
2023-02-23 03:03:05.599009: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:05.599013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:03:05.599017: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 150
2023-02-23 03:03:05.599021: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:05.599025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:05.599030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:05.599034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 213
2023-02-23 03:03:05.599038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:05.599042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:05.599046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:05.599050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:05.599054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 83
2023-02-23 03:03:05.599059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:03:05.599063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-23 03:03:05.599067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:05.599071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:05.599075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:05.599079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 51
2023-02-23 03:03:05.599083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:05.599087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:05.599091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:05.599096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:03:05.599100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:05.599115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 03:03:05.599120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:05.599124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:05.599129: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:05.599134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 03:03:05.599139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:05.599143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:05.599148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:05.599153: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:05.599158: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:05.599163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 205, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 205 0.0005 128 1]) is not valid.
[1.0 205 0.0005 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:03:08.718821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:08.718901: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24276675432
MaxInUse:                  24487191464
NumAllocs:                   647050407
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:08.718968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:08.718974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:08.718979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:08.718983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 493
2023-02-23 03:03:08.718988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:08.718993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 03:03:08.718997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:08.719001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:08.719005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:08.719009: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:08.719013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 223
2023-02-23 03:03:08.719018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:08.719022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:08.719026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:08.719030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:08.719034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 83
2023-02-23 03:03:08.719038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 3
2023-02-23 03:03:08.719042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:08.719047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:08.719051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:08.719055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:08.719059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 54
2023-02-23 03:03:08.719072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:08.719077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:08.719081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:08.719085: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 03:03:08.719089: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:08.719093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:08.719098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:08.719102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:08.719106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:08.719110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 03:03:08.719114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:08.719118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:08.719122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:08.719127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:08.719133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:08.719137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 205, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 205 0.0005 128 1]) is not valid.
[0.4 175 0.0005 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:03:11.851544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:11.851888: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24283595440
MaxInUse:                  24487191464
NumAllocs:                   647050463
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:11.851952: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:11.851960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:11.851965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:11.851970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 500
2023-02-23 03:03:11.851974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:11.851978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 20
2023-02-23 03:03:11.851982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:11.851986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:11.851990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:11.851994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:11.851998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 224
2023-02-23 03:03:11.852003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:11.852007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:11.852011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:11.852015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:11.852019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 84
2023-02-23 03:03:11.852023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 6
2023-02-23 03:03:11.852040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:11.852045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:11.852049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:11.852053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:11.852057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 54
2023-02-23 03:03:11.852061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:11.852065: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:11.852069: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:11.852074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 4
2023-02-23 03:03:11.852078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:11.852082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:11.852086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:11.852090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:11.852094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:11.852098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 03:03:11.852102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:11.852106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:11.852111: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:11.852115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:11.852119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:11.852123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 128, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 175 0.0005 128 1]) is not valid.
[0.4 210 0.0007 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:03:14.977462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:14.977546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24283595440
MaxInUse:                  24487191464
NumAllocs:                   647050519
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:14.977610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:14.977617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:14.977622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:14.977626: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 500
2023-02-23 03:03:14.977630: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:14.977635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 20
2023-02-23 03:03:14.977639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:14.977643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:14.977647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:14.977651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:14.977655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 224
2023-02-23 03:03:14.977659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:14.977663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:14.977677: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:14.977682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:14.977686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 84
2023-02-23 03:03:14.977690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 6
2023-02-23 03:03:14.977694: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:14.977698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:14.977703: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:14.977707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:14.977711: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 54
2023-02-23 03:03:14.977715: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:14.977719: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:14.977723: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:14.977727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 4
2023-02-23 03:03:14.977731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:14.977735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:14.977739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:14.977743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:14.977747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:14.977751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 03:03:14.977755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:14.977759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:14.977764: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:14.977768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:14.977772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:14.977776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 128, 210, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 210 0.0007 128 1]) is not valid.
[1.0 210 0.00030000000000000003 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:03:18.112658: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:18.112740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24300516032
MaxInUse:                  24487191464
NumAllocs:                   647050575
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:18.112805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:18.112812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:18.112817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:18.112822: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 507
2023-02-23 03:03:18.112826: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:18.112830: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 20
2023-02-23 03:03:18.112834: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:18.112838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:18.112853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:18.112858: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:18.112862: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 235
2023-02-23 03:03:18.112866: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:18.112870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:18.112875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:18.112879: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:18.112883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 85
2023-02-23 03:03:18.112887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 6
2023-02-23 03:03:18.112891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:18.112895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:18.112899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:18.112903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:18.112908: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:18.112912: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:18.112916: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:18.112920: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:18.112924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 4
2023-02-23 03:03:18.112928: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:18.112932: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:18.112936: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:18.112940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:18.112944: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:18.112949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:18.112953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:18.112957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:18.112961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:18.112965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:18.112969: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:18.112973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 210, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 210 0.00030000000000000003 128 1]) is not valid.
[1.0 175 0.0005 128 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:03:21.273025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:21.273105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24300516032
MaxInUse:                  24487191464
NumAllocs:                   647050631
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:21.273170: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:21.273177: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:21.273191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:21.273196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 507
2023-02-23 03:03:21.273200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:21.273204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 20
2023-02-23 03:03:21.273209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:21.273213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:21.273218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:21.273223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:21.273228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 235
2023-02-23 03:03:21.273232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:21.273236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:21.273240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:21.273246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:21.273251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 85
2023-02-23 03:03:21.273255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 6
2023-02-23 03:03:21.273260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:21.273265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:21.273270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:21.273274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:21.273279: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:21.273283: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:21.273287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:21.273291: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:21.273295: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 4
2023-02-23 03:03:21.273304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:21.273309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:21.273313: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:21.273317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:21.273321: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:21.273325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:21.273330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:21.273334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:21.273338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:21.273342: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:21.273346: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:21.273351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'logcosh', 128, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 175 0.0005 128 2]) is not valid.
[1.0 210 0.0005 32 1] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
2023-02-23 03:03:24.369198: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:24.369276: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24317436624
MaxInUse:                  24487191464
NumAllocs:                   647050687
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:24.369371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:24.369379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:24.369384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:24.369388: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 514
2023-02-23 03:03:24.369392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:24.369396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 20
2023-02-23 03:03:24.369400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:24.369404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 03:03:24.369408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:24.369413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:24.369417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 246
2023-02-23 03:03:24.369421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:24.369425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:24.369429: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:24.369433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:24.369437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 86
2023-02-23 03:03:24.369441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 6
2023-02-23 03:03:24.369445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:24.369449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 6
2023-02-23 03:03:24.369453: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:24.369457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:24.369461: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 60
2023-02-23 03:03:24.369466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:24.369470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:24.369474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:24.369478: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 4
2023-02-23 03:03:24.369482: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:24.369486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:24.369490: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 03:03:24.369494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:24.369498: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:24.369502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 40
2023-02-23 03:03:24.369506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:24.369510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:24.369517: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:24.369521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:24.369525: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:24.369529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 32, 210, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 210 0.0005 32 1]) is not valid.
[0.6 180 0.0005 128 1] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:03:27.452186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:27.452267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24310769568
MaxInUse:                  24487191464
NumAllocs:                   647050743
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:27.452343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:27.452350: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:27.452355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:27.452359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 514
2023-02-23 03:03:27.452363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:27.452367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 20
2023-02-23 03:03:27.452372: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:27.452376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:27.452380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:27.452384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:27.452388: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 236
2023-02-23 03:03:27.452392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:27.452396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:27.452400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:27.452405: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:27.452409: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 86
2023-02-23 03:03:27.452413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 6
2023-02-23 03:03:27.452417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:27.452421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:27.452425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:27.452429: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:27.452434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:27.452438: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:27.452442: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:27.452446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:27.452450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 4
2023-02-23 03:03:27.452454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:27.452458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:27.452462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:27.452476: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:27.452481: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:27.452485: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:27.452489: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:27.452493: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:27.452497: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:27.452502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:27.452506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:27.452510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 128, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_4.pkl
[0.4 180 0.0005 128 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:03:35.075870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:35.075951: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24317689576
MaxInUse:                  24487191464
NumAllocs:                   647050799
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:35.076018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:35.076025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:35.076029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:35.076034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 521
2023-02-23 03:03:35.076038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:35.076042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 30
2023-02-23 03:03:35.076046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:35.076050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:35.076055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:35.076059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:35.076063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 237
2023-02-23 03:03:35.076067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:35.076071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:35.076075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:35.076079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:35.076083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 87
2023-02-23 03:03:35.076087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 9
2023-02-23 03:03:35.076091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:35.076095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:35.076099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:35.076104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:35.076108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:35.076112: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:35.076116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:35.076129: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:35.076133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 6
2023-02-23 03:03:35.076138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:35.076142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:35.076147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:35.076151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:35.076155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:35.076159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:35.076163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:35.076168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:35.076172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:35.076176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:35.076180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:35.076184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'logcosh', 128, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0005 128 2]) is not valid.
[0.4 180 0.0007 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:03:38.139679: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:38.139760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24317689576
MaxInUse:                  24487191464
NumAllocs:                   647050855
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:38.139829: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:38.139836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:38.139841: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:38.139845: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 521
2023-02-23 03:03:38.139849: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:38.139853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 30
2023-02-23 03:03:38.139857: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:38.139862: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:38.139866: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:38.139870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:38.139874: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 237
2023-02-23 03:03:38.139878: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:38.139882: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:38.139886: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:38.139890: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:38.139894: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 87
2023-02-23 03:03:38.139898: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 9
2023-02-23 03:03:38.139903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:38.139907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:38.139920: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:38.139924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:38.139929: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:38.139933: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:38.139937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:38.139941: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:38.139945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 6
2023-02-23 03:03:38.139949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:38.139953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:38.139957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:38.139961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:38.139965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:38.139969: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:38.139973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:38.139978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:38.139982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:38.139986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:38.139990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:38.139994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0007 128 1]) is not valid.
[0.5 180 0.0005 32 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:03:41.285921: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:41.286000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24326282936
MaxInUse:                  24487191464
NumAllocs:                   647050911
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:41.286066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:41.286073: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:41.286077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:41.286082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 528
2023-02-23 03:03:41.286086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:41.286090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 30
2023-02-23 03:03:41.286094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 150
2023-02-23 03:03:41.286098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:41.286102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:41.286106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:41.286110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 238
2023-02-23 03:03:41.286114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:41.286118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:41.286123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:41.286135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:41.286140: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 88
2023-02-23 03:03:41.286144: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 9
2023-02-23 03:03:41.286148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-23 03:03:41.286152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:41.286156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:41.286160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:41.286165: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:41.286169: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:41.286173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:41.286177: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:41.286181: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 6
2023-02-23 03:03:41.286185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:41.286189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 03:03:41.286193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:41.286197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:41.286201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:41.286205: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:41.286209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:41.286214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:41.286218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:41.286222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:41.286226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:41.286230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 32, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 32 1]) is not valid.
[0.4 180 0.0005 128 1] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:03:44.315821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:44.315900: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24324609584
MaxInUse:                  24487191464
NumAllocs:                   647050967
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:44.315967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:44.315974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:44.315978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:44.315983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 528
2023-02-23 03:03:44.315987: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:44.315991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 40
2023-02-23 03:03:44.315995: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:44.315999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:44.316004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:44.316018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:44.316023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 238
2023-02-23 03:03:44.316027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:44.316031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:44.316035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:44.316039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:44.316043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 88
2023-02-23 03:03:44.316048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 12
2023-02-23 03:03:44.316052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:44.316056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:44.316060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:44.316064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:44.316068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:44.316072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:44.316077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:44.316081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:44.316085: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 8
2023-02-23 03:03:44.316089: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:44.316093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:44.316097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:44.316101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:44.316105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:44.316109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:44.316114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:44.316118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:44.316122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:44.316126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:44.316130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:44.316134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 128, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0005 128 1]) is not valid.
[0.4 180 0.0005 128 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:03:47.332923: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:47.333001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24331529592
MaxInUse:                  24487191464
NumAllocs:                   647051023
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:47.333068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:47.333076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:47.333081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:47.333085: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 535
2023-02-23 03:03:47.333101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:47.333106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:03:47.333110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:47.333115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:47.333119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:47.333123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:47.333127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 239
2023-02-23 03:03:47.333131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:47.333135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:47.333139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:47.333143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:47.333147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 89
2023-02-23 03:03:47.333152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:03:47.333156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:47.333160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:47.333164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:47.333168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:47.333172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:47.333176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:47.333180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:47.333185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:47.333189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:03:47.333193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:47.333197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:47.333201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:47.333205: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:47.333209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:47.333213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:47.333217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:47.333222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:47.333226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:47.333230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:47.333234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:47.333238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'logcosh', 128, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0005 128 2]) is not valid.
[0.4 180 0.00030000000000000003 128 1] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:03:53.270214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:53.270303: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24331529592
MaxInUse:                  24487191464
NumAllocs:                   647051079
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:53.270384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:53.270391: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:53.270396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:53.270400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 535
2023-02-23 03:03:53.270404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:53.270408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:03:53.270413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 140
2023-02-23 03:03:53.270417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:53.270421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:53.270425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:53.270429: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 239
2023-02-23 03:03:53.270433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:53.270437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:53.270441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:53.270445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:53.270450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 89
2023-02-23 03:03:53.270454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:03:53.270458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-23 03:03:53.270462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:53.270466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:53.270471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:53.270477: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:53.270481: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:53.270485: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:53.270489: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:53.270493: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:03:53.270497: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:53.270501: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 03:03:53.270505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:53.270510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:53.270514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:53.270518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:53.270522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:53.270526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:53.270530: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:53.270537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:53.270541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:53.270545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 128, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.00030000000000000003 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_5.pkl
[0.5 175 0.0005 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:03:58.965721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:03:58.966148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24340122952
MaxInUse:                  24487191464
NumAllocs:                   647051135
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:03:58.966218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:03:58.966225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:03:58.966230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:03:58.966234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 542
2023-02-23 03:03:58.966238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:03:58.966243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:03:58.966247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 150
2023-02-23 03:03:58.966251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:03:58.966255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:03:58.966259: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:03:58.966263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 240
2023-02-23 03:03:58.966267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:03:58.966271: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:03:58.966276: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:03:58.966280: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:03:58.966284: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 90
2023-02-23 03:03:58.966288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:03:58.966292: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-23 03:03:58.966303: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:03:58.966308: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:03:58.966312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:03:58.966316: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:03:58.966320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:03:58.966324: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:03:58.966328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:03:58.966334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:03:58.966339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:03:58.966343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 03:03:58.966347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:03:58.966351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:03:58.966371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:03:58.966375: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:03:58.966379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:03:58.966383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:03:58.966388: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:03:58.966392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:03:58.966396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:03:58.966400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0005 256 1]) is not valid.
[0.5 180 0.0005 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:02.038503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:02.038587: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24340122952
MaxInUse:                  24487191464
NumAllocs:                   647051191
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:02.038657: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:02.038664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:02.038668: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:02.038673: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 542
2023-02-23 03:04:02.038677: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:02.038681: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:02.038685: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 150
2023-02-23 03:04:02.038689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:02.038693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:02.038698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:02.038702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 240
2023-02-23 03:04:02.038706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:02.038710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:02.038714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:02.038718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:02.038722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 90
2023-02-23 03:04:02.038726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:02.038731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-23 03:04:02.038735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:02.038739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:02.038743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:02.038747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:02.038751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:02.038755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:02.038759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:02.038775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:02.038779: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:02.038784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 03:04:02.038788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:02.038792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:02.038796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:02.038800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:02.038804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:02.038808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:02.038813: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:02.038817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:02.038821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:02.038825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 256 1]) is not valid.
[0.5 180 0.0005 256 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:05.176718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:05.176796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24348716312
MaxInUse:                  24487191464
NumAllocs:                   647051247
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:05.176864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:05.176872: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:05.176877: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:05.176881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 549
2023-02-23 03:04:05.176885: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:05.176889: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:05.176893: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 160
2023-02-23 03:04:05.176898: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:05.176902: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:05.176906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:05.176910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 241
2023-02-23 03:04:05.176914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:05.176918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:05.176922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:05.176926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:05.176930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 91
2023-02-23 03:04:05.176934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:05.176938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 60
2023-02-23 03:04:05.176942: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:05.176946: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:05.176960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:05.176965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:05.176969: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:05.176973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:05.176977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:05.176981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:05.176985: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:05.176990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 40
2023-02-23 03:04:05.176994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:05.176998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:05.177002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:05.177006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:05.177010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:05.177014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:05.177018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:05.177022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:05.177026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:05.177030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 256, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 256 2]) is not valid.
[0.5 180 0.0007 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:08.265075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:08.265158: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24348716312
MaxInUse:                  24487191464
NumAllocs:                   647051303
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:08.265245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:08.265252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:08.265256: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:08.265260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 549
2023-02-23 03:04:08.265265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:08.265269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:08.265273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 160
2023-02-23 03:04:08.265277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:08.265281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:08.265285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:08.265289: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 241
2023-02-23 03:04:08.265294: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:08.265304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:08.265308: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:08.265312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:08.265329: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 91
2023-02-23 03:04:08.265334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:08.265338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 60
2023-02-23 03:04:08.265342: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:08.265347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:08.265351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:08.265355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:08.265359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:08.265363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:08.265367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:08.265371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:08.265375: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:08.265379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 40
2023-02-23 03:04:08.265384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:08.265388: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:08.265392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:08.265396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:08.265400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:08.265404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:08.265408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:08.265412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:08.265417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:08.265421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 64, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 64 1]) is not valid.
[0.5 175 0.00030000000000000003 128 1] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:11.337425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:11.337740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24357309672
MaxInUse:                  24487191464
NumAllocs:                   647051359
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:11.337808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:11.337815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:11.337820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:11.337824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 556
2023-02-23 03:04:11.337828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:11.337832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:11.337836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 170
2023-02-23 03:04:11.337841: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:11.337845: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:11.337849: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:11.337864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 242
2023-02-23 03:04:11.337868: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:11.337873: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:11.337877: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:11.337881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:11.337885: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 92
2023-02-23 03:04:11.337889: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:11.337893: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 63
2023-02-23 03:04:11.337897: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:11.337901: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:11.337905: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:11.337910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:11.337914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:11.337918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:11.337922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:11.337926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:11.337930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:11.337934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 42
2023-02-23 03:04:11.337938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:11.337942: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:11.337946: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:11.337950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:11.337954: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:11.337959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:11.337963: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:11.337967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:11.337971: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:11.337975: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 175, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.00030000000000000003 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_6.pkl
[0.5 180 0.00030000000000000003 128 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:17.050798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:17.050880: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24357309672
MaxInUse:                  24487191464
NumAllocs:                   647051415
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:17.050950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:17.050957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:17.050962: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:17.050966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 556
2023-02-23 03:04:17.050981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:17.050986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:17.050990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 170
2023-02-23 03:04:17.050994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:17.050998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:17.051003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:17.051007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 242
2023-02-23 03:04:17.051011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:17.051015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:17.051019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:17.051023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:17.051027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 92
2023-02-23 03:04:17.051031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:17.051035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 63
2023-02-23 03:04:17.051040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:17.051044: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:17.051048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:17.051052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:17.051056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:17.051060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:17.051064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:17.051068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:17.051072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:17.051077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 42
2023-02-23 03:04:17.051081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:17.051085: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:17.051089: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:17.051093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:17.051097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:17.051101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:17.051105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:17.051109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:17.051114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:17.051118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 128, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 128 2]) is not valid.
[0.5 180 0.0007 256 1] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:20.116326: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:20.116408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24365903032
MaxInUse:                  24487191464
NumAllocs:                   647051471
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:20.116492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:20.116501: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:20.116506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:20.116510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 563
2023-02-23 03:04:20.116515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:20.116519: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:20.116523: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 180
2023-02-23 03:04:20.116527: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:20.116531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:20.116535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:20.116539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 243
2023-02-23 03:04:20.116543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:20.116547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:20.116552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:20.116556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:20.116560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 93
2023-02-23 03:04:20.116564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:20.116568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 66
2023-02-23 03:04:20.116572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:20.116576: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:20.116580: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:20.116584: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:20.116589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:20.116593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:20.116597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:20.116601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:20.116605: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:20.116609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 44
2023-02-23 03:04:20.116613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:20.116617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:20.116621: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:20.116625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:20.116629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:20.116634: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:20.116638: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:20.116646: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:20.116650: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:20.116655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_7.pkl
[0.5 175 0.0005 128 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:26.006573: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:26.006652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24365903032
MaxInUse:                  24487191464
NumAllocs:                   647051527
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:26.006731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:26.006740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:26.006745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:26.006749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 563
2023-02-23 03:04:26.006753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:26.006758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:26.006762: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 180
2023-02-23 03:04:26.006766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:26.006770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:26.006774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:26.006778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 243
2023-02-23 03:04:26.006782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:26.006786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:26.006791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:26.006795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:26.006799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 93
2023-02-23 03:04:26.006803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:26.006807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 66
2023-02-23 03:04:26.006811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:26.006815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:26.006819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:26.006824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:26.006828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:26.006832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:26.006836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:26.006842: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:26.006846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:26.006850: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 44
2023-02-23 03:04:26.006854: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:26.006858: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:26.006871: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:26.006876: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:26.006880: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:26.006884: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:26.006888: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:26.006892: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:26.006896: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:26.006900: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 128, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0005 128 2]) is not valid.
[0.5 175 0.0005 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:04:29.046468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:29.046546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24374496392
MaxInUse:                  24487191464
NumAllocs:                   647051583
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:29.046615: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:29.046623: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:29.046627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:29.046631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 570
2023-02-23 03:04:29.046636: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:29.046640: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:29.046644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 190
2023-02-23 03:04:29.046648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 30
2023-02-23 03:04:29.046652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:29.046656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:29.046660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 244
2023-02-23 03:04:29.046664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:29.046669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:29.046673: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:29.046677: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:29.046681: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 94
2023-02-23 03:04:29.046685: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:29.046689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 69
2023-02-23 03:04:29.046693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 9
2023-02-23 03:04:29.046698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:29.046704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:29.046708: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:29.046713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:29.046717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:29.046721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:29.046742: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:29.046748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:29.046753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 46
2023-02-23 03:04:29.046757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 6
2023-02-23 03:04:29.046761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:29.046766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:29.046770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:29.046774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:29.046778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:29.046782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:29.046786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:29.046791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:29.046795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 16, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0005 16 1]) is not valid.
[0.6 180 0.00030000000000000003 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:04:32.104510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:32.104591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24376156568
MaxInUse:                  24487191464
NumAllocs:                   647051639
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:32.104663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:32.104672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:32.104677: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:32.104682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 570
2023-02-23 03:04:32.104686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:32.104691: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:32.104695: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 180
2023-02-23 03:04:32.104699: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 40
2023-02-23 03:04:32.104703: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:32.104707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:32.104711: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 244
2023-02-23 03:04:32.104715: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:32.104719: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:32.104724: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:32.104728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:32.104732: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 94
2023-02-23 03:04:32.104736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:32.104741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 66
2023-02-23 03:04:32.104746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 12
2023-02-23 03:04:32.104751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:32.104766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:32.104771: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:32.104775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:32.104779: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:32.104783: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:32.104787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:32.104791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:32.104795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 44
2023-02-23 03:04:32.104799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 8
2023-02-23 03:04:32.104803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:32.104807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:32.104811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:32.104816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:32.104820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:32.104824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:32.104828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:32.104832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:32.104836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 128, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.00030000000000000003 128 1]) is not valid.
[0.6 180 0.0005 256 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:04:35.215818: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:35.215899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24386410104
MaxInUse:                  24487191464
NumAllocs:                   647051695
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:35.215970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:35.215977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:35.215982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:35.215986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 577
2023-02-23 03:04:35.215990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:35.215994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:35.215998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 180
2023-02-23 03:04:35.216002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 50
2023-02-23 03:04:35.216006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:35.216010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:35.216014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 245
2023-02-23 03:04:35.216018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:35.216022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:35.216027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:35.216031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:35.216045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 95
2023-02-23 03:04:35.216049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:35.216053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 66
2023-02-23 03:04:35.216058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 15
2023-02-23 03:04:35.216062: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:35.216066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:35.216070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:35.216074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:35.216078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:35.216082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:35.216086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:35.216090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:35.216094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 44
2023-02-23 03:04:35.216098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 10
2023-02-23 03:04:35.216102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:35.216106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:35.216110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:35.216114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:35.216119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:35.216123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:35.216127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:35.216131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:35.216135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 256, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 256 1]) is not valid.
[0.6 180 0.0005 128 2] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:04:38.236252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:38.236337: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24386410104
MaxInUse:                  24487191464
NumAllocs:                   647051751
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:38.236409: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:38.236417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:38.236421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:38.236426: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 577
2023-02-23 03:04:38.236430: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:38.236434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:38.236438: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 180
2023-02-23 03:04:38.236442: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 50
2023-02-23 03:04:38.236446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:38.236450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:38.236465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 245
2023-02-23 03:04:38.236469: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:38.236474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:38.236478: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:38.236482: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:38.236486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 95
2023-02-23 03:04:38.236490: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:38.236494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 66
2023-02-23 03:04:38.236498: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 15
2023-02-23 03:04:38.236502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:38.236507: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:38.236511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:38.236515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:38.236519: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:38.236523: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:38.236527: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:38.236531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:38.236535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 44
2023-02-23 03:04:38.236539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 10
2023-02-23 03:04:38.236543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:38.236548: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:38.236552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:38.236556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:38.236560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:38.236564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:38.236568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:38.236572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:38.236576: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'logcosh', 128, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 128 2]) is not valid.
[0.6 180 0.0005 128 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:04:41.324614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:41.324939: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24396663640
MaxInUse:                  24487191464
NumAllocs:                   647051807
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:41.325011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:41.325018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:41.325023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:41.325027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 584
2023-02-23 03:04:41.325032: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:41.325045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:41.325050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 180
2023-02-23 03:04:41.325054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:04:41.325058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:41.325063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:41.325067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 246
2023-02-23 03:04:41.325071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:41.325075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:41.325079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:41.325083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:41.325087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 96
2023-02-23 03:04:41.325092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:41.325096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 66
2023-02-23 03:04:41.325100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:04:41.325104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:41.325108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:41.325112: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:41.325116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:41.325120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:41.325125: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:41.325129: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:41.325133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:41.325137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 44
2023-02-23 03:04:41.325141: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:04:41.325145: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:41.325149: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:41.325153: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:41.325157: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:41.325161: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:41.325166: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:41.325170: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:41.325174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:41.325178: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'logcosh', 128, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 128 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_8.pkl
[0.6 180 0.0007 128 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:04:47.291379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:04:47.291459: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24396663640
MaxInUse:                  24487191464
NumAllocs:                   647051863
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:04:47.291543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:04:47.291551: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:04:47.291555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:04:47.291560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 584
2023-02-23 03:04:47.291564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:04:47.291568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:04:47.291572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 180
2023-02-23 03:04:47.291576: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:04:47.291580: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:04:47.291584: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:04:47.291588: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 246
2023-02-23 03:04:47.291592: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:04:47.291596: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:04:47.291600: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:04:47.291604: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:04:47.291609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 96
2023-02-23 03:04:47.291613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:04:47.291617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 66
2023-02-23 03:04:47.291621: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:04:47.291625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:04:47.291629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:04:47.291633: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:04:47.291637: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:04:47.291641: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:04:47.291645: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:04:47.291649: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:04:47.291653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:04:47.291657: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 44
2023-02-23 03:04:47.291661: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:04:47.291665: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:04:47.291669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:04:47.291673: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:04:47.291677: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:04:47.291681: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:04:47.291686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:04:47.291692: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:04:47.291696: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:04:47.291700: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0007 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_9.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_10.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_11.pkl
[0.5 180 0.0007 128 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:01.237489: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:01.237804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24405257000
MaxInUse:                  24487191464
NumAllocs:                   647051919
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:01.237881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:01.237888: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:01.237893: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:01.237898: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 591
2023-02-23 03:05:01.237902: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:01.237906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:01.237910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 190
2023-02-23 03:05:01.237915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:01.237919: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:01.237923: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:01.237927: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 247
2023-02-23 03:05:01.237931: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:01.237936: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:01.237940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:01.237944: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:01.237948: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 97
2023-02-23 03:05:01.237953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:01.237957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 69
2023-02-23 03:05:01.237961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:01.237966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:01.237971: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:01.237976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:01.237980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:01.237984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:01.237988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:01.237992: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:01.237996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:01.238000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 46
2023-02-23 03:05:01.238004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:01.238022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:01.238027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:01.238031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:01.238035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:01.238039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:01.238043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:01.238047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:01.238051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:01.238056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 128 2]) is not valid.
[0.5 175 0.0005 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:05.573019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:05.573096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24405257000
MaxInUse:                  24487191464
NumAllocs:                   647051975
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:05.573172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:05.573179: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:05.573183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:05.573188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 591
2023-02-23 03:05:05.573192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:05.573196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:05.573200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 190
2023-02-23 03:05:05.573204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:05.573209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:05.573213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:05.573217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 247
2023-02-23 03:05:05.573221: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:05.573225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:05.573229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:05.573233: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:05.573237: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 97
2023-02-23 03:05:05.573241: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:05.573246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 69
2023-02-23 03:05:05.573250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:05.573254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:05.573258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:05.573262: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:05.573266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:05.573270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:05.573283: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:05.573287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:05.573292: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:05.573296: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 46
2023-02-23 03:05:05.573305: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:05.573310: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:05.573314: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:05.573318: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:05.573322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:05.573326: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:05.573330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:05.573334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:05.573338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:05.573342: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 64, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0005 64 1]) is not valid.
[0.5 180 0.0007 128 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:08.612062: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:08.612142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24413850360
MaxInUse:                  24487191464
NumAllocs:                   647052031
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:08.612215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:08.612221: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:08.612226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:08.612230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 598
2023-02-23 03:05:08.612235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:08.612239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:08.612243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 200
2023-02-23 03:05:08.612247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:08.612251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:08.612255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:08.612260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 248
2023-02-23 03:05:08.612264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:08.612268: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:08.612272: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:08.612276: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:08.612280: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 98
2023-02-23 03:05:08.612284: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:08.612288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 72
2023-02-23 03:05:08.612293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:08.612315: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:08.612320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:08.612324: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:08.612328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:08.612333: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:08.612337: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:08.612341: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:08.612345: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:08.612349: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 48
2023-02-23 03:05:08.612353: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:08.612357: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:08.612362: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:08.612366: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:08.612370: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:08.612374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:08.612378: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:08.612383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:08.612387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:08.612391: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 128, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 128 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_12.pkl
[0.4 180 0.0005 256 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:05:14.481557: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:14.481635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24412177008
MaxInUse:                  24487191464
NumAllocs:                   647052087
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:14.481709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:14.481716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:14.481721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:14.481725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 598
2023-02-23 03:05:14.481729: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:14.481733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 60
2023-02-23 03:05:14.481737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 190
2023-02-23 03:05:14.481741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:14.481745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:14.481749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:14.481753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 248
2023-02-23 03:05:14.481757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:14.481761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:14.481766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:14.481780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:14.481784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 98
2023-02-23 03:05:14.481788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-23 03:05:14.481792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 69
2023-02-23 03:05:14.481797: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:14.481801: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:14.481805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:14.481809: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:14.481813: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:14.481817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:14.481821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:14.481825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-23 03:05:14.481829: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:14.481833: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 46
2023-02-23 03:05:14.481837: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:14.481841: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:14.481845: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:14.481849: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:14.481853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:14.481858: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:14.481862: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:14.481866: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:14.481870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:14.481874: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 256, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0005 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_13.pkl
[0.5 180 0.00030000000000000003 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:20.470847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:20.470926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24413850360
MaxInUse:                  24487191464
NumAllocs:                   647052143
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:20.471001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:20.471008: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:20.471012: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:20.471016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 598
2023-02-23 03:05:20.471021: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:20.471025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:20.471029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 200
2023-02-23 03:05:20.471033: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:20.471048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:20.471052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:20.471056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 248
2023-02-23 03:05:20.471061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:20.471066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:20.471071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:20.471075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:20.471080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 98
2023-02-23 03:05:20.471084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:20.471088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 72
2023-02-23 03:05:20.471093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:20.471098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:20.471103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:20.471107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:20.471112: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:20.471117: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:20.471122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:20.471126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:20.471130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:20.471134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 48
2023-02-23 03:05:20.471138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:20.471142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:20.471146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:20.471151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:20.471155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:20.471159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:20.471163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:20.471167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:20.471171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:20.471175: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 32, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 32 1]) is not valid.
[0.5 180 0.00030000000000000003 32 1] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:23.520217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:23.520296: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24422443720
MaxInUse:                  24487191464
NumAllocs:                   647052199
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:23.520376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:23.520383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:23.520399: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:23.520404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 605
2023-02-23 03:05:23.520408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:23.520413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:23.520417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 210
2023-02-23 03:05:23.520421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:23.520425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:23.520429: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:23.520433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 249
2023-02-23 03:05:23.520437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:23.520441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:23.520445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:23.520449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:23.520453: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 99
2023-02-23 03:05:23.520457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:23.520462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 75
2023-02-23 03:05:23.520466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:23.520470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:23.520474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:23.520478: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:23.520482: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:23.520486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:23.520490: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:23.520494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:23.520498: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:23.520502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 50
2023-02-23 03:05:23.520506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:23.520510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:23.520514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:23.520518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:23.520522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:23.520526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:23.520530: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:23.520534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:23.520538: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:23.520542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 32, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 32 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_14.pkl
[0.5 180 0.00030000000000000003 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:29.338628: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:29.338935: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24422443720
MaxInUse:                  24487191464
NumAllocs:                   647052255
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:29.339028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:29.339036: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:29.339040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:29.339045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 605
2023-02-23 03:05:29.339049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:29.339053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:29.339057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 210
2023-02-23 03:05:29.339061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:29.339065: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:29.339069: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:29.339073: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 249
2023-02-23 03:05:29.339077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:29.339081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:29.339085: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:29.339090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:29.339094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 99
2023-02-23 03:05:29.339098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:29.339102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 75
2023-02-23 03:05:29.339106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:29.339110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:29.339114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:29.339118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:29.339122: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:29.339126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:29.339131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:29.339135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:29.339139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:29.339143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 50
2023-02-23 03:05:29.339147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:29.339151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:29.339155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:29.339159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:29.339163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:29.339170: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:29.339174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:29.339178: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:29.339182: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:29.339186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 64, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 64 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_15.pkl
[0.5 180 0.00030000000000000003 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:35.155493: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:35.155570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24431037080
MaxInUse:                  24487191464
NumAllocs:                   647052311
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:35.155645: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:35.155652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:35.155656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:35.155660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 612
2023-02-23 03:05:35.155665: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:35.155669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:35.155673: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 220
2023-02-23 03:05:35.155677: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:35.155681: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:35.155685: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:35.155689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 250
2023-02-23 03:05:35.155693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:35.155697: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:35.155702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:35.155706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:35.155710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 100
2023-02-23 03:05:35.155714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:35.155718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 78
2023-02-23 03:05:35.155722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:35.155726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:35.155730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:35.155734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:35.155738: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:35.155743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:35.155747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:35.155751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:35.155755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:35.155768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 52
2023-02-23 03:05:35.155772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:35.155777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:35.155781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:35.155785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:35.155789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:35.155793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:35.155797: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:35.155801: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:35.155805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:35.155809: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 256 1]) is not valid.
[0.5 180 0.00030000000000000003 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:38.262979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:38.263057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24431037080
MaxInUse:                  24487191464
NumAllocs:                   647052367
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:38.263132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:38.263138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:38.263143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:38.263147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 612
2023-02-23 03:05:38.263151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:38.263155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:38.263160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 220
2023-02-23 03:05:38.263164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 60
2023-02-23 03:05:38.263168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:38.263172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:38.263176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 250
2023-02-23 03:05:38.263180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:38.263184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:38.263188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:38.263193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:38.263198: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 100
2023-02-23 03:05:38.263203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:38.263207: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 78
2023-02-23 03:05:38.263211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-23 03:05:38.263215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:38.263220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:38.263225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:38.263240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:38.263246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:38.263250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:38.263254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:38.263258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:38.263263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 52
2023-02-23 03:05:38.263267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-23 03:05:38.263271: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:38.263275: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:38.263279: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:38.263283: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:38.263287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:38.263291: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:38.263295: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:38.263304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:38.263309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_16.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_17.pkl
[0.6 180 0.0005 32 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:05:47.077658: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:47.077737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24441290616
MaxInUse:                  24487191464
NumAllocs:                   647052423
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:47.077816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:47.077823: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:47.077828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:47.077832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 619
2023-02-23 03:05:47.077836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:47.077840: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:47.077844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 220
2023-02-23 03:05:47.077848: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 70
2023-02-23 03:05:47.077853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:47.077857: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:47.077861: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 251
2023-02-23 03:05:47.077865: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:47.077869: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:47.077873: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:47.077877: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:47.077893: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 101
2023-02-23 03:05:47.077898: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:47.077902: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 78
2023-02-23 03:05:47.077906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 21
2023-02-23 03:05:47.077910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:47.077914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:47.077918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:47.077922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:47.077926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:47.077930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:47.077935: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:47.077939: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:47.077943: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 52
2023-02-23 03:05:47.077947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 14
2023-02-23 03:05:47.077951: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:47.077955: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:47.077959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:47.077963: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:47.077967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:47.077971: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:47.077976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:47.077980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:47.077984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 32, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 32 1]) is not valid.
[0.5 175 0.0005 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:50.205528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:50.205609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24449883976
MaxInUse:                  24487191464
NumAllocs:                   647052479
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:50.205682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:50.205689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:50.205694: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:50.205698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 626
2023-02-23 03:05:50.205702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:50.205707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:50.205711: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 230
2023-02-23 03:05:50.205715: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 70
2023-02-23 03:05:50.205719: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:50.205723: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:50.205736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 252
2023-02-23 03:05:50.205741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:50.205745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:50.205749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:50.205753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:50.205757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 102
2023-02-23 03:05:50.205761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:50.205765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 81
2023-02-23 03:05:50.205769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 21
2023-02-23 03:05:50.205774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:50.205778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:50.205782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:50.205786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:50.205790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:50.205794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:50.205798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:50.205802: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:50.205806: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 54
2023-02-23 03:05:50.205810: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 14
2023-02-23 03:05:50.205814: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:50.205818: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:50.205822: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:50.205826: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:50.205831: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:50.205835: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:50.205839: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:50.205843: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:50.205847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 32, 175, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0005 32 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_18.pkl
[0.5 175 0.0007 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:05:55.936620: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:55.936698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24449883976
MaxInUse:                  24487191464
NumAllocs:                   647052535
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:55.936774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:55.936781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:55.936785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:55.936790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 626
2023-02-23 03:05:55.936794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:55.936808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:05:55.936813: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 230
2023-02-23 03:05:55.936817: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 70
2023-02-23 03:05:55.936821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:55.936825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:55.936830: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 252
2023-02-23 03:05:55.936834: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:55.936838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:55.936842: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:55.936846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:55.936850: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 102
2023-02-23 03:05:55.936854: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:05:55.936858: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 81
2023-02-23 03:05:55.936862: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 21
2023-02-23 03:05:55.936867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:55.936871: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:55.936875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:55.936879: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:55.936883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:55.936887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:55.936891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:05:55.936895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:55.936899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 54
2023-02-23 03:05:55.936903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 14
2023-02-23 03:05:55.936907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:55.936911: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:55.936915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:55.936919: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:55.936924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:55.936928: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:55.936932: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:55.936936: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:55.936940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 64, 175, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 175 0.0007 64 1]) is not valid.
[0.4 180 0.0005 64 1] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:05:59.067173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:05:59.067613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24456803984
MaxInUse:                  24487191464
NumAllocs:                   647052591
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:05:59.067689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:05:59.067707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:05:59.067713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:05:59.067717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 633
2023-02-23 03:05:59.067721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:05:59.067725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 60
2023-02-23 03:05:59.067729: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 230
2023-02-23 03:05:59.067734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 70
2023-02-23 03:05:59.067738: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:05:59.067742: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:05:59.067746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 253
2023-02-23 03:05:59.067750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:05:59.067754: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:05:59.067758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:05:59.067762: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:05:59.067766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 103
2023-02-23 03:05:59.067771: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-23 03:05:59.067775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 81
2023-02-23 03:05:59.067779: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 21
2023-02-23 03:05:59.067783: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:05:59.067787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:05:59.067791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:05:59.067795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:05:59.067799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:05:59.067803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:05:59.067807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-23 03:05:59.067811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:05:59.067815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 54
2023-02-23 03:05:59.067820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 14
2023-02-23 03:05:59.067824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:05:59.067828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:05:59.067832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:05:59.067836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:05:59.067840: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:05:59.067844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:05:59.067848: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:05:59.067854: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:05:59.067859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 64, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0005 64 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_19.pkl
[0.4 180 0.00030000000000000003 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:06:04.860412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:06:04.860505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24456803984
MaxInUse:                  24487191464
NumAllocs:                   647052647
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:06:04.860581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:06:04.860589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:06:04.860593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:06:04.860597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 633
2023-02-23 03:06:04.860602: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:06:04.860606: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 60
2023-02-23 03:06:04.860610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 230
2023-02-23 03:06:04.860614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 70
2023-02-23 03:06:04.860618: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:06:04.860622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:06:04.860626: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 253
2023-02-23 03:06:04.860630: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:06:04.860635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:06:04.860639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:06:04.860643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:06:04.860647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 103
2023-02-23 03:06:04.860651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-23 03:06:04.860655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 81
2023-02-23 03:06:04.860659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 21
2023-02-23 03:06:04.860664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:06:04.860668: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:06:04.860672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:06:04.860676: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:06:04.860680: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:06:04.860684: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:06:04.860688: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-23 03:06:04.860692: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:06:04.860697: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 54
2023-02-23 03:06:04.860701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 14
2023-02-23 03:06:04.860705: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:06:04.860709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:06:04.860718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:06:04.860723: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:06:04.860727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:06:04.860731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:06:04.860735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:06:04.860739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:06:04.860743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 64, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.00030000000000000003 64 1]) is not valid.
[0.5 180 0.0005 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:06:07.943559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:06:07.943640: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24458477336
MaxInUse:                  24487191464
NumAllocs:                   647052703
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:06:07.943715: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:06:07.943723: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:06:07.943729: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:06:07.943733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 633
2023-02-23 03:06:07.943737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:06:07.943741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:06:07.943745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 240
2023-02-23 03:06:07.943750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 70
2023-02-23 03:06:07.943754: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:06:07.943758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:06:07.943762: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 253
2023-02-23 03:06:07.943766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:06:07.943770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:06:07.943774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:06:07.943778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:06:07.943783: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 103
2023-02-23 03:06:07.943787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:06:07.943791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 84
2023-02-23 03:06:07.943795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 21
2023-02-23 03:06:07.943799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:06:07.943803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:06:07.943807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:06:07.943811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:06:07.943816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:06:07.943820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:06:07.943834: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:06:07.943838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:06:07.943842: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 56
2023-02-23 03:06:07.943846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 14
2023-02-23 03:06:07.943851: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:06:07.943855: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:06:07.943859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:06:07.943863: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:06:07.943867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:06:07.943871: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:06:07.943875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:06:07.943880: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:06:07.943884: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 64, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 64 2]) is not valid.
[0.5 180 0.0007 64 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:06:11.000486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:06:11.000570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24467070696
MaxInUse:                  24487191464
NumAllocs:                   647052759
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:06:11.000644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:06:11.000650: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:06:11.000655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:06:11.000660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 640
2023-02-23 03:06:11.000664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:06:11.000668: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:06:11.000672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 250
2023-02-23 03:06:11.000676: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 70
2023-02-23 03:06:11.000680: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:06:11.000685: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:06:11.000689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 254
2023-02-23 03:06:11.000693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:06:11.000697: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:06:11.000701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:06:11.000705: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:06:11.000709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 104
2023-02-23 03:06:11.000714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:06:11.000718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 87
2023-02-23 03:06:11.000722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 21
2023-02-23 03:06:11.000726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:06:11.000742: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:06:11.000747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:06:11.000751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:06:11.000755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:06:11.000759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:06:11.000763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:06:11.000767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:06:11.000771: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 58
2023-02-23 03:06:11.000776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 14
2023-02-23 03:06:11.000780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:06:11.000784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:06:11.000788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:06:11.000792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:06:11.000796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:06:11.000800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:06:11.000804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:06:11.000808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:06:11.000813: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 64, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 64 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_20.pkl
[0.6 180 0.0005 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:06:19.405757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:06:19.405836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24468730872
MaxInUse:                  24487191464
NumAllocs:                   647052815
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:06:19.405920: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:06:19.405927: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:06:19.405932: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:06:19.405936: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 640
2023-02-23 03:06:19.405940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:06:19.405945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:06:19.405949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 240
2023-02-23 03:06:19.405953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 80
2023-02-23 03:06:19.405957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:06:19.405961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:06:19.405965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 254
2023-02-23 03:06:19.405969: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:06:19.405973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:06:19.405977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:06:19.405982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:06:19.405996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 104
2023-02-23 03:06:19.406001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:06:19.406005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 84
2023-02-23 03:06:19.406009: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 24
2023-02-23 03:06:19.406013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:06:19.406017: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:06:19.406021: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:06:19.406025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:06:19.406030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:06:19.406034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:06:19.406038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:06:19.406042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:06:19.406046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 56
2023-02-23 03:06:19.406050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 16
2023-02-23 03:06:19.406054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:06:19.406058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:06:19.406062: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:06:19.406066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:06:19.406070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:06:19.406074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:06:19.406078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:06:19.406082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:06:19.406086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 64, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 64 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_21.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_22.pkl
[0.5 180 0.00030000000000000003 16 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:06:27.877793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:06:27.877870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24477324232
MaxInUse:                  24487191464
NumAllocs:                   647052871
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:06:27.877946: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:06:27.877953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:06:27.877958: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:06:27.877962: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 647
2023-02-23 03:06:27.877966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:06:27.877971: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:06:27.877975: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 250
2023-02-23 03:06:27.877979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 80
2023-02-23 03:06:27.877983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:06:27.877997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:06:27.878002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 255
2023-02-23 03:06:27.878006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:06:27.878010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:06:27.878014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:06:27.878019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:06:27.878023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 105
2023-02-23 03:06:27.878027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:06:27.878031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 87
2023-02-23 03:06:27.878035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 24
2023-02-23 03:06:27.878039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:06:27.878044: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:06:27.878048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:06:27.878052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:06:27.878056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:06:27.878060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:06:27.878064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:06:27.878068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:06:27.878072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 58
2023-02-23 03:06:27.878076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 16
2023-02-23 03:06:27.878080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:06:27.878084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:06:27.878089: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:06:27.878093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:06:27.878097: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:06:27.878101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:06:27.878105: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:06:27.878109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:06:27.878113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 16, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.00030000000000000003 16 1]) is not valid.
[0.4 180 0.0005 16 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:06:32.712799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:06:32.712873: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24484244240
MaxInUse:                  24489345760
NumAllocs:                   647052927
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:06:32.712948: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:06:32.712954: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:06:32.712959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:06:32.712971: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 654
2023-02-23 03:06:32.712976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:06:32.712980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 60
2023-02-23 03:06:32.712985: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 250
2023-02-23 03:06:32.712989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 80
2023-02-23 03:06:32.712993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:06:32.712997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:06:32.713001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 256
2023-02-23 03:06:32.713005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:06:32.713009: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:06:32.713014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:06:32.713018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:06:32.713022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 106
2023-02-23 03:06:32.713026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-23 03:06:32.713030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 87
2023-02-23 03:06:32.713034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 24
2023-02-23 03:06:32.713038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:06:32.713043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:06:32.713047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:06:32.713051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:06:32.713055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:06:32.713059: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:06:32.713063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-23 03:06:32.713067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:06:32.713071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 58
2023-02-23 03:06:32.713076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 16
2023-02-23 03:06:32.713080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:06:32.713084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:06:32.713088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:06:32.713092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:06:32.713096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:06:32.713100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:06:32.713104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:06:32.713108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:06:32.713113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 16, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0005 16 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_23.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_24.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_25.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_26.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_27.pkl
[0.5 180 0.0007 16 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:06:50.625021: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:06:50.625109: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24485917592
MaxInUse:                  24492303336
NumAllocs:                   647052983
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:06:50.625187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:06:50.625194: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:06:50.625199: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:06:50.625203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 654
2023-02-23 03:06:50.625207: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:06:50.625211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:06:50.625216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 260
2023-02-23 03:06:50.625220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 80
2023-02-23 03:06:50.625224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:06:50.625228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:06:50.625232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 256
2023-02-23 03:06:50.625236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:06:50.625240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:06:50.625245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:06:50.625249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:06:50.625253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 106
2023-02-23 03:06:50.625257: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:06:50.625261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 90
2023-02-23 03:06:50.625265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 24
2023-02-23 03:06:50.625269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:06:50.625274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:06:50.625278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:06:50.625282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:06:50.625286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:06:50.625290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:06:50.625294: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:06:50.625303: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:06:50.625308: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 60
2023-02-23 03:06:50.625312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 16
2023-02-23 03:06:50.625316: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:06:50.625320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:06:50.625324: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:06:50.625330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:06:50.625335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:06:50.625339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:06:50.625343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:06:50.625347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:06:50.625351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 16, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 16 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_28.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_29.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_30.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_31.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_32.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_33.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_34.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_35.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_36.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_37.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_38.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_39.pkl
[0.5 180 0.0005 32 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:07:31.025678: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:07:31.025758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24485917592
MaxInUse:                  24492303336
NumAllocs:                   647053039
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:07:31.025837: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:07:31.025844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:07:31.025848: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:07:31.025853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 654
2023-02-23 03:07:31.025857: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:07:31.025861: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:07:31.025865: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 260
2023-02-23 03:07:31.025869: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 80
2023-02-23 03:07:31.025873: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:07:31.025878: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:07:31.025882: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 256
2023-02-23 03:07:31.025886: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:07:31.025890: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:07:31.025894: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:07:31.025898: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:07:31.025903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 106
2023-02-23 03:07:31.025907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:07:31.025911: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 90
2023-02-23 03:07:31.025915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 24
2023-02-23 03:07:31.025919: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:07:31.025934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:07:31.025939: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:07:31.025943: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:07:31.025947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:07:31.025952: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:07:31.025956: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:07:31.025960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:07:31.025964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 60
2023-02-23 03:07:31.025968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 16
2023-02-23 03:07:31.025972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:07:31.025976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:07:31.025980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:07:31.025984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:07:31.025989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:07:31.025993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:07:31.025997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:07:31.026002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:07:31.026007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 32, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0005 32 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_40.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_41.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_42.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_43.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_44.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_45.pkl
[0.6 180 0.0007 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:07:53.766511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:07:53.766880: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24487577768
MaxInUse:                  24495237624
NumAllocs:                   647053095
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:07:53.766958: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:07:53.766965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:07:53.766970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:07:53.766974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 654
2023-02-23 03:07:53.766979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:07:53.766983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:07:53.766987: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 250
2023-02-23 03:07:53.766991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 90
2023-02-23 03:07:53.766995: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:07:53.766999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:07:53.767003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 256
2023-02-23 03:07:53.767007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:07:53.767024: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:07:53.767029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:07:53.767033: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:07:53.767037: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 106
2023-02-23 03:07:53.767041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:07:53.767045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 87
2023-02-23 03:07:53.767050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 27
2023-02-23 03:07:53.767054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:07:53.767058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:07:53.767062: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:07:53.767066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:07:53.767070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:07:53.767074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:07:53.767078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:07:53.767082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:07:53.767086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 58
2023-02-23 03:07:53.767091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 18
2023-02-23 03:07:53.767095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:07:53.767099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:07:53.767103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:07:53.767107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:07:53.767111: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:07:53.767115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:07:53.767119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:07:53.767123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:07:53.767127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 64, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0007 64 1]) is not valid.
[0.6 180 0.00030000000000000003 64 1] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:07:57.548029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:07:57.548107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24497831304
MaxInUse:                  24505491160
NumAllocs:                   647053151
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:07:57.548184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:07:57.548191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:07:57.548196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:07:57.548200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 661
2023-02-23 03:07:57.548204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:07:57.548209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 50
2023-02-23 03:07:57.548214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 250
2023-02-23 03:07:57.548229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 100
2023-02-23 03:07:57.548234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:07:57.548238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:07:57.548242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 257
2023-02-23 03:07:57.548246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:07:57.548250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:07:57.548254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:07:57.548259: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:07:57.548263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 107
2023-02-23 03:07:57.548267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 15
2023-02-23 03:07:57.548271: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 87
2023-02-23 03:07:57.548275: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 30
2023-02-23 03:07:57.548279: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:07:57.548283: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:07:57.548287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:07:57.548291: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:07:57.548295: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:07:57.548304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:07:57.548308: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 10
2023-02-23 03:07:57.548312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:07:57.548316: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 58
2023-02-23 03:07:57.548320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 20
2023-02-23 03:07:57.548324: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:07:57.548328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:07:57.548332: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:07:57.548336: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:07:57.548340: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:07:57.548344: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:07:57.548349: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:07:57.548353: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:07:57.548357: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 64, 180, 0.00030000000000000003, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.00030000000000000003 64 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_46.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_47.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_48.pkl
[0.4 180 0.0007 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
2023-02-23 03:08:11.282290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 148570112/25447170048
2023-02-23 03:08:11.282370: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24494497776
MaxInUse:                  24505491160
NumAllocs:                   647053207
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:08:11.282462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:08:11.282470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:08:11.282475: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:08:11.282479: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 661
2023-02-23 03:08:11.282483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:08:11.282487: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 60
2023-02-23 03:08:11.282491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 250
2023-02-23 03:08:11.282495: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 90
2023-02-23 03:08:11.282499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:08:11.282503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:08:11.282508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 257
2023-02-23 03:08:11.282512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:08:11.282516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:08:11.282520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:08:11.282524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:08:11.282528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 107
2023-02-23 03:08:11.282532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-23 03:08:11.282536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 87
2023-02-23 03:08:11.282540: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 27
2023-02-23 03:08:11.282544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:08:11.282548: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:08:11.282552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:08:11.282557: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:08:11.282561: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:08:11.282565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:08:11.282569: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-23 03:08:11.282573: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:08:11.282577: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 58
2023-02-23 03:08:11.282581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 18
2023-02-23 03:08:11.282585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:08:11.282589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:08:11.282593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:08:11.282597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:08:11.282601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:08:11.282605: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:08:11.282609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:08:11.282622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:08:11.282627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 64, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 180 0.0007 64 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_49.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_50.pkl
[0.5 180 0.0007 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-23 03:08:21.759966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 115015680/25447170048
2023-02-23 03:08:21.760048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24503091136
MaxInUse:                  24509476880
NumAllocs:                   647053263
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:08:21.760127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:08:21.760135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:08:21.760139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:08:21.760143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 668
2023-02-23 03:08:21.760148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:08:21.760152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 60
2023-02-23 03:08:21.760156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 260
2023-02-23 03:08:21.760160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 90
2023-02-23 03:08:21.760164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:08:21.760168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:08:21.760172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 258
2023-02-23 03:08:21.760176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:08:21.760180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:08:21.760184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:08:21.760189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:08:21.760193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 108
2023-02-23 03:08:21.760197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-23 03:08:21.760201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 90
2023-02-23 03:08:21.760205: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 27
2023-02-23 03:08:21.760209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:08:21.760213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:08:21.760217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:08:21.760221: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:08:21.760225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:08:21.760230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:08:21.760234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-23 03:08:21.760238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:08:21.760242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 60
2023-02-23 03:08:21.760246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 18
2023-02-23 03:08:21.760250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:08:21.760257: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:08:21.760261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:08:21.760265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:08:21.760270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:08:21.760274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:08:21.760278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:08:21.760282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:08:21.760286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 32, 180, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.0007 32 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_51.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_52.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_53.pkl
[0.6 180 0.0005 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 03:08:34.841039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 115015680/25447170048
2023-02-23 03:08:34.841114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24504751312
MaxInUse:                  24512411168
NumAllocs:                   647053319
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 03:08:34.841198: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 03:08:34.841206: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-23 03:08:34.841211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-23 03:08:34.841215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 668
2023-02-23 03:08:34.841220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 03:08:34.841224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 60
2023-02-23 03:08:34.841228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 250
2023-02-23 03:08:34.841232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 100
2023-02-23 03:08:34.841236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4044, 20
2023-02-23 03:08:34.841240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 42
2023-02-23 03:08:34.841244: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 258
2023-02-23 03:08:34.841248: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 10
2023-02-23 03:08:34.841252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 64
2023-02-23 03:08:34.841256: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 142
2023-02-23 03:08:34.841261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 66
2023-02-23 03:08:34.841265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 108
2023-02-23 03:08:34.841269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-23 03:08:34.841273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 87
2023-02-23 03:08:34.841277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 30
2023-02-23 03:08:34.841281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1019088, 6
2023-02-23 03:08:34.841285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1146096, 15
2023-02-23 03:08:34.841289: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 57
2023-02-23 03:08:34.841293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 3
2023-02-23 03:08:34.841302: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 24
2023-02-23 03:08:34.841315: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 57
2023-02-23 03:08:34.841319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-23 03:08:34.841323: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 27
2023-02-23 03:08:34.841328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 58
2023-02-23 03:08:34.841332: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 20
2023-02-23 03:08:34.841336: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5111616, 4
2023-02-23 03:08:34.841340: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 10
2023-02-23 03:08:34.841344: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 38
2023-02-23 03:08:34.841348: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 2
2023-02-23 03:08:34.841352: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 16
2023-02-23 03:08:34.841356: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 38
2023-02-23 03:08:34.841360: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 18
2023-02-23 03:08:34.841364: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-23 03:08:34.841368: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'logcosh', 64, 180, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 180 0.0005 64 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_54.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_55.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_56.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_57.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_58.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_59.pkl
Saved GA instance to file: ./tmp//ga_instance_generation_60.pkl
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:3349: UserWarning: Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.
  warnings.warn("Please use the plot_fitness() method instead of plot_result(). The plot_result() method will be removed in the future.")
Saved GA instance to file: ./tmp//ga_instance_generation_60.pkl
Best solutions :  [[0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]
 [0.5 180 0.0005 128 1]]
Best solutions fitness :  [29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474, 29.165915337825474]
Solution 0: [1.0, 90, 0.0005, 64, 2], Fitness: 21.813024857501702
Solution 1: [2.5, 120, 0.001, 32, 1], Fitness: 26.170239164208397
Solution 2: [2.0, 180, 0.0005, 128, 1], Fitness: 28.118637414530014
Solution 3: [0.5, 60, 0.0005, 16, 1], Fitness: 22.87879182838031
Solution 4: [2.0, 30, 0.001, 16, 1], Fitness: 22.226520630329137
Solution 5: [2.5, 90, 0.001, 32, 1], Fitness: 26.091199507795867
Solution 6: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 7: [2.5, 150, 0.002, 16, 1], Fitness: 22.196849500033093
Solution 8: [1.0, 210, 0.0005, 128, 2], Fitness: 22.945244013044224
Solution 9: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 10: [2.0, 150, 0.001, 256, 1], Fitness: 27.963263355224495
Solution 11: [2.0, 60, 0.0005, 16, 1], Fitness: 21.74383831358746
Solution 12: [1.0, 90, 0.002, 16, 2], Fitness: 19.71841845769708
Solution 13: [0.5, 180, 0.002, 128, 2], Fitness: 22.81215540932427
Solution 14: [0.5, 90, 0.002, 256, 2], Fitness: 22.221435035640475
Solution 15: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 16: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 17: [2.0, 180, 0.00030000000000000003, 16, 1], Fitness: 22.93443838234975
Solution 18: [2.0, 35, 0.0008, 64, 2], Fitness: 21.14378663040124
Solution 19: [1.0, 90, 0.0007, 256, 2], Fitness: 21.989598132908384
Solution 20: [1.0, 210, 0.0005, 128, 1], Fitness: 29.106259093204372
Solution 21: [1.9, 30, 0.001, 16, 1], Fitness: 21.785199878257913
Solution 22: [1.9, 30, 0.0012000000000000001, 64, 2], Fitness: 20.628017680072947
Solution 23: [0.9, 85, 0.0005, 64, 2], Fitness: 22.002239966593844
Solution 24: [1.0, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 25: [2.0, 180, 0.001, 16, 1], Fitness: 9.99999999999e-07
Solution 26: [2.0, 85, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 27: [1.1, 90, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 28: [1.0, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 29: [2.0, 180, 0.001, 16, 1], Fitness: 9.99999999999e-07
Solution 30: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 31: [1.0, 210, 0.0005, 128, 1], Fitness: 29.106259093204372
Solution 32: [0.9, 85, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 33: [1.0, 210, 0.0007, 64, 2], Fitness: 9.99999999999e-07
Solution 34: [1.9, 85, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 35: [1.0, 85, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 36: [0.9, 85, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 37: [1.0, 25, 0.0014000000000000002, 64, 2], Fitness: 9.99999999999e-07
Solution 38: [1.7999999999999998, 35, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 39: [0.9, 80, 0.0005, 256, 2], Fitness: 9.99999999999e-07
Solution 40: [0.8, 85, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 41: [1.1, 210, 0.0012000000000000001, 64, 2], Fitness: 9.99999999999e-07
Solution 42: [1.9, 30, 0.0012000000000000001, 128, 2], Fitness: 9.99999999999e-07
Solution 43: [0.9, 85, 0.00030000000000000003, 64, 1], Fitness: 9.99999999999e-07
Solution 44: [0.8, 210, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 45: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 46: [1.0, 210, 0.0005, 128, 1], Fitness: 29.106259093204372
Solution 47: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 48: [0.5, 180, 0.0005, 16, 2], Fitness: 9.99999999999e-07
Solution 49: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 50: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 51: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 52: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 53: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 54: [0.4, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 55: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 56: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 57: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 58: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 59: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 60: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 61: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 62: [0.6, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 63: [0.5, 205, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 64: [1.0, 210, 0.0005, 128, 2], Fitness: 22.945244013044224
Solution 65: [1.0, 205, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 66: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 67: [0.4, 210, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 68: [1.0, 210, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 69: [1.0, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 70: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 71: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 72: [1.0, 210, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 73: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 74: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 75: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 76: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 77: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 78: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 79: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 80: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 81: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 82: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 83: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 84: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 85: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 86: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 87: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 88: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 89: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 90: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 91: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 92: [0.5, 175, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 93: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 94: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 95: [0.5, 180, 0.0005, 256, 2], Fitness: 9.99999999999e-07
Solution 96: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 97: [0.5, 180, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 98: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 99: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 100: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 101: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 102: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 103: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 104: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 105: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 106: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 107: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 108: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 109: [0.5, 180, 0.00030000000000000003, 128, 2], Fitness: 9.99999999999e-07
Solution 110: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 111: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 112: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 113: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 114: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 115: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 116: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 117: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 118: [0.5, 180, 0.0007, 256, 1], Fitness: 9.99999999999e-07
Solution 119: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 120: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 121: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 122: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 123: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 124: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 125: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 126: [0.5, 175, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 127: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 128: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 129: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 130: [0.6, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 131: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 132: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 133: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 134: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 135: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 136: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 137: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 138: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 139: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 140: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 141: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 142: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 143: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 144: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 145: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 146: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 147: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 148: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 149: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 150: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 151: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 152: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 153: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 154: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 155: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 156: [0.5, 175, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 157: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 158: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 159: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 160: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 161: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 162: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 163: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 164: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 165: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 166: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 167: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 168: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 169: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 170: [0.5, 175, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 171: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 172: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 173: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 174: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 175: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 176: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 177: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 178: [0.5, 175, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 179: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 180: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 181: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 182: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 183: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 184: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 185: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 186: [0.5, 175, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 187: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 188: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 189: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 190: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 191: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 192: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 193: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 194: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 195: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 196: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 197: [0.5, 180, 0.0005, 16, 2], Fitness: 9.99999999999e-07
Solution 198: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 199: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 200: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 201: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 202: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 203: [0.5, 175, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 204: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 205: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 206: [0.4, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 207: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 208: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 209: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 210: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 211: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 212: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 213: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 214: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 215: [0.5, 180, 0.00030000000000000003, 32, 1], Fitness: 9.99999999999e-07
Solution 216: [0.5, 175, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 217: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 218: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 219: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 220: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 221: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 222: [0.5, 180, 0.00030000000000000003, 32, 1], Fitness: 9.99999999999e-07
Solution 223: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 224: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 225: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 226: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 227: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 228: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 229: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 230: [0.5, 180, 0.00030000000000000003, 64, 1], Fitness: 9.99999999999e-07
Solution 231: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 232: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 233: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 234: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 235: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 236: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 237: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 238: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 239: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 240: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 241: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 242: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 243: [0.5, 180, 0.00030000000000000003, 256, 1], Fitness: 9.99999999999e-07
Solution 244: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 245: [0.5, 180, 0.00030000000000000003, 256, 1], Fitness: 9.99999999999e-07
Solution 246: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 247: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 248: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 249: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 250: [0.5, 180, 0.0005, 16, 2], Fitness: 9.99999999999e-07
Solution 251: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 252: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 253: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 254: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 255: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 256: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 257: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 258: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 259: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 260: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 261: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 262: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 263: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 264: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 265: [0.5, 180, 0.00030000000000000003, 128, 2], Fitness: 9.99999999999e-07
Solution 266: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 267: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 268: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 269: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 270: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 271: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 272: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 273: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 274: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 275: [0.6, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 276: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 277: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 278: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 279: [0.5, 175, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 280: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 281: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 282: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 283: [0.6, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 284: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 285: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 286: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 287: [0.5, 175, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 288: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 289: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 290: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 291: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 292: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 293: [0.6, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 294: [0.5, 175, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 295: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 296: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 297: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 298: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 299: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 300: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 301: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 302: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 303: [0.5, 180, 0.00030000000000000003, 64, 1], Fitness: 9.99999999999e-07
Solution 304: [0.4, 180, 0.00030000000000000003, 64, 1], Fitness: 9.99999999999e-07
Solution 305: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 306: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 307: [0.4, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 308: [0.5, 180, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 309: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 310: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 311: [0.5, 180, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 312: [0.5, 180, 0.0007, 64, 2], Fitness: 9.99999999999e-07
Solution 313: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 314: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 315: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 316: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 317: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 318: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 319: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 320: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 321: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 322: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 323: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 324: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 325: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 326: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 327: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 328: [0.5, 175, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 329: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 330: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 331: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 332: [0.5, 180, 0.0007, 256, 1], Fitness: 9.99999999999e-07
Solution 333: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 334: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 335: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 336: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 337: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 338: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 339: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 340: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 341: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 342: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 343: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 344: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 345: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 346: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 347: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 348: [0.5, 180, 0.00030000000000000003, 16, 1], Fitness: 9.99999999999e-07
Solution 349: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 350: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 351: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 352: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 353: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 354: [0.4, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 355: [0.4, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 356: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 357: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 358: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 359: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 360: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 361: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 362: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 363: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 364: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 365: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 366: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 367: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 368: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 369: [0.5, 180, 0.0007, 256, 1], Fitness: 9.99999999999e-07
Solution 370: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 371: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 372: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 373: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 374: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 375: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 376: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 377: [0.6, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 378: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 379: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 380: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 381: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 382: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 383: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 384: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 385: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 386: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 387: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 388: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 389: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 390: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 391: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 392: [0.5, 175, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 393: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 394: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 395: [0.5, 175, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 396: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 397: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 398: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 399: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 400: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 401: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 402: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 403: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 404: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 405: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 406: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 407: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 408: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 409: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 410: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 411: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 412: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 413: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 414: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 415: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 416: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 417: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 418: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 419: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 420: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 421: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 422: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 423: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 424: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 425: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 426: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 427: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 428: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 429: [0.4, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 430: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 431: [0.5, 180, 0.0007, 16, 1], Fitness: 9.99999999999e-07
Solution 432: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 433: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 434: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 435: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 436: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 437: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 438: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 439: [0.5, 180, 0.00030000000000000003, 128, 2], Fitness: 9.99999999999e-07
Solution 440: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 441: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 442: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 443: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 444: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 445: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 446: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 447: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 448: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 449: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 450: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 451: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 452: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 453: [0.6, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 454: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 455: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 456: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 457: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 458: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 459: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 460: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 461: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 462: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 463: [0.5, 180, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 464: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 465: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 466: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 467: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 468: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 469: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 470: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 471: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 472: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 473: [0.5, 175, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 474: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 475: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 476: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 477: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 478: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 479: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 480: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 481: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 482: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 483: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 484: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 485: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 486: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 487: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 488: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 489: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 490: [0.4, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 491: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 492: [0.5, 175, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 493: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 494: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 495: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 496: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 497: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 498: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 499: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 500: [0.5, 180, 0.0007, 256, 1], Fitness: 9.99999999999e-07
Solution 501: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 502: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 503: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 504: [0.5, 180, 0.00030000000000000003, 32, 1], Fitness: 9.99999999999e-07
Solution 505: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 506: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 507: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 508: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 509: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 510: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 511: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 512: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 513: [0.5, 175, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 514: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 515: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 516: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 517: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 518: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 519: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 520: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 521: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 522: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 523: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 524: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 525: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 526: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 527: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 528: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 529: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 530: [0.5, 175, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 531: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 532: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 533: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 534: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 535: [0.4, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 536: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 537: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 538: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 539: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 540: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 541: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 542: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 543: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 544: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 545: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 546: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 547: [0.4, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 548: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 549: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 550: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 551: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 552: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 553: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 554: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 555: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 556: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 557: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 558: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 559: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 560: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 561: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 562: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 563: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 564: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 565: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 566: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 567: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 568: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 569: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 570: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 571: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 572: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 573: [0.6, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 574: [0.4, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 575: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 576: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 577: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 578: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 579: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 580: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 581: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 582: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 583: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 584: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 585: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 586: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 587: [0.4, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 588: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 589: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 590: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 591: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 592: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 593: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 594: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 595: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 596: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 597: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 598: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 599: [0.5, 180, 0.00030000000000000003, 128, 2], Fitness: 9.99999999999e-07
Solution 600: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 601: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 602: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 603: [0.5, 180, 0.0005, 32, 2], Fitness: 9.99999999999e-07
Solution 604: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 605: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 606: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 607: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 608: [0.5, 180, 0.0007, 256, 1], Fitness: 9.99999999999e-07
Solution 609: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 610: [0.5, 175, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 611: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 612: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 613: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 614: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 615: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 616: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 617: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 618: [0.6, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 619: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 620: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 621: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 622: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 623: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 624: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 625: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 626: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 627: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 628: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 629: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 630: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 631: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 632: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 633: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 634: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 635: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 636: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 637: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 638: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 639: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 640: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 641: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 642: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 643: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 644: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 645: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 646: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 647: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 648: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 649: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 650: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 651: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 652: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 653: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 654: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 655: [0.4, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 656: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 657: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 658: [0.5, 180, 0.00030000000000000003, 128, 2], Fitness: 9.99999999999e-07
Solution 659: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 660: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 661: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 662: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 663: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 664: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 665: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 666: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 667: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 668: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 669: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 670: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 671: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 672: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 673: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 674: [0.4, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 675: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 676: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 677: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 678: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 679: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 680: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 681: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 682: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 683: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 684: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 685: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 686: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 687: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 688: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 689: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 690: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 691: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 692: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 693: [0.6, 180, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 694: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 695: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 696: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 697: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 698: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 699: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 700: [0.4, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 701: [0.5, 180, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 702: [0.6, 180, 0.00030000000000000003, 64, 1], Fitness: 9.99999999999e-07
Solution 703: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 704: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 705: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 706: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 707: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 708: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 709: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 710: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 711: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 712: [0.5, 180, 0.0005, 32, 2], Fitness: 9.99999999999e-07
Solution 713: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 714: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 715: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 716: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 717: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 718: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 719: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 720: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 721: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 722: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 723: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 724: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 725: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 726: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 727: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 728: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 729: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 730: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 731: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 732: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 733: [0.5, 175, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 734: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 735: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 736: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 737: [0.4, 180, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 738: [0.5, 180, 0.00030000000000000003, 32, 1], Fitness: 9.99999999999e-07
Solution 739: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 740: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 741: [0.5, 180, 0.0007, 64, 2], Fitness: 9.99999999999e-07
Solution 742: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 743: [0.6, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 744: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 745: [0.4, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 746: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 747: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 748: [0.5, 175, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 749: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 750: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 751: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 752: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 753: [0.5, 180, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 754: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 755: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 756: [0.4, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 757: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 758: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 759: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 760: [0.4, 180, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 761: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 762: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 763: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 764: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 765: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 766: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 767: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 768: [0.5, 180, 0.00030000000000000003, 64, 1], Fitness: 9.99999999999e-07
Solution 769: [0.5, 180, 0.00030000000000000003, 32, 1], Fitness: 9.99999999999e-07
Solution 770: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 771: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 772: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 773: [0.5, 180, 0.0007, 32, 1], Fitness: 9.99999999999e-07
Solution 774: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 775: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 776: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 777: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 778: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 779: [0.4, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 780: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 781: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 782: [0.5, 180, 0.0005, 16, 2], Fitness: 9.99999999999e-07
Solution 783: [0.4, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 784: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 785: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 786: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 787: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 788: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 789: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 790: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 791: [0.5, 180, 0.0007, 128, 2], Fitness: 9.99999999999e-07
Solution 792: [0.5, 180, 0.0007, 256, 1], Fitness: 9.99999999999e-07
Solution 793: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 794: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 795: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 796: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 797: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 798: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 799: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 800: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 801: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 802: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 803: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 804: [0.5, 180, 0.00030000000000000003, 64, 1], Fitness: 9.99999999999e-07
Solution 805: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 806: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 807: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 808: [0.5, 180, 0.0007, 64, 1], Fitness: 9.99999999999e-07
Solution 809: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 810: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 811: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 812: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 813: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 814: [0.6, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 815: [0.5, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 816: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 817: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 818: [0.6, 180, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 819: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 820: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 821: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 822: [0.5, 180, 0.0005, 64, 2], Fitness: 9.99999999999e-07
Solution 823: [0.5, 180, 0.00030000000000000003, 32, 1], Fitness: 9.99999999999e-07
Solution 824: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 825: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 826: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 827: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 828: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 829: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 830: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 831: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 832: [0.5, 175, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 833: [0.6, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 834: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 835: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 836: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 837: [0.5, 175, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 838: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 839: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 840: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 841: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 842: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 843: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 844: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 845: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 846: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 847: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 848: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 849: [0.5, 180, 0.0005, 64, 1], Fitness: 28.52879141861904
Solution 850: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 851: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 852: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 853: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 854: [0.5, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 855: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 856: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 857: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 858: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 859: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 860: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 861: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 862: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 863: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 864: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 865: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 866: [0.4, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 867: [0.4, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 868: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 869: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 870: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 871: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 872: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 873: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 874: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 875: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 876: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 877: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 878: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 879: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 880: [0.5, 180, 0.0007, 32, 1], Fitness: 9.99999999999e-07
Solution 881: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 882: [0.6, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 883: [0.6, 180, 0.0005, 64, 1], Fitness: 9.99999999999e-07
Solution 884: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 885: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 886: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 887: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 888: [0.5, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 889: [0.4, 180, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 890: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 891: [0.5, 180, 0.0007, 16, 1], Fitness: 9.99999999999e-07
Solution 892: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 893: [0.4, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 894: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 895: [0.4, 180, 0.0005, 16, 1], Fitness: 9.99999999999e-07
Solution 896: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 897: [0.6, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 898: [0.5, 180, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 899: [0.5, 175, 0.00030000000000000003, 128, 1], Fitness: 9.99999999999e-07
Solution 900: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 901: [0.5, 180, 0.0005, 128, 1], Fitness: 29.165915337825474
Solution 902: [0.5, 175, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 903: [0.5, 175, 0.0005, 128, 2], Fitness: 9.99999999999e-07
Solution 904: [0.6, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 905: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 906: [0.5, 180, 0.0005, 32, 1], Fitness: 9.99999999999e-07
Solution 907: [0.5, 180, 0.0005, 32, 2], Fitness: 9.99999999999e-07
Solution 908: [0.4, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 909: [0.5, 180, 0.0007, 128, 1], Fitness: 9.99999999999e-07
Solution 910: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 911: [0.5, 180, 0.0005, 256, 1], Fitness: 9.99999999999e-07
Solution 912: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 913: [0.4, 175, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Solution 914: [0.6, 180, 0.0005, 128, 1], Fitness: 9.99999999999e-07
Thu Feb 23 03:09:15 CET 2023
done
