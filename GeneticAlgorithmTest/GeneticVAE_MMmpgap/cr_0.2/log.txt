start
Wed Feb 22 18:58:03 CET 2023
2023-02-22 18:58:04.591181: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 18:58:04.783473: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-22 18:59:03,490 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f70fc9ab040> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.0 90 0.0005 64 2]
 [2.5 120 0.001 32 1]
 [2.0 180 0.0005 128 1]
 [0.5 60 0.0005 16 1]
 [2.0 30 0.001 16 1]
 [2.5 90 0.001 32 1]
 [0.5 180 0.0005 128 1]
 [2.5 150 0.002 16 1]
 [1.0 210 0.0005 128 2]
 [0.5 180 0.0005 64 1]
 [2.0 150 0.001 256 1]
 [2.0 60 0.0005 16 1]
 [1.0 90 0.002 16 2]
 [0.5 180 0.002 128 2]
 [0.5 90 0.002 256 2]]
[1.0 90 0.0005 64 2] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-22 18:59:08.567436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 18:59:09.105165: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-22 18:59:09.106041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22295 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:e1:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/90
2023-02-22 18:59:14.643303: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0082 - 9s/epoch - 6ms/step
Epoch 2/90
1493/1493 - 7s - loss: 0.0077 - val_loss: 0.0077 - 7s/epoch - 4ms/step
Epoch 3/90
1493/1493 - 7s - loss: 0.0072 - val_loss: 0.0071 - 7s/epoch - 5ms/step
Epoch 4/90
1493/1493 - 7s - loss: 0.0069 - val_loss: 0.0068 - 7s/epoch - 5ms/step
Epoch 5/90
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 4ms/step
Epoch 6/90
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 7/90
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0064 - 7s/epoch - 5ms/step
Epoch 8/90
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 9/90
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 10/90
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 11/90
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 4ms/step
Epoch 12/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 13/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 4ms/step
Epoch 14/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 15/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 16/90
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 17/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 18/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 19/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 4ms/step
Epoch 20/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 21/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 22/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 23/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 24/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 25/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 26/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 27/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 28/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 29/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 30/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 31/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 32/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 33/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 34/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 35/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 36/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 37/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 38/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 39/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 40/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 41/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 42/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 43/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 44/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 45/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 46/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 47/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 48/90
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 49/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 50/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 4ms/step
Epoch 51/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 52/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 53/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 54/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 55/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 56/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 57/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 58/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 59/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 60/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 61/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 62/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 63/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 64/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 65/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 66/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 67/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 68/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 69/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 70/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 71/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 72/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 73/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 74/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 75/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 76/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 77/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 78/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 79/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 80/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 81/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 82/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 83/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 84/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
Epoch 85/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 86/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 87/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 88/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 89/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0060 - 7s/epoch - 5ms/step
Epoch 90/90
1493/1493 - 7s - loss: 0.0060 - val_loss: 0.0059 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005949758924543858
  1/332 [..............................] - ETA: 1:01 36/332 [==>...........................] - ETA: 0s   70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s141/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s290/332 [=========================>....] - ETA: 0s302/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.1131415274310148
cosine 0.08637488588741309
MAE: 0.04584317
RMSE: 0.0984895
r2: 0.3712883554719925
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 64, 90, 0.0005, 0.2, 252, 0.006017697509378195, 0.005949758924543858, 0.1131415274310148, 0.08637488588741309, 0.04584316909313202, 0.09848950058221817, 0.3712883554719925, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 120 0.001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/120
2985/2985 - 14s - loss: 0.0250 - val_loss: 0.0139 - 14s/epoch - 5ms/step
Epoch 2/120
2985/2985 - 13s - loss: 0.0130 - val_loss: 0.0119 - 13s/epoch - 4ms/step
Epoch 3/120
2985/2985 - 13s - loss: 0.0121 - val_loss: 0.0117 - 13s/epoch - 4ms/step
Epoch 4/120
2985/2985 - 13s - loss: 0.0118 - val_loss: 0.0114 - 13s/epoch - 4ms/step
Epoch 5/120
2985/2985 - 13s - loss: 0.0115 - val_loss: 0.0113 - 13s/epoch - 4ms/step
Epoch 6/120
2985/2985 - 13s - loss: 0.0113 - val_loss: 0.0109 - 13s/epoch - 4ms/step
Epoch 7/120
2985/2985 - 13s - loss: 0.0111 - val_loss: 0.0108 - 13s/epoch - 4ms/step
Epoch 8/120
2985/2985 - 13s - loss: 0.0110 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 9/120
2985/2985 - 13s - loss: 0.0110 - val_loss: 0.0108 - 13s/epoch - 4ms/step
Epoch 10/120
2985/2985 - 13s - loss: 0.0109 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 11/120
2985/2985 - 13s - loss: 0.0109 - val_loss: 0.0106 - 13s/epoch - 4ms/step
Epoch 12/120
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0110 - 13s/epoch - 4ms/step
Epoch 13/120
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 14/120
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 15/120
2985/2985 - 13s - loss: 0.0107 - val_loss: 0.0107 - 13s/epoch - 4ms/step
Epoch 16/120
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0109 - 13s/epoch - 4ms/step
Epoch 17/120
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 18/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 19/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 20/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 21/120
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 22/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0106 - 13s/epoch - 4ms/step
Epoch 23/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 24/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 25/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 26/120
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 27/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 28/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 29/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 30/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 31/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 32/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 33/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 34/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 35/120
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 36/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 37/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 38/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 39/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 40/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 41/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 42/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 43/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 44/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 45/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 46/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 47/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 48/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 49/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 50/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 51/120
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 52/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 53/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 54/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 55/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 56/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 57/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 58/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 59/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 60/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 61/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 62/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 63/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 64/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0102 - 12s/epoch - 4ms/step
Epoch 65/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 66/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 67/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 68/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 69/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 70/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 71/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 72/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 73/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 74/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 75/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 76/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 77/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 78/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 79/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 80/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 81/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 82/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 83/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 84/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 85/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 86/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 87/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 88/120
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0102 - 12s/epoch - 4ms/step
Epoch 89/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 90/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 91/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 92/120
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 93/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0105 - 13s/epoch - 4ms/step
Epoch 94/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0102 - 13s/epoch - 4ms/step
Epoch 95/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 96/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 97/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 98/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 99/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 100/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0103 - 12s/epoch - 4ms/step
Epoch 101/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 102/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 103/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 104/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 105/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 106/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0101 - 12s/epoch - 4ms/step
Epoch 107/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 108/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 109/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 110/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0104 - 13s/epoch - 4ms/step
Epoch 111/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 112/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 113/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 114/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0100 - 13s/epoch - 4ms/step
Epoch 115/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 116/120
2985/2985 - 12s - loss: 0.0100 - val_loss: 0.0100 - 12s/epoch - 4ms/step
Epoch 117/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0101 - 13s/epoch - 4ms/step
Epoch 118/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0099 - 13s/epoch - 4ms/step
Epoch 119/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0103 - 13s/epoch - 4ms/step
Epoch 120/120
2985/2985 - 13s - loss: 0.0100 - val_loss: 0.0105 - 13s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.010518183931708336
  1/332 [..............................] - ETA: 51s 36/332 [==>...........................] - ETA: 0s  73/332 [=====>........................] - ETA: 0s109/332 [========>.....................] - ETA: 0s145/332 [============>.................] - ETA: 0s181/332 [===============>..............] - ETA: 0s217/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s291/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07718394745016295
cosine 0.05923944822896259
MAE: 0.038210344
RMSE: 0.08615505
r2: 0.5189026899140003
RMSE zero-vector: 0.2430644284356365
['2.5custom_VAE', 'mse', 32, 120, 0.001, 0.2, 252, 0.010005587711930275, 0.010518183931708336, 0.07718394745016295, 0.05923944822896259, 0.038210343569517136, 0.08615504950284958, 0.5189026899140003, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 180 0.0005 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 5s - loss: 0.0303 - val_loss: 0.0184 - 5s/epoch - 7ms/step
Epoch 2/180
747/747 - 4s - loss: 0.0160 - val_loss: 0.0260 - 4s/epoch - 5ms/step
Epoch 3/180
747/747 - 4s - loss: 0.0148 - val_loss: 0.0148 - 4s/epoch - 5ms/step
Epoch 4/180
747/747 - 4s - loss: 0.0136 - val_loss: 0.0150 - 4s/epoch - 5ms/step
Epoch 5/180
747/747 - 4s - loss: 0.0129 - val_loss: 0.0145 - 4s/epoch - 5ms/step
Epoch 6/180
747/747 - 4s - loss: 0.0122 - val_loss: 0.0134 - 4s/epoch - 5ms/step
Epoch 7/180
747/747 - 4s - loss: 0.0118 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 8/180
747/747 - 4s - loss: 0.0113 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 9/180
747/747 - 4s - loss: 0.0111 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 10/180
747/747 - 4s - loss: 0.0120 - val_loss: 0.0133 - 4s/epoch - 5ms/step
Epoch 11/180
747/747 - 4s - loss: 0.0139 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 12/180
747/747 - 4s - loss: 0.0135 - val_loss: 0.0161 - 4s/epoch - 5ms/step
Epoch 13/180
747/747 - 4s - loss: 0.0170 - val_loss: 0.0443 - 4s/epoch - 5ms/step
Epoch 14/180
747/747 - 4s - loss: 0.0125 - val_loss: 0.0128 - 4s/epoch - 6ms/step
Epoch 15/180
747/747 - 4s - loss: 0.0134 - val_loss: 0.0144 - 4s/epoch - 5ms/step
Epoch 16/180
747/747 - 4s - loss: 0.0196 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 17/180
747/747 - 4s - loss: 0.0134 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 18/180
747/747 - 4s - loss: 0.0115 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 19/180
747/747 - 4s - loss: 0.0113 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 20/180
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 21/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 22/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 23/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 24/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 25/180
747/747 - 4s - loss: 0.0118 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 26/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 27/180
747/747 - 4s - loss: 0.0119 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 28/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 29/180
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 30/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 31/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 32/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 33/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 34/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 35/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 36/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 37/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 38/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 39/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 40/180
747/747 - 4s - loss: 0.0111 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 41/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 42/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 43/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 44/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 45/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 46/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 47/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 48/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 6ms/step
Epoch 49/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 50/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 51/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 52/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 53/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 54/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 55/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 56/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 57/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 6ms/step
Epoch 58/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 59/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 60/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 61/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 62/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 63/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 64/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 65/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 66/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 67/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 68/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 69/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 70/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 71/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 72/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 73/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 74/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 75/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 76/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 77/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 78/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 79/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 80/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 6ms/step
Epoch 81/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 82/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 83/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 84/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 85/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 86/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 87/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 88/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 89/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 90/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 91/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 92/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 93/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 94/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 95/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 96/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 97/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 98/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 99/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 100/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 101/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 102/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 103/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 104/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 105/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 106/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 107/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 108/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 109/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 110/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 111/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 112/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 113/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 114/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 115/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 116/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 117/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 118/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 119/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 120/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 121/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 122/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 123/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 124/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 125/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 126/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 6ms/step
Epoch 127/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 128/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 129/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 130/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 131/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 132/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 133/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 134/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 135/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 136/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 137/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 138/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 139/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 140/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 141/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 142/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 143/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 144/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 145/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 146/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 147/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 148/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 149/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 150/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 151/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 152/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 153/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 154/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 155/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 156/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 157/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 158/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 159/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 160/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 161/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 162/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 163/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 164/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 165/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 166/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 167/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 168/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 169/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 170/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 171/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 172/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 173/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 174/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 175/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 176/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 177/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 178/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 6ms/step
Epoch 179/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
Epoch 180/180
747/747 - 4s - loss: 0.0094 - val_loss: 0.0093 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009274762123823166
  1/332 [..............................] - ETA: 51s 37/332 [==>...........................] - ETA: 0s  73/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s141/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s215/332 [==================>...........] - ETA: 0s251/332 [=====================>........] - ETA: 0s288/332 [=========================>....] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0682173476403855
cosine 0.052263215835751387
MAE: 0.0355626
RMSE: 0.07725729
r2: 0.6131430030830872
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 128, 180, 0.0005, 0.2, 252, 0.0094473110511899, 0.009274762123823166, 0.0682173476403855, 0.052263215835751387, 0.03556260094046593, 0.07725729048252106, 0.6131430030830872, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 60 0.0005 16 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/60
5969/5969 - 27s - loss: 0.0170 - val_loss: 0.0129 - 27s/epoch - 5ms/step
Epoch 2/60
5969/5969 - 27s - loss: 0.0129 - val_loss: 0.0123 - 27s/epoch - 5ms/step
Epoch 3/60
5969/5969 - 27s - loss: 0.0121 - val_loss: 0.0118 - 27s/epoch - 4ms/step
Epoch 4/60
5969/5969 - 27s - loss: 0.0117 - val_loss: 0.0120 - 27s/epoch - 5ms/step
Epoch 5/60
5969/5969 - 27s - loss: 0.0115 - val_loss: 0.0123 - 27s/epoch - 4ms/step
Epoch 6/60
5969/5969 - 27s - loss: 0.0114 - val_loss: 0.0122 - 27s/epoch - 5ms/step
Epoch 7/60
5969/5969 - 27s - loss: 0.0113 - val_loss: 0.0129 - 27s/epoch - 5ms/step
Epoch 8/60
5969/5969 - 27s - loss: 0.0113 - val_loss: 0.0128 - 27s/epoch - 5ms/step
Epoch 9/60
5969/5969 - 27s - loss: 0.0112 - val_loss: 0.0135 - 27s/epoch - 4ms/step
Epoch 10/60
5969/5969 - 26s - loss: 0.0112 - val_loss: 0.0122 - 26s/epoch - 4ms/step
Epoch 11/60
5969/5969 - 27s - loss: 0.0112 - val_loss: 0.0132 - 27s/epoch - 4ms/step
Epoch 12/60
5969/5969 - 27s - loss: 0.0112 - val_loss: 0.0130 - 27s/epoch - 4ms/step
Epoch 13/60
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.0127 - 27s/epoch - 4ms/step
Epoch 14/60
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.0125 - 26s/epoch - 4ms/step
Epoch 15/60
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.0125 - 26s/epoch - 4ms/step
Epoch 16/60
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.0133 - 27s/epoch - 4ms/step
Epoch 17/60
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0133 - 27s/epoch - 5ms/step
Epoch 18/60
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0128 - 27s/epoch - 5ms/step
Epoch 19/60
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.0132 - 27s/epoch - 5ms/step
Epoch 20/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0149 - 27s/epoch - 5ms/step
Epoch 21/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0139 - 27s/epoch - 4ms/step
Epoch 22/60
5969/5969 - 28s - loss: 0.0109 - val_loss: 0.0138 - 28s/epoch - 5ms/step
Epoch 23/60
5969/5969 - 26s - loss: 0.0109 - val_loss: 0.0133 - 26s/epoch - 4ms/step
Epoch 24/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0148 - 27s/epoch - 4ms/step
Epoch 25/60
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.0148 - 27s/epoch - 5ms/step
Epoch 26/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0136 - 27s/epoch - 4ms/step
Epoch 27/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0158 - 27s/epoch - 5ms/step
Epoch 28/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0144 - 27s/epoch - 4ms/step
Epoch 29/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0126 - 27s/epoch - 5ms/step
Epoch 30/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0148 - 27s/epoch - 5ms/step
Epoch 31/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0138 - 27s/epoch - 5ms/step
Epoch 32/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0139 - 27s/epoch - 5ms/step
Epoch 33/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0135 - 27s/epoch - 5ms/step
Epoch 34/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0152 - 27s/epoch - 5ms/step
Epoch 35/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0140 - 27s/epoch - 5ms/step
Epoch 36/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0155 - 27s/epoch - 5ms/step
Epoch 37/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0148 - 27s/epoch - 5ms/step
Epoch 38/60
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0151 - 27s/epoch - 4ms/step
Epoch 39/60
5969/5969 - 28s - loss: 0.0107 - val_loss: 0.0155 - 28s/epoch - 5ms/step
Epoch 40/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0143 - 27s/epoch - 4ms/step
Epoch 41/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0147 - 26s/epoch - 4ms/step
Epoch 42/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0152 - 27s/epoch - 4ms/step
Epoch 43/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0144 - 27s/epoch - 4ms/step
Epoch 44/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0151 - 26s/epoch - 4ms/step
Epoch 45/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0147 - 26s/epoch - 4ms/step
Epoch 46/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0161 - 27s/epoch - 4ms/step
Epoch 47/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0172 - 26s/epoch - 4ms/step
Epoch 48/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0169 - 26s/epoch - 4ms/step
Epoch 49/60
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0163 - 25s/epoch - 4ms/step
Epoch 50/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0185 - 26s/epoch - 4ms/step
Epoch 51/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0169 - 26s/epoch - 4ms/step
Epoch 52/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0177 - 26s/epoch - 4ms/step
Epoch 53/60
5969/5969 - 27s - loss: 0.0107 - val_loss: 0.0151 - 27s/epoch - 4ms/step
Epoch 54/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0162 - 26s/epoch - 4ms/step
Epoch 55/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0171 - 26s/epoch - 4ms/step
Epoch 56/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0175 - 26s/epoch - 4ms/step
Epoch 57/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0159 - 26s/epoch - 4ms/step
Epoch 58/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0158 - 26s/epoch - 4ms/step
Epoch 59/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0167 - 26s/epoch - 4ms/step
Epoch 60/60
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0155 - 26s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.015452830120921135
  1/332 [..............................] - ETA: 57s 35/332 [==>...........................] - ETA: 0s  72/332 [=====>........................] - ETA: 0s108/332 [========>.....................] - ETA: 0s144/332 [============>.................] - ETA: 0s180/332 [===============>..............] - ETA: 0s217/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s290/332 [=========================>....] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.09118359852153941
cosine 0.07050176020437846
MAE: 0.0437076
RMSE: 0.11398748
r2: 0.15785209565542874
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 16, 60, 0.0005, 0.2, 252, 0.010654022917151451, 0.015452830120921135, 0.09118359852153941, 0.07050176020437846, 0.043707601726055145, 0.11398748308420181, 0.15785209565542874, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 30 0.001 16 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/30
5969/5969 - 28s - loss: 0.0209 - val_loss: 0.0129 - 28s/epoch - 5ms/step
Epoch 2/30
