start
Tue Feb 14 14:21:18 CET 2023
2023-02-14 14:21:20.992669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 14:21:21.193281: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-14 14:21:53,980 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f4c04b63dc0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.5 145 0.0012 64 1]
 [1.5 150 0.001 32 1]
 [1.5 70 0.0005 16 0]
 [2.5 50 0.0005 64 2]
 [2.0 110 0.0005 8 0]
 [2.0 170 0.0005 16 0]
 [2.0 10 0.001 16 1]
 [1.5 90 0.001 32 1]
 [0.5 10 0.001 64 0]
 [1.5 30 0.0005 8 1]]
[1.5 145 0.0012 64 1] 0
Shape of dataset to encode: (106113, 1264)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-14 14:21:58.678964: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 14:21:59.807706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9634 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1e:00.0, compute capability: 7.5
2023-02-14 14:21:59.809306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9634 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-02-14 14:21:59.840666: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-14 14:22:00.072402: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/batch_normalization_2/gamma/v/Assign' id:1115 op device:{requested: '', assigned: ''} def:{{{node training/Adam/batch_normalization_2/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/batch_normalization_2/gamma/v, training/Adam/batch_normalization_2/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 14:22:11.532877: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:452 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 13s - loss: 0.0153 - val_loss: 0.0073 - 13s/epoch - 140us/sample
Epoch 2/145
95501/95501 - 9s - loss: 0.0058 - val_loss: 0.0048 - 9s/epoch - 91us/sample
Epoch 3/145
95501/95501 - 9s - loss: 0.0049 - val_loss: 0.0049 - 9s/epoch - 92us/sample
Epoch 4/145
95501/95501 - 9s - loss: 0.0045 - val_loss: 0.0040 - 9s/epoch - 91us/sample
Epoch 5/145
95501/95501 - 9s - loss: 0.0038 - val_loss: 0.0033 - 9s/epoch - 90us/sample
Epoch 6/145
95501/95501 - 9s - loss: 0.0033 - val_loss: 0.0064 - 9s/epoch - 90us/sample
Epoch 7/145
95501/95501 - 9s - loss: 0.0037 - val_loss: 0.0028 - 9s/epoch - 91us/sample
Epoch 8/145
95501/95501 - 9s - loss: 0.0028 - val_loss: 0.0026 - 9s/epoch - 91us/sample
Epoch 9/145
95501/95501 - 9s - loss: 0.0027 - val_loss: 0.0024 - 9s/epoch - 91us/sample
Epoch 10/145
95501/95501 - 9s - loss: 0.0025 - val_loss: 0.0030 - 9s/epoch - 91us/sample
Epoch 11/145
95501/95501 - 9s - loss: 0.0025 - val_loss: 0.0027 - 9s/epoch - 89us/sample
Epoch 12/145
95501/95501 - 9s - loss: 0.0024 - val_loss: 0.0022 - 9s/epoch - 92us/sample
Epoch 13/145
95501/95501 - 9s - loss: 0.0023 - val_loss: 0.0021 - 9s/epoch - 92us/sample
Epoch 14/145
95501/95501 - 9s - loss: 0.0022 - val_loss: 0.0020 - 9s/epoch - 90us/sample
Epoch 15/145
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0020 - 9s/epoch - 90us/sample
Epoch 16/145
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0020 - 9s/epoch - 91us/sample
Epoch 17/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0020 - 9s/epoch - 91us/sample
Epoch 18/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0019 - 9s/epoch - 91us/sample
Epoch 19/145
95501/95501 - 8s - loss: 0.0020 - val_loss: 0.0019 - 8s/epoch - 89us/sample
Epoch 20/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0019 - 9s/epoch - 92us/sample
Epoch 21/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0019 - 9s/epoch - 90us/sample
Epoch 22/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0019 - 9s/epoch - 90us/sample
Epoch 23/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0019 - 9s/epoch - 90us/sample
Epoch 24/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0023 - 9s/epoch - 90us/sample
Epoch 25/145
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0018 - 9s/epoch - 90us/sample
Epoch 26/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0019 - 9s/epoch - 91us/sample
Epoch 27/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 93us/sample
Epoch 28/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 29/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 95us/sample
Epoch 30/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 31/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 97us/sample
Epoch 32/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 33/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 34/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 35/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 36/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 37/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 38/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 39/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 40/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 41/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 42/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 43/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 44/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 45/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0018 - 9s/epoch - 97us/sample
Epoch 46/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 47/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 48/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 49/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 50/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 51/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 52/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 53/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 54/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 55/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 56/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 57/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 58/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 59/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 60/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 61/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 62/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 63/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 64/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 65/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 66/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 67/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0018 - 9s/epoch - 97us/sample
Epoch 68/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 69/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 70/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 71/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 72/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 73/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 74/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 75/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 76/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 77/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 78/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 79/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 80/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 81/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 82/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 83/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 84/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 85/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 86/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 87/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 88/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 89/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 90/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 91/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 92/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 93/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 94/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 95/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 96/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 97/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 98/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 99/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 100/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 101/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 102/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 103/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 104/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 105/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 106/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 107/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 108/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 109/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 110/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 111/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 112/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 113/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 114/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 115/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 116/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 117/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 118/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 119/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 120/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 121/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 122/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 123/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 124/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 125/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 126/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 127/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 128/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 129/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 94us/sample
Epoch 130/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 131/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 132/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 133/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 134/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 135/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 136/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 137/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 138/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 139/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 98us/sample
Epoch 140/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 141/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 142/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 143/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 95us/sample
Epoch 144/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 145/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0015526602322992558
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 14:44:03.301357: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:423 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.010331724618364629
cosine 0.008152697806847216
MAE: 0.015191178
RMSE: 0.029759051
r2: 0.9425494380963224
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 64, 145, 0.0012, 0.1, 126, 0.0016222840786095727, 0.0015526602322992558, 0.010331724618364629, 0.008152697806847216, 0.01519117783755064, 0.029759051278233528, 0.9425494380963224, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1896)        7584        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1896)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-14 14:44:09.942743: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_1/kernel/Assign' id:1430 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_1/kernel, dense_dec1_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 14:44:23.140520: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1719 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 14s - loss: 0.0110 - val_loss: 0.0056 - 14s/epoch - 149us/sample
Epoch 2/150
95501/95501 - 14s - loss: 0.0058 - val_loss: 0.0041 - 14s/epoch - 144us/sample
Epoch 3/150
95501/95501 - 14s - loss: 0.0042 - val_loss: 0.0032 - 14s/epoch - 144us/sample
Epoch 4/150
95501/95501 - 14s - loss: 0.0034 - val_loss: 0.0028 - 14s/epoch - 142us/sample
Epoch 5/150
95501/95501 - 14s - loss: 0.0030 - val_loss: 0.0026 - 14s/epoch - 143us/sample
Epoch 6/150
95501/95501 - 14s - loss: 0.0027 - val_loss: 0.0023 - 14s/epoch - 144us/sample
Epoch 7/150
95501/95501 - 14s - loss: 0.0026 - val_loss: 0.0023 - 14s/epoch - 142us/sample
Epoch 8/150
95501/95501 - 13s - loss: 0.0025 - val_loss: 0.0025 - 13s/epoch - 141us/sample
Epoch 9/150
95501/95501 - 14s - loss: 0.0024 - val_loss: 0.0022 - 14s/epoch - 144us/sample
Epoch 10/150
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0021 - 14s/epoch - 143us/sample
Epoch 11/150
95501/95501 - 13s - loss: 0.0023 - val_loss: 0.0021 - 13s/epoch - 141us/sample
Epoch 12/150
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 144us/sample
Epoch 13/150
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 142us/sample
Epoch 14/150
95501/95501 - 13s - loss: 0.0022 - val_loss: 0.0020 - 13s/epoch - 139us/sample
Epoch 15/150
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 143us/sample
Epoch 16/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0020 - 14s/epoch - 142us/sample
Epoch 17/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 142us/sample
Epoch 18/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 142us/sample
Epoch 19/150
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0019 - 13s/epoch - 140us/sample
Epoch 20/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 143us/sample
Epoch 21/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0019 - 14s/epoch - 144us/sample
Epoch 22/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0019 - 13s/epoch - 141us/sample
Epoch 23/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 24/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0019 - 14s/epoch - 143us/sample
Epoch 25/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0019 - 14s/epoch - 143us/sample
Epoch 26/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 27/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 143us/sample
Epoch 28/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 143us/sample
Epoch 29/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 30/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 143us/sample
Epoch 31/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 143us/sample
Epoch 32/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 33/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 145us/sample
Epoch 34/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 35/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 139us/sample
Epoch 36/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 143us/sample
Epoch 37/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 38/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 141us/sample
Epoch 39/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 140us/sample
Epoch 40/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 41/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 42/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 43/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 44/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 45/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 46/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 47/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 140us/sample
Epoch 48/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 49/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 50/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 140us/sample
Epoch 51/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 52/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 53/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 54/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 55/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 56/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 57/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 139us/sample
Epoch 58/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 59/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 60/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 61/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 62/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 63/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 64/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 65/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 66/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 67/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 68/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 69/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 70/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 71/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 72/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 73/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 74/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 75/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 139us/sample
Epoch 76/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 77/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0026 - 13s/epoch - 141us/sample
Epoch 78/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 79/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 80/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 81/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 139us/sample
Epoch 82/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 83/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 84/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 85/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 86/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 87/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 88/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 139us/sample
Epoch 89/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 90/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 91/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 92/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 93/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 94/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 139us/sample
Epoch 95/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0019 - 14s/epoch - 142us/sample
Epoch 96/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 97/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 98/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 99/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 100/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 101/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 102/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 103/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 104/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 105/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 106/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 107/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 108/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 109/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 110/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 111/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 112/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 113/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 114/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 115/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 139us/sample
Epoch 116/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 117/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 118/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 119/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 120/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 121/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 122/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 123/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 124/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 125/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 138us/sample
Epoch 126/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 127/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 128/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 129/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 130/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 131/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 139us/sample
Epoch 132/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 133/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 134/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 135/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 136/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 137/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 138/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 139/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 140/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 141/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0020 - 14s/epoch - 142us/sample
Epoch 142/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 143/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 144/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 145/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 146/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 147/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 148/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 149/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 139us/sample
Epoch 150/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 139us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0016515206495040864
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 15:17:54.653360: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1690 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.011339965425614272
cosine 0.008947351019965718
MAE: 0.015907496
RMSE: 0.031189118
r2: 0.9368948521783158
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 32, 150, 0.001, 0.1, 126, 0.001759244683343984, 0.0016515206495040864, 0.011339965425614272, 0.008947351019965718, 0.015907496213912964, 0.031189117580652237, 0.9368948521783158, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 70 0.0005 16 0] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1896)        7584        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1896)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/70
2023-02-14 15:18:00.682869: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/iter/Assign' id:3488 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_4/Adam/iter, training_4/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 15:18:24.205588: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2999 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 25s - loss: 2.4311 - val_loss: 1.4367 - 25s/epoch - 263us/sample
Epoch 2/70
95501/95501 - 24s - loss: 1.4589 - val_loss: 1.4433 - 24s/epoch - 252us/sample
Epoch 3/70
95501/95501 - 24s - loss: 1.4423 - val_loss: 1.4567 - 24s/epoch - 250us/sample
Epoch 4/70
95501/95501 - 24s - loss: 337.8679 - val_loss: 1.4381 - 24s/epoch - 251us/sample
Epoch 5/70
95501/95501 - 24s - loss: 1.4369 - val_loss: 1.4506 - 24s/epoch - 250us/sample
Epoch 6/70
95501/95501 - 24s - loss: 1.4311 - val_loss: 1.4305 - 24s/epoch - 251us/sample
Epoch 7/70
95501/95501 - 23s - loss: 1.4333 - val_loss: 1.4436 - 23s/epoch - 245us/sample
Epoch 8/70
95501/95501 - 24s - loss: 1.4340 - val_loss: 1.4506 - 24s/epoch - 250us/sample
Epoch 9/70
95501/95501 - 24s - loss: 1.4312 - val_loss: 1.4260 - 24s/epoch - 253us/sample
Epoch 10/70
95501/95501 - 24s - loss: 1.4266 - val_loss: 1.4338 - 24s/epoch - 248us/sample
Epoch 11/70
95501/95501 - 24s - loss: 1.4245 - val_loss: 1.4249 - 24s/epoch - 251us/sample
Epoch 12/70
95501/95501 - 24s - loss: 1.4237 - val_loss: 1.4233 - 24s/epoch - 248us/sample
Epoch 13/70
95501/95501 - 24s - loss: 1.4225 - val_loss: 1.4194 - 24s/epoch - 249us/sample
Epoch 14/70
95501/95501 - 24s - loss: 1.4198 - val_loss: 1.4173 - 24s/epoch - 250us/sample
Epoch 15/70
95501/95501 - 24s - loss: 1.4198 - val_loss: 1.4177 - 24s/epoch - 248us/sample
Epoch 16/70
95501/95501 - 24s - loss: 1.4156 - val_loss: 1.4143 - 24s/epoch - 250us/sample
Epoch 17/70
95501/95501 - 24s - loss: 1.4153 - val_loss: 1.4205 - 24s/epoch - 251us/sample
Epoch 18/70
95501/95501 - 24s - loss: 1.4146 - val_loss: 1.4174 - 24s/epoch - 248us/sample
Epoch 19/70
95501/95501 - 24s - loss: 1.4131 - val_loss: 1.4255 - 24s/epoch - 249us/sample
Epoch 20/70
95501/95501 - 24s - loss: 1.4142 - val_loss: 1.4210 - 24s/epoch - 250us/sample
Epoch 21/70
95501/95501 - 23s - loss: 1.4165 - val_loss: 1.4151 - 23s/epoch - 246us/sample
Epoch 22/70
95501/95501 - 24s - loss: 1.4158 - val_loss: 1.4130 - 24s/epoch - 254us/sample
Epoch 23/70
95501/95501 - 24s - loss: 1.4167 - val_loss: 1.4178 - 24s/epoch - 252us/sample
Epoch 24/70
95501/95501 - 24s - loss: 1.4145 - val_loss: 1.4139 - 24s/epoch - 250us/sample
Epoch 25/70
95501/95501 - 24s - loss: 1.4162 - val_loss: 1.4166 - 24s/epoch - 250us/sample
Epoch 26/70
95501/95501 - 24s - loss: 1.4153 - val_loss: 1.4204 - 24s/epoch - 252us/sample
Epoch 27/70
95501/95501 - 24s - loss: 1.4144 - val_loss: 1.4127 - 24s/epoch - 253us/sample
Epoch 28/70
95501/95501 - 24s - loss: 1.4148 - val_loss: 1.4223 - 24s/epoch - 254us/sample
Epoch 29/70
95501/95501 - 24s - loss: 1.4156 - val_loss: 1.4170 - 24s/epoch - 249us/sample
Epoch 30/70
95501/95501 - 24s - loss: 1.4166 - val_loss: 1.4242 - 24s/epoch - 252us/sample
Epoch 31/70
95501/95501 - 24s - loss: 1.4141 - val_loss: 1.4146 - 24s/epoch - 251us/sample
Epoch 32/70
95501/95501 - 24s - loss: 1.4159 - val_loss: 1.4190 - 24s/epoch - 251us/sample
Epoch 33/70
95501/95501 - 24s - loss: 1.4181 - val_loss: 1.4158 - 24s/epoch - 253us/sample
Epoch 34/70
95501/95501 - 24s - loss: 1.4142 - val_loss: 1.4154 - 24s/epoch - 250us/sample
Epoch 35/70
95501/95501 - 24s - loss: 1.4150 - val_loss: 1.4147 - 24s/epoch - 253us/sample
Epoch 36/70
95501/95501 - 24s - loss: 1.4134 - val_loss: 1.4136 - 24s/epoch - 247us/sample
Epoch 37/70
95501/95501 - 24s - loss: 1.4133 - val_loss: 1.4143 - 24s/epoch - 253us/sample
Epoch 38/70
95501/95501 - 23s - loss: 1.4124 - val_loss: 1.4123 - 23s/epoch - 246us/sample
Epoch 39/70
95501/95501 - 23s - loss: 1.4135 - val_loss: 1.4162 - 23s/epoch - 246us/sample
Epoch 40/70
95501/95501 - 24s - loss: 1.4144 - val_loss: 1.4136 - 24s/epoch - 249us/sample
Epoch 41/70
95501/95501 - 24s - loss: 1.4140 - val_loss: 1.4142 - 24s/epoch - 247us/sample
Epoch 42/70
95501/95501 - 23s - loss: 1.4124 - val_loss: 1.4130 - 23s/epoch - 244us/sample
Epoch 43/70
95501/95501 - 23s - loss: 1.4157 - val_loss: 1.4159 - 23s/epoch - 246us/sample
Epoch 44/70
95501/95501 - 24s - loss: 1.4148 - val_loss: 1.4145 - 24s/epoch - 249us/sample
Epoch 45/70
95501/95501 - 24s - loss: 1.4145 - val_loss: 1.4161 - 24s/epoch - 249us/sample
Epoch 46/70
95501/95501 - 23s - loss: 1.4154 - val_loss: 1.4150 - 23s/epoch - 246us/sample
Epoch 47/70
95501/95501 - 24s - loss: 1.4132 - val_loss: 1.4144 - 24s/epoch - 246us/sample
Epoch 48/70
95501/95501 - 24s - loss: 1.4142 - val_loss: 1.4141 - 24s/epoch - 250us/sample
Epoch 49/70
95501/95501 - 24s - loss: 1.4127 - val_loss: 1.4157 - 24s/epoch - 248us/sample
Epoch 50/70
95501/95501 - 23s - loss: 1.4131 - val_loss: 1.4164 - 23s/epoch - 244us/sample
Epoch 51/70
95501/95501 - 24s - loss: 1.4130 - val_loss: 1.4125 - 24s/epoch - 248us/sample
Epoch 52/70
95501/95501 - 24s - loss: 1.4134 - val_loss: 1.4146 - 24s/epoch - 247us/sample
Epoch 53/70
95501/95501 - 24s - loss: 1.4129 - val_loss: 1.4132 - 24s/epoch - 249us/sample
Epoch 54/70
95501/95501 - 24s - loss: 1.4133 - val_loss: 1.4128 - 24s/epoch - 247us/sample
Epoch 55/70
95501/95501 - 24s - loss: 1.4129 - val_loss: 1.4131 - 24s/epoch - 249us/sample
Epoch 56/70
95501/95501 - 24s - loss: 1.4130 - val_loss: 1.4130 - 24s/epoch - 247us/sample
Epoch 57/70
95501/95501 - 23s - loss: 1.4126 - val_loss: 1.4137 - 23s/epoch - 245us/sample
Epoch 58/70
95501/95501 - 23s - loss: 1.4122 - val_loss: 1.4121 - 23s/epoch - 244us/sample
Epoch 59/70
95501/95501 - 24s - loss: 1.4114 - val_loss: 1.4130 - 24s/epoch - 246us/sample
Epoch 60/70
95501/95501 - 24s - loss: 1.4120 - val_loss: 1.4123 - 24s/epoch - 249us/sample
Epoch 61/70
95501/95501 - 24s - loss: 1.4119 - val_loss: 1.4121 - 24s/epoch - 247us/sample
Epoch 62/70
95501/95501 - 24s - loss: 1.4119 - val_loss: 1.4123 - 24s/epoch - 246us/sample
Epoch 63/70
95501/95501 - 24s - loss: 1.4118 - val_loss: 1.4142 - 24s/epoch - 249us/sample
Epoch 64/70
95501/95501 - 24s - loss: 1.4124 - val_loss: 1.4132 - 24s/epoch - 249us/sample
Epoch 65/70
95501/95501 - 23s - loss: 1.4118 - val_loss: 1.4120 - 23s/epoch - 245us/sample
Epoch 66/70
95501/95501 - 24s - loss: 1.4121 - val_loss: 1.4139 - 24s/epoch - 250us/sample
Epoch 67/70
95501/95501 - 23s - loss: 1.4118 - val_loss: 1.4132 - 23s/epoch - 245us/sample
Epoch 68/70
95501/95501 - 24s - loss: 1.4125 - val_loss: 1.4124 - 24s/epoch - 248us/sample
Epoch 69/70
95501/95501 - 24s - loss: 1.4114 - val_loss: 1.4130 - 24s/epoch - 248us/sample
Epoch 70/70
95501/95501 - 23s - loss: 1.4122 - val_loss: 1.4124 - 23s/epoch - 243us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.4124002874549093
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 15:45:44.725122: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2951 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7814598517268446
cosine 0.9128976423273487
MAE: 8.462898
RMSE: 24.09274
r2: -37652.45124831767
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'binary_crossentropy', 16, 70, 0.0005, 0.1, 126, 1.412175138160619, 1.4124002874549093, 0.7814598517268446, 0.9128976423273487, 8.462898254394531, 24.09273910522461, -37652.45124831767, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 3160)        12640       ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 3160)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          398286      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          398286      ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4425970     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,232,582
Trainable params: 9,219,690
Non-trainable params: 12,892
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 15:45:52.318153: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/dense_enc0_3/bias/m/Assign' id:4816 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/dense_enc0_3/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/dense_enc0_3/bias/m, training_6/Adam/dense_enc0_3/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 15:46:02.628357: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4330 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 11s - loss: 0.0099 - val_loss: 0.0053 - 11s/epoch - 119us/sample
Epoch 2/50
95501/95501 - 10s - loss: 0.0038 - val_loss: 0.0032 - 10s/epoch - 104us/sample
Epoch 3/50
95501/95501 - 10s - loss: 0.0032 - val_loss: 0.0086 - 10s/epoch - 103us/sample
Epoch 4/50
95501/95501 - 10s - loss: 0.0027 - val_loss: 0.0021 - 10s/epoch - 104us/sample
Epoch 5/50
95501/95501 - 10s - loss: 0.0022 - val_loss: 0.0058 - 10s/epoch - 104us/sample
Epoch 6/50
95501/95501 - 10s - loss: 0.1682 - val_loss: 0.0021 - 10s/epoch - 101us/sample
Epoch 7/50
95501/95501 - 10s - loss: 0.0021 - val_loss: 0.0021 - 10s/epoch - 101us/sample
Epoch 8/50
95501/95501 - 10s - loss: 0.0020 - val_loss: 0.0020 - 10s/epoch - 102us/sample
Epoch 9/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0017 - 9s/epoch - 92us/sample
Epoch 10/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 92us/sample
Epoch 11/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0039 - 9s/epoch - 91us/sample
Epoch 12/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 91us/sample
Epoch 13/50
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0019 - 9s/epoch - 92us/sample
Epoch 14/50
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 92us/sample
Epoch 15/50
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 93us/sample
Epoch 16/50
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 92us/sample
Epoch 17/50
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0044 - 9s/epoch - 92us/sample
Epoch 18/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0014 - 9s/epoch - 91us/sample
Epoch 19/50
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0014 - 9s/epoch - 92us/sample
Epoch 20/50
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0015 - 9s/epoch - 92us/sample
Epoch 21/50
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 92us/sample
Epoch 22/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 23/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 91us/sample
Epoch 24/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 25/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 26/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 27/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 28/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 29/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 30/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 31/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 32/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 33/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 34/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 35/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 36/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 37/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 38/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 39/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 40/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 41/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 42/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 43/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 44/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 45/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 46/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 47/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 48/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 49/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 50/50
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 92us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010657537762950467
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 15:53:20.142027: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4294 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014881696081637797
cosine 0.011762687621186735
MAE: 0.018032683
RMSE: 0.035665493
r2: 0.9174808633095554
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 64, 50, 0.0005, 0.1, 126, 0.0011048647588562488, 0.0010657537762950467, 0.014881696081637797, 0.011762687621186735, 0.018032683059573174, 0.035665493458509445, 0.9174808633095554, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 110 0.0005 8 0] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 2528)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          318654      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          318654      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3544330     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,389,670
Trainable params: 7,379,306
Non-trainable params: 10,364
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/110
2023-02-14 15:53:26.792983: W tensorflow/c/c_api.cc:291] Operation '{name:'training_8/Adam/batch_normalization_13/gamma/m/Assign' id:6217 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/batch_normalization_13/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/batch_normalization_13/gamma/m, training_8/Adam/batch_normalization_13/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 15:54:05.996482: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5636 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 42s - loss: 1.4611 - val_loss: 1.4551 - 42s/epoch - 435us/sample
Epoch 2/110
95501/95501 - 40s - loss: 1.4504 - val_loss: 1.4453 - 40s/epoch - 417us/sample
Epoch 3/110
95501/95501 - 40s - loss: 1.4420 - val_loss: 1.5040 - 40s/epoch - 419us/sample
Epoch 4/110
95501/95501 - 40s - loss: 1.4383 - val_loss: 1.4445 - 40s/epoch - 417us/sample
Epoch 5/110
95501/95501 - 40s - loss: 1.4383 - val_loss: 1.4396 - 40s/epoch - 418us/sample
Epoch 6/110
95501/95501 - 40s - loss: 1.4374 - val_loss: 1.4421 - 40s/epoch - 417us/sample
Epoch 7/110
95501/95501 - 40s - loss: 1.4369 - val_loss: 1.4410 - 40s/epoch - 419us/sample
Epoch 8/110
95501/95501 - 40s - loss: 1.4373 - val_loss: 1.4405 - 40s/epoch - 418us/sample
Epoch 9/110
95501/95501 - 40s - loss: 1.4377 - val_loss: 1.4396 - 40s/epoch - 417us/sample
Epoch 10/110
95501/95501 - 40s - loss: 1.4401 - val_loss: 1.4418 - 40s/epoch - 418us/sample
Epoch 11/110
95501/95501 - 40s - loss: 1.4395 - val_loss: 1.4414 - 40s/epoch - 418us/sample
Epoch 12/110
95501/95501 - 40s - loss: 1.4392 - val_loss: 1.4413 - 40s/epoch - 418us/sample
Epoch 13/110
95501/95501 - 40s - loss: 1.4391 - val_loss: 1.4413 - 40s/epoch - 418us/sample
Epoch 14/110
95501/95501 - 45s - loss: 1.4391 - val_loss: 1.4412 - 45s/epoch - 473us/sample
Epoch 15/110
95501/95501 - 48s - loss: 1.4390 - val_loss: 1.4412 - 48s/epoch - 506us/sample
Epoch 16/110
95501/95501 - 49s - loss: 1.4390 - val_loss: 1.4412 - 49s/epoch - 518us/sample
Epoch 17/110
95501/95501 - 49s - loss: 1.4388 - val_loss: 1.4401 - 49s/epoch - 515us/sample
Epoch 18/110
95501/95501 - 49s - loss: 1.4380 - val_loss: 1.4401 - 49s/epoch - 518us/sample
Epoch 19/110
95501/95501 - 49s - loss: 1.4380 - val_loss: 1.4401 - 49s/epoch - 516us/sample
Epoch 20/110
95501/95501 - 49s - loss: 1.4380 - val_loss: 1.4401 - 49s/epoch - 510us/sample
Epoch 21/110
95501/95501 - 50s - loss: 1.4380 - val_loss: 1.4401 - 50s/epoch - 518us/sample
Epoch 22/110
95501/95501 - 49s - loss: 1.4380 - val_loss: 1.4401 - 49s/epoch - 513us/sample
Epoch 23/110
95501/95501 - 50s - loss: 1.4380 - val_loss: 1.4401 - 50s/epoch - 519us/sample
Epoch 24/110
95501/95501 - 49s - loss: 1.4380 - val_loss: 1.4401 - 49s/epoch - 513us/sample
Epoch 25/110
95501/95501 - 50s - loss: 1.4380 - val_loss: 1.4401 - 50s/epoch - 519us/sample
Epoch 26/110
95501/95501 - 50s - loss: 1.4380 - val_loss: 1.4401 - 50s/epoch - 522us/sample
Epoch 27/110
95501/95501 - 49s - loss: 1.4380 - val_loss: 1.4401 - 49s/epoch - 508us/sample
Epoch 28/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 503us/sample
Epoch 29/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 503us/sample
Epoch 30/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 502us/sample
Epoch 31/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 504us/sample
Epoch 32/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 505us/sample
Epoch 33/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 506us/sample
Epoch 34/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 504us/sample
Epoch 35/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 501us/sample
Epoch 36/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 504us/sample
Epoch 37/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 499us/sample
Epoch 38/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 507us/sample
Epoch 39/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 501us/sample
Epoch 40/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 504us/sample
Epoch 41/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 499us/sample
Epoch 42/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4402 - 48s/epoch - 503us/sample
Epoch 43/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4398 - 48s/epoch - 502us/sample
Epoch 44/110
95501/95501 - 48s - loss: 1.4379 - val_loss: 1.4401 - 48s/epoch - 500us/sample
Epoch 45/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 503us/sample
Epoch 46/110
95501/95501 - 47s - loss: 1.4380 - val_loss: 1.4401 - 47s/epoch - 497us/sample
Epoch 47/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 504us/sample
Epoch 48/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 500us/sample
Epoch 49/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 504us/sample
Epoch 50/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 500us/sample
Epoch 51/110
95501/95501 - 49s - loss: 1.4380 - val_loss: 1.4401 - 49s/epoch - 509us/sample
Epoch 52/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 499us/sample
Epoch 53/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 500us/sample
Epoch 54/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 501us/sample
Epoch 55/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 500us/sample
Epoch 56/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 504us/sample
Epoch 57/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 500us/sample
Epoch 58/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 503us/sample
Epoch 59/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 499us/sample
Epoch 60/110
95501/95501 - 48s - loss: 1.4380 - val_loss: 1.4401 - 48s/epoch - 501us/sample
Epoch 61/110
95501/95501 - 48s - loss: 1.4401 - val_loss: 1.4480 - 48s/epoch - 499us/sample
Epoch 62/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 63/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 501us/sample
Epoch 64/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 65/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 66/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 498us/sample
Epoch 67/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 502us/sample
Epoch 68/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 504us/sample
Epoch 69/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 504us/sample
Epoch 70/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 502us/sample
Epoch 71/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 501us/sample
Epoch 72/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 500us/sample
Epoch 73/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 501us/sample
Epoch 74/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 501us/sample
Epoch 75/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 500us/sample
Epoch 76/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 505us/sample
Epoch 77/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 78/110
95501/95501 - 47s - loss: 1.4460 - val_loss: 1.4480 - 47s/epoch - 497us/sample
Epoch 79/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 508us/sample
Epoch 80/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 501us/sample
Epoch 81/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 502us/sample
Epoch 82/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 501us/sample
Epoch 83/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 84/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 498us/sample
Epoch 85/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 86/110
95501/95501 - 49s - loss: 1.4460 - val_loss: 1.4480 - 49s/epoch - 509us/sample
Epoch 87/110
95501/95501 - 47s - loss: 1.4460 - val_loss: 1.4480 - 47s/epoch - 494us/sample
Epoch 88/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 89/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 504us/sample
Epoch 90/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 504us/sample
Epoch 91/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 504us/sample
Epoch 92/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 500us/sample
Epoch 93/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 506us/sample
Epoch 94/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 502us/sample
Epoch 95/110
95501/95501 - 49s - loss: 1.4460 - val_loss: 1.4480 - 49s/epoch - 509us/sample
Epoch 96/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 97/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 501us/sample
Epoch 98/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 505us/sample
Epoch 99/110
95501/95501 - 47s - loss: 1.4460 - val_loss: 1.4480 - 47s/epoch - 495us/sample
Epoch 100/110
95501/95501 - 49s - loss: 1.4460 - val_loss: 1.4480 - 49s/epoch - 512us/sample
Epoch 101/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 498us/sample
Epoch 102/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 506us/sample
Epoch 103/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 504us/sample
Epoch 104/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 502us/sample
Epoch 105/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 505us/sample
Epoch 106/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 498us/sample
Epoch 107/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 504us/sample
Epoch 108/110
95501/95501 - 47s - loss: 1.4460 - val_loss: 1.4480 - 47s/epoch - 492us/sample
Epoch 109/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 503us/sample
Epoch 110/110
95501/95501 - 48s - loss: 1.4460 - val_loss: 1.4480 - 48s/epoch - 502us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.4480273534880463
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 17:19:52.301330: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:5588 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.8650109237952318
cosine 1.0806837913568927
MAE: 9.419954
RMSE: 17.481081
r2: -19816.229680507142
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'binary_crossentropy', 8, 110, 0.0005, 0.1, 126, 1.4460146337242967, 1.4480273534880463, 0.8650109237952318, 1.0806837913568927, 9.419954299926758, 17.481081008911133, -19816.229680507142, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 170 0.0005 16 0] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 2528)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          318654      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          318654      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3544330     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,389,670
Trainable params: 7,379,306
Non-trainable params: 10,364
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/170
2023-02-14 17:19:59.483602: W tensorflow/c/c_api.cc:291] Operation '{name:'training_10/Adam/batch_normalization_15/beta/m/Assign' id:7519 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/batch_normalization_15/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_10/Adam/batch_normalization_15/beta/m, training_10/Adam/batch_normalization_15/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 17:20:25.507409: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:6982 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 28s - loss: 1.4566 - val_loss: 1.4413 - 28s/epoch - 294us/sample
Epoch 2/170
95501/95501 - 27s - loss: 1.4403 - val_loss: 1.4359 - 27s/epoch - 278us/sample
Epoch 3/170
95501/95501 - 26s - loss: 1.4355 - val_loss: 1.4290 - 26s/epoch - 272us/sample
Epoch 4/170
95501/95501 - 27s - loss: 1.4248 - val_loss: 1.4161 - 27s/epoch - 279us/sample
Epoch 5/170
95501/95501 - 27s - loss: 1.4192 - val_loss: 1.4154 - 27s/epoch - 278us/sample
Epoch 6/170
95501/95501 - 27s - loss: 1.4184 - val_loss: 1.4445 - 27s/epoch - 280us/sample
Epoch 7/170
95501/95501 - 27s - loss: 1.4255 - val_loss: 1.4190 - 27s/epoch - 279us/sample
Epoch 8/170
95501/95501 - 26s - loss: 1.4155 - val_loss: 1.4129 - 26s/epoch - 275us/sample
Epoch 9/170
95501/95501 - 27s - loss: 1.4142 - val_loss: 1.4272 - 27s/epoch - 279us/sample
Epoch 10/170
95501/95501 - 27s - loss: 1.4128 - val_loss: 1.4110 - 27s/epoch - 278us/sample
Epoch 11/170
95501/95501 - 26s - loss: 1.4142 - val_loss: 1.4098 - 26s/epoch - 273us/sample
Epoch 12/170
95501/95501 - 27s - loss: 1.4076 - val_loss: 1.4095 - 27s/epoch - 279us/sample
Epoch 13/170
95501/95501 - 27s - loss: 1.4084 - val_loss: 1.4104 - 27s/epoch - 279us/sample
Epoch 14/170
95501/95501 - 27s - loss: 1.4096 - val_loss: 1.4131 - 27s/epoch - 279us/sample
Epoch 15/170
95501/95501 - 26s - loss: 1.4081 - val_loss: 1.4071 - 26s/epoch - 277us/sample
Epoch 16/170
95501/95501 - 27s - loss: 1.4096 - val_loss: 1.4085 - 27s/epoch - 278us/sample
Epoch 17/170
95501/95501 - 26s - loss: 1.4080 - val_loss: 1.4121 - 26s/epoch - 277us/sample
Epoch 18/170
95501/95501 - 26s - loss: 1.4094 - val_loss: 1.4089 - 26s/epoch - 277us/sample
Epoch 19/170
95501/95501 - 27s - loss: 1.4071 - val_loss: 1.4074 - 27s/epoch - 280us/sample
Epoch 20/170
95501/95501 - 27s - loss: 1.4074 - val_loss: 1.4074 - 27s/epoch - 278us/sample
Epoch 21/170
95501/95501 - 27s - loss: 1.4058 - val_loss: 1.4074 - 27s/epoch - 281us/sample
Epoch 22/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4073 - 26s/epoch - 275us/sample
Epoch 23/170
95501/95501 - 26s - loss: 1.4082 - val_loss: 1.4091 - 26s/epoch - 276us/sample
Epoch 24/170
95501/95501 - 26s - loss: 1.4114 - val_loss: 1.4156 - 26s/epoch - 274us/sample
Epoch 25/170
95501/95501 - 26s - loss: 1.4082 - val_loss: 1.4075 - 26s/epoch - 274us/sample
Epoch 26/170
95501/95501 - 27s - loss: 1.4081 - val_loss: 1.4099 - 27s/epoch - 282us/sample
Epoch 27/170
95501/95501 - 26s - loss: 1.4068 - val_loss: 1.4077 - 26s/epoch - 275us/sample
Epoch 28/170
95501/95501 - 27s - loss: 1.4062 - val_loss: 1.4084 - 27s/epoch - 281us/sample
Epoch 29/170
95501/95501 - 27s - loss: 1.4063 - val_loss: 1.4065 - 27s/epoch - 278us/sample
Epoch 30/170
95501/95501 - 27s - loss: 1.4055 - val_loss: 1.4070 - 27s/epoch - 279us/sample
Epoch 31/170
95501/95501 - 27s - loss: 1.4050 - val_loss: 1.4064 - 27s/epoch - 278us/sample
Epoch 32/170
95501/95501 - 26s - loss: 1.4066 - val_loss: 1.4057 - 26s/epoch - 277us/sample
Epoch 33/170
95501/95501 - 26s - loss: 1.4047 - val_loss: 1.4070 - 26s/epoch - 277us/sample
Epoch 34/170
95501/95501 - 27s - loss: 1.4133 - val_loss: 1.4056 - 27s/epoch - 278us/sample
Epoch 35/170
95501/95501 - 26s - loss: 1.4081 - val_loss: 1.4083 - 26s/epoch - 277us/sample
Epoch 36/170
95501/95501 - 26s - loss: 1.4069 - val_loss: 1.4074 - 26s/epoch - 277us/sample
Epoch 37/170
95501/95501 - 26s - loss: 1.4061 - val_loss: 1.4081 - 26s/epoch - 275us/sample
Epoch 38/170
95501/95501 - 26s - loss: 1.4072 - val_loss: 1.4063 - 26s/epoch - 274us/sample
Epoch 39/170
95501/95501 - 27s - loss: 1.4059 - val_loss: 1.4056 - 27s/epoch - 279us/sample
Epoch 40/170
95501/95501 - 27s - loss: 1.4058 - val_loss: 1.4058 - 27s/epoch - 279us/sample
Epoch 41/170
95501/95501 - 27s - loss: 1.4048 - val_loss: 1.4054 - 27s/epoch - 278us/sample
Epoch 42/170
95501/95501 - 27s - loss: 1.4043 - val_loss: 1.4068 - 27s/epoch - 278us/sample
Epoch 43/170
95501/95501 - 26s - loss: 1.4042 - val_loss: 1.4059 - 26s/epoch - 274us/sample
Epoch 44/170
95501/95501 - 26s - loss: 1.4055 - val_loss: 1.4051 - 26s/epoch - 277us/sample
Epoch 45/170
95501/95501 - 27s - loss: 1.4044 - val_loss: 1.4054 - 27s/epoch - 283us/sample
Epoch 46/170
95501/95501 - 26s - loss: 1.4046 - val_loss: 1.4053 - 26s/epoch - 271us/sample
Epoch 47/170
95501/95501 - 22s - loss: 1.4043 - val_loss: 1.4103 - 22s/epoch - 228us/sample
Epoch 48/170
95501/95501 - 22s - loss: 1.4076 - val_loss: 1.4078 - 22s/epoch - 229us/sample
Epoch 49/170
95501/95501 - 22s - loss: 1.4058 - val_loss: 1.4057 - 22s/epoch - 228us/sample
Epoch 50/170
95501/95501 - 22s - loss: 1.4063 - val_loss: 1.4130 - 22s/epoch - 228us/sample
Epoch 51/170
95501/95501 - 22s - loss: 1.4054 - val_loss: 1.4104 - 22s/epoch - 227us/sample
Epoch 52/170
95501/95501 - 22s - loss: 1.4047 - val_loss: 1.4089 - 22s/epoch - 228us/sample
Epoch 53/170
95501/95501 - 22s - loss: 1.4120 - val_loss: 1.4074 - 22s/epoch - 228us/sample
Epoch 54/170
95501/95501 - 22s - loss: 1.4058 - val_loss: 1.4074 - 22s/epoch - 228us/sample
Epoch 55/170
95501/95501 - 22s - loss: 1.4058 - val_loss: 1.4071 - 22s/epoch - 228us/sample
Epoch 56/170
95501/95501 - 22s - loss: 1.4061 - val_loss: 1.4064 - 22s/epoch - 229us/sample
Epoch 57/170
95501/95501 - 22s - loss: 1.4051 - val_loss: 1.4056 - 22s/epoch - 228us/sample
Epoch 58/170
95501/95501 - 22s - loss: 1.4046 - val_loss: 1.4064 - 22s/epoch - 227us/sample
Epoch 59/170
95501/95501 - 22s - loss: 1.4053 - val_loss: 1.4049 - 22s/epoch - 229us/sample
Epoch 60/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4052 - 22s/epoch - 228us/sample
Epoch 61/170
95501/95501 - 22s - loss: 1.4046 - val_loss: 1.4088 - 22s/epoch - 228us/sample
Epoch 62/170
95501/95501 - 22s - loss: 1.4049 - val_loss: 1.4059 - 22s/epoch - 228us/sample
Epoch 63/170
95501/95501 - 22s - loss: 1.4048 - val_loss: 1.4050 - 22s/epoch - 229us/sample
Epoch 64/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4044 - 22s/epoch - 228us/sample
Epoch 65/170
95501/95501 - 22s - loss: 1.4042 - val_loss: 1.4052 - 22s/epoch - 228us/sample
Epoch 66/170
95501/95501 - 22s - loss: 1.4059 - val_loss: 1.4053 - 22s/epoch - 228us/sample
Epoch 67/170
95501/95501 - 22s - loss: 1.4044 - val_loss: 1.4068 - 22s/epoch - 228us/sample
Epoch 68/170
95501/95501 - 22s - loss: 1.4038 - val_loss: 1.4055 - 22s/epoch - 228us/sample
Epoch 69/170
95501/95501 - 22s - loss: 1.4040 - val_loss: 1.4048 - 22s/epoch - 228us/sample
Epoch 70/170
95501/95501 - 22s - loss: 1.4051 - val_loss: 1.4066 - 22s/epoch - 228us/sample
Epoch 71/170
95501/95501 - 22s - loss: 1.4045 - val_loss: 1.4046 - 22s/epoch - 228us/sample
Epoch 72/170
95501/95501 - 22s - loss: 1.4038 - val_loss: 1.4058 - 22s/epoch - 228us/sample
Epoch 73/170
95501/95501 - 22s - loss: 1.4039 - val_loss: 1.4054 - 22s/epoch - 228us/sample
Epoch 74/170
95501/95501 - 22s - loss: 1.4059 - val_loss: 1.4064 - 22s/epoch - 229us/sample
Epoch 75/170
95501/95501 - 22s - loss: 1.4044 - val_loss: 1.4111 - 22s/epoch - 228us/sample
Epoch 76/170
95501/95501 - 22s - loss: 1.4047 - val_loss: 1.4057 - 22s/epoch - 228us/sample
Epoch 77/170
95501/95501 - 22s - loss: 1.4042 - val_loss: 1.4088 - 22s/epoch - 228us/sample
Epoch 78/170
95501/95501 - 22s - loss: 1.4044 - val_loss: 1.4052 - 22s/epoch - 228us/sample
Epoch 79/170
95501/95501 - 22s - loss: 1.4053 - val_loss: 1.4051 - 22s/epoch - 228us/sample
Epoch 80/170
95501/95501 - 22s - loss: 1.4043 - val_loss: 1.4056 - 22s/epoch - 228us/sample
Epoch 81/170
95501/95501 - 22s - loss: 1.4041 - val_loss: 1.4205 - 22s/epoch - 229us/sample
Epoch 82/170
95501/95501 - 22s - loss: 1.4085 - val_loss: 1.4065 - 22s/epoch - 229us/sample
Epoch 83/170
95501/95501 - 22s - loss: 1.4040 - val_loss: 1.4093 - 22s/epoch - 228us/sample
Epoch 84/170
95501/95501 - 22s - loss: 1.4038 - val_loss: 1.4058 - 22s/epoch - 228us/sample
Epoch 85/170
95501/95501 - 22s - loss: 1.4048 - val_loss: 1.4075 - 22s/epoch - 229us/sample
Epoch 86/170
95501/95501 - 22s - loss: 1.4040 - val_loss: 1.4090 - 22s/epoch - 228us/sample
Epoch 87/170
95501/95501 - 22s - loss: 1.4063 - val_loss: 1.4121 - 22s/epoch - 228us/sample
Epoch 88/170
95501/95501 - 22s - loss: 1.4062 - val_loss: 1.4096 - 22s/epoch - 227us/sample
Epoch 89/170
95501/95501 - 22s - loss: 1.4053 - val_loss: 1.4071 - 22s/epoch - 229us/sample
Epoch 90/170
95501/95501 - 22s - loss: 1.4049 - val_loss: 1.4079 - 22s/epoch - 228us/sample
Epoch 91/170
95501/95501 - 22s - loss: 1.4054 - val_loss: 1.4056 - 22s/epoch - 229us/sample
Epoch 92/170
95501/95501 - 22s - loss: 1.4047 - val_loss: 1.4054 - 22s/epoch - 229us/sample
Epoch 93/170
95501/95501 - 22s - loss: 1.4040 - val_loss: 1.4076 - 22s/epoch - 229us/sample
Epoch 94/170
95501/95501 - 22s - loss: 1.4039 - val_loss: 1.4050 - 22s/epoch - 227us/sample
Epoch 95/170
95501/95501 - 22s - loss: 1.4067 - val_loss: 1.4088 - 22s/epoch - 228us/sample
Epoch 96/170
95501/95501 - 22s - loss: 1.4048 - val_loss: 1.4090 - 22s/epoch - 229us/sample
Epoch 97/170
95501/95501 - 22s - loss: 1.4047 - val_loss: 1.4056 - 22s/epoch - 228us/sample
Epoch 98/170
95501/95501 - 22s - loss: 1.4042 - val_loss: 1.4060 - 22s/epoch - 228us/sample
Epoch 99/170
95501/95501 - 22s - loss: 1.4032 - val_loss: 1.4046 - 22s/epoch - 230us/sample
Epoch 100/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4055 - 22s/epoch - 229us/sample
Epoch 101/170
95501/95501 - 22s - loss: 1.4042 - val_loss: 1.4052 - 22s/epoch - 229us/sample
Epoch 102/170
95501/95501 - 22s - loss: 1.4031 - val_loss: 1.4076 - 22s/epoch - 228us/sample
Epoch 103/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4048 - 22s/epoch - 229us/sample
Epoch 104/170
95501/95501 - 22s - loss: 1.4055 - val_loss: 1.4066 - 22s/epoch - 228us/sample
Epoch 105/170
95501/95501 - 22s - loss: 1.4035 - val_loss: 1.4048 - 22s/epoch - 229us/sample
Epoch 106/170
95501/95501 - 22s - loss: 1.4054 - val_loss: 1.4060 - 22s/epoch - 230us/sample
Epoch 107/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4048 - 22s/epoch - 229us/sample
Epoch 108/170
95501/95501 - 22s - loss: 1.4034 - val_loss: 1.4046 - 22s/epoch - 229us/sample
Epoch 109/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4047 - 22s/epoch - 230us/sample
Epoch 110/170
95501/95501 - 22s - loss: 1.4053 - val_loss: 1.4121 - 22s/epoch - 229us/sample
Epoch 111/170
95501/95501 - 22s - loss: 1.4050 - val_loss: 1.4055 - 22s/epoch - 229us/sample
Epoch 112/170
95501/95501 - 22s - loss: 1.4043 - val_loss: 1.4085 - 22s/epoch - 229us/sample
Epoch 113/170
95501/95501 - 22s - loss: 1.4030 - val_loss: 1.4049 - 22s/epoch - 229us/sample
Epoch 114/170
95501/95501 - 22s - loss: 1.4040 - val_loss: 1.4048 - 22s/epoch - 228us/sample
Epoch 115/170
95501/95501 - 22s - loss: 1.4056 - val_loss: 1.4053 - 22s/epoch - 228us/sample
Epoch 116/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4057 - 22s/epoch - 229us/sample
Epoch 117/170
95501/95501 - 22s - loss: 1.4043 - val_loss: 1.4049 - 22s/epoch - 229us/sample
Epoch 118/170
95501/95501 - 22s - loss: 1.4043 - val_loss: 1.4060 - 22s/epoch - 229us/sample
Epoch 119/170
95501/95501 - 22s - loss: 1.4038 - val_loss: 1.4052 - 22s/epoch - 230us/sample
Epoch 120/170
95501/95501 - 22s - loss: 1.4031 - val_loss: 1.4045 - 22s/epoch - 229us/sample
Epoch 121/170
95501/95501 - 22s - loss: 1.4046 - val_loss: 1.4062 - 22s/epoch - 228us/sample
Epoch 122/170
95501/95501 - 22s - loss: 1.4044 - val_loss: 1.4048 - 22s/epoch - 228us/sample
Epoch 123/170
95501/95501 - 22s - loss: 1.4040 - val_loss: 1.4061 - 22s/epoch - 230us/sample
Epoch 124/170
95501/95501 - 22s - loss: 1.4049 - val_loss: 1.4053 - 22s/epoch - 228us/sample
Epoch 125/170
95501/95501 - 22s - loss: 1.4038 - val_loss: 1.4055 - 22s/epoch - 228us/sample
Epoch 126/170
95501/95501 - 22s - loss: 1.4045 - val_loss: 1.4055 - 22s/epoch - 229us/sample
Epoch 127/170
95501/95501 - 22s - loss: 1.4045 - val_loss: 1.4050 - 22s/epoch - 229us/sample
Epoch 128/170
95501/95501 - 22s - loss: 1.4084 - val_loss: 1.4111 - 22s/epoch - 229us/sample
Epoch 129/170
95501/95501 - 22s - loss: 1.4068 - val_loss: 1.4061 - 22s/epoch - 229us/sample
Epoch 130/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4084 - 22s/epoch - 230us/sample
Epoch 131/170
95501/95501 - 22s - loss: 1.4056 - val_loss: 1.4103 - 22s/epoch - 228us/sample
Epoch 132/170
95501/95501 - 22s - loss: 1.4052 - val_loss: 1.4067 - 22s/epoch - 228us/sample
Epoch 133/170
95501/95501 - 22s - loss: 1.4060 - val_loss: 1.4138 - 22s/epoch - 229us/sample
Epoch 134/170
95501/95501 - 22s - loss: 1.4055 - val_loss: 1.4049 - 22s/epoch - 230us/sample
Epoch 135/170
95501/95501 - 22s - loss: 1.4052 - val_loss: 1.4049 - 22s/epoch - 229us/sample
Epoch 136/170
95501/95501 - 22s - loss: 1.4042 - val_loss: 1.4079 - 22s/epoch - 228us/sample
Epoch 137/170
95501/95501 - 22s - loss: 1.4039 - val_loss: 1.4052 - 22s/epoch - 230us/sample
Epoch 138/170
95501/95501 - 22s - loss: 1.4035 - val_loss: 1.4058 - 22s/epoch - 229us/sample
Epoch 139/170
95501/95501 - 22s - loss: 1.4033 - val_loss: 1.4112 - 22s/epoch - 228us/sample
Epoch 140/170
95501/95501 - 22s - loss: 1.4030 - val_loss: 1.4055 - 22s/epoch - 229us/sample
Epoch 141/170
95501/95501 - 22s - loss: 1.4033 - val_loss: 1.4063 - 22s/epoch - 230us/sample
Epoch 142/170
95501/95501 - 22s - loss: 1.4034 - val_loss: 1.4050 - 22s/epoch - 228us/sample
Epoch 143/170
95501/95501 - 22s - loss: 1.4028 - val_loss: 1.4043 - 22s/epoch - 229us/sample
Epoch 144/170
95501/95501 - 22s - loss: 1.4032 - val_loss: 1.4043 - 22s/epoch - 230us/sample
Epoch 145/170
95501/95501 - 22s - loss: 1.4029 - val_loss: 1.4046 - 22s/epoch - 229us/sample
Epoch 146/170
95501/95501 - 22s - loss: 1.4035 - val_loss: 1.4162 - 22s/epoch - 229us/sample
Epoch 147/170
95501/95501 - 22s - loss: 1.4068 - val_loss: 1.4051 - 22s/epoch - 229us/sample
Epoch 148/170
95501/95501 - 22s - loss: 1.4041 - val_loss: 1.4045 - 22s/epoch - 229us/sample
Epoch 149/170
95501/95501 - 22s - loss: 1.4032 - val_loss: 1.4056 - 22s/epoch - 229us/sample
Epoch 150/170
95501/95501 - 22s - loss: 1.4033 - val_loss: 1.4047 - 22s/epoch - 228us/sample
Epoch 151/170
95501/95501 - 22s - loss: 1.4031 - val_loss: 1.4047 - 22s/epoch - 229us/sample
Epoch 152/170
95501/95501 - 22s - loss: 1.4036 - val_loss: 1.4115 - 22s/epoch - 229us/sample
Epoch 153/170
95501/95501 - 22s - loss: 1.4040 - val_loss: 1.4068 - 22s/epoch - 228us/sample
Epoch 154/170
95501/95501 - 22s - loss: 1.4069 - val_loss: 1.4047 - 22s/epoch - 228us/sample
Epoch 155/170
95501/95501 - 22s - loss: 1.4033 - val_loss: 1.4047 - 22s/epoch - 229us/sample
Epoch 156/170
95501/95501 - 22s - loss: 1.4028 - val_loss: 1.4049 - 22s/epoch - 229us/sample
Epoch 157/170
95501/95501 - 22s - loss: 1.4031 - val_loss: 1.4052 - 22s/epoch - 228us/sample
Epoch 158/170
95501/95501 - 22s - loss: 1.4032 - val_loss: 1.4047 - 22s/epoch - 228us/sample
Epoch 159/170
95501/95501 - 22s - loss: 1.4031 - val_loss: 1.4059 - 22s/epoch - 230us/sample
Epoch 160/170
95501/95501 - 22s - loss: 1.4034 - val_loss: 1.4049 - 22s/epoch - 228us/sample
Epoch 161/170
95501/95501 - 22s - loss: 1.4031 - val_loss: 1.4049 - 22s/epoch - 228us/sample
Epoch 162/170
95501/95501 - 22s - loss: 1.4034 - val_loss: 1.4050 - 22s/epoch - 228us/sample
Epoch 163/170
95501/95501 - 22s - loss: 1.4030 - val_loss: 1.4083 - 22s/epoch - 229us/sample
Epoch 164/170
95501/95501 - 22s - loss: 1.4052 - val_loss: 1.4049 - 22s/epoch - 228us/sample
Epoch 165/170
95501/95501 - 22s - loss: 1.4063 - val_loss: 1.4050 - 22s/epoch - 229us/sample
Epoch 166/170
95501/95501 - 22s - loss: 1.4029 - val_loss: 1.4049 - 22s/epoch - 228us/sample
Epoch 167/170
95501/95501 - 22s - loss: 1.4036 - val_loss: 1.4089 - 22s/epoch - 230us/sample
Epoch 168/170
95501/95501 - 22s - loss: 1.4031 - val_loss: 1.4052 - 22s/epoch - 228us/sample
Epoch 169/170
95501/95501 - 22s - loss: 1.4028 - val_loss: 1.4042 - 22s/epoch - 230us/sample
Epoch 170/170
95501/95501 - 22s - loss: 1.4037 - val_loss: 1.4053 - 22s/epoch - 228us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.4053202252364634
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:25:25.762437: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:6934 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7927191349652988
cosine 0.8903257021335376
MAE: 17.413427
RMSE: 64.103035
r2: -266558.06011700904
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'binary_crossentropy', 16, 170, 0.0005, 0.1, 126, 1.4036559570452425, 1.4053202252364634, 0.7927191349652988, 0.8903257021335376, 17.413427352905273, 64.10303497314453, -266558.06011700904, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 10 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 2528)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          318654      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          318654      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3544330     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,389,670
Trainable params: 7,379,306
Non-trainable params: 10,364
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 18:25:32.966449: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_20/gamma/Assign' id:8113 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_20/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_20/gamma, batch_normalization_20/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:25:53.967578: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:8306 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 23s - loss: 0.0116 - val_loss: 0.0057 - 23s/epoch - 240us/sample
Epoch 2/10
95501/95501 - 21s - loss: 0.0053 - val_loss: 0.0038 - 21s/epoch - 221us/sample
Epoch 3/10
95501/95501 - 21s - loss: 0.0041 - val_loss: 0.0032 - 21s/epoch - 221us/sample
Epoch 4/10
95501/95501 - 21s - loss: 0.0035 - val_loss: 0.0029 - 21s/epoch - 220us/sample
Epoch 5/10
95501/95501 - 21s - loss: 0.0033 - val_loss: 0.0028 - 21s/epoch - 220us/sample
Epoch 6/10
95501/95501 - 21s - loss: 0.0031 - val_loss: 0.0026 - 21s/epoch - 220us/sample
Epoch 7/10
95501/95501 - 21s - loss: 0.0030 - val_loss: 0.0025 - 21s/epoch - 220us/sample
Epoch 8/10
95501/95501 - 21s - loss: 0.0029 - val_loss: 0.0025 - 21s/epoch - 221us/sample
Epoch 9/10
95501/95501 - 21s - loss: 0.0029 - val_loss: 0.0025 - 21s/epoch - 221us/sample
Epoch 10/10
95501/95501 - 21s - loss: 0.0028 - val_loss: 0.0024 - 21s/epoch - 220us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0023746831297644165
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:29:05.161188: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:8277 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01983789523906902
cosine 0.01576035060030624
MAE: 0.021691713
RMSE: 0.041419275
r2: 0.8887083786458022
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 16, 10, 0.001, 0.1, 126, 0.0027973553109948784, 0.0023746831297644165, 0.01983789523906902, 0.01576035060030624, 0.021691713482141495, 0.04141927510499954, 0.8887083786458022, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 1896)         0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/90
2023-02-14 18:29:12.538018: W tensorflow/c/c_api.cc:291] Operation '{name:'training_14/Adam/batch_normalization_22/beta/v/Assign' id:10203 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/batch_normalization_22/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/batch_normalization_22/beta/v, training_14/Adam/batch_normalization_22/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:29:25.474617: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:9567 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 15s - loss: 0.0109 - val_loss: 0.0061 - 15s/epoch - 154us/sample
Epoch 2/90
95501/95501 - 13s - loss: 0.0058 - val_loss: 0.0042 - 13s/epoch - 131us/sample
Epoch 3/90
95501/95501 - 12s - loss: 0.0043 - val_loss: 0.0036 - 12s/epoch - 130us/sample
Epoch 4/90
95501/95501 - 12s - loss: 0.0037 - val_loss: 0.0030 - 12s/epoch - 130us/sample
Epoch 5/90
95501/95501 - 12s - loss: 0.0030 - val_loss: 0.0026 - 12s/epoch - 130us/sample
Epoch 6/90
95501/95501 - 12s - loss: 0.0028 - val_loss: 0.0024 - 12s/epoch - 130us/sample
Epoch 7/90
95501/95501 - 12s - loss: 0.0026 - val_loss: 0.0023 - 12s/epoch - 130us/sample
Epoch 8/90
95501/95501 - 12s - loss: 0.0025 - val_loss: 0.0022 - 12s/epoch - 130us/sample
Epoch 9/90
95501/95501 - 12s - loss: 0.0024 - val_loss: 0.0022 - 12s/epoch - 130us/sample
Epoch 10/90
95501/95501 - 13s - loss: 0.0023 - val_loss: 0.0021 - 13s/epoch - 131us/sample
Epoch 11/90
95501/95501 - 12s - loss: 0.0023 - val_loss: 0.0021 - 12s/epoch - 130us/sample
Epoch 12/90
95501/95501 - 12s - loss: 0.0023 - val_loss: 0.0021 - 12s/epoch - 130us/sample
Epoch 13/90
95501/95501 - 12s - loss: 0.0022 - val_loss: 0.0020 - 12s/epoch - 130us/sample
Epoch 14/90
95501/95501 - 12s - loss: 0.0022 - val_loss: 0.0020 - 12s/epoch - 130us/sample
Epoch 15/90
95501/95501 - 12s - loss: 0.0022 - val_loss: 0.0020 - 12s/epoch - 130us/sample
Epoch 16/90
95501/95501 - 12s - loss: 0.0022 - val_loss: 0.0019 - 12s/epoch - 130us/sample
Epoch 17/90
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0019 - 13s/epoch - 131us/sample
Epoch 18/90
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 131us/sample
Epoch 19/90
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 130us/sample
Epoch 20/90
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 130us/sample
Epoch 21/90
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 130us/sample
Epoch 22/90
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 130us/sample
Epoch 23/90
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 131us/sample
Epoch 24/90
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 131us/sample
Epoch 25/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 26/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 27/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 28/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 29/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 30/90
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 131us/sample
Epoch 31/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 32/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 33/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 34/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 35/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 36/90
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 132us/sample
Epoch 37/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 38/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 39/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 40/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 41/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 42/90
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 131us/sample
Epoch 43/90
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 44/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 45/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 46/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 47/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 48/90
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 131us/sample
Epoch 49/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 50/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 51/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 52/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 53/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 54/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 55/90
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 131us/sample
Epoch 56/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 57/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 58/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 59/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 60/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 61/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 62/90
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 131us/sample
Epoch 63/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 64/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 65/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 66/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 67/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 68/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 131us/sample
Epoch 69/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 131us/sample
Epoch 70/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 71/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 72/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 73/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 74/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 75/90
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 131us/sample
Epoch 76/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 77/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 78/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 79/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 80/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 81/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 82/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 131us/sample
Epoch 83/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 131us/sample
Epoch 84/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0018 - 12s/epoch - 130us/sample
Epoch 85/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 86/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 87/90
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0017 - 12s/epoch - 130us/sample
Epoch 88/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 131us/sample
Epoch 89/90
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 131us/sample
Epoch 90/90
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0017 - 12s/epoch - 130us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0016889714103109946
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:47:54.355456: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:9538 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0118569157571106
cosine 0.009370646261512851
MAE: 0.016190097
RMSE: 0.03184946
r2: 0.9341947455109029
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 32, 90, 0.001, 0.1, 126, 0.0018302668884010648, 0.0016889714103109946, 0.0118569157571106, 0.009370646261512851, 0.016190096735954285, 0.0318494588136673, 0.9341947455109029, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 10 0.001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 632)         2528        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 632)          0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          79758       ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          79758       ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         899410      ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,860,934
Trainable params: 1,858,154
Non-trainable params: 2,780
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 18:48:01.877977: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_26/beta/Assign' id:10626 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_26/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_26/beta, batch_normalization_26/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:48:11.037049: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/mul' id:10827 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 11s - loss: 8.6260 - val_loss: 0.4359 - 11s/epoch - 113us/sample
Epoch 2/10
95501/95501 - 9s - loss: 0.4533 - val_loss: 0.4676 - 9s/epoch - 89us/sample
Epoch 3/10
95501/95501 - 8s - loss: 0.4299 - val_loss: 0.3528 - 8s/epoch - 89us/sample
Epoch 4/10
95501/95501 - 9s - loss: 0.3863 - val_loss: 0.3216 - 9s/epoch - 89us/sample
Epoch 5/10
95501/95501 - 9s - loss: 0.3428 - val_loss: 0.3139 - 9s/epoch - 90us/sample
Epoch 6/10
95501/95501 - 9s - loss: 0.3225 - val_loss: 0.2944 - 9s/epoch - 89us/sample
Epoch 7/10
95501/95501 - 9s - loss: 0.4058 - val_loss: 0.2935 - 9s/epoch - 89us/sample
Epoch 8/10
95501/95501 - 9s - loss: 0.3116 - val_loss: 0.2736 - 9s/epoch - 89us/sample
Epoch 9/10
95501/95501 - 9s - loss: 0.3005 - val_loss: 0.2719 - 9s/epoch - 89us/sample
Epoch 10/10
95501/95501 - 9s - loss: 0.3419 - val_loss: 0.2957 - 9s/epoch - 89us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.29574751443922226
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 18:49:28.811358: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_8/outputlayer/BiasAdd' id:10779 op device:{requested: '', assigned: ''} def:{{{node decoder_model_8/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_8/outputlayer/MatMul, decoder_model_8/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2160086727961252
cosine 0.2964749471153959
MAE: 0.42782944
RMSE: 1.7278347
r2: -192.53378213814847
RMSE zero-vector: 0.23411466903540806
['0.5custom_VAE', 'binary_crossentropy', 64, 10, 0.001, 0.1, 126, 0.3418643473286917, 0.29574751443922226, 0.2160086727961252, 0.2964749471153959, 0.42782944440841675, 1.727834701538086, -192.53378213814847, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 1896)         0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/30
2023-02-14 18:49:36.466123: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/bottleneck_zlog_9/bias/m/Assign' id:12628 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/bottleneck_zlog_9/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/bottleneck_zlog_9/bias/m, training_18/Adam/bottleneck_zlog_9/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 18:50:13.326415: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_9/mul' id:12130 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 40s - loss: 0.0112 - val_loss: 0.0092 - 40s/epoch - 421us/sample
Epoch 2/30
95501/95501 - 37s - loss: 0.0062 - val_loss: 0.0165 - 37s/epoch - 387us/sample
Epoch 3/30
95501/95501 - 37s - loss: 0.0053 - val_loss: 0.0262 - 37s/epoch - 390us/sample
Epoch 4/30
95501/95501 - 37s - loss: 0.0049 - val_loss: 0.0264 - 37s/epoch - 391us/sample
Epoch 5/30
95501/95501 - 37s - loss: 0.0047 - val_loss: 0.0282 - 37s/epoch - 389us/sample
Epoch 6/30
95501/95501 - 37s - loss: 0.0046 - val_loss: 0.0306 - 37s/epoch - 390us/sample
Epoch 7/30
95501/95501 - 37s - loss: 0.0045 - val_loss: 0.0261 - 37s/epoch - 391us/sample
Epoch 8/30
95501/95501 - 37s - loss: 0.0044 - val_loss: 0.0254 - 37s/epoch - 390us/sample
Epoch 9/30
95501/95501 - 37s - loss: 0.0043 - val_loss: 0.0279 - 37s/epoch - 390us/sample
Epoch 10/30
95501/95501 - 37s - loss: 0.0043 - val_loss: 0.0272 - 37s/epoch - 390us/sample
Epoch 11/30
95501/95501 - 37s - loss: 0.0043 - val_loss: 0.0244 - 37s/epoch - 389us/sample
Epoch 12/30
95501/95501 - 37s - loss: 0.0042 - val_loss: 0.0253 - 37s/epoch - 388us/sample
Epoch 13/30
95501/95501 - 37s - loss: 0.0042 - val_loss: 0.0275 - 37s/epoch - 390us/sample
Epoch 14/30
95501/95501 - 37s - loss: 0.0041 - val_loss: 0.0251 - 37s/epoch - 390us/sample
Epoch 15/30
95501/95501 - 37s - loss: 0.0041 - val_loss: 0.0256 - 37s/epoch - 392us/sample
Epoch 16/30
95501/95501 - 37s - loss: 0.0041 - val_loss: 0.0259 - 37s/epoch - 389us/sample
Epoch 17/30
95501/95501 - 37s - loss: 0.0040 - val_loss: 0.0225 - 37s/epoch - 390us/sample
Epoch 18/30
95501/95501 - 37s - loss: 0.0040 - val_loss: 0.0227 - 37s/epoch - 391us/sample
Epoch 19/30
95501/95501 - 37s - loss: 0.0040 - val_loss: 0.0241 - 37s/epoch - 391us/sample
Epoch 20/30
95501/95501 - 37s - loss: 0.0040 - val_loss: 0.0206 - 37s/epoch - 390us/sample
Epoch 21/30
95501/95501 - 38s - loss: 0.0040 - val_loss: 0.0221 - 38s/epoch - 395us/sample
Epoch 22/30
95501/95501 - 37s - loss: 0.0040 - val_loss: 0.0223 - 37s/epoch - 391us/sample
Epoch 23/30
95501/95501 - 37s - loss: 0.0039 - val_loss: 0.0203 - 37s/epoch - 388us/sample
Epoch 24/30
95501/95501 - 37s - loss: 0.0039 - val_loss: 0.0227 - 37s/epoch - 391us/sample
Epoch 25/30
95501/95501 - 37s - loss: 0.0039 - val_loss: 0.0210 - 37s/epoch - 388us/sample
Epoch 26/30
95501/95501 - 37s - loss: 0.0039 - val_loss: 0.0223 - 37s/epoch - 390us/sample
Epoch 27/30
95501/95501 - 37s - loss: 0.0039 - val_loss: 0.0209 - 37s/epoch - 390us/sample
Epoch 28/30
95501/95501 - 37s - loss: 0.0039 - val_loss: 0.0241 - 37s/epoch - 389us/sample
Epoch 29/30
95501/95501 - 37s - loss: 0.0039 - val_loss: 0.0247 - 37s/epoch - 390us/sample
Epoch 30/30
95501/95501 - 37s - loss: 0.0038 - val_loss: 0.0240 - 37s/epoch - 390us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.024008675995839234
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:08:16.245783: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:12101 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.07100550880914717
cosine 0.06511971157618482
MAE: 0.046085574
RMSE: 0.15337124
r2: -0.5259822833482949
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 8, 30, 0.0005, 0.1, 126, 0.003846241723208689, 0.024008675995839234, 0.07100550880914717, 0.06511971157618482, 0.046085573732852936, 0.15337124466896057, -0.5259822833482949, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 50 0.0005 16 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 2022)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          254898      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          254898      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2838460     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,914,174
Trainable params: 5,905,834
Non-trainable params: 8,340
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 19:08:24.381521: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/bottleneck_zmean_10/kernel/m/Assign' id:13905 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/bottleneck_zmean_10/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/bottleneck_zmean_10/kernel/m, training_20/Adam/bottleneck_zmean_10/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:08:46.287520: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:13398 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 24s - loss: 0.0058 - val_loss: 0.0029 - 24s/epoch - 256us/sample
Epoch 2/50
95501/95501 - 22s - loss: 0.0029 - val_loss: 0.0021 - 22s/epoch - 225us/sample
Epoch 3/50
95501/95501 - 22s - loss: 0.0023 - val_loss: 0.0018 - 22s/epoch - 227us/sample
Epoch 4/50
95501/95501 - 21s - loss: 0.0020 - val_loss: 0.0017 - 21s/epoch - 225us/sample
Epoch 5/50
95501/95501 - 22s - loss: 0.0019 - val_loss: 0.0016 - 22s/epoch - 225us/sample
Epoch 6/50
95501/95501 - 22s - loss: 0.0018 - val_loss: 0.0016 - 22s/epoch - 225us/sample
Epoch 7/50
95501/95501 - 22s - loss: 0.0018 - val_loss: 0.0015 - 22s/epoch - 226us/sample
Epoch 8/50
95501/95501 - 22s - loss: 0.0017 - val_loss: 0.0016 - 22s/epoch - 226us/sample
Epoch 9/50
95501/95501 - 22s - loss: 0.0017 - val_loss: 0.0015 - 22s/epoch - 226us/sample
Epoch 10/50
95501/95501 - 22s - loss: 0.0017 - val_loss: 0.0015 - 22s/epoch - 227us/sample
Epoch 11/50
95501/95501 - 22s - loss: 0.0017 - val_loss: 0.0014 - 22s/epoch - 225us/sample
Epoch 12/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 225us/sample
Epoch 13/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 226us/sample
Epoch 14/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 227us/sample
Epoch 15/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 225us/sample
Epoch 16/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 226us/sample
Epoch 17/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 227us/sample
Epoch 18/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 226us/sample
Epoch 19/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 226us/sample
Epoch 20/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 225us/sample
Epoch 21/50
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 227us/sample
Epoch 22/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 226us/sample
Epoch 23/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 226us/sample
Epoch 24/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 226us/sample
Epoch 25/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 26/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0014 - 22s/epoch - 227us/sample
Epoch 27/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0014 - 22s/epoch - 227us/sample
Epoch 28/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 29/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 30/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 31/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 32/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 33/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 34/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 35/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 36/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 37/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 38/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 39/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 40/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 41/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 42/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 226us/sample
Epoch 43/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 44/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 45/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 46/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 47/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 228us/sample
Epoch 48/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 49/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
Epoch 50/50
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 227us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0013105852599264516
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:26:28.397517: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:13362 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.019650821247911437
cosine 0.015562507714633892
MAE: 0.020780753
RMSE: 0.0412263
r2: 0.8897425692044372
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'logcosh', 16, 50, 0.0005, 0.1, 126, 0.001457552030591617, 0.0013105852599264516, 0.019650821247911437, 0.015562507714633892, 0.02078075334429741, 0.04122630134224892, 0.8897425692044372, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.0012000000000000001 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 1896)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-14 19:26:36.915577: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/batch_normalization_35/gamma/m/Assign' id:15226 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/batch_normalization_35/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/batch_normalization_35/gamma/m, training_22/Adam/batch_normalization_35/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:26:50.697146: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:14685 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 16s - loss: 0.0110 - val_loss: 0.0067 - 16s/epoch - 169us/sample
Epoch 2/150
95501/95501 - 13s - loss: 0.0059 - val_loss: 0.0044 - 13s/epoch - 135us/sample
Epoch 3/150
95501/95501 - 13s - loss: 0.0043 - val_loss: 0.0033 - 13s/epoch - 135us/sample
Epoch 4/150
95501/95501 - 13s - loss: 0.0034 - val_loss: 0.0031 - 13s/epoch - 135us/sample
Epoch 5/150
95501/95501 - 13s - loss: 0.0030 - val_loss: 0.0026 - 13s/epoch - 135us/sample
Epoch 6/150
95501/95501 - 13s - loss: 0.0028 - val_loss: 0.0024 - 13s/epoch - 135us/sample
Epoch 7/150
95501/95501 - 13s - loss: 0.0026 - val_loss: 0.0023 - 13s/epoch - 135us/sample
Epoch 8/150
95501/95501 - 13s - loss: 0.0025 - val_loss: 0.0023 - 13s/epoch - 136us/sample
Epoch 9/150
95501/95501 - 13s - loss: 0.0024 - val_loss: 0.0031 - 13s/epoch - 135us/sample
Epoch 10/150
95501/95501 - 13s - loss: 0.0024 - val_loss: 0.0021 - 13s/epoch - 135us/sample
Epoch 11/150
95501/95501 - 13s - loss: 0.0023 - val_loss: 0.0022 - 13s/epoch - 135us/sample
Epoch 12/150
95501/95501 - 13s - loss: 0.0023 - val_loss: 0.0020 - 13s/epoch - 135us/sample
Epoch 13/150
95501/95501 - 13s - loss: 0.0022 - val_loss: 0.0020 - 13s/epoch - 135us/sample
Epoch 14/150
95501/95501 - 13s - loss: 0.0022 - val_loss: 0.0020 - 13s/epoch - 135us/sample
Epoch 15/150
95501/95501 - 13s - loss: 0.0022 - val_loss: 0.0020 - 13s/epoch - 136us/sample
Epoch 16/150
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0020 - 13s/epoch - 135us/sample
Epoch 17/150
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0019 - 13s/epoch - 135us/sample
Epoch 18/150
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0019 - 13s/epoch - 135us/sample
Epoch 19/150
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0019 - 13s/epoch - 135us/sample
Epoch 20/150
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0019 - 13s/epoch - 135us/sample
Epoch 21/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0019 - 13s/epoch - 136us/sample
Epoch 22/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0019 - 13s/epoch - 135us/sample
Epoch 23/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0019 - 13s/epoch - 136us/sample
Epoch 24/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0019 - 13s/epoch - 135us/sample
Epoch 25/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 26/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 27/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 136us/sample
Epoch 28/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 29/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 30/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 31/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 32/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 33/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 34/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 136us/sample
Epoch 35/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 36/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 37/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 38/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 39/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 40/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 136us/sample
Epoch 41/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 42/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 43/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 44/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 45/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 46/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 47/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 48/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 49/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 50/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 51/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 52/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 53/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 54/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 55/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 56/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 57/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 58/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 137us/sample
Epoch 59/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 60/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 61/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 62/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 63/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 137us/sample
Epoch 64/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 65/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 66/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 67/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 136us/sample
Epoch 68/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 69/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0018 - 13s/epoch - 134us/sample
Epoch 70/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 71/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 72/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 73/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 74/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 75/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 76/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 77/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 78/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 79/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 80/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 81/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 82/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 83/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 84/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 85/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 86/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 87/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 88/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 89/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 90/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 91/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 92/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 93/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 94/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 95/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 96/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 97/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 98/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 99/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 100/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 101/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 102/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 103/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 104/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 105/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 106/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 107/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 134us/sample
Epoch 108/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 109/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 110/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 111/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 112/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 113/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 114/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 135us/sample
Epoch 115/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 116/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 117/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 118/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 119/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 120/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 121/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 122/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 123/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 124/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 125/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 126/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 127/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 135us/sample
Epoch 128/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 129/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 130/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 131/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 132/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 133/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 135us/sample
Epoch 134/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 135/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 133us/sample
Epoch 136/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 137/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 138/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 139/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 135us/sample
Epoch 140/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 141/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 142/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 143/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 144/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 145/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 134us/sample
Epoch 146/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 147/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 148/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 149/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
Epoch 150/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 133us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0016135541417471337
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:58:44.142765: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:14656 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.011208670165374331
cosine 0.00884704742843234
MAE: 0.015781166
RMSE: 0.031011907
r2: 0.9376105389976959
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 32, 150, 0.0012000000000000001, 0.1, 126, 0.0017561330657294245, 0.0016135541417471337, 0.011208670165374331, 0.00884704742843234, 0.015781166031956673, 0.031011907383799553, 0.9376105389976959, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 50 0.0005 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 1769)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          223020      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          223020      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2485525     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,176,426
Trainable params: 5,169,098
Non-trainable params: 7,328
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 19:58:53.095876: W tensorflow/c/c_api.cc:291] Operation '{name:'training_24/Adam/batch_normalization_38/beta/m/Assign' id:16527 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/batch_normalization_38/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_24/Adam/batch_normalization_38/beta/m, training_24/Adam/batch_normalization_38/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:59:02.745113: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:15953 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 12s - loss: 0.0185 - val_loss: 0.0042 - 12s/epoch - 124us/sample
Epoch 2/50
95501/95501 - 9s - loss: 0.0037 - val_loss: 0.0031 - 9s/epoch - 90us/sample
Epoch 3/50
95501/95501 - 9s - loss: 0.0030 - val_loss: 0.0025 - 9s/epoch - 90us/sample
Epoch 4/50
95501/95501 - 9s - loss: 0.0026 - val_loss: 0.0023 - 9s/epoch - 90us/sample
Epoch 5/50
95501/95501 - 9s - loss: 0.0023 - val_loss: 0.0019 - 9s/epoch - 90us/sample
Epoch 6/50
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0018 - 9s/epoch - 90us/sample
Epoch 7/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0017 - 9s/epoch - 90us/sample
Epoch 8/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 90us/sample
Epoch 9/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0015 - 9s/epoch - 90us/sample
Epoch 10/50
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 91us/sample
Epoch 11/50
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0014 - 9s/epoch - 91us/sample
Epoch 12/50
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 90us/sample
Epoch 13/50
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0014 - 9s/epoch - 90us/sample
Epoch 14/50
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0019 - 9s/epoch - 90us/sample
Epoch 15/50
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0013 - 9s/epoch - 90us/sample
Epoch 16/50
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 90us/sample
Epoch 17/50
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 90us/sample
Epoch 18/50
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 91us/sample
Epoch 19/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0013 - 9s/epoch - 91us/sample
Epoch 20/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 21/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0013 - 9s/epoch - 90us/sample
Epoch 22/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 23/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 24/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 25/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 26/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 27/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 28/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 91us/sample
Epoch 29/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 91us/sample
Epoch 30/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 31/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 32/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 33/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 90us/sample
Epoch 34/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 90us/sample
Epoch 35/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 90us/sample
Epoch 36/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0013 - 9s/epoch - 90us/sample
Epoch 37/50
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0011 - 9s/epoch - 90us/sample
Epoch 38/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 39/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 91us/sample
Epoch 40/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 41/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 42/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 43/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 44/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 45/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 46/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 91us/sample
Epoch 47/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 48/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 49/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
Epoch 50/50
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 91us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.001106023417265866
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:06:07.772882: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:15917 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01534042736291436
cosine 0.012113381310584093
MAE: 0.018458515
RMSE: 0.03629617
r2: 0.9145365017804866
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'logcosh', 64, 50, 0.0005, 0.1, 126, 0.001154847420339043, 0.001106023417265866, 0.01534042736291436, 0.012113381310584093, 0.018458515405654907, 0.03629617020487785, 0.9145365017804866, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 145 0.0012 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 3160)        12640       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 3160)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          398286      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          398286      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4425970     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,232,582
Trainable params: 9,219,690
Non-trainable params: 12,892
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-14 20:06:16.819693: W tensorflow/c/c_api.cc:291] Operation '{name:'training_26/Adam/beta_1/Assign' id:17671 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/beta_1, training_26/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:06:27.342303: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:17240 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 13s - loss: 0.0192 - val_loss: 0.0077 - 13s/epoch - 135us/sample
Epoch 2/145
95501/95501 - 9s - loss: 0.1574 - val_loss: 0.0136 - 9s/epoch - 96us/sample
Epoch 3/145
95501/95501 - 9s - loss: 0.0062 - val_loss: 0.0048 - 9s/epoch - 97us/sample
Epoch 4/145
95501/95501 - 9s - loss: 0.0149 - val_loss: 7072.7388 - 9s/epoch - 98us/sample
Epoch 5/145
95501/95501 - 9s - loss: 0.0049 - val_loss: 0.0036 - 9s/epoch - 96us/sample
Epoch 6/145
95501/95501 - 9s - loss: 0.0039 - val_loss: 0.0034 - 9s/epoch - 97us/sample
Epoch 7/145
95501/95501 - 9s - loss: 0.0035 - val_loss: 0.0031 - 9s/epoch - 96us/sample
Epoch 8/145
95501/95501 - 9s - loss: 0.0038 - val_loss: 0.0038 - 9s/epoch - 96us/sample
Epoch 9/145
95501/95501 - 9s - loss: 0.0033 - val_loss: 0.0030 - 9s/epoch - 96us/sample
Epoch 10/145
95501/95501 - 9s - loss: 0.0029 - val_loss: 0.0029 - 9s/epoch - 96us/sample
Epoch 11/145
95501/95501 - 9s - loss: 0.0028 - val_loss: 0.0028 - 9s/epoch - 96us/sample
Epoch 12/145
95501/95501 - 9s - loss: 0.0041 - val_loss: 0.0033 - 9s/epoch - 97us/sample
Epoch 13/145
95501/95501 - 9s - loss: 0.0028 - val_loss: 0.0039 - 9s/epoch - 96us/sample
Epoch 14/145
95501/95501 - 9s - loss: 0.0026 - val_loss: 0.0055 - 9s/epoch - 96us/sample
Epoch 15/145
95501/95501 - 9s - loss: 0.0038 - val_loss: 0.0025 - 9s/epoch - 98us/sample
Epoch 16/145
95501/95501 - 9s - loss: 0.0026 - val_loss: 0.0025 - 9s/epoch - 96us/sample
Epoch 17/145
95501/95501 - 9s - loss: 0.0025 - val_loss: 0.0032 - 9s/epoch - 96us/sample
Epoch 18/145
95501/95501 - 9s - loss: 0.0024 - val_loss: 0.0024 - 9s/epoch - 96us/sample
Epoch 19/145
95501/95501 - 9s - loss: 0.0024 - val_loss: 0.0029 - 9s/epoch - 96us/sample
Epoch 20/145
95501/95501 - 9s - loss: 0.0023 - val_loss: 0.0024 - 9s/epoch - 96us/sample
Epoch 21/145
95501/95501 - 9s - loss: 0.0022 - val_loss: 0.0024 - 9s/epoch - 96us/sample
Epoch 22/145
95501/95501 - 9s - loss: 0.0022 - val_loss: 0.0020 - 9s/epoch - 96us/sample
Epoch 23/145
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0043 - 9s/epoch - 98us/sample
Epoch 24/145
95501/95501 - 9s - loss: 0.0025 - val_loss: 0.0019 - 9s/epoch - 97us/sample
Epoch 25/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0020 - 9s/epoch - 96us/sample
Epoch 26/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0021 - 9s/epoch - 96us/sample
Epoch 27/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0023 - 9s/epoch - 96us/sample
Epoch 28/145
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0019 - 9s/epoch - 96us/sample
Epoch 29/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 97us/sample
Epoch 30/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 31/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 32/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0018 - 9s/epoch - 97us/sample
Epoch 33/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0021 - 9s/epoch - 96us/sample
Epoch 34/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 35/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 36/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 37/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0022 - 9s/epoch - 95us/sample
Epoch 38/145
95501/95501 - 9s - loss: 0.0026 - val_loss: 0.0018 - 9s/epoch - 95us/sample
Epoch 39/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 95us/sample
Epoch 40/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 41/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 42/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0022 - 9s/epoch - 96us/sample
Epoch 43/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0026 - 9s/epoch - 95us/sample
Epoch 44/145
95501/95501 - 9s - loss: 0.0022 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 45/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 46/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 47/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0020 - 9s/epoch - 95us/sample
Epoch 48/145
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 49/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 50/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 51/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 52/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 53/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 54/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 55/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 56/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 57/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 58/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 59/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 60/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 61/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 95us/sample
Epoch 62/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0018 - 9s/epoch - 95us/sample
Epoch 63/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 64/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 65/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 66/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 67/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 68/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 69/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 70/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 71/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 72/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 73/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 74/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 75/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 76/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 77/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 78/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 79/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0018 - 9s/epoch - 95us/sample
Epoch 80/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 81/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0020 - 9s/epoch - 95us/sample
Epoch 82/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 83/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 95us/sample
Epoch 84/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 85/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 86/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 87/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0018 - 9s/epoch - 97us/sample
Epoch 88/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 89/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0019 - 9s/epoch - 97us/sample
Epoch 90/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 91/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 92/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 93/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 94/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 95/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 96/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 97/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 98/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 99/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 100/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 101/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 102/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 103/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0019 - 9s/epoch - 98us/sample
Epoch 104/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0018 - 9s/epoch - 96us/sample
Epoch 105/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 106/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 107/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 108/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 109/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 110/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 111/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 112/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 113/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 114/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 115/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 116/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 117/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0017 - 9s/epoch - 96us/sample
Epoch 118/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 119/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 120/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 121/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 122/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 123/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 124/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 97us/sample
Epoch 125/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 126/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0016 - 9s/epoch - 96us/sample
Epoch 127/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 98us/sample
Epoch 128/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 129/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 130/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 131/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 132/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 133/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 134/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 135/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 136/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 137/145
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 138/145
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 139/145
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 140/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 141/145
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 142/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 96us/sample
Epoch 143/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 144/145
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0015 - 9s/epoch - 97us/sample
Epoch 145/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 98us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0015424597935376867
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:28:31.875586: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:17211 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.010712851781104584
cosine 0.008464440562974001
MAE: 0.015250867
RMSE: 0.03026532
r2: 0.9405792002592746
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'mse', 64, 145, 0.0012, 0.1, 126, 0.0015967299713075217, 0.0015424597935376867, 0.010712851781104584, 0.008464440562974001, 0.01525086723268032, 0.03026532009243965, 0.9405792002592746, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0005 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 2022)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          254898      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          254898      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2838460     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,914,174
Trainable params: 5,905,834
Non-trainable params: 8,340
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-14 20:28:41.794376: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_42/moving_variance/Assign' id:18105 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_42/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_42/moving_variance, batch_normalization_42/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:28:52.010945: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:18508 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 13s - loss: 0.0070 - val_loss: 0.0039 - 13s/epoch - 133us/sample
Epoch 2/145
95501/95501 - 9s - loss: 0.0035 - val_loss: 0.0033 - 9s/epoch - 93us/sample
Epoch 3/145
95501/95501 - 9s - loss: 0.0029 - val_loss: 0.0046 - 9s/epoch - 94us/sample
Epoch 4/145
95501/95501 - 9s - loss: 0.0027 - val_loss: 0.0022 - 9s/epoch - 93us/sample
Epoch 5/145
95501/95501 - 9s - loss: 0.0022 - val_loss: 0.0019 - 9s/epoch - 93us/sample
Epoch 6/145
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0018 - 9s/epoch - 94us/sample
Epoch 7/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0023 - 9s/epoch - 93us/sample
Epoch 8/145
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 94us/sample
Epoch 9/145
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0015 - 9s/epoch - 93us/sample
Epoch 10/145
95501/95501 - 9s - loss: 0.0016 - val_loss: 0.0015 - 9s/epoch - 92us/sample
Epoch 11/145
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0014 - 9s/epoch - 92us/sample
Epoch 12/145
95501/95501 - 9s - loss: 0.0015 - val_loss: 0.0013 - 9s/epoch - 92us/sample
Epoch 13/145
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 93us/sample
Epoch 14/145
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 92us/sample
Epoch 15/145
95501/95501 - 9s - loss: 0.0014 - val_loss: 0.0013 - 9s/epoch - 92us/sample
Epoch 16/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0013 - 9s/epoch - 92us/sample
Epoch 17/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0013 - 9s/epoch - 93us/sample
Epoch 18/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 93us/sample
Epoch 19/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 20/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 21/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 22/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 23/145
95501/95501 - 9s - loss: 0.0013 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 24/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 93us/sample
Epoch 25/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 26/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 27/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 93us/sample
Epoch 28/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 29/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 30/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 31/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 32/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 33/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 34/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 92us/sample
Epoch 35/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 36/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 37/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 38/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 92us/sample
Epoch 39/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 40/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 41/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 42/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 43/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0012 - 9s/epoch - 94us/sample
Epoch 44/145
95501/95501 - 9s - loss: 0.0012 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 45/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 46/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 47/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 48/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 49/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 50/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 51/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 52/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 53/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 54/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 55/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 56/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 57/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 58/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 59/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 60/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 61/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 62/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 63/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 64/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 65/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 66/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 67/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 68/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 69/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 70/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 71/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 95us/sample
Epoch 72/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 73/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 74/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 75/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 76/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 77/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 78/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 79/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 80/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 95us/sample
Epoch 81/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 82/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 83/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 84/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 85/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 86/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 87/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 88/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 89/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 90/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 91/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 92/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 93/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 94/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 95/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 96/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 97/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 98/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 95us/sample
Epoch 99/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 100/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 101/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 102/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 103/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 104/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 105/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 106/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 107/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 108/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 109/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 110/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 111/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 112/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 113/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 114/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 115/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 116/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 117/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 118/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 119/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 120/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 121/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0012 - 9s/epoch - 93us/sample
Epoch 122/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 123/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 124/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 125/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 126/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 127/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 128/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 129/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 130/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 131/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 95us/sample
Epoch 132/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 133/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 134/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 135/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 136/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 137/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 138/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 139/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 140/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
Epoch 141/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 95us/sample
Epoch 142/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 94us/sample
Epoch 143/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 93us/sample
Epoch 144/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 93us/sample
Epoch 145/145
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0010 - 9s/epoch - 94us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010133983086951883
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:50:17.658366: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:18472 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.013435332304659403
cosine 0.010597603948126188
MAE: 0.0168747
RMSE: 0.033895172
r2: 0.9254694173273669
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'logcosh', 64, 145, 0.0005, 0.1, 126, 0.0010536746335901147, 0.0010133983086951883, 0.013435332304659403, 0.010597603948126188, 0.016874700784683228, 0.03389517217874527, 0.9254694173273669, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4 50 0.0005 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3033)         3836745     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 3033)        12132       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 3033)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          382284      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          382284      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4248805     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 8,862,250
Trainable params: 8,849,866
Non-trainable params: 12,384
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 20:50:28.231583: W tensorflow/c/c_api.cc:291] Operation '{name:'training_30/Adam/bottleneck_zlog_15/kernel/v/Assign' id:20404 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/bottleneck_zlog_15/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_30/Adam/bottleneck_zlog_15/kernel/v, training_30/Adam/bottleneck_zlog_15/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:50:39.070527: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:19792 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 14s - loss: 0.0149 - val_loss: 0.0071 - 14s/epoch - 142us/sample
Epoch 2/50
95501/95501 - 9s - loss: 0.0065 - val_loss: 0.0118 - 9s/epoch - 99us/sample
Epoch 3/50
95501/95501 - 9s - loss: 0.0057 - val_loss: 0.0070 - 9s/epoch - 98us/sample
Epoch 4/50
95501/95501 - 9s - loss: 0.0047 - val_loss: 0.0069 - 9s/epoch - 98us/sample
Epoch 5/50
95501/95501 - 9s - loss: 0.0049 - val_loss: 0.0035 - 9s/epoch - 98us/sample
Epoch 6/50
95501/95501 - 9s - loss: 0.0035 - val_loss: 0.0044 - 9s/epoch - 98us/sample
Epoch 7/50
95501/95501 - 9s - loss: 0.0032 - val_loss: 0.0028 - 9s/epoch - 98us/sample
Epoch 8/50
95501/95501 - 9s - loss: 0.0029 - val_loss: 0.0044 - 9s/epoch - 97us/sample
Epoch 9/50
95501/95501 - 9s - loss: 0.0027 - val_loss: 0.0025 - 9s/epoch - 98us/sample
Epoch 10/50
95501/95501 - 9s - loss: 0.0025 - val_loss: 0.0024 - 9s/epoch - 98us/sample
Epoch 11/50
95501/95501 - 9s - loss: 0.0024 - val_loss: 0.0023 - 9s/epoch - 98us/sample
Epoch 12/50
95501/95501 - 9s - loss: 0.0023 - val_loss: 0.0021 - 9s/epoch - 98us/sample
Epoch 13/50
95501/95501 - 9s - loss: 0.0022 - val_loss: 0.0029 - 9s/epoch - 98us/sample
Epoch 14/50
95501/95501 - 9s - loss: 0.0024 - val_loss: 0.0020 - 9s/epoch - 97us/sample
Epoch 15/50
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0020 - 9s/epoch - 98us/sample
Epoch 16/50
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0020 - 9s/epoch - 98us/sample
Epoch 17/50
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0019 - 9s/epoch - 98us/sample
Epoch 18/50
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0019 - 9s/epoch - 98us/sample
Epoch 19/50
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0021 - 9s/epoch - 98us/sample
Epoch 20/50
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0018 - 9s/epoch - 98us/sample
Epoch 21/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0020 - 9s/epoch - 99us/sample
Epoch 22/50
95501/95501 - 9s - loss: 0.0020 - val_loss: 0.0018 - 9s/epoch - 98us/sample
Epoch 23/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 98us/sample
Epoch 24/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 98us/sample
Epoch 25/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 98us/sample
Epoch 26/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 98us/sample
Epoch 27/50
95501/95501 - 9s - loss: 0.0019 - val_loss: 0.0018 - 9s/epoch - 98us/sample
Epoch 28/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 29/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 30/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0021 - 9s/epoch - 98us/sample
Epoch 31/50
95501/95501 - 9s - loss: 0.0021 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 32/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 33/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 34/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 35/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 97us/sample
Epoch 36/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 37/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 38/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0017 - 9s/epoch - 99us/sample
Epoch 39/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 40/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 41/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 42/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 43/50
95501/95501 - 9s - loss: 0.0018 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 44/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 98us/sample
Epoch 45/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 46/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 47/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 99us/sample
Epoch 48/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 49/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0016 - 9s/epoch - 98us/sample
Epoch 50/50
95501/95501 - 9s - loss: 0.0017 - val_loss: 0.0017 - 9s/epoch - 98us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0016776460132154763
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:58:18.566259: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:19763 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01174141450890765
cosine 0.00930615021087878
MAE: 0.017341789
RMSE: 0.031809732
r2: 0.9343592333939812
RMSE zero-vector: 0.23411466903540806
['2.4custom_VAE', 'mse', 64, 50, 0.0005, 0.1, 126, 0.0016697443836082878, 0.0016776460132154763, 0.01174141450890765, 0.00930615021087878, 0.017341788858175278, 0.0318097323179245, 0.9343592333939812, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 45 0.0005 32 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 1896)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/45
2023-02-14 20:58:28.795684: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/batch_normalization_48/beta/v/Assign' id:21676 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/batch_normalization_48/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/batch_normalization_48/beta/v, training_32/Adam/batch_normalization_48/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:58:43.502454: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:21063 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 18s - loss: 0.0060 - val_loss: 0.0031 - 18s/epoch - 186us/sample
Epoch 2/45
95501/95501 - 13s - loss: 0.0033 - val_loss: 0.0026 - 13s/epoch - 140us/sample
Epoch 3/45
95501/95501 - 13s - loss: 0.0024 - val_loss: 0.0019 - 13s/epoch - 141us/sample
Epoch 4/45
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0017 - 13s/epoch - 140us/sample
Epoch 5/45
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 140us/sample
Epoch 6/45
95501/95501 - 13s - loss: 0.0017 - val_loss: 0.0015 - 13s/epoch - 140us/sample
Epoch 7/45
95501/95501 - 13s - loss: 0.0016 - val_loss: 0.0015 - 13s/epoch - 140us/sample
Epoch 8/45
95501/95501 - 13s - loss: 0.0016 - val_loss: 0.0014 - 13s/epoch - 140us/sample
Epoch 9/45
95501/95501 - 13s - loss: 0.0015 - val_loss: 0.0014 - 13s/epoch - 140us/sample
Epoch 10/45
95501/95501 - 13s - loss: 0.0015 - val_loss: 0.0014 - 13s/epoch - 140us/sample
Epoch 11/45
95501/95501 - 13s - loss: 0.0015 - val_loss: 0.0013 - 13s/epoch - 141us/sample
Epoch 12/45
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0013 - 13s/epoch - 140us/sample
Epoch 13/45
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0013 - 13s/epoch - 140us/sample
Epoch 14/45
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0013 - 13s/epoch - 140us/sample
Epoch 15/45
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0013 - 13s/epoch - 140us/sample
Epoch 16/45
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 17/45
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0012 - 13s/epoch - 141us/sample
Epoch 18/45
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 19/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 20/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 21/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 22/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 23/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 141us/sample
Epoch 24/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 141us/sample
Epoch 25/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 26/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 27/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 28/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 29/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 141us/sample
Epoch 30/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 31/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 32/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 33/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 34/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 35/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 141us/sample
Epoch 36/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 37/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 38/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 39/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 140us/sample
Epoch 40/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0011 - 13s/epoch - 140us/sample
Epoch 41/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0012 - 13s/epoch - 141us/sample
Epoch 42/45
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0011 - 13s/epoch - 140us/sample
Epoch 43/45
95501/95501 - 13s - loss: 0.0012 - val_loss: 0.0011 - 13s/epoch - 140us/sample
Epoch 44/45
95501/95501 - 13s - loss: 0.0012 - val_loss: 0.0011 - 13s/epoch - 140us/sample
Epoch 45/45
95501/95501 - 13s - loss: 0.0012 - val_loss: 0.0012 - 13s/epoch - 140us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0012095869599381315
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:08:34.334710: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:21027 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.017006682717166575
cosine 0.013455613150131105
MAE: 0.01925473
RMSE: 0.038146075
r2: 0.9056029822157883
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 32, 45, 0.0005, 0.1, 126, 0.001244504458814766, 0.0012095869599381315, 0.017006682717166575, 0.013455613150131105, 0.01925472915172577, 0.03814607486128807, 0.9056029822157883, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0012 64 0] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 3160)        12640       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 3160)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          398286      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          398286      ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4425970     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,232,582
Trainable params: 9,219,690
Non-trainable params: 12,892
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 21:08:44.959608: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_52/gamma/Assign' id:22072 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_52/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_52/gamma, batch_normalization_52/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:08:56.288688: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:22366 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 14s - loss: 790.7586 - val_loss: 1.4647 - 14s/epoch - 151us/sample
Epoch 2/50
95501/95501 - 10s - loss: 1.4469 - val_loss: 1.4386 - 10s/epoch - 101us/sample
Epoch 3/50
95501/95501 - 10s - loss: 1.4535 - val_loss: 1.4425 - 10s/epoch - 100us/sample
Epoch 4/50
95501/95501 - 10s - loss: 1.4576 - val_loss: 1.4792 - 10s/epoch - 100us/sample
Epoch 5/50
95501/95501 - 10s - loss: 5087261357.8375 - val_loss: 1.4592 - 10s/epoch - 100us/sample
Epoch 6/50
95501/95501 - 10s - loss: 1.4417 - val_loss: 1.4607 - 10s/epoch - 101us/sample
Epoch 7/50
95501/95501 - 10s - loss: 1.4534 - val_loss: 1.4267 - 10s/epoch - 101us/sample
Epoch 8/50
95501/95501 - 10s - loss: 1.4867 - val_loss: 1.4326 - 10s/epoch - 101us/sample
Epoch 9/50
95501/95501 - 10s - loss: 1.4434 - val_loss: 1.4633 - 10s/epoch - 100us/sample
Epoch 10/50
95501/95501 - 10s - loss: 1.4461 - val_loss: 1.4319 - 10s/epoch - 101us/sample
Epoch 11/50
95501/95501 - 10s - loss: 1.4243 - val_loss: 1.4172 - 10s/epoch - 101us/sample
Epoch 12/50
95501/95501 - 10s - loss: 1.4841 - val_loss: 1.4393 - 10s/epoch - 100us/sample
Epoch 13/50
95501/95501 - 10s - loss: 1.4631 - val_loss: 1.4792 - 10s/epoch - 100us/sample
Epoch 14/50
95501/95501 - 10s - loss: 1.4344 - val_loss: 1.4234 - 10s/epoch - 101us/sample
Epoch 15/50
95501/95501 - 10s - loss: 1.4344 - val_loss: 1.4253 - 10s/epoch - 100us/sample
Epoch 16/50
95501/95501 - 10s - loss: 1.4435 - val_loss: 3.8423 - 10s/epoch - 101us/sample
Epoch 17/50
95501/95501 - 10s - loss: 1.4204 - val_loss: 1.4228 - 10s/epoch - 100us/sample
Epoch 18/50
95501/95501 - 10s - loss: 1.4521 - val_loss: 1.5968 - 10s/epoch - 101us/sample
Epoch 19/50
95501/95501 - 10s - loss: 1.4483 - val_loss: 1.4233 - 10s/epoch - 102us/sample
Epoch 20/50
95501/95501 - 10s - loss: 1.4182 - val_loss: 1.4165 - 10s/epoch - 100us/sample
Epoch 21/50
95501/95501 - 10s - loss: 1.4212 - val_loss: 1.4132 - 10s/epoch - 101us/sample
Epoch 22/50
95501/95501 - 10s - loss: 1.4285 - val_loss: 1.4261 - 10s/epoch - 100us/sample
Epoch 23/50
95501/95501 - 10s - loss: 1.4166 - val_loss: 1.4181 - 10s/epoch - 100us/sample
Epoch 24/50
95501/95501 - 10s - loss: 1.4355 - val_loss: 1.4674 - 10s/epoch - 101us/sample
Epoch 25/50
95501/95501 - 10s - loss: 1.4263 - val_loss: 1.4458 - 10s/epoch - 100us/sample
Epoch 26/50
95501/95501 - 10s - loss: 1.4363 - val_loss: 1.4185 - 10s/epoch - 100us/sample
Epoch 27/50
95501/95501 - 10s - loss: 1.4316 - val_loss: 1.4319 - 10s/epoch - 101us/sample
Epoch 28/50
95501/95501 - 10s - loss: 1.4663 - val_loss: 1.5783 - 10s/epoch - 100us/sample
Epoch 29/50
95501/95501 - 10s - loss: 1.4659 - val_loss: 1.4285 - 10s/epoch - 101us/sample
Epoch 30/50
95501/95501 - 10s - loss: 1.4207 - val_loss: 1.4206 - 10s/epoch - 100us/sample
Epoch 31/50
95501/95501 - 10s - loss: 1.4351 - val_loss: 1.4456 - 10s/epoch - 100us/sample
Epoch 32/50
95501/95501 - 10s - loss: 1.4197 - val_loss: 1.4136 - 10s/epoch - 101us/sample
Epoch 33/50
95501/95501 - 10s - loss: 1.4376 - val_loss: 1.4281 - 10s/epoch - 100us/sample
Epoch 34/50
95501/95501 - 10s - loss: 1.4182 - val_loss: 1.4149 - 10s/epoch - 100us/sample
Epoch 35/50
95501/95501 - 10s - loss: 1.4289 - val_loss: 1.5029 - 10s/epoch - 101us/sample
Epoch 36/50
95501/95501 - 10s - loss: 1.4243 - val_loss: 1.4166 - 10s/epoch - 101us/sample
Epoch 37/50
95501/95501 - 10s - loss: 1.4190 - val_loss: 1.4959 - 10s/epoch - 101us/sample
Epoch 38/50
95501/95501 - 10s - loss: 1.4149 - val_loss: 1.4142 - 10s/epoch - 100us/sample
Epoch 39/50
95501/95501 - 10s - loss: 1.4164 - val_loss: 1.4135 - 10s/epoch - 100us/sample
Epoch 40/50
95501/95501 - 10s - loss: 1.4274 - val_loss: 1.4238 - 10s/epoch - 101us/sample
Epoch 41/50
95501/95501 - 10s - loss: 1.4163 - val_loss: 1.4306 - 10s/epoch - 100us/sample
Epoch 42/50
95501/95501 - 10s - loss: 1.4137 - val_loss: 1.4126 - 10s/epoch - 101us/sample
Epoch 43/50
95501/95501 - 10s - loss: 1.8405 - val_loss: 1.4271 - 10s/epoch - 101us/sample
Epoch 44/50
95501/95501 - 10s - loss: 1.4184 - val_loss: 1.4159 - 10s/epoch - 102us/sample
Epoch 45/50
95501/95501 - 10s - loss: 1.4165 - val_loss: 1.4158 - 10s/epoch - 101us/sample
Epoch 46/50
95501/95501 - 10s - loss: 1.4131 - val_loss: 1.4136 - 10s/epoch - 100us/sample
Epoch 47/50
95501/95501 - 10s - loss: 1.4149 - val_loss: 1.4206 - 10s/epoch - 100us/sample
Epoch 48/50
95501/95501 - 10s - loss: 1.4138 - val_loss: 1.4120 - 10s/epoch - 101us/sample
Epoch 49/50
95501/95501 - 10s - loss: 1.4122 - val_loss: 1.4110 - 10s/epoch - 101us/sample
Epoch 50/50
95501/95501 - 10s - loss: 1.4319 - val_loss: 1.4172 - 10s/epoch - 100us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.4171909032746886
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:16:48.821642: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:22318 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.6690587744185588
cosine 0.897115715050018
MAE: 9.479867
RMSE: 18.787674
r2: -22888.412420699824
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'binary_crossentropy', 64, 50, 0.0012, 0.1, 126, 1.4318828196105213, 1.4171909032746886, 0.6690587744185588, 0.897115715050018, 9.479866981506348, 18.787673950195312, -22888.412420699824, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 65.82334742872027
Last generation's best solutions = [1.5 145 0.0012 64 1] with fitness 65.82334742872027.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object)]
Best solutions fitness :  [65.82334742872027]
[1.4 150 0.0012000000000000001 32 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 1769)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          223020      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          223020      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2485525     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,176,426
Trainable params: 5,169,098
Non-trainable params: 7,328
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-14 21:16:59.913466: W tensorflow/c/c_api.cc:291] Operation '{name:'training_36/Adam/batch_normalization_55/beta/m/Assign' id:24210 op device:{requested: '', assigned: ''} def:{{{node training_36/Adam/batch_normalization_55/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_36/Adam/batch_normalization_55/beta/m, training_36/Adam/batch_normalization_55/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:17:15.014549: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_18/mul' id:23690 op device:{requested: '', assigned: ''} def:{{{node loss_18/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_18/mul/x, loss_18/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 0.0116 - val_loss: 0.0055 - 19s/epoch - 194us/sample
Epoch 2/150
95501/95501 - 14s - loss: 0.0058 - val_loss: 0.0044 - 14s/epoch - 142us/sample
Epoch 3/150
95501/95501 - 14s - loss: 0.0087 - val_loss: 0.0035 - 14s/epoch - 141us/sample
Epoch 4/150
95501/95501 - 13s - loss: 0.0036 - val_loss: 0.0029 - 13s/epoch - 141us/sample
Epoch 5/150
95501/95501 - 13s - loss: 0.0031 - val_loss: 0.0026 - 13s/epoch - 141us/sample
Epoch 6/150
95501/95501 - 13s - loss: 0.0029 - val_loss: 0.0025 - 13s/epoch - 141us/sample
Epoch 7/150
95501/95501 - 14s - loss: 0.0027 - val_loss: 0.0025 - 14s/epoch - 141us/sample
Epoch 8/150
95501/95501 - 14s - loss: 0.0026 - val_loss: 0.0023 - 14s/epoch - 143us/sample
Epoch 9/150
95501/95501 - 14s - loss: 0.0025 - val_loss: 0.0022 - 14s/epoch - 142us/sample
Epoch 10/150
95501/95501 - 14s - loss: 0.0024 - val_loss: 0.0023 - 14s/epoch - 141us/sample
Epoch 11/150
95501/95501 - 14s - loss: 0.0024 - val_loss: 0.0021 - 14s/epoch - 141us/sample
Epoch 12/150
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0020 - 14s/epoch - 141us/sample
Epoch 13/150
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 142us/sample
Epoch 14/150
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 142us/sample
Epoch 15/150
95501/95501 - 13s - loss: 0.0022 - val_loss: 0.0020 - 13s/epoch - 141us/sample
Epoch 16/150
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 142us/sample
Epoch 17/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 142us/sample
Epoch 18/150
95501/95501 - 13s - loss: 0.0021 - val_loss: 0.0019 - 13s/epoch - 141us/sample
Epoch 19/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 143us/sample
Epoch 20/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 142us/sample
Epoch 21/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 141us/sample
Epoch 22/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 142us/sample
Epoch 23/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0019 - 13s/epoch - 141us/sample
Epoch 24/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 142us/sample
Epoch 25/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 26/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 27/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 28/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 29/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 141us/sample
Epoch 30/150
95501/95501 - 13s - loss: 0.0020 - val_loss: 0.0018 - 13s/epoch - 141us/sample
Epoch 31/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 32/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 33/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 34/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 141us/sample
Epoch 35/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 36/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 141us/sample
Epoch 37/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 38/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 39/150
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 40/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 141us/sample
Epoch 41/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 141us/sample
Epoch 42/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 43/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 44/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 143us/sample
Epoch 45/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 141us/sample
Epoch 46/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 47/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 48/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 49/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 50/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 51/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 52/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 53/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 54/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 55/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 56/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 57/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 58/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 59/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 60/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 61/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 62/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 63/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 64/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 65/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 66/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 67/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 68/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 69/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0019 - 14s/epoch - 141us/sample
Epoch 70/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 71/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 72/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 73/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 74/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 75/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 76/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 77/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 78/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 79/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 80/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 81/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 82/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 83/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 84/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 85/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 86/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 87/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 88/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 89/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 90/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 91/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 92/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 93/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 94/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 95/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 96/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 97/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 98/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 99/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 100/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 101/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 102/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 103/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 104/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 105/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 141us/sample
Epoch 106/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 107/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 108/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 144us/sample
Epoch 109/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 144us/sample
Epoch 110/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 111/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 145us/sample
Epoch 112/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 113/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 114/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 115/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 141us/sample
Epoch 116/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0023 - 14s/epoch - 141us/sample
Epoch 117/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 118/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 119/150
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 120/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 121/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 122/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 141us/sample
Epoch 123/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 124/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 141us/sample
Epoch 125/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 126/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 127/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 128/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 129/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 130/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 131/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 132/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 133/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 134/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 135/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 136/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 137/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 138/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 139/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 140/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 141/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0017 - 13s/epoch - 141us/sample
Epoch 142/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 143/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 144/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 145/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 146/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 142us/sample
Epoch 147/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 148/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 149/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
Epoch 150/150
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0016 - 13s/epoch - 141us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0016285839574931057
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:50:53.262290: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_18/outputlayer/BiasAdd' id:23661 op device:{requested: '', assigned: ''} def:{{{node decoder_model_18/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_18/outputlayer/MatMul, decoder_model_18/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.011330331176171157
cosine 0.008951364200923113
MAE: 0.015969161
RMSE: 0.031166758
r2: 0.9369855822280873
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 32, 150, 0.0012000000000000001, 0.1, 126, 0.0017734572985329035, 0.0016285839574931057, 0.011330331176171157, 0.008951364200923113, 0.015969160944223404, 0.03116675838828087, 0.9369855822280873, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 145 0.0012000000000000001 32 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 1896)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-14 21:51:04.437629: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_59/beta/Assign' id:24765 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_59/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_59/beta, batch_normalization_59/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:51:19.554134: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_19/mul' id:24958 op device:{requested: '', assigned: ''} def:{{{node loss_19/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_19/mul/x, loss_19/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 0.0057 - val_loss: 0.0036 - 19s/epoch - 195us/sample
Epoch 2/145
95501/95501 - 14s - loss: 0.0031 - val_loss: 0.0024 - 14s/epoch - 143us/sample
Epoch 3/145
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0019 - 14s/epoch - 143us/sample
Epoch 4/145
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0017 - 14s/epoch - 142us/sample
Epoch 5/145
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0015 - 14s/epoch - 142us/sample
Epoch 6/145
95501/95501 - 14s - loss: 0.0017 - val_loss: 0.0015 - 14s/epoch - 143us/sample
Epoch 7/145
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 143us/sample
Epoch 8/145
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 142us/sample
Epoch 9/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0014 - 14s/epoch - 142us/sample
Epoch 10/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0013 - 14s/epoch - 142us/sample
Epoch 11/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0013 - 14s/epoch - 142us/sample
Epoch 12/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 144us/sample
Epoch 13/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 143us/sample
Epoch 14/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 143us/sample
Epoch 15/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 142us/sample
Epoch 16/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 142us/sample
Epoch 17/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 18/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 144us/sample
Epoch 19/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 20/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 21/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 22/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 23/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 24/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 25/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 144us/sample
Epoch 26/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 27/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 28/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 142us/sample
Epoch 29/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 142us/sample
Epoch 30/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 31/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 32/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 142us/sample
Epoch 33/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 142us/sample
Epoch 34/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 142us/sample
Epoch 35/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 36/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 142us/sample
Epoch 37/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 144us/sample
Epoch 38/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 39/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 40/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 41/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 42/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 43/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 44/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 45/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 46/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 47/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 48/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 49/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 50/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 51/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 52/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 53/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 54/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 55/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 56/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 57/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 58/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 59/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 60/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 61/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 62/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 63/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 64/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 65/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 66/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 67/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 68/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 69/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 70/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 71/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 72/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 73/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 74/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 75/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 76/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 77/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 78/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 79/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 80/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 81/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 82/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 83/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 84/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 85/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 86/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 87/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 88/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 89/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 90/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 91/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 92/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 93/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 94/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 95/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 96/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 97/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 98/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 99/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 100/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 101/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 102/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 103/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 104/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 105/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 106/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 107/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 108/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 109/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 110/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 111/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 112/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 113/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 114/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 115/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 116/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 117/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 118/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 119/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 120/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 121/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 122/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 123/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 124/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 125/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 126/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 127/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 128/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 129/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 130/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 131/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 132/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 133/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 134/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 135/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 136/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 137/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 138/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 139/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 140/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 141/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 144us/sample
Epoch 142/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 143/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 144/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 145/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010675898260941404
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:24:05.371524: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:24922 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014601266540269603
cosine 0.011531702901703252
MAE: 0.017540941
RMSE: 0.035338476
r2: 0.9189871271591243
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 32, 145, 0.0012000000000000001, 0.1, 126, 0.0011619406215462077, 0.0010675898260941404, 0.014601266540269603, 0.011531702901703252, 0.017540941014885902, 0.03533847630023956, 0.9189871271591243, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0014 32 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 2022)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          254898      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          254898      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2838460     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,914,174
Trainable params: 5,905,834
Non-trainable params: 8,340
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-14 22:24:16.895651: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_62/gamma/Assign' id:26049 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_62/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_62/gamma, batch_normalization_62/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:24:32.277951: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:26249 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 0.0059 - val_loss: 0.0037 - 19s/epoch - 200us/sample
Epoch 2/145
95501/95501 - 14s - loss: 0.0032 - val_loss: 0.0023 - 14s/epoch - 145us/sample
Epoch 3/145
95501/95501 - 14s - loss: 0.0024 - val_loss: 0.0019 - 14s/epoch - 145us/sample
Epoch 4/145
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0017 - 14s/epoch - 145us/sample
Epoch 5/145
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0017 - 14s/epoch - 145us/sample
Epoch 6/145
95501/95501 - 14s - loss: 0.0017 - val_loss: 0.0015 - 14s/epoch - 146us/sample
Epoch 7/145
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 145us/sample
Epoch 8/145
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 145us/sample
Epoch 9/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0014 - 14s/epoch - 145us/sample
Epoch 10/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0013 - 14s/epoch - 145us/sample
Epoch 11/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0014 - 14s/epoch - 146us/sample
Epoch 12/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 146us/sample
Epoch 13/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 145us/sample
Epoch 14/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 145us/sample
Epoch 15/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 145us/sample
Epoch 16/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 146us/sample
Epoch 17/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 18/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 19/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 20/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 21/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 22/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 23/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 24/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 25/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 26/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 27/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 28/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 29/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 30/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 31/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 32/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 33/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 34/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 35/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 36/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 37/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 38/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 39/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 40/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 41/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 42/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 43/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 44/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 45/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 46/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 47/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 48/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 49/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 50/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 51/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 52/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 53/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 54/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 55/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 56/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 57/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 58/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 59/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 60/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 61/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 62/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 63/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 64/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 65/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 66/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 67/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 68/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 69/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 70/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 71/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 72/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 73/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 74/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 75/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 76/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 77/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 78/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 79/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 80/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 81/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 82/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 83/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 84/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 85/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 86/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 87/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 88/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 89/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 90/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 91/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 92/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 93/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 94/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 95/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 96/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 97/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 98/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 99/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 100/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 101/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 102/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 103/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 104/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 105/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 106/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 107/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 108/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 109/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 110/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 111/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 112/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 113/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 114/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 115/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 116/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 117/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 118/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 119/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 120/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 121/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 122/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 123/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 124/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 125/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 126/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 127/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 128/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 129/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 130/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 131/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 132/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 133/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 134/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 135/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 136/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 137/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 138/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 139/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 140/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 141/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 142/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 143/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 144/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 145/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010807155473951126
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:57:52.522058: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_20/outputlayer/BiasAdd' id:26213 op device:{requested: '', assigned: ''} def:{{{node decoder_model_20/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_20/outputlayer/MatMul, decoder_model_20/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014752236691637395
cosine 0.011660827630607757
MAE: 0.017607829
RMSE: 0.035565123
r2: 0.9179445827905099
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'logcosh', 32, 145, 0.0014, 0.1, 126, 0.0011609582067283691, 0.0010807155473951126, 0.014752236691637395, 0.011660827630607757, 0.017607828602194786, 0.035565122961997986, 0.9179445827905099, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 45 0.0005 32 0] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_63 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_63 (ReLU)                (None, 1769)         0           ['batch_normalization_63[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          223020      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          223020      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2485525     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,176,426
Trainable params: 5,169,098
Non-trainable params: 7,328
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/45
2023-02-14 22:58:04.295476: W tensorflow/c/c_api.cc:291] Operation '{name:'training_42/Adam/decay/Assign' id:28056 op device:{requested: '', assigned: ''} def:{{{node training_42/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_42/Adam/decay, training_42/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:58:20.227924: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_21/mul' id:27552 op device:{requested: '', assigned: ''} def:{{{node loss_21/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_21/mul/x, loss_21/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 20s - loss: 1.5552 - val_loss: 1.4219 - 20s/epoch - 207us/sample
Epoch 2/45
95501/95501 - 14s - loss: 1.4344 - val_loss: 1.4857 - 14s/epoch - 149us/sample
Epoch 3/45
95501/95501 - 14s - loss: 2.9567 - val_loss: 1.4293 - 14s/epoch - 150us/sample
Epoch 4/45
95501/95501 - 14s - loss: 1.4281 - val_loss: 1.4068 - 14s/epoch - 149us/sample
Epoch 5/45
95501/95501 - 14s - loss: 1.4191 - val_loss: 1.4045 - 14s/epoch - 149us/sample
Epoch 6/45
95501/95501 - 14s - loss: 1.4095 - val_loss: 1.4092 - 14s/epoch - 149us/sample
Epoch 7/45
95501/95501 - 14s - loss: 1.4089 - val_loss: 1.4048 - 14s/epoch - 149us/sample
Epoch 8/45
95501/95501 - 14s - loss: 1.4001 - val_loss: 1.3992 - 14s/epoch - 149us/sample
Epoch 9/45
95501/95501 - 14s - loss: 1.4158 - val_loss: 2.2038 - 14s/epoch - 149us/sample
Epoch 10/45
95501/95501 - 14s - loss: 1.4144 - val_loss: 1.4047 - 14s/epoch - 151us/sample
Epoch 11/45
95501/95501 - 14s - loss: 1.4083 - val_loss: 1.4407 - 14s/epoch - 149us/sample
Epoch 12/45
95501/95501 - 14s - loss: 1.4094 - val_loss: 1.3990 - 14s/epoch - 149us/sample
Epoch 13/45
95501/95501 - 14s - loss: 1.3958 - val_loss: 1.3913 - 14s/epoch - 149us/sample
Epoch 14/45
95501/95501 - 14s - loss: 1.3937 - val_loss: 1.3911 - 14s/epoch - 149us/sample
Epoch 15/45
95501/95501 - 14s - loss: 1.4151 - val_loss: 1.3927 - 14s/epoch - 150us/sample
Epoch 16/45
95501/95501 - 14s - loss: 1.3940 - val_loss: 1.3906 - 14s/epoch - 150us/sample
Epoch 17/45
95501/95501 - 14s - loss: 1.3940 - val_loss: 1.3997 - 14s/epoch - 149us/sample
Epoch 18/45
95501/95501 - 14s - loss: 1.3930 - val_loss: 1.3990 - 14s/epoch - 149us/sample
Epoch 19/45
95501/95501 - 14s - loss: 1.3967 - val_loss: 1.3887 - 14s/epoch - 149us/sample
Epoch 20/45
95501/95501 - 14s - loss: 1.3913 - val_loss: 1.3919 - 14s/epoch - 149us/sample
Epoch 21/45
95501/95501 - 14s - loss: 1.3902 - val_loss: 1.3884 - 14s/epoch - 150us/sample
Epoch 22/45
95501/95501 - 14s - loss: 1.3887 - val_loss: 1.3952 - 14s/epoch - 149us/sample
Epoch 23/45
95501/95501 - 14s - loss: 1.3900 - val_loss: 1.3877 - 14s/epoch - 149us/sample
Epoch 24/45
95501/95501 - 14s - loss: 1.3864 - val_loss: 1.3897 - 14s/epoch - 149us/sample
Epoch 25/45
95501/95501 - 14s - loss: 1.3930 - val_loss: 1.3902 - 14s/epoch - 149us/sample
Epoch 26/45
95501/95501 - 14s - loss: 1.3873 - val_loss: 1.3872 - 14s/epoch - 151us/sample
Epoch 27/45
95501/95501 - 14s - loss: 1.3855 - val_loss: 1.3924 - 14s/epoch - 150us/sample
Epoch 28/45
95501/95501 - 14s - loss: 1.3883 - val_loss: 1.3900 - 14s/epoch - 149us/sample
Epoch 29/45
95501/95501 - 14s - loss: 1.3860 - val_loss: 1.3852 - 14s/epoch - 149us/sample
Epoch 30/45
95501/95501 - 14s - loss: 1.3845 - val_loss: 1.3851 - 14s/epoch - 149us/sample
Epoch 31/45
95501/95501 - 14s - loss: 1.3847 - val_loss: 1.3875 - 14s/epoch - 149us/sample
Epoch 32/45
95501/95501 - 14s - loss: 1.3899 - val_loss: 1.3912 - 14s/epoch - 150us/sample
Epoch 33/45
95501/95501 - 14s - loss: 1.3906 - val_loss: 1.3893 - 14s/epoch - 149us/sample
Epoch 34/45
95501/95501 - 14s - loss: 1.3857 - val_loss: 1.3863 - 14s/epoch - 150us/sample
Epoch 35/45
95501/95501 - 14s - loss: 1.3862 - val_loss: 1.3880 - 14s/epoch - 149us/sample
Epoch 36/45
95501/95501 - 14s - loss: 1.3867 - val_loss: 1.3875 - 14s/epoch - 149us/sample
Epoch 37/45
95501/95501 - 14s - loss: 1.3861 - val_loss: 1.3879 - 14s/epoch - 150us/sample
Epoch 38/45
95501/95501 - 14s - loss: 1.3851 - val_loss: 1.3842 - 14s/epoch - 149us/sample
Epoch 39/45
95501/95501 - 14s - loss: 1.3839 - val_loss: 1.3846 - 14s/epoch - 149us/sample
Epoch 40/45
95501/95501 - 14s - loss: 1.3846 - val_loss: 1.3849 - 14s/epoch - 149us/sample
Epoch 41/45
95501/95501 - 14s - loss: 1.3862 - val_loss: 1.3890 - 14s/epoch - 149us/sample
Epoch 42/45
95501/95501 - 14s - loss: 1.3844 - val_loss: 1.3855 - 14s/epoch - 149us/sample
Epoch 43/45
95501/95501 - 14s - loss: 1.3845 - val_loss: 1.3849 - 14s/epoch - 150us/sample
Epoch 44/45
95501/95501 - 14s - loss: 1.3870 - val_loss: 1.3875 - 14s/epoch - 149us/sample
Epoch 45/45
95501/95501 - 14s - loss: 1.3863 - val_loss: 1.3879 - 14s/epoch - 149us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.3879010785178691
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:08:50.543483: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_21/outputlayer/BiasAdd' id:27504 op device:{requested: '', assigned: ''} def:{{{node decoder_model_21/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_21/outputlayer/MatMul, decoder_model_21/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7308333147107419
cosine 0.9117860975714888
MAE: 7.1828866
RMSE: 16.180399
r2: -16980.391815517298
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'binary_crossentropy', 32, 45, 0.0005, 0.1, 126, 1.386338369150893, 1.3879010785178691, 0.7308333147107419, 0.9117860975714888, 7.182886600494385, 16.18039894104004, -16980.391815517298, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.0014000000000000002 32 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_66 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_66 (ReLU)                (None, 1896)         0           ['batch_normalization_66[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-14 23:09:02.724182: W tensorflow/c/c_api.cc:291] Operation '{name:'training_44/Adam/batch_normalization_67/gamma/m/Assign' id:29421 op device:{requested: '', assigned: ''} def:{{{node training_44/Adam/batch_normalization_67/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_44/Adam/batch_normalization_67/gamma/m, training_44/Adam/batch_normalization_67/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:09:18.237582: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_22/mul' id:28883 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 2810.9397 - val_loss: 0.0034 - 19s/epoch - 204us/sample
Epoch 2/150
95501/95501 - 14s - loss: 0.0033 - val_loss: 0.0025 - 14s/epoch - 147us/sample
Epoch 3/150
95501/95501 - 14s - loss: 0.0025 - val_loss: 0.0020 - 14s/epoch - 145us/sample
Epoch 4/150
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0018 - 14s/epoch - 145us/sample
Epoch 5/150
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 145us/sample
Epoch 6/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 145us/sample
Epoch 7/150
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 146us/sample
Epoch 8/150
95501/95501 - 14s - loss: 0.0017 - val_loss: 0.0015 - 14s/epoch - 147us/sample
Epoch 9/150
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 147us/sample
Epoch 10/150
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0015 - 14s/epoch - 146us/sample
Epoch 11/150
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 146us/sample
Epoch 12/150
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0014 - 14s/epoch - 145us/sample
Epoch 13/150
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0014 - 14s/epoch - 146us/sample
Epoch 14/150
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0014 - 14s/epoch - 147us/sample
Epoch 15/150
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 145us/sample
Epoch 16/150
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 145us/sample
Epoch 17/150
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 145us/sample
Epoch 18/150
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 19/150
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 20/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 21/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 22/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 23/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 24/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 25/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 26/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 147us/sample
Epoch 27/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 28/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 29/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 30/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 31/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 32/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 148us/sample
Epoch 33/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 34/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 35/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 36/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 37/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 38/150
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 39/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 40/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 41/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 42/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 43/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 44/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 45/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 46/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 47/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 48/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 49/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 50/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 51/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 52/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 53/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 54/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 55/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 56/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 57/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 58/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 59/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 60/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 61/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 62/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 63/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 64/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 65/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 66/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 67/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 68/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 69/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 70/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 71/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 72/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 73/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 74/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 75/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 76/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 77/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 78/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 79/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 80/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 81/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 82/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 83/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 84/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 85/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 86/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 87/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 88/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 89/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 90/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 91/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 92/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 93/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 94/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 95/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 96/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 97/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 98/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 99/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 100/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 101/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 102/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 103/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 104/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 105/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 106/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 107/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 108/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 109/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 110/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 111/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 112/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 113/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 114/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 115/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 116/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 117/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 118/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 119/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 120/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 121/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 122/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 123/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 124/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 125/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 126/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 127/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 128/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 129/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 130/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 131/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 132/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 151us/sample
Epoch 133/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 134/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 135/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 136/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 137/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 138/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 151us/sample
Epoch 139/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 140/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 141/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 142/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 143/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 144/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 151us/sample
Epoch 145/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 146/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 149us/sample
Epoch 147/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 150us/sample
Epoch 148/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
Epoch 149/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 150us/sample
Epoch 150/150
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 149us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010651109303613506
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:44:02.191309: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_22/outputlayer/BiasAdd' id:28847 op device:{requested: '', assigned: ''} def:{{{node decoder_model_22/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_22/outputlayer/MatMul, decoder_model_22/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014442010702971823
cosine 0.011406753154642124
MAE: 0.01744615
RMSE: 0.03512774
r2: 0.9199503762193686
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 32, 150, 0.0014000000000000002, 0.1, 126, 0.0011512521514380402, 0.0010651109303613506, 0.014442010702971823, 0.011406753154642124, 0.01744614914059639, 0.03512774035334587, 0.9199503762193686, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 50 0.0005 32 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_69 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_69 (ReLU)                (None, 1769)         0           ['batch_normalization_69[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          223020      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          223020      ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2485525     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,176,426
Trainable params: 5,169,098
Non-trainable params: 7,328
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 23:44:14.913865: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_70/moving_mean/Assign' id:29902 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_70/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_70/moving_mean, batch_normalization_70/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:44:31.074809: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_23/mul' id:30167 op device:{requested: '', assigned: ''} def:{{{node loss_23/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_23/mul/x, loss_23/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 20s - loss: 0.0107 - val_loss: 0.0061 - 20s/epoch - 214us/sample
Epoch 2/50
95501/95501 - 14s - loss: 0.0057 - val_loss: 0.0049 - 14s/epoch - 150us/sample
Epoch 3/50
95501/95501 - 14s - loss: 0.0043 - val_loss: 0.0033 - 14s/epoch - 150us/sample
Epoch 4/50
95501/95501 - 14s - loss: 0.0034 - val_loss: 0.0029 - 14s/epoch - 149us/sample
Epoch 5/50
95501/95501 - 14s - loss: 0.0030 - val_loss: 0.0026 - 14s/epoch - 149us/sample
Epoch 6/50
95501/95501 - 14s - loss: 0.0028 - val_loss: 0.0024 - 14s/epoch - 149us/sample
Epoch 7/50
95501/95501 - 14s - loss: 0.0026 - val_loss: 0.0023 - 14s/epoch - 149us/sample
Epoch 8/50
95501/95501 - 14s - loss: 0.0025 - val_loss: 0.0023 - 14s/epoch - 149us/sample
Epoch 9/50
95501/95501 - 14s - loss: 0.0024 - val_loss: 0.0021 - 14s/epoch - 150us/sample
Epoch 10/50
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0021 - 14s/epoch - 150us/sample
Epoch 11/50
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0021 - 14s/epoch - 149us/sample
Epoch 12/50
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0020 - 14s/epoch - 149us/sample
Epoch 13/50
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 149us/sample
Epoch 14/50
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 149us/sample
Epoch 15/50
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0020 - 14s/epoch - 150us/sample
Epoch 16/50
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0019 - 14s/epoch - 149us/sample
Epoch 17/50
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 149us/sample
Epoch 18/50
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0021 - 14s/epoch - 149us/sample
Epoch 19/50
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 149us/sample
Epoch 20/50
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 150us/sample
Epoch 21/50
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 149us/sample
Epoch 22/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0019 - 14s/epoch - 149us/sample
Epoch 23/50
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0019 - 14s/epoch - 149us/sample
Epoch 24/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 25/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 150us/sample
Epoch 26/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 150us/sample
Epoch 27/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 28/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 29/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 30/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 31/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0019 - 14s/epoch - 150us/sample
Epoch 32/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 33/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 150us/sample
Epoch 34/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 35/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 36/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 37/50
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0018 - 14s/epoch - 150us/sample
Epoch 38/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 39/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 150us/sample
Epoch 40/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 149us/sample
Epoch 41/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 42/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 150us/sample
Epoch 43/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 150us/sample
Epoch 44/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 150us/sample
Epoch 45/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 149us/sample
Epoch 46/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0018 - 14s/epoch - 149us/sample
Epoch 47/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 149us/sample
Epoch 48/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 150us/sample
Epoch 49/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 150us/sample
Epoch 50/50
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0017 - 14s/epoch - 149us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.00173315636548075
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:56:12.680914: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_23/outputlayer/BiasAdd' id:30138 op device:{requested: '', assigned: ''} def:{{{node decoder_model_23/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_23/outputlayer/MatMul, decoder_model_23/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.012149865374311697
cosine 0.00960860155660433
MAE: 0.016937966
RMSE: 0.03235109
r2: 0.932105352979726
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 32, 50, 0.0005, 0.1, 126, 0.00190861771382591, 0.00173315636548075, 0.012149865374311697, 0.00960860155660433, 0.016937965527176857, 0.032351091504096985, 0.932105352979726, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 140 0.0012 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_72 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_72 (ReLU)                (None, 1896)         0           ['batch_normalization_72[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-14 23:56:25.687982: W tensorflow/c/c_api.cc:291] Operation '{name:'training_48/Adam/batch_normalization_73/gamma/v/Assign' id:32089 op device:{requested: '', assigned: ''} def:{{{node training_48/Adam/batch_normalization_73/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_48/Adam/batch_normalization_73/gamma/v, training_48/Adam/batch_normalization_73/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:56:37.598362: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_24/mul' id:31435 op device:{requested: '', assigned: ''} def:{{{node loss_24/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_24/mul/x, loss_24/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 16s - loss: 0.0113 - val_loss: 0.0037 - 16s/epoch - 168us/sample
Epoch 2/140
95501/95501 - 10s - loss: 0.0036 - val_loss: 0.0039 - 10s/epoch - 101us/sample
Epoch 3/140
95501/95501 - 10s - loss: 0.0031 - val_loss: 0.0026 - 10s/epoch - 102us/sample
Epoch 4/140
95501/95501 - 10s - loss: 0.0025 - val_loss: 0.0021 - 10s/epoch - 100us/sample
Epoch 5/140
95501/95501 - 10s - loss: 0.0022 - val_loss: 0.0019 - 10s/epoch - 100us/sample
Epoch 6/140
95501/95501 - 10s - loss: 0.0021 - val_loss: 0.0018 - 10s/epoch - 100us/sample
Epoch 7/140
95501/95501 - 10s - loss: 0.0019 - val_loss: 0.0017 - 10s/epoch - 100us/sample
Epoch 8/140
95501/95501 - 10s - loss: 0.0018 - val_loss: 0.0017 - 10s/epoch - 100us/sample
Epoch 9/140
95501/95501 - 10s - loss: 0.0017 - val_loss: 0.0016 - 10s/epoch - 100us/sample
Epoch 10/140
95501/95501 - 10s - loss: 0.0016 - val_loss: 0.0015 - 10s/epoch - 101us/sample
Epoch 11/140
95501/95501 - 10s - loss: 0.0016 - val_loss: 0.0015 - 10s/epoch - 100us/sample
Epoch 12/140
95501/95501 - 10s - loss: 0.0015 - val_loss: 0.0014 - 10s/epoch - 101us/sample
Epoch 13/140
95501/95501 - 10s - loss: 0.0015 - val_loss: 0.0014 - 10s/epoch - 102us/sample
Epoch 14/140
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0013 - 10s/epoch - 100us/sample
Epoch 15/140
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0014 - 10s/epoch - 101us/sample
Epoch 16/140
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0013 - 10s/epoch - 101us/sample
Epoch 17/140
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0014 - 10s/epoch - 100us/sample
Epoch 18/140
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0014 - 10s/epoch - 100us/sample
Epoch 19/140
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0013 - 10s/epoch - 100us/sample
Epoch 20/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 21/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 102us/sample
Epoch 22/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 23/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 24/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 101us/sample
Epoch 25/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 101us/sample
Epoch 26/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 101us/sample
Epoch 27/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 101us/sample
Epoch 28/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 101us/sample
Epoch 29/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 102us/sample
Epoch 30/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 101us/sample
Epoch 31/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 32/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 33/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 34/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 35/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 36/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 37/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 101us/sample
Epoch 38/140
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 39/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 40/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 41/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 42/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 43/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 44/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 45/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 102us/sample
Epoch 46/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 47/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 48/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 49/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 50/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 51/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 52/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 53/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 54/140
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 99us/sample
Epoch 55/140
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 97us/sample
Epoch 56/140
95501/95501 - 9s - loss: 0.0011 - val_loss: 0.0011 - 9s/epoch - 99us/sample
Epoch 57/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 58/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0012 - 10s/epoch - 100us/sample
Epoch 59/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 60/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 61/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 62/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 103us/sample
Epoch 63/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 64/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 65/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 66/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 67/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 68/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 69/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 70/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 71/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 72/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 73/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 74/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 75/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 76/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 77/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 78/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 102us/sample
Epoch 79/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 80/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 81/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 82/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 83/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 84/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 85/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 86/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 87/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 88/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 89/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 90/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 91/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 92/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 93/140
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 94/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 95/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 96/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 97/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 98/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 99/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 100/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 101/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 102/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 102us/sample
Epoch 103/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 101us/sample
Epoch 104/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 105/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 106/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 107/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 108/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 109/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 110/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 111/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 112/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 113/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 114/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 115/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 116/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 117/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 118/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 101us/sample
Epoch 119/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 102us/sample
Epoch 120/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 101us/sample
Epoch 121/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 122/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 123/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 101us/sample
Epoch 124/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 125/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 126/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 101us/sample
Epoch 127/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 101us/sample
Epoch 128/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 129/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 100us/sample
Epoch 130/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 101us/sample
Epoch 131/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 132/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 133/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 134/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 135/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 102us/sample
Epoch 136/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 137/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 138/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
Epoch 139/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 100us/sample
Epoch 140/140
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 101us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.001052299354380208
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:18:53.604735: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_24/outputlayer/BiasAdd' id:31399 op device:{requested: '', assigned: ''} def:{{{node decoder_model_24/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_24/outputlayer/MatMul, decoder_model_24/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014383095673601716
cosine 0.011353934862637942
MAE: 0.017930523
RMSE: 0.03507516
r2: 0.9201898287185909
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 64, 140, 0.0012, 0.1, 126, 0.001075683364535249, 0.001052299354380208, 0.014383095673601716, 0.011353934862637942, 0.017930522561073303, 0.03507516160607338, 0.9201898287185909, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 2
Fitness    = 65.82334742872027
Last generation's best solutions = [1.5 145 0.0012 64 1] with fitness 65.82334742872027.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object)]
Best solutions fitness :  [65.82334742872027, 65.82334742872027]
[1.5 150 0.0012000000000000001 8 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_75 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_75 (ReLU)                (None, 1896)         0           ['batch_normalization_75[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-15 00:19:06.933643: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_25/bias/Assign' id:32515 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_25/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_25/bias, dense_dec0_25/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:19:51.192462: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_25/mul' id:32719 op device:{requested: '', assigned: ''} def:{{{node loss_25/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul/x, loss_25/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 51s - loss: 0.0114 - val_loss: 0.0118 - 51s/epoch - 529us/sample
Epoch 2/150
95501/95501 - 44s - loss: 0.0061 - val_loss: 0.0138 - 44s/epoch - 456us/sample
Epoch 3/150
95501/95501 - 43s - loss: 0.0053 - val_loss: 0.0133 - 43s/epoch - 455us/sample
Epoch 4/150
95501/95501 - 43s - loss: 0.0049 - val_loss: 0.0152 - 43s/epoch - 455us/sample
Epoch 5/150
95501/95501 - 43s - loss: 0.0047 - val_loss: 0.0138 - 43s/epoch - 454us/sample
Epoch 6/150
95501/95501 - 44s - loss: 0.0045 - val_loss: 0.0125 - 44s/epoch - 456us/sample
Epoch 7/150
95501/95501 - 43s - loss: 0.0044 - val_loss: 0.0149 - 43s/epoch - 455us/sample
Epoch 8/150
95501/95501 - 43s - loss: 0.0044 - val_loss: 0.0141 - 43s/epoch - 455us/sample
Epoch 9/150
95501/95501 - 43s - loss: 0.0043 - val_loss: 0.0172 - 43s/epoch - 455us/sample
Epoch 10/150
95501/95501 - 43s - loss: 0.0043 - val_loss: 0.0161 - 43s/epoch - 454us/sample
Epoch 11/150
95501/95501 - 43s - loss: 0.0042 - val_loss: 0.0155 - 43s/epoch - 455us/sample
Epoch 12/150
95501/95501 - 43s - loss: 0.0041 - val_loss: 0.0175 - 43s/epoch - 454us/sample
Epoch 13/150
95501/95501 - 43s - loss: 0.0041 - val_loss: 0.0194 - 43s/epoch - 454us/sample
Epoch 14/150
95501/95501 - 44s - loss: 0.0041 - val_loss: 0.0203 - 44s/epoch - 456us/sample
Epoch 15/150
95501/95501 - 43s - loss: 0.0040 - val_loss: 0.0193 - 43s/epoch - 454us/sample
Epoch 16/150
95501/95501 - 43s - loss: 0.0040 - val_loss: 0.0192 - 43s/epoch - 455us/sample
Epoch 17/150
95501/95501 - 43s - loss: 0.0040 - val_loss: 0.0206 - 43s/epoch - 452us/sample
Epoch 18/150
95501/95501 - 44s - loss: 0.0040 - val_loss: 0.0181 - 44s/epoch - 456us/sample
Epoch 19/150
95501/95501 - 43s - loss: 0.0040 - val_loss: 0.0208 - 43s/epoch - 455us/sample
Epoch 20/150
95501/95501 - 43s - loss: 0.0040 - val_loss: 0.0217 - 43s/epoch - 453us/sample
Epoch 21/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0252 - 43s/epoch - 455us/sample
Epoch 22/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0216 - 43s/epoch - 455us/sample
Epoch 23/150
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0216 - 44s/epoch - 456us/sample
Epoch 24/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0215 - 43s/epoch - 454us/sample
Epoch 25/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0238 - 43s/epoch - 455us/sample
Epoch 26/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0217 - 43s/epoch - 455us/sample
Epoch 27/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0229 - 43s/epoch - 455us/sample
Epoch 28/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0226 - 43s/epoch - 454us/sample
Epoch 29/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0214 - 43s/epoch - 455us/sample
Epoch 30/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0250 - 43s/epoch - 455us/sample
Epoch 31/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0231 - 43s/epoch - 455us/sample
Epoch 32/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0252 - 43s/epoch - 453us/sample
Epoch 33/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0218 - 43s/epoch - 455us/sample
Epoch 34/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0244 - 43s/epoch - 454us/sample
Epoch 35/150
95501/95501 - 43s - loss: 0.0039 - val_loss: 0.0211 - 43s/epoch - 455us/sample
Epoch 36/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0252 - 43s/epoch - 454us/sample
Epoch 37/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0226 - 43s/epoch - 455us/sample
Epoch 38/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0226 - 43s/epoch - 454us/sample
Epoch 39/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0241 - 43s/epoch - 455us/sample
Epoch 40/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0223 - 43s/epoch - 454us/sample
Epoch 41/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0243 - 43s/epoch - 455us/sample
Epoch 42/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0242 - 43s/epoch - 454us/sample
Epoch 43/150
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0212 - 44s/epoch - 456us/sample
Epoch 44/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0242 - 43s/epoch - 454us/sample
Epoch 45/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0228 - 43s/epoch - 455us/sample
Epoch 46/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0225 - 43s/epoch - 455us/sample
Epoch 47/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0199 - 43s/epoch - 454us/sample
Epoch 48/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0191 - 43s/epoch - 453us/sample
Epoch 49/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0208 - 43s/epoch - 455us/sample
Epoch 50/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0215 - 43s/epoch - 454us/sample
Epoch 51/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0239 - 43s/epoch - 455us/sample
Epoch 52/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0227 - 43s/epoch - 454us/sample
Epoch 53/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0240 - 43s/epoch - 450us/sample
Epoch 54/150
95501/95501 - 43s - loss: 0.0038 - val_loss: 0.0214 - 43s/epoch - 448us/sample
Epoch 55/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0218 - 43s/epoch - 448us/sample
Epoch 56/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0223 - 43s/epoch - 447us/sample
Epoch 57/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0265 - 43s/epoch - 448us/sample
Epoch 58/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0211 - 43s/epoch - 448us/sample
Epoch 59/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0246 - 43s/epoch - 448us/sample
Epoch 60/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0265 - 43s/epoch - 447us/sample
Epoch 61/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0259 - 43s/epoch - 448us/sample
Epoch 62/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0227 - 43s/epoch - 447us/sample
Epoch 63/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0245 - 43s/epoch - 449us/sample
Epoch 64/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0230 - 43s/epoch - 447us/sample
Epoch 65/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0265 - 43s/epoch - 448us/sample
Epoch 66/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0265 - 43s/epoch - 448us/sample
Epoch 67/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0257 - 43s/epoch - 448us/sample
Epoch 68/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0277 - 43s/epoch - 448us/sample
Epoch 69/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0259 - 43s/epoch - 448us/sample
Epoch 70/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0232 - 43s/epoch - 447us/sample
Epoch 71/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0232 - 43s/epoch - 448us/sample
Epoch 72/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0262 - 43s/epoch - 447us/sample
Epoch 73/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0268 - 43s/epoch - 448us/sample
Epoch 74/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0230 - 43s/epoch - 447us/sample
Epoch 75/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0241 - 43s/epoch - 448us/sample
Epoch 76/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0269 - 43s/epoch - 447us/sample
Epoch 77/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0262 - 43s/epoch - 448us/sample
Epoch 78/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0256 - 43s/epoch - 448us/sample
Epoch 79/150
95501/95501 - 43s - loss: 0.0037 - val_loss: 0.0245 - 43s/epoch - 448us/sample
Epoch 80/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0290 - 43s/epoch - 448us/sample
Epoch 81/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0262 - 43s/epoch - 448us/sample
Epoch 82/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0254 - 43s/epoch - 448us/sample
Epoch 83/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0294 - 43s/epoch - 447us/sample
Epoch 84/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0249 - 43s/epoch - 449us/sample
Epoch 85/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0247 - 43s/epoch - 447us/sample
Epoch 86/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0291 - 43s/epoch - 448us/sample
Epoch 87/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0236 - 43s/epoch - 447us/sample
Epoch 88/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0773 - 43s/epoch - 449us/sample
Epoch 89/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0271 - 43s/epoch - 447us/sample
Epoch 90/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0311 - 43s/epoch - 448us/sample
Epoch 91/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0307 - 43s/epoch - 447us/sample
Epoch 92/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0269 - 43s/epoch - 448us/sample
Epoch 93/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0285 - 43s/epoch - 447us/sample
Epoch 94/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0268 - 43s/epoch - 448us/sample
Epoch 95/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0273 - 43s/epoch - 447us/sample
Epoch 96/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0278 - 43s/epoch - 448us/sample
Epoch 97/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0276 - 43s/epoch - 448us/sample
Epoch 98/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0278 - 43s/epoch - 448us/sample
Epoch 99/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0290 - 43s/epoch - 447us/sample
Epoch 100/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0279 - 43s/epoch - 448us/sample
Epoch 101/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0283 - 43s/epoch - 447us/sample
Epoch 102/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0310 - 43s/epoch - 448us/sample
Epoch 103/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0285 - 43s/epoch - 447us/sample
Epoch 104/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0277 - 43s/epoch - 448us/sample
Epoch 105/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0308 - 43s/epoch - 448us/sample
Epoch 106/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0238 - 43s/epoch - 448us/sample
Epoch 107/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0238 - 43s/epoch - 447us/sample
Epoch 108/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0285 - 43s/epoch - 449us/sample
Epoch 109/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0290 - 43s/epoch - 447us/sample
Epoch 110/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0318 - 43s/epoch - 449us/sample
Epoch 111/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0271 - 43s/epoch - 447us/sample
Epoch 112/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0308 - 43s/epoch - 448us/sample
Epoch 113/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0279 - 43s/epoch - 447us/sample
Epoch 114/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0281 - 43s/epoch - 448us/sample
Epoch 115/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0295 - 43s/epoch - 448us/sample
Epoch 116/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0308 - 43s/epoch - 447us/sample
Epoch 117/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0346 - 43s/epoch - 449us/sample
Epoch 118/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0294 - 43s/epoch - 448us/sample
Epoch 119/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0273 - 43s/epoch - 448us/sample
Epoch 120/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0271 - 43s/epoch - 449us/sample
Epoch 121/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0334 - 43s/epoch - 447us/sample
Epoch 122/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0285 - 43s/epoch - 449us/sample
Epoch 123/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0274 - 43s/epoch - 448us/sample
Epoch 124/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0294 - 43s/epoch - 449us/sample
Epoch 125/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0248 - 43s/epoch - 447us/sample
Epoch 126/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0289 - 43s/epoch - 448us/sample
Epoch 127/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0283 - 43s/epoch - 449us/sample
Epoch 128/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0317 - 43s/epoch - 447us/sample
Epoch 129/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0307 - 43s/epoch - 449us/sample
Epoch 130/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0329 - 43s/epoch - 448us/sample
Epoch 131/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0294 - 43s/epoch - 449us/sample
Epoch 132/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0317 - 43s/epoch - 447us/sample
Epoch 133/150
95501/95501 - 43s - loss: 0.0036 - val_loss: 0.0270 - 43s/epoch - 448us/sample
Epoch 134/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0302 - 43s/epoch - 447us/sample
Epoch 135/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0353 - 43s/epoch - 448us/sample
Epoch 136/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0333 - 43s/epoch - 448us/sample
Epoch 137/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0319 - 43s/epoch - 449us/sample
Epoch 138/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0332 - 43s/epoch - 448us/sample
Epoch 139/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0333 - 43s/epoch - 449us/sample
Epoch 140/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0329 - 43s/epoch - 447us/sample
Epoch 141/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0311 - 43s/epoch - 448us/sample
Epoch 142/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0318 - 43s/epoch - 448us/sample
Epoch 143/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0311 - 43s/epoch - 448us/sample
Epoch 144/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0316 - 43s/epoch - 447us/sample
Epoch 145/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0347 - 43s/epoch - 448us/sample
Epoch 146/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0289 - 43s/epoch - 447us/sample
Epoch 147/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0273 - 43s/epoch - 448us/sample
Epoch 148/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0277 - 43s/epoch - 447us/sample
Epoch 149/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0325 - 43s/epoch - 448us/sample
Epoch 150/150
95501/95501 - 43s - loss: 0.0035 - val_loss: 0.0341 - 43s/epoch - 448us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.034148725288363
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:06:41.741312: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_25/outputlayer/BiasAdd' id:32690 op device:{requested: '', assigned: ''} def:{{{node decoder_model_25/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_25/outputlayer/MatMul, decoder_model_25/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.06716822240258719
cosine 0.062460213615968006
MAE: 0.04659225
RMSE: 0.18198796
r2: -1.148562895853323
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 8, 150, 0.0012000000000000001, 0.1, 126, 0.0035133247295111403, 0.034148725288363, 0.06716822240258719, 0.062460213615968006, 0.04659225046634674, 0.1819879561662674, -1.148562895853323, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 145 0.0014000000000000002 32 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_78 (BatchN  (None, 3160)        12640       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_78 (ReLU)                (None, 3160)         0           ['batch_normalization_78[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          398286      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          398286      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4425970     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,232,582
Trainable params: 9,219,690
Non-trainable params: 12,892
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 02:06:55.218999: W tensorflow/c/c_api.cc:291] Operation '{name:'training_52/Adam/outputlayer_26/bias/v/Assign' id:34688 op device:{requested: '', assigned: ''} def:{{{node training_52/Adam/outputlayer_26/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_52/Adam/outputlayer_26/bias/v, training_52/Adam/outputlayer_26/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:07:12.369196: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_26/mul' id:33987 op device:{requested: '', assigned: ''} def:{{{node loss_26/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_26/mul/x, loss_26/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 22s - loss: 0.0117 - val_loss: 0.0035 - 22s/epoch - 228us/sample
Epoch 2/145
95501/95501 - 15s - loss: 0.0042 - val_loss: 0.0027 - 15s/epoch - 158us/sample
Epoch 3/145
95501/95501 - 15s - loss: 0.0033 - val_loss: 0.0027 - 15s/epoch - 157us/sample
Epoch 4/145
95501/95501 - 15s - loss: 0.0024 - val_loss: 0.0020 - 15s/epoch - 158us/sample
Epoch 5/145
95501/95501 - 15s - loss: 0.0022 - val_loss: 0.0019 - 15s/epoch - 157us/sample
Epoch 6/145
95501/95501 - 15s - loss: 0.0020 - val_loss: 0.0020 - 15s/epoch - 159us/sample
Epoch 7/145
95501/95501 - 15s - loss: 0.0018 - val_loss: 0.0016 - 15s/epoch - 158us/sample
Epoch 8/145
95501/95501 - 15s - loss: 0.0017 - val_loss: 0.0015 - 15s/epoch - 157us/sample
Epoch 9/145
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 157us/sample
Epoch 10/145
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 158us/sample
Epoch 11/145
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 12/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 159us/sample
Epoch 13/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 157us/sample
Epoch 14/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 157us/sample
Epoch 15/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 157us/sample
Epoch 16/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 17/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 18/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 19/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 20/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 21/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 22/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 23/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 24/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 25/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 26/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 27/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 28/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 157us/sample
Epoch 29/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 30/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 31/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 32/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 33/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 34/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 35/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 36/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 37/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 38/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 39/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 40/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 41/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 42/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 43/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 44/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0014 - 15s/epoch - 158us/sample
Epoch 45/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 46/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 47/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 48/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 49/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 50/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 51/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 52/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 53/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 54/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 55/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 56/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 57/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 58/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 59/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 60/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 61/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 62/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 63/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 64/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 65/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 66/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 67/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 68/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 69/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 70/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 71/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 72/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 73/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 74/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 75/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 76/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 77/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 78/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 79/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 80/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 81/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 82/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 83/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 84/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 85/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 86/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 87/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 88/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 89/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 90/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 91/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 92/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 93/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 94/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 95/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 96/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 97/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 98/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 99/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 100/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 101/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 102/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 103/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 104/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 105/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 106/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 107/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 108/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 109/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 110/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 111/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 112/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 113/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 114/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 115/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 116/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 117/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 118/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 119/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 120/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 121/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 159us/sample
Epoch 122/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 123/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 157us/sample
Epoch 124/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 157us/sample
Epoch 125/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 126/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 159us/sample
Epoch 127/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 128/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 129/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 130/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 131/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 132/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 159us/sample
Epoch 133/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 134/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 135/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 136/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 157us/sample
Epoch 137/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 138/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 159us/sample
Epoch 139/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 140/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 141/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 142/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 157us/sample
Epoch 143/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 159us/sample
Epoch 144/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
Epoch 145/145
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 158us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010357970645920823
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:43:26.236508: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_26/outputlayer/BiasAdd' id:33951 op device:{requested: '', assigned: ''} def:{{{node decoder_model_26/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_26/outputlayer/MatMul, decoder_model_26/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014138118325652961
cosine 0.011168694843268151
MAE: 0.017207516
RMSE: 0.034879725
r2: 0.9210768695664712
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 32, 145, 0.0014000000000000002, 0.1, 126, 0.0011076056646114713, 0.0010357970645920823, 0.014138118325652961, 0.011168694843268151, 0.017207516357302666, 0.03487972542643547, 0.9210768695664712, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 140 0.0016000000000000003 32 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_81 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_81 (ReLU)                (None, 1896)         0           ['batch_normalization_81[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-15 02:43:40.068771: W tensorflow/c/c_api.cc:291] Operation '{name:'training_54/Adam/outputlayer_27/kernel/v/Assign' id:35972 op device:{requested: '', assigned: ''} def:{{{node training_54/Adam/outputlayer_27/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_54/Adam/outputlayer_27/kernel/v, training_54/Adam/outputlayer_27/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:43:56.944423: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_27/mul' id:35278 op device:{requested: '', assigned: ''} def:{{{node loss_27/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_27/mul/x, loss_27/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 22s - loss: 0.0248 - val_loss: 0.0032 - 22s/epoch - 227us/sample
Epoch 2/140
95501/95501 - 15s - loss: 0.0033 - val_loss: 0.0025 - 15s/epoch - 155us/sample
Epoch 3/140
95501/95501 - 15s - loss: 0.0025 - val_loss: 0.0020 - 15s/epoch - 154us/sample
Epoch 4/140
95501/95501 - 15s - loss: 0.0021 - val_loss: 0.0018 - 15s/epoch - 153us/sample
Epoch 5/140
95501/95501 - 15s - loss: 0.0019 - val_loss: 0.0017 - 15s/epoch - 154us/sample
Epoch 6/140
95501/95501 - 15s - loss: 0.0018 - val_loss: 0.0016 - 15s/epoch - 154us/sample
Epoch 7/140
95501/95501 - 15s - loss: 0.0017 - val_loss: 0.0015 - 15s/epoch - 153us/sample
Epoch 8/140
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 154us/sample
Epoch 9/140
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 154us/sample
Epoch 10/140
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 154us/sample
Epoch 11/140
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 154us/sample
Epoch 12/140
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 154us/sample
Epoch 13/140
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 154us/sample
Epoch 14/140
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 155us/sample
Epoch 15/140
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 154us/sample
Epoch 16/140
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 153us/sample
Epoch 17/140
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 18/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 19/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 20/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 21/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 22/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 23/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 24/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 25/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 26/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 27/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 28/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 29/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 30/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 31/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 32/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 33/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 34/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 35/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 36/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 37/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 38/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 39/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 40/140
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 41/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 42/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0013 - 15s/epoch - 155us/sample
Epoch 43/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 44/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 45/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 46/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 47/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 48/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 49/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 50/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 51/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 52/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 53/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 54/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 55/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 56/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 57/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 58/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 59/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 60/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 61/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 62/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 63/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 64/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 65/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 66/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 67/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 68/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 69/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 70/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 71/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 72/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 73/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 74/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 75/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 76/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 77/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 78/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 79/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 80/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 81/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 82/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 83/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 84/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 85/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 86/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 87/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 88/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 89/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 90/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 91/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 92/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 93/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 94/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 95/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 96/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 97/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 98/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 99/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 100/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 101/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 102/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 103/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 104/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 105/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 106/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 107/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 108/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 109/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 110/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 111/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 112/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 113/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 114/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 115/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 116/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 117/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 118/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 119/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 120/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 121/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 122/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 123/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 124/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 125/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 126/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 127/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 128/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 129/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 130/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 131/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 132/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 133/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 134/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 135/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 136/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 137/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 138/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 139/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 140/140
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010711691823050442
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:18:18.609831: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_27/outputlayer/BiasAdd' id:35242 op device:{requested: '', assigned: ''} def:{{{node decoder_model_27/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_27/outputlayer/MatMul, decoder_model_27/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014425728929254004
cosine 0.011398254168109988
MAE: 0.01736449
RMSE: 0.035115775
r2: 0.9200051187272564
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 32, 140, 0.0016000000000000003, 0.1, 126, 0.0011531558113845614, 0.0010711691823050442, 0.014425728929254004, 0.011398254168109988, 0.017364490777254105, 0.03511577472090721, 0.9200051187272564, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 145 0.0012 8 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_84 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_84 (ReLU)                (None, 1896)         0           ['batch_normalization_84[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 03:18:33.011414: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_85/moving_variance/Assign' id:36302 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_85/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_85/moving_variance, batch_normalization_85/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:19:18.294629: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_28/mul' id:36562 op device:{requested: '', assigned: ''} def:{{{node loss_28/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_28/mul/x, loss_28/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 52s - loss: 0.0112 - val_loss: 0.0082 - 52s/epoch - 546us/sample
Epoch 2/145
95501/95501 - 44s - loss: 0.0059 - val_loss: 0.0068 - 44s/epoch - 463us/sample
Epoch 3/145
95501/95501 - 44s - loss: 0.0051 - val_loss: 0.0057 - 44s/epoch - 464us/sample
Epoch 4/145
95501/95501 - 44s - loss: 0.0047 - val_loss: 0.0053 - 44s/epoch - 463us/sample
Epoch 5/145
95501/95501 - 44s - loss: 0.0045 - val_loss: 0.0048 - 44s/epoch - 465us/sample
Epoch 6/145
95501/95501 - 44s - loss: 0.0044 - val_loss: 0.0049 - 44s/epoch - 464us/sample
Epoch 7/145
95501/95501 - 44s - loss: 0.0043 - val_loss: 0.0046 - 44s/epoch - 463us/sample
Epoch 8/145
95501/95501 - 44s - loss: 0.0042 - val_loss: 0.0046 - 44s/epoch - 463us/sample
Epoch 9/145
95501/95501 - 44s - loss: 0.0042 - val_loss: 0.0046 - 44s/epoch - 465us/sample
Epoch 10/145
95501/95501 - 44s - loss: 0.0041 - val_loss: 0.0040 - 44s/epoch - 463us/sample
Epoch 11/145
95501/95501 - 44s - loss: 0.0041 - val_loss: 0.0039 - 44s/epoch - 465us/sample
Epoch 12/145
95501/95501 - 44s - loss: 0.0040 - val_loss: 0.0040 - 44s/epoch - 464us/sample
Epoch 13/145
95501/95501 - 44s - loss: 0.0040 - val_loss: 0.0038 - 44s/epoch - 465us/sample
Epoch 14/145
95501/95501 - 44s - loss: 0.0040 - val_loss: 0.0038 - 44s/epoch - 463us/sample
Epoch 15/145
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0037 - 44s/epoch - 464us/sample
Epoch 16/145
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0037 - 44s/epoch - 463us/sample
Epoch 17/145
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0037 - 44s/epoch - 465us/sample
Epoch 18/145
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0036 - 44s/epoch - 463us/sample
Epoch 19/145
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0037 - 44s/epoch - 464us/sample
Epoch 20/145
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0038 - 44s/epoch - 458us/sample
Epoch 21/145
95501/95501 - 44s - loss: 0.0039 - val_loss: 0.0037 - 44s/epoch - 459us/sample
Epoch 22/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0039 - 44s/epoch - 458us/sample
Epoch 23/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0038 - 44s/epoch - 459us/sample
Epoch 24/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0038 - 44s/epoch - 458us/sample
Epoch 25/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0036 - 44s/epoch - 460us/sample
Epoch 26/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0037 - 44s/epoch - 458us/sample
Epoch 27/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0039 - 44s/epoch - 460us/sample
Epoch 28/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 29/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0036 - 44s/epoch - 461us/sample
Epoch 30/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0037 - 44s/epoch - 458us/sample
Epoch 31/145
95501/95501 - 44s - loss: 0.0038 - val_loss: 0.0037 - 44s/epoch - 459us/sample
Epoch 32/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 33/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0037 - 44s/epoch - 461us/sample
Epoch 34/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 35/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0034 - 44s/epoch - 460us/sample
Epoch 36/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 37/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 38/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 39/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 40/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0034 - 44s/epoch - 459us/sample
Epoch 41/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 42/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0033 - 44s/epoch - 458us/sample
Epoch 43/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 44/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 45/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 46/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 47/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0036 - 44s/epoch - 460us/sample
Epoch 48/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0035 - 44s/epoch - 457us/sample
Epoch 49/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0034 - 44s/epoch - 459us/sample
Epoch 50/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0042 - 44s/epoch - 458us/sample
Epoch 51/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 460us/sample
Epoch 52/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 53/145
95501/95501 - 44s - loss: 0.0037 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 54/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0038 - 44s/epoch - 458us/sample
Epoch 55/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 56/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0037 - 44s/epoch - 458us/sample
Epoch 57/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0037 - 44s/epoch - 459us/sample
Epoch 58/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 59/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 460us/sample
Epoch 60/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 61/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 62/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0038 - 44s/epoch - 458us/sample
Epoch 63/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 460us/sample
Epoch 64/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0037 - 44s/epoch - 457us/sample
Epoch 65/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 66/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 67/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0039 - 44s/epoch - 459us/sample
Epoch 68/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0037 - 44s/epoch - 458us/sample
Epoch 69/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0037 - 44s/epoch - 459us/sample
Epoch 70/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 71/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 72/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0038 - 44s/epoch - 460us/sample
Epoch 73/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0040 - 44s/epoch - 458us/sample
Epoch 74/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 75/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 76/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 77/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0037 - 44s/epoch - 460us/sample
Epoch 78/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0033 - 44s/epoch - 458us/sample
Epoch 79/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 460us/sample
Epoch 80/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0034 - 44s/epoch - 458us/sample
Epoch 81/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 460us/sample
Epoch 82/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0034 - 44s/epoch - 459us/sample
Epoch 83/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 84/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0037 - 44s/epoch - 460us/sample
Epoch 85/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 458us/sample
Epoch 86/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 87/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0034 - 44s/epoch - 458us/sample
Epoch 88/145
95501/95501 - 44s - loss: 0.0036 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 89/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 457us/sample
Epoch 90/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 91/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 92/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 93/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 94/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 95/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 459us/sample
Epoch 96/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0034 - 44s/epoch - 459us/sample
Epoch 97/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 98/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0034 - 44s/epoch - 458us/sample
Epoch 99/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 459us/sample
Epoch 100/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 457us/sample
Epoch 101/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 458us/sample
Epoch 102/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0034 - 44s/epoch - 457us/sample
Epoch 103/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0034 - 44s/epoch - 460us/sample
Epoch 104/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 457us/sample
Epoch 105/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 106/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 107/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 459us/sample
Epoch 108/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 458us/sample
Epoch 109/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 459us/sample
Epoch 110/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 458us/sample
Epoch 111/145
95501/95501 - 45s - loss: 0.0035 - val_loss: 0.0035 - 45s/epoch - 466us/sample
Epoch 112/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 463us/sample
Epoch 113/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 464us/sample
Epoch 114/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0039 - 44s/epoch - 464us/sample
Epoch 115/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0040 - 44s/epoch - 466us/sample
Epoch 116/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0038 - 44s/epoch - 466us/sample
Epoch 117/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0045 - 44s/epoch - 462us/sample
Epoch 118/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 464us/sample
Epoch 119/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 464us/sample
Epoch 120/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0040 - 44s/epoch - 464us/sample
Epoch 121/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0039 - 44s/epoch - 463us/sample
Epoch 122/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0033 - 44s/epoch - 464us/sample
Epoch 123/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 464us/sample
Epoch 124/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 464us/sample
Epoch 125/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 462us/sample
Epoch 126/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0034 - 44s/epoch - 465us/sample
Epoch 127/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 465us/sample
Epoch 128/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 465us/sample
Epoch 129/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0043 - 44s/epoch - 463us/sample
Epoch 130/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0041 - 44s/epoch - 464us/sample
Epoch 131/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0040 - 44s/epoch - 463us/sample
Epoch 132/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 464us/sample
Epoch 133/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 462us/sample
Epoch 134/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 465us/sample
Epoch 135/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0036 - 44s/epoch - 464us/sample
Epoch 136/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0039 - 44s/epoch - 464us/sample
Epoch 137/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0037 - 44s/epoch - 463us/sample
Epoch 138/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 465us/sample
Epoch 139/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 464us/sample
Epoch 140/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0040 - 44s/epoch - 465us/sample
Epoch 141/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0056 - 44s/epoch - 464us/sample
Epoch 142/145
95501/95501 - 44s - loss: 0.0034 - val_loss: 0.0039 - 44s/epoch - 464us/sample
Epoch 143/145
95501/95501 - 44s - loss: 0.0034 - val_loss: 0.0034 - 44s/epoch - 461us/sample
Epoch 144/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0039 - 44s/epoch - 464us/sample
Epoch 145/145
95501/95501 - 44s - loss: 0.0035 - val_loss: 0.0035 - 44s/epoch - 463us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0035044376690451812
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:04:56.674680: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_28/outputlayer/BiasAdd' id:36533 op device:{requested: '', assigned: ''} def:{{{node decoder_model_28/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_28/outputlayer/MatMul, decoder_model_28/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02957748999707527
cosine 0.0237430409130581
MAE: 0.02580624
RMSE: 0.053758346
r2: 0.8125217911298247
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 8, 145, 0.0012, 0.1, 126, 0.0034850317130069282, 0.0035044376690451812, 0.02957748999707527, 0.0237430409130581, 0.025806240737438202, 0.053758345544338226, 0.8125217911298247, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 145 0.0012 16 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_87 (BatchN  (None, 3286)        13144       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_87 (ReLU)                (None, 3286)         0           ['batch_normalization_87[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          414162      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          414162      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4601740     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,599,998
Trainable params: 9,586,602
Non-trainable params: 13,396
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 05:05:11.442274: W tensorflow/c/c_api.cc:291] Operation '{name:'training_58/Adam/batch_normalization_89/gamma/v/Assign' id:38513 op device:{requested: '', assigned: ''} def:{{{node training_58/Adam/batch_normalization_89/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_58/Adam/batch_normalization_89/gamma/v, training_58/Adam/batch_normalization_89/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:05:40.427558: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_29/mul' id:37830 op device:{requested: '', assigned: ''} def:{{{node loss_29/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_29/mul/x, loss_29/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 35s - loss: 0.0079 - val_loss: 0.0034 - 35s/epoch - 365us/sample
Epoch 2/145
95501/95501 - 27s - loss: 0.0032 - val_loss: 0.0026 - 27s/epoch - 284us/sample
Epoch 3/145
95501/95501 - 27s - loss: 0.0025 - val_loss: 0.0019 - 27s/epoch - 284us/sample
Epoch 4/145
95501/95501 - 27s - loss: 0.0021 - val_loss: 0.0018 - 27s/epoch - 284us/sample
Epoch 5/145
95501/95501 - 27s - loss: 0.0020 - val_loss: 0.0017 - 27s/epoch - 283us/sample
Epoch 6/145
95501/95501 - 27s - loss: 0.0019 - val_loss: 0.0016 - 27s/epoch - 283us/sample
Epoch 7/145
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0016 - 27s/epoch - 284us/sample
Epoch 8/145
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0015 - 27s/epoch - 284us/sample
Epoch 9/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 285us/sample
Epoch 10/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 285us/sample
Epoch 11/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 283us/sample
Epoch 12/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 285us/sample
Epoch 13/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0015 - 27s/epoch - 284us/sample
Epoch 14/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 15/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 283us/sample
Epoch 16/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 17/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 283us/sample
Epoch 18/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 19/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 20/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0015 - 27s/epoch - 284us/sample
Epoch 21/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 22/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 23/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 24/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 25/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 26/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 27/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 28/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 29/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 30/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 31/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 32/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 33/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 34/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 35/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 36/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 37/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 38/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 39/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 40/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 41/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 42/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 43/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 44/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 45/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 46/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 47/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0014 - 27s/epoch - 283us/sample
Epoch 48/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 49/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 50/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 51/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 52/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 53/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 54/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 55/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 56/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 282us/sample
Epoch 57/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 58/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 59/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 60/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 61/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 62/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 63/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 64/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 65/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 66/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 67/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 68/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 69/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 70/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 71/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 72/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 73/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 74/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 75/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 76/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 77/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 78/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 79/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 80/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 81/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 82/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 83/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 84/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 85/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 86/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 87/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 88/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 89/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 90/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 91/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 92/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 93/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 94/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 95/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 96/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 97/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 98/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 99/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 100/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 101/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 102/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 103/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 104/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 105/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 106/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 107/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 108/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 109/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 110/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 111/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 112/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 113/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 114/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 115/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 116/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 282us/sample
Epoch 117/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 118/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 119/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 120/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 121/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 122/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 123/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 124/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 125/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 126/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 127/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 128/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 129/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 283us/sample
Epoch 130/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 131/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 132/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 133/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 134/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 135/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 136/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 137/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 138/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 139/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 140/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 141/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 142/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 143/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 144/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 145/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0012191349105232653
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:10:49.479092: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_29/outputlayer/BiasAdd' id:37794 op device:{requested: '', assigned: ''} def:{{{node decoder_model_29/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_29/outputlayer/MatMul, decoder_model_29/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01854805821321885
cosine 0.014696880506651383
MAE: 0.01985766
RMSE: 0.040035613
r2: 0.8960195253374063
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'logcosh', 16, 145, 0.0012, 0.1, 126, 0.001364537158704322, 0.0012191349105232653, 0.01854805821321885, 0.014696880506651383, 0.019857659935951233, 0.040035612881183624, 0.8960195253374063, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.0014000000000000002 16 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_90 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_90 (ReLU)                (None, 1896)         0           ['batch_normalization_90[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-15 06:11:04.694472: W tensorflow/c/c_api.cc:291] Operation '{name:'training_60/Adam/batch_normalization_90/gamma/v/Assign' id:39730 op device:{requested: '', assigned: ''} def:{{{node training_60/Adam/batch_normalization_90/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_60/Adam/batch_normalization_90/gamma/v, training_60/Adam/batch_normalization_90/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:11:32.426644: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_30/mul' id:39124 op device:{requested: '', assigned: ''} def:{{{node loss_30/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_30/mul/x, loss_30/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 34s - loss: 0.0056 - val_loss: 0.0030 - 34s/epoch - 353us/sample
Epoch 2/150
95501/95501 - 26s - loss: 0.0028 - val_loss: 0.0021 - 26s/epoch - 268us/sample
Epoch 3/150
95501/95501 - 26s - loss: 0.0022 - val_loss: 0.0018 - 26s/epoch - 271us/sample
Epoch 4/150
95501/95501 - 26s - loss: 0.0020 - val_loss: 0.0018 - 26s/epoch - 269us/sample
Epoch 5/150
95501/95501 - 26s - loss: 0.0019 - val_loss: 0.0016 - 26s/epoch - 270us/sample
Epoch 6/150
95501/95501 - 26s - loss: 0.0018 - val_loss: 0.0016 - 26s/epoch - 271us/sample
Epoch 7/150
95501/95501 - 26s - loss: 0.0018 - val_loss: 0.0016 - 26s/epoch - 269us/sample
Epoch 8/150
95501/95501 - 26s - loss: 0.0018 - val_loss: 0.0015 - 26s/epoch - 270us/sample
Epoch 9/150
95501/95501 - 26s - loss: 0.0017 - val_loss: 0.0015 - 26s/epoch - 271us/sample
Epoch 10/150
95501/95501 - 26s - loss: 0.0017 - val_loss: 0.0015 - 26s/epoch - 269us/sample
Epoch 11/150
95501/95501 - 26s - loss: 0.0017 - val_loss: 0.0015 - 26s/epoch - 269us/sample
Epoch 12/150
95501/95501 - 26s - loss: 0.0017 - val_loss: 0.0015 - 26s/epoch - 269us/sample
Epoch 13/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0015 - 26s/epoch - 270us/sample
Epoch 14/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 15/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 16/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 270us/sample
Epoch 17/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 18/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 19/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 271us/sample
Epoch 20/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 21/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 270us/sample
Epoch 22/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 23/150
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 271us/sample
Epoch 24/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 25/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 26/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0014 - 26s/epoch - 270us/sample
Epoch 27/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 28/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 29/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 30/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 31/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 32/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 33/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 34/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 35/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 36/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 37/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 38/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 39/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 40/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 41/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 42/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 43/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 44/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 45/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0017 - 26s/epoch - 269us/sample
Epoch 46/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 47/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 48/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 49/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 50/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 51/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 52/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 53/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 54/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0014 - 26s/epoch - 269us/sample
Epoch 55/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 56/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 57/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 58/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 59/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 60/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 61/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 62/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 63/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 64/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 65/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 66/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 67/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 68/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 69/150
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 70/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 271us/sample
Epoch 71/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 72/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 73/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 74/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 75/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 76/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 77/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 78/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 79/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 80/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 81/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 82/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 83/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 84/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 85/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 86/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 87/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 88/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 89/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 90/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 270us/sample
Epoch 91/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 92/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 93/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 271us/sample
Epoch 94/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 95/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 96/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 97/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 98/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 99/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 269us/sample
Epoch 100/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 101/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 102/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 103/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 104/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 105/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 106/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 107/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 108/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 109/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 110/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 111/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 112/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 113/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 114/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 115/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 116/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 117/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 118/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 119/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 120/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 121/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 269us/sample
Epoch 122/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 123/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 124/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 269us/sample
Epoch 125/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 126/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 127/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 128/150
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0013 - 25s/epoch - 267us/sample
Epoch 129/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 130/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 269us/sample
Epoch 131/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 132/150
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0013 - 25s/epoch - 267us/sample
Epoch 133/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 134/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 269us/sample
Epoch 135/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 136/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 137/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 270us/sample
Epoch 138/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 139/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 140/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 141/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 142/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 143/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 144/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 145/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 267us/sample
Epoch 146/150
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 267us/sample
Epoch 147/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 269us/sample
Epoch 148/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 149/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 268us/sample
Epoch 150/150
95501/95501 - 26s - loss: 0.0014 - val_loss: 0.0012 - 26s/epoch - 269us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0012463499345039808
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:15:20.024536: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_30/outputlayer/BiasAdd' id:39088 op device:{requested: '', assigned: ''} def:{{{node decoder_model_30/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_30/outputlayer/MatMul, decoder_model_30/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.018464879313595466
cosine 0.014664079012441222
MAE: 0.019756522
RMSE: 0.039892547
r2: 0.8967612508181072
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 16, 150, 0.0014000000000000002, 0.1, 126, 0.0013848078712315893, 0.0012463499345039808, 0.018464879313595466, 0.014664079012441222, 0.01975652202963829, 0.03989254683256149, 0.8967612508181072, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 135 0.0012 32 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_93 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_93 (ReLU)                (None, 1896)         0           ['batch_normalization_93[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/135
2023-02-15 07:15:35.290927: W tensorflow/c/c_api.cc:291] Operation '{name:'training_62/Adam/learning_rate/Assign' id:40884 op device:{requested: '', assigned: ''} def:{{{node training_62/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_62/Adam/learning_rate, training_62/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:15:53.030518: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_31/mul' id:40415 op device:{requested: '', assigned: ''} def:{{{node loss_31/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_31/mul/x, loss_31/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 23s - loss: 0.0059 - val_loss: 0.0033 - 23s/epoch - 243us/sample
Epoch 2/135
95501/95501 - 15s - loss: 0.0032 - val_loss: 0.0024 - 15s/epoch - 158us/sample
Epoch 3/135
95501/95501 - 15s - loss: 0.0024 - val_loss: 0.0019 - 15s/epoch - 158us/sample
Epoch 4/135
95501/95501 - 15s - loss: 0.0020 - val_loss: 0.0017 - 15s/epoch - 159us/sample
Epoch 5/135
95501/95501 - 15s - loss: 0.0018 - val_loss: 0.0016 - 15s/epoch - 157us/sample
Epoch 6/135
95501/95501 - 15s - loss: 0.0017 - val_loss: 0.0015 - 15s/epoch - 158us/sample
Epoch 7/135
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 157us/sample
Epoch 8/135
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 158us/sample
Epoch 9/135
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 158us/sample
Epoch 10/135
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 157us/sample
Epoch 11/135
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 159us/sample
Epoch 12/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 13/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 14/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 15/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 16/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 17/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 158us/sample
Epoch 18/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 19/135
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 157us/sample
Epoch 20/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 21/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 22/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 23/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 24/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 25/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 26/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 27/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 28/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 29/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 30/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 31/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 32/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 33/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 34/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 159us/sample
Epoch 35/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 36/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 37/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 38/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 39/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 40/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 41/135
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 42/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 43/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 44/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 45/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 46/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 47/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 48/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 49/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 50/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 51/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 158us/sample
Epoch 52/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 53/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 54/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 55/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 56/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 57/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 58/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 59/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 60/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 61/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 62/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 63/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 64/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 65/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 66/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 67/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 68/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 69/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 70/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 71/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 72/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 73/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 74/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 75/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 76/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 156us/sample
Epoch 77/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 156us/sample
Epoch 78/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 79/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 80/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 81/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 82/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 83/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 84/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 85/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 86/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 87/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 88/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 89/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 90/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 91/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 92/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 93/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 94/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 95/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 96/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 97/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 98/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 99/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 100/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 101/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 102/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 103/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 104/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 105/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 106/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 107/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 108/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 109/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 110/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 111/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 112/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 113/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 114/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 115/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 116/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 117/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 118/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 119/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 120/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 121/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 122/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 123/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 124/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 125/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 126/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 127/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 128/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 129/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 130/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 131/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 132/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 133/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 134/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 135/135
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010794525589727924
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:49:39.302759: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_31/outputlayer/BiasAdd' id:40379 op device:{requested: '', assigned: ''} def:{{{node decoder_model_31/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_31/outputlayer/MatMul, decoder_model_31/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014640640356475077
cosine 0.011560235817028898
MAE: 0.017543184
RMSE: 0.035405517
r2: 0.9186795583787616
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 32, 135, 0.0012, 0.1, 126, 0.0011654014007924584, 0.0010794525589727924, 0.014640640356475077, 0.011560235817028898, 0.017543183639645576, 0.035405516624450684, 0.9186795583787616, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 3
Fitness    = 65.82334742872027
Last generation's best solutions = [1.5 145 0.0012 64 1] with fitness 65.82334742872027.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object)]
Best solutions fitness :  [65.82334742872027, 65.82334742872027, 65.82334742872027]
[1.6 135 0.0012 64 0] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_96 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_96 (ReLU)                (None, 2022)         0           ['batch_normalization_96[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          254898      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          254898      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2838460     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,914,174
Trainable params: 5,905,834
Non-trainable params: 8,340
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/135
2023-02-15 07:49:55.428277: W tensorflow/c/c_api.cc:291] Operation '{name:'training_64/Adam/dense_dec1_32/bias/m/Assign' id:42291 op device:{requested: '', assigned: ''} def:{{{node training_64/Adam/dense_dec1_32/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_64/Adam/dense_dec1_32/bias/m, training_64/Adam/dense_dec1_32/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:50:08.880653: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_32/mul' id:41718 op device:{requested: '', assigned: ''} def:{{{node loss_32/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_32/mul/x, loss_32/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 272875.2528 - val_loss: 1.4842 - 19s/epoch - 198us/sample
Epoch 2/135
95501/95501 - 10s - loss: 4.2623 - val_loss: 1.4437 - 10s/epoch - 109us/sample
Epoch 3/135
95501/95501 - 10s - loss: 1.4343 - val_loss: 1.4335 - 10s/epoch - 109us/sample
Epoch 4/135
95501/95501 - 10s - loss: 1.4364 - val_loss: 1.4346 - 10s/epoch - 109us/sample
Epoch 5/135
95501/95501 - 10s - loss: 1.4495 - val_loss: 1.4437 - 10s/epoch - 108us/sample
Epoch 6/135
95501/95501 - 10s - loss: 1.4363 - val_loss: 1.4267 - 10s/epoch - 109us/sample
Epoch 7/135
95501/95501 - 11s - loss: 2.5029 - val_loss: 1.4257 - 11s/epoch - 110us/sample
Epoch 8/135
95501/95501 - 10s - loss: 1.5225 - val_loss: 1.4184 - 10s/epoch - 110us/sample
Epoch 9/135
95501/95501 - 10s - loss: 1.4295 - val_loss: 1.4730 - 10s/epoch - 109us/sample
Epoch 10/135
95501/95501 - 10s - loss: 1.4249 - val_loss: 1.4167 - 10s/epoch - 109us/sample
Epoch 11/135
95501/95501 - 10s - loss: 1.4336 - val_loss: 1.4251 - 10s/epoch - 109us/sample
Epoch 12/135
95501/95501 - 10s - loss: 1.4191 - val_loss: 1.5430 - 10s/epoch - 109us/sample
Epoch 13/135
95501/95501 - 10s - loss: 1.4284 - val_loss: 1.4206 - 10s/epoch - 109us/sample
Epoch 14/135
95501/95501 - 10s - loss: 1.4238 - val_loss: 1.4276 - 10s/epoch - 109us/sample
Epoch 15/135
95501/95501 - 11s - loss: 1.4284 - val_loss: 1.4239 - 11s/epoch - 111us/sample
Epoch 16/135
95501/95501 - 10s - loss: 1.4298 - val_loss: 1.4369 - 10s/epoch - 109us/sample
Epoch 17/135
95501/95501 - 10s - loss: 1.4214 - val_loss: 1.4220 - 10s/epoch - 109us/sample
Epoch 18/135
95501/95501 - 10s - loss: 1.4310 - val_loss: 1.4249 - 10s/epoch - 109us/sample
Epoch 19/135
95501/95501 - 10s - loss: 1.4178 - val_loss: 1.4162 - 10s/epoch - 109us/sample
Epoch 20/135
95501/95501 - 10s - loss: 1.4160 - val_loss: 1.4142 - 10s/epoch - 109us/sample
Epoch 21/135
95501/95501 - 10s - loss: 1.4146 - val_loss: 1.4129 - 10s/epoch - 109us/sample
Epoch 22/135
95501/95501 - 10s - loss: 1.4199 - val_loss: 1.4096 - 10s/epoch - 110us/sample
Epoch 23/135
95501/95501 - 11s - loss: 1.4149 - val_loss: 1.4140 - 11s/epoch - 110us/sample
Epoch 24/135
95501/95501 - 10s - loss: 1.4204 - val_loss: 1.4276 - 10s/epoch - 109us/sample
Epoch 25/135
95501/95501 - 10s - loss: 1.4108 - val_loss: 1.4072 - 10s/epoch - 110us/sample
Epoch 26/135
95501/95501 - 10s - loss: 1.4401 - val_loss: 1.4169 - 10s/epoch - 109us/sample
Epoch 27/135
95501/95501 - 10s - loss: 1.4096 - val_loss: 1.4117 - 10s/epoch - 108us/sample
Epoch 28/135
95501/95501 - 10s - loss: 1.4100 - val_loss: 1.4101 - 10s/epoch - 109us/sample
Epoch 29/135
95501/95501 - 10s - loss: 1.4079 - val_loss: 1.4093 - 10s/epoch - 110us/sample
Epoch 30/135
95501/95501 - 11s - loss: 1.4274 - val_loss: 1.4215 - 11s/epoch - 111us/sample
Epoch 31/135
95501/95501 - 10s - loss: 1.4182 - val_loss: 1.4144 - 10s/epoch - 109us/sample
Epoch 32/135
95501/95501 - 10s - loss: 1.4066 - val_loss: 1.4092 - 10s/epoch - 109us/sample
Epoch 33/135
95501/95501 - 10s - loss: 1.4103 - val_loss: 1.4152 - 10s/epoch - 109us/sample
Epoch 34/135
95501/95501 - 10s - loss: 1.4046 - val_loss: 1.4098 - 10s/epoch - 109us/sample
Epoch 35/135
95501/95501 - 10s - loss: 1.4055 - val_loss: 1.4053 - 10s/epoch - 109us/sample
Epoch 36/135
95501/95501 - 10s - loss: 1.4133 - val_loss: 1.4207 - 10s/epoch - 109us/sample
Epoch 37/135
95501/95501 - 11s - loss: 1.4126 - val_loss: 1.4143 - 11s/epoch - 110us/sample
Epoch 38/135
95501/95501 - 10s - loss: 1.4098 - val_loss: 1.4094 - 10s/epoch - 110us/sample
Epoch 39/135
95501/95501 - 10s - loss: 1.4102 - val_loss: 1.4082 - 10s/epoch - 109us/sample
Epoch 40/135
95501/95501 - 10s - loss: 1.4058 - val_loss: 1.4054 - 10s/epoch - 109us/sample
Epoch 41/135
95501/95501 - 10s - loss: 1.4031 - val_loss: 1.4073 - 10s/epoch - 109us/sample
Epoch 42/135
95501/95501 - 10s - loss: 1.4045 - val_loss: 1.4009 - 10s/epoch - 109us/sample
Epoch 43/135
95501/95501 - 10s - loss: 1.4045 - val_loss: 1.4080 - 10s/epoch - 109us/sample
Epoch 44/135
95501/95501 - 10s - loss: 1.4037 - val_loss: 1.4231 - 10s/epoch - 110us/sample
Epoch 45/135
95501/95501 - 11s - loss: 1.4072 - val_loss: 1.4059 - 11s/epoch - 111us/sample
Epoch 46/135
95501/95501 - 10s - loss: 1.4046 - val_loss: 1.4019 - 10s/epoch - 110us/sample
Epoch 47/135
95501/95501 - 10s - loss: 1.3993 - val_loss: 1.4076 - 10s/epoch - 109us/sample
Epoch 48/135
95501/95501 - 10s - loss: 1.4008 - val_loss: 1.4017 - 10s/epoch - 109us/sample
Epoch 49/135
95501/95501 - 10s - loss: 1.4032 - val_loss: 1.4081 - 10s/epoch - 109us/sample
Epoch 50/135
95501/95501 - 10s - loss: 1.4042 - val_loss: 1.4049 - 10s/epoch - 109us/sample
Epoch 51/135
95501/95501 - 10s - loss: 1.4026 - val_loss: 1.4067 - 10s/epoch - 109us/sample
Epoch 52/135
95501/95501 - 11s - loss: 1.4022 - val_loss: 1.4025 - 11s/epoch - 110us/sample
Epoch 53/135
95501/95501 - 10s - loss: 1.3989 - val_loss: 1.3997 - 10s/epoch - 109us/sample
Epoch 54/135
95501/95501 - 10s - loss: 1.3987 - val_loss: 1.4031 - 10s/epoch - 109us/sample
Epoch 55/135
95501/95501 - 10s - loss: 1.3997 - val_loss: 1.4014 - 10s/epoch - 109us/sample
Epoch 56/135
95501/95501 - 10s - loss: 1.4013 - val_loss: 1.4080 - 10s/epoch - 109us/sample
Epoch 57/135
95501/95501 - 10s - loss: 1.4016 - val_loss: 1.4082 - 10s/epoch - 109us/sample
Epoch 58/135
95501/95501 - 10s - loss: 1.3984 - val_loss: 1.4230 - 10s/epoch - 110us/sample
Epoch 59/135
95501/95501 - 11s - loss: 1.4013 - val_loss: 1.4025 - 11s/epoch - 110us/sample
Epoch 60/135
95501/95501 - 11s - loss: 1.3991 - val_loss: 1.4000 - 11s/epoch - 110us/sample
Epoch 61/135
95501/95501 - 10s - loss: 1.4022 - val_loss: 1.4064 - 10s/epoch - 109us/sample
Epoch 62/135
95501/95501 - 10s - loss: 1.4030 - val_loss: 1.4005 - 10s/epoch - 109us/sample
Epoch 63/135
95501/95501 - 10s - loss: 1.4045 - val_loss: 1.4043 - 10s/epoch - 109us/sample
Epoch 64/135
95501/95501 - 10s - loss: 1.3993 - val_loss: 1.4022 - 10s/epoch - 109us/sample
Epoch 65/135
95501/95501 - 10s - loss: 1.3990 - val_loss: 1.3998 - 10s/epoch - 109us/sample
Epoch 66/135
95501/95501 - 10s - loss: 1.4028 - val_loss: 1.4070 - 10s/epoch - 110us/sample
Epoch 67/135
95501/95501 - 11s - loss: 1.4039 - val_loss: 1.4043 - 11s/epoch - 111us/sample
Epoch 68/135
95501/95501 - 10s - loss: 1.4016 - val_loss: 1.4025 - 10s/epoch - 107us/sample
Epoch 69/135
95501/95501 - 10s - loss: 1.4007 - val_loss: 1.4019 - 10s/epoch - 105us/sample
Epoch 70/135
95501/95501 - 10s - loss: 1.4021 - val_loss: 1.4031 - 10s/epoch - 106us/sample
Epoch 71/135
95501/95501 - 10s - loss: 1.4058 - val_loss: 1.4053 - 10s/epoch - 107us/sample
Epoch 72/135
95501/95501 - 10s - loss: 1.4023 - val_loss: 1.4016 - 10s/epoch - 109us/sample
Epoch 73/135
95501/95501 - 10s - loss: 1.3998 - val_loss: 1.4017 - 10s/epoch - 109us/sample
Epoch 74/135
95501/95501 - 10s - loss: 1.4003 - val_loss: 1.4065 - 10s/epoch - 109us/sample
Epoch 75/135
95501/95501 - 11s - loss: 1.4019 - val_loss: 1.4015 - 11s/epoch - 111us/sample
Epoch 76/135
95501/95501 - 10s - loss: 1.4016 - val_loss: 1.4031 - 10s/epoch - 109us/sample
Epoch 77/135
95501/95501 - 10s - loss: 1.3991 - val_loss: 1.4003 - 10s/epoch - 108us/sample
Epoch 78/135
95501/95501 - 10s - loss: 1.3980 - val_loss: 1.3998 - 10s/epoch - 109us/sample
Epoch 79/135
95501/95501 - 10s - loss: 1.3987 - val_loss: 1.4021 - 10s/epoch - 109us/sample
Epoch 80/135
95501/95501 - 10s - loss: 1.3988 - val_loss: 1.4015 - 10s/epoch - 109us/sample
Epoch 81/135
95501/95501 - 10s - loss: 1.3994 - val_loss: 1.4038 - 10s/epoch - 109us/sample
Epoch 82/135
95501/95501 - 11s - loss: 1.3990 - val_loss: 1.4022 - 11s/epoch - 111us/sample
Epoch 83/135
95501/95501 - 10s - loss: 1.4024 - val_loss: 1.4072 - 10s/epoch - 109us/sample
Epoch 84/135
95501/95501 - 10s - loss: 1.3992 - val_loss: 1.3983 - 10s/epoch - 109us/sample
Epoch 85/135
95501/95501 - 10s - loss: 1.3963 - val_loss: 1.4031 - 10s/epoch - 109us/sample
Epoch 86/135
95501/95501 - 10s - loss: 1.3981 - val_loss: 1.3992 - 10s/epoch - 109us/sample
Epoch 87/135
95501/95501 - 10s - loss: 1.3990 - val_loss: 1.4039 - 10s/epoch - 109us/sample
Epoch 88/135
95501/95501 - 10s - loss: 1.3983 - val_loss: 1.3984 - 10s/epoch - 109us/sample
Epoch 89/135
95501/95501 - 10s - loss: 1.3993 - val_loss: 1.4095 - 10s/epoch - 109us/sample
Epoch 90/135
95501/95501 - 11s - loss: 1.3999 - val_loss: 1.3992 - 11s/epoch - 110us/sample
Epoch 91/135
95501/95501 - 10s - loss: 1.4011 - val_loss: 1.3999 - 10s/epoch - 109us/sample
Epoch 92/135
95501/95501 - 10s - loss: 1.3956 - val_loss: 1.3962 - 10s/epoch - 109us/sample
Epoch 93/135
95501/95501 - 10s - loss: 1.4014 - val_loss: 1.4011 - 10s/epoch - 109us/sample
Epoch 94/135
95501/95501 - 10s - loss: 1.4002 - val_loss: 1.4082 - 10s/epoch - 109us/sample
Epoch 95/135
95501/95501 - 10s - loss: 1.3981 - val_loss: 1.3982 - 10s/epoch - 109us/sample
Epoch 96/135
95501/95501 - 11s - loss: 1.3981 - val_loss: 1.4012 - 11s/epoch - 110us/sample
Epoch 97/135
95501/95501 - 11s - loss: 1.3976 - val_loss: 1.3991 - 11s/epoch - 110us/sample
Epoch 98/135
95501/95501 - 10s - loss: 1.3987 - val_loss: 1.4055 - 10s/epoch - 109us/sample
Epoch 99/135
95501/95501 - 10s - loss: 1.4031 - val_loss: 1.4036 - 10s/epoch - 109us/sample
Epoch 100/135
95501/95501 - 10s - loss: 1.4008 - val_loss: 1.4004 - 10s/epoch - 109us/sample
Epoch 101/135
95501/95501 - 10s - loss: 1.3975 - val_loss: 1.3984 - 10s/epoch - 109us/sample
Epoch 102/135
95501/95501 - 10s - loss: 1.3980 - val_loss: 1.4009 - 10s/epoch - 109us/sample
Epoch 103/135
95501/95501 - 10s - loss: 1.3968 - val_loss: 1.3975 - 10s/epoch - 109us/sample
Epoch 104/135
95501/95501 - 11s - loss: 1.3980 - val_loss: 1.4090 - 11s/epoch - 110us/sample
Epoch 105/135
95501/95501 - 10s - loss: 1.4001 - val_loss: 1.4013 - 10s/epoch - 109us/sample
Epoch 106/135
95501/95501 - 10s - loss: 1.3977 - val_loss: 1.3993 - 10s/epoch - 109us/sample
Epoch 107/135
95501/95501 - 10s - loss: 1.3980 - val_loss: 1.3978 - 10s/epoch - 109us/sample
Epoch 108/135
95501/95501 - 10s - loss: 1.3977 - val_loss: 1.3985 - 10s/epoch - 109us/sample
Epoch 109/135
95501/95501 - 10s - loss: 1.4021 - val_loss: 1.4041 - 10s/epoch - 109us/sample
Epoch 110/135
95501/95501 - 10s - loss: 1.3992 - val_loss: 1.4068 - 10s/epoch - 109us/sample
Epoch 111/135
95501/95501 - 10s - loss: 1.4007 - val_loss: 1.3999 - 10s/epoch - 109us/sample
Epoch 112/135
95501/95501 - 11s - loss: 1.3978 - val_loss: 1.3983 - 11s/epoch - 110us/sample
Epoch 113/135
95501/95501 - 10s - loss: 1.3979 - val_loss: 1.3997 - 10s/epoch - 109us/sample
Epoch 114/135
95501/95501 - 10s - loss: 1.3973 - val_loss: 1.4073 - 10s/epoch - 109us/sample
Epoch 115/135
95501/95501 - 10s - loss: 1.3994 - val_loss: 1.3999 - 10s/epoch - 109us/sample
Epoch 116/135
95501/95501 - 10s - loss: 1.3996 - val_loss: 1.4005 - 10s/epoch - 109us/sample
Epoch 117/135
95501/95501 - 10s - loss: 1.3972 - val_loss: 1.3980 - 10s/epoch - 108us/sample
Epoch 118/135
95501/95501 - 10s - loss: 1.4016 - val_loss: 1.4024 - 10s/epoch - 109us/sample
Epoch 119/135
95501/95501 - 11s - loss: 1.3982 - val_loss: 1.4021 - 11s/epoch - 110us/sample
Epoch 120/135
95501/95501 - 10s - loss: 1.3975 - val_loss: 1.3984 - 10s/epoch - 110us/sample
Epoch 121/135
95501/95501 - 10s - loss: 1.3954 - val_loss: 1.3996 - 10s/epoch - 109us/sample
Epoch 122/135
95501/95501 - 10s - loss: 1.4022 - val_loss: 1.4037 - 10s/epoch - 109us/sample
Epoch 123/135
95501/95501 - 10s - loss: 1.3996 - val_loss: 1.3999 - 10s/epoch - 109us/sample
Epoch 124/135
95501/95501 - 10s - loss: 1.3967 - val_loss: 1.3979 - 10s/epoch - 109us/sample
Epoch 125/135
95501/95501 - 10s - loss: 1.3957 - val_loss: 1.3972 - 10s/epoch - 109us/sample
Epoch 126/135
95501/95501 - 10s - loss: 1.6331 - val_loss: 1.4006 - 10s/epoch - 110us/sample
Epoch 127/135
95501/95501 - 11s - loss: 1.3943 - val_loss: 1.3961 - 11s/epoch - 110us/sample
Epoch 128/135
95501/95501 - 10s - loss: 1.3972 - val_loss: 1.4000 - 10s/epoch - 109us/sample
Epoch 129/135
95501/95501 - 10s - loss: 1.3965 - val_loss: 1.4022 - 10s/epoch - 109us/sample
Epoch 130/135
95501/95501 - 10s - loss: 1.3972 - val_loss: 1.3972 - 10s/epoch - 109us/sample
Epoch 131/135
95501/95501 - 10s - loss: 1.3973 - val_loss: 1.3958 - 10s/epoch - 109us/sample
Epoch 132/135
95501/95501 - 10s - loss: 1.3943 - val_loss: 1.3968 - 10s/epoch - 109us/sample
Epoch 133/135
95501/95501 - 10s - loss: 1.3946 - val_loss: 1.3977 - 10s/epoch - 109us/sample
Epoch 134/135
95501/95501 - 10s - loss: 1.3961 - val_loss: 1.3973 - 10s/epoch - 110us/sample
Epoch 135/135
95501/95501 - 10s - loss: 1.3953 - val_loss: 1.3972 - 10s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.3971804747525765
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:13:30.396042: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_32/outputlayer/BiasAdd' id:41670 op device:{requested: '', assigned: ''} def:{{{node decoder_model_32/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_32/outputlayer/MatMul, decoder_model_32/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7914101421116622
cosine 0.954191445209315
MAE: 7.1154222
RMSE: 16.748983
r2: -18195.103363237147
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'binary_crossentropy', 64, 135, 0.0012, 0.1, 126, 1.3953227669060229, 1.3971804747525765, 0.7914101421116622, 0.954191445209315, 7.115422248840332, 16.74898338317871, -18195.103363237147, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 145 0.0016000000000000003 32 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_99 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_99 (ReLU)                (None, 1769)         0           ['batch_normalization_99[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          223020      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          223020      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2485525     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,176,426
Trainable params: 5,169,098
Non-trainable params: 7,328
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 08:13:46.290344: W tensorflow/c/c_api.cc:291] Operation '{name:'training_66/Adam/dense_enc0_33/kernel/v/Assign' id:43641 op device:{requested: '', assigned: ''} def:{{{node training_66/Adam/dense_enc0_33/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_66/Adam/dense_enc0_33/kernel/v, training_66/Adam/dense_enc0_33/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:14:04.473420: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_33/mul' id:43049 op device:{requested: '', assigned: ''} def:{{{node loss_33/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_33/mul/x, loss_33/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 24s - loss: 0.0057 - val_loss: 0.0031 - 24s/epoch - 252us/sample
Epoch 2/145
95501/95501 - 15s - loss: 0.0031 - val_loss: 0.0026 - 15s/epoch - 161us/sample
Epoch 3/145
95501/95501 - 15s - loss: 0.0024 - val_loss: 0.0019 - 15s/epoch - 162us/sample
Epoch 4/145
95501/95501 - 15s - loss: 0.0020 - val_loss: 0.0017 - 15s/epoch - 161us/sample
Epoch 5/145
95501/95501 - 15s - loss: 0.0018 - val_loss: 0.0016 - 15s/epoch - 161us/sample
Epoch 6/145
95501/95501 - 15s - loss: 0.0017 - val_loss: 0.0015 - 15s/epoch - 161us/sample
Epoch 7/145
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 160us/sample
Epoch 8/145
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 161us/sample
Epoch 9/145
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 162us/sample
Epoch 10/145
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 161us/sample
Epoch 11/145
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 161us/sample
Epoch 12/145
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 160us/sample
Epoch 13/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 161us/sample
Epoch 14/145
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0013 - 16s/epoch - 163us/sample
Epoch 15/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 160us/sample
Epoch 16/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 160us/sample
Epoch 17/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 18/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 19/145
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 162us/sample
Epoch 20/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 21/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 22/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 23/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 24/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 25/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 26/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0014 - 15s/epoch - 161us/sample
Epoch 27/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 28/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 29/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 30/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 31/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 32/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 33/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 34/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 35/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 36/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 37/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 38/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 39/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 40/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 41/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 42/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 43/145
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 44/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 45/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 46/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 47/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 48/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 49/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 50/145
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 162us/sample
Epoch 51/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 52/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 53/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 54/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 55/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 56/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 57/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 58/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 59/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 60/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 61/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 62/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 63/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 64/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 65/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 66/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 67/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 68/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 69/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 70/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 71/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 72/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 73/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 156us/sample
Epoch 74/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 75/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 76/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 77/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 78/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 157us/sample
Epoch 79/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 80/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 81/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 82/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 83/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 84/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 85/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 86/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 87/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 88/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 89/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 90/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 91/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 92/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 93/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 94/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 95/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 96/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 97/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 98/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 99/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 100/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 101/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 102/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 103/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 104/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 105/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 106/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 107/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 108/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 109/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 110/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 111/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 112/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 113/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 114/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 115/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 116/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 117/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 118/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 119/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 120/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 121/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 159us/sample
Epoch 122/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 123/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 124/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 125/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 126/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 127/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 128/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 129/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 130/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 131/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 132/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 133/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 134/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 135/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 136/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 137/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 138/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 139/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 140/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 141/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 142/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 158us/sample
Epoch 143/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 144/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
Epoch 145/145
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 157us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.001069005933619019
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:50:34.491717: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_33/outputlayer/BiasAdd' id:43013 op device:{requested: '', assigned: ''} def:{{{node decoder_model_33/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_33/outputlayer/MatMul, decoder_model_33/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014670593392341218
cosine 0.011593214395223274
MAE: 0.01754695
RMSE: 0.035389464
r2: 0.9187530558883702
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'logcosh', 32, 145, 0.0016000000000000003, 0.1, 126, 0.0011624513050711004, 0.001069005933619019, 0.014670593392341218, 0.011593214395223274, 0.01754694990813732, 0.03538946434855461, 0.9187530558883702, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.0012000000000000001 32 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_102 (Batch  (None, 1896)        7584        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_102 (ReLU)               (None, 1896)         0           ['batch_normalization_102[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-15 08:50:50.820678: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_104/moving_variance/Assign' id:44161 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_104/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_104/moving_variance, batch_normalization_104/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:51:09.007508: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_34/mul' id:44340 op device:{requested: '', assigned: ''} def:{{{node loss_34/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_34/mul/x, loss_34/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 24s - loss: 0.0059 - val_loss: 0.0033 - 24s/epoch - 253us/sample
Epoch 2/150
95501/95501 - 15s - loss: 0.0033 - val_loss: 0.0024 - 15s/epoch - 160us/sample
Epoch 3/150
95501/95501 - 15s - loss: 0.0024 - val_loss: 0.0022 - 15s/epoch - 161us/sample
Epoch 4/150
95501/95501 - 15s - loss: 0.0021 - val_loss: 0.0017 - 15s/epoch - 161us/sample
Epoch 5/150
95501/95501 - 15s - loss: 0.0018 - val_loss: 0.0016 - 15s/epoch - 160us/sample
Epoch 6/150
95501/95501 - 15s - loss: 0.0017 - val_loss: 0.0015 - 15s/epoch - 162us/sample
Epoch 7/150
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 161us/sample
Epoch 8/150
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 160us/sample
Epoch 9/150
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 160us/sample
Epoch 10/150
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0014 - 15s/epoch - 161us/sample
Epoch 11/150
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 160us/sample
Epoch 12/150
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 162us/sample
Epoch 13/150
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 160us/sample
Epoch 14/150
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 160us/sample
Epoch 15/150
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 161us/sample
Epoch 16/150
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 161us/sample
Epoch 17/150
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 160us/sample
Epoch 18/150
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 162us/sample
Epoch 19/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 20/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 21/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 22/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 23/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 24/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 25/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 26/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 27/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 28/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 29/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0012 - 16s/epoch - 163us/sample
Epoch 30/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 31/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 32/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 33/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 34/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 35/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 36/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 37/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 38/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 39/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 40/150
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 41/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 42/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 161us/sample
Epoch 43/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 44/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 45/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 46/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 162us/sample
Epoch 47/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 48/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 49/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 50/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 51/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 52/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 53/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 54/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 55/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 56/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 57/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 58/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 59/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 60/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 61/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 62/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 63/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 64/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 65/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 66/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 67/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 68/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 69/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 70/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 71/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 72/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 73/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 74/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 75/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 76/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 77/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 78/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 79/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 80/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 81/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 82/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 83/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 84/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 85/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 86/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 87/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 88/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 89/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 90/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 91/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 92/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 93/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 94/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 95/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 96/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 97/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 98/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 162us/sample
Epoch 99/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 100/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 101/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 102/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 103/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 104/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 105/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 106/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 107/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 108/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 109/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 110/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 111/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 112/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 113/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 114/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 115/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 116/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 162us/sample
Epoch 117/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 118/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 119/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 120/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 121/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 122/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 123/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 124/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 125/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 126/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 160us/sample
Epoch 127/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 128/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 129/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 130/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 131/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 132/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 133/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 134/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 135/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 136/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 137/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 138/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 139/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
Epoch 140/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 141/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 142/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 143/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 144/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 145/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 161us/sample
Epoch 146/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 147/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 148/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 149/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 160us/sample
Epoch 150/150
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 162us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 0.0010803593910196385
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:29:18.703213: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_34/outputlayer/BiasAdd' id:44304 op device:{requested: '', assigned: ''} def:{{{node decoder_model_34/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_34/outputlayer/MatMul, decoder_model_34/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0149282474215418
cosine 0.011797248890298383
MAE: 0.017783355
RMSE: 0.03574308
r2: 0.9171213609182459
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 32, 150, 0.0012000000000000001, 0.1, 126, 0.0011710071645577736, 0.0010803593910196385, 0.0149282474215418, 0.011797248890298383, 0.017783354967832565, 0.03574308007955551, 0.9171213609182459, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 145 0.0012 64 0] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_105 (Batch  (None, 1896)        7584        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_105 (ReLU)               (None, 1896)         0           ['batch_normalization_105[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          239022      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          239022      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2662690     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,546,758
Trainable params: 5,538,922
Non-trainable params: 7,836
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 09:29:35.465800: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_35/kernel/Assign' id:45413 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_35/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_35/kernel, dense_dec0_35/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:29:48.794569: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_35/mul' id:45643 op device:{requested: '', assigned: ''} def:{{{node loss_35/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_35/mul/x, loss_35/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 87399.8935 - val_loss: 1.4833 - 19s/epoch - 200us/sample
Epoch 2/145
95501/95501 - 10s - loss: 1.4655 - val_loss: 1.4622 - 10s/epoch - 105us/sample
Epoch 3/145
95501/95501 - 10s - loss: 1.4590 - val_loss: 1.4597 - 10s/epoch - 105us/sample
Epoch 4/145
95501/95501 - 10s - loss: 1.4694 - val_loss: 1.5207 - 10s/epoch - 105us/sample
Epoch 5/145
95501/95501 - 10s - loss: 1.4725 - val_loss: 1.4498 - 10s/epoch - 106us/sample
Epoch 6/145
95501/95501 - 10s - loss: 1.4366 - val_loss: 1.4304 - 10s/epoch - 106us/sample
Epoch 7/145
95501/95501 - 10s - loss: 1.4325 - val_loss: 1.4341 - 10s/epoch - 105us/sample
Epoch 8/145
95501/95501 - 10s - loss: 1.4271 - val_loss: 1.4321 - 10s/epoch - 105us/sample
Epoch 9/145
95501/95501 - 10s - loss: 1.4262 - val_loss: 1.4316 - 10s/epoch - 105us/sample
Epoch 10/145
95501/95501 - 10s - loss: 1.4686 - val_loss: 1.4559 - 10s/epoch - 106us/sample
Epoch 11/145
95501/95501 - 10s - loss: 1.4313 - val_loss: 1.4235 - 10s/epoch - 105us/sample
Epoch 12/145
95501/95501 - 10s - loss: 1.4265 - val_loss: 1.4746 - 10s/epoch - 105us/sample
Epoch 13/145
95501/95501 - 10s - loss: 1.4349 - val_loss: 1.4256 - 10s/epoch - 105us/sample
Epoch 14/145
95501/95501 - 10s - loss: 1.4329 - val_loss: 1.4647 - 10s/epoch - 106us/sample
Epoch 15/145
95501/95501 - 10s - loss: 1.4318 - val_loss: 1.4203 - 10s/epoch - 105us/sample
Epoch 16/145
95501/95501 - 10s - loss: 1.5195 - val_loss: 1.4537 - 10s/epoch - 105us/sample
Epoch 17/145
95501/95501 - 10s - loss: 1.4380 - val_loss: 1.4284 - 10s/epoch - 105us/sample
Epoch 18/145
95501/95501 - 10s - loss: 1.4234 - val_loss: 1.4822 - 10s/epoch - 105us/sample
Epoch 19/145
95501/95501 - 10s - loss: 1.4293 - val_loss: 1.4295 - 10s/epoch - 105us/sample
Epoch 20/145
95501/95501 - 10s - loss: 1.4306 - val_loss: 1.4229 - 10s/epoch - 105us/sample
Epoch 21/145
95501/95501 - 10s - loss: 1.4323 - val_loss: 1.4286 - 10s/epoch - 105us/sample
Epoch 22/145
95501/95501 - 10s - loss: 1.4210 - val_loss: 1.4243 - 10s/epoch - 106us/sample
Epoch 23/145
95501/95501 - 10s - loss: 1.4359 - val_loss: 1.4337 - 10s/epoch - 106us/sample
Epoch 24/145
95501/95501 - 10s - loss: 1.4390 - val_loss: 1.4362 - 10s/epoch - 105us/sample
Epoch 25/145
95501/95501 - 10s - loss: 1.4259 - val_loss: 1.4256 - 10s/epoch - 105us/sample
Epoch 26/145
95501/95501 - 10s - loss: 1.4332 - val_loss: 1.4235 - 10s/epoch - 105us/sample
Epoch 27/145
95501/95501 - 10s - loss: 1.4359 - val_loss: 1.4307 - 10s/epoch - 105us/sample
Epoch 28/145
95501/95501 - 10s - loss: 1.4452 - val_loss: 1.4332 - 10s/epoch - 105us/sample
Epoch 29/145
95501/95501 - 10s - loss: 1.4257 - val_loss: 1.4223 - 10s/epoch - 105us/sample
Epoch 30/145
95501/95501 - 10s - loss: 1.4178 - val_loss: 1.4298 - 10s/epoch - 107us/sample
Epoch 31/145
95501/95501 - 10s - loss: 1.4245 - val_loss: 1.4248 - 10s/epoch - 105us/sample
Epoch 32/145
95501/95501 - 10s - loss: 1.4272 - val_loss: 1.4223 - 10s/epoch - 106us/sample
Epoch 33/145
95501/95501 - 10s - loss: 1.4240 - val_loss: 1.4346 - 10s/epoch - 105us/sample
Epoch 34/145
95501/95501 - 10s - loss: 1.4434 - val_loss: 1.4442 - 10s/epoch - 105us/sample
Epoch 35/145
95501/95501 - 10s - loss: 1.4279 - val_loss: 1.4222 - 10s/epoch - 105us/sample
Epoch 36/145
95501/95501 - 10s - loss: 1.4232 - val_loss: 1.4203 - 10s/epoch - 105us/sample
Epoch 37/145
95501/95501 - 10s - loss: 1.4213 - val_loss: 1.4225 - 10s/epoch - 105us/sample
Epoch 38/145
95501/95501 - 10s - loss: 1.4353 - val_loss: 1.4302 - 10s/epoch - 106us/sample
Epoch 39/145
95501/95501 - 10s - loss: 1.4204 - val_loss: 1.4181 - 10s/epoch - 106us/sample
Epoch 40/145
95501/95501 - 10s - loss: 1.4188 - val_loss: 1.4644 - 10s/epoch - 105us/sample
Epoch 41/145
95501/95501 - 10s - loss: 1.4267 - val_loss: 1.4243 - 10s/epoch - 105us/sample
Epoch 42/145
95501/95501 - 10s - loss: 1.4194 - val_loss: 1.4186 - 10s/epoch - 105us/sample
Epoch 43/145
95501/95501 - 10s - loss: 1.4167 - val_loss: 1.4242 - 10s/epoch - 106us/sample
Epoch 44/145
95501/95501 - 10s - loss: 1.4208 - val_loss: 1.4172 - 10s/epoch - 105us/sample
Epoch 45/145
95501/95501 - 10s - loss: 1.4171 - val_loss: 1.4208 - 10s/epoch - 105us/sample
Epoch 46/145
95501/95501 - 10s - loss: 1.4154 - val_loss: 1.4164 - 10s/epoch - 106us/sample
Epoch 47/145
95501/95501 - 10s - loss: 1.4205 - val_loss: 1.4193 - 10s/epoch - 105us/sample
Epoch 48/145
95501/95501 - 10s - loss: 1.4153 - val_loss: 1.4168 - 10s/epoch - 105us/sample
Epoch 49/145
95501/95501 - 10s - loss: 1.4149 - val_loss: 1.4183 - 10s/epoch - 105us/sample
Epoch 50/145
95501/95501 - 10s - loss: 1.4175 - val_loss: 1.4162 - 10s/epoch - 105us/sample
Epoch 51/145
95501/95501 - 10s - loss: 1.4149 - val_loss: 1.4146 - 10s/epoch - 105us/sample
Epoch 52/145
95501/95501 - 10s - loss: 1.4133 - val_loss: 1.4154 - 10s/epoch - 105us/sample
Epoch 53/145
95501/95501 - 10s - loss: 1.4173 - val_loss: 1.4163 - 10s/epoch - 105us/sample
Epoch 54/145
95501/95501 - 10s - loss: 1.4138 - val_loss: 1.4143 - 10s/epoch - 106us/sample
Epoch 55/145
95501/95501 - 10s - loss: 1.4161 - val_loss: 1.4174 - 10s/epoch - 105us/sample
Epoch 56/145
95501/95501 - 10s - loss: 1.4161 - val_loss: 1.4145 - 10s/epoch - 106us/sample
Epoch 57/145
95501/95501 - 10s - loss: 1.4119 - val_loss: 1.4131 - 10s/epoch - 105us/sample
Epoch 58/145
95501/95501 - 10s - loss: 1.4141 - val_loss: 1.4147 - 10s/epoch - 105us/sample
Epoch 59/145
95501/95501 - 10s - loss: 1.4135 - val_loss: 1.4145 - 10s/epoch - 105us/sample
Epoch 60/145
95501/95501 - 10s - loss: 1.4124 - val_loss: 1.4133 - 10s/epoch - 106us/sample
Epoch 61/145
95501/95501 - 10s - loss: 1.4139 - val_loss: 1.4322 - 10s/epoch - 105us/sample
Epoch 62/145
95501/95501 - 10s - loss: 1.4171 - val_loss: 1.4141 - 10s/epoch - 106us/sample
Epoch 63/145
95501/95501 - 10s - loss: 1.4137 - val_loss: 1.4156 - 10s/epoch - 106us/sample
Epoch 64/145
95501/95501 - 10s - loss: 1.4125 - val_loss: 1.4124 - 10s/epoch - 105us/sample
Epoch 65/145
95501/95501 - 10s - loss: 1.4116 - val_loss: 1.4145 - 10s/epoch - 105us/sample
Epoch 66/145
95501/95501 - 10s - loss: 1.4111 - val_loss: 1.4114 - 10s/epoch - 105us/sample
Epoch 67/145
95501/95501 - 10s - loss: 1.4118 - val_loss: 1.4140 - 10s/epoch - 106us/sample
Epoch 68/145
95501/95501 - 10s - loss: 1.4122 - val_loss: 1.4126 - 10s/epoch - 105us/sample
Epoch 69/145
95501/95501 - 10s - loss: 1.4116 - val_loss: 1.4133 - 10s/epoch - 105us/sample
Epoch 70/145
95501/95501 - 10s - loss: 1.4125 - val_loss: 1.4147 - 10s/epoch - 106us/sample
Epoch 71/145
95501/95501 - 10s - loss: 1.4131 - val_loss: 1.4156 - 10s/epoch - 106us/sample
Epoch 72/145
95501/95501 - 10s - loss: 1.4124 - val_loss: 1.4132 - 10s/epoch - 105us/sample
Epoch 73/145
95501/95501 - 10s - loss: 1.4113 - val_loss: 1.4123 - 10s/epoch - 105us/sample
Epoch 74/145
95501/95501 - 10s - loss: 1.4104 - val_loss: 1.4125 - 10s/epoch - 105us/sample
Epoch 75/145
95501/95501 - 10s - loss: 1.4139 - val_loss: 1.4157 - 10s/epoch - 105us/sample
Epoch 76/145
95501/95501 - 10s - loss: 1.4150 - val_loss: 1.4197 - 10s/epoch - 105us/sample
Epoch 77/145
95501/95501 - 10s - loss: 1.4129 - val_loss: 1.4135 - 10s/epoch - 105us/sample
Epoch 78/145
95501/95501 - 10s - loss: 1.4111 - val_loss: 1.4123 - 10s/epoch - 106us/sample
Epoch 79/145
95501/95501 - 10s - loss: 1.4118 - val_loss: 1.4123 - 10s/epoch - 105us/sample
Epoch 80/145
95501/95501 - 10s - loss: 1.4136 - val_loss: 1.4129 - 10s/epoch - 106us/sample
Epoch 81/145
95501/95501 - 10s - loss: 1.4119 - val_loss: 1.4131 - 10s/epoch - 105us/sample
Epoch 82/145
95501/95501 - 10s - loss: 1.4106 - val_loss: 1.4125 - 10s/epoch - 105us/sample
Epoch 83/145
95501/95501 - 10s - loss: 1.4114 - val_loss: 1.4124 - 10s/epoch - 105us/sample
Epoch 84/145
95501/95501 - 10s - loss: 1.4102 - val_loss: 1.4120 - 10s/epoch - 105us/sample
Epoch 85/145
95501/95501 - 10s - loss: 1.4118 - val_loss: 1.4121 - 10s/epoch - 105us/sample
Epoch 86/145
95501/95501 - 10s - loss: 1.4107 - val_loss: 1.4156 - 10s/epoch - 107us/sample
Epoch 87/145
95501/95501 - 10s - loss: 1.4121 - val_loss: 1.4124 - 10s/epoch - 105us/sample
Epoch 88/145
95501/95501 - 10s - loss: 1.4104 - val_loss: 1.4116 - 10s/epoch - 105us/sample
Epoch 89/145
95501/95501 - 10s - loss: 1.4104 - val_loss: 1.4117 - 10s/epoch - 105us/sample
Epoch 90/145
95501/95501 - 10s - loss: 1.4108 - val_loss: 1.4124 - 10s/epoch - 105us/sample
Epoch 91/145
95501/95501 - 10s - loss: 1.4126 - val_loss: 1.4146 - 10s/epoch - 105us/sample
Epoch 92/145
95501/95501 - 10s - loss: 1.4125 - val_loss: 1.4132 - 10s/epoch - 106us/sample
Epoch 93/145
95501/95501 - 10s - loss: 1.4120 - val_loss: 1.4134 - 10s/epoch - 105us/sample
Epoch 94/145
95501/95501 - 10s - loss: 1.4109 - val_loss: 1.4131 - 10s/epoch - 107us/sample
Epoch 95/145
95501/95501 - 10s - loss: 1.4110 - val_loss: 1.4130 - 10s/epoch - 105us/sample
Epoch 96/145
95501/95501 - 10s - loss: 1.4126 - val_loss: 1.4159 - 10s/epoch - 106us/sample
Epoch 97/145
95501/95501 - 10s - loss: 1.4122 - val_loss: 1.4124 - 10s/epoch - 105us/sample
Epoch 98/145
95501/95501 - 10s - loss: 1.4118 - val_loss: 1.4129 - 10s/epoch - 106us/sample
Epoch 99/145
95501/95501 - 10s - loss: 1.4112 - val_loss: 1.4131 - 10s/epoch - 105us/sample
Epoch 100/145
95501/95501 - 10s - loss: 1.4113 - val_loss: 1.4130 - 10s/epoch - 105us/sample
Epoch 101/145
95501/95501 - 10s - loss: 1.4128 - val_loss: 1.4139 - 10s/epoch - 106us/sample
Epoch 102/145
95501/95501 - 10s - loss: 1.4111 - val_loss: 1.4120 - 10s/epoch - 106us/sample
Epoch 103/145
95501/95501 - 10s - loss: 1.4100 - val_loss: 1.4114 - 10s/epoch - 105us/sample
Epoch 104/145
95501/95501 - 10s - loss: 1.4131 - val_loss: 1.4134 - 10s/epoch - 105us/sample
Epoch 105/145
95501/95501 - 10s - loss: 1.4132 - val_loss: 1.4199 - 10s/epoch - 105us/sample
Epoch 106/145
95501/95501 - 10s - loss: 1.4128 - val_loss: 1.4120 - 10s/epoch - 105us/sample
Epoch 107/145
95501/95501 - 10s - loss: 1.4099 - val_loss: 1.4113 - 10s/epoch - 105us/sample
Epoch 108/145
95501/95501 - 10s - loss: 1.4135 - val_loss: 1.4157 - 10s/epoch - 106us/sample
Epoch 109/145
95501/95501 - 10s - loss: 1.4123 - val_loss: 1.4130 - 10s/epoch - 105us/sample
Epoch 110/145
95501/95501 - 10s - loss: 1.4110 - val_loss: 1.4119 - 10s/epoch - 106us/sample
Epoch 111/145
95501/95501 - 10s - loss: 1.4124 - val_loss: 1.4155 - 10s/epoch - 105us/sample
Epoch 112/145
95501/95501 - 10s - loss: 1.4117 - val_loss: 1.4154 - 10s/epoch - 105us/sample
Epoch 113/145
95501/95501 - 10s - loss: 1.4108 - val_loss: 1.4126 - 10s/epoch - 105us/sample
Epoch 114/145
95501/95501 - 10s - loss: 1.4110 - val_loss: 1.4122 - 10s/epoch - 105us/sample
Epoch 115/145
95501/95501 - 10s - loss: 1.4101 - val_loss: 1.4108 - 10s/epoch - 105us/sample
Epoch 116/145
95501/95501 - 10s - loss: 1.4098 - val_loss: 1.4112 - 10s/epoch - 105us/sample
Epoch 117/145
95501/95501 - 10s - loss: 1.4114 - val_loss: 1.4142 - 10s/epoch - 106us/sample
Epoch 118/145
95501/95501 - 10s - loss: 1.4111 - val_loss: 1.4117 - 10s/epoch - 105us/sample
Epoch 119/145
95501/95501 - 10s - loss: 1.4100 - val_loss: 1.4118 - 10s/epoch - 105us/sample
Epoch 120/145
95501/95501 - 10s - loss: 1.4113 - val_loss: 1.4131 - 10s/epoch - 105us/sample
Epoch 121/145
95501/95501 - 10s - loss: 1.4108 - val_loss: 1.4125 - 10s/epoch - 105us/sample
Epoch 122/145
95501/95501 - 10s - loss: 1.4103 - val_loss: 1.4111 - 10s/epoch - 105us/sample
Epoch 123/145
95501/95501 - 10s - loss: 1.4102 - val_loss: 1.4112 - 10s/epoch - 105us/sample
Epoch 124/145
95501/95501 - 10s - loss: 1.4101 - val_loss: 1.4118 - 10s/epoch - 105us/sample
Epoch 125/145
95501/95501 - 10s - loss: 1.4095 - val_loss: 1.4107 - 10s/epoch - 106us/sample
Epoch 126/145
95501/95501 - 10s - loss: 1.4099 - val_loss: 1.4112 - 10s/epoch - 106us/sample
Epoch 127/145
95501/95501 - 10s - loss: 1.4104 - val_loss: 1.4120 - 10s/epoch - 105us/sample
Epoch 128/145
95501/95501 - 10s - loss: 1.4098 - val_loss: 1.4111 - 10s/epoch - 105us/sample
Epoch 129/145
95501/95501 - 10s - loss: 1.4099 - val_loss: 1.4168 - 10s/epoch - 106us/sample
Epoch 130/145
95501/95501 - 10s - loss: 1.4117 - val_loss: 1.4121 - 10s/epoch - 106us/sample
Epoch 131/145
95501/95501 - 10s - loss: 1.4104 - val_loss: 1.4117 - 10s/epoch - 105us/sample
Epoch 132/145
95501/95501 - 10s - loss: 1.4104 - val_loss: 1.4116 - 10s/epoch - 105us/sample
Epoch 133/145
95501/95501 - 10s - loss: 1.4107 - val_loss: 1.4130 - 10s/epoch - 106us/sample
Epoch 134/145
95501/95501 - 10s - loss: 1.4103 - val_loss: 1.4118 - 10s/epoch - 106us/sample
Epoch 135/145
95501/95501 - 10s - loss: 1.4108 - val_loss: 1.4116 - 10s/epoch - 105us/sample
Epoch 136/145
95501/95501 - 10s - loss: 1.4103 - val_loss: 1.4117 - 10s/epoch - 106us/sample
Epoch 137/145
95501/95501 - 10s - loss: 1.4100 - val_loss: 1.4116 - 10s/epoch - 105us/sample
Epoch 138/145
95501/95501 - 10s - loss: 1.4099 - val_loss: 1.4124 - 10s/epoch - 105us/sample
Epoch 139/145
95501/95501 - 10s - loss: 1.4098 - val_loss: 1.4109 - 10s/epoch - 105us/sample
Epoch 140/145
95501/95501 - 10s - loss: 1.4089 - val_loss: 1.4107 - 10s/epoch - 106us/sample
Epoch 141/145
95501/95501 - 10s - loss: 1.4086 - val_loss: 1.4108 - 10s/epoch - 106us/sample
Epoch 142/145
95501/95501 - 10s - loss: 1.4091 - val_loss: 1.4106 - 10s/epoch - 106us/sample
Epoch 143/145
95501/95501 - 10s - loss: 1.4100 - val_loss: 1.4107 - 10s/epoch - 105us/sample
Epoch 144/145
95501/95501 - 10s - loss: 1.4093 - val_loss: 1.4106 - 10s/epoch - 105us/sample
Epoch 145/145
95501/95501 - 10s - loss: 1.4091 - val_loss: 1.4110 - 10s/epoch - 105us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.410952032263936
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:54:01.065003: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_35/outputlayer/BiasAdd' id:45595 op device:{requested: '', assigned: ''} def:{{{node decoder_model_35/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_35/outputlayer/MatMul, decoder_model_35/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7599722282916077
cosine 0.9185455804361237
MAE: 7.4186726
RMSE: 18.531221
r2: -22273.59201772343
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'binary_crossentropy', 64, 145, 0.0012, 0.1, 126, 1.4091017448575462, 1.410952032263936, 0.7599722282916077, 0.9185455804361237, 7.418672561645508, 18.531221389770508, -22273.59201772343, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0016000000000000003 32 0] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_108 (Batch  (None, 2022)        8088        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_108 (ReLU)               (None, 2022)         0           ['batch_normalization_108[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          254898      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          254898      ['re_lu_108[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2838460     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,914,174
Trainable params: 5,905,834
Non-trainable params: 8,340
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 09:54:17.682943: W tensorflow/c/c_api.cc:291] Operation '{name:'training_72/Adam/beta_1/Assign' id:47480 op device:{requested: '', assigned: ''} def:{{{node training_72/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_72/Adam/beta_1, training_72/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:54:36.580104: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_36/mul' id:46986 op device:{requested: '', assigned: ''} def:{{{node loss_36/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_36/mul/x, loss_36/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 25s - loss: 1.4678 - val_loss: 1.4935 - 25s/epoch - 264us/sample
Epoch 2/145
95501/95501 - 16s - loss: 1.4581 - val_loss: 1.4387 - 16s/epoch - 165us/sample
Epoch 3/145
95501/95501 - 16s - loss: 1.4558 - val_loss: 1.4461 - 16s/epoch - 165us/sample
Epoch 4/145
95501/95501 - 16s - loss: 1.4506 - val_loss: 1.4479 - 16s/epoch - 166us/sample
Epoch 5/145
95501/95501 - 16s - loss: 1.4361 - val_loss: 1.4543 - 16s/epoch - 166us/sample
Epoch 6/145
95501/95501 - 16s - loss: 1.4501 - val_loss: 1.4437 - 16s/epoch - 165us/sample
Epoch 7/145
95501/95501 - 16s - loss: 1.4321 - val_loss: 1.4290 - 16s/epoch - 166us/sample
Epoch 8/145
95501/95501 - 16s - loss: 1.4278 - val_loss: 1.4276 - 16s/epoch - 166us/sample
Epoch 9/145
95501/95501 - 16s - loss: 1.4371 - val_loss: 1.4388 - 16s/epoch - 167us/sample
Epoch 10/145
95501/95501 - 16s - loss: 1.4292 - val_loss: 1.4278 - 16s/epoch - 165us/sample
Epoch 11/145
95501/95501 - 16s - loss: 1.4279 - val_loss: 1.4288 - 16s/epoch - 165us/sample
Epoch 12/145
95501/95501 - 16s - loss: 1.4320 - val_loss: 1.4358 - 16s/epoch - 165us/sample
Epoch 13/145
95501/95501 - 16s - loss: 1.4287 - val_loss: 1.4258 - 16s/epoch - 166us/sample
Epoch 14/145
95501/95501 - 16s - loss: 1.4304 - val_loss: 1.4317 - 16s/epoch - 166us/sample
Epoch 15/145
95501/95501 - 16s - loss: 1.4285 - val_loss: 1.4284 - 16s/epoch - 166us/sample
Epoch 16/145
95501/95501 - 16s - loss: 1.4264 - val_loss: 1.4286 - 16s/epoch - 165us/sample
Epoch 17/145
95501/95501 - 16s - loss: 1.4269 - val_loss: 1.4248 - 16s/epoch - 166us/sample
Epoch 18/145
95501/95501 - 16s - loss: 1.4234 - val_loss: 1.4249 - 16s/epoch - 165us/sample
Epoch 19/145
95501/95501 - 16s - loss: 1.4229 - val_loss: 1.4226 - 16s/epoch - 166us/sample
Epoch 20/145
95501/95501 - 16s - loss: 1.4209 - val_loss: 1.4233 - 16s/epoch - 165us/sample
Epoch 21/145
95501/95501 - 16s - loss: 1.4207 - val_loss: 1.4220 - 16s/epoch - 166us/sample
Epoch 22/145
95501/95501 - 16s - loss: 1.4223 - val_loss: 1.4241 - 16s/epoch - 166us/sample
Epoch 23/145
95501/95501 - 16s - loss: 1.4216 - val_loss: 1.4267 - 16s/epoch - 165us/sample
Epoch 24/145
95501/95501 - 16s - loss: 1.4231 - val_loss: 1.4250 - 16s/epoch - 165us/sample
Epoch 25/145
95501/95501 - 16s - loss: 1.4223 - val_loss: 1.4236 - 16s/epoch - 167us/sample
Epoch 26/145
95501/95501 - 16s - loss: 1.4212 - val_loss: 1.4218 - 16s/epoch - 166us/sample
Epoch 27/145
95501/95501 - 16s - loss: 1.4213 - val_loss: 1.4225 - 16s/epoch - 166us/sample
Epoch 28/145
95501/95501 - 16s - loss: 1.4213 - val_loss: 1.4240 - 16s/epoch - 165us/sample
Epoch 29/145
95501/95501 - 16s - loss: 1.4214 - val_loss: 1.4205 - 16s/epoch - 166us/sample
Epoch 30/145
95501/95501 - 16s - loss: 1.4201 - val_loss: 1.4207 - 16s/epoch - 167us/sample
Epoch 31/145
95501/95501 - 16s - loss: 1.4199 - val_loss: 1.4232 - 16s/epoch - 166us/sample
Epoch 32/145
95501/95501 - 16s - loss: 1.4204 - val_loss: 1.4224 - 16s/epoch - 165us/sample
Epoch 33/145
95501/95501 - 16s - loss: 1.4223 - val_loss: 1.4232 - 16s/epoch - 166us/sample
Epoch 34/145
95501/95501 - 16s - loss: 1.4213 - val_loss: 1.4214 - 16s/epoch - 165us/sample
Epoch 35/145
95501/95501 - 16s - loss: 1.4199 - val_loss: 1.4205 - 16s/epoch - 165us/sample
Epoch 36/145
95501/95501 - 16s - loss: 1.4200 - val_loss: 1.4222 - 16s/epoch - 166us/sample
Epoch 37/145
95501/95501 - 16s - loss: 1.4215 - val_loss: 1.4214 - 16s/epoch - 165us/sample
Epoch 38/145
95501/95501 - 16s - loss: 1.4208 - val_loss: 1.4222 - 16s/epoch - 166us/sample
Epoch 39/145
95501/95501 - 16s - loss: 1.4197 - val_loss: 1.4204 - 16s/epoch - 165us/sample
Epoch 40/145
95501/95501 - 16s - loss: 1.4203 - val_loss: 1.4206 - 16s/epoch - 166us/sample
Epoch 41/145
95501/95501 - 16s - loss: 1.4196 - val_loss: 1.4208 - 16s/epoch - 166us/sample
Epoch 42/145
95501/95501 - 16s - loss: 1.4205 - val_loss: 1.4205 - 16s/epoch - 167us/sample
Epoch 43/145
95501/95501 - 16s - loss: 1.4204 - val_loss: 1.4232 - 16s/epoch - 165us/sample
Epoch 44/145
95501/95501 - 16s - loss: 1.4195 - val_loss: 1.4205 - 16s/epoch - 165us/sample
Epoch 45/145
95501/95501 - 16s - loss: 1.4203 - val_loss: 1.4200 - 16s/epoch - 165us/sample
Epoch 46/145
95501/95501 - 16s - loss: 1.4183 - val_loss: 1.4203 - 16s/epoch - 165us/sample
Epoch 47/145
95501/95501 - 16s - loss: 1.4186 - val_loss: 1.4192 - 16s/epoch - 166us/sample
Epoch 48/145
95501/95501 - 16s - loss: 1.4181 - val_loss: 1.4205 - 16s/epoch - 166us/sample
Epoch 49/145
95501/95501 - 16s - loss: 1.4183 - val_loss: 1.4199 - 16s/epoch - 165us/sample
Epoch 50/145
95501/95501 - 16s - loss: 1.4186 - val_loss: 1.4194 - 16s/epoch - 165us/sample
Epoch 51/145
95501/95501 - 16s - loss: 1.4172 - val_loss: 1.4195 - 16s/epoch - 166us/sample
Epoch 52/145
95501/95501 - 16s - loss: 1.4196 - val_loss: 1.4222 - 16s/epoch - 165us/sample
Epoch 53/145
95501/95501 - 16s - loss: 1.4177 - val_loss: 1.4571 - 16s/epoch - 166us/sample
Epoch 54/145
95501/95501 - 16s - loss: 1.4179 - val_loss: 1.4199 - 16s/epoch - 166us/sample
Epoch 55/145
95501/95501 - 16s - loss: 1.4179 - val_loss: 1.4195 - 16s/epoch - 165us/sample
Epoch 56/145
95501/95501 - 16s - loss: 1.4188 - val_loss: 1.4186 - 16s/epoch - 165us/sample
Epoch 57/145
95501/95501 - 16s - loss: 1.4174 - val_loss: 1.4192 - 16s/epoch - 165us/sample
Epoch 58/145
95501/95501 - 16s - loss: 1.4166 - val_loss: 1.4175 - 16s/epoch - 165us/sample
Epoch 59/145
95501/95501 - 16s - loss: 1.4165 - val_loss: 1.4185 - 16s/epoch - 166us/sample
Epoch 60/145
95501/95501 - 16s - loss: 1.4178 - val_loss: 1.4201 - 16s/epoch - 166us/sample
Epoch 61/145
95501/95501 - 16s - loss: 1.4188 - val_loss: 1.4185 - 16s/epoch - 166us/sample
Epoch 62/145
95501/95501 - 16s - loss: 1.4171 - val_loss: 1.4206 - 16s/epoch - 166us/sample
Epoch 63/145
95501/95501 - 16s - loss: 1.4185 - val_loss: 1.4235 - 16s/epoch - 166us/sample
Epoch 64/145
95501/95501 - 16s - loss: 1.4190 - val_loss: 1.4190 - 16s/epoch - 165us/sample
Epoch 65/145
95501/95501 - 16s - loss: 1.4170 - val_loss: 1.4187 - 16s/epoch - 166us/sample
Epoch 66/145
95501/95501 - 16s - loss: 1.4194 - val_loss: 1.4215 - 16s/epoch - 166us/sample
Epoch 67/145
95501/95501 - 16s - loss: 1.4192 - val_loss: 1.4201 - 16s/epoch - 165us/sample
Epoch 68/145
95501/95501 - 16s - loss: 1.4179 - val_loss: 1.4207 - 16s/epoch - 165us/sample
Epoch 69/145
95501/95501 - 16s - loss: 1.4169 - val_loss: 1.4176 - 16s/epoch - 165us/sample
Epoch 70/145
95501/95501 - 16s - loss: 1.4169 - val_loss: 1.4209 - 16s/epoch - 165us/sample
Epoch 71/145
95501/95501 - 16s - loss: 1.4171 - val_loss: 1.4181 - 16s/epoch - 166us/sample
Epoch 72/145
95501/95501 - 16s - loss: 1.4177 - val_loss: 1.4193 - 16s/epoch - 166us/sample
Epoch 73/145
95501/95501 - 16s - loss: 1.4193 - val_loss: 1.4275 - 16s/epoch - 165us/sample
Epoch 74/145
95501/95501 - 16s - loss: 1.4205 - val_loss: 1.4221 - 16s/epoch - 166us/sample
Epoch 75/145
95501/95501 - 16s - loss: 1.4197 - val_loss: 1.4199 - 16s/epoch - 165us/sample
Epoch 76/145
95501/95501 - 16s - loss: 1.4172 - val_loss: 1.4189 - 16s/epoch - 165us/sample
Epoch 77/145
95501/95501 - 16s - loss: 1.4187 - val_loss: 1.4204 - 16s/epoch - 166us/sample
Epoch 78/145
95501/95501 - 16s - loss: 1.4186 - val_loss: 1.4198 - 16s/epoch - 166us/sample
Epoch 79/145
95501/95501 - 16s - loss: 1.4176 - val_loss: 1.4187 - 16s/epoch - 165us/sample
Epoch 80/145
95501/95501 - 16s - loss: 1.4165 - val_loss: 1.4195 - 16s/epoch - 165us/sample
Epoch 81/145
95501/95501 - 16s - loss: 1.4172 - val_loss: 1.4188 - 16s/epoch - 166us/sample
Epoch 82/145
95501/95501 - 16s - loss: 1.4167 - val_loss: 1.4180 - 16s/epoch - 166us/sample
Epoch 83/145
95501/95501 - 16s - loss: 1.4165 - val_loss: 1.4178 - 16s/epoch - 166us/sample
Epoch 84/145
95501/95501 - 16s - loss: 1.4174 - val_loss: 1.4226 - 16s/epoch - 165us/sample
Epoch 85/145
95501/95501 - 16s - loss: 1.4170 - val_loss: 1.4180 - 16s/epoch - 166us/sample
Epoch 86/145
95501/95501 - 16s - loss: 1.4164 - val_loss: 1.4187 - 16s/epoch - 166us/sample
Epoch 87/145
95501/95501 - 16s - loss: 1.4163 - val_loss: 1.4172 - 16s/epoch - 166us/sample
Epoch 88/145
95501/95501 - 16s - loss: 1.4157 - val_loss: 1.4174 - 16s/epoch - 166us/sample
Epoch 89/145
95501/95501 - 16s - loss: 1.4170 - val_loss: 1.4190 - 16s/epoch - 166us/sample
Epoch 90/145
95501/95501 - 16s - loss: 1.4172 - val_loss: 1.4177 - 16s/epoch - 165us/sample
Epoch 91/145
95501/95501 - 16s - loss: 1.4169 - val_loss: 1.4186 - 16s/epoch - 165us/sample
Epoch 92/145
95501/95501 - 16s - loss: 1.4170 - val_loss: 1.4178 - 16s/epoch - 165us/sample
Epoch 93/145
95501/95501 - 16s - loss: 1.4161 - val_loss: 1.4177 - 16s/epoch - 165us/sample
Epoch 94/145
95501/95501 - 16s - loss: 1.4162 - val_loss: 1.4182 - 16s/epoch - 166us/sample
Epoch 95/145
95501/95501 - 16s - loss: 1.4172 - val_loss: 1.4185 - 16s/epoch - 166us/sample
Epoch 96/145
95501/95501 - 16s - loss: 1.4162 - val_loss: 1.4177 - 16s/epoch - 165us/sample
Epoch 97/145
95501/95501 - 16s - loss: 1.4166 - val_loss: 1.4183 - 16s/epoch - 166us/sample
Epoch 98/145
95501/95501 - 16s - loss: 1.4161 - val_loss: 1.4168 - 16s/epoch - 165us/sample
Epoch 99/145
95501/95501 - 16s - loss: 1.4153 - val_loss: 1.4169 - 16s/epoch - 165us/sample
Epoch 100/145
95501/95501 - 16s - loss: 1.4159 - val_loss: 1.4179 - 16s/epoch - 166us/sample
Epoch 101/145
95501/95501 - 16s - loss: 1.4153 - val_loss: 1.4166 - 16s/epoch - 165us/sample
Epoch 102/145
95501/95501 - 16s - loss: 1.4167 - val_loss: 1.4187 - 16s/epoch - 165us/sample
Epoch 103/145
95501/95501 - 16s - loss: 1.4164 - val_loss: 1.4169 - 16s/epoch - 165us/sample
Epoch 104/145
95501/95501 - 16s - loss: 1.4153 - val_loss: 1.4286 - 16s/epoch - 166us/sample
Epoch 105/145
95501/95501 - 16s - loss: 1.4197 - val_loss: 1.4193 - 16s/epoch - 166us/sample
Epoch 106/145
95501/95501 - 16s - loss: 1.4174 - val_loss: 1.4177 - 16s/epoch - 166us/sample
Epoch 107/145
95501/95501 - 16s - loss: 1.4149 - val_loss: 1.4157 - 16s/epoch - 166us/sample
Epoch 108/145
95501/95501 - 16s - loss: 1.4157 - val_loss: 1.4186 - 16s/epoch - 166us/sample
Epoch 109/145
95501/95501 - 16s - loss: 1.4153 - val_loss: 1.4164 - 16s/epoch - 165us/sample
Epoch 110/145
95501/95501 - 16s - loss: 1.4148 - val_loss: 1.4160 - 16s/epoch - 166us/sample
Epoch 111/145
95501/95501 - 16s - loss: 1.4148 - val_loss: 1.4192 - 16s/epoch - 166us/sample
Epoch 112/145
95501/95501 - 16s - loss: 1.4167 - val_loss: 1.4166 - 16s/epoch - 166us/sample
Epoch 113/145
95501/95501 - 16s - loss: 1.4145 - val_loss: 1.4157 - 16s/epoch - 165us/sample
Epoch 114/145
95501/95501 - 16s - loss: 1.4146 - val_loss: 1.4161 - 16s/epoch - 165us/sample
Epoch 115/145
95501/95501 - 16s - loss: 1.4141 - val_loss: 1.4159 - 16s/epoch - 165us/sample
Epoch 116/145
95501/95501 - 16s - loss: 1.4150 - val_loss: 1.4170 - 16s/epoch - 165us/sample
Epoch 117/145
95501/95501 - 16s - loss: 1.4157 - val_loss: 1.4173 - 16s/epoch - 166us/sample
Epoch 118/145
95501/95501 - 16s - loss: 1.4154 - val_loss: 1.4161 - 16s/epoch - 165us/sample
Epoch 119/145
95501/95501 - 16s - loss: 1.4145 - val_loss: 1.4172 - 16s/epoch - 166us/sample
Epoch 120/145
95501/95501 - 16s - loss: 1.4149 - val_loss: 1.4167 - 16s/epoch - 166us/sample
Epoch 121/145
95501/95501 - 16s - loss: 1.4160 - val_loss: 1.4193 - 16s/epoch - 166us/sample
Epoch 122/145
95501/95501 - 16s - loss: 1.4157 - val_loss: 1.4168 - 16s/epoch - 166us/sample
Epoch 123/145
95501/95501 - 16s - loss: 1.4149 - val_loss: 1.4162 - 16s/epoch - 166us/sample
Epoch 124/145
95501/95501 - 16s - loss: 1.4147 - val_loss: 1.4165 - 16s/epoch - 166us/sample
Epoch 125/145
95501/95501 - 16s - loss: 1.4152 - val_loss: 1.4160 - 16s/epoch - 166us/sample
Epoch 126/145
95501/95501 - 16s - loss: 1.4153 - val_loss: 1.4172 - 16s/epoch - 165us/sample
Epoch 127/145
95501/95501 - 16s - loss: 1.4156 - val_loss: 1.4161 - 16s/epoch - 165us/sample
Epoch 128/145
95501/95501 - 16s - loss: 1.4154 - val_loss: 1.4168 - 16s/epoch - 166us/sample
Epoch 129/145
95501/95501 - 16s - loss: 1.4155 - val_loss: 1.4171 - 16s/epoch - 166us/sample
Epoch 130/145
95501/95501 - 16s - loss: 1.4152 - val_loss: 1.4165 - 16s/epoch - 166us/sample
Epoch 131/145
95501/95501 - 16s - loss: 1.4150 - val_loss: 1.4175 - 16s/epoch - 166us/sample
Epoch 132/145
95501/95501 - 16s - loss: 1.4156 - val_loss: 1.4183 - 16s/epoch - 165us/sample
Epoch 133/145
95501/95501 - 16s - loss: 1.4162 - val_loss: 1.4176 - 16s/epoch - 165us/sample
Epoch 134/145
95501/95501 - 16s - loss: 1.4152 - val_loss: 1.4164 - 16s/epoch - 166us/sample
Epoch 135/145
95501/95501 - 16s - loss: 1.4147 - val_loss: 1.4165 - 16s/epoch - 166us/sample
Epoch 136/145
95501/95501 - 16s - loss: 1.4144 - val_loss: 1.4159 - 16s/epoch - 165us/sample
Epoch 137/145
95501/95501 - 16s - loss: 1.4141 - val_loss: 1.4154 - 16s/epoch - 165us/sample
Epoch 138/145
95501/95501 - 16s - loss: 1.4139 - val_loss: 1.4160 - 16s/epoch - 165us/sample
Epoch 139/145
95501/95501 - 16s - loss: 1.4139 - val_loss: 1.4153 - 16s/epoch - 165us/sample
Epoch 140/145
95501/95501 - 16s - loss: 1.4137 - val_loss: 1.4167 - 16s/epoch - 166us/sample
Epoch 141/145
95501/95501 - 16s - loss: 1.4144 - val_loss: 1.4151 - 16s/epoch - 166us/sample
Epoch 142/145
95501/95501 - 16s - loss: 1.4135 - val_loss: 1.4150 - 16s/epoch - 166us/sample
Epoch 143/145
95501/95501 - 16s - loss: 1.4135 - val_loss: 1.4155 - 16s/epoch - 165us/sample
Epoch 144/145
95501/95501 - 16s - loss: 1.4142 - val_loss: 1.4153 - 16s/epoch - 166us/sample
Epoch 145/145
95501/95501 - 16s - loss: 1.4140 - val_loss: 1.4169 - 16s/epoch - 167us/sample
COMPRESSED VECTOR SIZE: 126
Loss in the autoencoder: 1.4169130437651805
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:32:37.715681: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_36/outputlayer/BiasAdd' id:46938 op device:{requested: '', assigned: ''} def:{{{node decoder_model_36/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_36/outputlayer/MatMul, decoder_model_36/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7626940182262654
cosine 0.8936657254975027
MAE: 9.2225
RMSE: 24.98579
r2: -40493.28221482185
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'binary_crossentropy', 32, 145, 0.0016000000000000003, 0.1, 126, 1.4140452273577213, 1.4169130437651805, 0.7626940182262654, 0.8936657254975027, 9.22249984741211, 24.985790252685547, -40493.28221482185, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 140 0.0016000000000000003 16 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_111 (Batch  (None, 2022)        8088        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_111 (ReLU)               (None, 2022)         0           ['batch_normalization_111[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 126)          254898      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 126)          254898      ['re_lu_111[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 126)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2838460     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 5,914,174
Trainable params: 5,905,834
Non-trainable params: 8,340
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-15 10:32:54.816207: W tensorflow/c/c_api.cc:291] Operation '{name:'training_74/Adam/batch_normalization_113/gamma/v/Assign' id:48997 op device:{requested: '', assigned: ''} def:{{{node training_74/Adam/batch_normalization_113/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_74/Adam/batch_normalization_113/gamma/v, training_74/Adam/batch_normalization_113/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:33:24.163016: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_37/mul' id:48317 op device:{requested: '', assigned: ''} def:{{{node loss_37/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_37/mul/x, loss_37/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 36s - loss: 0.0057 - val_loss: 0.0029 - 36s/epoch - 381us/sample
Epoch 2/140
95501/95501 - 27s - loss: 0.0028 - val_loss: 0.0021 - 27s/epoch - 283us/sample
Epoch 3/140
95501/95501 - 27s - loss: 0.0022 - val_loss: 0.0018 - 27s/epoch - 280us/sample
Epoch 4/140
95501/95501 - 27s - loss: 0.0020 - val_loss: 0.0017 - 27s/epoch - 281us/sample
Epoch 5/140
95501/95501 - 27s - loss: 0.0019 - val_loss: 0.0017 - 27s/epoch - 284us/sample
Epoch 6/140
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0016 - 27s/epoch - 281us/sample
Epoch 7/140
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0017 - 27s/epoch - 280us/sample
Epoch 8/140
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0015 - 27s/epoch - 283us/sample
Epoch 9/140
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 282us/sample
Epoch 10/140
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 281us/sample
Epoch 11/140
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 283us/sample
Epoch 12/140
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0014 - 27s/epoch - 282us/sample
Epoch 13/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0015 - 27s/epoch - 281us/sample
Epoch 14/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 15/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 283us/sample
Epoch 16/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 17/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 18/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 283us/sample
Epoch 19/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 20/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 21/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 283us/sample
Epoch 22/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 23/140
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 24/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 283us/sample
Epoch 25/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 26/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 27/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 28/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 29/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 30/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 31/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 282us/sample
Epoch 32/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 33/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 34/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 280us/sample
Epoch 35/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 36/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 37/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 38/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 282us/sample
Epoch 39/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 282us/sample
Epoch 40/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 41/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 42/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 43/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 44/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 45/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 46/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 281us/sample
Epoch 47/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 48/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 282us/sample
Epoch 49/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 282us/sample
Epoch 50/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 280us/sample
Epoch 51/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 52/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 53/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 54/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 55/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 56/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 57/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 58/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 284us/sample
Epoch 59/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 280us/sample
Epoch 60/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 61/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 62/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 63/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 64/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 65/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 66/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 67/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 68/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 282us/sample
Epoch 69/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 70/140
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 71/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 72/140
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 281us/sample
Epoch 73/140
slurmstepd: error: *** JOB 36005553 ON mb-cas001 CANCELLED AT 2023-02-15T11:05:30 ***
