start
Thu Feb 23 14:30:08 CET 2023
2023-02-23 14:30:09.679805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-23 14:30:09.777147: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-23 14:30:09.803979: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-23 14:30:39,617 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fb425a137c0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.5 30 0.0005 64 1]
 [2.0 90 0.002 256 1]
 [0.5 30 0.002 128 1]
 [0.5 150 0.0005 128 1]
 [2.5 120 0.002 256 2]
 [0.5 150 0.002 128 1]
 [2.0 90 0.0005 256 1]
 [2.5 120 0.0005 32 1]
 [0.5 180 0.001 256 2]
 [0.5 90 0.002 32 2]
 [0.5 180 0.001 256 2]
 [2.0 30 0.0005 16 1]
 [1.0 150 0.002 128 2]
 [1.5 180 0.002 64 2]
 [1.0 150 0.0005 32 2]]
[2.5 30 0.0005 64 1] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-23 14:30:46.238466: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-23 14:30:46.607689: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-23 14:30:46.607809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20637 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:0f:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
Epoch 1/30
2023-02-23 14:30:48.978901: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
1493/1493 - 7s - loss: 0.0300 - mse: 0.0215 - val_loss: 0.0256 - val_mse: 0.0207 - 7s/epoch - 5ms/step
Epoch 2/30
1493/1493 - 5s - loss: 0.0162 - mse: 0.0132 - val_loss: 0.0155 - val_mse: 0.0124 - 5s/epoch - 4ms/step
Epoch 3/30
1493/1493 - 5s - loss: 0.0137 - mse: 0.0112 - val_loss: 0.0126 - val_mse: 0.0103 - 5s/epoch - 4ms/step
Epoch 4/30
1493/1493 - 5s - loss: 0.0123 - mse: 0.0102 - val_loss: 0.0117 - val_mse: 0.0095 - 5s/epoch - 4ms/step
Epoch 5/30
1493/1493 - 6s - loss: 0.0119 - mse: 0.0099 - val_loss: 0.0116 - val_mse: 0.0094 - 6s/epoch - 4ms/step
Epoch 6/30
1493/1493 - 6s - loss: 0.0117 - mse: 0.0096 - val_loss: 0.0113 - val_mse: 0.0091 - 6s/epoch - 4ms/step
Epoch 7/30
1493/1493 - 6s - loss: 0.0115 - mse: 0.0094 - val_loss: 0.0112 - val_mse: 0.0089 - 6s/epoch - 4ms/step
Epoch 8/30
1493/1493 - 6s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0112 - val_mse: 0.0088 - 6s/epoch - 4ms/step
Epoch 9/30
1493/1493 - 6s - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0111 - val_mse: 0.0088 - 6s/epoch - 4ms/step
Epoch 10/30
1493/1493 - 6s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0087 - 6s/epoch - 4ms/step
Epoch 11/30
1493/1493 - 5s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0110 - val_mse: 0.0086 - 5s/epoch - 4ms/step
Epoch 12/30
1493/1493 - 6s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0109 - val_mse: 0.0085 - 6s/epoch - 4ms/step
Epoch 13/30
1493/1493 - 5s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0109 - val_mse: 0.0085 - 5s/epoch - 4ms/step
Epoch 14/30
1493/1493 - 5s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0108 - val_mse: 0.0084 - 5s/epoch - 4ms/step
Epoch 15/30
1493/1493 - 6s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0109 - val_mse: 0.0084 - 6s/epoch - 4ms/step
Epoch 16/30
1493/1493 - 6s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0108 - val_mse: 0.0084 - 6s/epoch - 4ms/step
Epoch 17/30
1493/1493 - 6s - loss: 0.0110 - mse: 0.0087 - val_loss: 0.0108 - val_mse: 0.0083 - 6s/epoch - 4ms/step
Epoch 18/30
1493/1493 - 6s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0108 - val_mse: 0.0083 - 6s/epoch - 4ms/step
Epoch 19/30
1493/1493 - 6s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0108 - val_mse: 0.0083 - 6s/epoch - 4ms/step
Epoch 20/30
1493/1493 - 6s - loss: 0.0110 - mse: 0.0085 - val_loss: 0.0107 - val_mse: 0.0082 - 6s/epoch - 4ms/step
Epoch 21/30
1493/1493 - 6s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0107 - val_mse: 0.0082 - 6s/epoch - 4ms/step
Epoch 22/30
1493/1493 - 6s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0107 - val_mse: 0.0082 - 6s/epoch - 4ms/step
Epoch 23/30
1493/1493 - 6s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0105 - val_mse: 0.0079 - 6s/epoch - 4ms/step
Epoch 24/30
1493/1493 - 6s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0104 - val_mse: 0.0078 - 6s/epoch - 4ms/step
Epoch 25/30
1493/1493 - 6s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0104 - val_mse: 0.0077 - 6s/epoch - 4ms/step
Epoch 26/30
1493/1493 - 6s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0104 - val_mse: 0.0077 - 6s/epoch - 4ms/step
Epoch 27/30
1493/1493 - 6s - loss: 0.0106 - mse: 0.0079 - val_loss: 0.0103 - val_mse: 0.0076 - 6s/epoch - 4ms/step
Epoch 28/30
1493/1493 - 6s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0103 - val_mse: 0.0076 - 6s/epoch - 4ms/step
Epoch 29/30
1493/1493 - 6s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0102 - val_mse: 0.0076 - 6s/epoch - 4ms/step
Epoch 30/30
1493/1493 - 6s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0102 - val_mse: 0.0075 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.007504351437091827
['2.5custom_VAE', 'mse', 64, 30, 0.0005, 0.4, 505, 0.007804989814758301, 0.007504351437091827, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.0 90 0.002 256 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4743486     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,505,808
Trainable params: 10,494,686
Non-trainable params: 11,122
__________________________________________________________________________________________________
Epoch 1/90
374/374 - 2s - loss: 0.0425 - mse: 0.0307 - val_loss: 0.0318 - val_mse: 0.0221 - 2s/epoch - 7ms/step
Epoch 2/90
374/374 - 2s - loss: 0.0188 - mse: 0.0148 - val_loss: 0.0187 - val_mse: 0.0163 - 2s/epoch - 4ms/step
Epoch 3/90
374/374 - 2s - loss: 0.0163 - mse: 0.0134 - val_loss: 0.0283 - val_mse: 0.0214 - 2s/epoch - 4ms/step
Epoch 4/90
374/374 - 2s - loss: 0.0154 - mse: 0.0128 - val_loss: 0.0249 - val_mse: 0.0209 - 2s/epoch - 4ms/step
Epoch 5/90
374/374 - 2s - loss: 0.0148 - mse: 0.0122 - val_loss: 0.0166 - val_mse: 0.0142 - 2s/epoch - 4ms/step
Epoch 6/90
374/374 - 2s - loss: 0.0143 - mse: 0.0118 - val_loss: 0.0166 - val_mse: 0.0136 - 2s/epoch - 4ms/step
Epoch 7/90
374/374 - 2s - loss: 0.0139 - mse: 0.0114 - val_loss: 0.0156 - val_mse: 0.0128 - 2s/epoch - 4ms/step
Epoch 8/90
374/374 - 2s - loss: 0.0136 - mse: 0.0111 - val_loss: 0.0162 - val_mse: 0.0129 - 2s/epoch - 4ms/step
Epoch 9/90
374/374 - 2s - loss: 0.0133 - mse: 0.0108 - val_loss: 0.0146 - val_mse: 0.0120 - 2s/epoch - 4ms/step
Epoch 10/90
374/374 - 2s - loss: 0.0130 - mse: 0.0105 - val_loss: 0.0137 - val_mse: 0.0111 - 2s/epoch - 4ms/step
Epoch 11/90
374/374 - 2s - loss: 0.0128 - mse: 0.0103 - val_loss: 0.0139 - val_mse: 0.0111 - 2s/epoch - 4ms/step
Epoch 12/90
374/374 - 2s - loss: 0.0127 - mse: 0.0101 - val_loss: 0.0129 - val_mse: 0.0102 - 2s/epoch - 4ms/step
Epoch 13/90
374/374 - 2s - loss: 0.0125 - mse: 0.0100 - val_loss: 0.0129 - val_mse: 0.0098 - 2s/epoch - 4ms/step
Epoch 14/90
374/374 - 2s - loss: 0.0124 - mse: 0.0099 - val_loss: 0.0128 - val_mse: 0.0099 - 2s/epoch - 4ms/step
Epoch 15/90
374/374 - 2s - loss: 0.0123 - mse: 0.0098 - val_loss: 0.0127 - val_mse: 0.0096 - 2s/epoch - 4ms/step
Epoch 16/90
374/374 - 2s - loss: 0.0123 - mse: 0.0097 - val_loss: 0.0122 - val_mse: 0.0094 - 2s/epoch - 4ms/step
Epoch 17/90
374/374 - 2s - loss: 0.0122 - mse: 0.0096 - val_loss: 0.0122 - val_mse: 0.0095 - 2s/epoch - 4ms/step
Epoch 18/90
374/374 - 2s - loss: 0.0122 - mse: 0.0096 - val_loss: 0.0148 - val_mse: 0.0111 - 2s/epoch - 4ms/step
Epoch 19/90
374/374 - 2s - loss: 0.0155 - mse: 0.0111 - val_loss: 0.0152 - val_mse: 0.0115 - 2s/epoch - 4ms/step
Epoch 20/90
374/374 - 2s - loss: 0.0243 - mse: 0.0118 - val_loss: 0.0164 - val_mse: 0.0111 - 2s/epoch - 4ms/step
Epoch 21/90
374/374 - 2s - loss: 0.0277 - mse: 0.0121 - val_loss: 0.0143 - val_mse: 0.0107 - 2s/epoch - 4ms/step
Epoch 22/90
374/374 - 2s - loss: 0.0137 - mse: 0.0105 - val_loss: 0.0130 - val_mse: 0.0102 - 2s/epoch - 4ms/step
Epoch 23/90
374/374 - 2s - loss: 0.0130 - mse: 0.0102 - val_loss: 0.0128 - val_mse: 0.0099 - 2s/epoch - 4ms/step
Epoch 24/90
374/374 - 2s - loss: 0.0129 - mse: 0.0100 - val_loss: 0.0140 - val_mse: 0.0101 - 2s/epoch - 4ms/step
Epoch 25/90
374/374 - 2s - loss: 0.0228 - mse: 0.0104 - val_loss: 0.0207 - val_mse: 0.0111 - 2s/epoch - 4ms/step
Epoch 26/90
374/374 - 2s - loss: 0.0450 - mse: 0.0109 - val_loss: 0.0151 - val_mse: 0.0100 - 2s/epoch - 4ms/step
Epoch 27/90
374/374 - 2s - loss: 0.0141 - mse: 0.0100 - val_loss: 0.0147 - val_mse: 0.0099 - 2s/epoch - 4ms/step
Epoch 28/90
374/374 - 2s - loss: 0.0164 - mse: 0.0100 - val_loss: 0.0190 - val_mse: 0.0106 - 2s/epoch - 4ms/step
Epoch 29/90
374/374 - 2s - loss: 0.0304 - mse: 0.0102 - val_loss: 0.0154 - val_mse: 0.0098 - 2s/epoch - 4ms/step
Epoch 30/90
374/374 - 2s - loss: 0.0159 - mse: 0.0099 - val_loss: 0.0142 - val_mse: 0.0097 - 2s/epoch - 4ms/step
Epoch 31/90
374/374 - 2s - loss: 0.0141 - mse: 0.0098 - val_loss: 0.0146 - val_mse: 0.0098 - 2s/epoch - 4ms/step
Epoch 32/90
374/374 - 2s - loss: 0.0200 - mse: 0.0100 - val_loss: 0.0138 - val_mse: 0.0096 - 2s/epoch - 4ms/step
Epoch 33/90
374/374 - 2s - loss: 0.0138 - mse: 0.0097 - val_loss: 0.0139 - val_mse: 0.0095 - 2s/epoch - 4ms/step
Epoch 34/90
374/374 - 2s - loss: 0.0144 - mse: 0.0097 - val_loss: 0.0131 - val_mse: 0.0095 - 2s/epoch - 4ms/step
Epoch 35/90
374/374 - 2s - loss: 0.0132 - mse: 0.0096 - val_loss: 0.0133 - val_mse: 0.0096 - 2s/epoch - 4ms/step
Epoch 36/90
374/374 - 2s - loss: 0.0135 - mse: 0.0096 - val_loss: 0.0127 - val_mse: 0.0094 - 2s/epoch - 4ms/step
Epoch 37/90
374/374 - 2s - loss: 0.0129 - mse: 0.0095 - val_loss: 0.0126 - val_mse: 0.0094 - 2s/epoch - 4ms/step
Epoch 38/90
374/374 - 2s - loss: 0.0130 - mse: 0.0095 - val_loss: 0.0244 - val_mse: 0.0105 - 2s/epoch - 4ms/step
Epoch 39/90
374/374 - 2s - loss: 0.0283 - mse: 0.0098 - val_loss: 0.0279 - val_mse: 0.0108 - 2s/epoch - 4ms/step
Epoch 40/90
374/374 - 2s - loss: 0.0676 - mse: 0.0098 - val_loss: 0.0151 - val_mse: 0.0094 - 2s/epoch - 4ms/step
Epoch 41/90
374/374 - 2s - loss: 0.0143 - mse: 0.0095 - val_loss: 0.0135 - val_mse: 0.0093 - 2s/epoch - 4ms/step
Epoch 42/90
374/374 - 2s - loss: 0.0135 - mse: 0.0094 - val_loss: 0.0131 - val_mse: 0.0092 - 2s/epoch - 4ms/step
Epoch 43/90
374/374 - 2s - loss: 0.0134 - mse: 0.0094 - val_loss: 0.0284 - val_mse: 0.0109 - 2s/epoch - 4ms/step
Epoch 44/90
374/374 - 2s - loss: 0.0347 - mse: 0.0096 - val_loss: 0.0145 - val_mse: 0.0094 - 2s/epoch - 4ms/step
Epoch 45/90
374/374 - 2s - loss: 0.0138 - mse: 0.0094 - val_loss: 0.0131 - val_mse: 0.0092 - 2s/epoch - 4ms/step
Epoch 46/90
374/374 - 2s - loss: 0.0132 - mse: 0.0093 - val_loss: 0.0127 - val_mse: 0.0092 - 2s/epoch - 4ms/step
Epoch 47/90
374/374 - 2s - loss: 0.0129 - mse: 0.0093 - val_loss: 0.0125 - val_mse: 0.0091 - 2s/epoch - 4ms/step
Epoch 48/90
374/374 - 2s - loss: 0.0126 - mse: 0.0092 - val_loss: 0.0123 - val_mse: 0.0091 - 2s/epoch - 4ms/step
Epoch 49/90
374/374 - 2s - loss: 0.0124 - mse: 0.0092 - val_loss: 0.0122 - val_mse: 0.0090 - 2s/epoch - 4ms/step
Epoch 50/90
374/374 - 2s - loss: 0.0123 - mse: 0.0091 - val_loss: 0.0120 - val_mse: 0.0090 - 2s/epoch - 4ms/step
Epoch 51/90
374/374 - 2s - loss: 0.0121 - mse: 0.0091 - val_loss: 0.0119 - val_mse: 0.0089 - 2s/epoch - 4ms/step
Epoch 52/90
374/374 - 2s - loss: 0.0121 - mse: 0.0090 - val_loss: 0.0118 - val_mse: 0.0089 - 2s/epoch - 4ms/step
Epoch 53/90
374/374 - 2s - loss: 0.0119 - mse: 0.0090 - val_loss: 0.0117 - val_mse: 0.0088 - 2s/epoch - 4ms/step
Epoch 54/90
374/374 - 2s - loss: 0.0119 - mse: 0.0089 - val_loss: 0.0210 - val_mse: 0.0097 - 2s/epoch - 4ms/step
Epoch 55/90
374/374 - 2s - loss: 0.0209 - mse: 0.0091 - val_loss: 0.0121 - val_mse: 0.0088 - 2s/epoch - 4ms/step
Epoch 56/90
374/374 - 2s - loss: 0.0120 - mse: 0.0089 - val_loss: 0.0118 - val_mse: 0.0088 - 2s/epoch - 4ms/step
Epoch 57/90
374/374 - 2s - loss: 0.0118 - mse: 0.0089 - val_loss: 0.0141 - val_mse: 0.0089 - 2s/epoch - 4ms/step
Epoch 58/90
374/374 - 2s - loss: 0.0127 - mse: 0.0089 - val_loss: 0.0115 - val_mse: 0.0087 - 2s/epoch - 4ms/step
Epoch 59/90
374/374 - 2s - loss: 0.0116 - mse: 0.0088 - val_loss: 0.0114 - val_mse: 0.0087 - 2s/epoch - 4ms/step
Epoch 60/90
374/374 - 2s - loss: 0.0115 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0086 - 2s/epoch - 4ms/step
Epoch 61/90
374/374 - 2s - loss: 0.0114 - mse: 0.0087 - val_loss: 0.0113 - val_mse: 0.0086 - 2s/epoch - 4ms/step
Epoch 62/90
374/374 - 2s - loss: 0.0116 - mse: 0.0087 - val_loss: 0.0246 - val_mse: 0.0100 - 2s/epoch - 4ms/step
Epoch 63/90
374/374 - 2s - loss: 0.0305 - mse: 0.0090 - val_loss: 0.0128 - val_mse: 0.0086 - 2s/epoch - 4ms/step
Epoch 64/90
374/374 - 2s - loss: 0.0123 - mse: 0.0087 - val_loss: 0.0122 - val_mse: 0.0086 - 2s/epoch - 4ms/step
Epoch 65/90
374/374 - 2s - loss: 0.0120 - mse: 0.0087 - val_loss: 0.0172 - val_mse: 0.0090 - 2s/epoch - 4ms/step
Epoch 66/90
374/374 - 2s - loss: 0.0265 - mse: 0.0089 - val_loss: 0.0125 - val_mse: 0.0086 - 2s/epoch - 4ms/step
Epoch 67/90
374/374 - 2s - loss: 0.0122 - mse: 0.0087 - val_loss: 0.0120 - val_mse: 0.0085 - 2s/epoch - 4ms/step
Epoch 68/90
374/374 - 2s - loss: 0.0119 - mse: 0.0086 - val_loss: 0.0120 - val_mse: 0.0085 - 2s/epoch - 4ms/step
Epoch 69/90
374/374 - 2s - loss: 0.0119 - mse: 0.0085 - val_loss: 0.0137 - val_mse: 0.0085 - 2s/epoch - 4ms/step
Epoch 70/90
374/374 - 2s - loss: 0.0155 - mse: 0.0086 - val_loss: 0.0198 - val_mse: 0.0085 - 2s/epoch - 4ms/step
Epoch 71/90
374/374 - 2s - loss: 0.0150 - mse: 0.0087 - val_loss: 0.0117 - val_mse: 0.0084 - 2s/epoch - 4ms/step
Epoch 72/90
374/374 - 2s - loss: 0.0119 - mse: 0.0085 - val_loss: 0.0229 - val_mse: 0.0093 - 2s/epoch - 4ms/step
Epoch 73/90
374/374 - 2s - loss: 0.0392 - mse: 0.0087 - val_loss: 0.0138 - val_mse: 0.0084 - 2s/epoch - 4ms/step
Epoch 74/90
374/374 - 2s - loss: 0.0126 - mse: 0.0085 - val_loss: 0.0119 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 75/90
374/374 - 2s - loss: 0.0120 - mse: 0.0084 - val_loss: 0.0116 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 76/90
374/374 - 2s - loss: 0.0119 - mse: 0.0084 - val_loss: 0.0238 - val_mse: 0.0096 - 2s/epoch - 4ms/step
Epoch 77/90
374/374 - 2s - loss: 0.0381 - mse: 0.0087 - val_loss: 0.0128 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 78/90
374/374 - 2s - loss: 0.0123 - mse: 0.0084 - val_loss: 0.0118 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 79/90
374/374 - 2s - loss: 0.0119 - mse: 0.0084 - val_loss: 0.0116 - val_mse: 0.0082 - 2s/epoch - 4ms/step
Epoch 80/90
374/374 - 2s - loss: 0.0117 - mse: 0.0083 - val_loss: 0.0114 - val_mse: 0.0082 - 2s/epoch - 4ms/step
Epoch 81/90
374/374 - 2s - loss: 0.0115 - mse: 0.0083 - val_loss: 0.0113 - val_mse: 0.0081 - 2s/epoch - 4ms/step
Epoch 82/90
374/374 - 2s - loss: 0.0114 - mse: 0.0083 - val_loss: 0.0112 - val_mse: 0.0081 - 2s/epoch - 4ms/step
Epoch 83/90
374/374 - 2s - loss: 0.0113 - mse: 0.0082 - val_loss: 0.0113 - val_mse: 0.0081 - 2s/epoch - 4ms/step
Epoch 84/90
374/374 - 2s - loss: 0.0112 - mse: 0.0082 - val_loss: 0.0110 - val_mse: 0.0080 - 2s/epoch - 4ms/step
Epoch 85/90
374/374 - 2s - loss: 0.0111 - mse: 0.0081 - val_loss: 0.0110 - val_mse: 0.0080 - 2s/epoch - 4ms/step
Epoch 86/90
374/374 - 2s - loss: 0.0111 - mse: 0.0081 - val_loss: 0.0110 - val_mse: 0.0080 - 2s/epoch - 4ms/step
Epoch 87/90
374/374 - 2s - loss: 0.0110 - mse: 0.0081 - val_loss: 0.0109 - val_mse: 0.0079 - 2s/epoch - 4ms/step
Epoch 88/90
374/374 - 2s - loss: 0.0110 - mse: 0.0081 - val_loss: 0.0110 - val_mse: 0.0080 - 2s/epoch - 4ms/step
Epoch 89/90
374/374 - 2s - loss: 0.0109 - mse: 0.0080 - val_loss: 0.0109 - val_mse: 0.0079 - 2s/epoch - 4ms/step
Epoch 90/90
374/374 - 2s - loss: 0.0108 - mse: 0.0080 - val_loss: 0.0112 - val_mse: 0.0079 - 2s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.007939239963889122
['2.0custom_VAE', 'mse', 256, 90, 0.002, 0.4, 505, 0.008007694035768509, 0.007939239963889122, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[0.5 30 0.002 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
Epoch 1/30
747/747 - 3s - loss: 0.0239 - mse: 0.0191 - val_loss: 0.0152 - val_mse: 0.0134 - 3s/epoch - 4ms/step
Epoch 2/30
747/747 - 2s - loss: 0.0143 - mse: 0.0125 - val_loss: 0.0161 - val_mse: 0.0137 - 2s/epoch - 3ms/step
Epoch 3/30
747/747 - 2s - loss: 0.0138 - mse: 0.0119 - val_loss: 0.0140 - val_mse: 0.0124 - 2s/epoch - 3ms/step
Epoch 4/30
747/747 - 2s - loss: 0.0134 - mse: 0.0115 - val_loss: 0.0136 - val_mse: 0.0117 - 2s/epoch - 3ms/step
Epoch 5/30
747/747 - 2s - loss: 0.0132 - mse: 0.0112 - val_loss: 0.0130 - val_mse: 0.0109 - 2s/epoch - 3ms/step
Epoch 6/30
747/747 - 2s - loss: 0.0129 - mse: 0.0109 - val_loss: 0.0129 - val_mse: 0.0106 - 2s/epoch - 3ms/step
Epoch 7/30
747/747 - 2s - loss: 0.0127 - mse: 0.0107 - val_loss: 0.0126 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 8/30
747/747 - 2s - loss: 0.0126 - mse: 0.0105 - val_loss: 0.0126 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 9/30
747/747 - 2s - loss: 0.0124 - mse: 0.0103 - val_loss: 0.0123 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 10/30
747/747 - 2s - loss: 0.0123 - mse: 0.0102 - val_loss: 0.0122 - val_mse: 0.0099 - 2s/epoch - 3ms/step
Epoch 11/30
747/747 - 2s - loss: 0.0123 - mse: 0.0100 - val_loss: 0.0121 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 12/30
747/747 - 2s - loss: 0.0121 - mse: 0.0098 - val_loss: 0.0119 - val_mse: 0.0095 - 2s/epoch - 3ms/step
Epoch 13/30
747/747 - 2s - loss: 0.0118 - mse: 0.0095 - val_loss: 0.0116 - val_mse: 0.0092 - 2s/epoch - 3ms/step
Epoch 14/30
747/747 - 2s - loss: 0.0116 - mse: 0.0093 - val_loss: 0.0113 - val_mse: 0.0091 - 2s/epoch - 3ms/step
Epoch 15/30
747/747 - 2s - loss: 0.0114 - mse: 0.0091 - val_loss: 0.0112 - val_mse: 0.0088 - 2s/epoch - 3ms/step
Epoch 16/30
747/747 - 2s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0111 - val_mse: 0.0088 - 2s/epoch - 3ms/step
Epoch 17/30
747/747 - 2s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0111 - val_mse: 0.0087 - 2s/epoch - 3ms/step
Epoch 18/30
747/747 - 2s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0111 - val_mse: 0.0086 - 2s/epoch - 3ms/step
Epoch 19/30
747/747 - 2s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0109 - val_mse: 0.0085 - 2s/epoch - 3ms/step
Epoch 20/30
747/747 - 2s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0108 - val_mse: 0.0084 - 2s/epoch - 3ms/step
Epoch 21/30
747/747 - 2s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0108 - val_mse: 0.0083 - 2s/epoch - 3ms/step
Epoch 22/30
747/747 - 2s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0108 - val_mse: 0.0082 - 2s/epoch - 3ms/step
Epoch 23/30
747/747 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0108 - val_mse: 0.0082 - 2s/epoch - 3ms/step
Epoch 24/30
747/747 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0107 - val_mse: 0.0082 - 2s/epoch - 3ms/step
Epoch 25/30
747/747 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0107 - val_mse: 0.0082 - 2s/epoch - 3ms/step
Epoch 26/30
747/747 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0081 - 2s/epoch - 3ms/step
Epoch 27/30
747/747 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0106 - val_mse: 0.0081 - 2s/epoch - 3ms/step
Epoch 28/30
747/747 - 2s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0106 - val_mse: 0.0080 - 2s/epoch - 3ms/step
Epoch 29/30
747/747 - 2s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0106 - val_mse: 0.0080 - 2s/epoch - 3ms/step
Epoch 30/30
747/747 - 2s - loss: 0.0107 - mse: 0.0081 - val_loss: 0.0105 - val_mse: 0.0078 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.007779597770422697
['0.5custom_VAE', 'mse', 128, 30, 0.002, 0.4, 505, 0.00811926368623972, 0.007779597770422697, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[0.5 150 0.0005 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
Epoch 1/150
747/747 - 3s - loss: 0.0241 - mse: 0.0193 - val_loss: 0.0164 - val_mse: 0.0140 - 3s/epoch - 4ms/step
Epoch 2/150
747/747 - 2s - loss: 0.0144 - mse: 0.0125 - val_loss: 0.0144 - val_mse: 0.0122 - 2s/epoch - 3ms/step
Epoch 3/150
747/747 - 2s - loss: 0.0138 - mse: 0.0120 - val_loss: 0.0141 - val_mse: 0.0120 - 2s/epoch - 3ms/step
Epoch 4/150
747/747 - 2s - loss: 0.0135 - mse: 0.0116 - val_loss: 0.0139 - val_mse: 0.0123 - 2s/epoch - 3ms/step
Epoch 5/150
747/747 - 2s - loss: 0.0132 - mse: 0.0112 - val_loss: 0.0130 - val_mse: 0.0112 - 2s/epoch - 3ms/step
Epoch 6/150
747/747 - 2s - loss: 0.0129 - mse: 0.0109 - val_loss: 0.0129 - val_mse: 0.0110 - 2s/epoch - 3ms/step
Epoch 7/150
747/747 - 2s - loss: 0.0127 - mse: 0.0107 - val_loss: 0.0127 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 8/150
747/747 - 2s - loss: 0.0126 - mse: 0.0105 - val_loss: 0.0124 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 9/150
747/747 - 2s - loss: 0.0124 - mse: 0.0103 - val_loss: 0.0123 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 10/150
747/747 - 2s - loss: 0.0123 - mse: 0.0101 - val_loss: 0.0121 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 11/150
747/747 - 2s - loss: 0.0120 - mse: 0.0098 - val_loss: 0.0117 - val_mse: 0.0094 - 2s/epoch - 3ms/step
Epoch 12/150
747/747 - 2s - loss: 0.0116 - mse: 0.0094 - val_loss: 0.0114 - val_mse: 0.0092 - 2s/epoch - 3ms/step
Epoch 13/150
747/747 - 2s - loss: 0.0115 - mse: 0.0093 - val_loss: 0.0113 - val_mse: 0.0090 - 2s/epoch - 3ms/step
Epoch 14/150
747/747 - 2s - loss: 0.0114 - mse: 0.0091 - val_loss: 0.0111 - val_mse: 0.0090 - 2s/epoch - 3ms/step
Epoch 15/150
747/747 - 2s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0111 - val_mse: 0.0088 - 2s/epoch - 3ms/step
Epoch 16/150
747/747 - 2s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0110 - val_mse: 0.0086 - 2s/epoch - 3ms/step
Epoch 17/150
747/747 - 2s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0110 - val_mse: 0.0086 - 2s/epoch - 3ms/step
Epoch 18/150
747/747 - 2s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0109 - val_mse: 0.0085 - 2s/epoch - 3ms/step
Epoch 19/150
747/747 - 2s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0107 - val_mse: 0.0083 - 2s/epoch - 3ms/step
Epoch 20/150
747/747 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0107 - val_mse: 0.0081 - 2s/epoch - 3ms/step
Epoch 21/150
747/747 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0106 - val_mse: 0.0080 - 2s/epoch - 3ms/step
Epoch 22/150
747/747 - 2s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0105 - val_mse: 0.0080 - 2s/epoch - 3ms/step
Epoch 23/150
747/747 - 2s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0104 - val_mse: 0.0079 - 2s/epoch - 3ms/step
Epoch 24/150
747/747 - 2s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0104 - val_mse: 0.0077 - 2s/epoch - 3ms/step
Epoch 25/150
747/747 - 2s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0103 - val_mse: 0.0076 - 2s/epoch - 3ms/step
Epoch 26/150
747/747 - 2s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0076 - 2s/epoch - 3ms/step
Epoch 27/150
747/747 - 2s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0102 - val_mse: 0.0075 - 2s/epoch - 3ms/step
Epoch 28/150
747/747 - 2s - loss: 0.0104 - mse: 0.0076 - val_loss: 0.0102 - val_mse: 0.0074 - 2s/epoch - 3ms/step
Epoch 29/150
747/747 - 2s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0101 - val_mse: 0.0073 - 2s/epoch - 3ms/step
Epoch 30/150
747/747 - 2s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0100 - val_mse: 0.0072 - 2s/epoch - 3ms/step
Epoch 31/150
747/747 - 2s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0100 - val_mse: 0.0072 - 2s/epoch - 3ms/step
Epoch 32/150
747/747 - 2s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0100 - val_mse: 0.0072 - 2s/epoch - 3ms/step
Epoch 33/150
747/747 - 2s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0100 - val_mse: 0.0071 - 2s/epoch - 3ms/step
Epoch 34/150
747/747 - 2s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0099 - val_mse: 0.0071 - 2s/epoch - 3ms/step
Epoch 35/150
747/747 - 2s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0099 - val_mse: 0.0071 - 2s/epoch - 3ms/step
Epoch 36/150
747/747 - 2s - loss: 0.0101 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 37/150
747/747 - 2s - loss: 0.0101 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 38/150
747/747 - 2s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 39/150
747/747 - 2s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 40/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 41/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 42/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 43/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 44/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 45/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 46/150
747/747 - 2s - loss: 0.0100 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 47/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 48/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 49/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 50/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 51/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 52/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 53/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 54/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 55/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 56/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 57/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 58/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 59/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 60/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 61/150
747/747 - 2s - loss: 0.0098 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 62/150
747/747 - 2s - loss: 0.0098 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 63/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 64/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 65/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 66/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 67/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 68/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 69/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 70/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 71/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 72/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 73/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 74/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 75/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 76/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 77/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 78/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 79/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 80/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 81/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 82/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 83/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 84/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 85/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 86/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 87/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 88/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 89/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 90/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 91/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 92/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 93/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 94/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 95/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 96/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 97/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 98/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 99/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 100/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 101/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 102/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 103/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 104/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 105/150
747/747 - 2s - loss: 0.0096 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 106/150
747/747 - 2s - loss: 0.0096 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 107/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 108/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 109/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 110/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 111/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 112/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 113/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 114/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 115/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 116/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 117/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 118/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 119/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 120/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 121/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 122/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 123/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 124/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 125/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 126/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 127/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 128/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 129/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 130/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 131/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 132/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 133/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 134/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 135/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 136/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 137/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 138/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 139/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 140/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 141/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 142/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 143/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 144/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0093 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 145/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 146/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 147/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 148/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 149/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 150/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.006178092211484909
['0.5custom_VAE', 'mse', 128, 150, 0.0005, 0.4, 505, 0.006310374476015568, 0.006178092211484909, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.5 120 0.002 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
Epoch 1/120
374/374 - 3s - loss: 0.0268 - mse: 0.0416 - val_loss: 0.0368 - val_mse: 0.0330 - 3s/epoch - 7ms/step
Epoch 2/120
374/374 - 2s - loss: 0.0098 - mse: 0.0165 - val_loss: 0.0813 - val_mse: 0.1192 - 2s/epoch - 5ms/step
Epoch 3/120
374/374 - 2s - loss: 0.0103 - mse: 0.0172 - val_loss: 0.0125 - val_mse: 0.0222 - 2s/epoch - 5ms/step
Epoch 4/120
374/374 - 2s - loss: 0.0083 - mse: 0.0149 - val_loss: 0.0167 - val_mse: 0.0307 - 2s/epoch - 5ms/step
Epoch 5/120
374/374 - 2s - loss: 0.0082 - mse: 0.0146 - val_loss: 0.0093 - val_mse: 0.0168 - 2s/epoch - 5ms/step
Epoch 6/120
374/374 - 2s - loss: 0.0079 - mse: 0.0141 - val_loss: 0.0116 - val_mse: 0.0168 - 2s/epoch - 5ms/step
Epoch 7/120
374/374 - 2s - loss: 0.0080 - mse: 0.0139 - val_loss: 0.0117 - val_mse: 0.0158 - 2s/epoch - 5ms/step
Epoch 8/120
374/374 - 2s - loss: 0.0075 - mse: 0.0131 - val_loss: 0.0084 - val_mse: 0.0142 - 2s/epoch - 5ms/step
Epoch 9/120
374/374 - 2s - loss: 0.0073 - mse: 0.0128 - val_loss: 0.0097 - val_mse: 0.0130 - 2s/epoch - 5ms/step
Epoch 10/120
374/374 - 2s - loss: 0.0071 - mse: 0.0125 - val_loss: 0.0082 - val_mse: 0.0135 - 2s/epoch - 5ms/step
Epoch 11/120
374/374 - 2s - loss: 0.0069 - mse: 0.0122 - val_loss: 0.0090 - val_mse: 0.0128 - 2s/epoch - 5ms/step
Epoch 12/120
374/374 - 2s - loss: 0.0067 - mse: 0.0117 - val_loss: 0.0702 - val_mse: 0.0762 - 2s/epoch - 5ms/step
Epoch 13/120
374/374 - 2s - loss: 0.0077 - mse: 0.0119 - val_loss: 0.0071 - val_mse: 0.0114 - 2s/epoch - 5ms/step
Epoch 14/120
374/374 - 2s - loss: 0.0066 - mse: 0.0115 - val_loss: 0.0065 - val_mse: 0.0113 - 2s/epoch - 5ms/step
Epoch 15/120
374/374 - 2s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0135 - val_mse: 0.0139 - 2s/epoch - 5ms/step
Epoch 16/120
374/374 - 2s - loss: 0.0066 - mse: 0.0113 - val_loss: 0.0071 - val_mse: 0.0114 - 2s/epoch - 5ms/step
Epoch 17/120
374/374 - 2s - loss: 0.0069 - mse: 0.0117 - val_loss: 0.0095 - val_mse: 0.0124 - 2s/epoch - 5ms/step
Epoch 18/120
374/374 - 2s - loss: 0.0096 - mse: 0.0119 - val_loss: 0.0076 - val_mse: 0.0117 - 2s/epoch - 5ms/step
Epoch 19/120
374/374 - 2s - loss: 0.0073 - mse: 0.0115 - val_loss: 0.0075 - val_mse: 0.0115 - 2s/epoch - 5ms/step
Epoch 20/120
374/374 - 2s - loss: 0.0093 - mse: 0.0117 - val_loss: 0.0066 - val_mse: 0.0112 - 2s/epoch - 5ms/step
Epoch 21/120
374/374 - 2s - loss: 0.0065 - mse: 0.0113 - val_loss: 0.0064 - val_mse: 0.0112 - 2s/epoch - 5ms/step
Epoch 22/120
374/374 - 2s - loss: 0.0064 - mse: 0.0112 - val_loss: 0.0064 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 23/120
374/374 - 2s - loss: 0.0065 - mse: 0.0112 - val_loss: 0.0080 - val_mse: 0.0115 - 2s/epoch - 5ms/step
Epoch 24/120
374/374 - 2s - loss: 0.0103 - mse: 0.0115 - val_loss: 0.0067 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 25/120
374/374 - 2s - loss: 0.0076 - mse: 0.0114 - val_loss: 0.0091 - val_mse: 0.0119 - 2s/epoch - 5ms/step
Epoch 26/120
374/374 - 2s - loss: 0.0166 - mse: 0.0119 - val_loss: 0.0078 - val_mse: 0.0113 - 2s/epoch - 5ms/step
Epoch 27/120
374/374 - 2s - loss: 0.0084 - mse: 0.0115 - val_loss: 0.0100 - val_mse: 0.0116 - 2s/epoch - 5ms/step
Epoch 28/120
374/374 - 2s - loss: 0.0229 - mse: 0.0117 - val_loss: 0.0081 - val_mse: 0.0113 - 2s/epoch - 5ms/step
Epoch 29/120
374/374 - 2s - loss: 0.0074 - mse: 0.0113 - val_loss: 0.0072 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 30/120
374/374 - 2s - loss: 0.0070 - mse: 0.0113 - val_loss: 0.0068 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 31/120
374/374 - 2s - loss: 0.0069 - mse: 0.0112 - val_loss: 0.0080 - val_mse: 0.0112 - 2s/epoch - 5ms/step
Epoch 32/120
374/374 - 2s - loss: 0.0088 - mse: 0.0113 - val_loss: 0.0070 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 33/120
374/374 - 2s - loss: 0.0069 - mse: 0.0112 - val_loss: 0.0070 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 34/120
374/374 - 2s - loss: 0.0072 - mse: 0.0112 - val_loss: 0.0067 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 35/120
374/374 - 2s - loss: 0.0067 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 36/120
374/374 - 2s - loss: 0.0066 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 37/120
374/374 - 2s - loss: 0.0066 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 38/120
374/374 - 2s - loss: 0.0067 - mse: 0.0111 - val_loss: 0.0129 - val_mse: 0.0120 - 2s/epoch - 5ms/step
Epoch 39/120
374/374 - 2s - loss: 0.0165 - mse: 0.0113 - val_loss: 0.0088 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 40/120
374/374 - 2s - loss: 0.0070 - mse: 0.0111 - val_loss: 0.0068 - val_mse: 0.0109 - 2s/epoch - 5ms/step
Epoch 41/120
374/374 - 2s - loss: 0.0069 - mse: 0.0111 - val_loss: 0.0068 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 42/120
374/374 - 2s - loss: 0.0067 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0109 - 2s/epoch - 5ms/step
Epoch 43/120
374/374 - 2s - loss: 0.0066 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0109 - 2s/epoch - 5ms/step
Epoch 44/120
374/374 - 2s - loss: 0.0066 - mse: 0.0110 - val_loss: 0.0070 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 45/120
374/374 - 2s - loss: 0.0066 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0109 - 2s/epoch - 5ms/step
Epoch 46/120
374/374 - 2s - loss: 0.0065 - mse: 0.0110 - val_loss: 0.0064 - val_mse: 0.0109 - 2s/epoch - 5ms/step
Epoch 47/120
374/374 - 2s - loss: 0.0064 - mse: 0.0110 - val_loss: 0.0079 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 48/120
374/374 - 2s - loss: 0.0071 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0109 - 2s/epoch - 5ms/step
Epoch 49/120
374/374 - 2s - loss: 0.0064 - mse: 0.0110 - val_loss: 0.0079 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 50/120
374/374 - 2s - loss: 0.0073 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 51/120
374/374 - 2s - loss: 0.0064 - mse: 0.0110 - val_loss: 0.0085 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 52/120
374/374 - 2s - loss: 0.0076 - mse: 0.0110 - val_loss: 0.0105 - val_mse: 0.0114 - 2s/epoch - 5ms/step
Epoch 53/120
374/374 - 2s - loss: 0.0080 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 54/120
374/374 - 2s - loss: 0.0064 - mse: 0.0109 - val_loss: 0.0063 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 55/120
374/374 - 2s - loss: 0.0064 - mse: 0.0109 - val_loss: 0.0063 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 56/120
374/374 - 2s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0063 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 57/120
374/374 - 2s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0063 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 58/120
374/374 - 2s - loss: 0.0065 - mse: 0.0109 - val_loss: 0.0125 - val_mse: 0.0117 - 2s/epoch - 5ms/step
Epoch 59/120
374/374 - 2s - loss: 0.0169 - mse: 0.0112 - val_loss: 0.0073 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 60/120
374/374 - 2s - loss: 0.0073 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 61/120
374/374 - 2s - loss: 0.0066 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 62/120
374/374 - 2s - loss: 0.0065 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 63/120
374/374 - 2s - loss: 0.0065 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 64/120
374/374 - 2s - loss: 0.0064 - mse: 0.0109 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 65/120
374/374 - 2s - loss: 0.0064 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 66/120
374/374 - 2s - loss: 0.0064 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 67/120
374/374 - 2s - loss: 0.0063 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 68/120
374/374 - 2s - loss: 0.0063 - mse: 0.0108 - val_loss: 0.0077 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 69/120
374/374 - 2s - loss: 0.0071 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 70/120
374/374 - 2s - loss: 0.0064 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 71/120
374/374 - 2s - loss: 0.0063 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 72/120
374/374 - 2s - loss: 0.0063 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 73/120
374/374 - 2s - loss: 0.0063 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 74/120
374/374 - 2s - loss: 0.0063 - mse: 0.0108 - val_loss: 0.0062 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 75/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 76/120
374/374 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 77/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 78/120
374/374 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 79/120
374/374 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 80/120
374/374 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0065 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 81/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0116 - val_mse: 0.0110 - 2s/epoch - 5ms/step
Epoch 82/120
374/374 - 2s - loss: 0.0072 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 83/120
374/374 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 84/120
374/374 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0098 - val_mse: 0.0108 - 2s/epoch - 5ms/step
Epoch 85/120
374/374 - 2s - loss: 0.0078 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 86/120
374/374 - 2s - loss: 0.0065 - mse: 0.0108 - val_loss: 0.0110 - val_mse: 0.0113 - 2s/epoch - 5ms/step
Epoch 87/120
374/374 - 2s - loss: 0.0151 - mse: 0.0111 - val_loss: 0.0071 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 88/120
374/374 - 2s - loss: 0.0066 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 89/120
374/374 - 2s - loss: 0.0064 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 90/120
374/374 - 2s - loss: 0.0064 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 91/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 92/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 93/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 94/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0085 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 95/120
374/374 - 2s - loss: 0.0081 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 96/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 97/120
374/374 - 2s - loss: 0.0063 - mse: 0.0107 - val_loss: 0.0072 - val_mse: 0.0106 - 2s/epoch - 5ms/step
Epoch 98/120
374/374 - 2s - loss: 0.0070 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 99/120
374/374 - 2s - loss: 0.0063 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 100/120
374/374 - 2s - loss: 0.0063 - mse: 0.0106 - val_loss: 0.0087 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 101/120
374/374 - 2s - loss: 0.0084 - mse: 0.0107 - val_loss: 0.0063 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 102/120
374/374 - 2s - loss: 0.0063 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 103/120
374/374 - 2s - loss: 0.0063 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 104/120
374/374 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 105/120
374/374 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 106/120
374/374 - 2s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0104 - 2s/epoch - 5ms/step
Epoch 107/120
374/374 - 2s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 5ms/step
Epoch 108/120
374/374 - 2s - loss: 0.0062 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 5ms/step
Epoch 109/120
374/374 - 2s - loss: 0.0062 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 5ms/step
Epoch 110/120
374/374 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 5ms/step
Epoch 111/120
374/374 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 5ms/step
Epoch 112/120
374/374 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 5ms/step
Epoch 113/120
374/374 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 5ms/step
Epoch 114/120
374/374 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 5ms/step
Epoch 115/120
374/374 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0077 - val_mse: 0.0102 - 2s/epoch - 5ms/step
Epoch 116/120
374/374 - 2s - loss: 0.0062 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 5ms/step
Epoch 117/120
374/374 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0101 - 2s/epoch - 5ms/step
Epoch 118/120
374/374 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0062 - val_mse: 0.0101 - 2s/epoch - 5ms/step
Epoch 119/120
374/374 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 5ms/step
Epoch 120/120
374/374 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0061 - val_mse: 0.0101 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.010098285973072052
['2.5custom_VAE', 'logcosh', 256, 120, 0.002, 0.4, 505, 0.010218915529549122, 0.010098285973072052, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[0.5 150 0.002 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
Epoch 1/150
747/747 - 3s - loss: 0.0238 - mse: 0.0190 - val_loss: 0.0153 - val_mse: 0.0133 - 3s/epoch - 4ms/step
Epoch 2/150
747/747 - 2s - loss: 0.0144 - mse: 0.0125 - val_loss: 0.0148 - val_mse: 0.0126 - 2s/epoch - 3ms/step
Epoch 3/150
747/747 - 2s - loss: 0.0139 - mse: 0.0120 - val_loss: 0.0138 - val_mse: 0.0118 - 2s/epoch - 3ms/step
Epoch 4/150
747/747 - 2s - loss: 0.0134 - mse: 0.0115 - val_loss: 0.0137 - val_mse: 0.0118 - 2s/epoch - 3ms/step
Epoch 5/150
747/747 - 2s - loss: 0.0132 - mse: 0.0112 - val_loss: 0.0130 - val_mse: 0.0111 - 2s/epoch - 3ms/step
Epoch 6/150
747/747 - 2s - loss: 0.0129 - mse: 0.0109 - val_loss: 0.0129 - val_mse: 0.0108 - 2s/epoch - 3ms/step
Epoch 7/150
747/747 - 2s - loss: 0.0127 - mse: 0.0107 - val_loss: 0.0126 - val_mse: 0.0105 - 2s/epoch - 3ms/step
Epoch 8/150
747/747 - 2s - loss: 0.0126 - mse: 0.0105 - val_loss: 0.0124 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 9/150
747/747 - 2s - loss: 0.0124 - mse: 0.0103 - val_loss: 0.0123 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 10/150
747/747 - 2s - loss: 0.0123 - mse: 0.0102 - val_loss: 0.0126 - val_mse: 0.0099 - 2s/epoch - 3ms/step
Epoch 11/150
747/747 - 2s - loss: 0.0123 - mse: 0.0100 - val_loss: 0.0121 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 12/150
747/747 - 2s - loss: 0.0121 - mse: 0.0098 - val_loss: 0.0119 - val_mse: 0.0094 - 2s/epoch - 3ms/step
Epoch 13/150
747/747 - 2s - loss: 0.0119 - mse: 0.0096 - val_loss: 0.0116 - val_mse: 0.0093 - 2s/epoch - 3ms/step
Epoch 14/150
747/747 - 2s - loss: 0.0116 - mse: 0.0094 - val_loss: 0.0113 - val_mse: 0.0091 - 2s/epoch - 3ms/step
Epoch 15/150
747/747 - 2s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0112 - val_mse: 0.0089 - 2s/epoch - 3ms/step
Epoch 16/150
747/747 - 2s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0110 - val_mse: 0.0087 - 2s/epoch - 3ms/step
Epoch 17/150
747/747 - 2s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0111 - val_mse: 0.0088 - 2s/epoch - 3ms/step
Epoch 18/150
747/747 - 2s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0110 - val_mse: 0.0087 - 2s/epoch - 3ms/step
Epoch 19/150
747/747 - 2s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0109 - val_mse: 0.0085 - 2s/epoch - 3ms/step
Epoch 20/150
747/747 - 2s - loss: 0.0110 - mse: 0.0087 - val_loss: 0.0108 - val_mse: 0.0084 - 2s/epoch - 3ms/step
Epoch 21/150
747/747 - 2s - loss: 0.0110 - mse: 0.0085 - val_loss: 0.0108 - val_mse: 0.0083 - 2s/epoch - 3ms/step
Epoch 22/150
747/747 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0107 - val_mse: 0.0082 - 2s/epoch - 3ms/step
Epoch 23/150
747/747 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0080 - 2s/epoch - 3ms/step
Epoch 24/150
747/747 - 2s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0106 - val_mse: 0.0080 - 2s/epoch - 3ms/step
Epoch 25/150
747/747 - 2s - loss: 0.0107 - mse: 0.0081 - val_loss: 0.0105 - val_mse: 0.0079 - 2s/epoch - 3ms/step
Epoch 26/150
747/747 - 2s - loss: 0.0107 - mse: 0.0081 - val_loss: 0.0105 - val_mse: 0.0078 - 2s/epoch - 3ms/step
Epoch 27/150
747/747 - 2s - loss: 0.0106 - mse: 0.0079 - val_loss: 0.0103 - val_mse: 0.0077 - 2s/epoch - 3ms/step
Epoch 28/150
747/747 - 2s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0076 - 2s/epoch - 3ms/step
Epoch 29/150
747/747 - 2s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0102 - val_mse: 0.0074 - 2s/epoch - 3ms/step
Epoch 30/150
747/747 - 2s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0102 - val_mse: 0.0073 - 2s/epoch - 3ms/step
Epoch 31/150
747/747 - 2s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0101 - val_mse: 0.0074 - 2s/epoch - 3ms/step
Epoch 32/150
747/747 - 2s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0101 - val_mse: 0.0072 - 2s/epoch - 3ms/step
Epoch 33/150
747/747 - 2s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0100 - val_mse: 0.0072 - 2s/epoch - 3ms/step
Epoch 34/150
747/747 - 2s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0100 - val_mse: 0.0072 - 2s/epoch - 3ms/step
Epoch 35/150
747/747 - 2s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0100 - val_mse: 0.0071 - 2s/epoch - 3ms/step
Epoch 36/150
747/747 - 2s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0099 - val_mse: 0.0071 - 2s/epoch - 3ms/step
Epoch 37/150
747/747 - 2s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 38/150
747/747 - 2s - loss: 0.0101 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 39/150
747/747 - 2s - loss: 0.0101 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 40/150
747/747 - 2s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 41/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 42/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 43/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 44/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 45/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 46/150
747/747 - 2s - loss: 0.0100 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 47/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 48/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 49/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 50/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 51/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 52/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 53/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 54/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 55/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 56/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 57/150
747/747 - 2s - loss: 0.0098 - mse: 0.0069 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 58/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 59/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 60/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 61/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 62/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 63/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 64/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 65/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 66/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 67/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 68/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 69/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 70/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0096 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 71/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 72/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 73/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0096 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 74/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0096 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 75/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 76/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 77/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 78/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 79/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 80/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 81/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 82/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 83/150
747/747 - 2s - loss: 0.0097 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 84/150
747/747 - 2s - loss: 0.0097 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 85/150
747/747 - 2s - loss: 0.0097 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 86/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 87/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 88/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 89/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 90/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 91/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 92/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 93/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 94/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 95/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 96/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 97/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 98/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 99/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 100/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 101/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 102/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 103/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 104/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 105/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 106/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 107/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 108/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 109/150
747/747 - 2s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 110/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 111/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 112/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 113/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 114/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 115/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 116/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 117/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 118/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 119/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 120/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 121/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 122/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 123/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 124/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 125/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 126/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 127/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 128/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 129/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 130/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 131/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 132/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 133/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 134/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 135/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 136/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 137/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 138/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 139/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 140/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 141/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 142/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 143/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 144/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 145/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 146/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 147/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 148/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 149/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 150/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.006026931572705507
['0.5custom_VAE', 'mse', 128, 150, 0.002, 0.4, 505, 0.006205169949680567, 0.006026931572705507, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.0 90 0.0005 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4743486     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,505,808
Trainable params: 10,494,686
Non-trainable params: 11,122
__________________________________________________________________________________________________
Epoch 1/90
374/374 - 3s - loss: 0.0415 - mse: 0.0297 - val_loss: 0.0200 - val_mse: 0.0158 - 3s/epoch - 7ms/step
Epoch 2/90
374/374 - 2s - loss: 0.0173 - mse: 0.0139 - val_loss: 0.0218 - val_mse: 0.0180 - 2s/epoch - 4ms/step
Epoch 3/90
374/374 - 2s - loss: 0.0159 - mse: 0.0130 - val_loss: 0.0177 - val_mse: 0.0142 - 2s/epoch - 4ms/step
Epoch 4/90
374/374 - 2s - loss: 0.0153 - mse: 0.0125 - val_loss: 0.0482 - val_mse: 0.0344 - 2s/epoch - 4ms/step
Epoch 5/90
374/374 - 2s - loss: 0.0149 - mse: 0.0121 - val_loss: 0.0183 - val_mse: 0.0152 - 2s/epoch - 4ms/step
Epoch 6/90
374/374 - 2s - loss: 0.0140 - mse: 0.0115 - val_loss: 0.0183 - val_mse: 0.0145 - 2s/epoch - 4ms/step
Epoch 7/90
374/374 - 2s - loss: 0.0135 - mse: 0.0111 - val_loss: 0.0872 - val_mse: 0.0677 - 2s/epoch - 4ms/step
Epoch 8/90
374/374 - 2s - loss: 0.0181 - mse: 0.0135 - val_loss: 0.0226 - val_mse: 0.0135 - 2s/epoch - 4ms/step
Epoch 9/90
374/374 - 2s - loss: 0.0128 - mse: 0.0102 - val_loss: 0.0299 - val_mse: 0.0212 - 2s/epoch - 4ms/step
Epoch 10/90
374/374 - 2s - loss: 0.0129 - mse: 0.0102 - val_loss: 0.0253 - val_mse: 0.0166 - 2s/epoch - 4ms/step
Epoch 11/90
374/374 - 2s - loss: 0.0134 - mse: 0.0103 - val_loss: 0.0125 - val_mse: 0.0100 - 2s/epoch - 4ms/step
Epoch 12/90
374/374 - 2s - loss: 0.0120 - mse: 0.0097 - val_loss: 0.0125 - val_mse: 0.0099 - 2s/epoch - 4ms/step
Epoch 13/90
374/374 - 2s - loss: 0.0118 - mse: 0.0096 - val_loss: 0.0117 - val_mse: 0.0096 - 2s/epoch - 4ms/step
Epoch 14/90
374/374 - 2s - loss: 0.0117 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0095 - 2s/epoch - 4ms/step
Epoch 15/90
374/374 - 2s - loss: 0.0116 - mse: 0.0093 - val_loss: 0.0124 - val_mse: 0.0095 - 2s/epoch - 4ms/step
Epoch 16/90
374/374 - 2s - loss: 0.0116 - mse: 0.0092 - val_loss: 0.0114 - val_mse: 0.0089 - 2s/epoch - 4ms/step
Epoch 17/90
374/374 - 2s - loss: 0.0114 - mse: 0.0091 - val_loss: 0.0113 - val_mse: 0.0089 - 2s/epoch - 4ms/step
Epoch 18/90
374/374 - 2s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0112 - val_mse: 0.0088 - 2s/epoch - 4ms/step
Epoch 19/90
374/374 - 2s - loss: 0.0113 - mse: 0.0089 - val_loss: 0.0128 - val_mse: 0.0097 - 2s/epoch - 4ms/step
Epoch 20/90
374/374 - 2s - loss: 0.0121 - mse: 0.0094 - val_loss: 0.0112 - val_mse: 0.0089 - 2s/epoch - 4ms/step
Epoch 21/90
374/374 - 2s - loss: 0.0117 - mse: 0.0090 - val_loss: 0.0137 - val_mse: 0.0109 - 2s/epoch - 4ms/step
Epoch 22/90
374/374 - 2s - loss: 0.0140 - mse: 0.0102 - val_loss: 0.0121 - val_mse: 0.0096 - 2s/epoch - 4ms/step
Epoch 23/90
374/374 - 2s - loss: 0.0142 - mse: 0.0099 - val_loss: 0.0128 - val_mse: 0.0095 - 2s/epoch - 4ms/step
Epoch 24/90
374/374 - 2s - loss: 0.0119 - mse: 0.0093 - val_loss: 0.0114 - val_mse: 0.0091 - 2s/epoch - 4ms/step
Epoch 25/90
374/374 - 2s - loss: 0.0114 - mse: 0.0091 - val_loss: 0.0112 - val_mse: 0.0090 - 2s/epoch - 4ms/step
Epoch 26/90
374/374 - 2s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0113 - val_mse: 0.0088 - 2s/epoch - 4ms/step
Epoch 27/90
374/374 - 2s - loss: 0.0114 - mse: 0.0089 - val_loss: 0.0110 - val_mse: 0.0087 - 2s/epoch - 4ms/step
Epoch 28/90
374/374 - 2s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0109 - val_mse: 0.0086 - 2s/epoch - 4ms/step
Epoch 29/90
374/374 - 2s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0119 - val_mse: 0.0087 - 2s/epoch - 4ms/step
Epoch 30/90
374/374 - 2s - loss: 0.0125 - mse: 0.0088 - val_loss: 0.0111 - val_mse: 0.0085 - 2s/epoch - 4ms/step
Epoch 31/90
374/374 - 2s - loss: 0.0112 - mse: 0.0086 - val_loss: 0.0114 - val_mse: 0.0086 - 2s/epoch - 4ms/step
Epoch 32/90
374/374 - 2s - loss: 0.0118 - mse: 0.0086 - val_loss: 0.0114 - val_mse: 0.0084 - 2s/epoch - 4ms/step
Epoch 33/90
374/374 - 2s - loss: 0.0112 - mse: 0.0085 - val_loss: 0.0109 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 34/90
374/374 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0107 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 35/90
374/374 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0107 - val_mse: 0.0082 - 2s/epoch - 4ms/step
Epoch 36/90
374/374 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0106 - val_mse: 0.0081 - 2s/epoch - 4ms/step
Epoch 37/90
374/374 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0106 - val_mse: 0.0081 - 2s/epoch - 4ms/step
Epoch 38/90
374/374 - 2s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0106 - val_mse: 0.0081 - 2s/epoch - 4ms/step
Epoch 39/90
374/374 - 2s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0105 - val_mse: 0.0081 - 2s/epoch - 4ms/step
Epoch 40/90
374/374 - 2s - loss: 0.0107 - mse: 0.0081 - val_loss: 0.0105 - val_mse: 0.0080 - 2s/epoch - 4ms/step
Epoch 41/90
374/374 - 2s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0104 - val_mse: 0.0078 - 2s/epoch - 4ms/step
Epoch 42/90
374/374 - 2s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0104 - val_mse: 0.0077 - 2s/epoch - 4ms/step
Epoch 43/90
374/374 - 2s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0077 - 2s/epoch - 4ms/step
Epoch 44/90
374/374 - 2s - loss: 0.0108 - mse: 0.0077 - val_loss: 0.0123 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 45/90
374/374 - 2s - loss: 0.0140 - mse: 0.0085 - val_loss: 0.0128 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 46/90
374/374 - 2s - loss: 0.0131 - mse: 0.0083 - val_loss: 0.0110 - val_mse: 0.0079 - 2s/epoch - 4ms/step
Epoch 47/90
374/374 - 2s - loss: 0.0113 - mse: 0.0080 - val_loss: 0.0123 - val_mse: 0.0083 - 2s/epoch - 4ms/step
Epoch 48/90
374/374 - 2s - loss: 0.0160 - mse: 0.0082 - val_loss: 0.0118 - val_mse: 0.0079 - 2s/epoch - 4ms/step
Epoch 49/90
374/374 - 2s - loss: 0.0132 - mse: 0.0080 - val_loss: 0.0133 - val_mse: 0.0080 - 2s/epoch - 4ms/step
Epoch 50/90
374/374 - 2s - loss: 0.0208 - mse: 0.0084 - val_loss: 0.0122 - val_mse: 0.0079 - 2s/epoch - 4ms/step
Epoch 51/90
374/374 - 2s - loss: 0.0114 - mse: 0.0080 - val_loss: 0.0113 - val_mse: 0.0078 - 2s/epoch - 4ms/step
Epoch 52/90
374/374 - 2s - loss: 0.0111 - mse: 0.0079 - val_loss: 0.0112 - val_mse: 0.0078 - 2s/epoch - 4ms/step
Epoch 53/90
374/374 - 2s - loss: 0.0110 - mse: 0.0078 - val_loss: 0.0107 - val_mse: 0.0076 - 2s/epoch - 4ms/step
Epoch 54/90
374/374 - 2s - loss: 0.0109 - mse: 0.0078 - val_loss: 0.0107 - val_mse: 0.0076 - 2s/epoch - 4ms/step
Epoch 55/90
374/374 - 2s - loss: 0.0108 - mse: 0.0077 - val_loss: 0.0123 - val_mse: 0.0078 - 2s/epoch - 4ms/step
Epoch 56/90
374/374 - 2s - loss: 0.0114 - mse: 0.0077 - val_loss: 0.0106 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 57/90
374/374 - 2s - loss: 0.0107 - mse: 0.0077 - val_loss: 0.0111 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 58/90
374/374 - 2s - loss: 0.0115 - mse: 0.0077 - val_loss: 0.0106 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 59/90
374/374 - 2s - loss: 0.0107 - mse: 0.0076 - val_loss: 0.0105 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 60/90
374/374 - 2s - loss: 0.0106 - mse: 0.0076 - val_loss: 0.0105 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 61/90
374/374 - 2s - loss: 0.0106 - mse: 0.0076 - val_loss: 0.0141 - val_mse: 0.0079 - 2s/epoch - 4ms/step
Epoch 62/90
374/374 - 2s - loss: 0.0164 - mse: 0.0079 - val_loss: 0.0174 - val_mse: 0.0080 - 2s/epoch - 4ms/step
Epoch 63/90
374/374 - 2s - loss: 0.0316 - mse: 0.0081 - val_loss: 0.0121 - val_mse: 0.0077 - 2s/epoch - 4ms/step
Epoch 64/90
374/374 - 2s - loss: 0.0116 - mse: 0.0078 - val_loss: 0.0114 - val_mse: 0.0076 - 2s/epoch - 4ms/step
Epoch 65/90
374/374 - 2s - loss: 0.0116 - mse: 0.0077 - val_loss: 0.0112 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 66/90
374/374 - 2s - loss: 0.0111 - mse: 0.0076 - val_loss: 0.0108 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 67/90
374/374 - 2s - loss: 0.0110 - mse: 0.0076 - val_loss: 0.0108 - val_mse: 0.0075 - 2s/epoch - 4ms/step
Epoch 68/90
374/374 - 2s - loss: 0.0109 - mse: 0.0076 - val_loss: 0.0107 - val_mse: 0.0074 - 2s/epoch - 4ms/step
Epoch 69/90
374/374 - 2s - loss: 0.0108 - mse: 0.0075 - val_loss: 0.0106 - val_mse: 0.0074 - 2s/epoch - 4ms/step
Epoch 70/90
374/374 - 2s - loss: 0.0107 - mse: 0.0075 - val_loss: 0.0105 - val_mse: 0.0074 - 2s/epoch - 4ms/step
Epoch 71/90
374/374 - 2s - loss: 0.0107 - mse: 0.0075 - val_loss: 0.0105 - val_mse: 0.0074 - 2s/epoch - 4ms/step
Epoch 72/90
374/374 - 2s - loss: 0.0106 - mse: 0.0075 - val_loss: 0.0105 - val_mse: 0.0073 - 2s/epoch - 4ms/step
Epoch 73/90
374/374 - 2s - loss: 0.0106 - mse: 0.0074 - val_loss: 0.0104 - val_mse: 0.0073 - 2s/epoch - 4ms/step
Epoch 74/90
374/374 - 2s - loss: 0.0105 - mse: 0.0074 - val_loss: 0.0104 - val_mse: 0.0073 - 2s/epoch - 4ms/step
Epoch 75/90
374/374 - 2s - loss: 0.0105 - mse: 0.0074 - val_loss: 0.0103 - val_mse: 0.0072 - 2s/epoch - 4ms/step
Epoch 76/90
374/374 - 2s - loss: 0.0104 - mse: 0.0073 - val_loss: 0.0104 - val_mse: 0.0073 - 2s/epoch - 4ms/step
Epoch 77/90
374/374 - 2s - loss: 0.0104 - mse: 0.0073 - val_loss: 0.0103 - val_mse: 0.0072 - 2s/epoch - 4ms/step
Epoch 78/90
374/374 - 2s - loss: 0.0104 - mse: 0.0073 - val_loss: 0.0103 - val_mse: 0.0071 - 2s/epoch - 4ms/step
Epoch 79/90
374/374 - 2s - loss: 0.0103 - mse: 0.0073 - val_loss: 0.0106 - val_mse: 0.0072 - 2s/epoch - 4ms/step
Epoch 80/90
374/374 - 2s - loss: 0.0104 - mse: 0.0073 - val_loss: 0.0102 - val_mse: 0.0071 - 2s/epoch - 4ms/step
Epoch 81/90
374/374 - 2s - loss: 0.0103 - mse: 0.0072 - val_loss: 0.0106 - val_mse: 0.0072 - 2s/epoch - 4ms/step
Epoch 82/90
374/374 - 2s - loss: 0.0103 - mse: 0.0072 - val_loss: 0.0102 - val_mse: 0.0071 - 2s/epoch - 4ms/step
Epoch 83/90
374/374 - 2s - loss: 0.0102 - mse: 0.0072 - val_loss: 0.0119 - val_mse: 0.0073 - 2s/epoch - 4ms/step
Epoch 84/90
374/374 - 2s - loss: 0.0119 - mse: 0.0073 - val_loss: 0.0128 - val_mse: 0.0073 - 2s/epoch - 4ms/step
Epoch 85/90
374/374 - 2s - loss: 0.0110 - mse: 0.0073 - val_loss: 0.0102 - val_mse: 0.0071 - 2s/epoch - 4ms/step
Epoch 86/90
374/374 - 2s - loss: 0.0103 - mse: 0.0072 - val_loss: 0.0101 - val_mse: 0.0071 - 2s/epoch - 4ms/step
Epoch 87/90
374/374 - 2s - loss: 0.0102 - mse: 0.0072 - val_loss: 0.0100 - val_mse: 0.0070 - 2s/epoch - 4ms/step
Epoch 88/90
374/374 - 2s - loss: 0.0102 - mse: 0.0071 - val_loss: 0.0101 - val_mse: 0.0070 - 2s/epoch - 4ms/step
Epoch 89/90
374/374 - 2s - loss: 0.0101 - mse: 0.0071 - val_loss: 0.0101 - val_mse: 0.0070 - 2s/epoch - 4ms/step
Epoch 90/90
374/374 - 2s - loss: 0.0101 - mse: 0.0071 - val_loss: 0.0100 - val_mse: 0.0070 - 2s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.006972688715904951
['2.0custom_VAE', 'mse', 256, 90, 0.0005, 0.4, 505, 0.007103829178959131, 0.006972688715904951, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.5 120 0.0005 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
Epoch 1/120
2985/2985 - 11s - loss: 0.0276 - mse: 0.0197 - val_loss: 0.0149 - val_mse: 0.0117 - 11s/epoch - 4ms/step
Epoch 2/120
2985/2985 - 11s - loss: 0.0138 - mse: 0.0115 - val_loss: 0.0122 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 3/120
2985/2985 - 11s - loss: 0.0123 - mse: 0.0105 - val_loss: 0.0117 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 4/120
2985/2985 - 11s - loss: 0.0119 - mse: 0.0101 - val_loss: 0.0116 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 5/120
2985/2985 - 11s - loss: 0.0118 - mse: 0.0099 - val_loss: 0.0115 - val_mse: 0.0094 - 11s/epoch - 4ms/step
Epoch 6/120
2985/2985 - 11s - loss: 0.0116 - mse: 0.0097 - val_loss: 0.0114 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 7/120
2985/2985 - 11s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0117 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 8/120
2985/2985 - 11s - loss: 0.0115 - mse: 0.0095 - val_loss: 0.0114 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 9/120
2985/2985 - 11s - loss: 0.0115 - mse: 0.0094 - val_loss: 0.0116 - val_mse: 0.0094 - 11s/epoch - 4ms/step
Epoch 10/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0094 - val_loss: 0.0118 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 11/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0113 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 12/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0117 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 13/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0119 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 14/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0117 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 15/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0118 - val_mse: 0.0094 - 11s/epoch - 4ms/step
Epoch 16/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0115 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 17/120
2985/2985 - 11s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0118 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 18/120
2985/2985 - 11s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0121 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 19/120
2985/2985 - 11s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0122 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 20/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0113 - val_mse: 0.0087 - 11s/epoch - 4ms/step
Epoch 21/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0117 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 22/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0126 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 23/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0084 - val_loss: 0.0122 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 24/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0084 - val_loss: 0.0121 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 25/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0125 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 26/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0144 - val_mse: 0.0116 - 11s/epoch - 4ms/step
Epoch 27/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0128 - val_mse: 0.0101 - 11s/epoch - 4ms/step
Epoch 28/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0138 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 29/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0115 - val_mse: 0.0088 - 11s/epoch - 4ms/step
Epoch 30/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0129 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 31/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0113 - val_mse: 0.0087 - 11s/epoch - 4ms/step
Epoch 32/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0115 - val_mse: 0.0088 - 11s/epoch - 4ms/step
Epoch 33/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0079 - val_loss: 0.0111 - val_mse: 0.0084 - 11s/epoch - 4ms/step
Epoch 34/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0122 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 35/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0110 - val_mse: 0.0081 - 11s/epoch - 4ms/step
Epoch 36/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0117 - val_mse: 0.0089 - 11s/epoch - 4ms/step
Epoch 37/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0112 - val_mse: 0.0084 - 11s/epoch - 4ms/step
Epoch 38/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0118 - val_mse: 0.0090 - 11s/epoch - 4ms/step
Epoch 39/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0106 - val_mse: 0.0078 - 11s/epoch - 4ms/step
Epoch 40/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0123 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 41/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0121 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 42/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0114 - val_mse: 0.0086 - 11s/epoch - 4ms/step
Epoch 43/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0112 - val_mse: 0.0084 - 11s/epoch - 4ms/step
Epoch 44/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0121 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 45/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0114 - val_mse: 0.0086 - 11s/epoch - 4ms/step
Epoch 46/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0112 - val_mse: 0.0084 - 11s/epoch - 4ms/step
Epoch 47/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0123 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 48/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0111 - val_mse: 0.0083 - 11s/epoch - 4ms/step
Epoch 49/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0119 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 50/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0076 - val_loss: 0.0122 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 51/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0124 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 52/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0114 - val_mse: 0.0085 - 11s/epoch - 4ms/step
Epoch 53/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0111 - val_mse: 0.0083 - 11s/epoch - 4ms/step
Epoch 54/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0117 - val_mse: 0.0088 - 11s/epoch - 4ms/step
Epoch 55/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0121 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 56/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0135 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 57/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0144 - val_mse: 0.0115 - 11s/epoch - 4ms/step
Epoch 58/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0116 - val_mse: 0.0088 - 11s/epoch - 4ms/step
Epoch 59/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0112 - val_mse: 0.0084 - 11s/epoch - 4ms/step
Epoch 60/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0114 - val_mse: 0.0086 - 11s/epoch - 4ms/step
Epoch 61/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0110 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 62/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0111 - val_mse: 0.0083 - 11s/epoch - 4ms/step
Epoch 63/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0104 - val_mse: 0.0076 - 11s/epoch - 4ms/step
Epoch 64/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0123 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 65/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0109 - val_mse: 0.0081 - 11s/epoch - 4ms/step
Epoch 66/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0110 - val_mse: 0.0081 - 11s/epoch - 4ms/step
Epoch 67/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0114 - val_mse: 0.0085 - 11s/epoch - 4ms/step
Epoch 68/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0105 - val_mse: 0.0077 - 11s/epoch - 4ms/step
Epoch 69/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0125 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 70/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0104 - val_mse: 0.0076 - 11s/epoch - 4ms/step
Epoch 71/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0108 - val_mse: 0.0078 - 11s/epoch - 4ms/step
Epoch 72/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0113 - val_mse: 0.0085 - 11s/epoch - 4ms/step
Epoch 73/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0115 - val_mse: 0.0086 - 11s/epoch - 4ms/step
Epoch 74/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 75/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0116 - val_mse: 0.0087 - 11s/epoch - 4ms/step
Epoch 76/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0124 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 77/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0112 - val_mse: 0.0083 - 11s/epoch - 4ms/step
Epoch 78/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 79/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0113 - val_mse: 0.0083 - 11s/epoch - 4ms/step
Epoch 80/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0108 - val_mse: 0.0079 - 11s/epoch - 4ms/step
Epoch 81/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0111 - val_mse: 0.0083 - 11s/epoch - 4ms/step
Epoch 82/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 83/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0115 - val_mse: 0.0087 - 11s/epoch - 4ms/step
Epoch 84/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0119 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 85/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0114 - val_mse: 0.0085 - 11s/epoch - 4ms/step
Epoch 86/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 87/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0112 - val_mse: 0.0084 - 11s/epoch - 4ms/step
Epoch 88/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0118 - val_mse: 0.0089 - 11s/epoch - 4ms/step
Epoch 89/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0108 - val_mse: 0.0079 - 11s/epoch - 4ms/step
Epoch 90/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0109 - val_mse: 0.0079 - 11s/epoch - 4ms/step
Epoch 91/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0108 - val_mse: 0.0079 - 11s/epoch - 4ms/step
Epoch 92/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 93/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0124 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 94/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0116 - val_mse: 0.0088 - 11s/epoch - 4ms/step
Epoch 95/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0108 - val_mse: 0.0079 - 11s/epoch - 4ms/step
Epoch 96/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0118 - val_mse: 0.0089 - 11s/epoch - 4ms/step
Epoch 97/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0120 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 98/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0109 - val_mse: 0.0079 - 11s/epoch - 4ms/step
Epoch 99/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0106 - val_mse: 0.0077 - 11s/epoch - 4ms/step
Epoch 100/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0115 - val_mse: 0.0086 - 11s/epoch - 4ms/step
Epoch 101/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0105 - val_mse: 0.0076 - 11s/epoch - 4ms/step
Epoch 102/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 103/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0110 - val_mse: 0.0081 - 11s/epoch - 4ms/step
Epoch 104/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0113 - val_mse: 0.0084 - 11s/epoch - 4ms/step
Epoch 105/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0117 - val_mse: 0.0087 - 11s/epoch - 4ms/step
Epoch 106/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0105 - val_mse: 0.0077 - 11s/epoch - 4ms/step
Epoch 107/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0106 - val_mse: 0.0078 - 11s/epoch - 4ms/step
Epoch 108/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0105 - val_mse: 0.0076 - 11s/epoch - 4ms/step
Epoch 109/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0109 - val_mse: 0.0080 - 11s/epoch - 4ms/step
Epoch 110/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0109 - val_mse: 0.0080 - 11s/epoch - 4ms/step
Epoch 111/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0114 - val_mse: 0.0085 - 11s/epoch - 4ms/step
Epoch 112/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0106 - val_mse: 0.0078 - 11s/epoch - 4ms/step
Epoch 113/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0111 - val_mse: 0.0083 - 11s/epoch - 4ms/step
Epoch 114/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0109 - val_mse: 0.0080 - 11s/epoch - 4ms/step
Epoch 115/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 116/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0127 - val_mse: 0.0098 - 11s/epoch - 4ms/step
Epoch 117/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0111 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 118/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0110 - val_mse: 0.0080 - 11s/epoch - 4ms/step
Epoch 119/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0107 - val_mse: 0.0077 - 11s/epoch - 4ms/step
Epoch 120/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0106 - val_mse: 0.0077 - 11s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.007695656269788742
['2.5custom_VAE', 'mse', 32, 120, 0.0005, 0.4, 505, 0.007322793360799551, 0.007695656269788742, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[0.5 180 0.001 256 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE0.5_cr0.4_bs256_ep180_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
['0.5custom_VAE', 'logcosh', 256, 180, 0.001, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.001 256 2]) is not valid.
[0.5 90 0.002 32 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
Epoch 1/90
2985/2985 - 10s - loss: 0.0102 - mse: 0.0177 - val_loss: 0.0074 - val_mse: 0.0137 - 10s/epoch - 3ms/step
Epoch 2/90
2985/2985 - 9s - loss: 0.0071 - mse: 0.0131 - val_loss: 0.0068 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 3/90
2985/2985 - 9s - loss: 0.0068 - mse: 0.0127 - val_loss: 0.0067 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 4/90
2985/2985 - 9s - loss: 0.0068 - mse: 0.0124 - val_loss: 0.0067 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 5/90
2985/2985 - 9s - loss: 0.0068 - mse: 0.0124 - val_loss: 0.0067 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 6/90
2985/2985 - 9s - loss: 0.0067 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 7/90
2985/2985 - 9s - loss: 0.0068 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 8/90
2985/2985 - 9s - loss: 0.0067 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 9/90
2985/2985 - 9s - loss: 0.0067 - mse: 0.0122 - val_loss: 0.0067 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 10/90
2985/2985 - 9s - loss: 0.0065 - mse: 0.0118 - val_loss: 0.0065 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 11/90
2985/2985 - 9s - loss: 0.0064 - mse: 0.0115 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 12/90
2985/2985 - 9s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 13/90
2985/2985 - 9s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0065 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 14/90
2985/2985 - 9s - loss: 0.0064 - mse: 0.0113 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 15/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 16/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 17/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 18/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 19/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 20/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 21/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 22/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 23/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0067 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 24/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 25/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 26/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 27/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 28/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 29/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 30/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 31/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 32/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 33/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 34/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 35/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 36/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 37/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 38/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 39/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 40/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 41/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0127 - 9s/epoch - 3ms/step
Epoch 42/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0136 - 9s/epoch - 3ms/step
Epoch 43/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 44/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 45/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 46/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0135 - 9s/epoch - 3ms/step
Epoch 47/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 48/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0131 - 9s/epoch - 3ms/step
Epoch 49/90
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 50/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 51/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 52/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 53/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0127 - 9s/epoch - 3ms/step
Epoch 54/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 55/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 56/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 57/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 58/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 59/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 60/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 61/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 62/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 63/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 64/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0131 - 9s/epoch - 3ms/step
Epoch 65/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 66/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 67/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 68/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 69/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 70/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 71/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 72/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0110 - 9s/epoch - 3ms/step
Epoch 73/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0141 - 9s/epoch - 3ms/step
Epoch 74/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 75/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 76/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0134 - 9s/epoch - 3ms/step
Epoch 77/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 78/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 79/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 80/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 81/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 82/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0137 - 9s/epoch - 3ms/step
Epoch 83/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 84/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0127 - 9s/epoch - 3ms/step
Epoch 85/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0141 - 9s/epoch - 3ms/step
Epoch 86/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 87/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 88/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 89/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 90/90
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0127 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.012741061858832836
['0.5custom_VAE', 'logcosh', 32, 90, 0.002, 0.4, 505, 0.010790781117975712, 0.012741061858832836, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[0.5 180 0.001 256 2] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE0.5_cr0.4_bs256_ep180_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
['0.5custom_VAE', 'logcosh', 256, 180, 0.001, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 180 0.001 256 2]) is not valid.
[2.0 30 0.0005 16 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4743486     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,505,808
Trainable params: 10,494,686
Non-trainable params: 11,122
__________________________________________________________________________________________________
Epoch 1/30
5969/5969 - 21s - loss: 0.0220 - mse: 0.0169 - val_loss: 0.0130 - val_mse: 0.0113 - 21s/epoch - 3ms/step
Epoch 2/30
5969/5969 - 20s - loss: 0.0131 - mse: 0.0116 - val_loss: 0.0128 - val_mse: 0.0108 - 20s/epoch - 3ms/step
Epoch 3/30
5969/5969 - 20s - loss: 0.0129 - mse: 0.0112 - val_loss: 0.0133 - val_mse: 0.0112 - 20s/epoch - 3ms/step
Epoch 4/30
5969/5969 - 20s - loss: 0.0127 - mse: 0.0109 - val_loss: 0.0152 - val_mse: 0.0130 - 20s/epoch - 3ms/step
Epoch 5/30
5969/5969 - 20s - loss: 0.0122 - mse: 0.0104 - val_loss: 0.0154 - val_mse: 0.0133 - 20s/epoch - 3ms/step
Epoch 6/30
5969/5969 - 20s - loss: 0.0120 - mse: 0.0102 - val_loss: 0.0152 - val_mse: 0.0130 - 20s/epoch - 3ms/step
Epoch 7/30
5969/5969 - 20s - loss: 0.0119 - mse: 0.0101 - val_loss: 0.0201 - val_mse: 0.0179 - 20s/epoch - 3ms/step
Epoch 8/30
5969/5969 - 20s - loss: 0.0118 - mse: 0.0100 - val_loss: 0.0210 - val_mse: 0.0188 - 20s/epoch - 3ms/step
Epoch 9/30
5969/5969 - 20s - loss: 0.0118 - mse: 0.0099 - val_loss: 0.0282 - val_mse: 0.0260 - 20s/epoch - 3ms/step
Epoch 10/30
5969/5969 - 20s - loss: 0.0118 - mse: 0.0099 - val_loss: 0.0255 - val_mse: 0.0234 - 20s/epoch - 3ms/step
Epoch 11/30
5969/5969 - 20s - loss: 0.0118 - mse: 0.0098 - val_loss: 0.0302 - val_mse: 0.0278 - 20s/epoch - 3ms/step
Epoch 12/30
5969/5969 - 20s - loss: 0.0117 - mse: 0.0098 - val_loss: 0.0366 - val_mse: 0.0343 - 20s/epoch - 3ms/step
Epoch 13/30
5969/5969 - 20s - loss: 0.0117 - mse: 0.0097 - val_loss: 0.0283 - val_mse: 0.0259 - 20s/epoch - 3ms/step
Epoch 14/30
5969/5969 - 20s - loss: 0.0117 - mse: 0.0097 - val_loss: 0.0279 - val_mse: 0.0257 - 20s/epoch - 3ms/step
Epoch 15/30
5969/5969 - 20s - loss: 0.0117 - mse: 0.0097 - val_loss: 0.0315 - val_mse: 0.0291 - 20s/epoch - 3ms/step
Epoch 16/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0097 - val_loss: 0.0267 - val_mse: 0.0243 - 20s/epoch - 3ms/step
Epoch 17/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0374 - val_mse: 0.0351 - 20s/epoch - 3ms/step
Epoch 18/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0382 - val_mse: 0.0358 - 20s/epoch - 3ms/step
Epoch 19/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0377 - val_mse: 0.0353 - 20s/epoch - 3ms/step
Epoch 20/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0434 - val_mse: 0.0410 - 20s/epoch - 3ms/step
Epoch 21/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0403 - val_mse: 0.0379 - 20s/epoch - 3ms/step
Epoch 22/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0095 - val_loss: 0.0437 - val_mse: 0.0412 - 20s/epoch - 3ms/step
Epoch 23/30
5969/5969 - 20s - loss: 0.0116 - mse: 0.0095 - val_loss: 0.0408 - val_mse: 0.0384 - 20s/epoch - 3ms/step
Epoch 24/30
5969/5969 - 20s - loss: 0.0115 - mse: 0.0095 - val_loss: 0.0349 - val_mse: 0.0325 - 20s/epoch - 3ms/step
Epoch 25/30
5969/5969 - 20s - loss: 0.0115 - mse: 0.0094 - val_loss: 0.0387 - val_mse: 0.0362 - 20s/epoch - 3ms/step
Epoch 26/30
5969/5969 - 20s - loss: 0.0115 - mse: 0.0093 - val_loss: 0.0459 - val_mse: 0.0435 - 20s/epoch - 3ms/step
Epoch 27/30
5969/5969 - 20s - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0430 - val_mse: 0.0405 - 20s/epoch - 3ms/step
Epoch 28/30
5969/5969 - 20s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0661 - val_mse: 0.0636 - 20s/epoch - 3ms/step
Epoch 29/30
5969/5969 - 20s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0579 - val_mse: 0.0555 - 20s/epoch - 3ms/step
Epoch 30/30
5969/5969 - 20s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0613 - val_mse: 0.0588 - 20s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.05882181599736214
['2.0custom_VAE', 'mse', 16, 30, 0.0005, 0.4, 505, 0.009177236817777157, 0.05882181599736214, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[1.0 150 0.002 128 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
Epoch 1/150
747/747 - 3s - loss: 0.0158 - mse: 0.0254 - val_loss: 0.0085 - val_mse: 0.0157 - 3s/epoch - 5ms/step
Epoch 2/150
747/747 - 2s - loss: 0.0078 - mse: 0.0141 - val_loss: 0.0083 - val_mse: 0.0152 - 2s/epoch - 3ms/step
Epoch 3/150
747/747 - 2s - loss: 0.0076 - mse: 0.0138 - val_loss: 0.0077 - val_mse: 0.0138 - 2s/epoch - 3ms/step
Epoch 4/150
747/747 - 2s - loss: 0.0074 - mse: 0.0134 - val_loss: 0.0081 - val_mse: 0.0143 - 2s/epoch - 3ms/step
Epoch 5/150
747/747 - 2s - loss: 0.0072 - mse: 0.0131 - val_loss: 0.0075 - val_mse: 0.0127 - 2s/epoch - 3ms/step
Epoch 6/150
747/747 - 2s - loss: 0.0070 - mse: 0.0127 - val_loss: 0.0071 - val_mse: 0.0124 - 2s/epoch - 3ms/step
Epoch 7/150
747/747 - 2s - loss: 0.0069 - mse: 0.0125 - val_loss: 0.0070 - val_mse: 0.0127 - 2s/epoch - 3ms/step
Epoch 8/150
747/747 - 2s - loss: 0.0068 - mse: 0.0124 - val_loss: 0.0069 - val_mse: 0.0120 - 2s/epoch - 3ms/step
Epoch 9/150
747/747 - 2s - loss: 0.0068 - mse: 0.0123 - val_loss: 0.0068 - val_mse: 0.0124 - 2s/epoch - 3ms/step
Epoch 10/150
747/747 - 2s - loss: 0.0068 - mse: 0.0122 - val_loss: 0.0067 - val_mse: 0.0120 - 2s/epoch - 3ms/step
Epoch 11/150
747/747 - 2s - loss: 0.0067 - mse: 0.0122 - val_loss: 0.0067 - val_mse: 0.0118 - 2s/epoch - 3ms/step
Epoch 12/150
747/747 - 2s - loss: 0.0067 - mse: 0.0121 - val_loss: 0.0067 - val_mse: 0.0120 - 2s/epoch - 3ms/step
Epoch 13/150
747/747 - 2s - loss: 0.0067 - mse: 0.0121 - val_loss: 0.0067 - val_mse: 0.0118 - 2s/epoch - 3ms/step
Epoch 14/150
747/747 - 2s - loss: 0.0066 - mse: 0.0119 - val_loss: 0.0065 - val_mse: 0.0116 - 2s/epoch - 3ms/step
Epoch 15/150
747/747 - 2s - loss: 0.0064 - mse: 0.0115 - val_loss: 0.0063 - val_mse: 0.0112 - 2s/epoch - 3ms/step
Epoch 16/150
747/747 - 2s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0063 - val_mse: 0.0110 - 2s/epoch - 3ms/step
Epoch 17/150
747/747 - 2s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0062 - val_mse: 0.0109 - 2s/epoch - 3ms/step
Epoch 18/150
747/747 - 2s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0062 - val_mse: 0.0108 - 2s/epoch - 3ms/step
Epoch 19/150
747/747 - 2s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0062 - val_mse: 0.0108 - 2s/epoch - 3ms/step
Epoch 20/150
747/747 - 2s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0062 - val_mse: 0.0108 - 2s/epoch - 3ms/step
Epoch 21/150
747/747 - 2s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0062 - val_mse: 0.0107 - 2s/epoch - 3ms/step
Epoch 22/150
747/747 - 2s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0062 - val_mse: 0.0107 - 2s/epoch - 3ms/step
Epoch 23/150
747/747 - 2s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0061 - val_mse: 0.0106 - 2s/epoch - 3ms/step
Epoch 24/150
747/747 - 2s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0061 - val_mse: 0.0106 - 2s/epoch - 3ms/step
Epoch 25/150
747/747 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0106 - 2s/epoch - 3ms/step
Epoch 26/150
747/747 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0105 - 2s/epoch - 3ms/step
Epoch 27/150
747/747 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0105 - 2s/epoch - 3ms/step
Epoch 28/150
747/747 - 2s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0105 - 2s/epoch - 3ms/step
Epoch 29/150
747/747 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 30/150
747/747 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 2s/epoch - 3ms/step
Epoch 31/150
747/747 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 32/150
747/747 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 33/150
747/747 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 34/150
747/747 - 2s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 35/150
747/747 - 2s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 36/150
747/747 - 2s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 37/150
747/747 - 2s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 38/150
747/747 - 2s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 39/150
747/747 - 2s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 40/150
747/747 - 2s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 41/150
747/747 - 2s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 42/150
747/747 - 2s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 43/150
747/747 - 2s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 44/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 45/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 46/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 47/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 48/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 49/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 50/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 51/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 52/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 53/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 54/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 55/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 56/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 57/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 58/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 59/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 60/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 61/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 62/150
747/747 - 2s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 63/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 64/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0103 - 2s/epoch - 3ms/step
Epoch 65/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 66/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 67/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 68/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 69/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 70/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 71/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 72/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 73/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 74/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 75/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 76/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 77/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 78/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 79/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 80/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 81/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 82/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 83/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 84/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 85/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 86/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 87/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 88/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 89/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 90/150
747/747 - 2s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 91/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 92/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0061 - val_mse: 0.0102 - 2s/epoch - 3ms/step
Epoch 93/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 94/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 95/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 96/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 97/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 98/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0061 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 99/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 100/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 101/150
747/747 - 2s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0060 - val_mse: 0.0101 - 2s/epoch - 3ms/step
Epoch 102/150
747/747 - 2s - loss: 0.0061 - mse: 0.0101 - val_loss: 0.0060 - val_mse: 0.0100 - 2s/epoch - 3ms/step
Epoch 103/150
747/747 - 2s - loss: 0.0061 - mse: 0.0101 - val_loss: 0.0060 - val_mse: 0.0100 - 2s/epoch - 3ms/step
Epoch 104/150
747/747 - 2s - loss: 0.0060 - mse: 0.0100 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 105/150
747/747 - 2s - loss: 0.0060 - mse: 0.0100 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 106/150
747/747 - 2s - loss: 0.0060 - mse: 0.0099 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 107/150
747/747 - 2s - loss: 0.0060 - mse: 0.0099 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 108/150
747/747 - 2s - loss: 0.0060 - mse: 0.0099 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 109/150
747/747 - 2s - loss: 0.0060 - mse: 0.0099 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 110/150
747/747 - 2s - loss: 0.0060 - mse: 0.0099 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 111/150
747/747 - 2s - loss: 0.0060 - mse: 0.0099 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 112/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 113/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 114/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0099 - 2s/epoch - 3ms/step
Epoch 115/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 116/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 117/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 118/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 119/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 120/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 121/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 122/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 123/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 124/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 125/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 126/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0060 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 127/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 128/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 129/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 130/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 131/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 132/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 133/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 134/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 135/150
747/747 - 2s - loss: 0.0060 - mse: 0.0098 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 136/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 137/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 138/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 139/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 140/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0095 - 2s/epoch - 3ms/step
Epoch 141/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 142/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 143/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0095 - 2s/epoch - 3ms/step
Epoch 144/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 145/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0097 - 2s/epoch - 3ms/step
Epoch 146/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 147/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 148/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 149/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0095 - 2s/epoch - 3ms/step
Epoch 150/150
747/747 - 2s - loss: 0.0060 - mse: 0.0097 - val_loss: 0.0059 - val_mse: 0.0095 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.009537164121866226
['1.0custom_VAE', 'logcosh', 128, 150, 0.002, 0.4, 505, 0.009705876000225544, 0.009537164121866226, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[1.5 180 0.002 64 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          957985      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          957985      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3622318     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,944,312
Trainable params: 7,935,718
Non-trainable params: 8,594
__________________________________________________________________________________________________
Epoch 1/180
1493/1493 - 6s - loss: 0.0141 - mse: 0.0224 - val_loss: 0.0090 - val_mse: 0.0159 - 6s/epoch - 4ms/step
Epoch 2/180
1493/1493 - 5s - loss: 0.0081 - mse: 0.0145 - val_loss: 0.0077 - val_mse: 0.0140 - 5s/epoch - 3ms/step
Epoch 3/180
1493/1493 - 5s - loss: 0.0073 - mse: 0.0133 - val_loss: 0.0071 - val_mse: 0.0128 - 5s/epoch - 3ms/step
Epoch 4/180
1493/1493 - 5s - loss: 0.0070 - mse: 0.0127 - val_loss: 0.0068 - val_mse: 0.0122 - 5s/epoch - 3ms/step
Epoch 5/180
1493/1493 - 5s - loss: 0.0068 - mse: 0.0124 - val_loss: 0.0067 - val_mse: 0.0121 - 5s/epoch - 3ms/step
Epoch 6/180
1493/1493 - 5s - loss: 0.0068 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0118 - 5s/epoch - 3ms/step
Epoch 7/180
1493/1493 - 5s - loss: 0.0067 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0120 - 5s/epoch - 3ms/step
Epoch 8/180
1493/1493 - 5s - loss: 0.0067 - mse: 0.0121 - val_loss: 0.0065 - val_mse: 0.0117 - 5s/epoch - 3ms/step
Epoch 9/180
1493/1493 - 5s - loss: 0.0065 - mse: 0.0117 - val_loss: 0.0063 - val_mse: 0.0113 - 5s/epoch - 3ms/step
Epoch 10/180
1493/1493 - 5s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0063 - val_mse: 0.0111 - 5s/epoch - 3ms/step
Epoch 11/180
1493/1493 - 5s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0062 - val_mse: 0.0110 - 5s/epoch - 3ms/step
Epoch 12/180
1493/1493 - 5s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0062 - val_mse: 0.0109 - 5s/epoch - 3ms/step
Epoch 13/180
1493/1493 - 5s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0062 - val_mse: 0.0109 - 5s/epoch - 3ms/step
Epoch 14/180
1493/1493 - 5s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0062 - val_mse: 0.0109 - 5s/epoch - 3ms/step
Epoch 15/180
1493/1493 - 5s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0062 - val_mse: 0.0108 - 5s/epoch - 3ms/step
Epoch 16/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0062 - val_mse: 0.0108 - 5s/epoch - 3ms/step
Epoch 17/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 18/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0062 - val_mse: 0.0108 - 5s/epoch - 3ms/step
Epoch 19/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0062 - val_mse: 0.0108 - 5s/epoch - 3ms/step
Epoch 20/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 21/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 22/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0061 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 23/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0061 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 24/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 25/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 26/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 27/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 28/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 29/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 30/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 31/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 32/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 33/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 34/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 35/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 36/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 37/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 38/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 39/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 40/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 41/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 42/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 43/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 44/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 45/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 46/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 47/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 48/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 49/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 50/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 51/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 52/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 53/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 54/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 55/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 56/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 57/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 58/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 59/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 60/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 61/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 62/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 63/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 64/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 65/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 66/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 67/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 68/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 69/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 70/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 71/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 72/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 73/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 74/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 75/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 76/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 77/180
1493/1493 - 5s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 78/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 79/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 80/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 81/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 82/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 83/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0107 - 5s/epoch - 3ms/step
Epoch 84/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 85/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 86/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 87/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 88/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 89/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 90/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 91/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 92/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 93/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 94/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 95/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 96/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0062 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 97/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 98/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 99/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 100/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 101/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 102/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 103/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 104/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 105/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 106/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 107/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 108/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 109/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 110/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 111/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 112/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 113/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 114/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 115/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 116/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 117/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 118/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 119/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 120/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 121/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 122/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 123/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 124/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 125/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 126/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 127/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 128/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 129/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 130/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 131/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 5s/epoch - 3ms/step
Epoch 132/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 133/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 134/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 135/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 136/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 137/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 138/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 139/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 140/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 141/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 142/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 143/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 144/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 145/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 146/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 147/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 148/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 149/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 150/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 151/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 5s/epoch - 3ms/step
Epoch 152/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 153/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 5s/epoch - 3ms/step
Epoch 154/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 155/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 156/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 157/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 158/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 159/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 160/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 161/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0105 - 5s/epoch - 3ms/step
Epoch 162/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 163/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0106 - 5s/epoch - 3ms/step
Epoch 164/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 165/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 166/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0102 - 5s/epoch - 3ms/step
Epoch 167/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 168/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 169/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 170/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 5s/epoch - 3ms/step
Epoch 171/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 172/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 173/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 174/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 5s/epoch - 3ms/step
Epoch 175/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 176/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0102 - 5s/epoch - 3ms/step
Epoch 177/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0061 - val_mse: 0.0101 - 5s/epoch - 3ms/step
Epoch 178/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0061 - val_mse: 0.0103 - 5s/epoch - 3ms/step
Epoch 179/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0061 - val_mse: 0.0101 - 5s/epoch - 3ms/step
Epoch 180/180
1493/1493 - 5s - loss: 0.0061 - mse: 0.0102 - val_loss: 0.0061 - val_mse: 0.0101 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.010071863420307636
['1.5custom_VAE', 'logcosh', 64, 180, 0.002, 0.4, 505, 0.010152901522815228, 0.010071863420307636, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[1.0 150 0.0005 32 2] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
Epoch 1/150
2985/2985 - 10s - loss: 0.0116 - mse: 0.0192 - val_loss: 0.0077 - val_mse: 0.0145 - 10s/epoch - 3ms/step
Epoch 2/150
2985/2985 - 9s - loss: 0.0072 - mse: 0.0133 - val_loss: 0.0069 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 3/150
2985/2985 - 9s - loss: 0.0069 - mse: 0.0128 - val_loss: 0.0068 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 4/150
2985/2985 - 9s - loss: 0.0068 - mse: 0.0126 - val_loss: 0.0067 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 5/150
2985/2985 - 9s - loss: 0.0068 - mse: 0.0125 - val_loss: 0.0067 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 6/150
2985/2985 - 9s - loss: 0.0068 - mse: 0.0124 - val_loss: 0.0067 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 7/150
2985/2985 - 9s - loss: 0.0068 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 8/150
2985/2985 - 9s - loss: 0.0067 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 9/150
2985/2985 - 9s - loss: 0.0067 - mse: 0.0122 - val_loss: 0.0066 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 10/150
2985/2985 - 9s - loss: 0.0065 - mse: 0.0117 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 11/150
2985/2985 - 9s - loss: 0.0064 - mse: 0.0115 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 12/150
2985/2985 - 9s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 13/150
2985/2985 - 9s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 14/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 15/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0067 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 16/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0067 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 17/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 18/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 19/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 20/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0065 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 21/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 22/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 23/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 24/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 25/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 26/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0068 - val_mse: 0.0132 - 9s/epoch - 3ms/step
Epoch 27/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 28/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0067 - val_mse: 0.0132 - 9s/epoch - 3ms/step
Epoch 29/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 30/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 31/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 32/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0068 - val_mse: 0.0134 - 9s/epoch - 3ms/step
Epoch 33/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 34/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 35/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 36/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0071 - val_mse: 0.0154 - 9s/epoch - 3ms/step
Epoch 37/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 38/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 39/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 40/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 41/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 42/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 43/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0127 - 9s/epoch - 3ms/step
Epoch 44/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 45/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 46/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0069 - val_mse: 0.0144 - 9s/epoch - 3ms/step
Epoch 47/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 48/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0130 - 9s/epoch - 3ms/step
Epoch 49/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0130 - 9s/epoch - 3ms/step
Epoch 50/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 51/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0135 - 9s/epoch - 3ms/step
Epoch 52/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0131 - 9s/epoch - 3ms/step
Epoch 53/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 54/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 55/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 56/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0110 - val_loss: 0.0068 - val_mse: 0.0140 - 9s/epoch - 3ms/step
Epoch 57/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 58/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 59/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0139 - 9s/epoch - 3ms/step
Epoch 60/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0070 - val_mse: 0.0154 - 9s/epoch - 3ms/step
Epoch 61/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0069 - val_mse: 0.0150 - 9s/epoch - 3ms/step
Epoch 62/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 63/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 64/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0070 - val_mse: 0.0150 - 9s/epoch - 3ms/step
Epoch 65/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 66/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0142 - 9s/epoch - 3ms/step
Epoch 67/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0069 - val_mse: 0.0154 - 9s/epoch - 3ms/step
Epoch 68/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 69/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 70/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0137 - 9s/epoch - 3ms/step
Epoch 71/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0070 - val_mse: 0.0152 - 9s/epoch - 3ms/step
Epoch 72/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 73/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0071 - val_mse: 0.0168 - 9s/epoch - 3ms/step
Epoch 74/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0071 - val_mse: 0.0165 - 9s/epoch - 3ms/step
Epoch 75/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0144 - 9s/epoch - 3ms/step
Epoch 76/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 77/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0135 - 9s/epoch - 3ms/step
Epoch 78/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 79/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0137 - 9s/epoch - 3ms/step
Epoch 80/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0069 - val_mse: 0.0152 - 9s/epoch - 3ms/step
Epoch 81/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0149 - 9s/epoch - 3ms/step
Epoch 82/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0145 - 9s/epoch - 3ms/step
Epoch 83/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0070 - val_mse: 0.0158 - 9s/epoch - 3ms/step
Epoch 84/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 85/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 86/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 87/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 88/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0138 - 9s/epoch - 3ms/step
Epoch 89/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0130 - 9s/epoch - 3ms/step
Epoch 90/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 91/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0137 - 9s/epoch - 3ms/step
Epoch 92/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0134 - 9s/epoch - 3ms/step
Epoch 93/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0127 - 9s/epoch - 3ms/step
Epoch 94/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0138 - 9s/epoch - 3ms/step
Epoch 95/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0150 - 9s/epoch - 3ms/step
Epoch 96/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0130 - 9s/epoch - 3ms/step
Epoch 97/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0152 - 9s/epoch - 3ms/step
Epoch 98/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0063 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 99/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0135 - 9s/epoch - 3ms/step
Epoch 100/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 101/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 102/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0134 - 9s/epoch - 3ms/step
Epoch 103/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0071 - val_mse: 0.0166 - 9s/epoch - 3ms/step
Epoch 104/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 105/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 106/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0131 - 9s/epoch - 3ms/step
Epoch 107/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 108/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 109/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 110/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 111/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 112/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 113/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 114/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 115/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 116/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 117/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0154 - 9s/epoch - 3ms/step
Epoch 118/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 119/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0144 - 9s/epoch - 3ms/step
Epoch 120/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0139 - 9s/epoch - 3ms/step
Epoch 121/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0153 - 9s/epoch - 3ms/step
Epoch 122/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0142 - 9s/epoch - 3ms/step
Epoch 123/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 124/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 125/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 126/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0144 - 9s/epoch - 3ms/step
Epoch 127/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0134 - 9s/epoch - 3ms/step
Epoch 128/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 129/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 130/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0140 - 9s/epoch - 3ms/step
Epoch 131/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 132/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 133/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 134/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0144 - 9s/epoch - 3ms/step
Epoch 135/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0135 - 9s/epoch - 3ms/step
Epoch 136/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 137/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 138/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0139 - 9s/epoch - 3ms/step
Epoch 139/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 140/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0066 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 141/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0068 - val_mse: 0.0140 - 9s/epoch - 3ms/step
Epoch 142/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 143/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0069 - val_mse: 0.0151 - 9s/epoch - 3ms/step
Epoch 144/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0138 - 9s/epoch - 3ms/step
Epoch 145/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 146/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0135 - 9s/epoch - 3ms/step
Epoch 147/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 148/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 149/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 150/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.013348146341741085
['1.0custom_VAE', 'logcosh', 32, 150, 0.0005, 0.4, 505, 0.010745882987976074, 0.013348146341741085, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[1.0 150 0.00030000000000000003 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 2s - loss: 0.0348 - mse: 0.0257 - val_loss: 0.0255 - val_mse: 0.0181 - 2s/epoch - 6ms/step
Epoch 2/150
374/374 - 1s - loss: 0.0156 - mse: 0.0129 - val_loss: 0.0196 - val_mse: 0.0159 - 1s/epoch - 4ms/step
Epoch 3/150
374/374 - 1s - loss: 0.0147 - mse: 0.0125 - val_loss: 0.0177 - val_mse: 0.0154 - 1s/epoch - 4ms/step
Epoch 4/150
374/374 - 1s - loss: 0.0144 - mse: 0.0121 - val_loss: 0.0167 - val_mse: 0.0144 - 1s/epoch - 4ms/step
Epoch 5/150
374/374 - 1s - loss: 0.0140 - mse: 0.0118 - val_loss: 0.0149 - val_mse: 0.0126 - 1s/epoch - 4ms/step
Epoch 6/150
374/374 - 1s - loss: 0.0136 - mse: 0.0115 - val_loss: 0.0164 - val_mse: 0.0138 - 1s/epoch - 4ms/step
Epoch 7/150
374/374 - 1s - loss: 0.0135 - mse: 0.0113 - val_loss: 0.0144 - val_mse: 0.0121 - 1s/epoch - 4ms/step
Epoch 8/150
374/374 - 1s - loss: 0.0134 - mse: 0.0111 - val_loss: 0.0158 - val_mse: 0.0129 - 1s/epoch - 4ms/step
Epoch 9/150
374/374 - 1s - loss: 0.0132 - mse: 0.0109 - val_loss: 0.0145 - val_mse: 0.0112 - 1s/epoch - 4ms/step
Epoch 10/150
374/374 - 1s - loss: 0.0130 - mse: 0.0107 - val_loss: 0.0141 - val_mse: 0.0111 - 1s/epoch - 4ms/step
Epoch 11/150
374/374 - 1s - loss: 0.0128 - mse: 0.0105 - val_loss: 0.0132 - val_mse: 0.0106 - 1s/epoch - 4ms/step
Epoch 12/150
374/374 - 1s - loss: 0.0126 - mse: 0.0103 - val_loss: 0.0129 - val_mse: 0.0104 - 1s/epoch - 4ms/step
Epoch 13/150
374/374 - 1s - loss: 0.0125 - mse: 0.0102 - val_loss: 0.0128 - val_mse: 0.0101 - 1s/epoch - 4ms/step
Epoch 14/150
374/374 - 1s - loss: 0.0124 - mse: 0.0100 - val_loss: 0.0126 - val_mse: 0.0102 - 1s/epoch - 4ms/step
Epoch 15/150
374/374 - 1s - loss: 0.0124 - mse: 0.0099 - val_loss: 0.0133 - val_mse: 0.0098 - 1s/epoch - 4ms/step
Epoch 16/150
374/374 - 1s - loss: 0.0123 - mse: 0.0099 - val_loss: 0.0122 - val_mse: 0.0097 - 1s/epoch - 4ms/step
Epoch 17/150
374/374 - 1s - loss: 0.0122 - mse: 0.0097 - val_loss: 0.0121 - val_mse: 0.0097 - 1s/epoch - 4ms/step
Epoch 18/150
374/374 - 1s - loss: 0.0121 - mse: 0.0096 - val_loss: 0.0120 - val_mse: 0.0095 - 1s/epoch - 4ms/step
Epoch 19/150
374/374 - 1s - loss: 0.0121 - mse: 0.0095 - val_loss: 0.0120 - val_mse: 0.0093 - 1s/epoch - 4ms/step
Epoch 20/150
374/374 - 1s - loss: 0.0119 - mse: 0.0094 - val_loss: 0.0118 - val_mse: 0.0091 - 1s/epoch - 4ms/step
Epoch 21/150
374/374 - 1s - loss: 0.0118 - mse: 0.0092 - val_loss: 0.0117 - val_mse: 0.0091 - 1s/epoch - 4ms/step
Epoch 22/150
374/374 - 1s - loss: 0.0116 - mse: 0.0091 - val_loss: 0.0118 - val_mse: 0.0090 - 1s/epoch - 4ms/step
Epoch 23/150
374/374 - 1s - loss: 0.0116 - mse: 0.0091 - val_loss: 0.0118 - val_mse: 0.0090 - 1s/epoch - 4ms/step
Epoch 24/150
374/374 - 1s - loss: 0.0117 - mse: 0.0091 - val_loss: 0.0123 - val_mse: 0.0098 - 1s/epoch - 4ms/step
Epoch 25/150
374/374 - 1s - loss: 0.0122 - mse: 0.0095 - val_loss: 0.0114 - val_mse: 0.0088 - 1s/epoch - 4ms/step
Epoch 26/150
374/374 - 1s - loss: 0.0116 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0088 - 1s/epoch - 4ms/step
Epoch 27/150
374/374 - 1s - loss: 0.0119 - mse: 0.0089 - val_loss: 0.0116 - val_mse: 0.0089 - 1s/epoch - 4ms/step
Epoch 28/150
374/374 - 1s - loss: 0.0120 - mse: 0.0090 - val_loss: 0.0111 - val_mse: 0.0086 - 1s/epoch - 4ms/step
Epoch 29/150
374/374 - 1s - loss: 0.0112 - mse: 0.0087 - val_loss: 0.0113 - val_mse: 0.0086 - 1s/epoch - 4ms/step
Epoch 30/150
374/374 - 1s - loss: 0.0114 - mse: 0.0086 - val_loss: 0.0110 - val_mse: 0.0085 - 1s/epoch - 4ms/step
Epoch 31/150
374/374 - 1s - loss: 0.0112 - mse: 0.0085 - val_loss: 0.0109 - val_mse: 0.0084 - 1s/epoch - 4ms/step
Epoch 32/150
374/374 - 1s - loss: 0.0110 - mse: 0.0085 - val_loss: 0.0117 - val_mse: 0.0084 - 1s/epoch - 4ms/step
Epoch 33/150
374/374 - 1s - loss: 0.0122 - mse: 0.0085 - val_loss: 0.0112 - val_mse: 0.0084 - 1s/epoch - 4ms/step
Epoch 34/150
374/374 - 1s - loss: 0.0114 - mse: 0.0085 - val_loss: 0.0109 - val_mse: 0.0083 - 1s/epoch - 4ms/step
Epoch 35/150
374/374 - 1s - loss: 0.0111 - mse: 0.0084 - val_loss: 0.0108 - val_mse: 0.0083 - 1s/epoch - 4ms/step
Epoch 36/150
374/374 - 1s - loss: 0.0109 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0082 - 1s/epoch - 4ms/step
Epoch 37/150
374/374 - 1s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0081 - 1s/epoch - 4ms/step
Epoch 38/150
374/374 - 1s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0113 - val_mse: 0.0082 - 1s/epoch - 4ms/step
Epoch 39/150
374/374 - 1s - loss: 0.0110 - mse: 0.0082 - val_loss: 0.0107 - val_mse: 0.0081 - 1s/epoch - 4ms/step
Epoch 40/150
374/374 - 1s - loss: 0.0107 - mse: 0.0081 - val_loss: 0.0105 - val_mse: 0.0079 - 1s/epoch - 4ms/step
Epoch 41/150
374/374 - 1s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0105 - val_mse: 0.0079 - 1s/epoch - 4ms/step
Epoch 42/150
374/374 - 1s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0106 - val_mse: 0.0079 - 1s/epoch - 4ms/step
Epoch 43/150
374/374 - 1s - loss: 0.0108 - mse: 0.0080 - val_loss: 0.0104 - val_mse: 0.0078 - 1s/epoch - 4ms/step
Epoch 44/150
374/374 - 1s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0104 - val_mse: 0.0078 - 1s/epoch - 4ms/step
Epoch 45/150
374/374 - 1s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0104 - val_mse: 0.0077 - 1s/epoch - 4ms/step
Epoch 46/150
374/374 - 1s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0077 - 1s/epoch - 4ms/step
Epoch 47/150
374/374 - 1s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0077 - 1s/epoch - 4ms/step
Epoch 48/150
374/374 - 1s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0075 - 1s/epoch - 4ms/step
Epoch 49/150
374/374 - 1s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0106 - val_mse: 0.0077 - 1s/epoch - 4ms/step
Epoch 50/150
374/374 - 1s - loss: 0.0108 - mse: 0.0078 - val_loss: 0.0108 - val_mse: 0.0078 - 1s/epoch - 4ms/step
Epoch 51/150
374/374 - 1s - loss: 0.0111 - mse: 0.0079 - val_loss: 0.0105 - val_mse: 0.0077 - 1s/epoch - 4ms/step
Epoch 52/150
374/374 - 1s - loss: 0.0109 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0076 - 1s/epoch - 4ms/step
Epoch 53/150
374/374 - 1s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0103 - val_mse: 0.0076 - 1s/epoch - 4ms/step
Epoch 54/150
374/374 - 1s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0102 - val_mse: 0.0075 - 1s/epoch - 4ms/step
Epoch 55/150
374/374 - 1s - loss: 0.0104 - mse: 0.0076 - val_loss: 0.0102 - val_mse: 0.0074 - 1s/epoch - 4ms/step
Epoch 56/150
374/374 - 1s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0101 - val_mse: 0.0074 - 1s/epoch - 4ms/step
Epoch 57/150
374/374 - 1s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0102 - val_mse: 0.0074 - 1s/epoch - 4ms/step
Epoch 58/150
374/374 - 1s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0101 - val_mse: 0.0074 - 1s/epoch - 4ms/step
Epoch 59/150
374/374 - 1s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0101 - val_mse: 0.0073 - 1s/epoch - 4ms/step
Epoch 60/150
374/374 - 1s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0101 - val_mse: 0.0073 - 1s/epoch - 4ms/step
Epoch 61/150
374/374 - 1s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0100 - val_mse: 0.0072 - 1s/epoch - 4ms/step
Epoch 62/150
374/374 - 1s - loss: 0.0101 - mse: 0.0074 - val_loss: 0.0101 - val_mse: 0.0072 - 1s/epoch - 4ms/step
Epoch 63/150
374/374 - 1s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0101 - val_mse: 0.0072 - 1s/epoch - 4ms/step
Epoch 64/150
374/374 - 1s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0100 - val_mse: 0.0071 - 1s/epoch - 4ms/step
Epoch 65/150
374/374 - 1s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0099 - val_mse: 0.0071 - 1s/epoch - 4ms/step
Epoch 66/150
374/374 - 1s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0071 - 1s/epoch - 4ms/step
Epoch 67/150
374/374 - 1s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0071 - 1s/epoch - 4ms/step
Epoch 68/150
374/374 - 1s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 1s/epoch - 4ms/step
Epoch 69/150
374/374 - 1s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0071 - 1s/epoch - 4ms/step
Epoch 70/150
374/374 - 1s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0099 - val_mse: 0.0071 - 1s/epoch - 4ms/step
Epoch 71/150
374/374 - 1s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0099 - val_mse: 0.0071 - 1s/epoch - 4ms/step
Epoch 72/150
374/374 - 1s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0070 - 1s/epoch - 4ms/step
Epoch 73/150
374/374 - 1s - loss: 0.0099 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0070 - 1s/epoch - 4ms/step
Epoch 74/150
374/374 - 1s - loss: 0.0099 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 1s/epoch - 4ms/step
Epoch 75/150
374/374 - 1s - loss: 0.0099 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 1s/epoch - 4ms/step
Epoch 76/150
374/374 - 1s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0069 - 1s/epoch - 4ms/step
Epoch 77/150
374/374 - 1s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0069 - 1s/epoch - 4ms/step
Epoch 78/150
374/374 - 1s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0068 - 1s/epoch - 4ms/step
Epoch 79/150
374/374 - 1s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0069 - 1s/epoch - 4ms/step
Epoch 80/150
374/374 - 1s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0068 - 1s/epoch - 4ms/step
Epoch 81/150
374/374 - 1s - loss: 0.0098 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0068 - 1s/epoch - 4ms/step
Epoch 82/150
374/374 - 1s - loss: 0.0098 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0068 - 1s/epoch - 4ms/step
Epoch 83/150
374/374 - 1s - loss: 0.0098 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0068 - 1s/epoch - 4ms/step
Epoch 84/150
374/374 - 1s - loss: 0.0098 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 1s/epoch - 4ms/step
Epoch 85/150
374/374 - 1s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0067 - 1s/epoch - 4ms/step
Epoch 86/150
374/374 - 1s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0067 - 1s/epoch - 4ms/step
Epoch 87/150
374/374 - 1s - loss: 0.0097 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0067 - 1s/epoch - 4ms/step
Epoch 88/150
374/374 - 1s - loss: 0.0097 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 1s/epoch - 4ms/step
Epoch 89/150
374/374 - 1s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 1s/epoch - 4ms/step
Epoch 90/150
374/374 - 1s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 1s/epoch - 4ms/step
Epoch 91/150
374/374 - 1s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 1s/epoch - 4ms/step
Epoch 92/150
374/374 - 1s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 1s/epoch - 4ms/step
Epoch 93/150
374/374 - 1s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 94/150
374/374 - 1s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0066 - 1s/epoch - 4ms/step
Epoch 95/150
374/374 - 1s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 96/150
374/374 - 1s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 97/150
374/374 - 1s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 98/150
374/374 - 1s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 99/150
374/374 - 1s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 100/150
374/374 - 1s - loss: 0.0096 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 101/150
374/374 - 1s - loss: 0.0096 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 102/150
374/374 - 1s - loss: 0.0096 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 103/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 104/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 105/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 106/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0065 - 1s/epoch - 4ms/step
Epoch 107/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 108/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 109/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 110/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 111/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 112/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 113/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 114/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 1s/epoch - 4ms/step
Epoch 115/150
374/374 - 1s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 116/150
374/374 - 1s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 117/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 118/150
374/374 - 1s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 119/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 120/150
374/374 - 1s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 121/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 122/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 123/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 124/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 125/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 126/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 127/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 128/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 129/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 130/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 131/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 132/150
374/374 - 1s - loss: 0.0095 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 133/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 134/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 135/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 136/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0063 - 1s/epoch - 4ms/step
Epoch 137/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 138/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 139/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 140/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 141/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 142/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 143/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 144/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 145/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 146/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 1s/epoch - 4ms/step
Epoch 147/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 148/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 1s/epoch - 4ms/step
Epoch 149/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 1s/epoch - 4ms/step
Epoch 150/150
374/374 - 1s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.0061566480435431
['1.0custom_VAE', 'mse', 256, 150, 0.00030000000000000003, 0.4, 505, 0.006269149016588926, 0.0061566480435431, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.5 120 0.00030000000000000003 32 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
Epoch 1/120
2985/2985 - 12s - loss: 0.0267 - mse: 0.0195 - val_loss: 0.0152 - val_mse: 0.0118 - 12s/epoch - 4ms/step
Epoch 2/120
2985/2985 - 11s - loss: 0.0136 - mse: 0.0116 - val_loss: 0.0127 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 3/120
2985/2985 - 11s - loss: 0.0128 - mse: 0.0110 - val_loss: 0.0123 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 4/120
2985/2985 - 11s - loss: 0.0123 - mse: 0.0105 - val_loss: 0.0117 - val_mse: 0.0098 - 11s/epoch - 4ms/step
Epoch 5/120
2985/2985 - 11s - loss: 0.0119 - mse: 0.0101 - val_loss: 0.0115 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 6/120
2985/2985 - 11s - loss: 0.0117 - mse: 0.0098 - val_loss: 0.0117 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 7/120
2985/2985 - 11s - loss: 0.0117 - mse: 0.0097 - val_loss: 0.0118 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 8/120
2985/2985 - 11s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 9/120
2985/2985 - 11s - loss: 0.0115 - mse: 0.0095 - val_loss: 0.0120 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 10/120
2985/2985 - 11s - loss: 0.0115 - mse: 0.0094 - val_loss: 0.0119 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 11/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0122 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 12/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0124 - val_mse: 0.0101 - 11s/epoch - 4ms/step
Epoch 13/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0125 - val_mse: 0.0102 - 11s/epoch - 4ms/step
Epoch 14/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0128 - val_mse: 0.0104 - 11s/epoch - 4ms/step
Epoch 15/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0092 - val_loss: 0.0123 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 16/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0125 - val_mse: 0.0102 - 11s/epoch - 4ms/step
Epoch 17/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0129 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 18/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0125 - val_mse: 0.0102 - 11s/epoch - 4ms/step
Epoch 19/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0091 - val_loss: 0.0127 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 20/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0125 - val_mse: 0.0101 - 11s/epoch - 4ms/step
Epoch 21/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0134 - val_mse: 0.0110 - 11s/epoch - 4ms/step
Epoch 22/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0090 - val_loss: 0.0128 - val_mse: 0.0104 - 11s/epoch - 4ms/step
Epoch 23/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0128 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 24/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0142 - val_mse: 0.0116 - 11s/epoch - 4ms/step
Epoch 25/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0135 - val_mse: 0.0109 - 11s/epoch - 4ms/step
Epoch 26/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0157 - val_mse: 0.0132 - 11s/epoch - 4ms/step
Epoch 27/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0137 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 28/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0145 - val_mse: 0.0120 - 11s/epoch - 4ms/step
Epoch 29/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0139 - val_mse: 0.0113 - 11s/epoch - 4ms/step
Epoch 30/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0088 - val_loss: 0.0147 - val_mse: 0.0122 - 11s/epoch - 4ms/step
Epoch 31/120
2985/2985 - 11s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0131 - val_mse: 0.0105 - 11s/epoch - 4ms/step
Epoch 32/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0088 - val_loss: 0.0142 - val_mse: 0.0117 - 11s/epoch - 4ms/step
Epoch 33/120
2985/2985 - 11s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0123 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 34/120
2985/2985 - 11s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0133 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 35/120
2985/2985 - 11s - loss: 0.0111 - mse: 0.0087 - val_loss: 0.0125 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 36/120
2985/2985 - 11s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0137 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 37/120
2985/2985 - 11s - loss: 0.0110 - mse: 0.0086 - val_loss: 0.0161 - val_mse: 0.0135 - 11s/epoch - 4ms/step
Epoch 38/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0173 - val_mse: 0.0147 - 11s/epoch - 4ms/step
Epoch 39/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0137 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 40/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0125 - val_mse: 0.0098 - 11s/epoch - 4ms/step
Epoch 41/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0157 - val_mse: 0.0131 - 11s/epoch - 4ms/step
Epoch 42/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0166 - val_mse: 0.0138 - 11s/epoch - 4ms/step
Epoch 43/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0142 - val_mse: 0.0115 - 11s/epoch - 4ms/step
Epoch 44/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0139 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 45/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0081 - val_loss: 0.0138 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 46/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0081 - val_loss: 0.0160 - val_mse: 0.0134 - 11s/epoch - 4ms/step
Epoch 47/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0161 - val_mse: 0.0134 - 11s/epoch - 4ms/step
Epoch 48/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0149 - val_mse: 0.0122 - 11s/epoch - 4ms/step
Epoch 49/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0148 - val_mse: 0.0121 - 11s/epoch - 4ms/step
Epoch 50/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0151 - val_mse: 0.0123 - 11s/epoch - 4ms/step
Epoch 51/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0156 - val_mse: 0.0128 - 11s/epoch - 4ms/step
Epoch 52/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0186 - val_mse: 0.0158 - 11s/epoch - 4ms/step
Epoch 53/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0141 - val_mse: 0.0114 - 11s/epoch - 4ms/step
Epoch 54/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0146 - val_mse: 0.0118 - 11s/epoch - 4ms/step
Epoch 55/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0079 - val_loss: 0.0166 - val_mse: 0.0139 - 11s/epoch - 4ms/step
Epoch 56/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0079 - val_loss: 0.0167 - val_mse: 0.0140 - 11s/epoch - 4ms/step
Epoch 57/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0144 - val_mse: 0.0116 - 11s/epoch - 4ms/step
Epoch 58/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0135 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 59/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0163 - val_mse: 0.0136 - 11s/epoch - 4ms/step
Epoch 60/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0166 - val_mse: 0.0138 - 11s/epoch - 4ms/step
Epoch 61/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0172 - val_mse: 0.0145 - 11s/epoch - 4ms/step
Epoch 62/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0153 - val_mse: 0.0125 - 11s/epoch - 4ms/step
Epoch 63/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0135 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 64/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0192 - val_mse: 0.0164 - 11s/epoch - 4ms/step
Epoch 65/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0137 - val_mse: 0.0110 - 11s/epoch - 4ms/step
Epoch 66/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0155 - val_mse: 0.0127 - 11s/epoch - 4ms/step
Epoch 67/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0200 - val_mse: 0.0172 - 11s/epoch - 4ms/step
Epoch 68/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0139 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 69/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0160 - val_mse: 0.0132 - 11s/epoch - 4ms/step
Epoch 70/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0152 - val_mse: 0.0123 - 11s/epoch - 4ms/step
Epoch 71/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0159 - val_mse: 0.0131 - 11s/epoch - 4ms/step
Epoch 72/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0137 - val_mse: 0.0109 - 11s/epoch - 4ms/step
Epoch 73/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0318 - val_mse: 0.0290 - 11s/epoch - 4ms/step
Epoch 74/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0184 - val_mse: 0.0156 - 11s/epoch - 4ms/step
Epoch 75/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0159 - val_mse: 0.0131 - 11s/epoch - 4ms/step
Epoch 76/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0226 - val_mse: 0.0197 - 11s/epoch - 4ms/step
Epoch 77/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0198 - val_mse: 0.0170 - 11s/epoch - 4ms/step
Epoch 78/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0162 - val_mse: 0.0134 - 11s/epoch - 4ms/step
Epoch 79/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0157 - val_mse: 0.0128 - 11s/epoch - 4ms/step
Epoch 80/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0137 - val_mse: 0.0109 - 11s/epoch - 4ms/step
Epoch 81/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0150 - val_mse: 0.0121 - 11s/epoch - 4ms/step
Epoch 82/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0076 - val_loss: 0.0148 - val_mse: 0.0120 - 11s/epoch - 4ms/step
Epoch 83/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0076 - val_loss: 0.0173 - val_mse: 0.0145 - 11s/epoch - 4ms/step
Epoch 84/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0151 - val_mse: 0.0122 - 11s/epoch - 4ms/step
Epoch 85/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0157 - val_mse: 0.0128 - 11s/epoch - 4ms/step
Epoch 86/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0129 - val_mse: 0.0101 - 11s/epoch - 4ms/step
Epoch 87/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0136 - val_mse: 0.0108 - 11s/epoch - 4ms/step
Epoch 88/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0151 - val_mse: 0.0123 - 11s/epoch - 4ms/step
Epoch 89/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0135 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 90/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0142 - val_mse: 0.0113 - 11s/epoch - 4ms/step
Epoch 91/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0152 - val_mse: 0.0123 - 11s/epoch - 4ms/step
Epoch 92/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0156 - val_mse: 0.0127 - 11s/epoch - 4ms/step
Epoch 93/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0143 - val_mse: 0.0115 - 11s/epoch - 4ms/step
Epoch 94/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0184 - val_mse: 0.0155 - 11s/epoch - 4ms/step
Epoch 95/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0175 - val_mse: 0.0146 - 11s/epoch - 4ms/step
Epoch 96/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0176 - val_mse: 0.0147 - 11s/epoch - 4ms/step
Epoch 97/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0183 - val_mse: 0.0154 - 11s/epoch - 4ms/step
Epoch 98/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0141 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 99/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0139 - val_mse: 0.0109 - 11s/epoch - 4ms/step
Epoch 100/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0143 - val_mse: 0.0114 - 11s/epoch - 4ms/step
Epoch 101/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0176 - val_mse: 0.0146 - 11s/epoch - 4ms/step
Epoch 102/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0148 - val_mse: 0.0120 - 11s/epoch - 4ms/step
Epoch 103/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0153 - val_mse: 0.0125 - 11s/epoch - 4ms/step
Epoch 104/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0154 - val_mse: 0.0126 - 11s/epoch - 4ms/step
Epoch 105/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0156 - val_mse: 0.0127 - 11s/epoch - 4ms/step
Epoch 106/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0150 - val_mse: 0.0122 - 11s/epoch - 4ms/step
Epoch 107/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0147 - val_mse: 0.0119 - 11s/epoch - 4ms/step
Epoch 108/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0168 - val_mse: 0.0140 - 11s/epoch - 4ms/step
Epoch 109/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0185 - val_mse: 0.0156 - 11s/epoch - 4ms/step
Epoch 110/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0147 - val_mse: 0.0118 - 11s/epoch - 4ms/step
Epoch 111/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0157 - val_mse: 0.0128 - 11s/epoch - 4ms/step
Epoch 112/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0137 - val_mse: 0.0108 - 11s/epoch - 4ms/step
Epoch 113/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0149 - val_mse: 0.0120 - 11s/epoch - 4ms/step
Epoch 114/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0129 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 115/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0156 - val_mse: 0.0127 - 11s/epoch - 4ms/step
Epoch 116/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0137 - val_mse: 0.0108 - 11s/epoch - 4ms/step
Epoch 117/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0153 - val_mse: 0.0125 - 11s/epoch - 4ms/step
Epoch 118/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0112 - val_mse: 0.0082 - 11s/epoch - 4ms/step
Epoch 119/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0149 - val_mse: 0.0121 - 11s/epoch - 4ms/step
Epoch 120/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0131 - val_mse: 0.0102 - 11s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.010179209522902966
['2.5custom_VAE', 'mse', 32, 120, 0.00030000000000000003, 0.4, 505, 0.007397884503006935, 0.010179209522902966, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.5 120 0.00030000000000000003 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
Epoch 1/120
374/374 - 3s - loss: 0.0477 - mse: 0.0338 - val_loss: 0.0371 - val_mse: 0.0266 - 3s/epoch - 7ms/step
Epoch 2/120
374/374 - 2s - loss: 0.0178 - mse: 0.0136 - val_loss: 0.0610 - val_mse: 0.0306 - 2s/epoch - 5ms/step
Epoch 3/120
374/374 - 2s - loss: 0.0189 - mse: 0.0144 - val_loss: 0.0202 - val_mse: 0.0157 - 2s/epoch - 5ms/step
Epoch 4/120
374/374 - 2s - loss: 0.0148 - mse: 0.0120 - val_loss: 0.0207 - val_mse: 0.0171 - 2s/epoch - 5ms/step
Epoch 5/120
374/374 - 2s - loss: 0.0143 - mse: 0.0115 - val_loss: 0.0195 - val_mse: 0.0149 - 2s/epoch - 5ms/step
Epoch 6/120
374/374 - 2s - loss: 0.0140 - mse: 0.0112 - val_loss: 0.0170 - val_mse: 0.0135 - 2s/epoch - 5ms/step
Epoch 7/120
374/374 - 2s - loss: 0.0138 - mse: 0.0109 - val_loss: 0.0160 - val_mse: 0.0131 - 2s/epoch - 5ms/step
Epoch 8/120
374/374 - 2s - loss: 0.0134 - mse: 0.0106 - val_loss: 0.0183 - val_mse: 0.0142 - 2s/epoch - 5ms/step
Epoch 9/120
374/374 - 2s - loss: 0.0134 - mse: 0.0105 - val_loss: 0.0140 - val_mse: 0.0112 - 2s/epoch - 5ms/step
Epoch 10/120
374/374 - 2s - loss: 0.0128 - mse: 0.0102 - val_loss: 0.0138 - val_mse: 0.0105 - 2s/epoch - 5ms/step
Epoch 11/120
374/374 - 2s - loss: 0.0125 - mse: 0.0099 - val_loss: 0.0125 - val_mse: 0.0097 - 2s/epoch - 5ms/step
Epoch 12/120
374/374 - 2s - loss: 0.0121 - mse: 0.0096 - val_loss: 0.0708 - val_mse: 0.0461 - 2s/epoch - 5ms/step
Epoch 13/120
374/374 - 2s - loss: 0.0151 - mse: 0.0107 - val_loss: 0.0127 - val_mse: 0.0098 - 2s/epoch - 5ms/step
Epoch 14/120
374/374 - 2s - loss: 0.0122 - mse: 0.0097 - val_loss: 0.0160 - val_mse: 0.0107 - 2s/epoch - 5ms/step
Epoch 15/120
374/374 - 2s - loss: 0.0148 - mse: 0.0104 - val_loss: 0.0128 - val_mse: 0.0101 - 2s/epoch - 5ms/step
Epoch 16/120
374/374 - 2s - loss: 0.0137 - mse: 0.0102 - val_loss: 0.0147 - val_mse: 0.0111 - 2s/epoch - 5ms/step
Epoch 17/120
374/374 - 2s - loss: 0.0170 - mse: 0.0113 - val_loss: 0.0127 - val_mse: 0.0099 - 2s/epoch - 5ms/step
Epoch 18/120
374/374 - 2s - loss: 0.0149 - mse: 0.0100 - val_loss: 0.0156 - val_mse: 0.0119 - 2s/epoch - 5ms/step
Epoch 19/120
374/374 - 2s - loss: 0.0368 - mse: 0.0116 - val_loss: 0.0415 - val_mse: 0.0325 - 2s/epoch - 5ms/step
Epoch 20/120
374/374 - 2s - loss: 0.0460 - mse: 0.0119 - val_loss: 0.0202 - val_mse: 0.0102 - 2s/epoch - 5ms/step
Epoch 21/120
374/374 - 2s - loss: 0.0156 - mse: 0.0101 - val_loss: 0.0144 - val_mse: 0.0099 - 2s/epoch - 5ms/step
Epoch 22/120
374/374 - 2s - loss: 0.0144 - mse: 0.0100 - val_loss: 0.0140 - val_mse: 0.0097 - 2s/epoch - 5ms/step
Epoch 23/120
374/374 - 2s - loss: 0.0154 - mse: 0.0099 - val_loss: 0.0194 - val_mse: 0.0134 - 2s/epoch - 5ms/step
Epoch 24/120
374/374 - 2s - loss: 0.0295 - mse: 0.0110 - val_loss: 0.0166 - val_mse: 0.0099 - 2s/epoch - 5ms/step
Epoch 25/120
374/374 - 2s - loss: 0.0150 - mse: 0.0099 - val_loss: 0.0142 - val_mse: 0.0098 - 2s/epoch - 5ms/step
Epoch 26/120
374/374 - 2s - loss: 0.0156 - mse: 0.0099 - val_loss: 0.0141 - val_mse: 0.0098 - 2s/epoch - 5ms/step
Epoch 27/120
374/374 - 2s - loss: 0.0165 - mse: 0.0098 - val_loss: 0.0136 - val_mse: 0.0097 - 2s/epoch - 5ms/step
Epoch 28/120
374/374 - 2s - loss: 0.0150 - mse: 0.0098 - val_loss: 0.0133 - val_mse: 0.0096 - 2s/epoch - 5ms/step
Epoch 29/120
374/374 - 2s - loss: 0.0137 - mse: 0.0097 - val_loss: 0.0146 - val_mse: 0.0098 - 2s/epoch - 5ms/step
Epoch 30/120
374/374 - 2s - loss: 0.0154 - mse: 0.0097 - val_loss: 0.0131 - val_mse: 0.0095 - 2s/epoch - 5ms/step
Epoch 31/120
374/374 - 2s - loss: 0.0141 - mse: 0.0097 - val_loss: 0.0141 - val_mse: 0.0096 - 2s/epoch - 5ms/step
Epoch 32/120
374/374 - 2s - loss: 0.0145 - mse: 0.0096 - val_loss: 0.0129 - val_mse: 0.0095 - 2s/epoch - 5ms/step
Epoch 33/120
374/374 - 2s - loss: 0.0128 - mse: 0.0095 - val_loss: 0.0131 - val_mse: 0.0095 - 2s/epoch - 5ms/step
Epoch 34/120
374/374 - 2s - loss: 0.0169 - mse: 0.0097 - val_loss: 0.0233 - val_mse: 0.0104 - 2s/epoch - 5ms/step
Epoch 35/120
374/374 - 2s - loss: 0.0472 - mse: 0.0101 - val_loss: 0.0151 - val_mse: 0.0096 - 2s/epoch - 5ms/step
Epoch 36/120
374/374 - 2s - loss: 0.0139 - mse: 0.0096 - val_loss: 0.0133 - val_mse: 0.0094 - 2s/epoch - 5ms/step
Epoch 37/120
374/374 - 2s - loss: 0.0136 - mse: 0.0095 - val_loss: 0.0315 - val_mse: 0.0163 - 2s/epoch - 5ms/step
Epoch 38/120
374/374 - 2s - loss: 0.0217 - mse: 0.0097 - val_loss: 0.0138 - val_mse: 0.0094 - 2s/epoch - 5ms/step
Epoch 39/120
374/374 - 2s - loss: 0.0133 - mse: 0.0095 - val_loss: 0.0127 - val_mse: 0.0093 - 2s/epoch - 5ms/step
Epoch 40/120
374/374 - 2s - loss: 0.0129 - mse: 0.0094 - val_loss: 0.0126 - val_mse: 0.0093 - 2s/epoch - 5ms/step
Epoch 41/120
374/374 - 2s - loss: 0.0128 - mse: 0.0094 - val_loss: 0.0166 - val_mse: 0.0100 - 2s/epoch - 5ms/step
Epoch 42/120
374/374 - 2s - loss: 0.0209 - mse: 0.0095 - val_loss: 0.0136 - val_mse: 0.0093 - 2s/epoch - 5ms/step
Epoch 43/120
374/374 - 2s - loss: 0.0128 - mse: 0.0094 - val_loss: 0.0125 - val_mse: 0.0092 - 2s/epoch - 5ms/step
Epoch 44/120
374/374 - 2s - loss: 0.0125 - mse: 0.0093 - val_loss: 0.0122 - val_mse: 0.0092 - 2s/epoch - 5ms/step
Epoch 45/120
374/374 - 2s - loss: 0.0123 - mse: 0.0093 - val_loss: 0.0120 - val_mse: 0.0091 - 2s/epoch - 5ms/step
Epoch 46/120
374/374 - 2s - loss: 0.0122 - mse: 0.0093 - val_loss: 0.0173 - val_mse: 0.0098 - 2s/epoch - 5ms/step
Epoch 47/120
374/374 - 2s - loss: 0.0201 - mse: 0.0093 - val_loss: 0.0122 - val_mse: 0.0091 - 2s/epoch - 5ms/step
Epoch 48/120
374/374 - 2s - loss: 0.0122 - mse: 0.0092 - val_loss: 0.0120 - val_mse: 0.0091 - 2s/epoch - 5ms/step
Epoch 49/120
374/374 - 2s - loss: 0.0120 - mse: 0.0092 - val_loss: 0.0117 - val_mse: 0.0091 - 2s/epoch - 5ms/step
Epoch 50/120
374/374 - 2s - loss: 0.0121 - mse: 0.0092 - val_loss: 0.0184 - val_mse: 0.0096 - 2s/epoch - 5ms/step
Epoch 51/120
374/374 - 2s - loss: 0.0252 - mse: 0.0093 - val_loss: 0.0134 - val_mse: 0.0091 - 2s/epoch - 5ms/step
Epoch 52/120
374/374 - 2s - loss: 0.0141 - mse: 0.0093 - val_loss: 0.0125 - val_mse: 0.0090 - 2s/epoch - 5ms/step
Epoch 53/120
374/374 - 2s - loss: 0.0123 - mse: 0.0092 - val_loss: 0.0136 - val_mse: 0.0093 - 2s/epoch - 5ms/step
Epoch 54/120
374/374 - 2s - loss: 0.0132 - mse: 0.0092 - val_loss: 0.0118 - val_mse: 0.0090 - 2s/epoch - 5ms/step
Epoch 55/120
374/374 - 2s - loss: 0.0120 - mse: 0.0091 - val_loss: 0.0143 - val_mse: 0.0093 - 2s/epoch - 5ms/step
Epoch 56/120
374/374 - 2s - loss: 0.0181 - mse: 0.0093 - val_loss: 0.0157 - val_mse: 0.0092 - 2s/epoch - 5ms/step
Epoch 57/120
374/374 - 2s - loss: 0.0209 - mse: 0.0093 - val_loss: 0.0137 - val_mse: 0.0091 - 2s/epoch - 5ms/step
Epoch 58/120
374/374 - 2s - loss: 0.0156 - mse: 0.0092 - val_loss: 0.0124 - val_mse: 0.0090 - 2s/epoch - 5ms/step
Epoch 59/120
374/374 - 2s - loss: 0.0123 - mse: 0.0091 - val_loss: 0.0119 - val_mse: 0.0090 - 2s/epoch - 5ms/step
Epoch 60/120
374/374 - 2s - loss: 0.0120 - mse: 0.0091 - val_loss: 0.0118 - val_mse: 0.0089 - 2s/epoch - 5ms/step
Epoch 61/120
374/374 - 2s - loss: 0.0120 - mse: 0.0090 - val_loss: 0.0126 - val_mse: 0.0090 - 2s/epoch - 5ms/step
Epoch 62/120
374/374 - 2s - loss: 0.0129 - mse: 0.0090 - val_loss: 0.0117 - val_mse: 0.0089 - 2s/epoch - 5ms/step
Epoch 63/120
374/374 - 2s - loss: 0.0117 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0088 - 2s/epoch - 5ms/step
Epoch 64/120
374/374 - 2s - loss: 0.0116 - mse: 0.0089 - val_loss: 0.0113 - val_mse: 0.0087 - 2s/epoch - 5ms/step
Epoch 65/120
374/374 - 2s - loss: 0.0115 - mse: 0.0088 - val_loss: 0.0115 - val_mse: 0.0086 - 2s/epoch - 5ms/step
Epoch 66/120
374/374 - 2s - loss: 0.0115 - mse: 0.0087 - val_loss: 0.0147 - val_mse: 0.0090 - 2s/epoch - 5ms/step
Epoch 67/120
374/374 - 2s - loss: 0.0148 - mse: 0.0089 - val_loss: 0.0131 - val_mse: 0.0087 - 2s/epoch - 5ms/step
Epoch 68/120
374/374 - 2s - loss: 0.0135 - mse: 0.0088 - val_loss: 0.0138 - val_mse: 0.0089 - 2s/epoch - 5ms/step
Epoch 69/120
374/374 - 2s - loss: 0.0165 - mse: 0.0089 - val_loss: 0.0134 - val_mse: 0.0087 - 2s/epoch - 5ms/step
Epoch 70/120
374/374 - 2s - loss: 0.0127 - mse: 0.0088 - val_loss: 0.0114 - val_mse: 0.0086 - 2s/epoch - 5ms/step
Epoch 71/120
374/374 - 2s - loss: 0.0115 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0086 - 2s/epoch - 5ms/step
Epoch 72/120
374/374 - 2s - loss: 0.0114 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0085 - 2s/epoch - 5ms/step
Epoch 73/120
374/374 - 2s - loss: 0.0113 - mse: 0.0086 - val_loss: 0.0145 - val_mse: 0.0088 - 2s/epoch - 5ms/step
Epoch 74/120
374/374 - 2s - loss: 0.0140 - mse: 0.0087 - val_loss: 0.0113 - val_mse: 0.0085 - 2s/epoch - 5ms/step
Epoch 75/120
374/374 - 2s - loss: 0.0113 - mse: 0.0086 - val_loss: 0.0113 - val_mse: 0.0084 - 2s/epoch - 5ms/step
Epoch 76/120
374/374 - 2s - loss: 0.0115 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0084 - 2s/epoch - 5ms/step
Epoch 77/120
374/374 - 2s - loss: 0.0111 - mse: 0.0085 - val_loss: 0.0109 - val_mse: 0.0083 - 2s/epoch - 5ms/step
Epoch 78/120
374/374 - 2s - loss: 0.0110 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0082 - 2s/epoch - 5ms/step
Epoch 79/120
374/374 - 2s - loss: 0.0110 - mse: 0.0084 - val_loss: 0.0108 - val_mse: 0.0082 - 2s/epoch - 5ms/step
Epoch 80/120
374/374 - 2s - loss: 0.0109 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0081 - 2s/epoch - 5ms/step
Epoch 81/120
374/374 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0082 - 2s/epoch - 5ms/step
Epoch 82/120
374/374 - 2s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0149 - val_mse: 0.0086 - 2s/epoch - 5ms/step
Epoch 83/120
374/374 - 2s - loss: 0.0142 - mse: 0.0083 - val_loss: 0.0111 - val_mse: 0.0081 - 2s/epoch - 5ms/step
Epoch 84/120
374/374 - 2s - loss: 0.0113 - mse: 0.0082 - val_loss: 0.0107 - val_mse: 0.0081 - 2s/epoch - 5ms/step
Epoch 85/120
374/374 - 2s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0116 - val_mse: 0.0082 - 2s/epoch - 5ms/step
Epoch 86/120
374/374 - 2s - loss: 0.0121 - mse: 0.0083 - val_loss: 0.0118 - val_mse: 0.0082 - 2s/epoch - 5ms/step
Epoch 87/120
374/374 - 2s - loss: 0.0145 - mse: 0.0083 - val_loss: 0.0149 - val_mse: 0.0086 - 2s/epoch - 5ms/step
Epoch 88/120
374/374 - 2s - loss: 0.0235 - mse: 0.0086 - val_loss: 0.0116 - val_mse: 0.0081 - 2s/epoch - 5ms/step
Epoch 89/120
374/374 - 2s - loss: 0.0114 - mse: 0.0083 - val_loss: 0.0111 - val_mse: 0.0081 - 2s/epoch - 5ms/step
Epoch 90/120
374/374 - 2s - loss: 0.0112 - mse: 0.0082 - val_loss: 0.0110 - val_mse: 0.0080 - 2s/epoch - 5ms/step
Epoch 91/120
374/374 - 2s - loss: 0.0110 - mse: 0.0081 - val_loss: 0.0109 - val_mse: 0.0080 - 2s/epoch - 5ms/step
Epoch 92/120
374/374 - 2s - loss: 0.0111 - mse: 0.0081 - val_loss: 0.0127 - val_mse: 0.0081 - 2s/epoch - 5ms/step
Epoch 93/120
374/374 - 2s - loss: 0.0148 - mse: 0.0082 - val_loss: 0.0114 - val_mse: 0.0080 - 2s/epoch - 5ms/step
Epoch 94/120
374/374 - 2s - loss: 0.0111 - mse: 0.0081 - val_loss: 0.0112 - val_mse: 0.0080 - 2s/epoch - 5ms/step
Epoch 95/120
374/374 - 2s - loss: 0.0112 - mse: 0.0081 - val_loss: 0.0121 - val_mse: 0.0081 - 2s/epoch - 5ms/step
Epoch 96/120
374/374 - 2s - loss: 0.0145 - mse: 0.0081 - val_loss: 0.0110 - val_mse: 0.0079 - 2s/epoch - 5ms/step
Epoch 97/120
374/374 - 2s - loss: 0.0111 - mse: 0.0080 - val_loss: 0.0108 - val_mse: 0.0079 - 2s/epoch - 5ms/step
Epoch 98/120
374/374 - 2s - loss: 0.0109 - mse: 0.0080 - val_loss: 0.0107 - val_mse: 0.0079 - 2s/epoch - 5ms/step
Epoch 99/120
374/374 - 2s - loss: 0.0108 - mse: 0.0080 - val_loss: 0.0106 - val_mse: 0.0078 - 2s/epoch - 5ms/step
Epoch 100/120
374/374 - 2s - loss: 0.0108 - mse: 0.0079 - val_loss: 0.0106 - val_mse: 0.0078 - 2s/epoch - 5ms/step
Epoch 101/120
374/374 - 2s - loss: 0.0107 - mse: 0.0079 - val_loss: 0.0105 - val_mse: 0.0078 - 2s/epoch - 5ms/step
Epoch 102/120
374/374 - 2s - loss: 0.0107 - mse: 0.0079 - val_loss: 0.0105 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 103/120
374/374 - 2s - loss: 0.0106 - mse: 0.0079 - val_loss: 0.0105 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 104/120
374/374 - 2s - loss: 0.0106 - mse: 0.0078 - val_loss: 0.0105 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 105/120
374/374 - 2s - loss: 0.0106 - mse: 0.0078 - val_loss: 0.0104 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 106/120
374/374 - 2s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0104 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 107/120
374/374 - 2s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 108/120
374/374 - 2s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0131 - val_mse: 0.0082 - 2s/epoch - 5ms/step
Epoch 109/120
374/374 - 2s - loss: 0.0126 - mse: 0.0078 - val_loss: 0.0105 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 110/120
374/374 - 2s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0104 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 111/120
374/374 - 2s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0103 - val_mse: 0.0076 - 2s/epoch - 5ms/step
Epoch 112/120
374/374 - 2s - loss: 0.0105 - mse: 0.0077 - val_loss: 0.0103 - val_mse: 0.0076 - 2s/epoch - 5ms/step
Epoch 113/120
374/374 - 2s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0102 - val_mse: 0.0076 - 2s/epoch - 5ms/step
Epoch 114/120
374/374 - 2s - loss: 0.0103 - mse: 0.0077 - val_loss: 0.0104 - val_mse: 0.0076 - 2s/epoch - 5ms/step
Epoch 115/120
374/374 - 2s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0123 - val_mse: 0.0077 - 2s/epoch - 5ms/step
Epoch 116/120
374/374 - 2s - loss: 0.0111 - mse: 0.0077 - val_loss: 0.0102 - val_mse: 0.0075 - 2s/epoch - 5ms/step
Epoch 117/120
374/374 - 2s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0102 - val_mse: 0.0075 - 2s/epoch - 5ms/step
Epoch 118/120
374/374 - 2s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0103 - val_mse: 0.0075 - 2s/epoch - 5ms/step
Epoch 119/120
374/374 - 2s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0101 - val_mse: 0.0075 - 2s/epoch - 5ms/step
Epoch 120/120
374/374 - 2s - loss: 0.0102 - mse: 0.0076 - val_loss: 0.0102 - val_mse: 0.0075 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.0074842218309640884
['2.5custom_VAE', 'mse', 256, 120, 0.00030000000000000003, 0.4, 505, 0.007568742148578167, 0.0074842218309640884, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[1.1 150 0.0005 32 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1390)         1758350     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1390)        5560        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1390)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2724674     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,893,494
Trainable params: 5,886,924
Non-trainable params: 6,570
__________________________________________________________________________________________________
Epoch 1/150
2985/2985 - 10s - loss: 0.0117 - mse: 0.0194 - val_loss: 0.0077 - val_mse: 0.0134 - 10s/epoch - 3ms/step
Epoch 2/150
2985/2985 - 9s - loss: 0.0072 - mse: 0.0132 - val_loss: 0.0068 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 3/150
2985/2985 - 9s - loss: 0.0069 - mse: 0.0127 - val_loss: 0.0068 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 4/150
2985/2985 - 9s - loss: 0.0068 - mse: 0.0125 - val_loss: 0.0067 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 5/150
2985/2985 - 9s - loss: 0.0068 - mse: 0.0124 - val_loss: 0.0067 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 6/150
2985/2985 - 9s - loss: 0.0068 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 7/150
2985/2985 - 9s - loss: 0.0067 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 8/150
2985/2985 - 9s - loss: 0.0067 - mse: 0.0123 - val_loss: 0.0067 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 9/150
2985/2985 - 9s - loss: 0.0067 - mse: 0.0121 - val_loss: 0.0066 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 10/150
2985/2985 - 9s - loss: 0.0065 - mse: 0.0117 - val_loss: 0.0065 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 11/150
2985/2985 - 9s - loss: 0.0064 - mse: 0.0115 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 12/150
2985/2985 - 9s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0064 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 13/150
2985/2985 - 9s - loss: 0.0064 - mse: 0.0114 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 14/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 15/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 16/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0113 - val_loss: 0.0065 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 17/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 18/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 19/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 20/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0066 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 21/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0112 - val_loss: 0.0064 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 22/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 23/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 24/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0066 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 25/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0068 - val_mse: 0.0135 - 9s/epoch - 3ms/step
Epoch 26/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 27/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 28/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 29/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0111 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 30/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0067 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 31/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 32/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 33/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 34/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 35/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 36/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 37/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 38/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 39/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 40/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0063 - val_mse: 0.0110 - 9s/epoch - 3ms/step
Epoch 41/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0110 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 42/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 43/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 44/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 45/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 46/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 47/150
2985/2985 - 9s - loss: 0.0063 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 48/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 49/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 50/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 51/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 52/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 53/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 54/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 55/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 56/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 57/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0069 - val_mse: 0.0142 - 9s/epoch - 3ms/step
Epoch 58/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 59/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 60/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 61/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0068 - val_mse: 0.0137 - 9s/epoch - 3ms/step
Epoch 62/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 63/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 64/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0067 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 65/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 66/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0070 - val_mse: 0.0146 - 9s/epoch - 3ms/step
Epoch 67/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0066 - val_mse: 0.0126 - 9s/epoch - 3ms/step
Epoch 68/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 69/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 70/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0065 - val_mse: 0.0123 - 9s/epoch - 3ms/step
Epoch 71/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 72/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0109 - val_loss: 0.0064 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 73/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0071 - val_mse: 0.0164 - 9s/epoch - 3ms/step
Epoch 74/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 75/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 76/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0134 - 9s/epoch - 3ms/step
Epoch 77/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0131 - 9s/epoch - 3ms/step
Epoch 78/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0067 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 79/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0108 - val_loss: 0.0065 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 80/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0066 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 81/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0107 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 82/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0066 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 83/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0065 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 84/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0067 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 85/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0066 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 86/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0066 - val_mse: 0.0132 - 9s/epoch - 3ms/step
Epoch 87/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0106 - val_loss: 0.0065 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 88/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0066 - val_mse: 0.0128 - 9s/epoch - 3ms/step
Epoch 89/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 90/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 91/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0067 - val_mse: 0.0138 - 9s/epoch - 3ms/step
Epoch 92/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 93/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 94/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0065 - val_mse: 0.0122 - 9s/epoch - 3ms/step
Epoch 95/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0065 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 96/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 97/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 98/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0117 - 9s/epoch - 3ms/step
Epoch 99/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 100/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 101/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0065 - val_mse: 0.0129 - 9s/epoch - 3ms/step
Epoch 102/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 103/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0065 - val_mse: 0.0124 - 9s/epoch - 3ms/step
Epoch 104/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 105/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 106/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0105 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 107/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0104 - val_loss: 0.0064 - val_mse: 0.0118 - 9s/epoch - 3ms/step
Epoch 108/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 109/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 110/150
2985/2985 - 9s - loss: 0.0062 - mse: 0.0105 - val_loss: 0.0063 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 111/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0109 - 9s/epoch - 3ms/step
Epoch 112/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 113/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 114/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0064 - val_mse: 0.0121 - 9s/epoch - 3ms/step
Epoch 115/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 116/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 117/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0065 - val_mse: 0.0130 - 9s/epoch - 3ms/step
Epoch 118/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0108 - 9s/epoch - 3ms/step
Epoch 119/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 120/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0110 - 9s/epoch - 3ms/step
Epoch 121/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 122/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 123/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 124/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0064 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 125/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0106 - 9s/epoch - 3ms/step
Epoch 126/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 127/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0064 - val_mse: 0.0119 - 9s/epoch - 3ms/step
Epoch 128/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 129/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0116 - 9s/epoch - 3ms/step
Epoch 130/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 131/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0066 - val_mse: 0.0133 - 9s/epoch - 3ms/step
Epoch 132/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0113 - 9s/epoch - 3ms/step
Epoch 133/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 134/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0109 - 9s/epoch - 3ms/step
Epoch 135/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0065 - val_mse: 0.0125 - 9s/epoch - 3ms/step
Epoch 136/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0107 - 9s/epoch - 3ms/step
Epoch 137/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0106 - 9s/epoch - 3ms/step
Epoch 138/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 139/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 140/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0064 - val_mse: 0.0120 - 9s/epoch - 3ms/step
Epoch 141/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0107 - 9s/epoch - 3ms/step
Epoch 142/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0111 - 9s/epoch - 3ms/step
Epoch 143/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0114 - 9s/epoch - 3ms/step
Epoch 144/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0115 - 9s/epoch - 3ms/step
Epoch 145/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0063 - val_mse: 0.0112 - 9s/epoch - 3ms/step
Epoch 146/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0106 - 9s/epoch - 3ms/step
Epoch 147/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0106 - 9s/epoch - 3ms/step
Epoch 148/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0061 - val_mse: 0.0104 - 9s/epoch - 3ms/step
Epoch 149/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0062 - val_mse: 0.0108 - 9s/epoch - 3ms/step
Epoch 150/150
2985/2985 - 9s - loss: 0.0061 - mse: 0.0104 - val_loss: 0.0064 - val_mse: 0.0117 - 9s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.011682922951877117
['1.1custom_VAE', 'logcosh', 32, 150, 0.0005, 0.4, 505, 0.010373685508966446, 0.011682922951877117, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[1.0 150 0.0005 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
Epoch 1/150
747/747 - 3s - loss: 0.0273 - mse: 0.0207 - val_loss: 0.0169 - val_mse: 0.0143 - 3s/epoch - 5ms/step
Epoch 2/150
747/747 - 2s - loss: 0.0152 - mse: 0.0128 - val_loss: 0.0165 - val_mse: 0.0139 - 2s/epoch - 3ms/step
Epoch 3/150
747/747 - 2s - loss: 0.0144 - mse: 0.0122 - val_loss: 0.0159 - val_mse: 0.0126 - 2s/epoch - 3ms/step
Epoch 4/150
747/747 - 2s - loss: 0.0138 - mse: 0.0116 - val_loss: 0.0150 - val_mse: 0.0123 - 2s/epoch - 3ms/step
Epoch 5/150
747/747 - 2s - loss: 0.0133 - mse: 0.0111 - val_loss: 0.0131 - val_mse: 0.0108 - 2s/epoch - 3ms/step
Epoch 6/150
747/747 - 2s - loss: 0.0129 - mse: 0.0107 - val_loss: 0.0128 - val_mse: 0.0106 - 2s/epoch - 3ms/step
Epoch 7/150
747/747 - 2s - loss: 0.0127 - mse: 0.0105 - val_loss: 0.0126 - val_mse: 0.0104 - 2s/epoch - 3ms/step
Epoch 8/150
747/747 - 2s - loss: 0.0125 - mse: 0.0103 - val_loss: 0.0124 - val_mse: 0.0100 - 2s/epoch - 3ms/step
Epoch 9/150
747/747 - 2s - loss: 0.0124 - mse: 0.0101 - val_loss: 0.0122 - val_mse: 0.0098 - 2s/epoch - 3ms/step
Epoch 10/150
747/747 - 2s - loss: 0.0123 - mse: 0.0100 - val_loss: 0.0122 - val_mse: 0.0096 - 2s/epoch - 3ms/step
Epoch 11/150
747/747 - 2s - loss: 0.0122 - mse: 0.0098 - val_loss: 0.0121 - val_mse: 0.0094 - 2s/epoch - 3ms/step
Epoch 12/150
747/747 - 2s - loss: 0.0121 - mse: 0.0097 - val_loss: 0.0120 - val_mse: 0.0095 - 2s/epoch - 3ms/step
Epoch 13/150
747/747 - 2s - loss: 0.0121 - mse: 0.0096 - val_loss: 0.0120 - val_mse: 0.0093 - 2s/epoch - 3ms/step
Epoch 14/150
747/747 - 2s - loss: 0.0120 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0092 - 2s/epoch - 3ms/step
Epoch 15/150
747/747 - 2s - loss: 0.0118 - mse: 0.0094 - val_loss: 0.0115 - val_mse: 0.0091 - 2s/epoch - 3ms/step
Epoch 16/150
747/747 - 2s - loss: 0.0116 - mse: 0.0092 - val_loss: 0.0113 - val_mse: 0.0089 - 2s/epoch - 3ms/step
Epoch 17/150
747/747 - 2s - loss: 0.0114 - mse: 0.0090 - val_loss: 0.0113 - val_mse: 0.0088 - 2s/epoch - 3ms/step
Epoch 18/150
747/747 - 2s - loss: 0.0113 - mse: 0.0089 - val_loss: 0.0111 - val_mse: 0.0087 - 2s/epoch - 3ms/step
Epoch 19/150
747/747 - 2s - loss: 0.0112 - mse: 0.0088 - val_loss: 0.0111 - val_mse: 0.0086 - 2s/epoch - 3ms/step
Epoch 20/150
747/747 - 2s - loss: 0.0112 - mse: 0.0087 - val_loss: 0.0110 - val_mse: 0.0085 - 2s/epoch - 3ms/step
Epoch 21/150
747/747 - 2s - loss: 0.0112 - mse: 0.0087 - val_loss: 0.0109 - val_mse: 0.0084 - 2s/epoch - 3ms/step
Epoch 22/150
747/747 - 2s - loss: 0.0111 - mse: 0.0086 - val_loss: 0.0109 - val_mse: 0.0084 - 2s/epoch - 3ms/step
Epoch 23/150
747/747 - 2s - loss: 0.0110 - mse: 0.0085 - val_loss: 0.0109 - val_mse: 0.0083 - 2s/epoch - 3ms/step
Epoch 24/150
747/747 - 2s - loss: 0.0110 - mse: 0.0085 - val_loss: 0.0109 - val_mse: 0.0083 - 2s/epoch - 3ms/step
Epoch 25/150
747/747 - 2s - loss: 0.0110 - mse: 0.0084 - val_loss: 0.0108 - val_mse: 0.0082 - 2s/epoch - 3ms/step
Epoch 26/150
747/747 - 2s - loss: 0.0109 - mse: 0.0084 - val_loss: 0.0108 - val_mse: 0.0081 - 2s/epoch - 3ms/step
Epoch 27/150
747/747 - 2s - loss: 0.0109 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0081 - 2s/epoch - 3ms/step
Epoch 28/150
747/747 - 2s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0107 - val_mse: 0.0080 - 2s/epoch - 3ms/step
Epoch 29/150
747/747 - 2s - loss: 0.0108 - mse: 0.0081 - val_loss: 0.0106 - val_mse: 0.0079 - 2s/epoch - 3ms/step
Epoch 30/150
747/747 - 2s - loss: 0.0107 - mse: 0.0080 - val_loss: 0.0104 - val_mse: 0.0077 - 2s/epoch - 3ms/step
Epoch 31/150
747/747 - 2s - loss: 0.0106 - mse: 0.0079 - val_loss: 0.0103 - val_mse: 0.0077 - 2s/epoch - 3ms/step
Epoch 32/150
747/747 - 2s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0076 - 2s/epoch - 3ms/step
Epoch 33/150
747/747 - 2s - loss: 0.0105 - mse: 0.0077 - val_loss: 0.0103 - val_mse: 0.0075 - 2s/epoch - 3ms/step
Epoch 34/150
747/747 - 2s - loss: 0.0104 - mse: 0.0076 - val_loss: 0.0102 - val_mse: 0.0075 - 2s/epoch - 3ms/step
Epoch 35/150
747/747 - 2s - loss: 0.0104 - mse: 0.0076 - val_loss: 0.0101 - val_mse: 0.0074 - 2s/epoch - 3ms/step
Epoch 36/150
747/747 - 2s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0101 - val_mse: 0.0073 - 2s/epoch - 3ms/step
Epoch 37/150
747/747 - 2s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0102 - val_mse: 0.0073 - 2s/epoch - 3ms/step
Epoch 38/150
747/747 - 2s - loss: 0.0103 - mse: 0.0074 - val_loss: 0.0101 - val_mse: 0.0072 - 2s/epoch - 3ms/step
Epoch 39/150
747/747 - 2s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0100 - val_mse: 0.0071 - 2s/epoch - 3ms/step
Epoch 40/150
747/747 - 2s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0100 - val_mse: 0.0070 - 2s/epoch - 3ms/step
Epoch 41/150
747/747 - 2s - loss: 0.0101 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0069 - 2s/epoch - 3ms/step
Epoch 42/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0099 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 43/150
747/747 - 2s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 44/150
747/747 - 2s - loss: 0.0100 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 45/150
747/747 - 2s - loss: 0.0100 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 46/150
747/747 - 2s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 2s/epoch - 3ms/step
Epoch 47/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 48/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 49/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 2s/epoch - 3ms/step
Epoch 50/150
747/747 - 2s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 51/150
747/747 - 2s - loss: 0.0099 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 52/150
747/747 - 2s - loss: 0.0099 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 53/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 54/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 55/150
747/747 - 2s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 56/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0097 - val_mse: 0.0066 - 2s/epoch - 3ms/step
Epoch 57/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 58/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 59/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 60/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 61/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 62/150
747/747 - 2s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 63/150
747/747 - 2s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 64/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0096 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 65/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 2s/epoch - 3ms/step
Epoch 66/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 67/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 68/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 69/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 70/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 71/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 72/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 73/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 74/150
747/747 - 2s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 75/150
747/747 - 2s - loss: 0.0097 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 76/150
747/747 - 2s - loss: 0.0097 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0064 - 2s/epoch - 3ms/step
Epoch 77/150
747/747 - 2s - loss: 0.0097 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 78/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 79/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 80/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 81/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 82/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 83/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 84/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 85/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 86/150
747/747 - 2s - loss: 0.0096 - mse: 0.0065 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 87/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 88/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0095 - val_mse: 0.0063 - 2s/epoch - 3ms/step
Epoch 89/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 90/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 91/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 92/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 93/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 94/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 95/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 96/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 97/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 98/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 99/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 100/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 101/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 102/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 103/150
747/747 - 2s - loss: 0.0096 - mse: 0.0064 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 104/150
747/747 - 2s - loss: 0.0096 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 105/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 106/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 107/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 108/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0062 - 2s/epoch - 3ms/step
Epoch 109/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 110/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 111/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 112/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 113/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0094 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 114/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 115/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 116/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 117/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 118/150
747/747 - 2s - loss: 0.0095 - mse: 0.0063 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 119/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0061 - 2s/epoch - 3ms/step
Epoch 120/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 121/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 122/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 123/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 124/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 125/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 126/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 127/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 128/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 129/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 130/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 131/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 132/150
747/747 - 2s - loss: 0.0095 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 133/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 134/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 135/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 136/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 137/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 138/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0092 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 139/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 140/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 141/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 142/150
747/747 - 2s - loss: 0.0094 - mse: 0.0062 - val_loss: 0.0093 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 143/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0092 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 144/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0093 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 145/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0093 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 146/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0092 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 147/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0092 - val_mse: 0.0059 - 2s/epoch - 3ms/step
Epoch 148/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0092 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 149/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0092 - val_mse: 0.0060 - 2s/epoch - 3ms/step
Epoch 150/150
747/747 - 2s - loss: 0.0094 - mse: 0.0061 - val_loss: 0.0092 - val_mse: 0.0060 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.005986179690808058
['1.0custom_VAE', 'mse', 128, 150, 0.0005, 0.4, 505, 0.006114310119301081, 0.005986179690808058, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.5 120 0.00030000000000000003 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
Epoch 1/120
2985/2985 - 12s - loss: 0.0267 - mse: 0.0195 - val_loss: 0.0147 - val_mse: 0.0119 - 12s/epoch - 4ms/step
Epoch 2/120
2985/2985 - 11s - loss: 0.0140 - mse: 0.0118 - val_loss: 0.0126 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 3/120
2985/2985 - 11s - loss: 0.0124 - mse: 0.0107 - val_loss: 0.0118 - val_mse: 0.0099 - 11s/epoch - 4ms/step
Epoch 4/120
2985/2985 - 11s - loss: 0.0120 - mse: 0.0101 - val_loss: 0.0115 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 5/120
2985/2985 - 11s - loss: 0.0118 - mse: 0.0099 - val_loss: 0.0116 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 6/120
2985/2985 - 11s - loss: 0.0117 - mse: 0.0097 - val_loss: 0.0114 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 7/120
2985/2985 - 11s - loss: 0.0116 - mse: 0.0096 - val_loss: 0.0116 - val_mse: 0.0094 - 11s/epoch - 4ms/step
Epoch 8/120
2985/2985 - 11s - loss: 0.0115 - mse: 0.0095 - val_loss: 0.0117 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 9/120
2985/2985 - 11s - loss: 0.0115 - mse: 0.0094 - val_loss: 0.0119 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 10/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0094 - val_loss: 0.0114 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 11/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0114 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 12/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0093 - val_loss: 0.0116 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 13/120
2985/2985 - 11s - loss: 0.0114 - mse: 0.0092 - val_loss: 0.0122 - val_mse: 0.0098 - 11s/epoch - 4ms/step
Epoch 14/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0092 - val_loss: 0.0119 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 15/120
2985/2985 - 11s - loss: 0.0113 - mse: 0.0090 - val_loss: 0.0127 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 16/120
2985/2985 - 11s - loss: 0.0112 - mse: 0.0089 - val_loss: 0.0133 - val_mse: 0.0108 - 11s/epoch - 4ms/step
Epoch 17/120
2985/2985 - 11s - loss: 0.0111 - mse: 0.0088 - val_loss: 0.0117 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 18/120
2985/2985 - 11s - loss: 0.0110 - mse: 0.0087 - val_loss: 0.0131 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 19/120
2985/2985 - 11s - loss: 0.0110 - mse: 0.0087 - val_loss: 0.0132 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 20/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0086 - val_loss: 0.0120 - val_mse: 0.0096 - 11s/epoch - 4ms/step
Epoch 21/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0086 - val_loss: 0.0151 - val_mse: 0.0126 - 11s/epoch - 4ms/step
Epoch 22/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0137 - val_mse: 0.0113 - 11s/epoch - 4ms/step
Epoch 23/120
2985/2985 - 11s - loss: 0.0109 - mse: 0.0085 - val_loss: 0.0118 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 24/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0085 - val_loss: 0.0132 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 25/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0085 - val_loss: 0.0137 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 26/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0084 - val_loss: 0.0128 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 27/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0084 - val_loss: 0.0124 - val_mse: 0.0099 - 11s/epoch - 4ms/step
Epoch 28/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0084 - val_loss: 0.0154 - val_mse: 0.0128 - 11s/epoch - 4ms/step
Epoch 29/120
2985/2985 - 11s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0140 - val_mse: 0.0114 - 11s/epoch - 4ms/step
Epoch 30/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0083 - val_loss: 0.0137 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 31/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0123 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 32/120
2985/2985 - 11s - loss: 0.0107 - mse: 0.0082 - val_loss: 0.0145 - val_mse: 0.0118 - 11s/epoch - 4ms/step
Epoch 33/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0146 - val_mse: 0.0120 - 11s/epoch - 4ms/step
Epoch 34/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0152 - val_mse: 0.0125 - 11s/epoch - 4ms/step
Epoch 35/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0130 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 36/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0138 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 37/120
2985/2985 - 11s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0138 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 38/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0138 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 39/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0132 - val_mse: 0.0105 - 11s/epoch - 4ms/step
Epoch 40/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0118 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 41/120
2985/2985 - 11s - loss: 0.0105 - mse: 0.0078 - val_loss: 0.0167 - val_mse: 0.0140 - 11s/epoch - 4ms/step
Epoch 42/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0136 - val_mse: 0.0109 - 11s/epoch - 4ms/step
Epoch 43/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0139 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 44/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0078 - val_loss: 0.0151 - val_mse: 0.0124 - 11s/epoch - 4ms/step
Epoch 45/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0132 - val_mse: 0.0104 - 11s/epoch - 4ms/step
Epoch 46/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0152 - val_mse: 0.0123 - 11s/epoch - 4ms/step
Epoch 47/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0137 - val_mse: 0.0110 - 11s/epoch - 4ms/step
Epoch 48/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0118 - val_mse: 0.0091 - 11s/epoch - 4ms/step
Epoch 49/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0127 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 50/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0133 - val_mse: 0.0105 - 11s/epoch - 4ms/step
Epoch 51/120
2985/2985 - 11s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0149 - val_mse: 0.0120 - 11s/epoch - 4ms/step
Epoch 52/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0134 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 53/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0161 - val_mse: 0.0133 - 11s/epoch - 4ms/step
Epoch 54/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0142 - val_mse: 0.0114 - 11s/epoch - 4ms/step
Epoch 55/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0149 - val_mse: 0.0121 - 11s/epoch - 4ms/step
Epoch 56/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0146 - val_mse: 0.0118 - 11s/epoch - 4ms/step
Epoch 57/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0133 - val_mse: 0.0104 - 11s/epoch - 4ms/step
Epoch 58/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0140 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 59/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0181 - val_mse: 0.0153 - 11s/epoch - 4ms/step
Epoch 60/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0141 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 61/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0150 - val_mse: 0.0122 - 11s/epoch - 4ms/step
Epoch 62/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0152 - val_mse: 0.0123 - 11s/epoch - 4ms/step
Epoch 63/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0138 - val_mse: 0.0110 - 11s/epoch - 4ms/step
Epoch 64/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0150 - val_mse: 0.0121 - 11s/epoch - 4ms/step
Epoch 65/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0146 - val_mse: 0.0117 - 11s/epoch - 4ms/step
Epoch 66/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0145 - val_mse: 0.0116 - 11s/epoch - 4ms/step
Epoch 67/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0121 - val_mse: 0.0092 - 11s/epoch - 4ms/step
Epoch 68/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0125 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 69/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0149 - val_mse: 0.0120 - 11s/epoch - 4ms/step
Epoch 70/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0123 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 71/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0132 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 72/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0159 - val_mse: 0.0131 - 11s/epoch - 4ms/step
Epoch 73/120
2985/2985 - 11s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0165 - val_mse: 0.0137 - 11s/epoch - 4ms/step
Epoch 74/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0136 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 75/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0157 - val_mse: 0.0129 - 11s/epoch - 4ms/step
Epoch 76/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0148 - val_mse: 0.0119 - 11s/epoch - 4ms/step
Epoch 77/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0136 - val_mse: 0.0108 - 11s/epoch - 4ms/step
Epoch 78/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0130 - val_mse: 0.0102 - 11s/epoch - 4ms/step
Epoch 79/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0129 - val_mse: 0.0100 - 11s/epoch - 4ms/step
Epoch 80/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0139 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 81/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0126 - val_mse: 0.0098 - 11s/epoch - 4ms/step
Epoch 82/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0150 - val_mse: 0.0121 - 11s/epoch - 4ms/step
Epoch 83/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0135 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 84/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0135 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 85/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0206 - val_mse: 0.0177 - 11s/epoch - 4ms/step
Epoch 86/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0119 - val_mse: 0.0090 - 11s/epoch - 4ms/step
Epoch 87/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0139 - val_mse: 0.0111 - 11s/epoch - 4ms/step
Epoch 88/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0146 - val_mse: 0.0117 - 11s/epoch - 4ms/step
Epoch 89/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0140 - val_mse: 0.0112 - 11s/epoch - 4ms/step
Epoch 90/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0133 - val_mse: 0.0104 - 11s/epoch - 4ms/step
Epoch 91/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0123 - val_mse: 0.0094 - 11s/epoch - 4ms/step
Epoch 92/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0135 - val_mse: 0.0105 - 11s/epoch - 4ms/step
Epoch 93/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0118 - val_mse: 0.0089 - 11s/epoch - 4ms/step
Epoch 94/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0126 - val_mse: 0.0097 - 11s/epoch - 4ms/step
Epoch 95/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0133 - val_mse: 0.0104 - 11s/epoch - 4ms/step
Epoch 96/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0117 - val_mse: 0.0088 - 11s/epoch - 4ms/step
Epoch 97/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0128 - val_mse: 0.0099 - 11s/epoch - 4ms/step
Epoch 98/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0115 - val_mse: 0.0086 - 11s/epoch - 4ms/step
Epoch 99/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0127 - val_mse: 0.0098 - 11s/epoch - 4ms/step
Epoch 100/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0157 - val_mse: 0.0128 - 11s/epoch - 4ms/step
Epoch 101/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0148 - val_mse: 0.0119 - 11s/epoch - 4ms/step
Epoch 102/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0163 - val_mse: 0.0135 - 11s/epoch - 4ms/step
Epoch 103/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0122 - val_mse: 0.0093 - 11s/epoch - 4ms/step
Epoch 104/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0131 - val_mse: 0.0101 - 11s/epoch - 4ms/step
Epoch 105/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0123 - val_mse: 0.0095 - 11s/epoch - 4ms/step
Epoch 106/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0145 - val_mse: 0.0116 - 11s/epoch - 4ms/step
Epoch 107/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0136 - val_mse: 0.0107 - 11s/epoch - 4ms/step
Epoch 108/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0160 - val_mse: 0.0131 - 11s/epoch - 4ms/step
Epoch 109/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0142 - val_mse: 0.0113 - 11s/epoch - 4ms/step
Epoch 110/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0118 - val_mse: 0.0089 - 11s/epoch - 4ms/step
Epoch 111/120
2985/2985 - 11s - loss: 0.0102 - mse: 0.0073 - val_loss: 0.0138 - val_mse: 0.0109 - 11s/epoch - 4ms/step
Epoch 112/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0138 - val_mse: 0.0108 - 11s/epoch - 4ms/step
Epoch 113/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0134 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 114/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0149 - val_mse: 0.0119 - 11s/epoch - 4ms/step
Epoch 115/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0133 - val_mse: 0.0103 - 11s/epoch - 4ms/step
Epoch 116/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0135 - val_mse: 0.0105 - 11s/epoch - 4ms/step
Epoch 117/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0119 - val_mse: 0.0090 - 11s/epoch - 4ms/step
Epoch 118/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0135 - val_mse: 0.0106 - 11s/epoch - 4ms/step
Epoch 119/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0147 - val_mse: 0.0119 - 11s/epoch - 4ms/step
Epoch 120/120
2985/2985 - 11s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0143 - val_mse: 0.0114 - 11s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.011358034797012806
['2.5custom_VAE', 'mse', 32, 120, 0.00030000000000000003, 0.4, 505, 0.007302636280655861, 0.011358034797012806, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.5 90 0.0005 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
Epoch 1/90
747/747 - 4s - loss: 0.0357 - mse: 0.0250 - val_loss: 0.0186 - val_mse: 0.0148 - 4s/epoch - 5ms/step
Epoch 2/90
747/747 - 3s - loss: 0.0162 - mse: 0.0129 - val_loss: 0.0231 - val_mse: 0.0159 - 3s/epoch - 4ms/step
Epoch 3/90
747/747 - 3s - loss: 0.0149 - mse: 0.0118 - val_loss: 0.0206 - val_mse: 0.0130 - 3s/epoch - 4ms/step
Epoch 4/90
747/747 - 3s - loss: 0.0141 - mse: 0.0111 - val_loss: 0.0146 - val_mse: 0.0111 - 3s/epoch - 4ms/step
Epoch 5/90
747/747 - 3s - loss: 0.0132 - mse: 0.0106 - val_loss: 0.0248 - val_mse: 0.0163 - 3s/epoch - 4ms/step
Epoch 6/90
747/747 - 3s - loss: 0.0127 - mse: 0.0102 - val_loss: 0.0121 - val_mse: 0.0096 - 3s/epoch - 4ms/step
Epoch 7/90
747/747 - 3s - loss: 0.0120 - mse: 0.0098 - val_loss: 0.0117 - val_mse: 0.0092 - 3s/epoch - 4ms/step
Epoch 8/90
747/747 - 3s - loss: 0.0117 - mse: 0.0095 - val_loss: 0.0119 - val_mse: 0.0093 - 3s/epoch - 4ms/step
Epoch 9/90
747/747 - 3s - loss: 0.0117 - mse: 0.0094 - val_loss: 0.0122 - val_mse: 0.0096 - 3s/epoch - 4ms/step
Epoch 10/90
747/747 - 3s - loss: 0.0134 - mse: 0.0103 - val_loss: 0.0135 - val_mse: 0.0105 - 3s/epoch - 4ms/step
Epoch 11/90
747/747 - 3s - loss: 0.0171 - mse: 0.0106 - val_loss: 0.0148 - val_mse: 0.0104 - 3s/epoch - 4ms/step
Epoch 12/90
747/747 - 3s - loss: 0.0191 - mse: 0.0103 - val_loss: 0.0194 - val_mse: 0.0102 - 3s/epoch - 4ms/step
Epoch 13/90
747/747 - 3s - loss: 0.0178 - mse: 0.0100 - val_loss: 0.0127 - val_mse: 0.0095 - 3s/epoch - 4ms/step
Epoch 14/90
747/747 - 3s - loss: 0.0133 - mse: 0.0097 - val_loss: 0.0124 - val_mse: 0.0094 - 3s/epoch - 4ms/step
Epoch 15/90
747/747 - 3s - loss: 0.0125 - mse: 0.0095 - val_loss: 0.0135 - val_mse: 0.0093 - 3s/epoch - 4ms/step
Epoch 16/90
747/747 - 3s - loss: 0.0132 - mse: 0.0094 - val_loss: 0.0121 - val_mse: 0.0092 - 3s/epoch - 4ms/step
Epoch 17/90
747/747 - 3s - loss: 0.0121 - mse: 0.0093 - val_loss: 0.0119 - val_mse: 0.0091 - 3s/epoch - 4ms/step
Epoch 18/90
747/747 - 3s - loss: 0.0118 - mse: 0.0092 - val_loss: 0.0121 - val_mse: 0.0092 - 3s/epoch - 4ms/step
Epoch 19/90
747/747 - 3s - loss: 0.0117 - mse: 0.0092 - val_loss: 0.0114 - val_mse: 0.0090 - 3s/epoch - 4ms/step
Epoch 20/90
747/747 - 3s - loss: 0.0116 - mse: 0.0091 - val_loss: 0.0141 - val_mse: 0.0091 - 3s/epoch - 4ms/step
Epoch 21/90
747/747 - 3s - loss: 0.0141 - mse: 0.0092 - val_loss: 0.0213 - val_mse: 0.0115 - 3s/epoch - 4ms/step
Epoch 22/90
747/747 - 3s - loss: 0.0216 - mse: 0.0096 - val_loss: 0.0126 - val_mse: 0.0090 - 3s/epoch - 4ms/step
Epoch 23/90
747/747 - 3s - loss: 0.0123 - mse: 0.0091 - val_loss: 0.0121 - val_mse: 0.0089 - 3s/epoch - 4ms/step
Epoch 24/90
747/747 - 3s - loss: 0.0121 - mse: 0.0091 - val_loss: 0.0117 - val_mse: 0.0088 - 3s/epoch - 4ms/step
Epoch 25/90
747/747 - 3s - loss: 0.0118 - mse: 0.0090 - val_loss: 0.0196 - val_mse: 0.0124 - 3s/epoch - 4ms/step
Epoch 26/90
747/747 - 3s - loss: 0.0146 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0087 - 3s/epoch - 4ms/step
Epoch 27/90
747/747 - 3s - loss: 0.0116 - mse: 0.0089 - val_loss: 0.0193 - val_mse: 0.0106 - 3s/epoch - 4ms/step
Epoch 28/90
747/747 - 3s - loss: 0.0153 - mse: 0.0090 - val_loss: 0.0135 - val_mse: 0.0092 - 3s/epoch - 4ms/step
Epoch 29/90
747/747 - 3s - loss: 0.0124 - mse: 0.0089 - val_loss: 0.0113 - val_mse: 0.0086 - 3s/epoch - 4ms/step
Epoch 30/90
747/747 - 3s - loss: 0.0115 - mse: 0.0088 - val_loss: 0.0130 - val_mse: 0.0088 - 3s/epoch - 4ms/step
Epoch 31/90
747/747 - 3s - loss: 0.0121 - mse: 0.0088 - val_loss: 0.0112 - val_mse: 0.0086 - 3s/epoch - 4ms/step
Epoch 32/90
747/747 - 3s - loss: 0.0114 - mse: 0.0088 - val_loss: 0.0112 - val_mse: 0.0086 - 3s/epoch - 4ms/step
Epoch 33/90
747/747 - 3s - loss: 0.0113 - mse: 0.0087 - val_loss: 0.0111 - val_mse: 0.0086 - 3s/epoch - 4ms/step
Epoch 34/90
747/747 - 3s - loss: 0.0112 - mse: 0.0087 - val_loss: 0.0110 - val_mse: 0.0085 - 3s/epoch - 4ms/step
Epoch 35/90
747/747 - 3s - loss: 0.0112 - mse: 0.0087 - val_loss: 0.0110 - val_mse: 0.0085 - 3s/epoch - 4ms/step
Epoch 36/90
747/747 - 3s - loss: 0.0111 - mse: 0.0086 - val_loss: 0.0112 - val_mse: 0.0084 - 3s/epoch - 4ms/step
Epoch 37/90
747/747 - 3s - loss: 0.0111 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0083 - 3s/epoch - 4ms/step
Epoch 38/90
747/747 - 3s - loss: 0.0110 - mse: 0.0084 - val_loss: 0.0108 - val_mse: 0.0082 - 3s/epoch - 4ms/step
Epoch 39/90
747/747 - 3s - loss: 0.0108 - mse: 0.0083 - val_loss: 0.0106 - val_mse: 0.0080 - 3s/epoch - 4ms/step
Epoch 40/90
747/747 - 3s - loss: 0.0108 - mse: 0.0082 - val_loss: 0.0115 - val_mse: 0.0081 - 3s/epoch - 4ms/step
Epoch 41/90
747/747 - 3s - loss: 0.0109 - mse: 0.0082 - val_loss: 0.0105 - val_mse: 0.0079 - 3s/epoch - 4ms/step
Epoch 42/90
747/747 - 3s - loss: 0.0106 - mse: 0.0081 - val_loss: 0.0105 - val_mse: 0.0078 - 3s/epoch - 4ms/step
Epoch 43/90
747/747 - 3s - loss: 0.0106 - mse: 0.0080 - val_loss: 0.0105 - val_mse: 0.0078 - 3s/epoch - 4ms/step
Epoch 44/90
747/747 - 3s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0104 - val_mse: 0.0077 - 3s/epoch - 4ms/step
Epoch 45/90
747/747 - 3s - loss: 0.0105 - mse: 0.0079 - val_loss: 0.0107 - val_mse: 0.0076 - 3s/epoch - 4ms/step
Epoch 46/90
747/747 - 3s - loss: 0.0106 - mse: 0.0078 - val_loss: 0.0103 - val_mse: 0.0076 - 3s/epoch - 4ms/step
Epoch 47/90
747/747 - 3s - loss: 0.0104 - mse: 0.0077 - val_loss: 0.0102 - val_mse: 0.0075 - 3s/epoch - 4ms/step
Epoch 48/90
747/747 - 3s - loss: 0.0103 - mse: 0.0077 - val_loss: 0.0101 - val_mse: 0.0074 - 3s/epoch - 4ms/step
Epoch 49/90
747/747 - 3s - loss: 0.0103 - mse: 0.0076 - val_loss: 0.0101 - val_mse: 0.0074 - 3s/epoch - 4ms/step
Epoch 50/90
747/747 - 3s - loss: 0.0103 - mse: 0.0075 - val_loss: 0.0101 - val_mse: 0.0072 - 3s/epoch - 4ms/step
Epoch 51/90
747/747 - 3s - loss: 0.0102 - mse: 0.0075 - val_loss: 0.0100 - val_mse: 0.0072 - 3s/epoch - 4ms/step
Epoch 52/90
747/747 - 3s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0100 - val_mse: 0.0072 - 3s/epoch - 4ms/step
Epoch 53/90
747/747 - 3s - loss: 0.0102 - mse: 0.0074 - val_loss: 0.0100 - val_mse: 0.0071 - 3s/epoch - 4ms/step
Epoch 54/90
747/747 - 3s - loss: 0.0101 - mse: 0.0073 - val_loss: 0.0099 - val_mse: 0.0071 - 3s/epoch - 4ms/step
Epoch 55/90
747/747 - 3s - loss: 0.0101 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 3s/epoch - 4ms/step
Epoch 56/90
747/747 - 3s - loss: 0.0101 - mse: 0.0072 - val_loss: 0.0099 - val_mse: 0.0070 - 3s/epoch - 4ms/step
Epoch 57/90
747/747 - 3s - loss: 0.0100 - mse: 0.0072 - val_loss: 0.0098 - val_mse: 0.0069 - 3s/epoch - 4ms/step
Epoch 58/90
747/747 - 3s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 3s/epoch - 4ms/step
Epoch 59/90
747/747 - 3s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 3s/epoch - 4ms/step
Epoch 60/90
747/747 - 3s - loss: 0.0100 - mse: 0.0071 - val_loss: 0.0098 - val_mse: 0.0069 - 3s/epoch - 4ms/step
Epoch 61/90
747/747 - 3s - loss: 0.0100 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 3s/epoch - 4ms/step
Epoch 62/90
747/747 - 3s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0098 - val_mse: 0.0068 - 3s/epoch - 4ms/step
Epoch 63/90
747/747 - 3s - loss: 0.0099 - mse: 0.0070 - val_loss: 0.0097 - val_mse: 0.0067 - 3s/epoch - 4ms/step
Epoch 64/90
747/747 - 3s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 3s/epoch - 4ms/step
Epoch 65/90
747/747 - 3s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 3s/epoch - 4ms/step
Epoch 66/90
747/747 - 3s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 3s/epoch - 4ms/step
Epoch 67/90
747/747 - 3s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0067 - 3s/epoch - 4ms/step
Epoch 68/90
747/747 - 3s - loss: 0.0099 - mse: 0.0069 - val_loss: 0.0097 - val_mse: 0.0066 - 3s/epoch - 4ms/step
Epoch 69/90
747/747 - 3s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0097 - val_mse: 0.0067 - 3s/epoch - 4ms/step
Epoch 70/90
747/747 - 3s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 3s/epoch - 4ms/step
Epoch 71/90
747/747 - 3s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0067 - 3s/epoch - 4ms/step
Epoch 72/90
747/747 - 3s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 3s/epoch - 4ms/step
Epoch 73/90
747/747 - 3s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0066 - 3s/epoch - 4ms/step
Epoch 74/90
747/747 - 3s - loss: 0.0098 - mse: 0.0068 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 75/90
747/747 - 3s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 76/90
747/747 - 3s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 77/90
747/747 - 3s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 78/90
747/747 - 3s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 79/90
747/747 - 3s - loss: 0.0098 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 80/90
747/747 - 3s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 81/90
747/747 - 3s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0096 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 82/90
747/747 - 3s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0064 - 3s/epoch - 4ms/step
Epoch 83/90
747/747 - 3s - loss: 0.0097 - mse: 0.0067 - val_loss: 0.0095 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 84/90
747/747 - 3s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0065 - 3s/epoch - 4ms/step
Epoch 85/90
747/747 - 3s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0096 - val_mse: 0.0064 - 3s/epoch - 4ms/step
Epoch 86/90
747/747 - 3s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 3s/epoch - 4ms/step
Epoch 87/90
747/747 - 3s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 3s/epoch - 4ms/step
Epoch 88/90
747/747 - 3s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 3s/epoch - 4ms/step
Epoch 89/90
747/747 - 3s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0064 - 3s/epoch - 4ms/step
Epoch 90/90
747/747 - 3s - loss: 0.0097 - mse: 0.0066 - val_loss: 0.0095 - val_mse: 0.0063 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 505
MSE loss in the autoencoder: 0.006325831171125174
['2.5custom_VAE', 'mse', 128, 90, 0.0005, 0.4, 505, 0.006579090841114521, 0.006325831171125174, '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
[2.0 90 0.0007 256 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4743486     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,505,808
Trainable params: 10,494,686
Non-trainable params: 11,122
__________________________________________________________________________________________________
2023-02-23 17:48:14.166847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:14.166881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22294012532
MaxInUse:                  22426879172
NumAllocs:                   419014462
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:14.166937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:14.166943: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 122
2023-02-23 17:48:14.166947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:14.166950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:14.166953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 375
2023-02-23 17:48:14.166956: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 128
2023-02-23 17:48:14.166959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 153
2023-02-23 17:48:14.166968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:14.166971: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:14.166974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:14.166976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 154
2023-02-23 17:48:14.166979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 65
2023-02-23 17:48:14.166981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 48
2023-02-23 17:48:14.166984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 36
2023-02-23 17:48:14.166987: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:14.166989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 17:48:14.166992: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:14.166994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:14.166997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 63
2023-02-23 17:48:14.167000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 24
2023-02-23 17:48:14.167002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:14.167005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:14.167007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:14.167010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 42
2023-02-23 17:48:14.167013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:14.167015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'logcosh', 256, 90, 0.0007, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 90 0.0007 256 2]) is not valid.
[0.6 150 0.0007 128 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          383295      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          383295      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1603506     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,331,998
Trainable params: 3,327,956
Non-trainable params: 4,042
__________________________________________________________________________________________________
2023-02-23 17:48:15.953791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:15.953826: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22307340540
MaxInUse:                  22426879172
NumAllocs:                   419014522
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:15.953894: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:15.953900: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 126
2023-02-23 17:48:15.953903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:15.953907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:15.953910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 382
2023-02-23 17:48:15.953913: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 128
2023-02-23 17:48:15.953916: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:48:15.953918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 154
2023-02-23 17:48:15.953921: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:15.953924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:15.953927: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:15.953930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 154
2023-02-23 17:48:15.953933: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 66
2023-02-23 17:48:15.953935: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 48
2023-02-23 17:48:15.953945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:48:15.953948: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 36
2023-02-23 17:48:15.953951: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:15.953953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 17:48:15.953956: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:15.953959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:48:15.953961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:15.953964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 63
2023-02-23 17:48:15.953966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 24
2023-02-23 17:48:15.953969: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:15.953972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:15.953974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:15.953977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 42
2023-02-23 17:48:15.953980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:15.953982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 128, 150, 0.0007, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 150 0.0007 128 1]) is not valid.
[2.5 120 0.0005 16 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
2023-02-23 17:48:17.763820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:17.763857: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22346281764
MaxInUse:                  22426879172
NumAllocs:                   419014578
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:17.763913: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:17.763919: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 126
2023-02-23 17:48:17.763922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:17.763925: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:17.763928: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 382
2023-02-23 17:48:17.763931: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 128
2023-02-23 17:48:17.763934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 154
2023-02-23 17:48:17.763937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:17.763940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:17.763943: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:17.763946: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 164
2023-02-23 17:48:17.763949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 66
2023-02-23 17:48:17.763952: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 48
2023-02-23 17:48:17.763955: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 36
2023-02-23 17:48:17.763958: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:17.763960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 17:48:17.763963: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:17.763966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:17.763975: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 66
2023-02-23 17:48:17.763978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 24
2023-02-23 17:48:17.763981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:17.763983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:17.763986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:17.763988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 44
2023-02-23 17:48:17.763991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:17.763994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.5custom_VAE', 'mse', 16, 120, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.5 120 0.0005 16 1]) is not valid.
[2.0 90 0.0005 32 1] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1277145     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4743486     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,505,808
Trainable params: 10,494,686
Non-trainable params: 11,122
__________________________________________________________________________________________________
2023-02-23 17:48:19.565839: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:19.565874: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22388304996
MaxInUse:                  22426879172
NumAllocs:                   419014634
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:19.565931: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:19.565937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 126
2023-02-23 17:48:19.565940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:19.565943: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:19.565946: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 389
2023-02-23 17:48:19.565949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 128
2023-02-23 17:48:19.565951: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 155
2023-02-23 17:48:19.565954: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:19.565957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:19.565959: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-23 17:48:19.565962: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 164
2023-02-23 17:48:19.565964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 67
2023-02-23 17:48:19.565967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 48
2023-02-23 17:48:19.565970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 36
2023-02-23 17:48:19.565972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:19.565975: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 17:48:19.565977: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:19.565980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 33
2023-02-23 17:48:19.565983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 66
2023-02-23 17:48:19.565985: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 24
2023-02-23 17:48:19.565988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:19.565990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:19.565993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-23 17:48:19.565996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 44
2023-02-23 17:48:19.565998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:19.566008: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.0custom_VAE', 'mse', 32, 90, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.0 90 0.0005 32 1]) is not valid.
[1.0 150 0.00030000000000000003 32 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:48:21.350743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:21.350789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22367813028
MaxInUse:                  22426879172
NumAllocs:                   419014690
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:21.350846: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:21.350851: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 126
2023-02-23 17:48:21.350855: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:21.350858: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:21.350861: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 389
2023-02-23 17:48:21.350864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 128
2023-02-23 17:48:21.350867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 165
2023-02-23 17:48:21.350870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:21.350873: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:21.350875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:21.350878: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 164
2023-02-23 17:48:21.350881: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 67
2023-02-23 17:48:21.350883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 48
2023-02-23 17:48:21.350886: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 39
2023-02-23 17:48:21.350888: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:21.350891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 17:48:21.350894: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:21.350896: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:21.350899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 66
2023-02-23 17:48:21.350901: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 26
2023-02-23 17:48:21.350904: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:21.350907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:21.350909: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:21.350912: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 44
2023-02-23 17:48:21.350915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:21.350917: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'logcosh', 32, 150, 0.00030000000000000003, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 150 0.00030000000000000003 32 2]) is not valid.
[1.0 145 0.0005 128 1] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:48:23.163559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:23.163597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22389344292
MaxInUse:                  22426879172
NumAllocs:                   419014746
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:23.163655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:23.163661: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 126
2023-02-23 17:48:23.163672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:23.163675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:23.163682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 396
2023-02-23 17:48:23.163685: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 128
2023-02-23 17:48:23.163687: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 176
2023-02-23 17:48:23.163690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:23.163693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:23.163695: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:23.163698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 164
2023-02-23 17:48:23.163701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 68
2023-02-23 17:48:23.163703: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 48
2023-02-23 17:48:23.163706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 42
2023-02-23 17:48:23.163709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:23.163711: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 32
2023-02-23 17:48:23.163714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:23.163716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:23.163719: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 66
2023-02-23 17:48:23.163722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 28
2023-02-23 17:48:23.163724: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:23.163727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:23.163729: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:23.163732: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 44
2023-02-23 17:48:23.163735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:23.163737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 145, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 145 0.0005 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[0.5 145 0.0005 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
2023-02-23 17:48:30.642825: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:30.642865: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22379098308
MaxInUse:                  22426879172
NumAllocs:                   419014802
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:30.642929: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:30.642935: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 126
2023-02-23 17:48:30.642938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:30.642941: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:30.642945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 396
2023-02-23 17:48:30.642948: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 138
2023-02-23 17:48:30.642951: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 166
2023-02-23 17:48:30.642954: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:30.642956: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:30.642967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:30.642970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 164
2023-02-23 17:48:30.642973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 68
2023-02-23 17:48:30.642975: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 51
2023-02-23 17:48:30.642978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 39
2023-02-23 17:48:30.642981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:30.642983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 17:48:30.642986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:30.642989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:30.642991: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 66
2023-02-23 17:48:30.642994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 26
2023-02-23 17:48:30.642997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:30.642999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:30.643002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:30.643005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 44
2023-02-23 17:48:30.643007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:30.643010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 145, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 145 0.0005 128 1]) is not valid.
[0.9 150 0.0005 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1137)         1438305     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1137)        4548        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1137)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          574690      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          574690      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2275852     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,868,085
Trainable params: 4,862,527
Non-trainable params: 5,558
__________________________________________________________________________________________________
2023-02-23 17:48:32.414076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 454164480/23676715008
2023-02-23 17:48:32.414120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22398570656
MaxInUse:                  22426879172
NumAllocs:                   419014860
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:32.414186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:32.414192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-23 17:48:32.414196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:32.414199: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:32.414202: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 403
2023-02-23 17:48:32.414205: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 138
2023-02-23 17:48:32.414208: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 10
2023-02-23 17:48:32.414211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 167
2023-02-23 17:48:32.414214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:32.414216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:32.414219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:32.414222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 164
2023-02-23 17:48:32.414224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 69
2023-02-23 17:48:32.414227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 51
2023-02-23 17:48:32.414230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2296740, 3
2023-02-23 17:48:32.414232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 39
2023-02-23 17:48:32.414242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:32.414245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 17:48:32.414248: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:32.414250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:32.414253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 2
2023-02-23 17:48:32.414256: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 66
2023-02-23 17:48:32.414258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 26
2023-02-23 17:48:32.414261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:32.414264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:32.414266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:32.414269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 44
2023-02-23 17:48:32.414272: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:32.414274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.9custom_VAE', 'mse', 128, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.9 150 0.0005 128 1]) is not valid.
[2.5 115 0.0018 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1596305     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5864654     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,067,304
Trainable params: 13,053,654
Non-trainable params: 13,650
__________________________________________________________________________________________________
2023-02-23 17:48:34.182537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 353501184/23676715008
2023-02-23 17:48:34.182575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22431367532
MaxInUse:                  22482788752
NumAllocs:                   419014916
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:34.182634: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:34.182640: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-23 17:48:34.182643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:34.182646: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:34.182649: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 403
2023-02-23 17:48:34.182653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 138
2023-02-23 17:48:34.182655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 167
2023-02-23 17:48:34.182658: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:34.182661: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:34.182663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:34.182666: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:34.182669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 69
2023-02-23 17:48:34.182671: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 51
2023-02-23 17:48:34.182674: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 39
2023-02-23 17:48:34.182680: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:34.182682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 17:48:34.182685: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:34.182688: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:34.182690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:34.182693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 26
2023-02-23 17:48:34.182704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:34.182707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:34.182709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:34.182712: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:34.182714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:34.182717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.5custom_VAE', 'mse', 128, 115, 0.0018, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.5 115 0.0018 128 1]) is not valid.
[0.4 150 0.0005 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          255530      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          255530      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1154684     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,306,589
Trainable params: 2,303,559
Non-trainable params: 3,030
__________________________________________________________________________________________________
2023-02-23 17:48:35.945067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 353501184/23676715008
2023-02-23 17:48:35.945108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22440593888
MaxInUse:                  22482788752
NumAllocs:                   419014972
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:35.945164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:35.945170: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-23 17:48:35.945173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:35.945176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:35.945180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 420
2023-02-23 17:48:35.945183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 138
2023-02-23 17:48:35.945186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 168
2023-02-23 17:48:35.945188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:35.945191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:35.945194: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:35.945197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:35.945200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 73
2023-02-23 17:48:35.945203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 51
2023-02-23 17:48:35.945206: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 41
2023-02-23 17:48:35.945209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:35.945212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 17:48:35.945215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:35.945218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:35.945220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:35.945223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 26
2023-02-23 17:48:35.945226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:35.945229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:35.945232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:35.945235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:35.945238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:35.945241: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 128, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 150 0.0005 128 1]) is not valid.
[1.0 150 0.0005 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:48:37.735044: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 353501184/23676715008
2023-02-23 17:48:37.735088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22452898796
MaxInUse:                  22482788752
NumAllocs:                   419015028
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:37.735159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:37.735164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-23 17:48:37.735168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:37.735171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:37.735174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 410
2023-02-23 17:48:37.735176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 138
2023-02-23 17:48:37.735179: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 178
2023-02-23 17:48:37.735182: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:37.735185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:37.735188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:37.735191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:37.735194: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 70
2023-02-23 17:48:37.735197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 51
2023-02-23 17:48:37.735200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 42
2023-02-23 17:48:37.735203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:37.735206: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 17:48:37.735209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:37.735212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:37.735215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:37.735217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 28
2023-02-23 17:48:37.735220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:37.735223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:37.735226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:37.735229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:37.735232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:37.735235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 256, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 150 0.0005 256 1]) is not valid.
[2.4 150 0.002 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3033)         3836745     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3033)        12132       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3033)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1532170     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1532170     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5639356     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,552,573
Trainable params: 12,539,431
Non-trainable params: 13,142
__________________________________________________________________________________________________
2023-02-23 17:48:39.506801: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 319946752/23676715008
2023-02-23 17:48:39.506844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22503109104
MaxInUse:                  22533773760
NumAllocs:                   419015088
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:39.506914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:39.506920: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:39.506923: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:39.506926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:39.506929: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 417
2023-02-23 17:48:39.506941: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 138
2023-02-23 17:48:39.506944: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 179
2023-02-23 17:48:39.506947: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:39.506950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:39.506953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:39.506955: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 10
2023-02-23 17:48:39.506958: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:39.506961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 71
2023-02-23 17:48:39.506964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 51
2023-02-23 17:48:39.506967: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 42
2023-02-23 17:48:39.506970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:39.506973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 34
2023-02-23 17:48:39.506976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:39.506979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:39.506981: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6126660, 3
2023-02-23 17:48:39.506984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:39.506987: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 28
2023-02-23 17:48:39.506990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:39.506993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:39.506996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:39.506999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 2
2023-02-23 17:48:39.507002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:39.507005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:39.507007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.4custom_VAE', 'mse', 256, 150, 0.002, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.4 150 0.002 256 1]) is not valid.
[0.5 150 0.0005 128 2] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
2023-02-23 17:48:41.280376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 319946752/23676715008
2023-02-23 17:48:41.280412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22464184092
MaxInUse:                  22533773760
NumAllocs:                   419015144
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:41.280471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:41.280477: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:41.280480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:41.280483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:41.280486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 417
2023-02-23 17:48:41.280489: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:48:41.280492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 179
2023-02-23 17:48:41.280495: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:41.280498: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:41.280508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:41.280511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:41.280514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 71
2023-02-23 17:48:41.280517: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:48:41.280519: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 42
2023-02-23 17:48:41.280522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:41.280525: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:48:41.280528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:41.280531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:41.280534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:41.280537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 28
2023-02-23 17:48:41.280540: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:41.280543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:41.280546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:41.280549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:41.280551: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:41.280554: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 128, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 150 0.0005 128 2]) is not valid.
[1.1 150 0.0005 128 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1390)         1758350     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1390)        5560        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1390)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2724674     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,893,494
Trainable params: 5,886,924
Non-trainable params: 6,570
__________________________________________________________________________________________________
2023-02-23 17:48:43.079455: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 319946752/23676715008
2023-02-23 17:48:43.079493: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22487758068
MaxInUse:                  22533773760
NumAllocs:                   419015200
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:43.079557: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:43.079563: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:43.079567: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:43.079569: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:43.079572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 424
2023-02-23 17:48:43.079575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:48:43.079578: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 180
2023-02-23 17:48:43.079580: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 32
2023-02-23 17:48:43.079583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:43.079586: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:43.079589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:43.079591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 72
2023-02-23 17:48:43.079594: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:48:43.079597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 42
2023-02-23 17:48:43.079599: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 12
2023-02-23 17:48:43.079602: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:48:43.079612: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:43.079615: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:43.079617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:43.079620: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 28
2023-02-23 17:48:43.079623: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 8
2023-02-23 17:48:43.079625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:43.079628: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:43.079631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:43.079633: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:43.079636: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1custom_VAE', 'mse', 128, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1 150 0.0005 128 1]) is not valid.
[1.0 145 0.0005 128 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:48:44.854113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 319946752/23676715008
2023-02-23 17:48:44.854152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22485715356
MaxInUse:                  22533773760
NumAllocs:                   419015256
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:44.854218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:44.854224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:44.854228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:44.854231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:44.854234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 424
2023-02-23 17:48:44.854237: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:48:44.854240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 190
2023-02-23 17:48:44.854243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:44.854246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:44.854249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:44.854252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:44.854255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 72
2023-02-23 17:48:44.854258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:48:44.854261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 45
2023-02-23 17:48:44.854264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:44.854266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:48:44.854269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:44.854272: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:44.854275: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:44.854278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 30
2023-02-23 17:48:44.854281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:44.854284: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:44.854287: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:44.854297: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:44.854300: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:44.854303: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'logcosh', 128, 145, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 145 0.0005 128 2]) is not valid.
[2.4 120 0.00030000000000000003 256 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3033)         3836745     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3033)        12132       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3033)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          1532170     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          1532170     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5639356     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,552,573
Trainable params: 12,539,431
Non-trainable params: 13,142
__________________________________________________________________________________________________
2023-02-23 17:48:46.627003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:48:46.627053: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22535925648
MaxInUse:                  22566590304
NumAllocs:                   419015312
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:46.627118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:46.627124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:46.627127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:46.627130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:46.627133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 431
2023-02-23 17:48:46.627136: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:48:46.627139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 191
2023-02-23 17:48:46.627142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:46.627145: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:46.627148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:46.627151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 10
2023-02-23 17:48:46.627154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:46.627157: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 73
2023-02-23 17:48:46.627160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:48:46.627163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 45
2023-02-23 17:48:46.627166: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:46.627169: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:48:46.627172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:46.627174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:46.627177: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6126660, 3
2023-02-23 17:48:46.627180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:46.627183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 30
2023-02-23 17:48:46.627186: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:46.627189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:46.627192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:46.627195: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 2
2023-02-23 17:48:46.627198: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:46.627201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:46.627204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.4custom_VAE', 'logcosh', 256, 120, 0.00030000000000000003, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.4 120 0.00030000000000000003 256 2]) is not valid.
[0.6 150 0.0005 128 2] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          383295      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          383295      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1603506     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,331,998
Trainable params: 3,327,956
Non-trainable params: 4,042
__________________________________________________________________________________________________
2023-02-23 17:48:48.404861: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:48:48.404902: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22499043348
MaxInUse:                  22566590304
NumAllocs:                   419015368
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:48.404975: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:48.404980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:48.404984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:48.404987: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:48.404990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 431
2023-02-23 17:48:48.404993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:48:48.404996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:48:48.404999: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 191
2023-02-23 17:48:48.405002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:48.405004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:48.405007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:48.405010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:48.405013: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 73
2023-02-23 17:48:48.405016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:48:48.405019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:48:48.405022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 45
2023-02-23 17:48:48.405025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:48.405028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:48:48.405031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:48.405034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:48:48.405037: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:48.405040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:48.405043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 30
2023-02-23 17:48:48.405045: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:48.405048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:48.405051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:48.405054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:48.405057: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:48.405060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'logcosh', 128, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 150 0.0005 128 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[1.1 150 0.0005 128 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1390)         1758350     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1390)        5560        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1390)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2724674     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,893,494
Trainable params: 5,886,924
Non-trainable params: 6,570
__________________________________________________________________________________________________
2023-02-23 17:48:57.082603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:48:57.082644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22522617324
MaxInUse:                  22566590304
NumAllocs:                   419015424
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:57.082713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:57.082726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:57.082730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:57.082733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:57.082736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 438
2023-02-23 17:48:57.082739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:48:57.082742: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:48:57.082745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 192
2023-02-23 17:48:57.082748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 32
2023-02-23 17:48:57.082751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:57.082754: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:57.082757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:57.082760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 74
2023-02-23 17:48:57.082763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:48:57.082766: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:48:57.082769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 45
2023-02-23 17:48:57.082772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 12
2023-02-23 17:48:57.082775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:48:57.082778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:57.082781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:48:57.082784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:57.082787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:57.082790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 30
2023-02-23 17:48:57.082793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 8
2023-02-23 17:48:57.082795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:57.082798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:57.082801: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:57.082804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:57.082807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1custom_VAE', 'logcosh', 128, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1 150 0.0005 128 2]) is not valid.
[1.0 150 0.0005 256 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:48:58.917148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:48:58.917188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22520574612
MaxInUse:                  22566590304
NumAllocs:                   419015480
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:48:58.917253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:48:58.917258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:48:58.917262: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:48:58.917265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:48:58.917268: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 438
2023-02-23 17:48:58.917279: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:48:58.917282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:48:58.917285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 202
2023-02-23 17:48:58.917288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:48:58.917291: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:48:58.917294: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:48:58.917297: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:48:58.917300: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 74
2023-02-23 17:48:58.917303: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:48:58.917306: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:48:58.917309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 48
2023-02-23 17:48:58.917312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:48:58.917314: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:48:58.917317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:48:58.917320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:48:58.917323: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:48:58.917326: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:48:58.917329: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 32
2023-02-23 17:48:58.917332: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:48:58.917335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:48:58.917338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:48:58.917341: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:48:58.917344: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:48:58.917347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'logcosh', 256, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 150 0.0005 256 2]) is not valid.
[1.0 150 0.002 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:49:00.702454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:49:00.702491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22542105876
MaxInUse:                  22566590304
NumAllocs:                   419015536
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:00.702552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:00.702558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:00.702561: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:00.702564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:00.702567: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 445
2023-02-23 17:49:00.702571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 148
2023-02-23 17:49:00.702574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:00.702577: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 213
2023-02-23 17:49:00.702579: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:00.702589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:00.702592: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:00.702595: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:00.702598: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 75
2023-02-23 17:49:00.702601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 54
2023-02-23 17:49:00.702604: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:00.702607: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 51
2023-02-23 17:49:00.702610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:00.702613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-23 17:49:00.702616: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:00.702619: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:00.702622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:00.702625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:00.702627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 17:49:00.702630: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:00.702633: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:00.702636: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:00.702639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:00.702642: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:00.702645: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 150, 0.002, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 150 0.002 128 1]) is not valid.
[0.5 150 0.0022 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
2023-02-23 17:49:02.474614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:49:02.474655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22531859892
MaxInUse:                  22566590304
NumAllocs:                   419015592
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:02.474720: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:02.474726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:02.474729: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:02.474732: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:02.474736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 445
2023-02-23 17:49:02.474739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 158
2023-02-23 17:49:02.474742: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:02.474745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 203
2023-02-23 17:49:02.474748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:02.474750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:02.474753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:02.474756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:02.474759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 75
2023-02-23 17:49:02.474770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 57
2023-02-23 17:49:02.474773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:02.474776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 48
2023-02-23 17:49:02.474779: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:02.474782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 17:49:02.474785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:02.474788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:02.474791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:02.474794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:02.474796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 32
2023-02-23 17:49:02.474799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:02.474802: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:02.474805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:02.474808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:02.474811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:02.474814: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 150, 0.0022, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 150 0.0022 256 1]) is not valid.
[1.0 150 0.0005 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:49:04.239198: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:49:04.239238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22553391156
MaxInUse:                  22566590304
NumAllocs:                   419015648
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:04.239300: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:04.239306: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:04.239309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:04.239312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:04.239316: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 452
2023-02-23 17:49:04.239319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 158
2023-02-23 17:49:04.239322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:04.239325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 214
2023-02-23 17:49:04.239328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:04.239331: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:04.239334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:04.239337: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:04.239339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 76
2023-02-23 17:49:04.239342: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 57
2023-02-23 17:49:04.239345: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:04.239348: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 51
2023-02-23 17:49:04.239351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:04.239362: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 17:49:04.239365: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:04.239368: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:04.239371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:04.239374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:04.239377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 17:49:04.239380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:04.239383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:04.239385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:04.239388: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:04.239391: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:04.239394: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 64, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 150 0.0005 64 1]) is not valid.
[1.0 150 0.002 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:49:06.008225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:49:06.008267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22553391156
MaxInUse:                  22587698948
NumAllocs:                   419015704
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:06.008328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:06.008334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:06.008338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:06.008341: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:06.008344: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 452
2023-02-23 17:49:06.008347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 158
2023-02-23 17:49:06.008350: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:06.008353: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 214
2023-02-23 17:49:06.008356: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:06.008359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:06.008362: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:06.008365: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:06.008368: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 76
2023-02-23 17:49:06.008371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 57
2023-02-23 17:49:06.008374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:06.008377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 51
2023-02-23 17:49:06.008380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:06.008383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 17:49:06.008386: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:06.008389: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:06.008392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:06.008395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:06.008406: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 17:49:06.008410: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:06.008413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:06.008416: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:06.008419: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:06.008421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:06.008424: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 150, 0.002, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 150 0.002 128 1]) is not valid.
[0.6 150 0.002 128 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          383295      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          383295      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1603506     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,331,998
Trainable params: 3,327,956
Non-trainable params: 4,042
__________________________________________________________________________________________________
2023-02-23 17:49:07.773386: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 286392320/23676715008
2023-02-23 17:49:07.773418: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22566719148
MaxInUse:                  22587698948
NumAllocs:                   419015760
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:07.773482: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:07.773487: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:07.773491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:07.773494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:07.773497: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 459
2023-02-23 17:49:07.773500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 158
2023-02-23 17:49:07.773503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 20
2023-02-23 17:49:07.773506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 215
2023-02-23 17:49:07.773509: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:07.773512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:07.773515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:07.773518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:07.773521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 77
2023-02-23 17:49:07.773524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 57
2023-02-23 17:49:07.773527: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 6
2023-02-23 17:49:07.773530: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 51
2023-02-23 17:49:07.773533: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:07.773535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 17:49:07.773538: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:07.773541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 4
2023-02-23 17:49:07.773544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:07.773547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:07.773550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 34
2023-02-23 17:49:07.773553: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:07.773556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:07.773559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:07.773569: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:07.773572: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:07.773575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.6custom_VAE', 'mse', 128, 150, 0.002, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.6 150 0.002 128 1]) is not valid.
[1.0 150 0.0007 128 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
2023-02-23 17:49:09.542911: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 252837888/23676715008
2023-02-23 17:49:09.542953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22574922420
MaxInUse:                  22601026940
NumAllocs:                   419015816
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:09.543016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:09.543022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:09.543026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:09.543029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:09.543032: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 459
2023-02-23 17:49:09.543035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 158
2023-02-23 17:49:09.543038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:09.543041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 225
2023-02-23 17:49:09.543043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:09.543046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:09.543049: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:09.543052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:09.543055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 77
2023-02-23 17:49:09.543058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 57
2023-02-23 17:49:09.543061: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:09.543064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 54
2023-02-23 17:49:09.543067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:09.543070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 17:49:09.543072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:09.543075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:09.543078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:09.543081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:09.543084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 17:49:09.543087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:09.543090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:09.543092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:09.543095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:09.543098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:09.543101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0custom_VAE', 'mse', 128, 150, 0.0007, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0 150 0.0007 128 1]) is not valid.
[1.1 150 0.0005 16 1] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1390)         1758350     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1390)        5560        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1390)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          702455      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2724674     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,893,494
Trainable params: 5,886,924
Non-trainable params: 6,570
__________________________________________________________________________________________________
2023-02-23 17:49:11.316064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 252837888/23676715008
2023-02-23 17:49:11.316103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22598496396
MaxInUse:                  22612547036
NumAllocs:                   419015872
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:11.316175: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:11.316181: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:11.316184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:11.316187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:11.316190: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 466
2023-02-23 17:49:11.316193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 158
2023-02-23 17:49:11.316196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:11.316199: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 226
2023-02-23 17:49:11.316202: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 32
2023-02-23 17:49:11.316204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:11.316207: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:11.316210: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:11.316213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 78
2023-02-23 17:49:11.316216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 57
2023-02-23 17:49:11.316219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:11.316222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 54
2023-02-23 17:49:11.316225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 12
2023-02-23 17:49:11.316228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-23 17:49:11.316231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:11.316234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:11.316236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:11.316239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:11.316242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 17:49:11.316245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 8
2023-02-23 17:49:11.316248: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:11.316251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:11.316254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:11.316257: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:11.316260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1custom_VAE', 'mse', 16, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1 150 0.0005 16 1]) is not valid.
[0.5 150 0.0005 16 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          319665      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1379982     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,821,320
Trainable params: 2,817,782
Non-trainable params: 3,538
__________________________________________________________________________________________________
2023-02-23 17:49:13.094208: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 252837888/23676715008
2023-02-23 17:49:13.094249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22586207700
MaxInUse:                  22616167420
NumAllocs:                   419015928
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:13.094316: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:13.094322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:13.094333: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:13.094337: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:13.094340: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 466
2023-02-23 17:49:13.094343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 168
2023-02-23 17:49:13.094346: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:13.094349: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 226
2023-02-23 17:49:13.094352: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:13.094355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:13.094358: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:13.094361: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:13.094364: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 78
2023-02-23 17:49:13.094367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 60
2023-02-23 17:49:13.094370: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:13.094373: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 54
2023-02-23 17:49:13.094376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:13.094379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 40
2023-02-23 17:49:13.094381: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:13.094384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:13.094387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:13.094390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:13.094393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 17:49:13.094396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:13.094399: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:13.094402: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:13.094405: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:13.094408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:13.094411: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 16, 150, 0.0005, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 150 0.0005 16 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_3.pkl
[0.9 150 0.0007 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1137)         1438305     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1137)        4548        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1137)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          574690      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          574690      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2275852     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,868,085
Trainable params: 4,862,527
Non-trainable params: 5,558
__________________________________________________________________________________________________
2023-02-23 17:49:20.744449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 252837888/23676715008
2023-02-23 17:49:20.744491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22605680040
MaxInUse:                  22617172344
NumAllocs:                   419015984
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 17:49:20.744558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 17:49:20.744565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-23 17:49:20.744568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 68
2023-02-23 17:49:20.744571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 17:49:20.744574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 473
2023-02-23 17:49:20.744585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 168
2023-02-23 17:49:20.744588: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 10
2023-02-23 17:49:20.744591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4548, 10
2023-02-23 17:49:20.744594: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 227
2023-02-23 17:49:20.744597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 22
2023-02-23 17:49:20.744600: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 22
2023-02-23 17:49:20.744603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 76
2023-02-23 17:49:20.744606: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 174
2023-02-23 17:49:20.744609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1020100, 79
2023-02-23 17:49:20.744612: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1276640, 60
2023-02-23 17:49:20.744615: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 17:49:20.744618: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2296740, 3
2023-02-23 17:49:20.744621: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 54
2023-02-23 17:49:20.744624: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2807800, 9
2023-02-23 17:49:20.744627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 40
2023-02-23 17:49:20.744629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3829920, 9
2023-02-23 17:49:20.744632: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 2
2023-02-23 17:49:20.744635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5106560, 30
2023-02-23 17:49:20.744638: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 2
2023-02-23 17:49:20.744641: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6383200, 69
2023-02-23 17:49:20.744644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 36
2023-02-23 17:49:20.744647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 6
2023-02-23 17:49:20.744649: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-23 17:49:20.744652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 20
2023-02-23 17:49:20.744654: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 46
2023-02-23 17:49:20.744657: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 16
2023-02-23 17:49:20.744660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 40
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.9custom_VAE', 'mse', 128, 150, 0.0007, 0.4, 505, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: score is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.9 150 0.0007 128 1]) is not valid.
[1.0 150 0.0005 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 505)          638825      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 505)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2501150     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,382,816
Trainable params: 5,376,750
Non-trainable params: 6,066
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.0_cr0.4_bs128_ep150_loss_logcosh_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
Traceback (most recent call last):
  File "genetic.py", line 67, in <module>
    main()
  File "genetic.py", line 43, in main
    genetic_hypertune_autoencoder(prefix_name = 'geneticVAE_MMmp_gap',
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.4/../../genetic_hypertune.py", line 213, in genetic_hypertune_autoencoder
    ga_instance.run()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1413, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.4/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 384, in train_autoencoder
    model = load_model(encoder_path, custom_objects={'SamplingLayer': SamplingLayer, 'VAELossLayer':VAELossLayer})
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 115, in _process_kwargs
    raise TypeError(
TypeError: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.
Thu Feb 23 17:49:32 CET 2023
done
