start
Tue Feb 14 14:25:48 CET 2023
2023-02-14 14:25:50.321661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 14:25:50.533190: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-14 14:26:23,105 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f0d916c6dc0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.5 145 0.0012 64 1]
 [1.5 150 0.001 32 1]
 [1.5 70 0.0005 16 0]
 [2.5 50 0.0005 64 2]
 [2.0 110 0.0005 8 0]
 [2.0 170 0.0005 16 0]
 [2.0 10 0.001 16 1]
 [1.5 90 0.001 32 1]
 [0.5 10 0.001 64 0]
 [1.5 30 0.0005 8 1]]
[1.5 145 0.0012 64 1] 0
Shape of dataset to encode: (106113, 1264)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-14 14:26:27.112606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 14:26:28.266348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9634 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5
2023-02-14 14:26:28.267728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9634 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5
2023-02-14 14:26:28.292662: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-14 14:26:28.602869: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/bottleneck_zmean/bias/v/Assign' id:1060 op device:{requested: '', assigned: ''} def:{{{node training/Adam/bottleneck_zmean/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/bottleneck_zmean/bias/v, training/Adam/bottleneck_zmean/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 14:26:40.121490: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:452 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 14s - loss: 0.0114 - val_loss: 0.0053 - 14s/epoch - 143us/sample
Epoch 2/145
95501/95501 - 11s - loss: 0.0047 - val_loss: 0.0044 - 11s/epoch - 113us/sample
Epoch 3/145
95501/95501 - 11s - loss: 0.0041 - val_loss: 0.0045 - 11s/epoch - 110us/sample
Epoch 4/145
95501/95501 - 11s - loss: 0.0036 - val_loss: 0.0029 - 11s/epoch - 113us/sample
Epoch 5/145
95501/95501 - 10s - loss: 0.0031 - val_loss: 0.0030 - 10s/epoch - 110us/sample
Epoch 6/145
95501/95501 - 11s - loss: 0.0025 - val_loss: 0.0022 - 11s/epoch - 111us/sample
Epoch 7/145
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0027 - 11s/epoch - 110us/sample
Epoch 8/145
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0028 - 11s/epoch - 111us/sample
Epoch 9/145
95501/95501 - 10s - loss: 0.0021 - val_loss: 0.0020 - 10s/epoch - 110us/sample
Epoch 10/145
95501/95501 - 10s - loss: 0.0018 - val_loss: 0.0016 - 10s/epoch - 109us/sample
Epoch 11/145
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0017 - 11s/epoch - 111us/sample
Epoch 12/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 13/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0014 - 11s/epoch - 110us/sample
Epoch 14/145
95501/95501 - 10s - loss: 0.0015 - val_loss: 0.0016 - 10s/epoch - 108us/sample
Epoch 15/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0013 - 11s/epoch - 110us/sample
Epoch 16/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0016 - 11s/epoch - 112us/sample
Epoch 17/145
95501/95501 - 10s - loss: 0.0015 - val_loss: 0.0013 - 10s/epoch - 110us/sample
Epoch 18/145
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0013 - 10s/epoch - 109us/sample
Epoch 19/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0016 - 11s/epoch - 110us/sample
Epoch 20/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0012 - 11s/epoch - 110us/sample
Epoch 21/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0017 - 11s/epoch - 110us/sample
Epoch 22/145
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 108us/sample
Epoch 23/145
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 24/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0014 - 11s/epoch - 112us/sample
Epoch 25/145
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0013 - 10s/epoch - 110us/sample
Epoch 26/145
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 110us/sample
Epoch 27/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0014 - 11s/epoch - 111us/sample
Epoch 28/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0013 - 11s/epoch - 113us/sample
Epoch 29/145
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 30/145
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 31/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 32/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 110us/sample
Epoch 33/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 34/145
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 108us/sample
Epoch 35/145
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 36/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 37/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0013 - 11s/epoch - 112us/sample
Epoch 38/145
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 39/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 40/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 41/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 42/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 43/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 44/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 45/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 46/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 110us/sample
Epoch 47/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 48/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 49/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 50/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 51/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 113us/sample
Epoch 52/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 53/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 54/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 55/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9380e-04 - 11s/epoch - 112us/sample
Epoch 56/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 57/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 58/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 59/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9101e-04 - 11s/epoch - 111us/sample
Epoch 60/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8106e-04 - 11s/epoch - 113us/sample
Epoch 61/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 62/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9837e-04 - 11s/epoch - 111us/sample
Epoch 63/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8281e-04 - 10s/epoch - 109us/sample
Epoch 64/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 65/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8455e-04 - 11s/epoch - 111us/sample
Epoch 66/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7214e-04 - 11s/epoch - 112us/sample
Epoch 67/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 68/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 69/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7841e-04 - 11s/epoch - 110us/sample
Epoch 70/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7585e-04 - 10s/epoch - 109us/sample
Epoch 71/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.9731e-04 - 11s/epoch - 111us/sample
Epoch 72/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7329e-04 - 11s/epoch - 112us/sample
Epoch 73/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7283e-04 - 11s/epoch - 110us/sample
Epoch 74/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5932e-04 - 11s/epoch - 110us/sample
Epoch 75/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 76/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6013e-04 - 11s/epoch - 113us/sample
Epoch 77/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 78/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.5798e-04 - 11s/epoch - 112us/sample
Epoch 79/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7378e-04 - 11s/epoch - 111us/sample
Epoch 80/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 81/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7016e-04 - 11s/epoch - 110us/sample
Epoch 82/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4998e-04 - 11s/epoch - 112us/sample
Epoch 83/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 84/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6193e-04 - 11s/epoch - 111us/sample
Epoch 85/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6015e-04 - 11s/epoch - 112us/sample
Epoch 86/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 107us/sample
Epoch 87/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5669e-04 - 11s/epoch - 111us/sample
Epoch 88/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 89/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 90/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.5969e-04 - 11s/epoch - 111us/sample
Epoch 91/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7526e-04 - 10s/epoch - 110us/sample
Epoch 92/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6543e-04 - 11s/epoch - 112us/sample
Epoch 93/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4370e-04 - 11s/epoch - 111us/sample
Epoch 94/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 95/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5440e-04 - 11s/epoch - 111us/sample
Epoch 96/145
95501/95501 - 11s - loss: 9.9763e-04 - val_loss: 9.4614e-04 - 11s/epoch - 112us/sample
Epoch 97/145
95501/95501 - 10s - loss: 9.9650e-04 - val_loss: 9.8489e-04 - 10s/epoch - 109us/sample
Epoch 98/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5537e-04 - 11s/epoch - 112us/sample
Epoch 99/145
95501/95501 - 11s - loss: 9.9147e-04 - val_loss: 9.4858e-04 - 11s/epoch - 111us/sample
Epoch 100/145
95501/95501 - 10s - loss: 9.8820e-04 - val_loss: 9.4483e-04 - 10s/epoch - 109us/sample
Epoch 101/145
95501/95501 - 10s - loss: 9.8932e-04 - val_loss: 9.3734e-04 - 10s/epoch - 108us/sample
Epoch 102/145
95501/95501 - 10s - loss: 9.8767e-04 - val_loss: 9.8280e-04 - 10s/epoch - 109us/sample
Epoch 103/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4507e-04 - 10s/epoch - 110us/sample
Epoch 104/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4621e-04 - 10s/epoch - 110us/sample
Epoch 105/145
95501/95501 - 10s - loss: 9.8653e-04 - val_loss: 9.9952e-04 - 10s/epoch - 108us/sample
Epoch 106/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.9979e-04 - 11s/epoch - 110us/sample
Epoch 107/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.4560e-04 - 10s/epoch - 109us/sample
Epoch 108/145
95501/95501 - 11s - loss: 9.9079e-04 - val_loss: 9.9101e-04 - 11s/epoch - 110us/sample
Epoch 109/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 110/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.5196e-04 - 10s/epoch - 108us/sample
Epoch 111/145
95501/95501 - 10s - loss: 9.9539e-04 - val_loss: 9.5332e-04 - 10s/epoch - 110us/sample
Epoch 112/145
95501/95501 - 10s - loss: 9.8953e-04 - val_loss: 9.4932e-04 - 10s/epoch - 109us/sample
Epoch 113/145
95501/95501 - 11s - loss: 9.8526e-04 - val_loss: 9.4171e-04 - 11s/epoch - 110us/sample
Epoch 114/145
95501/95501 - 10s - loss: 9.8188e-04 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 115/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.4903e-04 - 10s/epoch - 109us/sample
Epoch 116/145
95501/95501 - 11s - loss: 9.8756e-04 - val_loss: 9.3664e-04 - 11s/epoch - 112us/sample
Epoch 117/145
95501/95501 - 10s - loss: 9.8194e-04 - val_loss: 9.3207e-04 - 10s/epoch - 108us/sample
Epoch 118/145
95501/95501 - 10s - loss: 9.7287e-04 - val_loss: 9.3302e-04 - 10s/epoch - 108us/sample
Epoch 119/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4426e-04 - 10s/epoch - 110us/sample
Epoch 120/145
95501/95501 - 11s - loss: 9.8234e-04 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 121/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4046e-04 - 10s/epoch - 108us/sample
Epoch 122/145
95501/95501 - 11s - loss: 9.8120e-04 - val_loss: 9.5007e-04 - 11s/epoch - 111us/sample
Epoch 123/145
95501/95501 - 10s - loss: 9.9615e-04 - val_loss: 9.3173e-04 - 10s/epoch - 109us/sample
Epoch 124/145
95501/95501 - 11s - loss: 9.7026e-04 - val_loss: 9.6703e-04 - 11s/epoch - 112us/sample
Epoch 125/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7937e-04 - 10s/epoch - 107us/sample
Epoch 126/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.2718e-04 - 10s/epoch - 110us/sample
Epoch 127/145
95501/95501 - 10s - loss: 9.7451e-04 - val_loss: 9.7654e-04 - 10s/epoch - 109us/sample
Epoch 128/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.5197e-04 - 10s/epoch - 109us/sample
Epoch 129/145
95501/95501 - 10s - loss: 9.8274e-04 - val_loss: 9.2981e-04 - 10s/epoch - 108us/sample
Epoch 130/145
95501/95501 - 11s - loss: 9.7253e-04 - val_loss: 9.1982e-04 - 11s/epoch - 114us/sample
Epoch 131/145
95501/95501 - 11s - loss: 9.6715e-04 - val_loss: 9.2307e-04 - 11s/epoch - 110us/sample
Epoch 132/145
95501/95501 - 11s - loss: 9.7527e-04 - val_loss: 9.3004e-04 - 11s/epoch - 111us/sample
Epoch 133/145
95501/95501 - 10s - loss: 9.7864e-04 - val_loss: 9.1935e-04 - 10s/epoch - 107us/sample
Epoch 134/145
95501/95501 - 11s - loss: 9.6331e-04 - val_loss: 9.2160e-04 - 11s/epoch - 110us/sample
Epoch 135/145
95501/95501 - 10s - loss: 9.6415e-04 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 136/145
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 137/145
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.2527e-04 - 10s/epoch - 109us/sample
Epoch 138/145
95501/95501 - 11s - loss: 9.7116e-04 - val_loss: 9.2460e-04 - 11s/epoch - 111us/sample
Epoch 139/145
95501/95501 - 11s - loss: 9.6677e-04 - val_loss: 0.0011 - 11s/epoch - 110us/sample
Epoch 140/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.2139e-04 - 11s/epoch - 111us/sample
Epoch 141/145
95501/95501 - 10s - loss: 9.6754e-04 - val_loss: 9.1931e-04 - 10s/epoch - 109us/sample
Epoch 142/145
95501/95501 - 10s - loss: 9.8193e-04 - val_loss: 9.8543e-04 - 10s/epoch - 109us/sample
Epoch 143/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.1728e-04 - 11s/epoch - 110us/sample
Epoch 144/145
95501/95501 - 11s - loss: 9.6433e-04 - val_loss: 9.1769e-04 - 11s/epoch - 110us/sample
Epoch 145/145
95501/95501 - 10s - loss: 9.5903e-04 - val_loss: 9.1085e-04 - 10s/epoch - 109us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0009108473789419023
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 14:51:57.843390: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:423 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005963038811807672
cosine 0.00470592511787623
MAE: 0.011784878
RMSE: 0.022669123
r2: 0.9666633627611598
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 64, 145, 0.0012, 0.3, 379, 0.0009590256946571109, 0.0009108473789419023, 0.005963038811807672, 0.00470592511787623, 0.011784877628087997, 0.022669123485684395, 0.9666633627611598, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1896)        7584        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1896)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-14 14:52:03.961951: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/batch_normalization_3/gamma/m/Assign' id:2186 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/batch_normalization_3/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/batch_normalization_3/gamma/m, training_2/Adam/batch_normalization_3/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 14:52:20.265603: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1719 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 17s - loss: 0.0097 - val_loss: 0.0055 - 17s/epoch - 183us/sample
Epoch 2/150
95501/95501 - 16s - loss: 0.0048 - val_loss: 0.0034 - 16s/epoch - 169us/sample
Epoch 3/150
95501/95501 - 16s - loss: 0.0036 - val_loss: 0.0026 - 16s/epoch - 169us/sample
Epoch 4/150
95501/95501 - 16s - loss: 0.0027 - val_loss: 0.0022 - 16s/epoch - 170us/sample
Epoch 5/150
95501/95501 - 16s - loss: 0.0023 - val_loss: 0.0019 - 16s/epoch - 166us/sample
Epoch 6/150
95501/95501 - 16s - loss: 0.0020 - val_loss: 0.0017 - 16s/epoch - 169us/sample
Epoch 7/150
95501/95501 - 17s - loss: 0.0019 - val_loss: 0.0017 - 17s/epoch - 175us/sample
Epoch 8/150
95501/95501 - 16s - loss: 0.0018 - val_loss: 0.0015 - 16s/epoch - 169us/sample
Epoch 9/150
95501/95501 - 16s - loss: 0.0017 - val_loss: 0.0015 - 16s/epoch - 172us/sample
Epoch 10/150
95501/95501 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 170us/sample
Epoch 11/150
95501/95501 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 168us/sample
Epoch 12/150
95501/95501 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 168us/sample
Epoch 13/150
95501/95501 - 16s - loss: 0.0015 - val_loss: 0.0013 - 16s/epoch - 168us/sample
Epoch 14/150
95501/95501 - 16s - loss: 0.0015 - val_loss: 0.0013 - 16s/epoch - 167us/sample
Epoch 15/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0013 - 16s/epoch - 172us/sample
Epoch 16/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0013 - 16s/epoch - 167us/sample
Epoch 17/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0012 - 16s/epoch - 171us/sample
Epoch 18/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0013 - 16s/epoch - 170us/sample
Epoch 19/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0012 - 16s/epoch - 169us/sample
Epoch 20/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0012 - 16s/epoch - 169us/sample
Epoch 21/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0012 - 16s/epoch - 170us/sample
Epoch 22/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0012 - 16s/epoch - 169us/sample
Epoch 23/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0012 - 16s/epoch - 167us/sample
Epoch 24/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0012 - 16s/epoch - 169us/sample
Epoch 25/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0012 - 16s/epoch - 170us/sample
Epoch 26/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 168us/sample
Epoch 27/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 170us/sample
Epoch 28/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0012 - 16s/epoch - 170us/sample
Epoch 29/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 168us/sample
Epoch 30/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 169us/sample
Epoch 31/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 166us/sample
Epoch 32/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 169us/sample
Epoch 33/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 170us/sample
Epoch 34/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 164us/sample
Epoch 35/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 169us/sample
Epoch 36/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 170us/sample
Epoch 37/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 168us/sample
Epoch 38/150
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0012 - 17s/epoch - 173us/sample
Epoch 39/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 166us/sample
Epoch 40/150
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 173us/sample
Epoch 41/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 169us/sample
Epoch 42/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 172us/sample
Epoch 43/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 172us/sample
Epoch 44/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 167us/sample
Epoch 45/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 167us/sample
Epoch 46/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 171us/sample
Epoch 47/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 166us/sample
Epoch 48/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 170us/sample
Epoch 49/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 172us/sample
Epoch 50/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 168us/sample
Epoch 51/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 172us/sample
Epoch 52/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 166us/sample
Epoch 53/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 170us/sample
Epoch 54/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 168us/sample
Epoch 55/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 166us/sample
Epoch 56/150
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 175us/sample
Epoch 57/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 171us/sample
Epoch 58/150
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 173us/sample
Epoch 59/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 168us/sample
Epoch 60/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 166us/sample
Epoch 61/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 173us/sample
Epoch 62/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 173us/sample
Epoch 63/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 173us/sample
Epoch 64/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 171us/sample
Epoch 65/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 66/150
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0011 - 17s/epoch - 173us/sample
Epoch 67/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 169us/sample
Epoch 68/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 166us/sample
Epoch 69/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 168us/sample
Epoch 70/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 170us/sample
Epoch 71/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 172us/sample
Epoch 72/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 170us/sample
Epoch 73/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 167us/sample
Epoch 74/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 171us/sample
Epoch 75/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 167us/sample
Epoch 76/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 77/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 165us/sample
Epoch 78/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 79/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 171us/sample
Epoch 80/150
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 173us/sample
Epoch 81/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 171us/sample
Epoch 82/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 83/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 167us/sample
Epoch 84/150
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 173us/sample
Epoch 85/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 172us/sample
Epoch 86/150
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 173us/sample
Epoch 87/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 88/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 167us/sample
Epoch 89/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 171us/sample
Epoch 90/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 170us/sample
Epoch 91/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 168us/sample
Epoch 92/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 93/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 167us/sample
Epoch 94/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9697e-04 - 16s/epoch - 171us/sample
Epoch 95/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9998e-04 - 16s/epoch - 166us/sample
Epoch 96/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9485e-04 - 16s/epoch - 164us/sample
Epoch 97/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9798e-04 - 16s/epoch - 166us/sample
Epoch 98/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9097e-04 - 16s/epoch - 170us/sample
Epoch 99/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9887e-04 - 16s/epoch - 170us/sample
Epoch 100/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 101/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9306e-04 - 16s/epoch - 164us/sample
Epoch 102/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7894e-04 - 16s/epoch - 168us/sample
Epoch 103/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0011 - 16s/epoch - 167us/sample
Epoch 104/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9012e-04 - 16s/epoch - 166us/sample
Epoch 105/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8331e-04 - 16s/epoch - 166us/sample
Epoch 106/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8226e-04 - 16s/epoch - 165us/sample
Epoch 107/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9374e-04 - 16s/epoch - 170us/sample
Epoch 108/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 171us/sample
Epoch 109/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9457e-04 - 16s/epoch - 167us/sample
Epoch 110/150
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.8907e-04 - 17s/epoch - 174us/sample
Epoch 111/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 169us/sample
Epoch 112/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9368e-04 - 16s/epoch - 166us/sample
Epoch 113/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9134e-04 - 16s/epoch - 170us/sample
Epoch 114/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0011 - 16s/epoch - 166us/sample
Epoch 115/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9285e-04 - 16s/epoch - 171us/sample
Epoch 116/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8649e-04 - 16s/epoch - 172us/sample
Epoch 117/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0011 - 16s/epoch - 165us/sample
Epoch 118/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7736e-04 - 16s/epoch - 166us/sample
Epoch 119/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7444e-04 - 16s/epoch - 168us/sample
Epoch 120/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8343e-04 - 16s/epoch - 172us/sample
Epoch 121/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8314e-04 - 16s/epoch - 168us/sample
Epoch 122/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8491e-04 - 16s/epoch - 171us/sample
Epoch 123/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9922e-04 - 16s/epoch - 170us/sample
Epoch 124/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7319e-04 - 16s/epoch - 165us/sample
Epoch 125/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7854e-04 - 16s/epoch - 169us/sample
Epoch 126/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8121e-04 - 16s/epoch - 167us/sample
Epoch 127/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8137e-04 - 16s/epoch - 165us/sample
Epoch 128/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8409e-04 - 16s/epoch - 169us/sample
Epoch 129/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 0.0010 - 16s/epoch - 168us/sample
Epoch 130/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8013e-04 - 16s/epoch - 166us/sample
Epoch 131/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7678e-04 - 16s/epoch - 166us/sample
Epoch 132/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.6744e-04 - 16s/epoch - 168us/sample
Epoch 133/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7289e-04 - 16s/epoch - 169us/sample
Epoch 134/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7512e-04 - 16s/epoch - 168us/sample
Epoch 135/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7818e-04 - 16s/epoch - 163us/sample
Epoch 136/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9412e-04 - 16s/epoch - 168us/sample
Epoch 137/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7599e-04 - 16s/epoch - 166us/sample
Epoch 138/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8398e-04 - 16s/epoch - 168us/sample
Epoch 139/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7393e-04 - 16s/epoch - 167us/sample
Epoch 140/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.6346e-04 - 16s/epoch - 162us/sample
Epoch 141/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8419e-04 - 16s/epoch - 170us/sample
Epoch 142/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.9868e-04 - 16s/epoch - 167us/sample
Epoch 143/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7491e-04 - 16s/epoch - 167us/sample
Epoch 144/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8256e-04 - 16s/epoch - 166us/sample
Epoch 145/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.6751e-04 - 16s/epoch - 166us/sample
Epoch 146/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.7098e-04 - 16s/epoch - 167us/sample
Epoch 147/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.6261e-04 - 16s/epoch - 172us/sample
Epoch 148/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8690e-04 - 16s/epoch - 166us/sample
Epoch 149/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.8786e-04 - 16s/epoch - 167us/sample
Epoch 150/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.6274e-04 - 16s/epoch - 169us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0009627443166253783
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 15:32:23.766256: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1690 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006635370886510065
cosine 0.005237700571395566
MAE: 0.012438438
RMSE: 0.023897955
r2: 0.9629514006563281
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 32, 150, 0.001, 0.3, 379, 0.00107020008775889, 0.0009627443166253783, 0.006635370886510065, 0.005237700571395566, 0.012438437901437283, 0.02389795519411564, 0.9629514006563281, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 70 0.0005 16 0] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1896)        7584        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1896)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/70
2023-02-14 15:32:30.370420: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/dense_dec1_2/bias/m/Assign' id:3572 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/dense_dec1_2/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/dense_dec1_2/bias/m, training_4/Adam/dense_dec1_2/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 15:32:58.546301: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2999 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 30s - loss: 1.4942 - val_loss: 1.4360 - 30s/epoch - 311us/sample
Epoch 2/70
95501/95501 - 28s - loss: 1.4280 - val_loss: 1.4253 - 28s/epoch - 294us/sample
Epoch 3/70
95501/95501 - 28s - loss: 1.4232 - val_loss: 1.4196 - 28s/epoch - 296us/sample
Epoch 4/70
95501/95501 - 28s - loss: 1.4205 - val_loss: 1.4181 - 28s/epoch - 297us/sample
Epoch 5/70
95501/95501 - 28s - loss: 1.4173 - val_loss: 1.4171 - 28s/epoch - 298us/sample
Epoch 6/70
95501/95501 - 27s - loss: 1.4156 - val_loss: 1.4169 - 27s/epoch - 287us/sample
Epoch 7/70
95501/95501 - 28s - loss: 1.4150 - val_loss: 1.4167 - 28s/epoch - 298us/sample
Epoch 8/70
95501/95501 - 28s - loss: 1.4143 - val_loss: 1.4174 - 28s/epoch - 295us/sample
Epoch 9/70
95501/95501 - 28s - loss: 1.4162 - val_loss: 1.4401 - 28s/epoch - 294us/sample
Epoch 10/70
95501/95501 - 28s - loss: 1.1946 - val_loss: 1.0398 - 28s/epoch - 298us/sample
Epoch 11/70
95501/95501 - 29s - loss: 0.9997 - val_loss: 0.9560 - 29s/epoch - 301us/sample
Epoch 12/70
95501/95501 - 29s - loss: 0.9054 - val_loss: 0.8295 - 29s/epoch - 301us/sample
Epoch 13/70
95501/95501 - 28s - loss: 0.8024 - val_loss: 0.9209 - 28s/epoch - 291us/sample
Epoch 14/70
95501/95501 - 28s - loss: 0.7295 - val_loss: 0.6986 - 28s/epoch - 295us/sample
Epoch 15/70
95501/95501 - 28s - loss: 0.6963 - val_loss: 0.6978 - 28s/epoch - 294us/sample
Epoch 16/70
95501/95501 - 28s - loss: 0.6749 - val_loss: 0.6543 - 28s/epoch - 293us/sample
Epoch 17/70
95501/95501 - 28s - loss: 0.6505 - val_loss: 0.6317 - 28s/epoch - 288us/sample
Epoch 18/70
95501/95501 - 28s - loss: 0.6261 - val_loss: 0.5957 - 28s/epoch - 298us/sample
Epoch 19/70
95501/95501 - 28s - loss: 0.6158 - val_loss: 0.5953 - 28s/epoch - 295us/sample
Epoch 20/70
95501/95501 - 29s - loss: 0.5935 - val_loss: 0.5587 - 29s/epoch - 299us/sample
Epoch 21/70
95501/95501 - 28s - loss: 0.5391 - val_loss: 0.5375 - 28s/epoch - 294us/sample
Epoch 22/70
95501/95501 - 28s - loss: 0.5305 - val_loss: 0.5279 - 28s/epoch - 292us/sample
Epoch 23/70
95501/95501 - 28s - loss: 0.5247 - val_loss: 0.5263 - 28s/epoch - 295us/sample
Epoch 24/70
95501/95501 - 28s - loss: 0.5209 - val_loss: 0.5166 - 28s/epoch - 295us/sample
Epoch 25/70
95501/95501 - 29s - loss: 0.5109 - val_loss: 0.5091 - 29s/epoch - 301us/sample
Epoch 26/70
95501/95501 - 28s - loss: 0.5078 - val_loss: 0.5087 - 28s/epoch - 293us/sample
Epoch 27/70
95501/95501 - 28s - loss: 0.5077 - val_loss: 0.5101 - 28s/epoch - 295us/sample
Epoch 28/70
95501/95501 - 28s - loss: 0.5075 - val_loss: 0.5080 - 28s/epoch - 297us/sample
Epoch 29/70
95501/95501 - 29s - loss: 0.5069 - val_loss: 0.5083 - 29s/epoch - 300us/sample
Epoch 30/70
95501/95501 - 29s - loss: 0.5073 - val_loss: 0.5081 - 29s/epoch - 308us/sample
Epoch 31/70
95501/95501 - 29s - loss: 0.5069 - val_loss: 0.5079 - 29s/epoch - 308us/sample
Epoch 32/70
95501/95501 - 25s - loss: 0.5062 - val_loss: 0.5073 - 25s/epoch - 260us/sample
Epoch 33/70
95501/95501 - 25s - loss: 0.5070 - val_loss: 0.5089 - 25s/epoch - 257us/sample
Epoch 34/70
95501/95501 - 25s - loss: 0.5065 - val_loss: 0.5075 - 25s/epoch - 259us/sample
Epoch 35/70
95501/95501 - 25s - loss: 0.5064 - val_loss: 0.5066 - 25s/epoch - 259us/sample
Epoch 36/70
95501/95501 - 25s - loss: 0.5065 - val_loss: 0.5083 - 25s/epoch - 259us/sample
Epoch 37/70
95501/95501 - 25s - loss: 0.5065 - val_loss: 0.5080 - 25s/epoch - 260us/sample
Epoch 38/70
95501/95501 - 25s - loss: 0.5067 - val_loss: 0.5066 - 25s/epoch - 259us/sample
Epoch 39/70
95501/95501 - 25s - loss: 0.5065 - val_loss: 0.5076 - 25s/epoch - 261us/sample
Epoch 40/70
95501/95501 - 25s - loss: 0.5074 - val_loss: 0.5071 - 25s/epoch - 261us/sample
Epoch 41/70
95501/95501 - 25s - loss: 0.5032 - val_loss: 0.5025 - 25s/epoch - 259us/sample
Epoch 42/70
95501/95501 - 25s - loss: 0.5017 - val_loss: 0.5021 - 25s/epoch - 259us/sample
Epoch 43/70
95501/95501 - 25s - loss: 0.5015 - val_loss: 0.5037 - 25s/epoch - 257us/sample
Epoch 44/70
95501/95501 - 25s - loss: 0.5011 - val_loss: 0.5008 - 25s/epoch - 261us/sample
Epoch 45/70
95501/95501 - 25s - loss: 0.5010 - val_loss: 0.5019 - 25s/epoch - 260us/sample
Epoch 46/70
95501/95501 - 25s - loss: 0.5010 - val_loss: 0.5013 - 25s/epoch - 259us/sample
Epoch 47/70
95501/95501 - 25s - loss: 0.5010 - val_loss: 0.5019 - 25s/epoch - 257us/sample
Epoch 48/70
95501/95501 - 24s - loss: 0.5026 - val_loss: 0.5020 - 24s/epoch - 254us/sample
Epoch 49/70
95501/95501 - 24s - loss: 0.5023 - val_loss: 0.5017 - 24s/epoch - 253us/sample
Epoch 50/70
95501/95501 - 24s - loss: 0.5009 - val_loss: 0.5017 - 24s/epoch - 254us/sample
Epoch 51/70
95501/95501 - 24s - loss: 0.5014 - val_loss: 0.5017 - 24s/epoch - 253us/sample
Epoch 52/70
95501/95501 - 24s - loss: 0.5008 - val_loss: 0.5014 - 24s/epoch - 253us/sample
Epoch 53/70
95501/95501 - 24s - loss: 0.5010 - val_loss: 0.5009 - 24s/epoch - 252us/sample
Epoch 54/70
95501/95501 - 25s - loss: 0.5008 - val_loss: 0.5010 - 25s/epoch - 257us/sample
Epoch 55/70
95501/95501 - 24s - loss: 0.5011 - val_loss: 0.5011 - 24s/epoch - 254us/sample
Epoch 56/70
95501/95501 - 24s - loss: 0.5007 - val_loss: 0.5014 - 24s/epoch - 252us/sample
Epoch 57/70
95501/95501 - 24s - loss: 0.5009 - val_loss: 0.5008 - 24s/epoch - 254us/sample
Epoch 58/70
95501/95501 - 24s - loss: 0.5012 - val_loss: 0.5014 - 24s/epoch - 253us/sample
Epoch 59/70
95501/95501 - 24s - loss: 0.5013 - val_loss: 0.5017 - 24s/epoch - 254us/sample
Epoch 60/70
95501/95501 - 24s - loss: 0.5010 - val_loss: 0.5016 - 24s/epoch - 252us/sample
Epoch 61/70
95501/95501 - 24s - loss: 0.5012 - val_loss: 0.5009 - 24s/epoch - 254us/sample
Epoch 62/70
95501/95501 - 24s - loss: 0.5007 - val_loss: 0.5014 - 24s/epoch - 254us/sample
Epoch 63/70
95501/95501 - 24s - loss: 0.5006 - val_loss: 0.5008 - 24s/epoch - 255us/sample
Epoch 64/70
95501/95501 - 24s - loss: 0.5008 - val_loss: 0.5017 - 24s/epoch - 255us/sample
Epoch 65/70
95501/95501 - 24s - loss: 0.5011 - val_loss: 0.5016 - 24s/epoch - 255us/sample
Epoch 66/70
95501/95501 - 24s - loss: 0.5005 - val_loss: 0.5018 - 24s/epoch - 256us/sample
Epoch 67/70
95501/95501 - 24s - loss: 0.5008 - val_loss: 0.5011 - 24s/epoch - 253us/sample
Epoch 68/70
95501/95501 - 25s - loss: 0.5000 - val_loss: 0.5006 - 25s/epoch - 257us/sample
Epoch 69/70
95501/95501 - 28s - loss: 0.4998 - val_loss: 0.5007 - 28s/epoch - 293us/sample
Epoch 70/70
95501/95501 - 31s - loss: 0.4978 - val_loss: 0.4984 - 31s/epoch - 320us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.4984314044691058
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 16:03:13.323912: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2951 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.2556122289826589
cosine 0.3140700104026088
MAE: 0.12118752
RMSE: 0.18989627
r2: -1.3375948707492122
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'binary_crossentropy', 16, 70, 0.0005, 0.3, 379, 0.49784905152665826, 0.4984314044691058, 0.2556122289826589, 0.3140700104026088, 0.12118752300739288, 0.18989627063274384, -1.3375948707492122, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 3160)        12640       ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 3160)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          1198019     ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          1198019     ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5354480     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 11,760,558
Trainable params: 11,747,160
Non-trainable params: 13,398
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 16:03:21.581717: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/bottleneck_zmean_3/kernel/m/Assign' id:4837 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/bottleneck_zmean_3/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/bottleneck_zmean_3/kernel/m, training_6/Adam/bottleneck_zmean_3/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 16:03:34.934974: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4330 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 15s - loss: 0.0075 - val_loss: 0.0042 - 15s/epoch - 152us/sample
Epoch 2/50
95501/95501 - 13s - loss: 0.0053 - val_loss: 0.0038 - 13s/epoch - 133us/sample
Epoch 3/50
95501/95501 - 12s - loss: 0.0031 - val_loss: 0.0024 - 12s/epoch - 130us/sample
Epoch 4/50
95501/95501 - 13s - loss: 0.0024 - val_loss: 0.0019 - 13s/epoch - 133us/sample
Epoch 5/50
95501/95501 - 13s - loss: 0.0101 - val_loss: 0.0084 - 13s/epoch - 132us/sample
Epoch 6/50
95501/95501 - 13s - loss: 0.0022 - val_loss: 0.0022 - 13s/epoch - 131us/sample
Epoch 7/50
95501/95501 - 13s - loss: 0.0019 - val_loss: 0.0018 - 13s/epoch - 135us/sample
Epoch 8/50
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0014 - 12s/epoch - 122us/sample
Epoch 9/50
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0013 - 12s/epoch - 122us/sample
Epoch 10/50
95501/95501 - 12s - loss: 0.0013 - val_loss: 0.0016 - 12s/epoch - 122us/sample
Epoch 11/50
95501/95501 - 12s - loss: 0.0015 - val_loss: 0.0013 - 12s/epoch - 122us/sample
Epoch 12/50
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0014 - 12s/epoch - 122us/sample
Epoch 13/50
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0012 - 12s/epoch - 123us/sample
Epoch 14/50
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0018 - 12s/epoch - 121us/sample
Epoch 15/50
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.9275e-04 - 12s/epoch - 120us/sample
Epoch 16/50
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5533e-04 - 11s/epoch - 116us/sample
Epoch 17/50
95501/95501 - 12s - loss: 9.6557e-04 - val_loss: 9.1667e-04 - 12s/epoch - 122us/sample
Epoch 18/50
95501/95501 - 11s - loss: 9.2111e-04 - val_loss: 8.5226e-04 - 11s/epoch - 120us/sample
Epoch 19/50
95501/95501 - 11s - loss: 9.0694e-04 - val_loss: 8.3213e-04 - 11s/epoch - 120us/sample
Epoch 20/50
95501/95501 - 12s - loss: 8.6273e-04 - val_loss: 8.1328e-04 - 12s/epoch - 121us/sample
Epoch 21/50
95501/95501 - 11s - loss: 8.4853e-04 - val_loss: 7.9317e-04 - 11s/epoch - 118us/sample
Epoch 22/50
95501/95501 - 11s - loss: 8.2725e-04 - val_loss: 7.8756e-04 - 11s/epoch - 113us/sample
Epoch 23/50
95501/95501 - 12s - loss: 8.1238e-04 - val_loss: 7.7886e-04 - 12s/epoch - 121us/sample
Epoch 24/50
95501/95501 - 11s - loss: 7.9697e-04 - val_loss: 7.6888e-04 - 11s/epoch - 118us/sample
Epoch 25/50
95501/95501 - 11s - loss: 7.9235e-04 - val_loss: 7.7369e-04 - 11s/epoch - 119us/sample
Epoch 26/50
95501/95501 - 12s - loss: 7.7662e-04 - val_loss: 7.3234e-04 - 12s/epoch - 123us/sample
Epoch 27/50
95501/95501 - 11s - loss: 7.6726e-04 - val_loss: 7.2588e-04 - 11s/epoch - 119us/sample
Epoch 28/50
95501/95501 - 11s - loss: 7.6450e-04 - val_loss: 0.0012 - 11s/epoch - 120us/sample
Epoch 29/50
95501/95501 - 11s - loss: 7.8027e-04 - val_loss: 7.2045e-04 - 11s/epoch - 120us/sample
Epoch 30/50
95501/95501 - 11s - loss: 7.5044e-04 - val_loss: 7.1250e-04 - 11s/epoch - 118us/sample
Epoch 31/50
95501/95501 - 11s - loss: 7.4279e-04 - val_loss: 7.1758e-04 - 11s/epoch - 120us/sample
Epoch 32/50
95501/95501 - 12s - loss: 7.3653e-04 - val_loss: 7.0260e-04 - 12s/epoch - 121us/sample
Epoch 33/50
95501/95501 - 11s - loss: 7.3209e-04 - val_loss: 7.2568e-04 - 11s/epoch - 119us/sample
Epoch 34/50
95501/95501 - 12s - loss: 7.4632e-04 - val_loss: 7.0550e-04 - 12s/epoch - 120us/sample
Epoch 35/50
95501/95501 - 11s - loss: 7.3261e-04 - val_loss: 6.9438e-04 - 11s/epoch - 120us/sample
Epoch 36/50
95501/95501 - 11s - loss: 7.2489e-04 - val_loss: 7.6020e-04 - 11s/epoch - 120us/sample
Epoch 37/50
95501/95501 - 11s - loss: 7.5719e-04 - val_loss: 6.9257e-04 - 11s/epoch - 120us/sample
Epoch 38/50
95501/95501 - 11s - loss: 7.1741e-04 - val_loss: 6.9005e-04 - 11s/epoch - 119us/sample
Epoch 39/50
95501/95501 - 11s - loss: 7.1242e-04 - val_loss: 6.9955e-04 - 11s/epoch - 119us/sample
Epoch 40/50
95501/95501 - 11s - loss: 7.0979e-04 - val_loss: 6.7552e-04 - 11s/epoch - 120us/sample
Epoch 41/50
95501/95501 - 11s - loss: 7.0750e-04 - val_loss: 7.3695e-04 - 11s/epoch - 118us/sample
Epoch 42/50
95501/95501 - 11s - loss: 7.4980e-04 - val_loss: 8.3920e-04 - 11s/epoch - 120us/sample
Epoch 43/50
95501/95501 - 12s - loss: 8.1268e-04 - val_loss: 6.8590e-04 - 12s/epoch - 122us/sample
Epoch 44/50
95501/95501 - 11s - loss: 7.1354e-04 - val_loss: 6.8040e-04 - 11s/epoch - 118us/sample
Epoch 45/50
95501/95501 - 12s - loss: 7.0409e-04 - val_loss: 6.8867e-04 - 12s/epoch - 121us/sample
Epoch 46/50
95501/95501 - 11s - loss: 7.1421e-04 - val_loss: 7.0948e-04 - 11s/epoch - 119us/sample
Epoch 47/50
95501/95501 - 11s - loss: 7.1868e-04 - val_loss: 6.8077e-04 - 11s/epoch - 119us/sample
Epoch 48/50
95501/95501 - 11s - loss: 6.9443e-04 - val_loss: 6.6909e-04 - 11s/epoch - 119us/sample
Epoch 49/50
95501/95501 - 11s - loss: 6.9502e-04 - val_loss: 6.6579e-04 - 11s/epoch - 117us/sample
Epoch 50/50
95501/95501 - 12s - loss: 7.0289e-04 - val_loss: 0.0011 - 12s/epoch - 123us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0010791799522897028
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 16:13:04.163680: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4294 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01860739404172031
cosine 0.014784021098749439
MAE: 0.030837683
RMSE: 0.04049005
r2: 0.893686683729724
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 64, 50, 0.0005, 0.3, 379, 0.0007028919322595096, 0.0010791799522897028, 0.01860739404172031, 0.014784021098749439, 0.030837683007121086, 0.0404900498688221, 0.893686683729724, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 110 0.0005 8 0] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 2528)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          958491      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          958491      ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4312944     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,437,958
Trainable params: 9,427,088
Non-trainable params: 10,870
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/110
2023-02-14 16:13:11.798606: W tensorflow/c/c_api.cc:291] Operation '{name:'training_8/Adam/batch_normalization_13/gamma/v/Assign' id:6333 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/batch_normalization_13/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/batch_normalization_13/gamma/v, training_8/Adam/batch_normalization_13/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 16:14:12.439220: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5636 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 64s - loss: 1.4362 - val_loss: 1.4313 - 64s/epoch - 667us/sample
Epoch 2/110
95501/95501 - 60s - loss: 1.4228 - val_loss: 1.5086 - 60s/epoch - 628us/sample
Epoch 3/110
95501/95501 - 60s - loss: 1.4206 - val_loss: 1.4197 - 60s/epoch - 625us/sample
Epoch 4/110
95501/95501 - 60s - loss: 1.4180 - val_loss: 1.4235 - 60s/epoch - 627us/sample
Epoch 5/110
95501/95501 - 60s - loss: 1.4207 - val_loss: 1.4739 - 60s/epoch - 627us/sample
Epoch 6/110
95501/95501 - 60s - loss: 1.4191 - val_loss: 1.4234 - 60s/epoch - 627us/sample
Epoch 7/110
95501/95501 - 60s - loss: 1.4174 - val_loss: 1.4178 - 60s/epoch - 627us/sample
Epoch 8/110
95501/95501 - 60s - loss: 1.4158 - val_loss: 1.4184 - 60s/epoch - 628us/sample
Epoch 9/110
95501/95501 - 60s - loss: 1.4173 - val_loss: 1.4198 - 60s/epoch - 625us/sample
Epoch 10/110
95501/95501 - 60s - loss: 1.4172 - val_loss: 1.4178 - 60s/epoch - 629us/sample
Epoch 11/110
95501/95501 - 60s - loss: 1.4183 - val_loss: 1.4199 - 60s/epoch - 625us/sample
Epoch 12/110
95501/95501 - 60s - loss: 1.4176 - val_loss: 1.4176 - 60s/epoch - 626us/sample
Epoch 13/110
95501/95501 - 60s - loss: 1.4182 - val_loss: 1.4211 - 60s/epoch - 631us/sample
Epoch 14/110
95501/95501 - 59s - loss: 1.4182 - val_loss: 1.4201 - 59s/epoch - 622us/sample
Epoch 15/110
95501/95501 - 60s - loss: 1.4195 - val_loss: 1.4199 - 60s/epoch - 625us/sample
Epoch 16/110
95501/95501 - 60s - loss: 1.4200 - val_loss: 1.4232 - 60s/epoch - 627us/sample
Epoch 17/110
95501/95501 - 60s - loss: 1.4185 - val_loss: 1.4198 - 60s/epoch - 629us/sample
Epoch 18/110
95501/95501 - 60s - loss: 1.4189 - val_loss: 1.4188 - 60s/epoch - 631us/sample
Epoch 19/110
95501/95501 - 60s - loss: 1.4174 - val_loss: 1.4182 - 60s/epoch - 629us/sample
Epoch 20/110
95501/95501 - 59s - loss: 1.4192 - val_loss: 1.4183 - 59s/epoch - 622us/sample
Epoch 21/110
95501/95501 - 59s - loss: 1.4172 - val_loss: 1.4178 - 59s/epoch - 622us/sample
Epoch 22/110
95501/95501 - 61s - loss: 1.4239 - val_loss: 1.4246 - 61s/epoch - 637us/sample
Epoch 23/110
95501/95501 - 61s - loss: 1.4236 - val_loss: 1.4262 - 61s/epoch - 635us/sample
Epoch 24/110
95501/95501 - 60s - loss: 1.4247 - val_loss: 1.4241 - 60s/epoch - 633us/sample
Epoch 25/110
95501/95501 - 59s - loss: 1.4272 - val_loss: 1.4352 - 59s/epoch - 617us/sample
Epoch 26/110
95501/95501 - 59s - loss: 1.4241 - val_loss: 1.4240 - 59s/epoch - 617us/sample
Epoch 27/110
95501/95501 - 59s - loss: 1.4241 - val_loss: 1.4250 - 59s/epoch - 619us/sample
Epoch 28/110
95501/95501 - 60s - loss: 1.4240 - val_loss: 1.4283 - 60s/epoch - 627us/sample
Epoch 29/110
95501/95501 - 60s - loss: 1.4249 - val_loss: 1.4243 - 60s/epoch - 625us/sample
Epoch 30/110
95501/95501 - 60s - loss: 1.4235 - val_loss: 1.4240 - 60s/epoch - 632us/sample
Epoch 31/110
95501/95501 - 60s - loss: 1.4230 - val_loss: 1.4234 - 60s/epoch - 626us/sample
Epoch 32/110
95501/95501 - 61s - loss: 1.4237 - val_loss: 1.4249 - 61s/epoch - 635us/sample
Epoch 33/110
95501/95501 - 60s - loss: 1.4240 - val_loss: 1.4242 - 60s/epoch - 631us/sample
Epoch 34/110
95501/95501 - 60s - loss: 1.4230 - val_loss: 1.4240 - 60s/epoch - 632us/sample
Epoch 35/110
95501/95501 - 60s - loss: 1.4224 - val_loss: 1.4233 - 60s/epoch - 630us/sample
Epoch 36/110
95501/95501 - 60s - loss: 1.4239 - val_loss: 1.4242 - 60s/epoch - 626us/sample
Epoch 37/110
95501/95501 - 60s - loss: 1.4231 - val_loss: 1.4241 - 60s/epoch - 623us/sample
Epoch 38/110
95501/95501 - 59s - loss: 1.4251 - val_loss: 1.4402 - 59s/epoch - 619us/sample
Epoch 39/110
95501/95501 - 61s - loss: 1.4239 - val_loss: 1.4240 - 61s/epoch - 636us/sample
Epoch 40/110
95501/95501 - 61s - loss: 1.4230 - val_loss: 1.4237 - 61s/epoch - 636us/sample
Epoch 41/110
95501/95501 - 60s - loss: 1.4231 - val_loss: 1.4236 - 60s/epoch - 632us/sample
Epoch 42/110
95501/95501 - 60s - loss: 1.4228 - val_loss: 1.4239 - 60s/epoch - 633us/sample
Epoch 43/110
95501/95501 - 60s - loss: 1.4231 - val_loss: 1.4248 - 60s/epoch - 625us/sample
Epoch 44/110
95501/95501 - 60s - loss: 1.4242 - val_loss: 1.4235 - 60s/epoch - 626us/sample
Epoch 45/110
95501/95501 - 60s - loss: 1.4232 - val_loss: 1.4232 - 60s/epoch - 624us/sample
Epoch 46/110
95501/95501 - 59s - loss: 1.4225 - val_loss: 1.4248 - 59s/epoch - 621us/sample
Epoch 47/110
95501/95501 - 60s - loss: 1.4226 - val_loss: 1.4243 - 60s/epoch - 623us/sample
Epoch 48/110
95501/95501 - 60s - loss: 1.4230 - val_loss: 1.4234 - 60s/epoch - 628us/sample
Epoch 49/110
95501/95501 - 59s - loss: 1.4245 - val_loss: 1.4256 - 59s/epoch - 622us/sample
Epoch 50/110
95501/95501 - 60s - loss: 1.4247 - val_loss: 1.4270 - 60s/epoch - 629us/sample
Epoch 51/110
95501/95501 - 60s - loss: 1.4245 - val_loss: 1.4254 - 60s/epoch - 623us/sample
Epoch 52/110
95501/95501 - 59s - loss: 1.4243 - val_loss: 1.4257 - 59s/epoch - 620us/sample
Epoch 53/110
95501/95501 - 60s - loss: 1.4255 - val_loss: 1.4258 - 60s/epoch - 632us/sample
Epoch 54/110
95501/95501 - 60s - loss: 1.4244 - val_loss: 1.4261 - 60s/epoch - 630us/sample
Epoch 55/110
95501/95501 - 60s - loss: 1.4263 - val_loss: 1.4263 - 60s/epoch - 626us/sample
Epoch 56/110
95501/95501 - 60s - loss: 1.4245 - val_loss: 1.4256 - 60s/epoch - 629us/sample
Epoch 57/110
95501/95501 - 60s - loss: 1.4249 - val_loss: 1.4296 - 60s/epoch - 625us/sample
Epoch 58/110
95501/95501 - 60s - loss: 1.4245 - val_loss: 1.4256 - 60s/epoch - 624us/sample
Epoch 59/110
95501/95501 - 60s - loss: 1.4255 - val_loss: 1.4259 - 60s/epoch - 625us/sample
Epoch 60/110
95501/95501 - 60s - loss: 1.4244 - val_loss: 1.4255 - 60s/epoch - 625us/sample
Epoch 61/110
95501/95501 - 60s - loss: 1.4244 - val_loss: 1.4257 - 60s/epoch - 629us/sample
Epoch 62/110
95501/95501 - 60s - loss: 1.4255 - val_loss: 1.4273 - 60s/epoch - 626us/sample
Epoch 63/110
95501/95501 - 60s - loss: 1.4244 - val_loss: 1.4256 - 60s/epoch - 629us/sample
Epoch 64/110
95501/95501 - 61s - loss: 1.4245 - val_loss: 1.4257 - 61s/epoch - 636us/sample
Epoch 65/110
95501/95501 - 60s - loss: 1.4272 - val_loss: 1.4263 - 60s/epoch - 627us/sample
Epoch 66/110
95501/95501 - 59s - loss: 1.4258 - val_loss: 1.4259 - 59s/epoch - 622us/sample
Epoch 67/110
95501/95501 - 60s - loss: 1.4275 - val_loss: 1.4425 - 60s/epoch - 625us/sample
Epoch 68/110
95501/95501 - 60s - loss: 1.4262 - val_loss: 1.4262 - 60s/epoch - 630us/sample
Epoch 69/110
95501/95501 - 62s - loss: 1.4246 - val_loss: 1.4261 - 62s/epoch - 645us/sample
Epoch 70/110
95501/95501 - 61s - loss: 1.4263 - val_loss: 1.4261 - 61s/epoch - 638us/sample
Epoch 71/110
95501/95501 - 60s - loss: 1.4246 - val_loss: 1.4258 - 60s/epoch - 633us/sample
Epoch 72/110
95501/95501 - 61s - loss: 1.4251 - val_loss: 1.4258 - 61s/epoch - 638us/sample
Epoch 73/110
95501/95501 - 60s - loss: 1.4251 - val_loss: 1.4256 - 60s/epoch - 631us/sample
Epoch 74/110
95501/95501 - 61s - loss: 1.4250 - val_loss: 1.4257 - 61s/epoch - 639us/sample
Epoch 75/110
95501/95501 - 61s - loss: 1.4249 - val_loss: 1.4261 - 61s/epoch - 635us/sample
Epoch 76/110
95501/95501 - 60s - loss: 1.4250 - val_loss: 1.4260 - 60s/epoch - 631us/sample
Epoch 77/110
95501/95501 - 61s - loss: 1.4243 - val_loss: 1.4269 - 61s/epoch - 636us/sample
Epoch 78/110
95501/95501 - 61s - loss: 1.4256 - val_loss: 1.4259 - 61s/epoch - 639us/sample
Epoch 79/110
95501/95501 - 61s - loss: 1.4245 - val_loss: 1.4257 - 61s/epoch - 636us/sample
Epoch 80/110
95501/95501 - 60s - loss: 1.4256 - val_loss: 1.4255 - 60s/epoch - 630us/sample
Epoch 81/110
95501/95501 - 61s - loss: 1.4248 - val_loss: 1.4271 - 61s/epoch - 644us/sample
Epoch 82/110
95501/95501 - 60s - loss: 1.4247 - val_loss: 1.4260 - 60s/epoch - 633us/sample
Epoch 83/110
95501/95501 - 61s - loss: 1.4251 - val_loss: 1.4265 - 61s/epoch - 640us/sample
Epoch 84/110
95501/95501 - 61s - loss: 1.4246 - val_loss: 1.4260 - 61s/epoch - 642us/sample
Epoch 85/110
95501/95501 - 61s - loss: 1.4248 - val_loss: 1.4257 - 61s/epoch - 637us/sample
Epoch 86/110
95501/95501 - 61s - loss: 1.4251 - val_loss: 1.4257 - 61s/epoch - 641us/sample
Epoch 87/110
95501/95501 - 60s - loss: 1.4246 - val_loss: 1.4263 - 60s/epoch - 626us/sample
Epoch 88/110
95501/95501 - 48s - loss: 1.4247 - val_loss: 1.4255 - 48s/epoch - 501us/sample
Epoch 89/110
95501/95501 - 48s - loss: 1.4242 - val_loss: 1.4256 - 48s/epoch - 503us/sample
Epoch 90/110
95501/95501 - 48s - loss: 1.4245 - val_loss: 1.4260 - 48s/epoch - 500us/sample
Epoch 91/110
95501/95501 - 48s - loss: 1.4246 - val_loss: 1.4255 - 48s/epoch - 498us/sample
Epoch 92/110
95501/95501 - 48s - loss: 1.4246 - val_loss: 1.4257 - 48s/epoch - 500us/sample
Epoch 93/110
95501/95501 - 48s - loss: 1.4246 - val_loss: 1.4256 - 48s/epoch - 501us/sample
Epoch 94/110
95501/95501 - 48s - loss: 1.4250 - val_loss: 1.4305 - 48s/epoch - 498us/sample
Epoch 95/110
95501/95501 - 48s - loss: 1.4262 - val_loss: 1.4255 - 48s/epoch - 500us/sample
Epoch 96/110
95501/95501 - 48s - loss: 1.4245 - val_loss: 1.4255 - 48s/epoch - 498us/sample
Epoch 97/110
95501/95501 - 48s - loss: 1.4243 - val_loss: 1.4276 - 48s/epoch - 500us/sample
Epoch 98/110
95501/95501 - 48s - loss: 1.4262 - val_loss: 1.4255 - 48s/epoch - 499us/sample
Epoch 99/110
95501/95501 - 48s - loss: 1.4246 - val_loss: 1.4257 - 48s/epoch - 499us/sample
Epoch 100/110
95501/95501 - 48s - loss: 1.4266 - val_loss: 1.4257 - 48s/epoch - 500us/sample
Epoch 101/110
95501/95501 - 48s - loss: 1.4258 - val_loss: 1.4388 - 48s/epoch - 501us/sample
Epoch 102/110
95501/95501 - 48s - loss: 1.4253 - val_loss: 1.4255 - 48s/epoch - 501us/sample
Epoch 103/110
95501/95501 - 48s - loss: 1.4243 - val_loss: 1.4261 - 48s/epoch - 498us/sample
Epoch 104/110
95501/95501 - 48s - loss: 1.4245 - val_loss: 1.4256 - 48s/epoch - 502us/sample
Epoch 105/110
95501/95501 - 48s - loss: 1.4244 - val_loss: 1.4255 - 48s/epoch - 503us/sample
Epoch 106/110
95501/95501 - 48s - loss: 1.4244 - val_loss: 1.4255 - 48s/epoch - 502us/sample
Epoch 107/110
95501/95501 - 48s - loss: 1.4248 - val_loss: 1.4260 - 48s/epoch - 503us/sample
Epoch 108/110
95501/95501 - 48s - loss: 1.4245 - val_loss: 1.4256 - 48s/epoch - 502us/sample
Epoch 109/110
95501/95501 - 48s - loss: 1.4249 - val_loss: 1.4263 - 48s/epoch - 502us/sample
Epoch 110/110
95501/95501 - 48s - loss: 1.4246 - val_loss: 1.4259 - 48s/epoch - 502us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 1.4258509627657479
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 17:58:44.304756: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:5588 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7948182125075733
cosine 0.8801374723040475
MAE: 19.85564
RMSE: 73.788414
r2: -353195.2045532516
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'binary_crossentropy', 8, 110, 0.0005, 0.3, 379, 1.4246344105315916, 1.4258509627657479, 0.7948182125075733, 0.8801374723040475, 19.855640411376953, 73.78841400146484, -353195.2045532516, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 170 0.0005 16 0] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 2528)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          958491      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          958491      ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4312944     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,437,958
Trainable params: 9,427,088
Non-trainable params: 10,870
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/170
2023-02-14 17:58:51.911522: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_17/gamma/Assign' id:6770 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_17/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_17/gamma, batch_normalization_17/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 17:59:17.801277: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:6982 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 28s - loss: 1.5062 - val_loss: 1.4710 - 28s/epoch - 291us/sample
Epoch 2/170
95501/95501 - 26s - loss: 1.4632 - val_loss: 1.4500 - 26s/epoch - 275us/sample
Epoch 3/170
95501/95501 - 26s - loss: 1.4494 - val_loss: 1.4489 - 26s/epoch - 273us/sample
Epoch 4/170
95501/95501 - 26s - loss: 1.5458 - val_loss: 1.7852 - 26s/epoch - 272us/sample
Epoch 5/170
95501/95501 - 26s - loss: 1.4310 - val_loss: 8.7153 - 26s/epoch - 272us/sample
Epoch 6/170
95501/95501 - 26s - loss: 1.4201 - val_loss: 1.4234 - 26s/epoch - 274us/sample
Epoch 7/170
95501/95501 - 26s - loss: 1.4219 - val_loss: 1.4227 - 26s/epoch - 272us/sample
Epoch 8/170
95501/95501 - 26s - loss: 1.4228 - val_loss: 1.4291 - 26s/epoch - 273us/sample
Epoch 9/170
95501/95501 - 26s - loss: 1.4185 - val_loss: 1.4183 - 26s/epoch - 273us/sample
Epoch 10/170
95501/95501 - 26s - loss: 1.4183 - val_loss: 1.4166 - 26s/epoch - 273us/sample
Epoch 11/170
95501/95501 - 26s - loss: 1.4163 - val_loss: 1.4994 - 26s/epoch - 274us/sample
Epoch 12/170
95501/95501 - 26s - loss: 1.4156 - val_loss: 1.4169 - 26s/epoch - 272us/sample
Epoch 13/170
95501/95501 - 26s - loss: 1.4146 - val_loss: 1.4152 - 26s/epoch - 274us/sample
Epoch 14/170
95501/95501 - 26s - loss: 1.4138 - val_loss: 1.4150 - 26s/epoch - 273us/sample
Epoch 15/170
95501/95501 - 26s - loss: 1.4128 - val_loss: 1.4168 - 26s/epoch - 274us/sample
Epoch 16/170
95501/95501 - 26s - loss: 1.4132 - val_loss: 1.4133 - 26s/epoch - 274us/sample
Epoch 17/170
95501/95501 - 26s - loss: 1.4130 - val_loss: 1.4150 - 26s/epoch - 272us/sample
Epoch 18/170
95501/95501 - 26s - loss: 1.4134 - val_loss: 1.4144 - 26s/epoch - 271us/sample
Epoch 19/170
95501/95501 - 26s - loss: 1.4124 - val_loss: 1.4168 - 26s/epoch - 275us/sample
Epoch 20/170
95501/95501 - 26s - loss: 1.4137 - val_loss: 1.4171 - 26s/epoch - 273us/sample
Epoch 21/170
95501/95501 - 26s - loss: 1.4135 - val_loss: 1.4136 - 26s/epoch - 274us/sample
Epoch 22/170
95501/95501 - 26s - loss: 1.4131 - val_loss: 1.4157 - 26s/epoch - 272us/sample
Epoch 23/170
95501/95501 - 26s - loss: 1.4157 - val_loss: 1.4210 - 26s/epoch - 273us/sample
Epoch 24/170
95501/95501 - 26s - loss: 1.4185 - val_loss: 1.4155 - 26s/epoch - 274us/sample
Epoch 25/170
95501/95501 - 26s - loss: 1.4153 - val_loss: 1.4168 - 26s/epoch - 272us/sample
Epoch 26/170
95501/95501 - 26s - loss: 1.4138 - val_loss: 1.4165 - 26s/epoch - 272us/sample
Epoch 27/170
95501/95501 - 26s - loss: 1.4136 - val_loss: 1.4152 - 26s/epoch - 275us/sample
Epoch 28/170
95501/95501 - 26s - loss: 1.4126 - val_loss: 1.4161 - 26s/epoch - 273us/sample
Epoch 29/170
95501/95501 - 26s - loss: 1.4136 - val_loss: 1.4143 - 26s/epoch - 272us/sample
Epoch 30/170
95501/95501 - 26s - loss: 1.4118 - val_loss: 1.4130 - 26s/epoch - 274us/sample
Epoch 31/170
95501/95501 - 26s - loss: 1.4131 - val_loss: 1.4137 - 26s/epoch - 273us/sample
Epoch 32/170
95501/95501 - 26s - loss: 1.4115 - val_loss: 1.4127 - 26s/epoch - 272us/sample
Epoch 33/170
95501/95501 - 26s - loss: 1.4114 - val_loss: 1.4134 - 26s/epoch - 273us/sample
Epoch 34/170
95501/95501 - 26s - loss: 1.4117 - val_loss: 1.4126 - 26s/epoch - 271us/sample
Epoch 35/170
95501/95501 - 26s - loss: 1.4119 - val_loss: 1.4134 - 26s/epoch - 272us/sample
Epoch 36/170
95501/95501 - 26s - loss: 1.4119 - val_loss: 1.4185 - 26s/epoch - 274us/sample
Epoch 37/170
95501/95501 - 26s - loss: 1.4160 - val_loss: 1.4162 - 26s/epoch - 272us/sample
Epoch 38/170
95501/95501 - 26s - loss: 1.4132 - val_loss: 1.4144 - 26s/epoch - 273us/sample
Epoch 39/170
95501/95501 - 26s - loss: 1.4135 - val_loss: 1.4143 - 26s/epoch - 274us/sample
Epoch 40/170
95501/95501 - 26s - loss: 1.4139 - val_loss: 1.4140 - 26s/epoch - 273us/sample
Epoch 41/170
95501/95501 - 26s - loss: 1.4165 - val_loss: 1.4141 - 26s/epoch - 272us/sample
Epoch 42/170
95501/95501 - 26s - loss: 1.4119 - val_loss: 1.4127 - 26s/epoch - 273us/sample
Epoch 43/170
95501/95501 - 26s - loss: 1.4121 - val_loss: 1.4130 - 26s/epoch - 273us/sample
Epoch 44/170
95501/95501 - 26s - loss: 1.4111 - val_loss: 1.4119 - 26s/epoch - 272us/sample
Epoch 45/170
95501/95501 - 26s - loss: 1.4108 - val_loss: 1.4144 - 26s/epoch - 275us/sample
Epoch 46/170
95501/95501 - 26s - loss: 1.4109 - val_loss: 1.4135 - 26s/epoch - 273us/sample
Epoch 47/170
95501/95501 - 26s - loss: 1.4108 - val_loss: 1.4125 - 26s/epoch - 273us/sample
Epoch 48/170
95501/95501 - 26s - loss: 1.4108 - val_loss: 1.4124 - 26s/epoch - 273us/sample
Epoch 49/170
95501/95501 - 26s - loss: 1.4150 - val_loss: 1.4135 - 26s/epoch - 273us/sample
Epoch 50/170
95501/95501 - 26s - loss: 1.4118 - val_loss: 1.4139 - 26s/epoch - 273us/sample
Epoch 51/170
95501/95501 - 26s - loss: 1.4115 - val_loss: 1.4172 - 26s/epoch - 274us/sample
Epoch 52/170
95501/95501 - 26s - loss: 1.4117 - val_loss: 1.4125 - 26s/epoch - 273us/sample
Epoch 53/170
95501/95501 - 26s - loss: 1.4110 - val_loss: 1.4128 - 26s/epoch - 273us/sample
Epoch 54/170
95501/95501 - 26s - loss: 1.4107 - val_loss: 1.4130 - 26s/epoch - 274us/sample
Epoch 55/170
95501/95501 - 26s - loss: 1.4112 - val_loss: 1.4131 - 26s/epoch - 272us/sample
Epoch 56/170
95501/95501 - 26s - loss: 1.4123 - val_loss: 1.4127 - 26s/epoch - 272us/sample
Epoch 57/170
95501/95501 - 26s - loss: 1.4109 - val_loss: 1.4127 - 26s/epoch - 275us/sample
Epoch 58/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4116 - 26s/epoch - 272us/sample
Epoch 59/170
95501/95501 - 26s - loss: 1.4112 - val_loss: 1.4125 - 26s/epoch - 274us/sample
Epoch 60/170
95501/95501 - 26s - loss: 1.4115 - val_loss: 1.4124 - 26s/epoch - 274us/sample
Epoch 61/170
95501/95501 - 26s - loss: 1.4109 - val_loss: 1.4118 - 26s/epoch - 272us/sample
Epoch 62/170
95501/95501 - 26s - loss: 1.4304 - val_loss: 1.4147 - 26s/epoch - 274us/sample
Epoch 63/170
95501/95501 - 26s - loss: 1.4132 - val_loss: 1.4141 - 26s/epoch - 273us/sample
Epoch 64/170
95501/95501 - 26s - loss: 1.4125 - val_loss: 1.4124 - 26s/epoch - 273us/sample
Epoch 65/170
95501/95501 - 26s - loss: 1.4111 - val_loss: 1.4124 - 26s/epoch - 273us/sample
Epoch 66/170
95501/95501 - 26s - loss: 1.4106 - val_loss: 1.4115 - 26s/epoch - 272us/sample
Epoch 67/170
95501/95501 - 26s - loss: 1.4115 - val_loss: 1.4135 - 26s/epoch - 273us/sample
Epoch 68/170
95501/95501 - 26s - loss: 1.4116 - val_loss: 1.4122 - 26s/epoch - 274us/sample
Epoch 69/170
95501/95501 - 26s - loss: 1.4108 - val_loss: 1.4126 - 26s/epoch - 273us/sample
Epoch 70/170
95501/95501 - 26s - loss: 1.4118 - val_loss: 1.4139 - 26s/epoch - 275us/sample
Epoch 71/170
95501/95501 - 26s - loss: 1.4111 - val_loss: 1.4125 - 26s/epoch - 275us/sample
Epoch 72/170
95501/95501 - 26s - loss: 1.4106 - val_loss: 1.4153 - 26s/epoch - 274us/sample
Epoch 73/170
95501/95501 - 26s - loss: 1.4110 - val_loss: 1.4118 - 26s/epoch - 275us/sample
Epoch 74/170
95501/95501 - 26s - loss: 1.4106 - val_loss: 1.4125 - 26s/epoch - 274us/sample
Epoch 75/170
95501/95501 - 26s - loss: 1.4112 - val_loss: 1.4120 - 26s/epoch - 274us/sample
Epoch 76/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4116 - 26s/epoch - 273us/sample
Epoch 77/170
95501/95501 - 26s - loss: 1.4109 - val_loss: 1.4126 - 26s/epoch - 275us/sample
Epoch 78/170
95501/95501 - 26s - loss: 1.4108 - val_loss: 1.4123 - 26s/epoch - 274us/sample
Epoch 79/170
95501/95501 - 26s - loss: 1.4106 - val_loss: 1.4205 - 26s/epoch - 275us/sample
Epoch 80/170
95501/95501 - 26s - loss: 1.4125 - val_loss: 1.4130 - 26s/epoch - 275us/sample
Epoch 81/170
95501/95501 - 26s - loss: 1.4107 - val_loss: 1.4136 - 26s/epoch - 275us/sample
Epoch 82/170
95501/95501 - 26s - loss: 1.4146 - val_loss: 1.4125 - 26s/epoch - 275us/sample
Epoch 83/170
95501/95501 - 26s - loss: 1.4118 - val_loss: 1.4127 - 26s/epoch - 275us/sample
Epoch 84/170
95501/95501 - 26s - loss: 1.4124 - val_loss: 1.4133 - 26s/epoch - 275us/sample
Epoch 85/170
95501/95501 - 26s - loss: 1.4108 - val_loss: 1.4123 - 26s/epoch - 273us/sample
Epoch 86/170
95501/95501 - 26s - loss: 1.4109 - val_loss: 1.4131 - 26s/epoch - 275us/sample
Epoch 87/170
95501/95501 - 26s - loss: 1.4117 - val_loss: 1.4126 - 26s/epoch - 275us/sample
Epoch 88/170
95501/95501 - 26s - loss: 1.4112 - val_loss: 1.4126 - 26s/epoch - 275us/sample
Epoch 89/170
95501/95501 - 26s - loss: 1.4109 - val_loss: 1.4122 - 26s/epoch - 276us/sample
Epoch 90/170
95501/95501 - 26s - loss: 1.4113 - val_loss: 1.4224 - 26s/epoch - 274us/sample
Epoch 91/170
95501/95501 - 26s - loss: 1.4115 - val_loss: 1.4127 - 26s/epoch - 275us/sample
Epoch 92/170
95501/95501 - 26s - loss: 1.4105 - val_loss: 1.4121 - 26s/epoch - 276us/sample
Epoch 93/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4115 - 26s/epoch - 276us/sample
Epoch 94/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4119 - 26s/epoch - 274us/sample
Epoch 95/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4120 - 26s/epoch - 275us/sample
Epoch 96/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4123 - 26s/epoch - 275us/sample
Epoch 97/170
95501/95501 - 26s - loss: 1.4104 - val_loss: 1.4116 - 26s/epoch - 274us/sample
Epoch 98/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4122 - 26s/epoch - 276us/sample
Epoch 99/170
95501/95501 - 26s - loss: 1.4099 - val_loss: 1.4117 - 26s/epoch - 276us/sample
Epoch 100/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4143 - 26s/epoch - 276us/sample
Epoch 101/170
95501/95501 - 26s - loss: 1.4112 - val_loss: 1.4120 - 26s/epoch - 275us/sample
Epoch 102/170
95501/95501 - 26s - loss: 1.4104 - val_loss: 1.4121 - 26s/epoch - 276us/sample
Epoch 103/170
95501/95501 - 26s - loss: 1.4111 - val_loss: 1.4123 - 26s/epoch - 275us/sample
Epoch 104/170
95501/95501 - 26s - loss: 1.4103 - val_loss: 1.4115 - 26s/epoch - 275us/sample
Epoch 105/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4114 - 26s/epoch - 276us/sample
Epoch 106/170
95501/95501 - 26s - loss: 1.4099 - val_loss: 1.4114 - 26s/epoch - 275us/sample
Epoch 107/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4121 - 26s/epoch - 275us/sample
Epoch 108/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4116 - 26s/epoch - 276us/sample
Epoch 109/170
95501/95501 - 26s - loss: 1.4103 - val_loss: 1.4116 - 26s/epoch - 275us/sample
Epoch 110/170
95501/95501 - 26s - loss: 1.4105 - val_loss: 1.4117 - 26s/epoch - 275us/sample
Epoch 111/170
95501/95501 - 26s - loss: 1.4101 - val_loss: 1.4116 - 26s/epoch - 276us/sample
Epoch 112/170
95501/95501 - 26s - loss: 1.4105 - val_loss: 1.4128 - 26s/epoch - 275us/sample
Epoch 113/170
95501/95501 - 26s - loss: 1.4106 - val_loss: 1.4118 - 26s/epoch - 274us/sample
Epoch 114/170
95501/95501 - 27s - loss: 1.4101 - val_loss: 1.4138 - 27s/epoch - 278us/sample
Epoch 115/170
95501/95501 - 26s - loss: 1.4103 - val_loss: 1.4114 - 26s/epoch - 277us/sample
Epoch 116/170
95501/95501 - 26s - loss: 1.4103 - val_loss: 1.4125 - 26s/epoch - 277us/sample
Epoch 117/170
95501/95501 - 26s - loss: 1.4106 - val_loss: 1.4120 - 26s/epoch - 275us/sample
Epoch 118/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4115 - 26s/epoch - 272us/sample
Epoch 119/170
95501/95501 - 26s - loss: 1.4098 - val_loss: 1.4113 - 26s/epoch - 274us/sample
Epoch 120/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4114 - 26s/epoch - 275us/sample
Epoch 121/170
95501/95501 - 26s - loss: 1.4099 - val_loss: 1.4115 - 26s/epoch - 273us/sample
Epoch 122/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4118 - 26s/epoch - 272us/sample
Epoch 123/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4116 - 26s/epoch - 274us/sample
Epoch 124/170
95501/95501 - 26s - loss: 1.4099 - val_loss: 1.4114 - 26s/epoch - 272us/sample
Epoch 125/170
95501/95501 - 26s - loss: 1.4099 - val_loss: 1.4115 - 26s/epoch - 273us/sample
Epoch 126/170
95501/95501 - 26s - loss: 1.4095 - val_loss: 1.4112 - 26s/epoch - 273us/sample
Epoch 127/170
95501/95501 - 26s - loss: 1.4098 - val_loss: 1.4113 - 26s/epoch - 272us/sample
Epoch 128/170
95501/95501 - 26s - loss: 1.4093 - val_loss: 1.4109 - 26s/epoch - 273us/sample
Epoch 129/170
95501/95501 - 26s - loss: 1.4093 - val_loss: 1.4109 - 26s/epoch - 275us/sample
Epoch 130/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4126 - 26s/epoch - 273us/sample
Epoch 131/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4112 - 26s/epoch - 272us/sample
Epoch 132/170
95501/95501 - 26s - loss: 1.4096 - val_loss: 1.4108 - 26s/epoch - 274us/sample
Epoch 133/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4111 - 26s/epoch - 271us/sample
Epoch 134/170
95501/95501 - 26s - loss: 1.4093 - val_loss: 1.4109 - 26s/epoch - 273us/sample
Epoch 135/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4117 - 26s/epoch - 274us/sample
Epoch 136/170
95501/95501 - 26s - loss: 1.4102 - val_loss: 1.4119 - 26s/epoch - 272us/sample
Epoch 137/170
95501/95501 - 26s - loss: 1.4103 - val_loss: 1.4118 - 26s/epoch - 273us/sample
Epoch 138/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4117 - 26s/epoch - 273us/sample
Epoch 139/170
95501/95501 - 26s - loss: 1.4098 - val_loss: 1.4112 - 26s/epoch - 272us/sample
Epoch 140/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4112 - 26s/epoch - 274us/sample
Epoch 141/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4113 - 26s/epoch - 273us/sample
Epoch 142/170
95501/95501 - 26s - loss: 1.4100 - val_loss: 1.4113 - 26s/epoch - 271us/sample
Epoch 143/170
95501/95501 - 26s - loss: 1.4096 - val_loss: 1.4111 - 26s/epoch - 274us/sample
Epoch 144/170
95501/95501 - 26s - loss: 1.4095 - val_loss: 1.4114 - 26s/epoch - 273us/sample
Epoch 145/170
95501/95501 - 26s - loss: 1.4099 - val_loss: 1.4112 - 26s/epoch - 273us/sample
Epoch 146/170
95501/95501 - 26s - loss: 1.4099 - val_loss: 1.4139 - 26s/epoch - 273us/sample
Epoch 147/170
95501/95501 - 26s - loss: 1.4103 - val_loss: 1.4113 - 26s/epoch - 274us/sample
Epoch 148/170
95501/95501 - 26s - loss: 1.4095 - val_loss: 1.4110 - 26s/epoch - 271us/sample
Epoch 149/170
95501/95501 - 26s - loss: 1.4096 - val_loss: 1.4114 - 26s/epoch - 272us/sample
Epoch 150/170
95501/95501 - 26s - loss: 1.4095 - val_loss: 1.4111 - 26s/epoch - 273us/sample
Epoch 151/170
95501/95501 - 26s - loss: 1.4094 - val_loss: 1.4117 - 26s/epoch - 272us/sample
Epoch 152/170
95501/95501 - 26s - loss: 1.4093 - val_loss: 1.4112 - 26s/epoch - 273us/sample
Epoch 153/170
95501/95501 - 26s - loss: 1.4097 - val_loss: 1.4119 - 26s/epoch - 272us/sample
Epoch 154/170
95501/95501 - 26s - loss: 1.4096 - val_loss: 1.4111 - 26s/epoch - 272us/sample
Epoch 155/170
95501/95501 - 26s - loss: 1.4093 - val_loss: 1.4112 - 26s/epoch - 273us/sample
Epoch 156/170
95501/95501 - 26s - loss: 1.4092 - val_loss: 1.4108 - 26s/epoch - 273us/sample
Epoch 157/170
95501/95501 - 26s - loss: 1.4091 - val_loss: 1.4111 - 26s/epoch - 272us/sample
Epoch 158/170
95501/95501 - 26s - loss: 1.4092 - val_loss: 1.4107 - 26s/epoch - 273us/sample
Epoch 159/170
95501/95501 - 26s - loss: 1.4092 - val_loss: 1.4109 - 26s/epoch - 273us/sample
Epoch 160/170
95501/95501 - 26s - loss: 1.4092 - val_loss: 1.4114 - 26s/epoch - 271us/sample
Epoch 161/170
95501/95501 - 26s - loss: 1.4090 - val_loss: 1.4107 - 26s/epoch - 274us/sample
Epoch 162/170
95501/95501 - 26s - loss: 1.4092 - val_loss: 1.4106 - 26s/epoch - 275us/sample
Epoch 163/170
95501/95501 - 26s - loss: 1.4094 - val_loss: 1.4113 - 26s/epoch - 273us/sample
Epoch 164/170
95501/95501 - 26s - loss: 1.4101 - val_loss: 1.4107 - 26s/epoch - 273us/sample
Epoch 165/170
95501/95501 - 26s - loss: 1.4091 - val_loss: 1.4107 - 26s/epoch - 274us/sample
Epoch 166/170
95501/95501 - 26s - loss: 1.4090 - val_loss: 1.4108 - 26s/epoch - 273us/sample
Epoch 167/170
95501/95501 - 26s - loss: 1.4090 - val_loss: 1.4108 - 26s/epoch - 273us/sample
Epoch 168/170
95501/95501 - 26s - loss: 1.4092 - val_loss: 1.4121 - 26s/epoch - 273us/sample
Epoch 169/170
95501/95501 - 26s - loss: 1.4093 - val_loss: 1.4111 - 26s/epoch - 274us/sample
Epoch 170/170
95501/95501 - 26s - loss: 1.4090 - val_loss: 1.4108 - 26s/epoch - 273us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 1.410819296651726
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:12:55.239764: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:6934 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7928088733323709
cosine 0.8831448722987019
MAE: 16.565235
RMSE: 62.469982
r2: -253151.36853027993
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'binary_crossentropy', 16, 170, 0.0005, 0.3, 379, 1.4090448475197594, 1.410819296651726, 0.7928088733323709, 0.8831448722987019, 16.565235137939453, 62.4699821472168, -253151.36853027993, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 10 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 2528)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          958491      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          958491      ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4312944     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,437,958
Trainable params: 9,427,088
Non-trainable params: 10,870
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 19:13:02.988143: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_6/kernel/Assign' id:7959 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_6/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_6/kernel, bottleneck_zmean_6/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:13:27.897431: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:8306 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 27s - loss: 0.0143 - val_loss: 0.0056 - 27s/epoch - 283us/sample
Epoch 2/10
95501/95501 - 25s - loss: 0.0050 - val_loss: 0.0035 - 25s/epoch - 259us/sample
Epoch 3/10
95501/95501 - 25s - loss: 0.0036 - val_loss: 0.0028 - 25s/epoch - 259us/sample
Epoch 4/10
95501/95501 - 25s - loss: 0.0029 - val_loss: 0.0025 - 25s/epoch - 260us/sample
Epoch 5/10
95501/95501 - 25s - loss: 0.0025 - val_loss: 0.0021 - 25s/epoch - 261us/sample
Epoch 6/10
95501/95501 - 25s - loss: 0.0023 - val_loss: 0.0020 - 25s/epoch - 259us/sample
Epoch 7/10
95501/95501 - 25s - loss: 0.0022 - val_loss: 0.0018 - 25s/epoch - 259us/sample
Epoch 8/10
95501/95501 - 25s - loss: 0.0021 - val_loss: 0.0018 - 25s/epoch - 260us/sample
Epoch 9/10
95501/95501 - 25s - loss: 0.0020 - val_loss: 0.0017 - 25s/epoch - 261us/sample
Epoch 10/10
95501/95501 - 25s - loss: 0.0020 - val_loss: 0.0017 - 25s/epoch - 260us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0016798372562978905
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:17:12.844937: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:8277 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014373734419273063
cosine 0.011449894431488404
MAE: 0.018961372
RMSE: 0.035711896
r2: 0.9172659561395327
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 16, 10, 0.001, 0.3, 379, 0.0019837526583595722, 0.0016798372562978905, 0.014373734419273063, 0.011449894431488404, 0.01896137185394764, 0.03571189567446709, 0.9172659561395327, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 1896)         0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/90
2023-02-14 19:17:20.464709: W tensorflow/c/c_api.cc:291] Operation '{name:'training_14/Adam/batch_normalization_23/beta/v/Assign' id:10231 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/batch_normalization_23/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/batch_normalization_23/beta/v, training_14/Adam/batch_normalization_23/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:17:35.454782: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:9567 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 17s - loss: 0.0114 - val_loss: 0.0054 - 17s/epoch - 176us/sample
Epoch 2/90
95501/95501 - 15s - loss: 0.0058 - val_loss: 2.8622 - 15s/epoch - 154us/sample
Epoch 3/90
95501/95501 - 15s - loss: 0.0040 - val_loss: 0.0031 - 15s/epoch - 152us/sample
Epoch 4/90
95501/95501 - 15s - loss: 0.0034 - val_loss: 0.0025 - 15s/epoch - 153us/sample
Epoch 5/90
95501/95501 - 15s - loss: 0.0026 - val_loss: 0.0021 - 15s/epoch - 152us/sample
Epoch 6/90
95501/95501 - 15s - loss: 0.0022 - val_loss: 0.0021 - 15s/epoch - 152us/sample
Epoch 7/90
95501/95501 - 15s - loss: 0.0020 - val_loss: 0.0017 - 15s/epoch - 153us/sample
Epoch 8/90
95501/95501 - 15s - loss: 0.0019 - val_loss: 0.0016 - 15s/epoch - 153us/sample
Epoch 9/90
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0015 - 14s/epoch - 152us/sample
Epoch 10/90
95501/95501 - 15s - loss: 0.0017 - val_loss: 0.0015 - 15s/epoch - 153us/sample
Epoch 11/90
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 152us/sample
Epoch 12/90
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 152us/sample
Epoch 13/90
95501/95501 - 15s - loss: 0.0016 - val_loss: 0.0014 - 15s/epoch - 152us/sample
Epoch 14/90
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 152us/sample
Epoch 15/90
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 153us/sample
Epoch 16/90
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 153us/sample
Epoch 17/90
95501/95501 - 15s - loss: 0.0015 - val_loss: 0.0013 - 15s/epoch - 152us/sample
Epoch 18/90
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 152us/sample
Epoch 19/90
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0013 - 15s/epoch - 152us/sample
Epoch 20/90
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 21/90
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 22/90
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 152us/sample
Epoch 23/90
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 152us/sample
Epoch 24/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 152us/sample
Epoch 25/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 152us/sample
Epoch 26/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 152us/sample
Epoch 27/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 28/90
95501/95501 - 15s - loss: 0.0014 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 29/90
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 151us/sample
Epoch 30/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 152us/sample
Epoch 31/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 32/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 33/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 34/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 152us/sample
Epoch 35/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 152us/sample
Epoch 36/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 37/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0012 - 15s/epoch - 152us/sample
Epoch 38/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 39/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 153us/sample
Epoch 40/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 41/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 42/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 43/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 44/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 154us/sample
Epoch 45/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 46/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 47/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 48/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0012 - 15s/epoch - 155us/sample
Epoch 49/90
95501/95501 - 15s - loss: 0.0013 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 50/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 51/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 52/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 53/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 54/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 55/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 56/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 57/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 58/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 59/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 60/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 61/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 62/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 63/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 64/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 65/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 153us/sample
Epoch 66/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 67/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 68/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 69/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 70/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 71/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 72/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 73/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 153us/sample
Epoch 74/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 155us/sample
Epoch 75/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 154us/sample
Epoch 76/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 77/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 78/90
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 79/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 80/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 155us/sample
Epoch 81/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 82/90
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 83/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 84/90
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 85/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0011 - 15s/epoch - 153us/sample
Epoch 86/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 87/90
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 88/90
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 89/90
95501/95501 - 15s - loss: 0.0011 - val_loss: 0.0010 - 15s/epoch - 154us/sample
Epoch 90/90
95501/95501 - 15s - loss: 0.0012 - val_loss: 0.0010 - 15s/epoch - 154us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0010267951374096306
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:39:20.508930: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:9538 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.007409505958921991
cosine 0.005867549912955596
MAE: 0.013045595
RMSE: 0.025231
r2: 0.9587038551705056
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 32, 90, 0.001, 0.3, 379, 0.0011614964751637028, 0.0010267951374096306, 0.007409505958921991, 0.005867549912955596, 0.013045595027506351, 0.025231000036001205, 0.9587038551705056, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 10 0.001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 632)         2528        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 632)          0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          239907      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          239907      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1188336     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,470,158
Trainable params: 2,466,872
Non-trainable params: 3,286
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 19:39:28.826633: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec0_8/kernel/Assign' id:10607 op device:{requested: '', assigned: ''} def:{{{node dense_dec0_8/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec0_8/kernel, dense_dec0_8/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:39:39.568678: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/mul' id:10827 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 13s - loss: 0.5908 - val_loss: 0.4944 - 13s/epoch - 132us/sample
Epoch 2/10
95501/95501 - 10s - loss: 0.4738 - val_loss: 0.3614 - 10s/epoch - 103us/sample
Epoch 3/10
95501/95501 - 10s - loss: 0.3841 - val_loss: 0.3513 - 10s/epoch - 104us/sample
Epoch 4/10
95501/95501 - 10s - loss: 0.3536 - val_loss: 0.3125 - 10s/epoch - 103us/sample
Epoch 5/10
95501/95501 - 10s - loss: 0.3373 - val_loss: 0.3065 - 10s/epoch - 104us/sample
Epoch 6/10
95501/95501 - 10s - loss: 0.3713 - val_loss: 0.2822 - 10s/epoch - 104us/sample
Epoch 7/10
95501/95501 - 10s - loss: 0.3200 - val_loss: 0.2874 - 10s/epoch - 104us/sample
Epoch 8/10
95501/95501 - 10s - loss: 0.3058 - val_loss: 0.2689 - 10s/epoch - 103us/sample
Epoch 9/10
95501/95501 - 10s - loss: 0.2885 - val_loss: 0.2609 - 10s/epoch - 104us/sample
Epoch 10/10
95501/95501 - 10s - loss: 0.2895 - val_loss: 0.2746 - 10s/epoch - 104us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.2746089708903939
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:41:09.844255: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_8/outputlayer/BiasAdd' id:10779 op device:{requested: '', assigned: ''} def:{{{node decoder_model_8/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_8/outputlayer/MatMul, decoder_model_8/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.20459918268909555
cosine 0.27957363107194005
MAE: 0.5079782
RMSE: 4.1387005
r2: -1109.212438126815
RMSE zero-vector: 0.23411466903540806
['0.5custom_VAE', 'binary_crossentropy', 64, 10, 0.001, 0.3, 379, 0.2895064005287436, 0.2746089708903939, 0.20459918268909555, 0.27957363107194005, 0.5079782009124756, 4.138700485229492, -1109.212438126815, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 1896)         0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/30
2023-02-14 19:41:18.098574: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/dense_dec1_9/bias/m/Assign' id:12640 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/dense_dec1_9/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/dense_dec1_9/bias/m, training_18/Adam/dense_dec1_9/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:42:04.040991: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_9/mul' id:12130 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 50s - loss: 0.0104 - val_loss: 0.0070 - 50s/epoch - 519us/sample
Epoch 2/30
95501/95501 - 46s - loss: 0.0056 - val_loss: 0.0078 - 46s/epoch - 485us/sample
Epoch 3/30
95501/95501 - 46s - loss: 0.0047 - val_loss: 0.0076 - 46s/epoch - 483us/sample
Epoch 4/30
95501/95501 - 46s - loss: 0.0043 - val_loss: 0.0065 - 46s/epoch - 483us/sample
Epoch 5/30
95501/95501 - 46s - loss: 0.0040 - val_loss: 0.0066 - 46s/epoch - 481us/sample
Epoch 6/30
95501/95501 - 46s - loss: 0.0038 - val_loss: 0.0060 - 46s/epoch - 485us/sample
Epoch 7/30
95501/95501 - 46s - loss: 0.0037 - val_loss: 0.0063 - 46s/epoch - 483us/sample
Epoch 8/30
95501/95501 - 46s - loss: 0.0036 - val_loss: 0.0070 - 46s/epoch - 484us/sample
Epoch 9/30
95501/95501 - 46s - loss: 0.0035 - val_loss: 0.0048 - 46s/epoch - 483us/sample
Epoch 10/30
95501/95501 - 46s - loss: 0.0034 - val_loss: 0.0047 - 46s/epoch - 484us/sample
Epoch 11/30
95501/95501 - 46s - loss: 0.0034 - val_loss: 0.0047 - 46s/epoch - 483us/sample
Epoch 12/30
95501/95501 - 46s - loss: 0.0034 - val_loss: 0.0044 - 46s/epoch - 483us/sample
Epoch 13/30
95501/95501 - 46s - loss: 0.0033 - val_loss: 0.0044 - 46s/epoch - 483us/sample
Epoch 14/30
95501/95501 - 46s - loss: 0.0032 - val_loss: 0.0054 - 46s/epoch - 485us/sample
Epoch 15/30
95501/95501 - 46s - loss: 0.0032 - val_loss: 0.0052 - 46s/epoch - 483us/sample
Epoch 16/30
95501/95501 - 46s - loss: 0.0032 - val_loss: 0.0045 - 46s/epoch - 482us/sample
Epoch 17/30
95501/95501 - 46s - loss: 0.0032 - val_loss: 0.0047 - 46s/epoch - 483us/sample
Epoch 18/30
95501/95501 - 46s - loss: 0.0032 - val_loss: 0.0046 - 46s/epoch - 486us/sample
Epoch 19/30
95501/95501 - 46s - loss: 0.0031 - val_loss: 0.0046 - 46s/epoch - 482us/sample
Epoch 20/30
95501/95501 - 46s - loss: 0.0031 - val_loss: 0.0047 - 46s/epoch - 486us/sample
Epoch 21/30
95501/95501 - 46s - loss: 0.0031 - val_loss: 0.0041 - 46s/epoch - 483us/sample
Epoch 22/30
95501/95501 - 46s - loss: 0.0031 - val_loss: 0.0042 - 46s/epoch - 482us/sample
Epoch 23/30
95501/95501 - 46s - loss: 0.0031 - val_loss: 0.0041 - 46s/epoch - 481us/sample
Epoch 24/30
95501/95501 - 46s - loss: 0.0030 - val_loss: 0.0044 - 46s/epoch - 486us/sample
Epoch 25/30
95501/95501 - 46s - loss: 0.0030 - val_loss: 0.0042 - 46s/epoch - 484us/sample
Epoch 26/30
95501/95501 - 46s - loss: 0.0030 - val_loss: 0.0037 - 46s/epoch - 486us/sample
Epoch 27/30
95501/95501 - 46s - loss: 0.0030 - val_loss: 0.0038 - 46s/epoch - 487us/sample
Epoch 28/30
95501/95501 - 46s - loss: 0.0030 - val_loss: 0.0043 - 46s/epoch - 486us/sample
Epoch 29/30
95501/95501 - 46s - loss: 0.0030 - val_loss: 0.0039 - 46s/epoch - 486us/sample
Epoch 30/30
95501/95501 - 47s - loss: 0.0030 - val_loss: 0.0044 - 47s/epoch - 487us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.004402804051698254
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:04:27.126744: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:12101 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.032837901274258355
cosine 0.026924348460518097
MAE: 0.027475206
RMSE: 0.063339785
r2: 0.7397369411998845
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 8, 30, 0.0005, 0.3, 379, 0.0029657541031084195, 0.004402804051698254, 0.032837901274258355, 0.026924348460518097, 0.027475206181406975, 0.06333978474140167, 0.7397369411998845, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 15 0.001 16 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 2528)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          958491      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          958491      ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4312944     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,437,958
Trainable params: 9,427,088
Non-trainable params: 10,870
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/15
2023-02-14 20:04:36.104213: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/learning_rate/Assign' id:13837 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/learning_rate, training_20/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:05:02.220608: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:13391 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 29s - loss: 0.0119 - val_loss: 0.0054 - 29s/epoch - 303us/sample
Epoch 2/15
95501/95501 - 26s - loss: 0.0204 - val_loss: 4.8325 - 26s/epoch - 269us/sample
Epoch 3/15
95501/95501 - 26s - loss: 0.0038 - val_loss: 0.0029 - 26s/epoch - 270us/sample
Epoch 4/15
95501/95501 - 26s - loss: 0.0033 - val_loss: 0.0029 - 26s/epoch - 269us/sample
Epoch 5/15
95501/95501 - 26s - loss: 0.0028 - val_loss: 0.0040 - 26s/epoch - 269us/sample
Epoch 6/15
95501/95501 - 26s - loss: 0.0025 - val_loss: 0.0021 - 26s/epoch - 269us/sample
Epoch 7/15
95501/95501 - 26s - loss: 0.0024 - val_loss: 0.0019 - 26s/epoch - 270us/sample
Epoch 8/15
95501/95501 - 26s - loss: 0.0022 - val_loss: 0.0019 - 26s/epoch - 269us/sample
Epoch 9/15
95501/95501 - 26s - loss: 0.0022 - val_loss: 0.0018 - 26s/epoch - 270us/sample
Epoch 10/15
95501/95501 - 26s - loss: 0.0021 - val_loss: 0.0018 - 26s/epoch - 271us/sample
Epoch 11/15
95501/95501 - 26s - loss: 0.0020 - val_loss: 0.0018 - 26s/epoch - 269us/sample
Epoch 12/15
95501/95501 - 26s - loss: 0.0020 - val_loss: 0.0017 - 26s/epoch - 271us/sample
Epoch 13/15
95501/95501 - 26s - loss: 0.0020 - val_loss: 0.0017 - 26s/epoch - 269us/sample
Epoch 14/15
95501/95501 - 26s - loss: 0.0019 - val_loss: 0.0016 - 26s/epoch - 269us/sample
Epoch 15/15
95501/95501 - 26s - loss: 0.0019 - val_loss: 0.0016 - 26s/epoch - 270us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.001617779174806951
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:11:04.628555: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:13362 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.013904207670774004
cosine 0.011082546540444363
MAE: 0.018535472
RMSE: 0.035032857
r2: 0.9203823006268269
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 16, 15, 0.001, 0.3, 379, 0.0018916007075210676, 0.001617779174806951, 0.013904207670774004, 0.011082546540444363, 0.018535472452640533, 0.035032857209444046, 0.9203823006268269, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 10 0.0008 8 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 2528)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          958491      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          958491      ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4312944     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,437,958
Trainable params: 9,427,088
Non-trainable params: 10,870
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 20:11:16.047125: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/bottleneck_zlog_11/bias/m/Assign' id:15150 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/bottleneck_zlog_11/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/bottleneck_zlog_11/bias/m, training_22/Adam/bottleneck_zlog_11/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:12:03.246050: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:14652 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 51s - loss: 0.0115 - val_loss: 0.0088 - 51s/epoch - 535us/sample
Epoch 2/10
95501/95501 - 47s - loss: 0.0056 - val_loss: 0.0107 - 47s/epoch - 497us/sample
Epoch 3/10
95501/95501 - 47s - loss: 0.0046 - val_loss: 0.0107 - 47s/epoch - 495us/sample
Epoch 4/10
95501/95501 - 48s - loss: 0.0041 - val_loss: 0.0077 - 48s/epoch - 500us/sample
Epoch 5/10
95501/95501 - 47s - loss: 0.0038 - val_loss: 0.0071 - 47s/epoch - 495us/sample
Epoch 6/10
95501/95501 - 47s - loss: 0.0037 - val_loss: 0.0067 - 47s/epoch - 497us/sample
Epoch 7/10
95501/95501 - 47s - loss: 0.0035 - val_loss: 0.0058 - 47s/epoch - 496us/sample
Epoch 8/10
95501/95501 - 48s - loss: 0.0034 - val_loss: 0.0050 - 48s/epoch - 499us/sample
Epoch 9/10
95501/95501 - 48s - loss: 0.0033 - val_loss: 0.0051 - 48s/epoch - 498us/sample
Epoch 10/10
95501/95501 - 47s - loss: 0.0033 - val_loss: 0.0052 - 47s/epoch - 497us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.005197603036282683
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:19:13.501033: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:14623 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.03841204597529402
cosine 0.031232411883661892
MAE: 0.030804839
RMSE: 0.068628676
r2: 0.694457487839202
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 8, 10, 0.0008, 0.3, 379, 0.0032836961441767222, 0.005197603036282683, 0.03841204597529402, 0.031232411883661892, 0.030804838985204697, 0.06862867623567581, 0.694457487839202, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 25 0.001 16 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 1896)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/25
2023-02-14 20:19:23.261229: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_38/gamma/Assign' id:15720 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_38/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_38/gamma, batch_normalization_38/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:19:49.266543: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:15913 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 29s - loss: 0.0095 - val_loss: 0.0045 - 29s/epoch - 305us/sample
Epoch 2/25
95501/95501 - 25s - loss: 0.0045 - val_loss: 0.0032 - 25s/epoch - 264us/sample
Epoch 3/25
95501/95501 - 25s - loss: 0.0035 - val_loss: 0.0026 - 25s/epoch - 264us/sample
Epoch 4/25
95501/95501 - 25s - loss: 0.0029 - val_loss: 0.0027 - 25s/epoch - 263us/sample
Epoch 5/25
95501/95501 - 25s - loss: 0.0026 - val_loss: 0.0023 - 25s/epoch - 265us/sample
Epoch 6/25
95501/95501 - 25s - loss: 0.0024 - val_loss: 0.0020 - 25s/epoch - 263us/sample
Epoch 7/25
95501/95501 - 25s - loss: 0.0023 - val_loss: 0.0019 - 25s/epoch - 262us/sample
Epoch 8/25
95501/95501 - 25s - loss: 0.0021 - val_loss: 0.0018 - 25s/epoch - 263us/sample
Epoch 9/25
95501/95501 - 25s - loss: 0.0021 - val_loss: 0.0018 - 25s/epoch - 266us/sample
Epoch 10/25
95501/95501 - 25s - loss: 0.0021 - val_loss: 0.0017 - 25s/epoch - 262us/sample
Epoch 11/25
95501/95501 - 25s - loss: 0.0020 - val_loss: 0.0017 - 25s/epoch - 265us/sample
Epoch 12/25
95501/95501 - 25s - loss: 0.0019 - val_loss: 0.0017 - 25s/epoch - 264us/sample
Epoch 13/25
95501/95501 - 25s - loss: 0.0019 - val_loss: 0.0016 - 25s/epoch - 264us/sample
Epoch 14/25
95501/95501 - 25s - loss: 0.0019 - val_loss: 0.0018 - 25s/epoch - 263us/sample
Epoch 15/25
95501/95501 - 25s - loss: 0.0019 - val_loss: 0.0016 - 25s/epoch - 264us/sample
Epoch 16/25
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0016 - 25s/epoch - 262us/sample
Epoch 17/25
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 263us/sample
Epoch 18/25
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 265us/sample
Epoch 19/25
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0016 - 25s/epoch - 263us/sample
Epoch 20/25
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 263us/sample
Epoch 21/25
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0015 - 25s/epoch - 266us/sample
Epoch 22/25
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0015 - 25s/epoch - 262us/sample
Epoch 23/25
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0014 - 25s/epoch - 263us/sample
Epoch 24/25
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0015 - 25s/epoch - 266us/sample
Epoch 25/25
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0014 - 25s/epoch - 264us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0014425543818975277
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:29:56.131999: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:15884 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.012195201609613668
cosine 0.009684685065842465
MAE: 0.016988501
RMSE: 0.032748736
r2: 0.930425842677424
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 16, 25, 0.001, 0.3, 379, 0.0016821887304099862, 0.0014425543818975277, 0.012195201609613668, 0.009684685065842465, 0.016988500952720642, 0.032748736441135406, 0.930425842677424, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 5 0.001 8 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 2528)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          958491      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          958491      ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4312944     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,437,958
Trainable params: 9,427,088
Non-trainable params: 10,870
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/5
2023-02-14 20:30:05.640354: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_13/bias/Assign' id:16832 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_13/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_13/bias, bottleneck_zmean_13/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:30:55.069826: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:17174 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 54s - loss: 0.0114 - val_loss: 0.0149 - 54s/epoch - 563us/sample
Epoch 2/5
95501/95501 - 50s - loss: 0.0057 - val_loss: 0.0199 - 50s/epoch - 521us/sample
Epoch 3/5
95501/95501 - 50s - loss: 0.0048 - val_loss: 0.0150 - 50s/epoch - 522us/sample
Epoch 4/5
95501/95501 - 50s - loss: 0.0044 - val_loss: 0.0223 - 50s/epoch - 523us/sample
Epoch 5/5
95501/95501 - 50s - loss: 0.0042 - val_loss: 0.0193 - 50s/epoch - 523us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.019322168569249384
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:34:17.928022: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:17145 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.06714754272778889
cosine 0.06101740935176352
MAE: 0.045743145
RMSE: 0.1376628
r2: -0.22940121951113282
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 8, 5, 0.001, 0.3, 379, 0.004156480998020363, 0.019322168569249384, 0.06714754272778889, 0.06101740935176352, 0.04574314504861832, 0.13766279816627502, -0.22940121951113282, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 145 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 1769)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          670830      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          670830      ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3062112     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,648,633
Trainable params: 6,640,799
Non-trainable params: 7,834
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-14 20:34:28.429400: W tensorflow/c/c_api.cc:291] Operation '{name:'training_28/Adam/batch_normalization_44/beta/v/Assign' id:19102 op device:{requested: '', assigned: ''} def:{{{node training_28/Adam/batch_normalization_44/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_28/Adam/batch_normalization_44/beta/v, training_28/Adam/batch_normalization_44/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:34:54.759854: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:18435 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 30s - loss: 0.0096 - val_loss: 0.0047 - 30s/epoch - 312us/sample
Epoch 2/145
95501/95501 - 25s - loss: 0.0045 - val_loss: 0.0031 - 25s/epoch - 267us/sample
Epoch 3/145
95501/95501 - 25s - loss: 0.0033 - val_loss: 0.0027 - 25s/epoch - 266us/sample
Epoch 4/145
95501/95501 - 26s - loss: 0.0028 - val_loss: 0.0023 - 26s/epoch - 268us/sample
Epoch 5/145
95501/95501 - 25s - loss: 0.0026 - val_loss: 0.0021 - 25s/epoch - 266us/sample
Epoch 6/145
95501/95501 - 25s - loss: 0.0024 - val_loss: 0.0020 - 25s/epoch - 266us/sample
Epoch 7/145
95501/95501 - 25s - loss: 0.0022 - val_loss: 0.0019 - 25s/epoch - 267us/sample
Epoch 8/145
95501/95501 - 25s - loss: 0.0022 - val_loss: 0.0018 - 25s/epoch - 266us/sample
Epoch 9/145
95501/95501 - 25s - loss: 0.0021 - val_loss: 0.0018 - 25s/epoch - 267us/sample
Epoch 10/145
95501/95501 - 26s - loss: 0.0020 - val_loss: 0.0017 - 26s/epoch - 267us/sample
Epoch 11/145
95501/95501 - 26s - loss: 0.0020 - val_loss: 0.0017 - 26s/epoch - 268us/sample
Epoch 12/145
95501/95501 - 25s - loss: 0.0019 - val_loss: 0.0017 - 25s/epoch - 267us/sample
Epoch 13/145
95501/95501 - 25s - loss: 0.0019 - val_loss: 0.0016 - 25s/epoch - 267us/sample
Epoch 14/145
95501/95501 - 25s - loss: 0.0019 - val_loss: 0.0016 - 25s/epoch - 266us/sample
Epoch 15/145
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 266us/sample
Epoch 16/145
95501/95501 - 26s - loss: 0.0018 - val_loss: 0.0016 - 26s/epoch - 268us/sample
Epoch 17/145
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 267us/sample
Epoch 18/145
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 265us/sample
Epoch 19/145
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 266us/sample
Epoch 20/145
95501/95501 - 25s - loss: 0.0018 - val_loss: 0.0015 - 25s/epoch - 266us/sample
Epoch 21/145
95501/95501 - 26s - loss: 0.0017 - val_loss: 0.0015 - 26s/epoch - 268us/sample
Epoch 22/145
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0015 - 25s/epoch - 266us/sample
Epoch 23/145
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 24/145
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 25/145
95501/95501 - 26s - loss: 0.0017 - val_loss: 0.0015 - 26s/epoch - 268us/sample
Epoch 26/145
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0015 - 25s/epoch - 266us/sample
Epoch 27/145
95501/95501 - 26s - loss: 0.0017 - val_loss: 0.0014 - 26s/epoch - 267us/sample
Epoch 28/145
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0014 - 25s/epoch - 267us/sample
Epoch 29/145
95501/95501 - 25s - loss: 0.0017 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 30/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 31/145
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 268us/sample
Epoch 32/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 33/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 265us/sample
Epoch 34/145
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 267us/sample
Epoch 35/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 36/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 267us/sample
Epoch 37/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 38/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 264us/sample
Epoch 39/145
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0014 - 26s/epoch - 267us/sample
Epoch 40/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 266us/sample
Epoch 41/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 265us/sample
Epoch 42/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0014 - 25s/epoch - 267us/sample
Epoch 43/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 44/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 45/145
95501/95501 - 26s - loss: 0.0016 - val_loss: 0.0013 - 26s/epoch - 268us/sample
Epoch 46/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 47/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 48/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 49/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 50/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 267us/sample
Epoch 51/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 52/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 53/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 54/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 55/145
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 56/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0014 - 25s/epoch - 265us/sample
Epoch 57/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 263us/sample
Epoch 58/145
95501/95501 - 25s - loss: 0.0016 - val_loss: 0.0013 - 25s/epoch - 263us/sample
Epoch 59/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 60/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 263us/sample
Epoch 61/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 62/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 63/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 64/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 65/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 66/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0014 - 25s/epoch - 263us/sample
Epoch 67/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 68/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 69/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0014 - 25s/epoch - 263us/sample
Epoch 70/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 71/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 72/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 263us/sample
Epoch 73/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 74/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 263us/sample
Epoch 75/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 263us/sample
Epoch 76/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 77/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 78/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 79/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 80/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 81/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 82/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 83/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 84/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 267us/sample
Epoch 85/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 86/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 87/145
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 269us/sample
Epoch 88/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 267us/sample
Epoch 89/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 90/145
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 91/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 92/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 93/145
95501/95501 - 26s - loss: 0.0015 - val_loss: 0.0013 - 26s/epoch - 267us/sample
Epoch 94/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0014 - 25s/epoch - 267us/sample
Epoch 95/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 266us/sample
Epoch 96/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 267us/sample
Epoch 97/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 98/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 267us/sample
Epoch 99/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 266us/sample
Epoch 100/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 101/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 262us/sample
Epoch 102/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 262us/sample
Epoch 103/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 104/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 105/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 106/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 263us/sample
Epoch 107/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 263us/sample
Epoch 108/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 109/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 110/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 111/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 112/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 113/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 114/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 115/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 116/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 263us/sample
Epoch 117/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 118/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 119/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 120/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 121/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 122/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 123/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 264us/sample
Epoch 124/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 125/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 126/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 127/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 128/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 129/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 130/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 263us/sample
Epoch 131/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 266us/sample
Epoch 132/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 263us/sample
Epoch 133/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 134/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 264us/sample
Epoch 135/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 263us/sample
Epoch 136/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0014 - 25s/epoch - 263us/sample
Epoch 137/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 266us/sample
Epoch 138/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 139/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 140/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 266us/sample
Epoch 141/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 142/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0013 - 25s/epoch - 265us/sample
Epoch 143/145
95501/95501 - 25s - loss: 0.0015 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 144/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 265us/sample
Epoch 145/145
95501/95501 - 25s - loss: 0.0014 - val_loss: 0.0012 - 25s/epoch - 266us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0012110611968408726
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:35:46.148352: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:18406 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.009647732702372796
cosine 0.007662998323525027
MAE: 0.014952937
RMSE: 0.02907262
r2: 0.9451691498812888
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 16, 145, 0.001, 0.3, 379, 0.0014487187450343757, 0.0012110611968408726, 0.009647732702372796, 0.007662998323525027, 0.01495293714106083, 0.02907261997461319, 0.9451691498812888, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 10 0.0008 16 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 2401)        9604        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 2401)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          910358      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          910358      ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4103648     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 8,971,233
Trainable params: 8,960,871
Non-trainable params: 10,362
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 21:35:56.228733: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_dec1_15/kernel/Assign' id:19410 op device:{requested: '', assigned: ''} def:{{{node dense_dec1_15/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_dec1_15/kernel, dense_dec1_15/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:36:23.873319: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:19699 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 31s - loss: 0.0249 - val_loss: 0.0050 - 31s/epoch - 327us/sample
Epoch 2/10
95501/95501 - 27s - loss: 0.0054 - val_loss: 0.0040 - 27s/epoch - 283us/sample
Epoch 3/10
95501/95501 - 27s - loss: 0.0039 - val_loss: 0.0033 - 27s/epoch - 284us/sample
Epoch 4/10
95501/95501 - 27s - loss: 0.0032 - val_loss: 0.0025 - 27s/epoch - 284us/sample
Epoch 5/10
95501/95501 - 27s - loss: 0.0026 - val_loss: 0.0022 - 27s/epoch - 284us/sample
Epoch 6/10
95501/95501 - 27s - loss: 0.0024 - val_loss: 0.0020 - 27s/epoch - 283us/sample
Epoch 7/10
95501/95501 - 27s - loss: 0.0022 - val_loss: 0.0019 - 27s/epoch - 284us/sample
Epoch 8/10
95501/95501 - 27s - loss: 0.0022 - val_loss: 0.0018 - 27s/epoch - 285us/sample
Epoch 9/10
95501/95501 - 27s - loss: 0.0021 - val_loss: 0.0017 - 27s/epoch - 284us/sample
Epoch 10/10
95501/95501 - 27s - loss: 0.0020 - val_loss: 0.0017 - 27s/epoch - 283us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0016758448609824601
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:40:30.261413: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:19670 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.014628023768522763
cosine 0.011670551691421957
MAE: 0.018961677
RMSE: 0.035702165
r2: 0.9173109569793039
RMSE zero-vector: 0.23411466903540806
['1.9custom_VAE', 'mse', 16, 10, 0.0008, 0.3, 379, 0.00199640841168118, 0.0016758448609824601, 0.014628023768522763, 0.011670551691421957, 0.018961677327752113, 0.035702165216207504, 0.9173109569793039, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 10 0.001 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 2654)        10616       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 2654)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          1006245     ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          1006245     ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4520592     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,901,008
Trainable params: 9,889,634
Non-trainable params: 11,374
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 21:40:42.879306: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/bottleneck_zlog_16/bias/v/Assign' id:21580 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/bottleneck_zlog_16/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/bottleneck_zlog_16/bias/v, training_32/Adam/bottleneck_zlog_16/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:40:54.970298: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:20963 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 15s - loss: 0.0131 - val_loss: 0.0054 - 15s/epoch - 159us/sample
Epoch 2/10
95501/95501 - 11s - loss: 0.0063 - val_loss: 0.0040 - 11s/epoch - 112us/sample
Epoch 3/10
95501/95501 - 11s - loss: 0.0045 - val_loss: 0.0035 - 11s/epoch - 111us/sample
Epoch 4/10
95501/95501 - 11s - loss: 0.1854 - val_loss: 0.0051 - 11s/epoch - 111us/sample
Epoch 5/10
95501/95501 - 11s - loss: 0.0036 - val_loss: 0.0101 - 11s/epoch - 111us/sample
Epoch 6/10
95501/95501 - 11s - loss: 325.7932 - val_loss: 0.0035 - 11s/epoch - 111us/sample
Epoch 7/10
95501/95501 - 11s - loss: 0.0028 - val_loss: 0.0025 - 11s/epoch - 112us/sample
Epoch 8/10
95501/95501 - 11s - loss: 0.0024 - val_loss: 0.0023 - 11s/epoch - 111us/sample
Epoch 9/10
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0057 - 11s/epoch - 112us/sample
Epoch 10/10
95501/95501 - 11s - loss: 0.0025 - val_loss: 0.0018 - 11s/epoch - 111us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0018198031903505381
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:42:32.504372: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:20934 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01472732901595648
cosine 0.01165734239335425
MAE: 0.019436847
RMSE: 0.03552031
r2: 0.918151172170488
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 64, 10, 0.001, 0.3, 379, 0.002500807070513228, 0.0018198031903505381, 0.01472732901595648, 0.01165734239335425, 0.019436847418546677, 0.03552031144499779, 0.918151172170488, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 5 0.001 16 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 2654)        10616       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 2654)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          1006245     ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          1006245     ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4520592     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,901,008
Trainable params: 9,889,634
Non-trainable params: 11,374
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/5
2023-02-14 21:42:43.759954: W tensorflow/c/c_api.cc:291] Operation '{name:'training_34/Adam/iter/Assign' id:22653 op device:{requested: '', assigned: ''} def:{{{node training_34/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_34/Adam/iter, training_34/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:43:11.656794: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:22227 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 32s - loss: 0.0149 - val_loss: 0.0052 - 32s/epoch - 333us/sample
Epoch 2/5
95501/95501 - 27s - loss: 0.0053 - val_loss: 0.0038 - 27s/epoch - 286us/sample
Epoch 3/5
95501/95501 - 27s - loss: 0.0037 - val_loss: 0.0030 - 27s/epoch - 285us/sample
Epoch 4/5
95501/95501 - 27s - loss: 0.0030 - val_loss: 0.0025 - 27s/epoch - 284us/sample
Epoch 5/5
95501/95501 - 27s - loss: 0.0026 - val_loss: 0.0021 - 27s/epoch - 285us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.002126044183994071
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:45:03.273734: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:22198 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01927874790826834
cosine 0.015386868278080681
MAE: 0.022252955
RMSE: 0.041182373
r2: 0.8899773685360555
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 16, 5, 0.001, 0.3, 379, 0.002590645886291196, 0.002126044183994071, 0.01927874790826834, 0.015386868278080681, 0.02225295454263687, 0.041182372719049454, 0.8899773685360555, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 84.84730891968614
Last generation's best solutions = [1.5 145 0.0012 64 1] with fitness 84.84730891968614.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object)]
Best solutions fitness :  [84.84730891968614]
[1.4 140 0.001 8 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 1769)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          670830      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          670830      ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3062112     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,648,633
Trainable params: 6,640,799
Non-trainable params: 7,834
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-14 21:45:14.988324: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_54/beta/Assign' id:23078 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_54/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_54/beta, batch_normalization_54/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:46:03.522730: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_18/mul' id:23488 op device:{requested: '', assigned: ''} def:{{{node loss_18/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_18/mul/x, loss_18/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 54s - loss: 0.0102 - val_loss: 0.0070 - 54s/epoch - 565us/sample
Epoch 2/140
95501/95501 - 49s - loss: 0.0054 - val_loss: 0.0073 - 49s/epoch - 510us/sample
Epoch 3/140
95501/95501 - 49s - loss: 0.0045 - val_loss: 0.0078 - 49s/epoch - 509us/sample
Epoch 4/140
95501/95501 - 49s - loss: 0.0041 - val_loss: 0.0055 - 49s/epoch - 511us/sample
Epoch 5/140
95501/95501 - 49s - loss: 0.0038 - val_loss: 0.0050 - 49s/epoch - 511us/sample
Epoch 6/140
95501/95501 - 48s - loss: 0.0036 - val_loss: 0.0046 - 48s/epoch - 508us/sample
Epoch 7/140
95501/95501 - 48s - loss: 0.0035 - val_loss: 0.0046 - 48s/epoch - 508us/sample
Epoch 8/140
95501/95501 - 49s - loss: 0.0034 - val_loss: 0.0045 - 49s/epoch - 510us/sample
Epoch 9/140
95501/95501 - 49s - loss: 0.0033 - val_loss: 0.0040 - 49s/epoch - 510us/sample
Epoch 10/140
95501/95501 - 49s - loss: 0.0032 - val_loss: 0.0039 - 49s/epoch - 510us/sample
Epoch 11/140
95501/95501 - 49s - loss: 0.0032 - val_loss: 0.0033 - 49s/epoch - 508us/sample
Epoch 12/140
95501/95501 - 49s - loss: 0.0031 - val_loss: 0.0034 - 49s/epoch - 512us/sample
Epoch 13/140
95501/95501 - 49s - loss: 0.0031 - val_loss: 0.0037 - 49s/epoch - 508us/sample
Epoch 14/140
95501/95501 - 49s - loss: 0.0031 - val_loss: 0.0032 - 49s/epoch - 510us/sample
Epoch 15/140
95501/95501 - 49s - loss: 0.0030 - val_loss: 0.0031 - 49s/epoch - 510us/sample
Epoch 16/140
95501/95501 - 49s - loss: 0.0030 - val_loss: 0.0031 - 49s/epoch - 511us/sample
Epoch 17/140
95501/95501 - 49s - loss: 0.0030 - val_loss: 0.0032 - 49s/epoch - 509us/sample
Epoch 18/140
95501/95501 - 49s - loss: 0.0029 - val_loss: 0.0031 - 49s/epoch - 510us/sample
Epoch 19/140
95501/95501 - 49s - loss: 0.0030 - val_loss: 0.0029 - 49s/epoch - 510us/sample
Epoch 20/140
95501/95501 - 49s - loss: 0.0029 - val_loss: 0.0030 - 49s/epoch - 510us/sample
Epoch 21/140
95501/95501 - 48s - loss: 0.0029 - val_loss: 0.0031 - 48s/epoch - 506us/sample
Epoch 22/140
95501/95501 - 49s - loss: 0.0029 - val_loss: 0.0031 - 49s/epoch - 511us/sample
Epoch 23/140
95501/95501 - 48s - loss: 0.0029 - val_loss: 0.0029 - 48s/epoch - 507us/sample
Epoch 24/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0031 - 49s/epoch - 512us/sample
Epoch 25/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0028 - 49s/epoch - 509us/sample
Epoch 26/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0030 - 49s/epoch - 509us/sample
Epoch 27/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0029 - 49s/epoch - 510us/sample
Epoch 28/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0030 - 49s/epoch - 509us/sample
Epoch 29/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0029 - 49s/epoch - 510us/sample
Epoch 30/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0030 - 49s/epoch - 510us/sample
Epoch 31/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0030 - 49s/epoch - 508us/sample
Epoch 32/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0031 - 49s/epoch - 510us/sample
Epoch 33/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 509us/sample
Epoch 34/140
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0030 - 49s/epoch - 509us/sample
Epoch 35/140
95501/95501 - 48s - loss: 0.0027 - val_loss: 0.0029 - 48s/epoch - 507us/sample
Epoch 36/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 512us/sample
Epoch 37/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0029 - 49s/epoch - 508us/sample
Epoch 38/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0029 - 49s/epoch - 512us/sample
Epoch 39/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 510us/sample
Epoch 40/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 510us/sample
Epoch 41/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 510us/sample
Epoch 42/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0029 - 49s/epoch - 508us/sample
Epoch 43/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 44/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0029 - 49s/epoch - 510us/sample
Epoch 45/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 510us/sample
Epoch 46/140
95501/95501 - 48s - loss: 0.0027 - val_loss: 0.0029 - 48s/epoch - 507us/sample
Epoch 47/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 510us/sample
Epoch 48/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0028 - 49s/epoch - 510us/sample
Epoch 49/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0028 - 49s/epoch - 508us/sample
Epoch 50/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 51/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0026 - 49s/epoch - 509us/sample
Epoch 52/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0028 - 49s/epoch - 511us/sample
Epoch 53/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 54/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 511us/sample
Epoch 55/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0028 - 49s/epoch - 511us/sample
Epoch 56/140
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0027 - 49s/epoch - 511us/sample
Epoch 57/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0028 - 49s/epoch - 511us/sample
Epoch 58/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 59/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 60/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 61/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 62/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 63/140
95501/95501 - 48s - loss: 0.0026 - val_loss: 0.0027 - 48s/epoch - 506us/sample
Epoch 64/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 65/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0028 - 49s/epoch - 509us/sample
Epoch 66/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0028 - 49s/epoch - 509us/sample
Epoch 67/140
95501/95501 - 48s - loss: 0.0026 - val_loss: 0.0027 - 48s/epoch - 506us/sample
Epoch 68/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 511us/sample
Epoch 69/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 70/140
95501/95501 - 48s - loss: 0.0025 - val_loss: 0.0026 - 48s/epoch - 507us/sample
Epoch 71/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 508us/sample
Epoch 72/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 73/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 74/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 75/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0026 - 49s/epoch - 511us/sample
Epoch 76/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 77/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 511us/sample
Epoch 78/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 511us/sample
Epoch 79/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 508us/sample
Epoch 80/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0026 - 49s/epoch - 508us/sample
Epoch 81/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 82/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 512us/sample
Epoch 83/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0028 - 49s/epoch - 511us/sample
Epoch 84/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0032 - 49s/epoch - 513us/sample
Epoch 85/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 511us/sample
Epoch 86/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 87/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 88/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0028 - 49s/epoch - 513us/sample
Epoch 89/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 509us/sample
Epoch 90/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 508us/sample
Epoch 91/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 508us/sample
Epoch 92/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 511us/sample
Epoch 93/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 94/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 508us/sample
Epoch 95/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 511us/sample
Epoch 96/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 511us/sample
Epoch 97/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 98/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 99/140
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 100/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 101/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 102/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 509us/sample
Epoch 103/140
95501/95501 - 48s - loss: 0.0025 - val_loss: 0.0026 - 48s/epoch - 507us/sample
Epoch 104/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 508us/sample
Epoch 105/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 509us/sample
Epoch 106/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 508us/sample
Epoch 107/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 511us/sample
Epoch 108/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 509us/sample
Epoch 109/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 110/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 111/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 509us/sample
Epoch 112/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0028 - 49s/epoch - 513us/sample
Epoch 113/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 511us/sample
Epoch 114/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 509us/sample
Epoch 115/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 509us/sample
Epoch 116/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 511us/sample
Epoch 117/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 118/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 119/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 120/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 511us/sample
Epoch 121/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 122/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 123/140
95501/95501 - 49s - loss: 0.0024 - val_loss: 0.0025 - 49s/epoch - 510us/sample
Epoch 124/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 125/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 510us/sample
Epoch 126/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 127/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 128/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0028 - 49s/epoch - 509us/sample
Epoch 129/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 510us/sample
Epoch 130/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 511us/sample
Epoch 131/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 132/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 133/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 508us/sample
Epoch 134/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 509us/sample
Epoch 135/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0025 - 49s/epoch - 509us/sample
Epoch 136/140
95501/95501 - 49s - loss: 0.0024 - val_loss: 0.0028 - 49s/epoch - 511us/sample
Epoch 137/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 138/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0027 - 49s/epoch - 510us/sample
Epoch 139/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 512us/sample
Epoch 140/140
95501/95501 - 49s - loss: 0.0025 - val_loss: 0.0026 - 49s/epoch - 510us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.002560177604066502
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:38:55.164526: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_18/outputlayer/BiasAdd' id:23459 op device:{requested: '', assigned: ''} def:{{{node decoder_model_18/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_18/outputlayer/MatMul, decoder_model_18/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.022386974842198787
cosine 0.01822462352185801
MAE: 0.022874035
RMSE: 0.046394434
r2: 0.8603660571755374
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 8, 140, 0.001, 0.3, 379, 0.0024707050486263427, 0.002560177604066502, 0.022386974842198787, 0.01822462352185801, 0.02287403494119644, 0.046394433826208115, 0.8603660571755374, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 145 0.0012 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 2654)        10616       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 2654)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          1006245     ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          1006245     ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4520592     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,901,008
Trainable params: 9,889,634
Non-trainable params: 11,374
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-14 23:39:07.132208: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_59/gamma/Assign' id:24556 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_59/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_59/gamma, batch_normalization_59/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:39:19.850656: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_19/mul' id:24749 op device:{requested: '', assigned: ''} def:{{{node loss_19/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_19/mul/x, loss_19/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 16s - loss: 0.0119 - val_loss: 0.0068 - 16s/epoch - 170us/sample
Epoch 2/145
95501/95501 - 11s - loss: 0.0050 - val_loss: 0.0048 - 11s/epoch - 112us/sample
Epoch 3/145
95501/95501 - 11s - loss: 15.2802 - val_loss: 0.0040 - 11s/epoch - 111us/sample
Epoch 4/145
95501/95501 - 11s - loss: 0.0040 - val_loss: 0.0035 - 11s/epoch - 112us/sample
Epoch 5/145
95501/95501 - 11s - loss: 0.0034 - val_loss: 0.0035 - 11s/epoch - 111us/sample
Epoch 6/145
95501/95501 - 11s - loss: 8.4452 - val_loss: 0.0108 - 11s/epoch - 112us/sample
Epoch 7/145
95501/95501 - 11s - loss: 255.5358 - val_loss: 0.0028 - 11s/epoch - 111us/sample
Epoch 8/145
95501/95501 - 11s - loss: 0.0025 - val_loss: 0.0020 - 11s/epoch - 112us/sample
Epoch 9/145
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0021 - 11s/epoch - 112us/sample
Epoch 10/145
95501/95501 - 11s - loss: 0.0021 - val_loss: 0.0079 - 11s/epoch - 112us/sample
Epoch 11/145
95501/95501 - 11s - loss: 0.0025 - val_loss: 0.0027 - 11s/epoch - 111us/sample
Epoch 12/145
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0094 - 11s/epoch - 111us/sample
Epoch 13/145
95501/95501 - 11s - loss: 0.0023 - val_loss: 0.0016 - 11s/epoch - 111us/sample
Epoch 14/145
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0042 - 11s/epoch - 112us/sample
Epoch 15/145
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0020 - 11s/epoch - 111us/sample
Epoch 16/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0016 - 11s/epoch - 111us/sample
Epoch 17/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 18/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0018 - 11s/epoch - 113us/sample
Epoch 19/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0042 - 11s/epoch - 111us/sample
Epoch 20/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0048 - 11s/epoch - 111us/sample
Epoch 21/145
95501/95501 - 11s - loss: 0.0019 - val_loss: 0.0014 - 11s/epoch - 112us/sample
Epoch 22/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 23/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0014 - 11s/epoch - 111us/sample
Epoch 24/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 25/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 111us/sample
Epoch 26/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0025 - 11s/epoch - 112us/sample
Epoch 27/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 28/145
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0013 - 10s/epoch - 110us/sample
Epoch 29/145
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0026 - 10s/epoch - 109us/sample
Epoch 30/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 111us/sample
Epoch 31/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0025 - 11s/epoch - 111us/sample
Epoch 32/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0012 - 11s/epoch - 111us/sample
Epoch 33/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 34/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0017 - 11s/epoch - 113us/sample
Epoch 35/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 36/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 37/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 38/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 39/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 40/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0013 - 11s/epoch - 112us/sample
Epoch 41/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 42/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 43/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 44/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 45/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 46/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 47/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 48/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 112us/sample
Epoch 49/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 50/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 51/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 52/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 53/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 54/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 55/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 56/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 57/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8914e-04 - 11s/epoch - 112us/sample
Epoch 58/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 59/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 60/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8190e-04 - 11s/epoch - 112us/sample
Epoch 61/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7612e-04 - 11s/epoch - 112us/sample
Epoch 62/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 63/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7084e-04 - 11s/epoch - 112us/sample
Epoch 64/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 65/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7254e-04 - 11s/epoch - 112us/sample
Epoch 66/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8338e-04 - 11s/epoch - 112us/sample
Epoch 67/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5729e-04 - 11s/epoch - 112us/sample
Epoch 68/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6533e-04 - 11s/epoch - 112us/sample
Epoch 69/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0013 - 11s/epoch - 112us/sample
Epoch 70/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 71/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.5690e-04 - 11s/epoch - 112us/sample
Epoch 72/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6811e-04 - 11s/epoch - 111us/sample
Epoch 73/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6729e-04 - 11s/epoch - 111us/sample
Epoch 74/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 75/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 76/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6904e-04 - 11s/epoch - 111us/sample
Epoch 77/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5915e-04 - 11s/epoch - 112us/sample
Epoch 78/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 79/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4566e-04 - 11s/epoch - 112us/sample
Epoch 80/145
95501/95501 - 11s - loss: 9.9336e-04 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 81/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.4999e-04 - 11s/epoch - 112us/sample
Epoch 82/145
95501/95501 - 11s - loss: 9.9255e-04 - val_loss: 9.4293e-04 - 11s/epoch - 113us/sample
Epoch 83/145
95501/95501 - 11s - loss: 9.8813e-04 - val_loss: 9.4176e-04 - 11s/epoch - 112us/sample
Epoch 84/145
95501/95501 - 11s - loss: 9.8257e-04 - val_loss: 9.6965e-04 - 11s/epoch - 112us/sample
Epoch 85/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.3503e-04 - 11s/epoch - 111us/sample
Epoch 86/145
95501/95501 - 11s - loss: 9.7911e-04 - val_loss: 9.5334e-04 - 11s/epoch - 112us/sample
Epoch 87/145
95501/95501 - 11s - loss: 9.9748e-04 - val_loss: 9.4258e-04 - 11s/epoch - 111us/sample
Epoch 88/145
95501/95501 - 11s - loss: 9.7635e-04 - val_loss: 9.4039e-04 - 11s/epoch - 112us/sample
Epoch 89/145
95501/95501 - 11s - loss: 9.7723e-04 - val_loss: 9.5950e-04 - 11s/epoch - 112us/sample
Epoch 90/145
95501/95501 - 11s - loss: 9.8187e-04 - val_loss: 9.4011e-04 - 11s/epoch - 113us/sample
Epoch 91/145
95501/95501 - 11s - loss: 9.7574e-04 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 92/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.3333e-04 - 11s/epoch - 112us/sample
Epoch 93/145
95501/95501 - 11s - loss: 9.9189e-04 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 94/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 95/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.3772e-04 - 11s/epoch - 112us/sample
Epoch 96/145
95501/95501 - 11s - loss: 9.7127e-04 - val_loss: 9.3198e-04 - 11s/epoch - 111us/sample
Epoch 97/145
95501/95501 - 11s - loss: 9.6816e-04 - val_loss: 9.2855e-04 - 11s/epoch - 110us/sample
Epoch 98/145
95501/95501 - 11s - loss: 9.9864e-04 - val_loss: 9.5664e-04 - 11s/epoch - 112us/sample
Epoch 99/145
95501/95501 - 11s - loss: 9.8756e-04 - val_loss: 9.4067e-04 - 11s/epoch - 113us/sample
Epoch 100/145
95501/95501 - 11s - loss: 9.6941e-04 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 101/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.2754e-04 - 11s/epoch - 113us/sample
Epoch 102/145
95501/95501 - 11s - loss: 9.7128e-04 - val_loss: 9.2233e-04 - 11s/epoch - 113us/sample
Epoch 103/145
95501/95501 - 11s - loss: 9.6301e-04 - val_loss: 9.2311e-04 - 11s/epoch - 113us/sample
Epoch 104/145
95501/95501 - 11s - loss: 9.6556e-04 - val_loss: 9.1743e-04 - 11s/epoch - 114us/sample
Epoch 105/145
95501/95501 - 11s - loss: 9.5921e-04 - val_loss: 9.4034e-04 - 11s/epoch - 114us/sample
Epoch 106/145
95501/95501 - 11s - loss: 9.7012e-04 - val_loss: 9.4636e-04 - 11s/epoch - 113us/sample
Epoch 107/145
95501/95501 - 11s - loss: 9.7415e-04 - val_loss: 9.9691e-04 - 11s/epoch - 113us/sample
Epoch 108/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.2076e-04 - 11s/epoch - 113us/sample
Epoch 109/145
95501/95501 - 11s - loss: 9.5668e-04 - val_loss: 9.1042e-04 - 11s/epoch - 113us/sample
Epoch 110/145
95501/95501 - 11s - loss: 9.5169e-04 - val_loss: 9.1590e-04 - 11s/epoch - 113us/sample
Epoch 111/145
95501/95501 - 11s - loss: 9.8705e-04 - val_loss: 9.9882e-04 - 11s/epoch - 113us/sample
Epoch 112/145
95501/95501 - 11s - loss: 9.7978e-04 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 113/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9883e-04 - 11s/epoch - 114us/sample
Epoch 114/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 113us/sample
Epoch 115/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.1525e-04 - 11s/epoch - 113us/sample
Epoch 116/145
95501/95501 - 11s - loss: 9.5882e-04 - val_loss: 9.1393e-04 - 11s/epoch - 113us/sample
Epoch 117/145
95501/95501 - 11s - loss: 9.5743e-04 - val_loss: 9.2165e-04 - 11s/epoch - 113us/sample
Epoch 118/145
95501/95501 - 11s - loss: 9.5360e-04 - val_loss: 0.0010 - 11s/epoch - 113us/sample
Epoch 119/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.3236e-04 - 11s/epoch - 113us/sample
Epoch 120/145
95501/95501 - 11s - loss: 9.6591e-04 - val_loss: 9.1373e-04 - 11s/epoch - 113us/sample
Epoch 121/145
95501/95501 - 11s - loss: 9.4956e-04 - val_loss: 9.7956e-04 - 11s/epoch - 113us/sample
Epoch 122/145
95501/95501 - 11s - loss: 9.7221e-04 - val_loss: 9.0566e-04 - 11s/epoch - 113us/sample
Epoch 123/145
95501/95501 - 11s - loss: 9.5314e-04 - val_loss: 9.2687e-04 - 11s/epoch - 113us/sample
Epoch 124/145
95501/95501 - 11s - loss: 9.5443e-04 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 125/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.2626e-04 - 11s/epoch - 114us/sample
Epoch 126/145
95501/95501 - 11s - loss: 9.6604e-04 - val_loss: 9.2769e-04 - 11s/epoch - 113us/sample
Epoch 127/145
95501/95501 - 11s - loss: 9.4225e-04 - val_loss: 9.0608e-04 - 11s/epoch - 113us/sample
Epoch 128/145
95501/95501 - 11s - loss: 9.4896e-04 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 129/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.1573e-04 - 11s/epoch - 114us/sample
Epoch 130/145
95501/95501 - 11s - loss: 9.5928e-04 - val_loss: 0.0012 - 11s/epoch - 113us/sample
Epoch 131/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.8477e-04 - 11s/epoch - 113us/sample
Epoch 132/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.2526e-04 - 11s/epoch - 113us/sample
Epoch 133/145
95501/95501 - 11s - loss: 9.5204e-04 - val_loss: 9.1250e-04 - 11s/epoch - 113us/sample
Epoch 134/145
95501/95501 - 11s - loss: 9.4779e-04 - val_loss: 0.0010 - 11s/epoch - 113us/sample
Epoch 135/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 136/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.0666e-04 - 11s/epoch - 114us/sample
Epoch 137/145
95501/95501 - 11s - loss: 9.5025e-04 - val_loss: 9.0146e-04 - 11s/epoch - 113us/sample
Epoch 138/145
95501/95501 - 11s - loss: 9.4152e-04 - val_loss: 9.0211e-04 - 11s/epoch - 113us/sample
Epoch 139/145
95501/95501 - 11s - loss: 9.4636e-04 - val_loss: 9.7193e-04 - 11s/epoch - 113us/sample
Epoch 140/145
95501/95501 - 11s - loss: 9.9517e-04 - val_loss: 9.1410e-04 - 11s/epoch - 113us/sample
Epoch 141/145
95501/95501 - 11s - loss: 9.4022e-04 - val_loss: 9.2815e-04 - 11s/epoch - 114us/sample
Epoch 142/145
95501/95501 - 11s - loss: 9.5094e-04 - val_loss: 9.0434e-04 - 11s/epoch - 113us/sample
Epoch 143/145
95501/95501 - 11s - loss: 9.4145e-04 - val_loss: 9.0159e-04 - 11s/epoch - 114us/sample
Epoch 144/145
95501/95501 - 11s - loss: 9.4476e-04 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 145/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.0069e-04 - 11s/epoch - 113us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0009006914829291527
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:05:04.813423: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:24720 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006047955946206956
cosine 0.004769636956287518
MAE: 0.011809299
RMSE: 0.022809543
r2: 0.9662490848744486
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 64, 145, 0.0012, 0.3, 379, 0.0010252037052365358, 0.0009006914829291527, 0.006047955946206956, 0.004769636956287518, 0.011809298768639565, 0.02280954271554947, 0.9662490848744486, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 145 0.0012 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 1769)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          670830      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          670830      ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3062112     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,648,633
Trainable params: 6,640,799
Non-trainable params: 7,834
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 00:05:17.148898: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_20/bias/Assign' id:25668 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_20/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_20/bias, bottleneck_zmean_20/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:05:30.084580: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:26017 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 17s - loss: 0.0060 - val_loss: 0.0038 - 17s/epoch - 174us/sample
Epoch 2/145
95501/95501 - 11s - loss: 0.0029 - val_loss: 0.0050 - 11s/epoch - 116us/sample
Epoch 3/145
95501/95501 - 11s - loss: 0.0025 - val_loss: 0.0022 - 11s/epoch - 117us/sample
Epoch 4/145
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0016 - 11s/epoch - 116us/sample
Epoch 5/145
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0020 - 11s/epoch - 115us/sample
Epoch 6/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0013 - 11s/epoch - 115us/sample
Epoch 7/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0014 - 11s/epoch - 115us/sample
Epoch 8/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 9/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 10/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 11/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.3603e-04 - 11s/epoch - 114us/sample
Epoch 12/145
95501/95501 - 11s - loss: 9.8641e-04 - val_loss: 9.0998e-04 - 11s/epoch - 117us/sample
Epoch 13/145
95501/95501 - 11s - loss: 9.4437e-04 - val_loss: 8.8652e-04 - 11s/epoch - 115us/sample
Epoch 14/145
95501/95501 - 11s - loss: 9.1782e-04 - val_loss: 9.0995e-04 - 11s/epoch - 115us/sample
Epoch 15/145
95501/95501 - 11s - loss: 8.9818e-04 - val_loss: 8.3673e-04 - 11s/epoch - 115us/sample
Epoch 16/145
95501/95501 - 11s - loss: 8.8178e-04 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 17/145
95501/95501 - 11s - loss: 9.0507e-04 - val_loss: 8.0515e-04 - 11s/epoch - 115us/sample
Epoch 18/145
95501/95501 - 11s - loss: 8.5044e-04 - val_loss: 7.9910e-04 - 11s/epoch - 115us/sample
Epoch 19/145
95501/95501 - 11s - loss: 8.9120e-04 - val_loss: 7.8905e-04 - 11s/epoch - 117us/sample
Epoch 20/145
95501/95501 - 11s - loss: 8.3607e-04 - val_loss: 7.8266e-04 - 11s/epoch - 115us/sample
Epoch 21/145
95501/95501 - 11s - loss: 8.1928e-04 - val_loss: 7.7157e-04 - 11s/epoch - 115us/sample
Epoch 22/145
95501/95501 - 11s - loss: 8.2649e-04 - val_loss: 9.4951e-04 - 11s/epoch - 115us/sample
Epoch 23/145
95501/95501 - 11s - loss: 8.0899e-04 - val_loss: 7.9514e-04 - 11s/epoch - 115us/sample
Epoch 24/145
95501/95501 - 11s - loss: 8.1093e-04 - val_loss: 7.5157e-04 - 11s/epoch - 115us/sample
Epoch 25/145
95501/95501 - 11s - loss: 7.8916e-04 - val_loss: 7.5461e-04 - 11s/epoch - 115us/sample
Epoch 26/145
95501/95501 - 11s - loss: 7.8103e-04 - val_loss: 7.8522e-04 - 11s/epoch - 116us/sample
Epoch 27/145
95501/95501 - 11s - loss: 7.9620e-04 - val_loss: 7.3349e-04 - 11s/epoch - 115us/sample
Epoch 28/145
95501/95501 - 11s - loss: 7.7326e-04 - val_loss: 7.4524e-04 - 11s/epoch - 114us/sample
Epoch 29/145
95501/95501 - 11s - loss: 7.7856e-04 - val_loss: 8.0016e-04 - 11s/epoch - 115us/sample
Epoch 30/145
95501/95501 - 11s - loss: 8.1025e-04 - val_loss: 7.6123e-04 - 11s/epoch - 114us/sample
Epoch 31/145
95501/95501 - 11s - loss: 7.5700e-04 - val_loss: 7.2006e-04 - 11s/epoch - 114us/sample
Epoch 32/145
95501/95501 - 11s - loss: 7.5644e-04 - val_loss: 7.3126e-04 - 11s/epoch - 115us/sample
Epoch 33/145
95501/95501 - 11s - loss: 7.5731e-04 - val_loss: 7.4114e-04 - 11s/epoch - 116us/sample
Epoch 34/145
95501/95501 - 11s - loss: 7.6197e-04 - val_loss: 7.1253e-04 - 11s/epoch - 116us/sample
Epoch 35/145
95501/95501 - 11s - loss: 7.4558e-04 - val_loss: 7.7950e-04 - 11s/epoch - 114us/sample
Epoch 36/145
95501/95501 - 11s - loss: 7.9594e-04 - val_loss: 7.0506e-04 - 11s/epoch - 115us/sample
Epoch 37/145
95501/95501 - 11s - loss: 7.3945e-04 - val_loss: 7.6003e-04 - 11s/epoch - 114us/sample
Epoch 38/145
95501/95501 - 11s - loss: 7.6188e-04 - val_loss: 6.9795e-04 - 11s/epoch - 115us/sample
Epoch 39/145
95501/95501 - 11s - loss: 7.3430e-04 - val_loss: 7.0476e-04 - 11s/epoch - 114us/sample
Epoch 40/145
95501/95501 - 11s - loss: 7.2899e-04 - val_loss: 6.9307e-04 - 11s/epoch - 116us/sample
Epoch 41/145
95501/95501 - 11s - loss: 7.2635e-04 - val_loss: 6.9640e-04 - 11s/epoch - 115us/sample
Epoch 42/145
95501/95501 - 11s - loss: 7.2126e-04 - val_loss: 6.8673e-04 - 11s/epoch - 114us/sample
Epoch 43/145
95501/95501 - 11s - loss: 7.1964e-04 - val_loss: 6.8758e-04 - 11s/epoch - 114us/sample
Epoch 44/145
95501/95501 - 11s - loss: 7.1720e-04 - val_loss: 7.0548e-04 - 11s/epoch - 114us/sample
Epoch 45/145
95501/95501 - 11s - loss: 7.2731e-04 - val_loss: 6.8154e-04 - 11s/epoch - 114us/sample
Epoch 46/145
95501/95501 - 11s - loss: 7.1084e-04 - val_loss: 6.8220e-04 - 11s/epoch - 115us/sample
Epoch 47/145
95501/95501 - 11s - loss: 7.0841e-04 - val_loss: 6.8714e-04 - 11s/epoch - 115us/sample
Epoch 48/145
95501/95501 - 11s - loss: 7.1844e-04 - val_loss: 7.7319e-04 - 11s/epoch - 117us/sample
Epoch 49/145
95501/95501 - 11s - loss: 7.6123e-04 - val_loss: 7.2925e-04 - 11s/epoch - 115us/sample
Epoch 50/145
95501/95501 - 11s - loss: 7.6496e-04 - val_loss: 6.7908e-04 - 11s/epoch - 114us/sample
Epoch 51/145
95501/95501 - 11s - loss: 7.1122e-04 - val_loss: 6.9020e-04 - 11s/epoch - 113us/sample
Epoch 52/145
95501/95501 - 11s - loss: 7.2718e-04 - val_loss: 7.2146e-04 - 11s/epoch - 114us/sample
Epoch 53/145
95501/95501 - 11s - loss: 7.3384e-04 - val_loss: 6.8203e-04 - 11s/epoch - 114us/sample
Epoch 54/145
95501/95501 - 11s - loss: 7.0776e-04 - val_loss: 6.6853e-04 - 11s/epoch - 115us/sample
Epoch 55/145
95501/95501 - 11s - loss: 7.0061e-04 - val_loss: 6.6865e-04 - 11s/epoch - 115us/sample
Epoch 56/145
95501/95501 - 11s - loss: 6.9910e-04 - val_loss: 6.7600e-04 - 11s/epoch - 115us/sample
Epoch 57/145
95501/95501 - 11s - loss: 7.0399e-04 - val_loss: 6.6122e-04 - 11s/epoch - 114us/sample
Epoch 58/145
95501/95501 - 11s - loss: 7.0035e-04 - val_loss: 6.8675e-04 - 11s/epoch - 114us/sample
Epoch 59/145
95501/95501 - 11s - loss: 6.9394e-04 - val_loss: 6.6073e-04 - 11s/epoch - 113us/sample
Epoch 60/145
95501/95501 - 11s - loss: 6.9017e-04 - val_loss: 6.6003e-04 - 11s/epoch - 114us/sample
Epoch 61/145
95501/95501 - 11s - loss: 6.8873e-04 - val_loss: 6.6130e-04 - 11s/epoch - 115us/sample
Epoch 62/145
95501/95501 - 11s - loss: 6.9470e-04 - val_loss: 6.5478e-04 - 11s/epoch - 115us/sample
Epoch 63/145
95501/95501 - 11s - loss: 6.8610e-04 - val_loss: 7.1265e-04 - 11s/epoch - 116us/sample
Epoch 64/145
95501/95501 - 11s - loss: 7.2660e-04 - val_loss: 7.0799e-04 - 11s/epoch - 114us/sample
Epoch 65/145
95501/95501 - 11s - loss: 7.4120e-04 - val_loss: 6.6690e-04 - 11s/epoch - 114us/sample
Epoch 66/145
95501/95501 - 11s - loss: 6.9245e-04 - val_loss: 7.5554e-04 - 11s/epoch - 114us/sample
Epoch 67/145
95501/95501 - 11s - loss: 7.4797e-04 - val_loss: 6.5863e-04 - 11s/epoch - 113us/sample
Epoch 68/145
95501/95501 - 11s - loss: 6.9729e-04 - val_loss: 6.6007e-04 - 11s/epoch - 114us/sample
Epoch 69/145
95501/95501 - 11s - loss: 6.8967e-04 - val_loss: 6.6837e-04 - 11s/epoch - 114us/sample
Epoch 70/145
95501/95501 - 11s - loss: 6.9699e-04 - val_loss: 6.4900e-04 - 11s/epoch - 114us/sample
Epoch 71/145
95501/95501 - 11s - loss: 6.8243e-04 - val_loss: 6.4826e-04 - 11s/epoch - 116us/sample
Epoch 72/145
95501/95501 - 11s - loss: 6.8071e-04 - val_loss: 6.6156e-04 - 11s/epoch - 114us/sample
Epoch 73/145
95501/95501 - 11s - loss: 6.9464e-04 - val_loss: 6.5162e-04 - 11s/epoch - 114us/sample
Epoch 74/145
95501/95501 - 11s - loss: 6.7876e-04 - val_loss: 6.5372e-04 - 11s/epoch - 114us/sample
Epoch 75/145
95501/95501 - 10s - loss: 6.8362e-04 - val_loss: 6.5539e-04 - 10s/epoch - 109us/sample
Epoch 76/145
95501/95501 - 10s - loss: 6.7625e-04 - val_loss: 6.4976e-04 - 10s/epoch - 108us/sample
Epoch 77/145
95501/95501 - 10s - loss: 6.7517e-04 - val_loss: 6.3910e-04 - 10s/epoch - 108us/sample
Epoch 78/145
95501/95501 - 10s - loss: 6.7206e-04 - val_loss: 6.5680e-04 - 10s/epoch - 108us/sample
Epoch 79/145
95501/95501 - 10s - loss: 6.7729e-04 - val_loss: 6.4687e-04 - 10s/epoch - 109us/sample
Epoch 80/145
95501/95501 - 10s - loss: 6.6910e-04 - val_loss: 6.7629e-04 - 10s/epoch - 108us/sample
Epoch 81/145
95501/95501 - 10s - loss: 6.8399e-04 - val_loss: 6.4034e-04 - 10s/epoch - 107us/sample
Epoch 82/145
95501/95501 - 10s - loss: 6.6930e-04 - val_loss: 6.4072e-04 - 10s/epoch - 108us/sample
Epoch 83/145
95501/95501 - 10s - loss: 6.7466e-04 - val_loss: 6.4160e-04 - 10s/epoch - 108us/sample
Epoch 84/145
95501/95501 - 10s - loss: 6.6626e-04 - val_loss: 6.7860e-04 - 10s/epoch - 108us/sample
Epoch 85/145
95501/95501 - 10s - loss: 7.0684e-04 - val_loss: 6.4201e-04 - 10s/epoch - 108us/sample
Epoch 86/145
95501/95501 - 10s - loss: 6.6939e-04 - val_loss: 7.1512e-04 - 10s/epoch - 109us/sample
Epoch 87/145
95501/95501 - 10s - loss: 7.2648e-04 - val_loss: 6.4213e-04 - 10s/epoch - 108us/sample
Epoch 88/145
95501/95501 - 10s - loss: 6.6966e-04 - val_loss: 6.3814e-04 - 10s/epoch - 108us/sample
Epoch 89/145
95501/95501 - 10s - loss: 6.7129e-04 - val_loss: 6.4479e-04 - 10s/epoch - 108us/sample
Epoch 90/145
95501/95501 - 10s - loss: 6.7103e-04 - val_loss: 6.4119e-04 - 10s/epoch - 108us/sample
Epoch 91/145
95501/95501 - 10s - loss: 6.7072e-04 - val_loss: 6.3790e-04 - 10s/epoch - 108us/sample
Epoch 92/145
95501/95501 - 10s - loss: 6.7040e-04 - val_loss: 6.5709e-04 - 10s/epoch - 108us/sample
Epoch 93/145
95501/95501 - 10s - loss: 6.9605e-04 - val_loss: 6.5518e-04 - 10s/epoch - 109us/sample
Epoch 94/145
95501/95501 - 10s - loss: 6.7716e-04 - val_loss: 6.9056e-04 - 10s/epoch - 108us/sample
Epoch 95/145
95501/95501 - 10s - loss: 7.2259e-04 - val_loss: 6.3968e-04 - 10s/epoch - 108us/sample
Epoch 96/145
95501/95501 - 10s - loss: 6.7118e-04 - val_loss: 6.3970e-04 - 10s/epoch - 108us/sample
Epoch 97/145
95501/95501 - 10s - loss: 6.6584e-04 - val_loss: 6.3579e-04 - 10s/epoch - 108us/sample
Epoch 98/145
95501/95501 - 10s - loss: 6.6307e-04 - val_loss: 6.7886e-04 - 10s/epoch - 108us/sample
Epoch 99/145
95501/95501 - 10s - loss: 6.9010e-04 - val_loss: 6.8652e-04 - 10s/epoch - 108us/sample
Epoch 100/145
95501/95501 - 10s - loss: 7.1788e-04 - val_loss: 6.4138e-04 - 10s/epoch - 108us/sample
Epoch 101/145
95501/95501 - 10s - loss: 6.6858e-04 - val_loss: 6.3834e-04 - 10s/epoch - 109us/sample
Epoch 102/145
95501/95501 - 10s - loss: 6.6209e-04 - val_loss: 6.5450e-04 - 10s/epoch - 109us/sample
Epoch 103/145
95501/95501 - 10s - loss: 6.7364e-04 - val_loss: 6.3100e-04 - 10s/epoch - 108us/sample
Epoch 104/145
95501/95501 - 10s - loss: 6.5991e-04 - val_loss: 6.2997e-04 - 10s/epoch - 108us/sample
Epoch 105/145
95501/95501 - 10s - loss: 6.5921e-04 - val_loss: 6.2909e-04 - 10s/epoch - 108us/sample
Epoch 106/145
95501/95501 - 10s - loss: 6.6021e-04 - val_loss: 6.9853e-04 - 10s/epoch - 107us/sample
Epoch 107/145
95501/95501 - 10s - loss: 7.0527e-04 - val_loss: 6.4749e-04 - 10s/epoch - 108us/sample
Epoch 108/145
95501/95501 - 10s - loss: 6.7634e-04 - val_loss: 6.3017e-04 - 10s/epoch - 108us/sample
Epoch 109/145
95501/95501 - 10s - loss: 6.5795e-04 - val_loss: 6.2631e-04 - 10s/epoch - 109us/sample
Epoch 110/145
95501/95501 - 10s - loss: 6.6014e-04 - val_loss: 6.3249e-04 - 10s/epoch - 109us/sample
Epoch 111/145
95501/95501 - 10s - loss: 6.5507e-04 - val_loss: 6.3209e-04 - 10s/epoch - 108us/sample
Epoch 112/145
95501/95501 - 10s - loss: 6.5279e-04 - val_loss: 6.3040e-04 - 10s/epoch - 108us/sample
Epoch 113/145
95501/95501 - 10s - loss: 6.5067e-04 - val_loss: 6.2421e-04 - 10s/epoch - 108us/sample
Epoch 114/145
95501/95501 - 10s - loss: 6.5246e-04 - val_loss: 6.6703e-04 - 10s/epoch - 108us/sample
Epoch 115/145
95501/95501 - 10s - loss: 6.7970e-04 - val_loss: 6.3823e-04 - 10s/epoch - 108us/sample
Epoch 116/145
95501/95501 - 10s - loss: 6.5209e-04 - val_loss: 6.4290e-04 - 10s/epoch - 108us/sample
Epoch 117/145
95501/95501 - 10s - loss: 7.1569e-04 - val_loss: 6.3441e-04 - 10s/epoch - 109us/sample
Epoch 118/145
95501/95501 - 10s - loss: 6.5548e-04 - val_loss: 6.3010e-04 - 10s/epoch - 108us/sample
Epoch 119/145
95501/95501 - 10s - loss: 6.5408e-04 - val_loss: 6.2190e-04 - 10s/epoch - 108us/sample
Epoch 120/145
95501/95501 - 10s - loss: 6.4991e-04 - val_loss: 6.1986e-04 - 10s/epoch - 108us/sample
Epoch 121/145
95501/95501 - 10s - loss: 6.4968e-04 - val_loss: 6.2025e-04 - 10s/epoch - 108us/sample
Epoch 122/145
95501/95501 - 10s - loss: 6.7466e-04 - val_loss: 6.5242e-04 - 10s/epoch - 108us/sample
Epoch 123/145
95501/95501 - 10s - loss: 6.6489e-04 - val_loss: 6.5030e-04 - 10s/epoch - 108us/sample
Epoch 124/145
95501/95501 - 10s - loss: 6.6126e-04 - val_loss: 6.2066e-04 - 10s/epoch - 109us/sample
Epoch 125/145
95501/95501 - 10s - loss: 6.4969e-04 - val_loss: 6.1839e-04 - 10s/epoch - 110us/sample
Epoch 126/145
95501/95501 - 10s - loss: 6.4716e-04 - val_loss: 6.2225e-04 - 10s/epoch - 108us/sample
Epoch 127/145
95501/95501 - 10s - loss: 6.4548e-04 - val_loss: 6.3476e-04 - 10s/epoch - 107us/sample
Epoch 128/145
95501/95501 - 10s - loss: 6.4658e-04 - val_loss: 6.2575e-04 - 10s/epoch - 108us/sample
Epoch 129/145
95501/95501 - 10s - loss: 6.4579e-04 - val_loss: 6.2038e-04 - 10s/epoch - 108us/sample
Epoch 130/145
95501/95501 - 10s - loss: 6.4879e-04 - val_loss: 6.5887e-04 - 10s/epoch - 109us/sample
Epoch 131/145
95501/95501 - 10s - loss: 6.8222e-04 - val_loss: 6.3982e-04 - 10s/epoch - 108us/sample
Epoch 132/145
95501/95501 - 10s - loss: 6.7214e-04 - val_loss: 6.1872e-04 - 10s/epoch - 108us/sample
Epoch 133/145
95501/95501 - 10s - loss: 6.4651e-04 - val_loss: 6.6738e-04 - 10s/epoch - 110us/sample
Epoch 134/145
95501/95501 - 10s - loss: 6.9019e-04 - val_loss: 6.2257e-04 - 10s/epoch - 109us/sample
Epoch 135/145
95501/95501 - 10s - loss: 6.4818e-04 - val_loss: 6.2233e-04 - 10s/epoch - 108us/sample
Epoch 136/145
95501/95501 - 10s - loss: 6.5055e-04 - val_loss: 6.2467e-04 - 10s/epoch - 109us/sample
Epoch 137/145
95501/95501 - 10s - loss: 6.4426e-04 - val_loss: 6.1237e-04 - 10s/epoch - 108us/sample
Epoch 138/145
95501/95501 - 10s - loss: 6.4568e-04 - val_loss: 6.2109e-04 - 10s/epoch - 108us/sample
Epoch 139/145
95501/95501 - 10s - loss: 6.4648e-04 - val_loss: 7.1201e-04 - 10s/epoch - 108us/sample
Epoch 140/145
95501/95501 - 10s - loss: 7.1746e-04 - val_loss: 6.2452e-04 - 10s/epoch - 108us/sample
Epoch 141/145
95501/95501 - 10s - loss: 6.5001e-04 - val_loss: 6.7289e-04 - 10s/epoch - 109us/sample
Epoch 142/145
95501/95501 - 10s - loss: 6.8759e-04 - val_loss: 6.1959e-04 - 10s/epoch - 109us/sample
Epoch 143/145
95501/95501 - 10s - loss: 6.6072e-04 - val_loss: 6.1756e-04 - 10s/epoch - 108us/sample
Epoch 144/145
95501/95501 - 10s - loss: 6.4340e-04 - val_loss: 6.1345e-04 - 10s/epoch - 108us/sample
Epoch 145/145
95501/95501 - 10s - loss: 6.4407e-04 - val_loss: 6.1167e-04 - 10s/epoch - 108us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0006116734539376992
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:31:06.230877: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_20/outputlayer/BiasAdd' id:25981 op device:{requested: '', assigned: ''} def:{{{node decoder_model_20/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_20/outputlayer/MatMul, decoder_model_20/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.007956133966530398
cosine 0.006277248111707599
MAE: 0.013360351
RMSE: 0.026148034
r2: 0.955645633893364
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'logcosh', 64, 145, 0.0012, 0.3, 379, 0.0006440738112305972, 0.0006116734539376992, 0.007956133966530398, 0.006277248111707599, 0.013360351324081421, 0.026148034259676933, 0.955645633893364, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 145 0.0012 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_63 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_63 (ReLU)                (None, 1896)         0           ['batch_normalization_63[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 00:31:18.081447: W tensorflow/c/c_api.cc:291] Operation '{name:'training_42/Adam/dense_dec1_21/kernel/v/Assign' id:27952 op device:{requested: '', assigned: ''} def:{{{node training_42/Adam/dense_dec1_21/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_42/Adam/dense_dec1_21/kernel/v, training_42/Adam/dense_dec1_21/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:31:30.513036: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_21/mul' id:27308 op device:{requested: '', assigned: ''} def:{{{node loss_21/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_21/mul/x, loss_21/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 16s - loss: 0.0068 - val_loss: 0.0036 - 16s/epoch - 170us/sample
Epoch 2/145
95501/95501 - 11s - loss: 0.0030 - val_loss: 0.0029 - 11s/epoch - 110us/sample
Epoch 3/145
95501/95501 - 11s - loss: 47.8372 - val_loss: 0.0075 - 11s/epoch - 111us/sample
Epoch 4/145
95501/95501 - 11s - loss: 0.0021 - val_loss: 0.0018 - 11s/epoch - 110us/sample
Epoch 5/145
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0021 - 11s/epoch - 110us/sample
Epoch 6/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0016 - 11s/epoch - 110us/sample
Epoch 7/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0013 - 11s/epoch - 110us/sample
Epoch 8/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 111us/sample
Epoch 9/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 110us/sample
Epoch 10/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 112us/sample
Epoch 11/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0021 - 11s/epoch - 111us/sample
Epoch 12/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.8138e-04 - 11s/epoch - 110us/sample
Epoch 13/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.1036e-04 - 11s/epoch - 110us/sample
Epoch 14/145
95501/95501 - 11s - loss: 9.6962e-04 - val_loss: 9.1255e-04 - 11s/epoch - 111us/sample
Epoch 15/145
95501/95501 - 11s - loss: 9.6099e-04 - val_loss: 9.3855e-04 - 11s/epoch - 111us/sample
Epoch 16/145
95501/95501 - 11s - loss: 9.2701e-04 - val_loss: 9.8136e-04 - 11s/epoch - 110us/sample
Epoch 17/145
95501/95501 - 11s - loss: 9.1820e-04 - val_loss: 8.5996e-04 - 11s/epoch - 110us/sample
Epoch 18/145
95501/95501 - 11s - loss: 8.9249e-04 - val_loss: 8.7565e-04 - 11s/epoch - 111us/sample
Epoch 19/145
95501/95501 - 11s - loss: 8.8412e-04 - val_loss: 0.0015 - 11s/epoch - 112us/sample
Epoch 20/145
95501/95501 - 11s - loss: 9.5622e-04 - val_loss: 0.0013 - 11s/epoch - 110us/sample
Epoch 21/145
95501/95501 - 11s - loss: 9.3227e-04 - val_loss: 8.1015e-04 - 11s/epoch - 111us/sample
Epoch 22/145
95501/95501 - 10s - loss: 8.4799e-04 - val_loss: 8.0283e-04 - 10s/epoch - 110us/sample
Epoch 23/145
95501/95501 - 11s - loss: 8.4282e-04 - val_loss: 8.2981e-04 - 11s/epoch - 110us/sample
Epoch 24/145
95501/95501 - 11s - loss: 8.3081e-04 - val_loss: 8.1305e-04 - 11s/epoch - 111us/sample
Epoch 25/145
95501/95501 - 11s - loss: 8.2879e-04 - val_loss: 7.8652e-04 - 11s/epoch - 111us/sample
Epoch 26/145
95501/95501 - 11s - loss: 8.0593e-04 - val_loss: 7.7981e-04 - 11s/epoch - 111us/sample
Epoch 27/145
95501/95501 - 11s - loss: 7.9692e-04 - val_loss: 7.5399e-04 - 11s/epoch - 111us/sample
Epoch 28/145
95501/95501 - 11s - loss: 7.8951e-04 - val_loss: 7.5193e-04 - 11s/epoch - 110us/sample
Epoch 29/145
95501/95501 - 11s - loss: 7.8776e-04 - val_loss: 7.3709e-04 - 11s/epoch - 111us/sample
Epoch 30/145
95501/95501 - 10s - loss: 7.7334e-04 - val_loss: 7.3799e-04 - 10s/epoch - 110us/sample
Epoch 31/145
95501/95501 - 11s - loss: 7.6938e-04 - val_loss: 8.0292e-04 - 11s/epoch - 111us/sample
Epoch 32/145
95501/95501 - 11s - loss: 7.9805e-04 - val_loss: 7.2196e-04 - 11s/epoch - 111us/sample
Epoch 33/145
95501/95501 - 11s - loss: 7.5321e-04 - val_loss: 7.2128e-04 - 11s/epoch - 111us/sample
Epoch 34/145
95501/95501 - 11s - loss: 7.6435e-04 - val_loss: 7.3036e-04 - 11s/epoch - 111us/sample
Epoch 35/145
95501/95501 - 11s - loss: 7.5219e-04 - val_loss: 7.2453e-04 - 11s/epoch - 111us/sample
Epoch 36/145
95501/95501 - 11s - loss: 7.4204e-04 - val_loss: 7.1578e-04 - 11s/epoch - 111us/sample
Epoch 37/145
95501/95501 - 11s - loss: 7.3771e-04 - val_loss: 7.1259e-04 - 11s/epoch - 111us/sample
Epoch 38/145
95501/95501 - 11s - loss: 7.4064e-04 - val_loss: 7.0101e-04 - 11s/epoch - 110us/sample
Epoch 39/145
95501/95501 - 11s - loss: 7.2965e-04 - val_loss: 7.3426e-04 - 11s/epoch - 111us/sample
Epoch 40/145
95501/95501 - 11s - loss: 7.4253e-04 - val_loss: 6.9413e-04 - 11s/epoch - 110us/sample
Epoch 41/145
95501/95501 - 11s - loss: 7.2396e-04 - val_loss: 6.9252e-04 - 11s/epoch - 111us/sample
Epoch 42/145
95501/95501 - 11s - loss: 7.2298e-04 - val_loss: 6.9697e-04 - 11s/epoch - 112us/sample
Epoch 43/145
95501/95501 - 11s - loss: 7.2113e-04 - val_loss: 6.8677e-04 - 11s/epoch - 111us/sample
Epoch 44/145
95501/95501 - 11s - loss: 7.1727e-04 - val_loss: 6.8359e-04 - 11s/epoch - 110us/sample
Epoch 45/145
95501/95501 - 11s - loss: 7.1417e-04 - val_loss: 6.9863e-04 - 11s/epoch - 110us/sample
Epoch 46/145
95501/95501 - 11s - loss: 7.2812e-04 - val_loss: 7.5867e-04 - 11s/epoch - 111us/sample
Epoch 47/145
95501/95501 - 11s - loss: 7.6180e-04 - val_loss: 6.7932e-04 - 11s/epoch - 111us/sample
Epoch 48/145
95501/95501 - 11s - loss: 7.1312e-04 - val_loss: 6.7602e-04 - 11s/epoch - 111us/sample
Epoch 49/145
95501/95501 - 11s - loss: 7.1009e-04 - val_loss: 6.8217e-04 - 11s/epoch - 111us/sample
Epoch 50/145
95501/95501 - 11s - loss: 7.1391e-04 - val_loss: 6.7717e-04 - 11s/epoch - 110us/sample
Epoch 51/145
95501/95501 - 11s - loss: 7.0273e-04 - val_loss: 6.7017e-04 - 11s/epoch - 112us/sample
Epoch 52/145
95501/95501 - 11s - loss: 7.0546e-04 - val_loss: 8.2547e-04 - 11s/epoch - 111us/sample
Epoch 53/145
95501/95501 - 11s - loss: 7.3486e-04 - val_loss: 6.6973e-04 - 11s/epoch - 111us/sample
Epoch 54/145
95501/95501 - 11s - loss: 7.0092e-04 - val_loss: 6.6624e-04 - 11s/epoch - 110us/sample
Epoch 55/145
95501/95501 - 11s - loss: 6.9896e-04 - val_loss: 6.6771e-04 - 11s/epoch - 110us/sample
Epoch 56/145
95501/95501 - 11s - loss: 6.9245e-04 - val_loss: 6.6335e-04 - 11s/epoch - 111us/sample
Epoch 57/145
95501/95501 - 11s - loss: 6.9316e-04 - val_loss: 6.6052e-04 - 11s/epoch - 110us/sample
Epoch 58/145
95501/95501 - 11s - loss: 6.8952e-04 - val_loss: 6.6933e-04 - 11s/epoch - 111us/sample
Epoch 59/145
95501/95501 - 11s - loss: 6.9495e-04 - val_loss: 6.6102e-04 - 11s/epoch - 111us/sample
Epoch 60/145
95501/95501 - 11s - loss: 6.8779e-04 - val_loss: 7.0531e-04 - 11s/epoch - 111us/sample
Epoch 61/145
95501/95501 - 11s - loss: 6.8689e-04 - val_loss: 6.6184e-04 - 11s/epoch - 110us/sample
Epoch 62/145
95501/95501 - 11s - loss: 6.8240e-04 - val_loss: 6.6096e-04 - 11s/epoch - 110us/sample
Epoch 63/145
95501/95501 - 11s - loss: 6.8222e-04 - val_loss: 6.6534e-04 - 11s/epoch - 110us/sample
Epoch 64/145
95501/95501 - 11s - loss: 7.0785e-04 - val_loss: 6.6191e-04 - 11s/epoch - 110us/sample
Epoch 65/145
95501/95501 - 11s - loss: 6.8226e-04 - val_loss: 6.5576e-04 - 11s/epoch - 111us/sample
Epoch 66/145
95501/95501 - 11s - loss: 6.7836e-04 - val_loss: 6.5084e-04 - 11s/epoch - 111us/sample
Epoch 67/145
95501/95501 - 11s - loss: 6.8047e-04 - val_loss: 6.5566e-04 - 11s/epoch - 111us/sample
Epoch 68/145
95501/95501 - 11s - loss: 6.8127e-04 - val_loss: 6.5520e-04 - 11s/epoch - 110us/sample
Epoch 69/145
95501/95501 - 11s - loss: 6.7609e-04 - val_loss: 6.5099e-04 - 11s/epoch - 111us/sample
Epoch 70/145
95501/95501 - 11s - loss: 6.7620e-04 - val_loss: 7.4674e-04 - 11s/epoch - 110us/sample
Epoch 71/145
95501/95501 - 11s - loss: 7.3335e-04 - val_loss: 6.5335e-04 - 11s/epoch - 111us/sample
Epoch 72/145
95501/95501 - 11s - loss: 6.7995e-04 - val_loss: 6.4485e-04 - 11s/epoch - 110us/sample
Epoch 73/145
95501/95501 - 11s - loss: 6.7517e-04 - val_loss: 6.5166e-04 - 11s/epoch - 111us/sample
Epoch 74/145
95501/95501 - 11s - loss: 6.7863e-04 - val_loss: 6.4385e-04 - 11s/epoch - 111us/sample
Epoch 75/145
95501/95501 - 11s - loss: 6.7326e-04 - val_loss: 7.2361e-04 - 11s/epoch - 111us/sample
Epoch 76/145
95501/95501 - 11s - loss: 7.3851e-04 - val_loss: 6.5081e-04 - 11s/epoch - 111us/sample
Epoch 77/145
95501/95501 - 11s - loss: 6.7376e-04 - val_loss: 6.4006e-04 - 11s/epoch - 110us/sample
Epoch 78/145
95501/95501 - 11s - loss: 6.7092e-04 - val_loss: 6.4585e-04 - 11s/epoch - 110us/sample
Epoch 79/145
95501/95501 - 11s - loss: 6.7622e-04 - val_loss: 6.6159e-04 - 11s/epoch - 110us/sample
Epoch 80/145
95501/95501 - 11s - loss: 6.7292e-04 - val_loss: 6.4192e-04 - 11s/epoch - 110us/sample
Epoch 81/145
95501/95501 - 11s - loss: 6.6866e-04 - val_loss: 6.5269e-04 - 11s/epoch - 110us/sample
Epoch 82/145
95501/95501 - 11s - loss: 6.6789e-04 - val_loss: 6.5998e-04 - 11s/epoch - 110us/sample
Epoch 83/145
95501/95501 - 11s - loss: 6.8828e-04 - val_loss: 7.1519e-04 - 11s/epoch - 111us/sample
Epoch 84/145
95501/95501 - 11s - loss: 7.1608e-04 - val_loss: 6.4168e-04 - 11s/epoch - 111us/sample
Epoch 85/145
95501/95501 - 11s - loss: 6.7410e-04 - val_loss: 6.4330e-04 - 11s/epoch - 110us/sample
Epoch 86/145
95501/95501 - 11s - loss: 6.6830e-04 - val_loss: 6.5841e-04 - 11s/epoch - 111us/sample
Epoch 87/145
95501/95501 - 11s - loss: 6.8153e-04 - val_loss: 6.4001e-04 - 11s/epoch - 110us/sample
Epoch 88/145
95501/95501 - 11s - loss: 6.6501e-04 - val_loss: 6.6198e-04 - 11s/epoch - 111us/sample
Epoch 89/145
95501/95501 - 11s - loss: 6.9264e-04 - val_loss: 6.4364e-04 - 11s/epoch - 110us/sample
Epoch 90/145
95501/95501 - 11s - loss: 6.6427e-04 - val_loss: 6.3431e-04 - 11s/epoch - 110us/sample
Epoch 91/145
95501/95501 - 11s - loss: 6.6146e-04 - val_loss: 6.3502e-04 - 11s/epoch - 111us/sample
Epoch 92/145
95501/95501 - 11s - loss: 6.6175e-04 - val_loss: 7.1926e-04 - 11s/epoch - 110us/sample
Epoch 93/145
95501/95501 - 11s - loss: 7.0893e-04 - val_loss: 6.3522e-04 - 11s/epoch - 111us/sample
Epoch 94/145
95501/95501 - 11s - loss: 6.6237e-04 - val_loss: 6.3519e-04 - 11s/epoch - 110us/sample
Epoch 95/145
95501/95501 - 11s - loss: 6.5792e-04 - val_loss: 6.3391e-04 - 11s/epoch - 110us/sample
Epoch 96/145
95501/95501 - 11s - loss: 6.5778e-04 - val_loss: 6.3799e-04 - 11s/epoch - 110us/sample
Epoch 97/145
95501/95501 - 11s - loss: 6.6009e-04 - val_loss: 6.3465e-04 - 11s/epoch - 111us/sample
Epoch 98/145
95501/95501 - 11s - loss: 6.5658e-04 - val_loss: 6.2269e-04 - 11s/epoch - 110us/sample
Epoch 99/145
95501/95501 - 11s - loss: 6.5621e-04 - val_loss: 6.4743e-04 - 11s/epoch - 112us/sample
Epoch 100/145
95501/95501 - 11s - loss: 6.5238e-04 - val_loss: 6.3386e-04 - 11s/epoch - 110us/sample
Epoch 101/145
95501/95501 - 11s - loss: 6.5529e-04 - val_loss: 6.2890e-04 - 11s/epoch - 110us/sample
Epoch 102/145
95501/95501 - 11s - loss: 6.5888e-04 - val_loss: 7.5348e-04 - 11s/epoch - 110us/sample
Epoch 103/145
95501/95501 - 11s - loss: 7.4093e-04 - val_loss: 6.3998e-04 - 11s/epoch - 110us/sample
Epoch 104/145
95501/95501 - 11s - loss: 6.5908e-04 - val_loss: 6.3014e-04 - 11s/epoch - 110us/sample
Epoch 105/145
95501/95501 - 11s - loss: 7.0704e-04 - val_loss: 6.4410e-04 - 11s/epoch - 111us/sample
Epoch 106/145
95501/95501 - 11s - loss: 6.6393e-04 - val_loss: 6.8996e-04 - 11s/epoch - 110us/sample
Epoch 107/145
95501/95501 - 11s - loss: 7.2319e-04 - val_loss: 6.3815e-04 - 11s/epoch - 111us/sample
Epoch 108/145
95501/95501 - 11s - loss: 6.6030e-04 - val_loss: 6.2899e-04 - 11s/epoch - 110us/sample
Epoch 109/145
95501/95501 - 11s - loss: 6.9704e-04 - val_loss: 6.3288e-04 - 11s/epoch - 110us/sample
Epoch 110/145
95501/95501 - 11s - loss: 6.5675e-04 - val_loss: 6.3096e-04 - 11s/epoch - 110us/sample
Epoch 111/145
95501/95501 - 11s - loss: 6.5382e-04 - val_loss: 6.2706e-04 - 11s/epoch - 111us/sample
Epoch 112/145
95501/95501 - 11s - loss: 6.5120e-04 - val_loss: 6.3427e-04 - 11s/epoch - 110us/sample
Epoch 113/145
95501/95501 - 11s - loss: 6.5872e-04 - val_loss: 6.2238e-04 - 11s/epoch - 111us/sample
Epoch 114/145
95501/95501 - 11s - loss: 6.5232e-04 - val_loss: 6.2833e-04 - 11s/epoch - 112us/sample
Epoch 115/145
95501/95501 - 11s - loss: 6.5372e-04 - val_loss: 6.2469e-04 - 11s/epoch - 111us/sample
Epoch 116/145
95501/95501 - 11s - loss: 6.4908e-04 - val_loss: 6.2277e-04 - 11s/epoch - 111us/sample
Epoch 117/145
95501/95501 - 11s - loss: 6.4627e-04 - val_loss: 6.1828e-04 - 11s/epoch - 110us/sample
Epoch 118/145
95501/95501 - 11s - loss: 6.4679e-04 - val_loss: 6.1734e-04 - 11s/epoch - 111us/sample
Epoch 119/145
95501/95501 - 11s - loss: 6.4505e-04 - val_loss: 6.2150e-04 - 11s/epoch - 110us/sample
Epoch 120/145
95501/95501 - 11s - loss: 6.4356e-04 - val_loss: 6.1786e-04 - 11s/epoch - 110us/sample
Epoch 121/145
95501/95501 - 11s - loss: 6.4565e-04 - val_loss: 6.1483e-04 - 11s/epoch - 111us/sample
Epoch 122/145
95501/95501 - 11s - loss: 6.4497e-04 - val_loss: 6.7127e-04 - 11s/epoch - 111us/sample
Epoch 123/145
95501/95501 - 11s - loss: 6.7080e-04 - val_loss: 6.2292e-04 - 11s/epoch - 111us/sample
Epoch 124/145
95501/95501 - 11s - loss: 6.4827e-04 - val_loss: 6.1913e-04 - 11s/epoch - 110us/sample
Epoch 125/145
95501/95501 - 11s - loss: 6.4133e-04 - val_loss: 6.2125e-04 - 11s/epoch - 110us/sample
Epoch 126/145
95501/95501 - 11s - loss: 6.4114e-04 - val_loss: 6.1557e-04 - 11s/epoch - 110us/sample
Epoch 127/145
95501/95501 - 11s - loss: 6.4213e-04 - val_loss: 6.2153e-04 - 11s/epoch - 111us/sample
Epoch 128/145
95501/95501 - 11s - loss: 6.4214e-04 - val_loss: 6.2516e-04 - 11s/epoch - 110us/sample
Epoch 129/145
95501/95501 - 11s - loss: 6.5082e-04 - val_loss: 7.3000e-04 - 11s/epoch - 111us/sample
Epoch 130/145
95501/95501 - 11s - loss: 7.2127e-04 - val_loss: 6.7676e-04 - 11s/epoch - 110us/sample
Epoch 131/145
95501/95501 - 11s - loss: 6.8708e-04 - val_loss: 6.2814e-04 - 11s/epoch - 112us/sample
Epoch 132/145
95501/95501 - 11s - loss: 6.4858e-04 - val_loss: 6.1549e-04 - 11s/epoch - 111us/sample
Epoch 133/145
95501/95501 - 11s - loss: 6.4247e-04 - val_loss: 6.2207e-04 - 11s/epoch - 110us/sample
Epoch 134/145
95501/95501 - 11s - loss: 6.4297e-04 - val_loss: 6.1578e-04 - 11s/epoch - 111us/sample
Epoch 135/145
95501/95501 - 11s - loss: 6.4185e-04 - val_loss: 6.4906e-04 - 11s/epoch - 110us/sample
Epoch 136/145
95501/95501 - 11s - loss: 6.7110e-04 - val_loss: 6.1663e-04 - 11s/epoch - 110us/sample
Epoch 137/145
95501/95501 - 11s - loss: 6.4028e-04 - val_loss: 6.3160e-04 - 11s/epoch - 110us/sample
Epoch 138/145
95501/95501 - 11s - loss: 6.3917e-04 - val_loss: 6.1240e-04 - 11s/epoch - 110us/sample
Epoch 139/145
95501/95501 - 11s - loss: 6.4588e-04 - val_loss: 6.3280e-04 - 11s/epoch - 111us/sample
Epoch 140/145
95501/95501 - 11s - loss: 6.4459e-04 - val_loss: 6.1669e-04 - 11s/epoch - 111us/sample
Epoch 141/145
95501/95501 - 11s - loss: 6.3764e-04 - val_loss: 6.2403e-04 - 11s/epoch - 111us/sample
Epoch 142/145
95501/95501 - 11s - loss: 6.4145e-04 - val_loss: 6.1122e-04 - 11s/epoch - 110us/sample
Epoch 143/145
95501/95501 - 11s - loss: 6.3686e-04 - val_loss: 6.1591e-04 - 11s/epoch - 110us/sample
Epoch 144/145
95501/95501 - 11s - loss: 6.3862e-04 - val_loss: 6.2331e-04 - 11s/epoch - 110us/sample
Epoch 145/145
95501/95501 - 11s - loss: 6.3530e-04 - val_loss: 6.2083e-04 - 11s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0006208277079798042
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:56:53.604835: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_21/outputlayer/BiasAdd' id:27272 op device:{requested: '', assigned: ''} def:{{{node decoder_model_21/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_21/outputlayer/MatMul, decoder_model_21/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.00815562629223409
cosine 0.006444007680634485
MAE: 0.013532735
RMSE: 0.026462276
r2: 0.954573210953798
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 64, 145, 0.0012, 0.3, 379, 0.0006353017456936312, 0.0006208277079798042, 0.00815562629223409, 0.006444007680634485, 0.013532735407352448, 0.02646227553486824, 0.954573210953798, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 145 0.0008 8 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_66 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_66 (ReLU)                (None, 1896)         0           ['batch_normalization_66[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 00:57:05.892205: W tensorflow/c/c_api.cc:291] Operation '{name:'training_44/Adam/decay/Assign' id:29033 op device:{requested: '', assigned: ''} def:{{{node training_44/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_44/Adam/decay, training_44/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:57:55.839848: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_22/mul' id:28592 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 56s - loss: 0.0103 - val_loss: 0.0088 - 56s/epoch - 586us/sample
Epoch 2/145
95501/95501 - 50s - loss: 0.0053 - val_loss: 0.0072 - 50s/epoch - 521us/sample
Epoch 3/145
95501/95501 - 50s - loss: 0.0045 - val_loss: 0.0066 - 50s/epoch - 522us/sample
Epoch 4/145
95501/95501 - 50s - loss: 0.0041 - val_loss: 0.0057 - 50s/epoch - 523us/sample
Epoch 5/145
95501/95501 - 50s - loss: 0.0038 - val_loss: 0.0042 - 50s/epoch - 522us/sample
Epoch 6/145
95501/95501 - 50s - loss: 0.0036 - val_loss: 0.0050 - 50s/epoch - 521us/sample
Epoch 7/145
95501/95501 - 50s - loss: 0.0035 - val_loss: 0.0038 - 50s/epoch - 523us/sample
Epoch 8/145
95501/95501 - 50s - loss: 0.0034 - val_loss: 0.0039 - 50s/epoch - 522us/sample
Epoch 9/145
95501/95501 - 50s - loss: 0.0033 - val_loss: 0.0038 - 50s/epoch - 521us/sample
Epoch 10/145
95501/95501 - 50s - loss: 0.0033 - val_loss: 0.0039 - 50s/epoch - 520us/sample
Epoch 11/145
95501/95501 - 50s - loss: 0.0032 - val_loss: 0.0037 - 50s/epoch - 522us/sample
Epoch 12/145
95501/95501 - 50s - loss: 0.0032 - val_loss: 0.0034 - 50s/epoch - 523us/sample
Epoch 13/145
95501/95501 - 50s - loss: 0.0031 - val_loss: 0.0035 - 50s/epoch - 520us/sample
Epoch 14/145
95501/95501 - 50s - loss: 0.0031 - val_loss: 0.0036 - 50s/epoch - 522us/sample
Epoch 15/145
95501/95501 - 50s - loss: 0.0031 - val_loss: 0.0040 - 50s/epoch - 522us/sample
Epoch 16/145
95501/95501 - 50s - loss: 0.0030 - val_loss: 0.0032 - 50s/epoch - 520us/sample
Epoch 17/145
95501/95501 - 50s - loss: 0.0030 - val_loss: 0.0033 - 50s/epoch - 522us/sample
Epoch 18/145
95501/95501 - 50s - loss: 0.0030 - val_loss: 0.0031 - 50s/epoch - 519us/sample
Epoch 19/145
95501/95501 - 50s - loss: 0.0030 - val_loss: 0.0031 - 50s/epoch - 520us/sample
Epoch 20/145
95501/95501 - 50s - loss: 0.0029 - val_loss: 0.0029 - 50s/epoch - 522us/sample
Epoch 21/145
95501/95501 - 50s - loss: 0.0029 - val_loss: 0.0034 - 50s/epoch - 519us/sample
Epoch 22/145
95501/95501 - 50s - loss: 0.0029 - val_loss: 0.0029 - 50s/epoch - 520us/sample
Epoch 23/145
95501/95501 - 50s - loss: 0.0029 - val_loss: 0.0032 - 50s/epoch - 519us/sample
Epoch 24/145
95501/95501 - 50s - loss: 0.0029 - val_loss: 0.0029 - 50s/epoch - 519us/sample
Epoch 25/145
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0030 - 49s/epoch - 518us/sample
Epoch 26/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0028 - 50s/epoch - 519us/sample
Epoch 27/145
95501/95501 - 50s - loss: 0.0029 - val_loss: 0.0029 - 50s/epoch - 519us/sample
Epoch 28/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0029 - 50s/epoch - 520us/sample
Epoch 29/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0028 - 50s/epoch - 520us/sample
Epoch 30/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 31/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0027 - 50s/epoch - 522us/sample
Epoch 32/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0029 - 50s/epoch - 520us/sample
Epoch 33/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0034 - 50s/epoch - 522us/sample
Epoch 34/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0028 - 50s/epoch - 522us/sample
Epoch 35/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0031 - 50s/epoch - 521us/sample
Epoch 36/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0030 - 50s/epoch - 521us/sample
Epoch 37/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0030 - 50s/epoch - 522us/sample
Epoch 38/145
95501/95501 - 50s - loss: 0.0028 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 39/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 522us/sample
Epoch 40/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0030 - 50s/epoch - 521us/sample
Epoch 41/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0031 - 50s/epoch - 522us/sample
Epoch 42/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 43/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0027 - 50s/epoch - 522us/sample
Epoch 44/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 522us/sample
Epoch 45/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 46/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 522us/sample
Epoch 47/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 48/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0030 - 50s/epoch - 522us/sample
Epoch 49/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 50/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0028 - 50s/epoch - 521us/sample
Epoch 51/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0026 - 50s/epoch - 519us/sample
Epoch 52/145
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0029 - 49s/epoch - 517us/sample
Epoch 53/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 519us/sample
Epoch 54/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 519us/sample
Epoch 55/145
95501/95501 - 49s - loss: 0.0027 - val_loss: 0.0030 - 49s/epoch - 517us/sample
Epoch 56/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 519us/sample
Epoch 57/145
95501/95501 - 49s - loss: 0.0028 - val_loss: 0.0030 - 49s/epoch - 517us/sample
Epoch 58/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 520us/sample
Epoch 59/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0030 - 50s/epoch - 519us/sample
Epoch 60/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0031 - 50s/epoch - 520us/sample
Epoch 61/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 520us/sample
Epoch 62/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 520us/sample
Epoch 63/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0031 - 50s/epoch - 521us/sample
Epoch 64/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 65/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0026 - 50s/epoch - 523us/sample
Epoch 66/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 67/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0031 - 50s/epoch - 521us/sample
Epoch 68/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0027 - 50s/epoch - 521us/sample
Epoch 69/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0028 - 50s/epoch - 521us/sample
Epoch 70/145
95501/95501 - 50s - loss: 0.0027 - val_loss: 0.0029 - 50s/epoch - 523us/sample
Epoch 71/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 523us/sample
Epoch 72/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0027 - 50s/epoch - 520us/sample
Epoch 73/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 521us/sample
Epoch 74/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0028 - 50s/epoch - 522us/sample
Epoch 75/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 521us/sample
Epoch 76/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 522us/sample
Epoch 77/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0031 - 50s/epoch - 520us/sample
Epoch 78/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0028 - 50s/epoch - 519us/sample
Epoch 79/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0028 - 50s/epoch - 521us/sample
Epoch 80/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0028 - 50s/epoch - 521us/sample
Epoch 81/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 522us/sample
Epoch 82/145
95501/95501 - 49s - loss: 0.0026 - val_loss: 0.0031 - 49s/epoch - 518us/sample
Epoch 83/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 520us/sample
Epoch 84/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 520us/sample
Epoch 85/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 523us/sample
Epoch 86/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0027 - 50s/epoch - 524us/sample
Epoch 87/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0028 - 50s/epoch - 524us/sample
Epoch 88/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 524us/sample
Epoch 89/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0027 - 50s/epoch - 526us/sample
Epoch 90/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0028 - 50s/epoch - 524us/sample
Epoch 91/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0028 - 50s/epoch - 526us/sample
Epoch 92/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 527us/sample
Epoch 93/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0027 - 50s/epoch - 525us/sample
Epoch 94/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 527us/sample
Epoch 95/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 96/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0027 - 50s/epoch - 527us/sample
Epoch 97/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0032 - 50s/epoch - 528us/sample
Epoch 98/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 526us/sample
Epoch 99/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0029 - 50s/epoch - 527us/sample
Epoch 100/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0031 - 50s/epoch - 528us/sample
Epoch 101/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 528us/sample
Epoch 102/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0029 - 50s/epoch - 527us/sample
Epoch 103/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 104/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0060 - 50s/epoch - 528us/sample
Epoch 105/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0031 - 50s/epoch - 526us/sample
Epoch 106/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0035 - 50s/epoch - 527us/sample
Epoch 107/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0032 - 50s/epoch - 526us/sample
Epoch 108/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 527us/sample
Epoch 109/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0032 - 50s/epoch - 526us/sample
Epoch 110/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0033 - 50s/epoch - 529us/sample
Epoch 111/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0029 - 50s/epoch - 526us/sample
Epoch 112/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0465 - 50s/epoch - 527us/sample
Epoch 113/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0033 - 50s/epoch - 528us/sample
Epoch 114/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0027 - 50s/epoch - 528us/sample
Epoch 115/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0031 - 50s/epoch - 528us/sample
Epoch 116/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0030 - 50s/epoch - 527us/sample
Epoch 117/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 118/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0031 - 50s/epoch - 526us/sample
Epoch 119/145
95501/95501 - 50s - loss: 0.0026 - val_loss: 0.0031 - 50s/epoch - 525us/sample
Epoch 120/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0031 - 50s/epoch - 526us/sample
Epoch 121/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 122/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0028 - 50s/epoch - 525us/sample
Epoch 123/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 124/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0028 - 50s/epoch - 525us/sample
Epoch 125/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 126/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0033 - 50s/epoch - 527us/sample
Epoch 127/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0037 - 50s/epoch - 525us/sample
Epoch 128/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0028 - 50s/epoch - 526us/sample
Epoch 129/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 130/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0028 - 50s/epoch - 528us/sample
Epoch 131/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0031 - 50s/epoch - 527us/sample
Epoch 132/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 525us/sample
Epoch 133/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0029 - 50s/epoch - 527us/sample
Epoch 134/145
95501/95501 - 51s - loss: 0.0025 - val_loss: 0.0032 - 51s/epoch - 529us/sample
Epoch 135/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0033 - 50s/epoch - 528us/sample
Epoch 136/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0038 - 50s/epoch - 528us/sample
Epoch 137/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 527us/sample
Epoch 138/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0032 - 50s/epoch - 528us/sample
Epoch 139/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0028 - 50s/epoch - 528us/sample
Epoch 140/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0041 - 50s/epoch - 527us/sample
Epoch 141/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0043 - 50s/epoch - 526us/sample
Epoch 142/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 529us/sample
Epoch 143/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0080 - 50s/epoch - 526us/sample
Epoch 144/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0030 - 50s/epoch - 527us/sample
Epoch 145/145
95501/95501 - 50s - loss: 0.0025 - val_loss: 0.0033 - 50s/epoch - 525us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.003305820574793688
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:57:53.634014: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_22/outputlayer/BiasAdd' id:28563 op device:{requested: '', assigned: ''} def:{{{node decoder_model_22/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_22/outputlayer/MatMul, decoder_model_22/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.027798178409501106
cosine 0.02262670031014677
MAE: 0.024063412
RMSE: 0.053291734
r2: 0.8157628492377389
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 8, 145, 0.0008, 0.3, 379, 0.0025352554210941462, 0.003305820574793688, 0.027798178409501106, 0.02262670031014677, 0.024063412100076675, 0.05329173430800438, 0.8157628492377389, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 10 0.001 8 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_69 (BatchN  (None, 2654)        10616       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_69 (ReLU)                (None, 2654)         0           ['batch_normalization_69[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          1006245     ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          1006245     ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4520592     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,901,008
Trainable params: 9,889,634
Non-trainable params: 11,374
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-15 02:58:06.908781: W tensorflow/c/c_api.cc:291] Operation '{name:'training_46/Adam/bottleneck_zmean_23/kernel/m/Assign' id:30334 op device:{requested: '', assigned: ''} def:{{{node training_46/Adam/bottleneck_zmean_23/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_46/Adam/bottleneck_zmean_23/kernel/m, training_46/Adam/bottleneck_zmean_23/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:59:00.524805: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_23/mul' id:29853 op device:{requested: '', assigned: ''} def:{{{node loss_23/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_23/mul/x, loss_23/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 60s - loss: 0.0123 - val_loss: 0.0116 - 60s/epoch - 627us/sample
Epoch 2/10
95501/95501 - 53s - loss: 0.0056 - val_loss: 0.0148 - 53s/epoch - 558us/sample
Epoch 3/10
95501/95501 - 53s - loss: 0.0046 - val_loss: 0.0111 - 53s/epoch - 558us/sample
Epoch 4/10
95501/95501 - 53s - loss: 0.0042 - val_loss: 0.0092 - 53s/epoch - 557us/sample
Epoch 5/10
95501/95501 - 53s - loss: 0.0039 - val_loss: 0.0067 - 53s/epoch - 558us/sample
Epoch 6/10
95501/95501 - 53s - loss: 0.0037 - val_loss: 0.0065 - 53s/epoch - 556us/sample
Epoch 7/10
95501/95501 - 53s - loss: 0.0036 - val_loss: 0.0057 - 53s/epoch - 558us/sample
Epoch 8/10
95501/95501 - 53s - loss: 0.0035 - val_loss: 0.0058 - 53s/epoch - 559us/sample
Epoch 9/10
95501/95501 - 53s - loss: 0.0034 - val_loss: 0.0055 - 53s/epoch - 555us/sample
Epoch 10/10
95501/95501 - 53s - loss: 0.0034 - val_loss: 0.0060 - 53s/epoch - 557us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.005960020274759361
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:07:04.044803: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_23/outputlayer/BiasAdd' id:29824 op device:{requested: '', assigned: ''} def:{{{node decoder_model_23/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_23/outputlayer/MatMul, decoder_model_23/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.04541278507488879
cosine 0.037718169997761126
MAE: 0.033221245
RMSE: 0.07431533
r2: 0.6417255377880936
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 8, 10, 0.001, 0.3, 379, 0.0033660285205116755, 0.005960020274759361, 0.04541278507488879, 0.037718169997761126, 0.03322124481201172, 0.07431533187627792, 0.6417255377880936, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 140 0.0012 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_72 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_72 (ReLU)                (None, 1769)         0           ['batch_normalization_72[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          670830      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          670830      ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3062112     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,648,633
Trainable params: 6,640,799
Non-trainable params: 7,834
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-15 03:07:17.734754: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_24/kernel/Assign' id:30790 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_24/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_24/kernel, bottleneck_zlog_24/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:07:31.048220: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_24/mul' id:31114 op device:{requested: '', assigned: ''} def:{{{node loss_24/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_24/mul/x, loss_24/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 18s - loss: 0.0109 - val_loss: 0.0062 - 18s/epoch - 185us/sample
Epoch 2/140
95501/95501 - 11s - loss: 0.0046 - val_loss: 0.0052 - 11s/epoch - 116us/sample
Epoch 3/140
95501/95501 - 11s - loss: 0.0041 - val_loss: 0.0033 - 11s/epoch - 116us/sample
Epoch 4/140
95501/95501 - 11s - loss: 0.0035 - val_loss: 0.0032 - 11s/epoch - 116us/sample
Epoch 5/140
95501/95501 - 11s - loss: 0.0030 - val_loss: 0.0025 - 11s/epoch - 115us/sample
Epoch 6/140
95501/95501 - 11s - loss: 0.0026 - val_loss: 0.0022 - 11s/epoch - 115us/sample
Epoch 7/140
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0023 - 11s/epoch - 115us/sample
Epoch 8/140
95501/95501 - 11s - loss: 0.0021 - val_loss: 0.0017 - 11s/epoch - 116us/sample
Epoch 9/140
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0016 - 11s/epoch - 116us/sample
Epoch 10/140
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0017 - 11s/epoch - 116us/sample
Epoch 11/140
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0016 - 11s/epoch - 116us/sample
Epoch 12/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0015 - 11s/epoch - 116us/sample
Epoch 13/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0014 - 11s/epoch - 115us/sample
Epoch 14/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0016 - 11s/epoch - 115us/sample
Epoch 15/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0013 - 11s/epoch - 116us/sample
Epoch 16/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 115us/sample
Epoch 17/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0013 - 11s/epoch - 115us/sample
Epoch 18/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0014 - 11s/epoch - 116us/sample
Epoch 19/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 116us/sample
Epoch 20/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0017 - 11s/epoch - 116us/sample
Epoch 21/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 22/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0016 - 11s/epoch - 115us/sample
Epoch 23/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 24/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 25/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 26/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 27/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 28/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 29/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0013 - 11s/epoch - 115us/sample
Epoch 30/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 116us/sample
Epoch 31/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 32/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 33/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 34/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 35/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 36/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0013 - 11s/epoch - 115us/sample
Epoch 37/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 38/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 39/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 40/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 41/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 42/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 43/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 116us/sample
Epoch 44/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0013 - 11s/epoch - 115us/sample
Epoch 45/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 46/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 47/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 48/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 116us/sample
Epoch 49/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 50/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 117us/sample
Epoch 51/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 52/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 53/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 54/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 116us/sample
Epoch 55/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9927e-04 - 11s/epoch - 115us/sample
Epoch 56/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 116us/sample
Epoch 57/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 58/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 59/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 116us/sample
Epoch 60/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 112us/sample
Epoch 61/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 62/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.8827e-04 - 11s/epoch - 110us/sample
Epoch 63/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 64/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8876e-04 - 11s/epoch - 110us/sample
Epoch 65/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8561e-04 - 11s/epoch - 111us/sample
Epoch 66/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8993e-04 - 11s/epoch - 111us/sample
Epoch 67/140
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8776e-04 - 10s/epoch - 110us/sample
Epoch 68/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7286e-04 - 11s/epoch - 110us/sample
Epoch 69/140
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8620e-04 - 10s/epoch - 110us/sample
Epoch 70/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.9435e-04 - 11s/epoch - 110us/sample
Epoch 71/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 72/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 73/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7248e-04 - 11s/epoch - 111us/sample
Epoch 74/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8215e-04 - 11s/epoch - 111us/sample
Epoch 75/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 76/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7278e-04 - 11s/epoch - 111us/sample
Epoch 77/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 78/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7146e-04 - 11s/epoch - 111us/sample
Epoch 79/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7627e-04 - 11s/epoch - 111us/sample
Epoch 80/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6231e-04 - 11s/epoch - 110us/sample
Epoch 81/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 82/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0013 - 11s/epoch - 110us/sample
Epoch 83/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7742e-04 - 11s/epoch - 110us/sample
Epoch 84/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5657e-04 - 11s/epoch - 110us/sample
Epoch 85/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5825e-04 - 11s/epoch - 110us/sample
Epoch 86/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 87/140
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 88/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6963e-04 - 11s/epoch - 111us/sample
Epoch 89/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6279e-04 - 11s/epoch - 111us/sample
Epoch 90/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8128e-04 - 11s/epoch - 111us/sample
Epoch 91/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5092e-04 - 11s/epoch - 110us/sample
Epoch 92/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5446e-04 - 11s/epoch - 111us/sample
Epoch 93/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4766e-04 - 11s/epoch - 110us/sample
Epoch 94/140
95501/95501 - 11s - loss: 9.9992e-04 - val_loss: 9.9957e-04 - 11s/epoch - 110us/sample
Epoch 95/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4568e-04 - 11s/epoch - 110us/sample
Epoch 96/140
95501/95501 - 11s - loss: 9.9790e-04 - val_loss: 9.5313e-04 - 11s/epoch - 110us/sample
Epoch 97/140
95501/95501 - 11s - loss: 9.9981e-04 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 98/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7236e-04 - 11s/epoch - 111us/sample
Epoch 99/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5270e-04 - 11s/epoch - 110us/sample
Epoch 100/140
95501/95501 - 11s - loss: 9.9897e-04 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 101/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.4514e-04 - 11s/epoch - 110us/sample
Epoch 102/140
95501/95501 - 11s - loss: 9.9770e-04 - val_loss: 9.7675e-04 - 11s/epoch - 110us/sample
Epoch 103/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4435e-04 - 11s/epoch - 111us/sample
Epoch 104/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4823e-04 - 11s/epoch - 110us/sample
Epoch 105/140
95501/95501 - 11s - loss: 9.9432e-04 - val_loss: 9.6444e-04 - 11s/epoch - 111us/sample
Epoch 106/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 107/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6074e-04 - 11s/epoch - 111us/sample
Epoch 108/140
95501/95501 - 11s - loss: 9.9430e-04 - val_loss: 9.3270e-04 - 11s/epoch - 110us/sample
Epoch 109/140
95501/95501 - 11s - loss: 9.8785e-04 - val_loss: 9.3718e-04 - 11s/epoch - 111us/sample
Epoch 110/140
95501/95501 - 11s - loss: 9.8645e-04 - val_loss: 9.4045e-04 - 11s/epoch - 111us/sample
Epoch 111/140
95501/95501 - 11s - loss: 9.8984e-04 - val_loss: 9.4423e-04 - 11s/epoch - 110us/sample
Epoch 112/140
95501/95501 - 11s - loss: 9.8158e-04 - val_loss: 9.3423e-04 - 11s/epoch - 110us/sample
Epoch 113/140
95501/95501 - 11s - loss: 9.8496e-04 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 114/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 115/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.3520e-04 - 11s/epoch - 110us/sample
Epoch 116/140
95501/95501 - 10s - loss: 9.8511e-04 - val_loss: 9.7025e-04 - 10s/epoch - 110us/sample
Epoch 117/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.3960e-04 - 11s/epoch - 110us/sample
Epoch 118/140
95501/95501 - 11s - loss: 9.8510e-04 - val_loss: 9.3863e-04 - 11s/epoch - 111us/sample
Epoch 119/140
95501/95501 - 11s - loss: 9.8216e-04 - val_loss: 9.3632e-04 - 11s/epoch - 110us/sample
Epoch 120/140
95501/95501 - 10s - loss: 9.8549e-04 - val_loss: 9.7796e-04 - 10s/epoch - 110us/sample
Epoch 121/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.4612e-04 - 11s/epoch - 111us/sample
Epoch 122/140
95501/95501 - 11s - loss: 9.8175e-04 - val_loss: 9.2797e-04 - 11s/epoch - 110us/sample
Epoch 123/140
95501/95501 - 11s - loss: 9.7842e-04 - val_loss: 9.3367e-04 - 11s/epoch - 110us/sample
Epoch 124/140
95501/95501 - 11s - loss: 9.7399e-04 - val_loss: 9.2539e-04 - 11s/epoch - 110us/sample
Epoch 125/140
95501/95501 - 11s - loss: 9.8116e-04 - val_loss: 9.2365e-04 - 11s/epoch - 111us/sample
Epoch 126/140
95501/95501 - 11s - loss: 9.7151e-04 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 127/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.2016e-04 - 11s/epoch - 111us/sample
Epoch 128/140
95501/95501 - 11s - loss: 9.7417e-04 - val_loss: 0.0010 - 11s/epoch - 111us/sample
Epoch 129/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.2866e-04 - 11s/epoch - 111us/sample
Epoch 130/140
95501/95501 - 11s - loss: 9.7879e-04 - val_loss: 9.3127e-04 - 11s/epoch - 110us/sample
Epoch 131/140
95501/95501 - 11s - loss: 9.7884e-04 - val_loss: 9.8820e-04 - 11s/epoch - 111us/sample
Epoch 132/140
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 110us/sample
Epoch 133/140
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4243e-04 - 10s/epoch - 110us/sample
Epoch 134/140
95501/95501 - 11s - loss: 9.7543e-04 - val_loss: 9.2422e-04 - 11s/epoch - 110us/sample
Epoch 135/140
95501/95501 - 11s - loss: 9.7311e-04 - val_loss: 9.2118e-04 - 11s/epoch - 110us/sample
Epoch 136/140
95501/95501 - 11s - loss: 9.7219e-04 - val_loss: 9.3655e-04 - 11s/epoch - 111us/sample
Epoch 137/140
95501/95501 - 11s - loss: 9.8138e-04 - val_loss: 9.3785e-04 - 11s/epoch - 110us/sample
Epoch 138/140
95501/95501 - 11s - loss: 9.7880e-04 - val_loss: 9.1454e-04 - 11s/epoch - 110us/sample
Epoch 139/140
95501/95501 - 10s - loss: 9.6607e-04 - val_loss: 9.1873e-04 - 10s/epoch - 110us/sample
Epoch 140/140
95501/95501 - 11s - loss: 9.6965e-04 - val_loss: 9.2321e-04 - 11s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0009232129210703106
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 03:32:28.086007: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_24/outputlayer/BiasAdd' id:31085 op device:{requested: '', assigned: ''} def:{{{node decoder_model_24/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_24/outputlayer/MatMul, decoder_model_24/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005977165168786651
cosine 0.0047212352662368675
MAE: 0.011892907
RMSE: 0.022685835
r2: 0.9666138838278574
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 64, 140, 0.0012, 0.3, 379, 0.0009696534534039307, 0.0009232129210703106, 0.005977165168786651, 0.0047212352662368675, 0.011892907321453094, 0.022685835137963295, 0.9666138838278574, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 145 0.001 16 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_75 (BatchN  (None, 1643)        6572        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_75 (ReLU)                (None, 1643)         0           ['batch_normalization_75[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          623076      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          623076      ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2854464     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,185,583
Trainable params: 6,178,253
Non-trainable params: 7,330
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 03:32:41.255785: W tensorflow/c/c_api.cc:291] Operation '{name:'training_50/Adam/bottleneck_zlog_25/kernel/m/Assign' id:32871 op device:{requested: '', assigned: ''} def:{{{node training_50/Adam/bottleneck_zlog_25/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_50/Adam/bottleneck_zlog_25/kernel/m, training_50/Adam/bottleneck_zlog_25/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 03:33:10.026354: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_25/mul' id:32375 op device:{requested: '', assigned: ''} def:{{{node loss_25/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul/x, loss_25/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 34s - loss: 0.0095 - val_loss: 0.0045 - 34s/epoch - 356us/sample
Epoch 2/145
95501/95501 - 27s - loss: 0.0045 - val_loss: 0.0034 - 27s/epoch - 285us/sample
Epoch 3/145
95501/95501 - 27s - loss: 0.0033 - val_loss: 0.0026 - 27s/epoch - 284us/sample
Epoch 4/145
95501/95501 - 27s - loss: 0.0028 - val_loss: 0.0023 - 27s/epoch - 287us/sample
Epoch 5/145
95501/95501 - 27s - loss: 0.0025 - val_loss: 0.0021 - 27s/epoch - 286us/sample
Epoch 6/145
95501/95501 - 27s - loss: 0.0023 - val_loss: 0.0020 - 27s/epoch - 286us/sample
Epoch 7/145
95501/95501 - 27s - loss: 0.0022 - val_loss: 0.0019 - 27s/epoch - 287us/sample
Epoch 8/145
95501/95501 - 27s - loss: 0.0022 - val_loss: 0.0018 - 27s/epoch - 286us/sample
Epoch 9/145
95501/95501 - 27s - loss: 0.0021 - val_loss: 0.0018 - 27s/epoch - 286us/sample
Epoch 10/145
95501/95501 - 27s - loss: 0.0020 - val_loss: 0.0017 - 27s/epoch - 287us/sample
Epoch 11/145
95501/95501 - 27s - loss: 0.0020 - val_loss: 0.0017 - 27s/epoch - 286us/sample
Epoch 12/145
95501/95501 - 27s - loss: 0.0019 - val_loss: 0.0016 - 27s/epoch - 286us/sample
Epoch 13/145
95501/95501 - 27s - loss: 0.0019 - val_loss: 0.0016 - 27s/epoch - 288us/sample
Epoch 14/145
95501/95501 - 27s - loss: 0.0019 - val_loss: 0.0016 - 27s/epoch - 286us/sample
Epoch 15/145
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0016 - 27s/epoch - 287us/sample
Epoch 16/145
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0015 - 27s/epoch - 286us/sample
Epoch 17/145
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0015 - 27s/epoch - 286us/sample
Epoch 18/145
95501/95501 - 27s - loss: 0.0018 - val_loss: 0.0015 - 27s/epoch - 285us/sample
Epoch 19/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 287us/sample
Epoch 20/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 285us/sample
Epoch 21/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 284us/sample
Epoch 22/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0015 - 27s/epoch - 286us/sample
Epoch 23/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 24/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 25/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 26/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 27/145
95501/95501 - 27s - loss: 0.0017 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 28/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 29/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 30/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 31/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 32/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 33/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 34/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 35/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 36/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 37/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 38/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 39/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 40/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 41/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 42/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 43/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 44/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 45/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 46/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 47/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 48/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 49/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 50/145
95501/95501 - 27s - loss: 0.0016 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 51/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 52/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 53/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 54/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 55/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 56/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 57/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 58/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 59/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0015 - 27s/epoch - 286us/sample
Epoch 60/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 61/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 62/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 63/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 64/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 65/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 66/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 67/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 68/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 69/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 70/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 286us/sample
Epoch 71/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 72/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 73/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 74/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 287us/sample
Epoch 75/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 76/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 77/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 78/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 288us/sample
Epoch 79/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 80/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 81/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 82/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 83/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 84/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 85/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 86/145
95501/95501 - 28s - loss: 0.0015 - val_loss: 0.0012 - 28s/epoch - 288us/sample
Epoch 87/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 283us/sample
Epoch 88/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 89/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 288us/sample
Epoch 90/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 91/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 284us/sample
Epoch 92/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 93/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 94/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 95/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 96/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 97/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 98/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0015 - 27s/epoch - 286us/sample
Epoch 99/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 288us/sample
Epoch 100/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 101/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 102/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 103/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 104/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 105/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 106/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 107/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 108/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 109/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 110/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 111/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 112/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 287us/sample
Epoch 113/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 114/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 115/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 116/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 117/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 118/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 285us/sample
Epoch 119/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 120/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 121/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 284us/sample
Epoch 122/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 123/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 124/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 125/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 126/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 127/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 128/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 129/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 130/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 131/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 132/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 133/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 134/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 135/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 136/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 137/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 138/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 139/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 286us/sample
Epoch 140/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 141/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0014 - 27s/epoch - 285us/sample
Epoch 142/145
95501/95501 - 27s - loss: 0.0015 - val_loss: 0.0012 - 27s/epoch - 285us/sample
Epoch 143/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 287us/sample
Epoch 144/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0013 - 27s/epoch - 286us/sample
Epoch 145/145
95501/95501 - 27s - loss: 0.0014 - val_loss: 0.0012 - 27s/epoch - 285us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0012115137117535376
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:38:47.052255: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_25/outputlayer/BiasAdd' id:32346 op device:{requested: '', assigned: ''} def:{{{node decoder_model_25/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_25/outputlayer/MatMul, decoder_model_25/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.009689530246086134
cosine 0.007693247624581326
MAE: 0.015138463
RMSE: 0.029072668
r2: 0.945168780043804
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 16, 145, 0.001, 0.3, 379, 0.0014357938813648683, 0.0012115137117535376, 0.009689530246086134, 0.007693247624581326, 0.015138463117182255, 0.02907266840338707, 0.945168780043804, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 2
Fitness    = 84.84730891968614
Last generation's best solutions = [1.5 145 0.0012 64 1] with fitness 84.84730891968614.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object)]
Best solutions fitness :  [84.84730891968614, 84.84730891968614]
[1.2999999999999998 135 0.0012 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_78 (BatchN  (None, 1643)        6572        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_78 (ReLU)                (None, 1643)         0           ['batch_normalization_78[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          623076      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          623076      ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2854464     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 6,185,583
Trainable params: 6,178,253
Non-trainable params: 7,330
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/135
2023-02-15 04:39:01.070484: W tensorflow/c/c_api.cc:291] Operation '{name:'training_52/Adam/batch_normalization_78/gamma/m/Assign' id:34106 op device:{requested: '', assigned: ''} def:{{{node training_52/Adam/batch_normalization_78/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_52/Adam/batch_normalization_78/gamma/m, training_52/Adam/batch_normalization_78/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:39:13.821020: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_26/mul' id:33639 op device:{requested: '', assigned: ''} def:{{{node loss_26/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_26/mul/x, loss_26/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 17s - loss: 0.0111 - val_loss: 0.0067 - 17s/epoch - 180us/sample
Epoch 2/135
95501/95501 - 10s - loss: 0.0047 - val_loss: 0.0039 - 10s/epoch - 109us/sample
Epoch 3/135
95501/95501 - 10s - loss: 0.0039 - val_loss: 0.0036 - 10s/epoch - 109us/sample
Epoch 4/135
95501/95501 - 10s - loss: 0.0035 - val_loss: 0.0036 - 10s/epoch - 109us/sample
Epoch 5/135
95501/95501 - 10s - loss: 0.0030 - val_loss: 0.0027 - 10s/epoch - 109us/sample
Epoch 6/135
95501/95501 - 10s - loss: 0.0026 - val_loss: 0.0027 - 10s/epoch - 109us/sample
Epoch 7/135
95501/95501 - 10s - loss: 0.0025 - val_loss: 0.0028 - 10s/epoch - 109us/sample
Epoch 8/135
95501/95501 - 11s - loss: 0.0023 - val_loss: 0.0019 - 11s/epoch - 110us/sample
Epoch 9/135
95501/95501 - 10s - loss: 0.0019 - val_loss: 0.0017 - 10s/epoch - 109us/sample
Epoch 10/135
95501/95501 - 10s - loss: 0.0018 - val_loss: 0.0016 - 10s/epoch - 110us/sample
Epoch 11/135
95501/95501 - 10s - loss: 0.0017 - val_loss: 0.0017 - 10s/epoch - 109us/sample
Epoch 12/135
95501/95501 - 10s - loss: 0.0017 - val_loss: 0.0014 - 10s/epoch - 109us/sample
Epoch 13/135
95501/95501 - 10s - loss: 0.0015 - val_loss: 0.0014 - 10s/epoch - 109us/sample
Epoch 14/135
95501/95501 - 10s - loss: 0.0016 - val_loss: 0.0014 - 10s/epoch - 109us/sample
Epoch 15/135
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0013 - 10s/epoch - 110us/sample
Epoch 16/135
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 110us/sample
Epoch 17/135
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0013 - 10s/epoch - 109us/sample
Epoch 18/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0013 - 10s/epoch - 109us/sample
Epoch 19/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 20/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 21/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 22/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 23/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 24/135
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 111us/sample
Epoch 25/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0024 - 10s/epoch - 109us/sample
Epoch 26/135
95501/95501 - 10s - loss: 0.0017 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 27/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 28/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0022 - 10s/epoch - 109us/sample
Epoch 29/135
95501/95501 - 10s - loss: 0.0016 - val_loss: 0.0014 - 10s/epoch - 109us/sample
Epoch 30/135
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 31/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 32/135
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0017 - 11s/epoch - 110us/sample
Epoch 33/135
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 34/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0013 - 10s/epoch - 109us/sample
Epoch 35/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 36/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0013 - 10s/epoch - 109us/sample
Epoch 37/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 38/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 39/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 40/135
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0017 - 11s/epoch - 110us/sample
Epoch 41/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 42/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 43/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 44/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 45/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 46/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0012 - 10s/epoch - 110us/sample
Epoch 47/135
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 48/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 49/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0012 - 10s/epoch - 110us/sample
Epoch 50/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 51/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 52/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 53/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 54/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 55/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 56/135
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 57/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 58/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 59/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 60/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 61/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 62/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 63/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 64/135
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 65/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 66/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 67/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 68/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 69/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 70/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.9457e-04 - 10s/epoch - 109us/sample
Epoch 71/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 72/135
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 73/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0012 - 10s/epoch - 110us/sample
Epoch 74/135
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 75/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.9517e-04 - 10s/epoch - 109us/sample
Epoch 76/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 77/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.9748e-04 - 10s/epoch - 109us/sample
Epoch 78/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 79/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.9835e-04 - 10s/epoch - 110us/sample
Epoch 80/135
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 81/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 82/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8950e-04 - 10s/epoch - 109us/sample
Epoch 83/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 84/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.8322e-04 - 10s/epoch - 109us/sample
Epoch 85/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7777e-04 - 10s/epoch - 109us/sample
Epoch 86/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.8050e-04 - 10s/epoch - 110us/sample
Epoch 87/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8983e-04 - 10s/epoch - 109us/sample
Epoch 88/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8286e-04 - 11s/epoch - 110us/sample
Epoch 89/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7948e-04 - 10s/epoch - 109us/sample
Epoch 90/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 91/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.8608e-04 - 10s/epoch - 110us/sample
Epoch 92/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8246e-04 - 10s/epoch - 109us/sample
Epoch 93/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 94/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 95/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.7038e-04 - 10s/epoch - 110us/sample
Epoch 96/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7860e-04 - 11s/epoch - 111us/sample
Epoch 97/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8272e-04 - 10s/epoch - 109us/sample
Epoch 98/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 99/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.6437e-04 - 10s/epoch - 109us/sample
Epoch 100/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7125e-04 - 10s/epoch - 109us/sample
Epoch 101/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7371e-04 - 10s/epoch - 110us/sample
Epoch 102/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8042e-04 - 10s/epoch - 109us/sample
Epoch 103/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6135e-04 - 11s/epoch - 111us/sample
Epoch 104/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 105/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.8328e-04 - 10s/epoch - 110us/sample
Epoch 106/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.6550e-04 - 10s/epoch - 109us/sample
Epoch 107/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.6681e-04 - 10s/epoch - 109us/sample
Epoch 108/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 109/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.6273e-04 - 10s/epoch - 109us/sample
Epoch 110/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.8565e-04 - 10s/epoch - 109us/sample
Epoch 111/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4930e-04 - 10s/epoch - 110us/sample
Epoch 112/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 113/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.7271e-04 - 10s/epoch - 109us/sample
Epoch 114/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 115/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.6478e-04 - 10s/epoch - 109us/sample
Epoch 116/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 117/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 118/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.6315e-04 - 10s/epoch - 109us/sample
Epoch 119/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7993e-04 - 11s/epoch - 110us/sample
Epoch 120/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4948e-04 - 10s/epoch - 109us/sample
Epoch 121/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 122/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7330e-04 - 10s/epoch - 110us/sample
Epoch 123/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.4765e-04 - 10s/epoch - 109us/sample
Epoch 124/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0011 - 10s/epoch - 109us/sample
Epoch 125/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 110us/sample
Epoch 126/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.7698e-04 - 10s/epoch - 109us/sample
Epoch 127/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5803e-04 - 11s/epoch - 110us/sample
Epoch 128/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 129/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.6150e-04 - 10s/epoch - 110us/sample
Epoch 130/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 131/135
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.8662e-04 - 10s/epoch - 109us/sample
Epoch 132/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.3759e-04 - 10s/epoch - 109us/sample
Epoch 133/135
95501/95501 - 10s - loss: 9.9358e-04 - val_loss: 9.7763e-04 - 10s/epoch - 110us/sample
Epoch 134/135
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.9685e-04 - 10s/epoch - 109us/sample
Epoch 135/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4521e-04 - 11s/epoch - 110us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0009452135306646113
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:02:36.924908: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_26/outputlayer/BiasAdd' id:33610 op device:{requested: '', assigned: ''} def:{{{node decoder_model_26/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_26/outputlayer/MatMul, decoder_model_26/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006136425027656771
cosine 0.004844607736899904
MAE: 0.012015141
RMSE: 0.022977574
r2: 0.9657501357199244
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 64, 135, 0.0012, 0.3, 379, 0.0010232057383435537, 0.0009452135306646113, 0.006136425027656771, 0.004844607736899904, 0.012015140615403652, 0.022977573797106743, 0.9657501357199244, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0012 64 0] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_81 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_81 (ReLU)                (None, 2022)         0           ['batch_normalization_81[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          766717      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          766717      ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3479056     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,578,408
Trainable params: 7,569,562
Non-trainable params: 8,846
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 05:02:51.309894: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_82/beta/Assign' id:34630 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_82/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_82/beta, batch_normalization_82/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:03:04.973150: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_27/mul' id:34919 op device:{requested: '', assigned: ''} def:{{{node loss_27/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_27/mul/x, loss_27/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 18s - loss: 2.2983 - val_loss: 1.4590 - 18s/epoch - 192us/sample
Epoch 2/145
95501/95501 - 11s - loss: 1.4703 - val_loss: 1.4606 - 11s/epoch - 116us/sample
Epoch 3/145
95501/95501 - 11s - loss: 1.4415 - val_loss: 1.4388 - 11s/epoch - 116us/sample
Epoch 4/145
95501/95501 - 11s - loss: 1.4345 - val_loss: 1.4356 - 11s/epoch - 116us/sample
Epoch 5/145
95501/95501 - 11s - loss: 1.4343 - val_loss: 1.4463 - 11s/epoch - 116us/sample
Epoch 6/145
95501/95501 - 11s - loss: 1.4391 - val_loss: 1.4391 - 11s/epoch - 117us/sample
Epoch 7/145
95501/95501 - 11s - loss: 1.4339 - val_loss: 1.4255 - 11s/epoch - 116us/sample
Epoch 8/145
95501/95501 - 11s - loss: 1.4223 - val_loss: 1.4678 - 11s/epoch - 116us/sample
Epoch 9/145
95501/95501 - 11s - loss: 1.4170 - val_loss: 1.4113 - 11s/epoch - 116us/sample
Epoch 10/145
95501/95501 - 11s - loss: 1.4038 - val_loss: 1.3976 - 11s/epoch - 116us/sample
Epoch 11/145
95501/95501 - 11s - loss: 1.4326 - val_loss: 1.4081 - 11s/epoch - 115us/sample
Epoch 12/145
95501/95501 - 11s - loss: 1.4191 - val_loss: 1.4111 - 11s/epoch - 116us/sample
Epoch 13/145
95501/95501 - 11s - loss: 1.4051 - val_loss: 1.4037 - 11s/epoch - 118us/sample
Epoch 14/145
95501/95501 - 11s - loss: 1.4064 - val_loss: 1.4035 - 11s/epoch - 120us/sample
Epoch 15/145
95501/95501 - 11s - loss: 1.4019 - val_loss: 1.3993 - 11s/epoch - 118us/sample
Epoch 16/145
95501/95501 - 11s - loss: 1.3980 - val_loss: 1.3976 - 11s/epoch - 119us/sample
Epoch 17/145
95501/95501 - 11s - loss: 1.3959 - val_loss: 1.3986 - 11s/epoch - 119us/sample
Epoch 18/145
95501/95501 - 11s - loss: 1.3947 - val_loss: 1.3946 - 11s/epoch - 120us/sample
Epoch 19/145
95501/95501 - 11s - loss: 1.4077 - val_loss: 1.4161 - 11s/epoch - 118us/sample
Epoch 20/145
95501/95501 - 11s - loss: 1.4068 - val_loss: 1.4027 - 11s/epoch - 120us/sample
Epoch 21/145
95501/95501 - 11s - loss: 1.3987 - val_loss: 1.3961 - 11s/epoch - 119us/sample
Epoch 22/145
95501/95501 - 11s - loss: 1.4054 - val_loss: 1.4029 - 11s/epoch - 118us/sample
Epoch 23/145
95501/95501 - 11s - loss: 1.3987 - val_loss: 1.4292 - 11s/epoch - 119us/sample
Epoch 24/145
95501/95501 - 11s - loss: 1.4021 - val_loss: 1.4006 - 11s/epoch - 118us/sample
Epoch 25/145
95501/95501 - 11s - loss: 1.3979 - val_loss: 1.4145 - 11s/epoch - 119us/sample
Epoch 26/145
95501/95501 - 11s - loss: 1.4042 - val_loss: 1.4000 - 11s/epoch - 119us/sample
Epoch 27/145
95501/95501 - 11s - loss: 1.3947 - val_loss: 1.3945 - 11s/epoch - 119us/sample
Epoch 28/145
95501/95501 - 11s - loss: 1.3946 - val_loss: 1.4011 - 11s/epoch - 120us/sample
Epoch 29/145
95501/95501 - 11s - loss: 1.3948 - val_loss: 1.3935 - 11s/epoch - 118us/sample
Epoch 30/145
95501/95501 - 11s - loss: 1.3971 - val_loss: 1.4037 - 11s/epoch - 119us/sample
Epoch 31/145
95501/95501 - 11s - loss: 1.3958 - val_loss: 1.3960 - 11s/epoch - 119us/sample
Epoch 32/145
95501/95501 - 11s - loss: 1.3946 - val_loss: 1.3968 - 11s/epoch - 119us/sample
Epoch 33/145
95501/95501 - 11s - loss: 1.3934 - val_loss: 1.3945 - 11s/epoch - 118us/sample
Epoch 34/145
95501/95501 - 11s - loss: 1.3915 - val_loss: 1.3929 - 11s/epoch - 118us/sample
Epoch 35/145
95501/95501 - 11s - loss: 1.3896 - val_loss: 1.3901 - 11s/epoch - 120us/sample
Epoch 36/145
95501/95501 - 11s - loss: 1.3937 - val_loss: 1.3984 - 11s/epoch - 119us/sample
Epoch 37/145
95501/95501 - 11s - loss: 1.3911 - val_loss: 1.3906 - 11s/epoch - 119us/sample
Epoch 38/145
95501/95501 - 11s - loss: 1.3901 - val_loss: 1.3921 - 11s/epoch - 118us/sample
Epoch 39/145
95501/95501 - 11s - loss: 1.3898 - val_loss: 1.3903 - 11s/epoch - 119us/sample
Epoch 40/145
95501/95501 - 11s - loss: 1.3911 - val_loss: 1.3923 - 11s/epoch - 118us/sample
Epoch 41/145
95501/95501 - 11s - loss: 1.3908 - val_loss: 1.3907 - 11s/epoch - 119us/sample
Epoch 42/145
95501/95501 - 11s - loss: 1.3895 - val_loss: 1.3904 - 11s/epoch - 119us/sample
Epoch 43/145
95501/95501 - 11s - loss: 1.3897 - val_loss: 1.3916 - 11s/epoch - 118us/sample
Epoch 44/145
95501/95501 - 11s - loss: 1.3905 - val_loss: 1.3925 - 11s/epoch - 119us/sample
Epoch 45/145
95501/95501 - 11s - loss: 1.3924 - val_loss: 1.3916 - 11s/epoch - 119us/sample
Epoch 46/145
95501/95501 - 11s - loss: 1.3940 - val_loss: 1.3942 - 11s/epoch - 119us/sample
Epoch 47/145
95501/95501 - 11s - loss: 1.3915 - val_loss: 1.3938 - 11s/epoch - 119us/sample
Epoch 48/145
95501/95501 - 11s - loss: 1.3896 - val_loss: 1.3906 - 11s/epoch - 119us/sample
Epoch 49/145
95501/95501 - 11s - loss: 1.3888 - val_loss: 1.3907 - 11s/epoch - 120us/sample
Epoch 50/145
95501/95501 - 11s - loss: 1.3893 - val_loss: 1.3916 - 11s/epoch - 119us/sample
Epoch 51/145
95501/95501 - 11s - loss: 1.3910 - val_loss: 1.3937 - 11s/epoch - 119us/sample
Epoch 52/145
95501/95501 - 11s - loss: 1.3918 - val_loss: 1.3990 - 11s/epoch - 119us/sample
Epoch 53/145
95501/95501 - 11s - loss: 1.3915 - val_loss: 1.3925 - 11s/epoch - 118us/sample
Epoch 54/145
95501/95501 - 11s - loss: 1.3902 - val_loss: 1.3919 - 11s/epoch - 119us/sample
Epoch 55/145
95501/95501 - 11s - loss: 1.3895 - val_loss: 1.3910 - 11s/epoch - 118us/sample
Epoch 56/145
95501/95501 - 11s - loss: 1.3884 - val_loss: 1.3905 - 11s/epoch - 120us/sample
Epoch 57/145
95501/95501 - 11s - loss: 1.3891 - val_loss: 1.3894 - 11s/epoch - 119us/sample
Epoch 58/145
95501/95501 - 11s - loss: 1.3902 - val_loss: 1.3909 - 11s/epoch - 118us/sample
Epoch 59/145
95501/95501 - 11s - loss: 1.3905 - val_loss: 1.3923 - 11s/epoch - 118us/sample
Epoch 60/145
95501/95501 - 11s - loss: 1.3902 - val_loss: 1.3941 - 11s/epoch - 118us/sample
Epoch 61/145
95501/95501 - 11s - loss: 1.3905 - val_loss: 1.3908 - 11s/epoch - 119us/sample
Epoch 62/145
95501/95501 - 11s - loss: 1.3897 - val_loss: 1.3911 - 11s/epoch - 118us/sample
Epoch 63/145
95501/95501 - 11s - loss: 1.3893 - val_loss: 1.3907 - 11s/epoch - 119us/sample
Epoch 64/145
95501/95501 - 11s - loss: 1.3891 - val_loss: 1.3912 - 11s/epoch - 120us/sample
Epoch 65/145
95501/95501 - 11s - loss: 1.3882 - val_loss: 1.3884 - 11s/epoch - 118us/sample
Epoch 66/145
95501/95501 - 11s - loss: 1.3883 - val_loss: 1.3962 - 11s/epoch - 119us/sample
Epoch 67/145
95501/95501 - 11s - loss: 1.3907 - val_loss: 1.3923 - 11s/epoch - 118us/sample
Epoch 68/145
95501/95501 - 11s - loss: 1.3897 - val_loss: 1.3900 - 11s/epoch - 119us/sample
Epoch 69/145
95501/95501 - 11s - loss: 1.3880 - val_loss: 1.3897 - 11s/epoch - 118us/sample
Epoch 70/145
95501/95501 - 11s - loss: 1.3870 - val_loss: 1.3888 - 11s/epoch - 118us/sample
Epoch 71/145
95501/95501 - 11s - loss: 1.3874 - val_loss: 1.3919 - 11s/epoch - 119us/sample
Epoch 72/145
95501/95501 - 11s - loss: 1.3883 - val_loss: 1.3888 - 11s/epoch - 119us/sample
Epoch 73/145
95501/95501 - 11s - loss: 1.3865 - val_loss: 1.3881 - 11s/epoch - 118us/sample
Epoch 74/145
95501/95501 - 11s - loss: 1.3870 - val_loss: 1.3929 - 11s/epoch - 118us/sample
Epoch 75/145
95501/95501 - 11s - loss: 1.3880 - val_loss: 1.3887 - 11s/epoch - 119us/sample
Epoch 76/145
95501/95501 - 11s - loss: 1.3871 - val_loss: 1.3874 - 11s/epoch - 119us/sample
Epoch 77/145
95501/95501 - 11s - loss: 1.3873 - val_loss: 1.3884 - 11s/epoch - 118us/sample
Epoch 78/145
95501/95501 - 11s - loss: 1.3867 - val_loss: 1.3885 - 11s/epoch - 120us/sample
Epoch 79/145
95501/95501 - 11s - loss: 1.3863 - val_loss: 1.3875 - 11s/epoch - 119us/sample
Epoch 80/145
95501/95501 - 11s - loss: 1.3857 - val_loss: 1.3871 - 11s/epoch - 119us/sample
Epoch 81/145
95501/95501 - 11s - loss: 1.3866 - val_loss: 1.3879 - 11s/epoch - 118us/sample
Epoch 82/145
95501/95501 - 11s - loss: 1.3860 - val_loss: 1.3882 - 11s/epoch - 119us/sample
Epoch 83/145
95501/95501 - 11s - loss: 1.3856 - val_loss: 1.3868 - 11s/epoch - 118us/sample
Epoch 84/145
95501/95501 - 11s - loss: 1.3867 - val_loss: 1.3878 - 11s/epoch - 119us/sample
Epoch 85/145
95501/95501 - 11s - loss: 1.3867 - val_loss: 1.3879 - 11s/epoch - 120us/sample
Epoch 86/145
95501/95501 - 11s - loss: 1.3860 - val_loss: 1.3879 - 11s/epoch - 119us/sample
Epoch 87/145
95501/95501 - 11s - loss: 1.3865 - val_loss: 1.3873 - 11s/epoch - 118us/sample
Epoch 88/145
95501/95501 - 11s - loss: 1.3860 - val_loss: 1.3882 - 11s/epoch - 119us/sample
Epoch 89/145
95501/95501 - 11s - loss: 1.3865 - val_loss: 1.3876 - 11s/epoch - 118us/sample
Epoch 90/145
95501/95501 - 11s - loss: 1.3856 - val_loss: 1.3874 - 11s/epoch - 119us/sample
Epoch 91/145
95501/95501 - 11s - loss: 1.3864 - val_loss: 1.3883 - 11s/epoch - 119us/sample
Epoch 92/145
95501/95501 - 11s - loss: 1.3864 - val_loss: 1.3876 - 11s/epoch - 119us/sample
Epoch 93/145
95501/95501 - 12s - loss: 1.3859 - val_loss: 1.3894 - 12s/epoch - 120us/sample
Epoch 94/145
95501/95501 - 11s - loss: 1.3869 - val_loss: 1.3880 - 11s/epoch - 119us/sample
Epoch 95/145
95501/95501 - 11s - loss: 1.3868 - val_loss: 1.3887 - 11s/epoch - 118us/sample
Epoch 96/145
95501/95501 - 11s - loss: 1.3867 - val_loss: 1.3883 - 11s/epoch - 119us/sample
Epoch 97/145
95501/95501 - 11s - loss: 1.3859 - val_loss: 1.3880 - 11s/epoch - 119us/sample
Epoch 98/145
95501/95501 - 11s - loss: 1.3864 - val_loss: 1.3884 - 11s/epoch - 119us/sample
Epoch 99/145
95501/95501 - 11s - loss: 1.3863 - val_loss: 1.3875 - 11s/epoch - 119us/sample
Epoch 100/145
95501/95501 - 11s - loss: 1.3857 - val_loss: 1.3871 - 11s/epoch - 120us/sample
Epoch 101/145
95501/95501 - 11s - loss: 1.3856 - val_loss: 1.3875 - 11s/epoch - 119us/sample
Epoch 102/145
95501/95501 - 11s - loss: 1.3861 - val_loss: 1.3881 - 11s/epoch - 119us/sample
Epoch 103/145
95501/95501 - 11s - loss: 1.3858 - val_loss: 1.3872 - 11s/epoch - 119us/sample
Epoch 104/145
95501/95501 - 11s - loss: 1.3852 - val_loss: 1.3868 - 11s/epoch - 119us/sample
Epoch 105/145
95501/95501 - 11s - loss: 1.3848 - val_loss: 1.3863 - 11s/epoch - 119us/sample
Epoch 106/145
95501/95501 - 11s - loss: 1.3853 - val_loss: 1.3863 - 11s/epoch - 119us/sample
Epoch 107/145
95501/95501 - 11s - loss: 1.3860 - val_loss: 1.3874 - 11s/epoch - 119us/sample
Epoch 108/145
95501/95501 - 11s - loss: 1.3859 - val_loss: 1.3875 - 11s/epoch - 119us/sample
Epoch 109/145
95501/95501 - 11s - loss: 1.3862 - val_loss: 1.3896 - 11s/epoch - 119us/sample
Epoch 110/145
95501/95501 - 11s - loss: 1.3868 - val_loss: 1.3879 - 11s/epoch - 119us/sample
Epoch 111/145
95501/95501 - 11s - loss: 1.3861 - val_loss: 1.3873 - 11s/epoch - 119us/sample
Epoch 112/145
95501/95501 - 11s - loss: 1.3862 - val_loss: 1.3879 - 11s/epoch - 119us/sample
Epoch 113/145
95501/95501 - 11s - loss: 1.3867 - val_loss: 1.3894 - 11s/epoch - 119us/sample
Epoch 114/145
95501/95501 - 11s - loss: 1.3863 - val_loss: 1.3875 - 11s/epoch - 118us/sample
Epoch 115/145
95501/95501 - 11s - loss: 1.3853 - val_loss: 1.3864 - 11s/epoch - 120us/sample
Epoch 116/145
95501/95501 - 11s - loss: 1.3849 - val_loss: 1.3863 - 11s/epoch - 119us/sample
Epoch 117/145
95501/95501 - 11s - loss: 1.3850 - val_loss: 1.3875 - 11s/epoch - 119us/sample
Epoch 118/145
95501/95501 - 11s - loss: 1.3854 - val_loss: 1.3867 - 11s/epoch - 119us/sample
Epoch 119/145
95501/95501 - 11s - loss: 1.3848 - val_loss: 1.3862 - 11s/epoch - 118us/sample
Epoch 120/145
95501/95501 - 11s - loss: 1.3850 - val_loss: 1.3871 - 11s/epoch - 119us/sample
Epoch 121/145
95501/95501 - 11s - loss: 1.3853 - val_loss: 1.3872 - 11s/epoch - 120us/sample
Epoch 122/145
95501/95501 - 11s - loss: 1.3855 - val_loss: 1.3876 - 11s/epoch - 119us/sample
Epoch 123/145
95501/95501 - 11s - loss: 1.3852 - val_loss: 1.3871 - 11s/epoch - 119us/sample
Epoch 124/145
95501/95501 - 11s - loss: 1.3859 - val_loss: 1.3872 - 11s/epoch - 119us/sample
Epoch 125/145
95501/95501 - 11s - loss: 1.3857 - val_loss: 1.3880 - 11s/epoch - 119us/sample
Epoch 126/145
95501/95501 - 11s - loss: 1.3894 - val_loss: 1.3917 - 11s/epoch - 119us/sample
Epoch 127/145
95501/95501 - 11s - loss: 1.3890 - val_loss: 1.3890 - 11s/epoch - 119us/sample
Epoch 128/145
95501/95501 - 11s - loss: 1.3862 - val_loss: 1.3873 - 11s/epoch - 119us/sample
Epoch 129/145
95501/95501 - 11s - loss: 1.3853 - val_loss: 1.3870 - 11s/epoch - 119us/sample
Epoch 130/145
95501/95501 - 11s - loss: 1.3856 - val_loss: 1.3871 - 11s/epoch - 120us/sample
Epoch 131/145
95501/95501 - 11s - loss: 1.3852 - val_loss: 1.3868 - 11s/epoch - 119us/sample
Epoch 132/145
95501/95501 - 11s - loss: 1.3849 - val_loss: 1.3865 - 11s/epoch - 119us/sample
Epoch 133/145
95501/95501 - 11s - loss: 1.3850 - val_loss: 1.3865 - 11s/epoch - 119us/sample
Epoch 134/145
95501/95501 - 11s - loss: 1.3852 - val_loss: 1.3874 - 11s/epoch - 119us/sample
Epoch 135/145
95501/95501 - 11s - loss: 1.3871 - val_loss: 1.3889 - 11s/epoch - 119us/sample
Epoch 136/145
95501/95501 - 11s - loss: 1.3867 - val_loss: 1.3878 - 11s/epoch - 118us/sample
Epoch 137/145
95501/95501 - 11s - loss: 1.3859 - val_loss: 1.3881 - 11s/epoch - 120us/sample
Epoch 138/145
95501/95501 - 11s - loss: 1.3855 - val_loss: 1.3870 - 11s/epoch - 119us/sample
Epoch 139/145
95501/95501 - 11s - loss: 1.3847 - val_loss: 1.3862 - 11s/epoch - 119us/sample
Epoch 140/145
95501/95501 - 11s - loss: 1.3846 - val_loss: 1.3871 - 11s/epoch - 119us/sample
Epoch 141/145
95501/95501 - 11s - loss: 1.3846 - val_loss: 1.3864 - 11s/epoch - 119us/sample
Epoch 142/145
95501/95501 - 11s - loss: 1.3845 - val_loss: 1.3868 - 11s/epoch - 119us/sample
Epoch 143/145
95501/95501 - 11s - loss: 1.3857 - val_loss: 1.3879 - 11s/epoch - 119us/sample
Epoch 144/145
95501/95501 - 11s - loss: 1.3852 - val_loss: 1.3865 - 11s/epoch - 119us/sample
Epoch 145/145
95501/95501 - 11s - loss: 1.3847 - val_loss: 1.3871 - 11s/epoch - 120us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 1.3870791246250267
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:30:19.456063: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_27/outputlayer/BiasAdd' id:34871 op device:{requested: '', assigned: ''} def:{{{node decoder_model_27/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_27/outputlayer/MatMul, decoder_model_27/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7485741613579228
cosine 0.8658560703100265
MAE: 8.403633
RMSE: 27.843363
r2: -50288.3620431692
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'binary_crossentropy', 64, 145, 0.0012, 0.3, 379, 1.3846870189491933, 1.3870791246250267, 0.7485741613579228, 0.8658560703100265, 8.403633117675781, 27.84336280822754, -50288.3620431692, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0012 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_84 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_84 (ReLU)                (None, 2022)         0           ['batch_normalization_84[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          766717      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          766717      ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3479056     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,578,408
Trainable params: 7,569,562
Non-trainable params: 8,846
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 05:30:34.052441: W tensorflow/c/c_api.cc:291] Operation '{name:'training_56/Adam/beta_2/Assign' id:36682 op device:{requested: '', assigned: ''} def:{{{node training_56/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_56/Adam/beta_2, training_56/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:30:47.642436: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_28/mul' id:36246 op device:{requested: '', assigned: ''} def:{{{node loss_28/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_28/mul/x, loss_28/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 0.0118 - val_loss: 0.0127 - 19s/epoch - 194us/sample
Epoch 2/145
95501/95501 - 11s - loss: 0.0055 - val_loss: 0.0101 - 11s/epoch - 114us/sample
Epoch 3/145
95501/95501 - 11s - loss: 0.0050 - val_loss: 0.0040 - 11s/epoch - 114us/sample
Epoch 4/145
95501/95501 - 11s - loss: 0.0036 - val_loss: 0.0031 - 11s/epoch - 116us/sample
Epoch 5/145
95501/95501 - 11s - loss: 0.0031 - val_loss: 0.0029 - 11s/epoch - 114us/sample
Epoch 6/145
95501/95501 - 11s - loss: 0.0026 - val_loss: 0.0022 - 11s/epoch - 114us/sample
Epoch 7/145
95501/95501 - 11s - loss: 0.0023 - val_loss: 0.0025 - 11s/epoch - 114us/sample
Epoch 8/145
95501/95501 - 11s - loss: 0.0021 - val_loss: 0.0019 - 11s/epoch - 114us/sample
Epoch 9/145
95501/95501 - 11s - loss: 0.0019 - val_loss: 0.0016 - 11s/epoch - 114us/sample
Epoch 10/145
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0016 - 11s/epoch - 115us/sample
Epoch 11/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0015 - 11s/epoch - 114us/sample
Epoch 12/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0014 - 11s/epoch - 115us/sample
Epoch 13/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0014 - 11s/epoch - 115us/sample
Epoch 14/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0014 - 11s/epoch - 114us/sample
Epoch 15/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 114us/sample
Epoch 16/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 114us/sample
Epoch 17/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0013 - 11s/epoch - 114us/sample
Epoch 18/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0015 - 11s/epoch - 114us/sample
Epoch 19/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0012 - 11s/epoch - 114us/sample
Epoch 20/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 21/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 114us/sample
Epoch 22/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0017 - 11s/epoch - 114us/sample
Epoch 23/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 24/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 25/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 114us/sample
Epoch 26/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 27/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 28/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 29/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 30/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 31/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 113us/sample
Epoch 32/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 33/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0016 - 11s/epoch - 114us/sample
Epoch 34/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 35/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 36/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 116us/sample
Epoch 37/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 38/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0041 - 11s/epoch - 114us/sample
Epoch 39/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 40/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 41/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 42/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 43/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 116us/sample
Epoch 44/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 45/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 114us/sample
Epoch 46/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0013 - 11s/epoch - 114us/sample
Epoch 47/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 48/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 49/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 50/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 51/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9061e-04 - 11s/epoch - 116us/sample
Epoch 52/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 53/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0012 - 11s/epoch - 114us/sample
Epoch 54/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 55/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 56/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.8959e-04 - 11s/epoch - 114us/sample
Epoch 57/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 58/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 59/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 60/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 61/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 9.9302e-04 - 11s/epoch - 114us/sample
Epoch 62/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.9489e-04 - 11s/epoch - 114us/sample
Epoch 63/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8158e-04 - 11s/epoch - 114us/sample
Epoch 64/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 65/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 66/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6911e-04 - 11s/epoch - 115us/sample
Epoch 67/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 68/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7101e-04 - 11s/epoch - 114us/sample
Epoch 69/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6823e-04 - 11s/epoch - 114us/sample
Epoch 70/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 71/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 72/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 73/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6673e-04 - 11s/epoch - 114us/sample
Epoch 74/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 75/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6817e-04 - 11s/epoch - 115us/sample
Epoch 76/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6875e-04 - 11s/epoch - 114us/sample
Epoch 77/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 78/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7766e-04 - 11s/epoch - 114us/sample
Epoch 79/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 80/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6204e-04 - 11s/epoch - 114us/sample
Epoch 81/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5429e-04 - 11s/epoch - 114us/sample
Epoch 82/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9099e-04 - 11s/epoch - 116us/sample
Epoch 83/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4791e-04 - 11s/epoch - 114us/sample
Epoch 84/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.8332e-04 - 11s/epoch - 114us/sample
Epoch 85/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 86/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.8022e-04 - 11s/epoch - 114us/sample
Epoch 87/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5493e-04 - 11s/epoch - 114us/sample
Epoch 88/145
95501/95501 - 11s - loss: 9.9631e-04 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 89/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.5000e-04 - 11s/epoch - 115us/sample
Epoch 90/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 91/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.5190e-04 - 11s/epoch - 114us/sample
Epoch 92/145
95501/95501 - 11s - loss: 9.8901e-04 - val_loss: 9.4866e-04 - 11s/epoch - 114us/sample
Epoch 93/145
95501/95501 - 11s - loss: 9.9540e-04 - val_loss: 9.4074e-04 - 11s/epoch - 114us/sample
Epoch 94/145
95501/95501 - 11s - loss: 9.8232e-04 - val_loss: 9.4444e-04 - 11s/epoch - 114us/sample
Epoch 95/145
95501/95501 - 11s - loss: 9.9375e-04 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 96/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.4776e-04 - 11s/epoch - 114us/sample
Epoch 97/145
95501/95501 - 11s - loss: 9.9334e-04 - val_loss: 9.6687e-04 - 11s/epoch - 116us/sample
Epoch 98/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 99/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.5092e-04 - 11s/epoch - 114us/sample
Epoch 100/145
95501/95501 - 11s - loss: 9.8833e-04 - val_loss: 9.3787e-04 - 11s/epoch - 114us/sample
Epoch 101/145
95501/95501 - 11s - loss: 9.8369e-04 - val_loss: 9.7089e-04 - 11s/epoch - 114us/sample
Epoch 102/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4555e-04 - 11s/epoch - 114us/sample
Epoch 103/145
95501/95501 - 11s - loss: 9.8410e-04 - val_loss: 9.4888e-04 - 11s/epoch - 114us/sample
Epoch 104/145
95501/95501 - 11s - loss: 9.8470e-04 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 105/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.2458e-04 - 11s/epoch - 116us/sample
Epoch 106/145
95501/95501 - 11s - loss: 9.7629e-04 - val_loss: 9.3962e-04 - 11s/epoch - 114us/sample
Epoch 107/145
95501/95501 - 11s - loss: 9.8533e-04 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 108/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 109/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7983e-04 - 11s/epoch - 114us/sample
Epoch 110/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.9942e-04 - 11s/epoch - 114us/sample
Epoch 111/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6346e-04 - 11s/epoch - 114us/sample
Epoch 112/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.3300e-04 - 11s/epoch - 115us/sample
Epoch 113/145
95501/95501 - 11s - loss: 9.7803e-04 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 114/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.4191e-04 - 11s/epoch - 114us/sample
Epoch 115/145
95501/95501 - 11s - loss: 9.8320e-04 - val_loss: 9.2216e-04 - 11s/epoch - 114us/sample
Epoch 116/145
95501/95501 - 11s - loss: 9.7349e-04 - val_loss: 9.2072e-04 - 11s/epoch - 114us/sample
Epoch 117/145
95501/95501 - 11s - loss: 9.7736e-04 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 118/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.2389e-04 - 11s/epoch - 114us/sample
Epoch 119/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4730e-04 - 11s/epoch - 114us/sample
Epoch 120/145
95501/95501 - 11s - loss: 9.7381e-04 - val_loss: 9.3221e-04 - 11s/epoch - 116us/sample
Epoch 121/145
95501/95501 - 11s - loss: 9.6376e-04 - val_loss: 9.2135e-04 - 11s/epoch - 115us/sample
Epoch 122/145
95501/95501 - 11s - loss: 9.5970e-04 - val_loss: 9.1398e-04 - 11s/epoch - 114us/sample
Epoch 123/145
95501/95501 - 11s - loss: 9.6759e-04 - val_loss: 9.2499e-04 - 11s/epoch - 114us/sample
Epoch 124/145
95501/95501 - 11s - loss: 9.6513e-04 - val_loss: 9.5752e-04 - 11s/epoch - 114us/sample
Epoch 125/145
95501/95501 - 11s - loss: 9.8164e-04 - val_loss: 9.1786e-04 - 11s/epoch - 114us/sample
Epoch 126/145
95501/95501 - 11s - loss: 9.6244e-04 - val_loss: 9.1353e-04 - 11s/epoch - 114us/sample
Epoch 127/145
95501/95501 - 11s - loss: 9.6496e-04 - val_loss: 9.1891e-04 - 11s/epoch - 114us/sample
Epoch 128/145
95501/95501 - 11s - loss: 9.5404e-04 - val_loss: 9.3659e-04 - 11s/epoch - 116us/sample
Epoch 129/145
95501/95501 - 11s - loss: 9.7158e-04 - val_loss: 9.0773e-04 - 11s/epoch - 114us/sample
Epoch 130/145
95501/95501 - 11s - loss: 9.4792e-04 - val_loss: 9.1214e-04 - 11s/epoch - 114us/sample
Epoch 131/145
95501/95501 - 11s - loss: 9.4899e-04 - val_loss: 9.2553e-04 - 11s/epoch - 114us/sample
Epoch 132/145
95501/95501 - 11s - loss: 9.7809e-04 - val_loss: 9.4323e-04 - 11s/epoch - 114us/sample
Epoch 133/145
95501/95501 - 11s - loss: 9.8821e-04 - val_loss: 9.2963e-04 - 11s/epoch - 114us/sample
Epoch 134/145
95501/95501 - 11s - loss: 9.6178e-04 - val_loss: 9.0708e-04 - 11s/epoch - 115us/sample
Epoch 135/145
95501/95501 - 11s - loss: 9.5328e-04 - val_loss: 9.3403e-04 - 11s/epoch - 114us/sample
Epoch 136/145
95501/95501 - 11s - loss: 9.7935e-04 - val_loss: 0.0010 - 11s/epoch - 116us/sample
Epoch 137/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.0932e-04 - 11s/epoch - 115us/sample
Epoch 138/145
95501/95501 - 11s - loss: 9.5462e-04 - val_loss: 9.4584e-04 - 11s/epoch - 114us/sample
Epoch 139/145
95501/95501 - 11s - loss: 9.6895e-04 - val_loss: 9.0811e-04 - 11s/epoch - 114us/sample
Epoch 140/145
95501/95501 - 11s - loss: 9.4611e-04 - val_loss: 9.1203e-04 - 11s/epoch - 114us/sample
Epoch 141/145
95501/95501 - 11s - loss: 9.4804e-04 - val_loss: 0.0011 - 11s/epoch - 114us/sample
Epoch 142/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.1515e-04 - 11s/epoch - 115us/sample
Epoch 143/145
95501/95501 - 11s - loss: 9.6498e-04 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 144/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.2471e-04 - 11s/epoch - 116us/sample
Epoch 145/145
95501/95501 - 11s - loss: 9.7273e-04 - val_loss: 9.7003e-04 - 11s/epoch - 114us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0009700321047875751
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:57:02.958005: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_28/outputlayer/BiasAdd' id:36217 op device:{requested: '', assigned: ''} def:{{{node decoder_model_28/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_28/outputlayer/MatMul, decoder_model_28/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006348047213108639
cosine 0.005017773827830666
MAE: 0.012272993
RMSE: 0.023493147
r2: 0.9641954511052879
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 64, 145, 0.0012, 0.3, 379, 0.0009727282800137525, 0.0009700321047875751, 0.006348047213108639, 0.005017773827830666, 0.012272993102669716, 0.023493146523833275, 0.9641954511052879, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 140 0.0012 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_87 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_87 (ReLU)                (None, 2022)         0           ['batch_normalization_87[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          766717      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          766717      ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3479056     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,578,408
Trainable params: 7,569,562
Non-trainable params: 8,846
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-15 05:57:17.565488: W tensorflow/c/c_api.cc:291] Operation '{name:'training_58/Adam/bottleneck_zlog_29/bias/v/Assign' id:38121 op device:{requested: '', assigned: ''} def:{{{node training_58/Adam/bottleneck_zlog_29/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_58/Adam/bottleneck_zlog_29/bias/v, training_58/Adam/bottleneck_zlog_29/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:57:31.298605: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_29/mul' id:37507 op device:{requested: '', assigned: ''} def:{{{node loss_29/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_29/mul/x, loss_29/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 0.0112 - val_loss: 0.0206 - 19s/epoch - 196us/sample
Epoch 2/140
95501/95501 - 11s - loss: 0.0047 - val_loss: 0.0041 - 11s/epoch - 118us/sample
Epoch 3/140
95501/95501 - 11s - loss: 0.0041 - val_loss: 0.0040 - 11s/epoch - 118us/sample
Epoch 4/140
95501/95501 - 11s - loss: 0.0035 - val_loss: 0.0029 - 11s/epoch - 117us/sample
Epoch 5/140
95501/95501 - 11s - loss: 0.0029 - val_loss: 0.0026 - 11s/epoch - 118us/sample
Epoch 6/140
95501/95501 - 11s - loss: 0.0026 - val_loss: 0.0020 - 11s/epoch - 117us/sample
Epoch 7/140
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0039 - 11s/epoch - 117us/sample
Epoch 8/140
95501/95501 - 11s - loss: 0.0060 - val_loss: 0.0028 - 11s/epoch - 118us/sample
Epoch 9/140
95501/95501 - 11s - loss: 0.0027 - val_loss: 0.0023 - 11s/epoch - 118us/sample
Epoch 10/140
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0019 - 11s/epoch - 117us/sample
Epoch 11/140
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0021 - 11s/epoch - 119us/sample
Epoch 12/140
95501/95501 - 11s - loss: 0.0019 - val_loss: 0.0018 - 11s/epoch - 118us/sample
Epoch 13/140
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0016 - 11s/epoch - 117us/sample
Epoch 14/140
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0016 - 11s/epoch - 118us/sample
Epoch 15/140
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0015 - 11s/epoch - 117us/sample
Epoch 16/140
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0048 - 11s/epoch - 117us/sample
Epoch 17/140
95501/95501 - 11s - loss: 0.0028 - val_loss: 0.0017 - 11s/epoch - 118us/sample
Epoch 18/140
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0019 - 11s/epoch - 118us/sample
Epoch 19/140
95501/95501 - 11s - loss: 0.0021 - val_loss: 0.0016 - 11s/epoch - 118us/sample
Epoch 20/140
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0015 - 11s/epoch - 117us/sample
Epoch 21/140
95501/95501 - 11s - loss: 0.0021 - val_loss: 0.0015 - 11s/epoch - 118us/sample
Epoch 22/140
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0014 - 11s/epoch - 117us/sample
Epoch 23/140
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0016 - 11s/epoch - 117us/sample
Epoch 24/140
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0014 - 11s/epoch - 118us/sample
Epoch 25/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0018 - 11s/epoch - 118us/sample
Epoch 26/140
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0014 - 11s/epoch - 119us/sample
Epoch 27/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0017 - 11s/epoch - 117us/sample
Epoch 28/140
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0013 - 11s/epoch - 117us/sample
Epoch 29/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0013 - 11s/epoch - 118us/sample
Epoch 30/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 117us/sample
Epoch 31/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0014 - 11s/epoch - 118us/sample
Epoch 32/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 118us/sample
Epoch 33/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 118us/sample
Epoch 34/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 117us/sample
Epoch 35/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 118us/sample
Epoch 36/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 118us/sample
Epoch 37/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 38/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0014 - 11s/epoch - 117us/sample
Epoch 39/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 40/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 118us/sample
Epoch 41/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 120us/sample
Epoch 42/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 43/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 118us/sample
Epoch 44/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0014 - 11s/epoch - 117us/sample
Epoch 45/140
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 46/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 47/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 48/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 119us/sample
Epoch 49/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 50/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 51/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 52/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 53/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0027 - 11s/epoch - 117us/sample
Epoch 54/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 55/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0013 - 11s/epoch - 119us/sample
Epoch 56/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0016 - 11s/epoch - 118us/sample
Epoch 57/140
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 117us/sample
Epoch 58/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 59/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 60/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 61/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 62/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 63/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 64/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 65/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 66/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0013 - 11s/epoch - 117us/sample
Epoch 67/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 68/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 118us/sample
Epoch 69/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 70/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 71/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 72/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 73/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 74/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 115us/sample
Epoch 75/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 76/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 77/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 118us/sample
Epoch 78/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 79/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 80/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 81/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 82/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 83/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 84/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 85/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 86/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 87/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 117us/sample
Epoch 88/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 89/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 90/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 91/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 92/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 119us/sample
Epoch 93/140
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 94/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 95/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 96/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 97/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 98/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 99/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 100/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 101/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 102/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 103/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 104/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 105/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 106/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 107/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 108/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 109/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 110/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 111/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 112/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 117us/sample
Epoch 113/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 114/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 115/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 116/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 117/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 118/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 119/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 120/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 121/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 119us/sample
Epoch 122/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0012 - 11s/epoch - 119us/sample
Epoch 123/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 124/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 117us/sample
Epoch 125/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 126/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 117us/sample
Epoch 127/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 128/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 129/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 120us/sample
Epoch 130/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 131/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 132/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 133/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 118us/sample
Epoch 134/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 135/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 117us/sample
Epoch 136/140
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0010 - 11s/epoch - 119us/sample
Epoch 137/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 117us/sample
Epoch 138/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 118us/sample
Epoch 139/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 117us/sample
Epoch 140/140
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 118us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0010443233481695741
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:23:36.676925: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_29/outputlayer/BiasAdd' id:37478 op device:{requested: '', assigned: ''} def:{{{node decoder_model_29/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_29/outputlayer/MatMul, decoder_model_29/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.007285457072148357
cosine 0.005746147271331373
MAE: 0.013024858
RMSE: 0.025015924
r2: 0.959405503912232
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 64, 140, 0.0012, 0.3, 379, 0.001095386945808051, 0.0010443233481695741, 0.007285457072148357, 0.005746147271331373, 0.013024858199059963, 0.025015924125909805, 0.959405503912232, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 145 0.0012 64 0] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_90 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_90 (ReLU)                (None, 1896)         0           ['batch_normalization_90[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 06:23:52.162096: W tensorflow/c/c_api.cc:291] Operation '{name:'training_60/Adam/batch_normalization_91/gamma/m/Assign' id:39365 op device:{requested: '', assigned: ''} def:{{{node training_60/Adam/batch_normalization_91/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_60/Adam/batch_normalization_91/gamma/m, training_60/Adam/batch_normalization_91/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:24:06.677461: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_30/mul' id:38787 op device:{requested: '', assigned: ''} def:{{{node loss_30/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_30/mul/x, loss_30/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 20s - loss: 7.4201 - val_loss: 1.4991 - 20s/epoch - 207us/sample
Epoch 2/145
95501/95501 - 12s - loss: 1.4812 - val_loss: 1.4687 - 12s/epoch - 121us/sample
Epoch 3/145
95501/95501 - 12s - loss: 1.4654 - val_loss: 1.4708 - 12s/epoch - 121us/sample
Epoch 4/145
95501/95501 - 12s - loss: 1.4761 - val_loss: 1.4762 - 12s/epoch - 121us/sample
Epoch 5/145
95501/95501 - 12s - loss: 1.4643 - val_loss: 1.4581 - 12s/epoch - 121us/sample
Epoch 6/145
95501/95501 - 11s - loss: 1.4909 - val_loss: 1.4852 - 11s/epoch - 120us/sample
Epoch 7/145
95501/95501 - 12s - loss: 1.4671 - val_loss: 1.4600 - 12s/epoch - 121us/sample
Epoch 8/145
95501/95501 - 12s - loss: 1.4553 - val_loss: 1.4563 - 12s/epoch - 121us/sample
Epoch 9/145
95501/95501 - 12s - loss: 1.4704 - val_loss: 1.4594 - 12s/epoch - 122us/sample
Epoch 10/145
95501/95501 - 12s - loss: 1.4662 - val_loss: 1.4778 - 12s/epoch - 122us/sample
Epoch 11/145
95501/95501 - 12s - loss: 1.4623 - val_loss: 1.4719 - 12s/epoch - 121us/sample
Epoch 12/145
95501/95501 - 12s - loss: 1.5020 - val_loss: 1.5068 - 12s/epoch - 121us/sample
Epoch 13/145
95501/95501 - 12s - loss: 1.4761 - val_loss: 1.4686 - 12s/epoch - 121us/sample
Epoch 14/145
95501/95501 - 12s - loss: 1.5285 - val_loss: 1.4657 - 12s/epoch - 121us/sample
Epoch 15/145
95501/95501 - 12s - loss: 1.4450 - val_loss: 1.4383 - 12s/epoch - 121us/sample
Epoch 16/145
95501/95501 - 12s - loss: 1.4397 - val_loss: 1.4360 - 12s/epoch - 123us/sample
Epoch 17/145
95501/95501 - 12s - loss: 1.4341 - val_loss: 1.4341 - 12s/epoch - 121us/sample
Epoch 18/145
95501/95501 - 12s - loss: 1.4387 - val_loss: 1.4734 - 12s/epoch - 121us/sample
Epoch 19/145
95501/95501 - 12s - loss: 1.4520 - val_loss: 1.4610 - 12s/epoch - 121us/sample
Epoch 20/145
95501/95501 - 12s - loss: 1.4440 - val_loss: 1.4454 - 12s/epoch - 121us/sample
Epoch 21/145
95501/95501 - 12s - loss: 1.4370 - val_loss: 1.4411 - 12s/epoch - 121us/sample
Epoch 22/145
95501/95501 - 12s - loss: 1.4521 - val_loss: 1.4485 - 12s/epoch - 121us/sample
Epoch 23/145
95501/95501 - 12s - loss: 1.4320 - val_loss: 1.4321 - 12s/epoch - 121us/sample
Epoch 24/145
95501/95501 - 12s - loss: 1.4304 - val_loss: 1.4357 - 12s/epoch - 122us/sample
Epoch 25/145
95501/95501 - 12s - loss: 1.4352 - val_loss: 1.4320 - 12s/epoch - 122us/sample
Epoch 26/145
95501/95501 - 12s - loss: 1.4347 - val_loss: 1.4369 - 12s/epoch - 121us/sample
Epoch 27/145
95501/95501 - 12s - loss: 1.4328 - val_loss: 1.4394 - 12s/epoch - 121us/sample
Epoch 28/145
95501/95501 - 12s - loss: 1.4280 - val_loss: 1.4319 - 12s/epoch - 121us/sample
Epoch 29/145
95501/95501 - 12s - loss: 1.4274 - val_loss: 1.4287 - 12s/epoch - 121us/sample
Epoch 30/145
95501/95501 - 12s - loss: 1.4235 - val_loss: 1.4245 - 12s/epoch - 122us/sample
Epoch 31/145
95501/95501 - 12s - loss: 1.4294 - val_loss: 1.4341 - 12s/epoch - 122us/sample
Epoch 32/145
95501/95501 - 12s - loss: 1.4262 - val_loss: 1.4272 - 12s/epoch - 122us/sample
Epoch 33/145
95501/95501 - 12s - loss: 1.4250 - val_loss: 1.4268 - 12s/epoch - 121us/sample
Epoch 34/145
95501/95501 - 12s - loss: 1.4235 - val_loss: 1.4243 - 12s/epoch - 121us/sample
Epoch 35/145
95501/95501 - 12s - loss: 1.4212 - val_loss: 1.4219 - 12s/epoch - 121us/sample
Epoch 36/145
95501/95501 - 12s - loss: 1.4222 - val_loss: 1.4254 - 12s/epoch - 121us/sample
Epoch 37/145
95501/95501 - 12s - loss: 1.4202 - val_loss: 1.4239 - 12s/epoch - 122us/sample
Epoch 38/145
95501/95501 - 12s - loss: 1.4205 - val_loss: 1.4208 - 12s/epoch - 121us/sample
Epoch 39/145
95501/95501 - 12s - loss: 1.4229 - val_loss: 1.4238 - 12s/epoch - 122us/sample
Epoch 40/145
95501/95501 - 12s - loss: 1.4199 - val_loss: 1.4212 - 12s/epoch - 121us/sample
Epoch 41/145
95501/95501 - 12s - loss: 1.4220 - val_loss: 1.4232 - 12s/epoch - 122us/sample
Epoch 42/145
95501/95501 - 12s - loss: 1.4201 - val_loss: 1.4209 - 12s/epoch - 121us/sample
Epoch 43/145
95501/95501 - 12s - loss: 1.4195 - val_loss: 1.4221 - 12s/epoch - 121us/sample
Epoch 44/145
95501/95501 - 12s - loss: 1.4208 - val_loss: 1.4216 - 12s/epoch - 121us/sample
Epoch 45/145
95501/95501 - 12s - loss: 1.4199 - val_loss: 1.4208 - 12s/epoch - 121us/sample
Epoch 46/145
95501/95501 - 11s - loss: 1.4202 - val_loss: 1.4211 - 11s/epoch - 120us/sample
Epoch 47/145
95501/95501 - 12s - loss: 1.4185 - val_loss: 1.4198 - 12s/epoch - 122us/sample
Epoch 48/145
95501/95501 - 12s - loss: 1.4186 - val_loss: 1.4198 - 12s/epoch - 121us/sample
Epoch 49/145
95501/95501 - 12s - loss: 1.4177 - val_loss: 1.4192 - 12s/epoch - 122us/sample
Epoch 50/145
95501/95501 - 11s - loss: 1.4196 - val_loss: 1.4214 - 11s/epoch - 120us/sample
Epoch 51/145
95501/95501 - 12s - loss: 1.4188 - val_loss: 1.4205 - 12s/epoch - 121us/sample
Epoch 52/145
95501/95501 - 12s - loss: 1.4185 - val_loss: 1.4204 - 12s/epoch - 122us/sample
Epoch 53/145
95501/95501 - 12s - loss: 1.4189 - val_loss: 1.4199 - 12s/epoch - 121us/sample
Epoch 54/145
95501/95501 - 12s - loss: 1.4182 - val_loss: 1.4202 - 12s/epoch - 123us/sample
Epoch 55/145
95501/95501 - 12s - loss: 1.4179 - val_loss: 1.4195 - 12s/epoch - 120us/sample
Epoch 56/145
95501/95501 - 12s - loss: 1.4180 - val_loss: 1.4224 - 12s/epoch - 122us/sample
Epoch 57/145
95501/95501 - 12s - loss: 1.4224 - val_loss: 1.4275 - 12s/epoch - 122us/sample
Epoch 58/145
95501/95501 - 12s - loss: 1.4232 - val_loss: 1.4231 - 12s/epoch - 121us/sample
Epoch 59/145
95501/95501 - 12s - loss: 1.4197 - val_loss: 1.4208 - 12s/epoch - 121us/sample
Epoch 60/145
95501/95501 - 12s - loss: 1.4181 - val_loss: 1.4198 - 12s/epoch - 121us/sample
Epoch 61/145
95501/95501 - 12s - loss: 1.4171 - val_loss: 1.4186 - 12s/epoch - 123us/sample
Epoch 62/145
95501/95501 - 12s - loss: 1.4164 - val_loss: 1.4183 - 12s/epoch - 121us/sample
Epoch 63/145
95501/95501 - 12s - loss: 1.4164 - val_loss: 1.4182 - 12s/epoch - 121us/sample
Epoch 64/145
95501/95501 - 12s - loss: 1.4177 - val_loss: 1.4203 - 12s/epoch - 121us/sample
Epoch 65/145
95501/95501 - 12s - loss: 1.4190 - val_loss: 1.4198 - 12s/epoch - 121us/sample
Epoch 66/145
95501/95501 - 12s - loss: 1.4175 - val_loss: 1.4195 - 12s/epoch - 121us/sample
Epoch 67/145
95501/95501 - 12s - loss: 1.4203 - val_loss: 1.4216 - 12s/epoch - 122us/sample
Epoch 68/145
95501/95501 - 12s - loss: 1.4195 - val_loss: 1.4199 - 12s/epoch - 122us/sample
Epoch 69/145
95501/95501 - 12s - loss: 1.4174 - val_loss: 1.4203 - 12s/epoch - 121us/sample
Epoch 70/145
95501/95501 - 12s - loss: 1.4184 - val_loss: 1.4201 - 12s/epoch - 122us/sample
Epoch 71/145
95501/95501 - 12s - loss: 1.4177 - val_loss: 1.4202 - 12s/epoch - 121us/sample
Epoch 72/145
95501/95501 - 12s - loss: 1.4175 - val_loss: 1.4191 - 12s/epoch - 122us/sample
Epoch 73/145
95501/95501 - 12s - loss: 1.4167 - val_loss: 1.4197 - 12s/epoch - 121us/sample
Epoch 74/145
95501/95501 - 12s - loss: 1.4170 - val_loss: 1.4185 - 12s/epoch - 121us/sample
Epoch 75/145
95501/95501 - 12s - loss: 1.4163 - val_loss: 1.4180 - 12s/epoch - 122us/sample
Epoch 76/145
95501/95501 - 12s - loss: 1.4173 - val_loss: 1.4191 - 12s/epoch - 122us/sample
Epoch 77/145
95501/95501 - 12s - loss: 1.4187 - val_loss: 1.4197 - 12s/epoch - 122us/sample
Epoch 78/145
95501/95501 - 12s - loss: 1.4169 - val_loss: 1.4186 - 12s/epoch - 121us/sample
Epoch 79/145
95501/95501 - 12s - loss: 1.4176 - val_loss: 1.4202 - 12s/epoch - 121us/sample
Epoch 80/145
95501/95501 - 12s - loss: 1.4185 - val_loss: 1.4220 - 12s/epoch - 121us/sample
Epoch 81/145
95501/95501 - 12s - loss: 1.4190 - val_loss: 1.4203 - 12s/epoch - 122us/sample
Epoch 82/145
95501/95501 - 12s - loss: 1.4179 - val_loss: 1.4208 - 12s/epoch - 122us/sample
Epoch 83/145
95501/95501 - 12s - loss: 1.4175 - val_loss: 1.4192 - 12s/epoch - 122us/sample
Epoch 84/145
95501/95501 - 12s - loss: 1.4167 - val_loss: 1.4205 - 12s/epoch - 122us/sample
Epoch 85/145
95501/95501 - 12s - loss: 1.4185 - val_loss: 1.4203 - 12s/epoch - 121us/sample
Epoch 86/145
95501/95501 - 12s - loss: 1.4171 - val_loss: 1.4187 - 12s/epoch - 121us/sample
Epoch 87/145
95501/95501 - 12s - loss: 1.4174 - val_loss: 1.4193 - 12s/epoch - 121us/sample
Epoch 88/145
95501/95501 - 12s - loss: 1.4188 - val_loss: 1.4202 - 12s/epoch - 121us/sample
Epoch 89/145
95501/95501 - 12s - loss: 1.4179 - val_loss: 1.4200 - 12s/epoch - 121us/sample
Epoch 90/145
95501/95501 - 12s - loss: 1.4174 - val_loss: 1.4197 - 12s/epoch - 123us/sample
Epoch 91/145
95501/95501 - 12s - loss: 1.4176 - val_loss: 1.4202 - 12s/epoch - 122us/sample
Epoch 92/145
95501/95501 - 12s - loss: 1.4179 - val_loss: 1.4199 - 12s/epoch - 122us/sample
Epoch 93/145
95501/95501 - 12s - loss: 1.4177 - val_loss: 1.4187 - 12s/epoch - 121us/sample
Epoch 94/145
95501/95501 - 12s - loss: 1.4161 - val_loss: 1.4178 - 12s/epoch - 121us/sample
Epoch 95/145
95501/95501 - 12s - loss: 1.4163 - val_loss: 1.4194 - 12s/epoch - 121us/sample
Epoch 96/145
95501/95501 - 12s - loss: 1.4169 - val_loss: 1.4189 - 12s/epoch - 121us/sample
Epoch 97/145
95501/95501 - 12s - loss: 1.4166 - val_loss: 1.4188 - 12s/epoch - 121us/sample
Epoch 98/145
95501/95501 - 12s - loss: 1.4164 - val_loss: 1.4186 - 12s/epoch - 123us/sample
Epoch 99/145
95501/95501 - 11s - loss: 1.4163 - val_loss: 1.4182 - 11s/epoch - 120us/sample
Epoch 100/145
95501/95501 - 12s - loss: 1.4175 - val_loss: 1.4202 - 12s/epoch - 121us/sample
Epoch 101/145
95501/95501 - 12s - loss: 1.4179 - val_loss: 1.4190 - 12s/epoch - 121us/sample
Epoch 102/145
95501/95501 - 12s - loss: 1.4155 - val_loss: 1.4173 - 12s/epoch - 121us/sample
Epoch 103/145
95501/95501 - 12s - loss: 1.4136 - val_loss: 1.4151 - 12s/epoch - 121us/sample
Epoch 104/145
95501/95501 - 12s - loss: 1.4132 - val_loss: 1.4147 - 12s/epoch - 121us/sample
Epoch 105/145
95501/95501 - 12s - loss: 1.4127 - val_loss: 1.4143 - 12s/epoch - 122us/sample
Epoch 106/145
95501/95501 - 12s - loss: 1.4128 - val_loss: 1.4145 - 12s/epoch - 122us/sample
Epoch 107/145
95501/95501 - 12s - loss: 1.4128 - val_loss: 1.4164 - 12s/epoch - 121us/sample
Epoch 108/145
95501/95501 - 12s - loss: 1.4133 - val_loss: 1.4151 - 12s/epoch - 122us/sample
Epoch 109/145
95501/95501 - 12s - loss: 1.4130 - val_loss: 1.4151 - 12s/epoch - 121us/sample
Epoch 110/145
95501/95501 - 12s - loss: 1.4136 - val_loss: 1.4149 - 12s/epoch - 121us/sample
Epoch 111/145
95501/95501 - 12s - loss: 1.4126 - val_loss: 1.4140 - 12s/epoch - 120us/sample
Epoch 112/145
95501/95501 - 12s - loss: 1.4125 - val_loss: 1.4144 - 12s/epoch - 122us/sample
Epoch 113/145
95501/95501 - 12s - loss: 1.4120 - val_loss: 1.4149 - 12s/epoch - 121us/sample
Epoch 114/145
95501/95501 - 12s - loss: 1.4127 - val_loss: 1.4145 - 12s/epoch - 121us/sample
Epoch 115/145
95501/95501 - 12s - loss: 1.4128 - val_loss: 1.4171 - 12s/epoch - 121us/sample
Epoch 116/145
95501/95501 - 12s - loss: 1.4143 - val_loss: 1.4154 - 12s/epoch - 122us/sample
Epoch 117/145
95501/95501 - 12s - loss: 1.4138 - val_loss: 1.4160 - 12s/epoch - 121us/sample
Epoch 118/145
95501/95501 - 12s - loss: 1.4131 - val_loss: 1.4148 - 12s/epoch - 122us/sample
Epoch 119/145
95501/95501 - 12s - loss: 1.4129 - val_loss: 1.4152 - 12s/epoch - 121us/sample
Epoch 120/145
95501/95501 - 12s - loss: 1.4138 - val_loss: 1.4153 - 12s/epoch - 122us/sample
Epoch 121/145
95501/95501 - 12s - loss: 1.4130 - val_loss: 1.4146 - 12s/epoch - 121us/sample
Epoch 122/145
95501/95501 - 12s - loss: 1.4132 - val_loss: 1.4162 - 12s/epoch - 121us/sample
Epoch 123/145
95501/95501 - 12s - loss: 1.4127 - val_loss: 1.4142 - 12s/epoch - 122us/sample
Epoch 124/145
95501/95501 - 12s - loss: 1.4121 - val_loss: 1.4147 - 12s/epoch - 121us/sample
Epoch 125/145
95501/95501 - 12s - loss: 1.4126 - val_loss: 1.4145 - 12s/epoch - 121us/sample
Epoch 126/145
95501/95501 - 12s - loss: 1.4127 - val_loss: 1.4155 - 12s/epoch - 121us/sample
Epoch 127/145
95501/95501 - 12s - loss: 1.4120 - val_loss: 1.4140 - 12s/epoch - 122us/sample
Epoch 128/145
95501/95501 - 12s - loss: 1.4114 - val_loss: 1.4140 - 12s/epoch - 121us/sample
Epoch 129/145
95501/95501 - 12s - loss: 1.4121 - val_loss: 1.4140 - 12s/epoch - 121us/sample
Epoch 130/145
95501/95501 - 12s - loss: 1.4118 - val_loss: 1.4136 - 12s/epoch - 121us/sample
Epoch 131/145
95501/95501 - 12s - loss: 1.4119 - val_loss: 1.4144 - 12s/epoch - 121us/sample
Epoch 132/145
95501/95501 - 12s - loss: 1.4123 - val_loss: 1.4145 - 12s/epoch - 121us/sample
Epoch 133/145
95501/95501 - 12s - loss: 1.4123 - val_loss: 1.4141 - 12s/epoch - 121us/sample
Epoch 134/145
95501/95501 - 12s - loss: 1.4123 - val_loss: 1.4142 - 12s/epoch - 121us/sample
Epoch 135/145
95501/95501 - 12s - loss: 1.4124 - val_loss: 1.4142 - 12s/epoch - 122us/sample
Epoch 136/145
95501/95501 - 12s - loss: 1.4116 - val_loss: 1.4135 - 12s/epoch - 121us/sample
Epoch 137/145
95501/95501 - 12s - loss: 1.4114 - val_loss: 1.4135 - 12s/epoch - 121us/sample
Epoch 138/145
95501/95501 - 12s - loss: 1.4124 - val_loss: 1.4139 - 12s/epoch - 122us/sample
Epoch 139/145
95501/95501 - 12s - loss: 1.4115 - val_loss: 1.4132 - 12s/epoch - 121us/sample
Epoch 140/145
95501/95501 - 12s - loss: 1.4116 - val_loss: 1.4140 - 12s/epoch - 121us/sample
Epoch 141/145
95501/95501 - 12s - loss: 1.4121 - val_loss: 1.4141 - 12s/epoch - 122us/sample
Epoch 142/145
95501/95501 - 12s - loss: 1.4118 - val_loss: 1.4139 - 12s/epoch - 122us/sample
Epoch 143/145
95501/95501 - 12s - loss: 1.4121 - val_loss: 1.4161 - 12s/epoch - 122us/sample
Epoch 144/145
95501/95501 - 12s - loss: 1.4133 - val_loss: 1.4155 - 12s/epoch - 121us/sample
Epoch 145/145
95501/95501 - 12s - loss: 1.4131 - val_loss: 1.4148 - 12s/epoch - 121us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 1.414754756207831
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:51:58.006316: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_30/outputlayer/BiasAdd' id:38739 op device:{requested: '', assigned: ''} def:{{{node decoder_model_30/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_30/outputlayer/MatMul, decoder_model_30/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7327528771956258
cosine 0.913017986752408
MAE: 7.1990733
RMSE: 18.746874
r2: -22795.120154850465
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'binary_crossentropy', 64, 145, 0.0012, 0.3, 379, 1.4131267181477711, 1.414754756207831, 0.7327528771956258, 0.913017986752408, 7.199073314666748, 18.74687385559082, -22795.120154850465, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0012 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_93 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_93 (ReLU)                (None, 2022)         0           ['batch_normalization_93[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          766717      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          766717      ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3479056     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,578,408
Trainable params: 7,569,562
Non-trainable params: 8,846
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 06:52:13.675327: W tensorflow/c/c_api.cc:291] Operation '{name:'training_62/Adam/batch_normalization_95/gamma/m/Assign' id:40652 op device:{requested: '', assigned: ''} def:{{{node training_62/Adam/batch_normalization_95/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_62/Adam/batch_normalization_95/gamma/m, training_62/Adam/batch_normalization_95/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:52:33.751164: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_31/mul' id:40111 op device:{requested: '', assigned: ''} def:{{{node loss_31/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_31/mul/x, loss_31/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 26s - loss: 0.0099 - val_loss: 0.0052 - 26s/epoch - 272us/sample
Epoch 2/145
95501/95501 - 17s - loss: 0.0060 - val_loss: 0.0037 - 17s/epoch - 182us/sample
Epoch 3/145
95501/95501 - 17s - loss: 0.0036 - val_loss: 0.0026 - 17s/epoch - 182us/sample
Epoch 4/145
95501/95501 - 17s - loss: 0.0030 - val_loss: 0.0023 - 17s/epoch - 181us/sample
Epoch 5/145
95501/95501 - 17s - loss: 0.0024 - val_loss: 0.0021 - 17s/epoch - 182us/sample
Epoch 6/145
95501/95501 - 17s - loss: 0.0021 - val_loss: 0.0020 - 17s/epoch - 182us/sample
Epoch 7/145
95501/95501 - 17s - loss: 0.0020 - val_loss: 0.0016 - 17s/epoch - 182us/sample
Epoch 8/145
95501/95501 - 17s - loss: 0.0018 - val_loss: 0.0016 - 17s/epoch - 182us/sample
Epoch 9/145
95501/95501 - 17s - loss: 0.0017 - val_loss: 0.0015 - 17s/epoch - 182us/sample
Epoch 10/145
95501/95501 - 17s - loss: 0.0016 - val_loss: 0.0014 - 17s/epoch - 181us/sample
Epoch 11/145
95501/95501 - 17s - loss: 0.0016 - val_loss: 0.0014 - 17s/epoch - 182us/sample
Epoch 12/145
95501/95501 - 17s - loss: 0.0015 - val_loss: 0.0013 - 17s/epoch - 183us/sample
Epoch 13/145
95501/95501 - 17s - loss: 0.0015 - val_loss: 0.0014 - 17s/epoch - 182us/sample
Epoch 14/145
95501/95501 - 17s - loss: 0.0015 - val_loss: 0.0013 - 17s/epoch - 182us/sample
Epoch 15/145
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0013 - 17s/epoch - 181us/sample
Epoch 16/145
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0013 - 17s/epoch - 183us/sample
Epoch 17/145
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0013 - 17s/epoch - 182us/sample
Epoch 18/145
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0012 - 17s/epoch - 181us/sample
Epoch 19/145
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0012 - 17s/epoch - 181us/sample
Epoch 20/145
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0012 - 17s/epoch - 181us/sample
Epoch 21/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0012 - 17s/epoch - 182us/sample
Epoch 22/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0012 - 17s/epoch - 183us/sample
Epoch 23/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0012 - 17s/epoch - 181us/sample
Epoch 24/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0012 - 17s/epoch - 182us/sample
Epoch 25/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 26/145
95501/95501 - 18s - loss: 0.0013 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 27/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 183us/sample
Epoch 28/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 29/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 30/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 31/145
95501/95501 - 18s - loss: 0.0013 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 32/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 33/145
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 34/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 35/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 36/145
95501/95501 - 18s - loss: 0.0012 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 37/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 38/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 39/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 40/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 41/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 183us/sample
Epoch 42/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 43/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 44/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 45/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 46/145
95501/95501 - 18s - loss: 0.0012 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 47/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 48/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 49/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 50/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 51/145
95501/95501 - 18s - loss: 0.0012 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 52/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 53/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 54/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 55/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 181us/sample
Epoch 56/145
95501/95501 - 18s - loss: 0.0012 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 57/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 58/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 59/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 181us/sample
Epoch 60/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 61/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 183us/sample
Epoch 62/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 63/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 181us/sample
Epoch 64/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 65/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 66/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 183us/sample
Epoch 67/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 68/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 69/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 70/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 183us/sample
Epoch 71/145
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 72/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 73/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 74/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0011 - 17s/epoch - 182us/sample
Epoch 75/145
95501/95501 - 18s - loss: 0.0012 - val_loss: 0.0010 - 18s/epoch - 183us/sample
Epoch 76/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 77/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 78/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 79/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 181us/sample
Epoch 80/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 182us/sample
Epoch 81/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 181us/sample
Epoch 82/145
95501/95501 - 17s - loss: 0.0011 - val_loss: 0.0010 - 17s/epoch - 183us/sample
Epoch 83/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 84/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 85/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9804e-04 - 18s/epoch - 188us/sample
Epoch 86/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 87/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 184us/sample
Epoch 88/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 89/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 90/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 186us/sample
Epoch 91/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 92/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 93/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 94/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 95/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9568e-04 - 18s/epoch - 187us/sample
Epoch 96/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 97/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 98/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9941e-04 - 18s/epoch - 184us/sample
Epoch 99/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9611e-04 - 18s/epoch - 185us/sample
Epoch 100/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9662e-04 - 18s/epoch - 186us/sample
Epoch 101/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9579e-04 - 18s/epoch - 185us/sample
Epoch 102/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9442e-04 - 18s/epoch - 184us/sample
Epoch 103/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 184us/sample
Epoch 104/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9296e-04 - 18s/epoch - 184us/sample
Epoch 105/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0011 - 18s/epoch - 187us/sample
Epoch 106/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 184us/sample
Epoch 107/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8763e-04 - 18s/epoch - 184us/sample
Epoch 108/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0011 - 18s/epoch - 185us/sample
Epoch 109/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8385e-04 - 18s/epoch - 184us/sample
Epoch 110/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8888e-04 - 18s/epoch - 187us/sample
Epoch 111/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8531e-04 - 18s/epoch - 185us/sample
Epoch 112/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9327e-04 - 18s/epoch - 185us/sample
Epoch 113/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 114/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9449e-04 - 18s/epoch - 184us/sample
Epoch 115/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9899e-04 - 18s/epoch - 187us/sample
Epoch 116/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 117/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9723e-04 - 18s/epoch - 184us/sample
Epoch 118/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0011 - 18s/epoch - 185us/sample
Epoch 119/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8782e-04 - 18s/epoch - 185us/sample
Epoch 120/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9525e-04 - 18s/epoch - 187us/sample
Epoch 121/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.7476e-04 - 18s/epoch - 185us/sample
Epoch 122/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8374e-04 - 18s/epoch - 184us/sample
Epoch 123/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8780e-04 - 18s/epoch - 184us/sample
Epoch 124/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8103e-04 - 18s/epoch - 184us/sample
Epoch 125/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9641e-04 - 18s/epoch - 187us/sample
Epoch 126/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.9704e-04 - 18s/epoch - 185us/sample
Epoch 127/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0011 - 18s/epoch - 184us/sample
Epoch 128/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8606e-04 - 18s/epoch - 185us/sample
Epoch 129/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 130/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8695e-04 - 18s/epoch - 188us/sample
Epoch 131/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8098e-04 - 18s/epoch - 184us/sample
Epoch 132/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.6785e-04 - 18s/epoch - 184us/sample
Epoch 133/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 134/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 185us/sample
Epoch 135/145
95501/95501 - 18s - loss: 0.0012 - val_loss: 9.8060e-04 - 18s/epoch - 187us/sample
Epoch 136/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8158e-04 - 18s/epoch - 184us/sample
Epoch 137/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8494e-04 - 18s/epoch - 185us/sample
Epoch 138/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8998e-04 - 18s/epoch - 184us/sample
Epoch 139/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.8015e-04 - 18s/epoch - 185us/sample
Epoch 140/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 0.0010 - 18s/epoch - 186us/sample
Epoch 141/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.6668e-04 - 18s/epoch - 184us/sample
Epoch 142/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.7783e-04 - 18s/epoch - 184us/sample
Epoch 143/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.7065e-04 - 18s/epoch - 184us/sample
Epoch 144/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.7588e-04 - 18s/epoch - 184us/sample
Epoch 145/145
95501/95501 - 18s - loss: 0.0011 - val_loss: 9.6755e-04 - 18s/epoch - 187us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.0009675541384904928
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:34:38.203307: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_31/outputlayer/BiasAdd' id:40082 op device:{requested: '', assigned: ''} def:{{{node decoder_model_31/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_31/outputlayer/MatMul, decoder_model_31/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006708926528613612
cosine 0.005294985150868516
MAE: 0.012526208
RMSE: 0.024049161
r2: 0.9624808069180805
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 32, 145, 0.0012, 0.3, 379, 0.0010787540767866507, 0.0009675541384904928, 0.006708926528613612, 0.005294985150868516, 0.012526207603514194, 0.02404916100203991, 0.9624808069180805, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 135 0.0012 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_96 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_96 (ReLU)                (None, 1896)         0           ['batch_normalization_96[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/135
2023-02-15 07:34:53.869544: W tensorflow/c/c_api.cc:291] Operation '{name:'training_64/Adam/bottleneck_zmean_32/kernel/m/Assign' id:41853 op device:{requested: '', assigned: ''} def:{{{node training_64/Adam/bottleneck_zmean_32/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_64/Adam/bottleneck_zmean_32/kernel/m, training_64/Adam/bottleneck_zmean_32/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:35:08.715413: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_32/mul' id:41372 op device:{requested: '', assigned: ''} def:{{{node loss_32/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_32/mul/x, loss_32/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 20s - loss: 0.0117 - val_loss: 0.0090 - 20s/epoch - 213us/sample
Epoch 2/135
95501/95501 - 12s - loss: 0.0055 - val_loss: 0.0043 - 12s/epoch - 124us/sample
Epoch 3/135
95501/95501 - 12s - loss: 124.1223 - val_loss: 0.0109 - 12s/epoch - 123us/sample
Epoch 4/135
95501/95501 - 12s - loss: 0.0041 - val_loss: 0.0030 - 12s/epoch - 124us/sample
Epoch 5/135
95501/95501 - 12s - loss: 0.0032 - val_loss: 0.0029 - 12s/epoch - 123us/sample
Epoch 6/135
95501/95501 - 12s - loss: 0.0028 - val_loss: 0.0027 - 12s/epoch - 123us/sample
Epoch 7/135
95501/95501 - 12s - loss: 0.0025 - val_loss: 0.0021 - 12s/epoch - 123us/sample
Epoch 8/135
95501/95501 - 12s - loss: 0.0022 - val_loss: 0.0019 - 12s/epoch - 123us/sample
Epoch 9/135
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0018 - 12s/epoch - 123us/sample
Epoch 10/135
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0019 - 12s/epoch - 123us/sample
Epoch 11/135
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0016 - 12s/epoch - 123us/sample
Epoch 12/135
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0015 - 12s/epoch - 125us/sample
Epoch 13/135
95501/95501 - 12s - loss: 0.0016 - val_loss: 0.0015 - 12s/epoch - 123us/sample
Epoch 14/135
95501/95501 - 12s - loss: 0.0016 - val_loss: 0.0015 - 12s/epoch - 123us/sample
Epoch 15/135
95501/95501 - 12s - loss: 0.0015 - val_loss: 0.0042 - 12s/epoch - 123us/sample
Epoch 16/135
95501/95501 - 12s - loss: 0.0016 - val_loss: 0.0045 - 12s/epoch - 123us/sample
Epoch 17/135
95501/95501 - 12s - loss: 0.0017 - val_loss: 0.0024 - 12s/epoch - 123us/sample
Epoch 18/135
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0015 - 12s/epoch - 123us/sample
Epoch 19/135
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0030 - 12s/epoch - 125us/sample
Epoch 20/135
95501/95501 - 12s - loss: 0.0015 - val_loss: 0.0013 - 12s/epoch - 124us/sample
Epoch 21/135
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0013 - 12s/epoch - 123us/sample
Epoch 22/135
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0013 - 12s/epoch - 123us/sample
Epoch 23/135
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0025 - 12s/epoch - 123us/sample
Epoch 24/135
95501/95501 - 12s - loss: 0.0016 - val_loss: 0.0014 - 12s/epoch - 124us/sample
Epoch 25/135
95501/95501 - 12s - loss: 0.0013 - val_loss: 0.0017 - 12s/epoch - 123us/sample
Epoch 26/135
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0017 - 12s/epoch - 125us/sample
Epoch 27/135
95501/95501 - 12s - loss: 0.0013 - val_loss: 0.0012 - 12s/epoch - 123us/sample
Epoch 28/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0013 - 12s/epoch - 123us/sample
Epoch 29/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0012 - 12s/epoch - 123us/sample
Epoch 30/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0014 - 12s/epoch - 123us/sample
Epoch 31/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 32/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 33/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0016 - 12s/epoch - 125us/sample
Epoch 34/135
95501/95501 - 12s - loss: 0.0013 - val_loss: 0.0011 - 12s/epoch - 124us/sample
Epoch 35/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0012 - 12s/epoch - 123us/sample
Epoch 36/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 37/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 38/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 39/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 40/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0012 - 12s/epoch - 125us/sample
Epoch 41/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 42/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 43/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 44/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 45/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0012 - 12s/epoch - 123us/sample
Epoch 46/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 47/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 124us/sample
Epoch 48/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 124us/sample
Epoch 49/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 50/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 51/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 52/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 53/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 54/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 55/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 125us/sample
Epoch 56/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 57/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 58/135
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0010 - 12s/epoch - 124us/sample
Epoch 59/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.8984e-04 - 12s/epoch - 123us/sample
Epoch 60/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 61/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0010 - 12s/epoch - 124us/sample
Epoch 62/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0010 - 12s/epoch - 124us/sample
Epoch 63/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 64/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0012 - 12s/epoch - 123us/sample
Epoch 65/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 123us/sample
Epoch 66/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.8215e-04 - 12s/epoch - 123us/sample
Epoch 67/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0011 - 12s/epoch - 123us/sample
Epoch 68/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.7881e-04 - 12s/epoch - 123us/sample
Epoch 69/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.7938e-04 - 12s/epoch - 125us/sample
Epoch 70/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.8399e-04 - 12s/epoch - 123us/sample
Epoch 71/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.9259e-04 - 12s/epoch - 123us/sample
Epoch 72/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.6974e-04 - 12s/epoch - 123us/sample
Epoch 73/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.9206e-04 - 12s/epoch - 123us/sample
Epoch 74/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.6478e-04 - 12s/epoch - 123us/sample
Epoch 75/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.6668e-04 - 11s/epoch - 120us/sample
Epoch 76/135
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.7475e-04 - 11s/epoch - 118us/sample
Epoch 77/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.7748e-04 - 12s/epoch - 122us/sample
Epoch 78/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.8030e-04 - 12s/epoch - 123us/sample
Epoch 79/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.7339e-04 - 12s/epoch - 126us/sample
Epoch 80/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.6791e-04 - 12s/epoch - 126us/sample
Epoch 81/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0013 - 12s/epoch - 127us/sample
Epoch 82/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.5737e-04 - 12s/epoch - 127us/sample
Epoch 83/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.7782e-04 - 12s/epoch - 128us/sample
Epoch 84/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.5495e-04 - 12s/epoch - 127us/sample
Epoch 85/135
95501/95501 - 12s - loss: 9.9991e-04 - val_loss: 9.6223e-04 - 12s/epoch - 126us/sample
Epoch 86/135
95501/95501 - 12s - loss: 9.9992e-04 - val_loss: 9.8373e-04 - 12s/epoch - 126us/sample
Epoch 87/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.8285e-04 - 12s/epoch - 127us/sample
Epoch 88/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.8153e-04 - 12s/epoch - 126us/sample
Epoch 89/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.4619e-04 - 12s/epoch - 126us/sample
Epoch 90/135
95501/95501 - 12s - loss: 9.9269e-04 - val_loss: 9.4091e-04 - 12s/epoch - 128us/sample
Epoch 91/135
95501/95501 - 12s - loss: 9.9853e-04 - val_loss: 9.4624e-04 - 12s/epoch - 127us/sample
Epoch 92/135
95501/95501 - 12s - loss: 9.9248e-04 - val_loss: 9.4807e-04 - 12s/epoch - 126us/sample
Epoch 93/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0010 - 12s/epoch - 126us/sample
Epoch 94/135
95501/95501 - 12s - loss: 9.9003e-04 - val_loss: 9.4320e-04 - 12s/epoch - 127us/sample
Epoch 95/135
95501/95501 - 12s - loss: 9.8981e-04 - val_loss: 9.8976e-04 - 12s/epoch - 126us/sample
Epoch 96/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.6216e-04 - 12s/epoch - 127us/sample
Epoch 97/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.3744e-04 - 12s/epoch - 127us/sample
Epoch 98/135
95501/95501 - 12s - loss: 9.8456e-04 - val_loss: 9.5080e-04 - 12s/epoch - 126us/sample
Epoch 99/135
95501/95501 - 12s - loss: 9.8101e-04 - val_loss: 9.3281e-04 - 12s/epoch - 126us/sample
Epoch 100/135
95501/95501 - 12s - loss: 9.7778e-04 - val_loss: 9.4555e-04 - 12s/epoch - 127us/sample
Epoch 101/135
95501/95501 - 12s - loss: 9.9319e-04 - val_loss: 9.4490e-04 - 12s/epoch - 127us/sample
Epoch 102/135
95501/95501 - 12s - loss: 9.8548e-04 - val_loss: 9.5218e-04 - 12s/epoch - 127us/sample
Epoch 103/135
95501/95501 - 12s - loss: 9.8276e-04 - val_loss: 9.4467e-04 - 12s/epoch - 127us/sample
Epoch 104/135
95501/95501 - 12s - loss: 9.7657e-04 - val_loss: 9.3336e-04 - 12s/epoch - 128us/sample
Epoch 105/135
95501/95501 - 12s - loss: 9.7543e-04 - val_loss: 9.3789e-04 - 12s/epoch - 126us/sample
Epoch 106/135
95501/95501 - 12s - loss: 9.7361e-04 - val_loss: 9.3211e-04 - 12s/epoch - 127us/sample
Epoch 107/135
95501/95501 - 12s - loss: 9.6888e-04 - val_loss: 9.3577e-04 - 12s/epoch - 126us/sample
Epoch 108/135
95501/95501 - 12s - loss: 9.7448e-04 - val_loss: 0.0010 - 12s/epoch - 126us/sample
Epoch 109/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.3551e-04 - 12s/epoch - 126us/sample
Epoch 110/135
95501/95501 - 12s - loss: 9.7199e-04 - val_loss: 0.0011 - 12s/epoch - 127us/sample
Epoch 111/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 128us/sample
Epoch 112/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.3055e-04 - 12s/epoch - 126us/sample
Epoch 113/135
95501/95501 - 12s - loss: 9.8284e-04 - val_loss: 9.7549e-04 - 12s/epoch - 127us/sample
Epoch 114/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0010 - 12s/epoch - 126us/sample
Epoch 115/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.4878e-04 - 12s/epoch - 126us/sample
Epoch 116/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.3068e-04 - 12s/epoch - 127us/sample
Epoch 117/135
95501/95501 - 12s - loss: 9.7524e-04 - val_loss: 9.4110e-04 - 12s/epoch - 126us/sample
Epoch 118/135
95501/95501 - 12s - loss: 9.8731e-04 - val_loss: 0.0010 - 12s/epoch - 128us/sample
Epoch 119/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.2139e-04 - 12s/epoch - 127us/sample
Epoch 120/135
95501/95501 - 12s - loss: 9.7046e-04 - val_loss: 9.1974e-04 - 12s/epoch - 126us/sample
Epoch 121/135
95501/95501 - 12s - loss: 9.7118e-04 - val_loss: 9.2401e-04 - 12s/epoch - 126us/sample
Epoch 122/135
95501/95501 - 12s - loss: 9.6759e-04 - val_loss: 9.2206e-04 - 12s/epoch - 126us/sample
Epoch 123/135
95501/95501 - 12s - loss: 9.6655e-04 - val_loss: 0.0011 - 12s/epoch - 126us/sample
Epoch 124/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.2828e-04 - 12s/epoch - 126us/sample
Epoch 125/135
95501/95501 - 12s - loss: 9.6920e-04 - val_loss: 0.0010 - 12s/epoch - 128us/sample
Epoch 126/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.2643e-04 - 12s/epoch - 127us/sample
Epoch 127/135
95501/95501 - 12s - loss: 9.7139e-04 - val_loss: 0.0010 - 12s/epoch - 127us/sample
Epoch 128/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.2425e-04 - 12s/epoch - 126us/sample
Epoch 129/135
95501/95501 - 12s - loss: 9.6590e-04 - val_loss: 9.3644e-04 - 12s/epoch - 127us/sample
Epoch 130/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.2090e-04 - 12s/epoch - 127us/sample
Epoch 131/135
95501/95501 - 12s - loss: 9.7526e-04 - val_loss: 0.0011 - 12s/epoch - 127us/sample
Epoch 132/135
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.7920e-04 - 12s/epoch - 128us/sample
Epoch 133/135
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.2149e-04 - 12s/epoch - 126us/sample
Epoch 134/135
95501/95501 - 12s - loss: 9.6762e-04 - val_loss: 9.2412e-04 - 12s/epoch - 126us/sample
Epoch 135/135
95501/95501 - 12s - loss: 9.6609e-04 - val_loss: 9.2188e-04 - 12s/epoch - 127us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.000921881457796349
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:01:47.326706: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_32/outputlayer/BiasAdd' id:41343 op device:{requested: '', assigned: ''} def:{{{node decoder_model_32/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_32/outputlayer/MatMul, decoder_model_32/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0060642324065065725
cosine 0.004786099644593369
MAE: 0.011892355
RMSE: 0.02285699
r2: 0.9661087160754859
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 64, 135, 0.0012, 0.3, 379, 0.0009660927601021857, 0.000921881457796349, 0.0060642324065065725, 0.004786099644593369, 0.011892355047166348, 0.02285698987543583, 0.9661087160754859, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 3
Fitness    = 84.84730891968614
Last generation's best solutions = [1.5 145 0.0012 64 1] with fitness 84.84730891968614.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.5, 145, 0.0012, 64, 1], dtype=object)]
Best solutions fitness :  [84.84730891968614, 84.84730891968614, 84.84730891968614]
[1.5 140 0.0012 64 0] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_99 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_99 (ReLU)                (None, 1896)         0           ['batch_normalization_99[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-15 08:02:03.555015: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_33/bias/Assign' id:42314 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_33/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_33/bias, bottleneck_zlog_33/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:02:19.338630: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_33/mul' id:42652 op device:{requested: '', assigned: ''} def:{{{node loss_33/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_33/mul/x, loss_33/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 22s - loss: 1.3769 - val_loss: 1.4556 - 22s/epoch - 227us/sample
Epoch 2/140
95501/95501 - 13s - loss: 1.4451 - val_loss: 1.4796 - 13s/epoch - 132us/sample
Epoch 3/140
95501/95501 - 13s - loss: 1.4326 - val_loss: 1.4470 - 13s/epoch - 133us/sample
Epoch 4/140
95501/95501 - 13s - loss: 1.4270 - val_loss: 1.4209 - 13s/epoch - 131us/sample
Epoch 5/140
95501/95501 - 13s - loss: 1.4259 - val_loss: 1.4421 - 13s/epoch - 132us/sample
Epoch 6/140
95501/95501 - 13s - loss: 1.4185 - val_loss: 1.4219 - 13s/epoch - 133us/sample
Epoch 7/140
95501/95501 - 13s - loss: 1.4254 - val_loss: 1.4129 - 13s/epoch - 132us/sample
Epoch 8/140
95501/95501 - 13s - loss: 1.4123 - val_loss: 1.4090 - 13s/epoch - 133us/sample
Epoch 9/140
95501/95501 - 13s - loss: 1.4243 - val_loss: 1.4606 - 13s/epoch - 132us/sample
Epoch 10/140
95501/95501 - 13s - loss: 1.4286 - val_loss: 1.4189 - 13s/epoch - 132us/sample
Epoch 11/140
95501/95501 - 13s - loss: 1.4328 - val_loss: 1.4306 - 13s/epoch - 133us/sample
Epoch 12/140
95501/95501 - 13s - loss: 1.4215 - val_loss: 1.4527 - 13s/epoch - 132us/sample
Epoch 13/140
95501/95501 - 13s - loss: 1.4257 - val_loss: 1.4177 - 13s/epoch - 131us/sample
Epoch 14/140
95501/95501 - 13s - loss: 1.4159 - val_loss: 1.4163 - 13s/epoch - 132us/sample
Epoch 15/140
95501/95501 - 13s - loss: 1.4126 - val_loss: 1.4064 - 13s/epoch - 134us/sample
Epoch 16/140
95501/95501 - 13s - loss: 1.4063 - val_loss: 1.4073 - 13s/epoch - 131us/sample
Epoch 17/140
95501/95501 - 13s - loss: 1.4070 - val_loss: 1.4090 - 13s/epoch - 132us/sample
Epoch 18/140
95501/95501 - 13s - loss: 1.4078 - val_loss: 1.4121 - 13s/epoch - 132us/sample
Epoch 19/140
95501/95501 - 13s - loss: 1.4060 - val_loss: 1.4061 - 13s/epoch - 132us/sample
Epoch 20/140
95501/95501 - 13s - loss: 1.4006 - val_loss: 1.4294 - 13s/epoch - 132us/sample
Epoch 21/140
95501/95501 - 13s - loss: 1.4046 - val_loss: 1.4058 - 13s/epoch - 133us/sample
Epoch 22/140
95501/95501 - 13s - loss: 1.4014 - val_loss: 1.4034 - 13s/epoch - 132us/sample
Epoch 23/140
95501/95501 - 13s - loss: 1.3991 - val_loss: 1.4003 - 13s/epoch - 133us/sample
Epoch 24/140
95501/95501 - 13s - loss: 1.4039 - val_loss: 1.4035 - 13s/epoch - 132us/sample
Epoch 25/140
95501/95501 - 13s - loss: 1.4011 - val_loss: 1.4011 - 13s/epoch - 132us/sample
Epoch 26/140
95501/95501 - 13s - loss: 1.4037 - val_loss: 1.4027 - 13s/epoch - 133us/sample
Epoch 27/140
95501/95501 - 13s - loss: 1.4021 - val_loss: 1.4033 - 13s/epoch - 132us/sample
Epoch 28/140
95501/95501 - 13s - loss: 1.4040 - val_loss: 1.4068 - 13s/epoch - 133us/sample
Epoch 29/140
95501/95501 - 13s - loss: 1.4041 - val_loss: 1.4037 - 13s/epoch - 133us/sample
Epoch 30/140
95501/95501 - 13s - loss: 1.4043 - val_loss: 1.4037 - 13s/epoch - 133us/sample
Epoch 31/140
95501/95501 - 13s - loss: 1.4028 - val_loss: 1.4029 - 13s/epoch - 132us/sample
Epoch 32/140
95501/95501 - 13s - loss: 1.3994 - val_loss: 1.4025 - 13s/epoch - 132us/sample
Epoch 33/140
95501/95501 - 13s - loss: 1.3990 - val_loss: 1.4017 - 13s/epoch - 133us/sample
Epoch 34/140
95501/95501 - 13s - loss: 1.4003 - val_loss: 1.3991 - 13s/epoch - 134us/sample
Epoch 35/140
95501/95501 - 13s - loss: 1.4008 - val_loss: 1.4021 - 13s/epoch - 133us/sample
Epoch 36/140
95501/95501 - 13s - loss: 1.3989 - val_loss: 1.4002 - 13s/epoch - 133us/sample
Epoch 37/140
95501/95501 - 13s - loss: 1.3985 - val_loss: 1.4005 - 13s/epoch - 133us/sample
Epoch 38/140
95501/95501 - 13s - loss: 1.3972 - val_loss: 1.3989 - 13s/epoch - 132us/sample
Epoch 39/140
95501/95501 - 13s - loss: 1.3978 - val_loss: 1.4006 - 13s/epoch - 133us/sample
Epoch 40/140
95501/95501 - 13s - loss: 1.3968 - val_loss: 1.3984 - 13s/epoch - 132us/sample
Epoch 41/140
95501/95501 - 13s - loss: 1.3966 - val_loss: 1.3985 - 13s/epoch - 134us/sample
Epoch 42/140
95501/95501 - 13s - loss: 1.3957 - val_loss: 1.3977 - 13s/epoch - 133us/sample
Epoch 43/140
95501/95501 - 13s - loss: 1.4034 - val_loss: 1.4014 - 13s/epoch - 132us/sample
Epoch 44/140
95501/95501 - 13s - loss: 1.3978 - val_loss: 1.3998 - 13s/epoch - 132us/sample
Epoch 45/140
95501/95501 - 13s - loss: 1.3968 - val_loss: 1.3974 - 13s/epoch - 132us/sample
Epoch 46/140
95501/95501 - 13s - loss: 1.3958 - val_loss: 1.3990 - 13s/epoch - 132us/sample
Epoch 47/140
95501/95501 - 13s - loss: 1.3960 - val_loss: 1.3985 - 13s/epoch - 132us/sample
Epoch 48/140
95501/95501 - 13s - loss: 1.3954 - val_loss: 1.3979 - 13s/epoch - 135us/sample
Epoch 49/140
95501/95501 - 13s - loss: 1.3969 - val_loss: 1.4030 - 13s/epoch - 132us/sample
Epoch 50/140
95501/95501 - 13s - loss: 1.3973 - val_loss: 1.3981 - 13s/epoch - 132us/sample
Epoch 51/140
95501/95501 - 13s - loss: 1.3960 - val_loss: 1.3981 - 13s/epoch - 132us/sample
Epoch 52/140
95501/95501 - 13s - loss: 1.3971 - val_loss: 1.3995 - 13s/epoch - 133us/sample
Epoch 53/140
95501/95501 - 13s - loss: 1.3966 - val_loss: 1.3981 - 13s/epoch - 132us/sample
Epoch 54/140
95501/95501 - 12s - loss: 1.4000 - val_loss: 1.4030 - 12s/epoch - 129us/sample
Epoch 55/140
95501/95501 - 12s - loss: 1.4004 - val_loss: 1.4009 - 12s/epoch - 122us/sample
Epoch 56/140
95501/95501 - 12s - loss: 1.3976 - val_loss: 1.3993 - 12s/epoch - 124us/sample
Epoch 57/140
95501/95501 - 12s - loss: 1.3983 - val_loss: 1.4007 - 12s/epoch - 127us/sample
Epoch 58/140
95501/95501 - 12s - loss: 1.3976 - val_loss: 1.3990 - 12s/epoch - 128us/sample
Epoch 59/140
95501/95501 - 12s - loss: 1.3957 - val_loss: 1.3976 - 12s/epoch - 129us/sample
Epoch 60/140
95501/95501 - 12s - loss: 1.3964 - val_loss: 1.3986 - 12s/epoch - 129us/sample
Epoch 61/140
95501/95501 - 12s - loss: 1.3961 - val_loss: 1.3983 - 12s/epoch - 130us/sample
Epoch 62/140
95501/95501 - 12s - loss: 1.3962 - val_loss: 1.3980 - 12s/epoch - 130us/sample
Epoch 63/140
95501/95501 - 12s - loss: 1.3949 - val_loss: 1.3992 - 12s/epoch - 128us/sample
Epoch 64/140
95501/95501 - 12s - loss: 1.3966 - val_loss: 1.3988 - 12s/epoch - 130us/sample
Epoch 65/140
95501/95501 - 12s - loss: 1.3970 - val_loss: 1.3994 - 12s/epoch - 129us/sample
Epoch 66/140
95501/95501 - 12s - loss: 1.3955 - val_loss: 1.3972 - 12s/epoch - 129us/sample
Epoch 67/140
95501/95501 - 12s - loss: 1.3957 - val_loss: 1.3972 - 12s/epoch - 129us/sample
Epoch 68/140
95501/95501 - 12s - loss: 1.3960 - val_loss: 1.3973 - 12s/epoch - 130us/sample
Epoch 69/140
95501/95501 - 12s - loss: 1.3951 - val_loss: 1.3983 - 12s/epoch - 129us/sample
Epoch 70/140
95501/95501 - 12s - loss: 1.3958 - val_loss: 1.3985 - 12s/epoch - 129us/sample
Epoch 71/140
95501/95501 - 12s - loss: 1.3962 - val_loss: 1.3981 - 12s/epoch - 129us/sample
Epoch 72/140
95501/95501 - 12s - loss: 1.3951 - val_loss: 1.3968 - 12s/epoch - 129us/sample
Epoch 73/140
95501/95501 - 12s - loss: 1.3944 - val_loss: 1.3967 - 12s/epoch - 128us/sample
Epoch 74/140
95501/95501 - 12s - loss: 1.3944 - val_loss: 1.3968 - 12s/epoch - 129us/sample
Epoch 75/140
95501/95501 - 12s - loss: 1.3946 - val_loss: 1.3967 - 12s/epoch - 131us/sample
Epoch 76/140
95501/95501 - 12s - loss: 1.3940 - val_loss: 1.3961 - 12s/epoch - 129us/sample
Epoch 77/140
95501/95501 - 12s - loss: 1.3962 - val_loss: 1.4022 - 12s/epoch - 129us/sample
Epoch 78/140
95501/95501 - 12s - loss: 1.3977 - val_loss: 1.3998 - 12s/epoch - 129us/sample
Epoch 79/140
95501/95501 - 12s - loss: 1.3955 - val_loss: 1.3972 - 12s/epoch - 128us/sample
Epoch 80/140
95501/95501 - 12s - loss: 1.3948 - val_loss: 1.3967 - 12s/epoch - 128us/sample
Epoch 81/140
95501/95501 - 12s - loss: 1.3941 - val_loss: 1.3961 - 12s/epoch - 129us/sample
Epoch 82/140
95501/95501 - 12s - loss: 1.3944 - val_loss: 1.3965 - 12s/epoch - 130us/sample
Epoch 83/140
95501/95501 - 12s - loss: 1.3969 - val_loss: 1.4007 - 12s/epoch - 128us/sample
Epoch 84/140
95501/95501 - 12s - loss: 1.3973 - val_loss: 1.3980 - 12s/epoch - 129us/sample
Epoch 85/140
95501/95501 - 12s - loss: 1.3959 - val_loss: 1.3978 - 12s/epoch - 129us/sample
Epoch 86/140
95501/95501 - 12s - loss: 1.3969 - val_loss: 1.3985 - 12s/epoch - 129us/sample
Epoch 87/140
95501/95501 - 12s - loss: 1.3959 - val_loss: 1.3974 - 12s/epoch - 129us/sample
Epoch 88/140
95501/95501 - 12s - loss: 1.3949 - val_loss: 1.3967 - 12s/epoch - 129us/sample
Epoch 89/140
95501/95501 - 12s - loss: 1.3945 - val_loss: 1.3974 - 12s/epoch - 130us/sample
Epoch 90/140
95501/95501 - 12s - loss: 1.3959 - val_loss: 1.3980 - 12s/epoch - 129us/sample
Epoch 91/140
95501/95501 - 12s - loss: 1.3979 - val_loss: 1.4021 - 12s/epoch - 129us/sample
Epoch 92/140
95501/95501 - 12s - loss: 1.3978 - val_loss: 1.3989 - 12s/epoch - 129us/sample
Epoch 93/140
95501/95501 - 12s - loss: 1.3964 - val_loss: 1.3976 - 12s/epoch - 129us/sample
Epoch 94/140
95501/95501 - 12s - loss: 1.3958 - val_loss: 1.3972 - 12s/epoch - 129us/sample
Epoch 95/140
95501/95501 - 12s - loss: 1.3945 - val_loss: 1.3965 - 12s/epoch - 129us/sample
Epoch 96/140
95501/95501 - 12s - loss: 1.3946 - val_loss: 1.3964 - 12s/epoch - 130us/sample
Epoch 97/140
95501/95501 - 12s - loss: 1.3947 - val_loss: 1.3969 - 12s/epoch - 129us/sample
Epoch 98/140
95501/95501 - 12s - loss: 1.3952 - val_loss: 1.3980 - 12s/epoch - 128us/sample
Epoch 99/140
95501/95501 - 12s - loss: 1.3951 - val_loss: 1.3974 - 12s/epoch - 129us/sample
Epoch 100/140
95501/95501 - 12s - loss: 1.3946 - val_loss: 1.3968 - 12s/epoch - 129us/sample
Epoch 101/140
95501/95501 - 12s - loss: 1.3948 - val_loss: 1.3965 - 12s/epoch - 128us/sample
Epoch 102/140
95501/95501 - 12s - loss: 1.3944 - val_loss: 1.3971 - 12s/epoch - 130us/sample
Epoch 103/140
95501/95501 - 12s - loss: 1.3961 - val_loss: 1.3989 - 12s/epoch - 128us/sample
Epoch 104/140
95501/95501 - 12s - loss: 1.3968 - val_loss: 1.3996 - 12s/epoch - 129us/sample
Epoch 105/140
95501/95501 - 12s - loss: 1.3950 - val_loss: 1.3966 - 12s/epoch - 129us/sample
Epoch 106/140
95501/95501 - 12s - loss: 1.3965 - val_loss: 1.4011 - 12s/epoch - 129us/sample
Epoch 107/140
95501/95501 - 12s - loss: 1.3960 - val_loss: 1.3970 - 12s/epoch - 129us/sample
Epoch 108/140
95501/95501 - 12s - loss: 1.3952 - val_loss: 1.3977 - 12s/epoch - 129us/sample
Epoch 109/140
95501/95501 - 12s - loss: 1.3961 - val_loss: 1.3986 - 12s/epoch - 130us/sample
Epoch 110/140
95501/95501 - 12s - loss: 1.3955 - val_loss: 1.3969 - 12s/epoch - 129us/sample
Epoch 111/140
95501/95501 - 12s - loss: 1.3951 - val_loss: 1.3970 - 12s/epoch - 129us/sample
Epoch 112/140
95501/95501 - 12s - loss: 1.3937 - val_loss: 1.3958 - 12s/epoch - 129us/sample
Epoch 113/140
95501/95501 - 12s - loss: 1.3942 - val_loss: 1.3962 - 12s/epoch - 129us/sample
Epoch 114/140
95501/95501 - 12s - loss: 1.3947 - val_loss: 1.3971 - 12s/epoch - 129us/sample
Epoch 115/140
95501/95501 - 12s - loss: 1.3947 - val_loss: 1.3969 - 12s/epoch - 129us/sample
Epoch 116/140
95501/95501 - 12s - loss: 1.3947 - val_loss: 1.3971 - 12s/epoch - 130us/sample
Epoch 117/140
95501/95501 - 12s - loss: 1.3948 - val_loss: 1.3967 - 12s/epoch - 129us/sample
Epoch 118/140
95501/95501 - 12s - loss: 1.3946 - val_loss: 1.3962 - 12s/epoch - 128us/sample
Epoch 119/140
95501/95501 - 12s - loss: 1.3946 - val_loss: 1.3990 - 12s/epoch - 129us/sample
Epoch 120/140
95501/95501 - 12s - loss: 1.3957 - val_loss: 1.3977 - 12s/epoch - 129us/sample
Epoch 121/140
95501/95501 - 12s - loss: 1.3947 - val_loss: 1.3963 - 12s/epoch - 129us/sample
Epoch 122/140
95501/95501 - 12s - loss: 1.3945 - val_loss: 1.3958 - 12s/epoch - 129us/sample
Epoch 123/140
95501/95501 - 12s - loss: 1.3940 - val_loss: 1.3962 - 12s/epoch - 131us/sample
Epoch 124/140
95501/95501 - 12s - loss: 1.3941 - val_loss: 1.3967 - 12s/epoch - 128us/sample
Epoch 125/140
95501/95501 - 12s - loss: 1.3936 - val_loss: 1.3961 - 12s/epoch - 129us/sample
Epoch 126/140
95501/95501 - 12s - loss: 1.3941 - val_loss: 1.3960 - 12s/epoch - 129us/sample
Epoch 127/140
95501/95501 - 12s - loss: 1.3948 - val_loss: 1.3970 - 12s/epoch - 130us/sample
Epoch 128/140
95501/95501 - 12s - loss: 1.3962 - val_loss: 1.3999 - 12s/epoch - 129us/sample
Epoch 129/140
95501/95501 - 12s - loss: 1.3966 - val_loss: 1.3978 - 12s/epoch - 129us/sample
Epoch 130/140
95501/95501 - 12s - loss: 1.3947 - val_loss: 1.3963 - 12s/epoch - 131us/sample
Epoch 131/140
95501/95501 - 12s - loss: 1.3943 - val_loss: 1.3969 - 12s/epoch - 129us/sample
Epoch 132/140
95501/95501 - 12s - loss: 1.3951 - val_loss: 1.3969 - 12s/epoch - 128us/sample
Epoch 133/140
95501/95501 - 12s - loss: 1.3943 - val_loss: 1.3971 - 12s/epoch - 129us/sample
Epoch 134/140
95501/95501 - 12s - loss: 1.3947 - val_loss: 1.3969 - 12s/epoch - 129us/sample
Epoch 135/140
95501/95501 - 12s - loss: 1.3941 - val_loss: 1.3963 - 12s/epoch - 129us/sample
Epoch 136/140
95501/95501 - 12s - loss: 1.3939 - val_loss: 1.3962 - 12s/epoch - 130us/sample
Epoch 137/140
95501/95501 - 12s - loss: 1.3938 - val_loss: 1.4002 - 12s/epoch - 130us/sample
Epoch 138/140
95501/95501 - 12s - loss: 1.3957 - val_loss: 1.3975 - 12s/epoch - 129us/sample
Epoch 139/140
95501/95501 - 12s - loss: 1.3945 - val_loss: 1.3965 - 12s/epoch - 128us/sample
Epoch 140/140
95501/95501 - 12s - loss: 1.3942 - val_loss: 1.3963 - 12s/epoch - 129us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 1.396294978084629
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:31:11.924128: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_33/outputlayer/BiasAdd' id:42604 op device:{requested: '', assigned: ''} def:{{{node decoder_model_33/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_33/outputlayer/MatMul, decoder_model_33/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7245204025195666
cosine 0.8626086328131262
MAE: 7.5059466
RMSE: 19.797098
r2: -25421.475888960555
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'binary_crossentropy', 64, 140, 0.0012, 0.3, 379, 1.394218836589276, 1.396294978084629, 0.7245204025195666, 0.8626086328131262, 7.505946636199951, 19.79709815979004, -25421.475888960555, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 145 0.0012 8 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_102 (Batch  (None, 1896)        7584        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_102 (ReLU)               (None, 1896)         0           ['batch_normalization_102[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          718963      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          718963      ['re_lu_102[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3271408     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,115,358
Trainable params: 7,107,016
Non-trainable params: 8,342
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 08:31:29.149705: W tensorflow/c/c_api.cc:291] Operation '{name:'outputlayer_34/kernel/Assign' id:43853 op device:{requested: '', assigned: ''} def:{{{node outputlayer_34/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](outputlayer_34/kernel, outputlayer_34/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:32:26.467441: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_34/mul' id:43976 op device:{requested: '', assigned: ''} def:{{{node loss_34/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_34/mul/x, loss_34/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 66s - loss: 0.0105 - val_loss: 0.0075 - 66s/epoch - 690us/sample
Epoch 2/145
95501/95501 - 57s - loss: 0.0054 - val_loss: 0.0066 - 57s/epoch - 592us/sample
Epoch 3/145
95501/95501 - 56s - loss: 0.0045 - val_loss: 0.0074 - 56s/epoch - 591us/sample
Epoch 4/145
95501/95501 - 57s - loss: 0.0041 - val_loss: 0.0062 - 57s/epoch - 593us/sample
Epoch 5/145
95501/95501 - 56s - loss: 0.0038 - val_loss: 0.0054 - 56s/epoch - 591us/sample
Epoch 6/145
95501/95501 - 57s - loss: 0.0036 - val_loss: 0.0047 - 57s/epoch - 592us/sample
Epoch 7/145
95501/95501 - 57s - loss: 0.0035 - val_loss: 0.0044 - 57s/epoch - 593us/sample
Epoch 8/145
95501/95501 - 56s - loss: 0.0034 - val_loss: 0.0037 - 56s/epoch - 590us/sample
Epoch 9/145
95501/95501 - 57s - loss: 0.0033 - val_loss: 0.0039 - 57s/epoch - 592us/sample
Epoch 10/145
95501/95501 - 57s - loss: 0.0032 - val_loss: 0.0036 - 57s/epoch - 592us/sample
Epoch 11/145
95501/95501 - 57s - loss: 0.0032 - val_loss: 0.0034 - 57s/epoch - 593us/sample
Epoch 12/145
95501/95501 - 57s - loss: 0.0031 - val_loss: 0.0036 - 57s/epoch - 592us/sample
Epoch 13/145
95501/95501 - 56s - loss: 0.0031 - val_loss: 0.0034 - 56s/epoch - 591us/sample
Epoch 14/145
95501/95501 - 56s - loss: 0.0031 - val_loss: 0.0033 - 56s/epoch - 591us/sample
Epoch 15/145
95501/95501 - 57s - loss: 0.0030 - val_loss: 0.0033 - 57s/epoch - 593us/sample
Epoch 16/145
95501/95501 - 56s - loss: 0.0030 - val_loss: 0.0035 - 56s/epoch - 591us/sample
Epoch 17/145
95501/95501 - 56s - loss: 0.0030 - val_loss: 0.0033 - 56s/epoch - 591us/sample
Epoch 18/145
95501/95501 - 57s - loss: 0.0030 - val_loss: 0.0031 - 57s/epoch - 592us/sample
Epoch 19/145
95501/95501 - 57s - loss: 0.0029 - val_loss: 0.0029 - 57s/epoch - 595us/sample
Epoch 20/145
95501/95501 - 57s - loss: 0.0029 - val_loss: 0.0029 - 57s/epoch - 592us/sample
Epoch 21/145
95501/95501 - 56s - loss: 0.0029 - val_loss: 0.0029 - 56s/epoch - 587us/sample
Epoch 22/145
95501/95501 - 57s - loss: 0.0029 - val_loss: 0.0030 - 57s/epoch - 593us/sample
Epoch 23/145
95501/95501 - 57s - loss: 0.0029 - val_loss: 0.0029 - 57s/epoch - 592us/sample
Epoch 24/145
95501/95501 - 57s - loss: 0.0028 - val_loss: 0.0030 - 57s/epoch - 592us/sample
Epoch 25/145
95501/95501 - 57s - loss: 0.0029 - val_loss: 0.0030 - 57s/epoch - 593us/sample
Epoch 26/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0027 - 56s/epoch - 590us/sample
Epoch 27/145
95501/95501 - 57s - loss: 0.0028 - val_loss: 0.0026 - 57s/epoch - 594us/sample
Epoch 28/145
95501/95501 - 57s - loss: 0.0028 - val_loss: 0.0028 - 57s/epoch - 592us/sample
Epoch 29/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0029 - 56s/epoch - 590us/sample
Epoch 30/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0028 - 56s/epoch - 591us/sample
Epoch 31/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0029 - 56s/epoch - 592us/sample
Epoch 32/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0028 - 56s/epoch - 591us/sample
Epoch 33/145
95501/95501 - 57s - loss: 0.0028 - val_loss: 0.0028 - 57s/epoch - 593us/sample
Epoch 34/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0030 - 56s/epoch - 592us/sample
Epoch 35/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0028 - 56s/epoch - 591us/sample
Epoch 36/145
95501/95501 - 56s - loss: 0.0028 - val_loss: 0.0030 - 56s/epoch - 592us/sample
Epoch 37/145
95501/95501 - 57s - loss: 0.0027 - val_loss: 0.0031 - 57s/epoch - 592us/sample
Epoch 38/145
95501/95501 - 57s - loss: 0.0027 - val_loss: 0.0031 - 57s/epoch - 592us/sample
Epoch 39/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0028 - 56s/epoch - 592us/sample
Epoch 40/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 41/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 42/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 43/145
95501/95501 - 57s - loss: 0.0027 - val_loss: 0.0029 - 57s/epoch - 593us/sample
Epoch 44/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 45/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0030 - 56s/epoch - 590us/sample
Epoch 46/145
95501/95501 - 57s - loss: 0.0027 - val_loss: 0.0029 - 57s/epoch - 593us/sample
Epoch 47/145
95501/95501 - 57s - loss: 0.0027 - val_loss: 0.0029 - 57s/epoch - 594us/sample
Epoch 48/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0029 - 56s/epoch - 589us/sample
Epoch 49/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0031 - 56s/epoch - 591us/sample
Epoch 50/145
95501/95501 - 57s - loss: 0.0027 - val_loss: 0.0029 - 57s/epoch - 593us/sample
Epoch 51/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0027 - 56s/epoch - 591us/sample
Epoch 52/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0027 - 56s/epoch - 591us/sample
Epoch 53/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0029 - 56s/epoch - 590us/sample
Epoch 54/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0029 - 56s/epoch - 592us/sample
Epoch 55/145
95501/95501 - 56s - loss: 0.0027 - val_loss: 0.0027 - 56s/epoch - 591us/sample
Epoch 56/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0028 - 56s/epoch - 590us/sample
Epoch 57/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0029 - 56s/epoch - 592us/sample
Epoch 58/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 592us/sample
Epoch 59/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 60/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 592us/sample
Epoch 61/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0030 - 56s/epoch - 591us/sample
Epoch 62/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0031 - 56s/epoch - 587us/sample
Epoch 63/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0030 - 57s/epoch - 599us/sample
Epoch 64/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 596us/sample
Epoch 65/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0030 - 57s/epoch - 597us/sample
Epoch 66/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0030 - 57s/epoch - 596us/sample
Epoch 67/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0027 - 57s/epoch - 597us/sample
Epoch 68/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 598us/sample
Epoch 69/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 598us/sample
Epoch 70/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 595us/sample
Epoch 71/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 598us/sample
Epoch 72/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 597us/sample
Epoch 73/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 596us/sample
Epoch 74/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0030 - 57s/epoch - 598us/sample
Epoch 75/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 598us/sample
Epoch 76/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0026 - 57s/epoch - 596us/sample
Epoch 77/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 597us/sample
Epoch 78/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0027 - 57s/epoch - 599us/sample
Epoch 79/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0030 - 57s/epoch - 596us/sample
Epoch 80/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0033 - 57s/epoch - 597us/sample
Epoch 81/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0030 - 57s/epoch - 596us/sample
Epoch 82/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0032 - 57s/epoch - 597us/sample
Epoch 83/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0028 - 57s/epoch - 599us/sample
Epoch 84/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 597us/sample
Epoch 85/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 597us/sample
Epoch 86/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0028 - 57s/epoch - 598us/sample
Epoch 87/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0029 - 57s/epoch - 596us/sample
Epoch 88/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0027 - 56s/epoch - 589us/sample
Epoch 89/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 593us/sample
Epoch 90/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0027 - 57s/epoch - 595us/sample
Epoch 91/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 592us/sample
Epoch 92/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0027 - 57s/epoch - 593us/sample
Epoch 93/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 595us/sample
Epoch 94/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0035 - 57s/epoch - 594us/sample
Epoch 95/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0029 - 57s/epoch - 595us/sample
Epoch 96/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0029 - 57s/epoch - 594us/sample
Epoch 97/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 98/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0026 - 57s/epoch - 593us/sample
Epoch 99/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0031 - 57s/epoch - 593us/sample
Epoch 100/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0028 - 56s/epoch - 590us/sample
Epoch 101/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0031 - 57s/epoch - 592us/sample
Epoch 102/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0029 - 57s/epoch - 593us/sample
Epoch 103/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0032 - 57s/epoch - 592us/sample
Epoch 104/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0026 - 56s/epoch - 590us/sample
Epoch 105/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0029 - 57s/epoch - 593us/sample
Epoch 106/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0031 - 57s/epoch - 592us/sample
Epoch 107/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0029 - 57s/epoch - 594us/sample
Epoch 108/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0030 - 57s/epoch - 592us/sample
Epoch 109/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0028 - 56s/epoch - 590us/sample
Epoch 110/145
95501/95501 - 57s - loss: 0.0026 - val_loss: 0.0028 - 57s/epoch - 592us/sample
Epoch 111/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0027 - 57s/epoch - 593us/sample
Epoch 112/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0028 - 56s/epoch - 591us/sample
Epoch 113/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0030 - 57s/epoch - 593us/sample
Epoch 114/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0033 - 57s/epoch - 594us/sample
Epoch 115/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0032 - 57s/epoch - 593us/sample
Epoch 116/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 117/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0030 - 57s/epoch - 592us/sample
Epoch 118/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0031 - 57s/epoch - 592us/sample
Epoch 119/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0029 - 56s/epoch - 591us/sample
Epoch 120/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0031 - 57s/epoch - 592us/sample
Epoch 121/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0030 - 57s/epoch - 592us/sample
Epoch 122/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0028 - 57s/epoch - 593us/sample
Epoch 123/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0029 - 57s/epoch - 593us/sample
Epoch 124/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0028 - 57s/epoch - 593us/sample
Epoch 125/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0028 - 57s/epoch - 593us/sample
Epoch 126/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0028 - 57s/epoch - 592us/sample
Epoch 127/145
95501/95501 - 57s - loss: 0.0025 - val_loss: 0.0028 - 57s/epoch - 593us/sample
Epoch 128/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0029 - 56s/epoch - 590us/sample
Epoch 129/145
95501/95501 - 56s - loss: 0.0026 - val_loss: 0.0027 - 56s/epoch - 585us/sample
Epoch 130/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0029 - 56s/epoch - 585us/sample
Epoch 131/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0027 - 56s/epoch - 587us/sample
Epoch 132/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0028 - 56s/epoch - 585us/sample
Epoch 133/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0029 - 56s/epoch - 585us/sample
Epoch 134/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0032 - 56s/epoch - 586us/sample
Epoch 135/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0027 - 56s/epoch - 586us/sample
Epoch 136/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0027 - 56s/epoch - 584us/sample
Epoch 137/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0029 - 56s/epoch - 585us/sample
Epoch 138/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0026 - 56s/epoch - 586us/sample
Epoch 139/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0026 - 56s/epoch - 585us/sample
Epoch 140/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0026 - 56s/epoch - 584us/sample
Epoch 141/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0031 - 56s/epoch - 584us/sample
Epoch 142/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0027 - 56s/epoch - 584us/sample
Epoch 143/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0027 - 56s/epoch - 587us/sample
Epoch 144/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0028 - 56s/epoch - 585us/sample
Epoch 145/145
95501/95501 - 56s - loss: 0.0025 - val_loss: 0.0026 - 56s/epoch - 583us/sample
COMPRESSED VECTOR SIZE: 379
Loss in the autoencoder: 0.002649142498328097
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:48:13.673630: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_34/outputlayer/BiasAdd' id:43947 op device:{requested: '', assigned: ''} def:{{{node decoder_model_34/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_34/outputlayer/MatMul, decoder_model_34/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.023875655462460565
cosine 0.019202107310160303
MAE: 0.022977242
RMSE: 0.047066532
r2: 0.8562909497094717
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 8, 145, 0.0012, 0.3, 379, 0.0024553855984592557, 0.002649142498328097, 0.023875655462460565, 0.019202107310160303, 0.02297724224627018, 0.04706653207540512, 0.8562909497094717, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 140 0.0012 8 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_105 (Batch  (None, 2022)        8088        ['dense_enc0[0][0]']             
 Normalization)                                                                                   
                                                                                                  
 re_lu_105 (ReLU)               (None, 2022)         0           ['batch_normalization_105[0][0]']
                                                                                                  
 bottleneck_zmean (Dense)       (None, 379)          766717      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck_zlog (Dense)        (None, 379)          766717      ['re_lu_105[0][0]']              
                                                                                                  
 bottleneck (Lambda)            (None, 379)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3479056     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 7,578,408
Trainable params: 7,569,562
Non-trainable params: 8,846
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-15 10:48:30.837420: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_106/moving_variance/Assign' id:44977 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_106/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_106/moving_variance, batch_normalization_106/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:49:26.781238: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_35/mul' id:45237 op device:{requested: '', assigned: ''} def:{{{node loss_35/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_35/mul/x, loss_35/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 64s - loss: 0.0173 - val_loss: 0.0118 - 64s/epoch - 675us/sample
Epoch 2/140
95501/95501 - 56s - loss: 0.0059 - val_loss: 0.0197 - 56s/epoch - 582us/sample
Epoch 3/140
95501/95501 - 56s - loss: 0.0047 - val_loss: 0.0210 - 56s/epoch - 583us/sample
Epoch 4/140
95501/95501 - 56s - loss: 0.0043 - val_loss: 0.0251 - 56s/epoch - 581us/sample
Epoch 5/140
95501/95501 - 55s - loss: 0.0040 - val_loss: 0.0228 - 55s/epoch - 580us/sample
Epoch 6/140
95501/95501 - 56s - loss: 0.0038 - val_loss: 0.0238 - 56s/epoch - 582us/sample
Epoch 7/140
95501/95501 - 55s - loss: 0.0037 - val_loss: 0.0243 - 55s/epoch - 581us/sample
Epoch 8/140
95501/95501 - 55s - loss: 0.0036 - val_loss: 0.0254 - 55s/epoch - 580us/sample
Epoch 9/140
95501/95501 - 56s - loss: 0.0035 - val_loss: 0.0272 - 56s/epoch - 581us/sample
Epoch 10/140
95501/95501 - 55s - loss: 0.0035 - val_loss: 0.0290 - 55s/epoch - 579us/sample
Epoch 11/140
95501/95501 - 55s - loss: 0.0034 - val_loss: 0.0291 - 55s/epoch - 580us/sample
Epoch 12/140
95501/95501 - 56s - loss: 0.0034 - val_loss: 0.0300 - 56s/epoch - 583us/sample
Epoch 13/140
95501/95501 - 55s - loss: 0.0033 - val_loss: 0.0303 - 55s/epoch - 580us/sample
Epoch 14/140
95501/95501 - 55s - loss: 0.0033 - val_loss: 0.0280 - 55s/epoch - 579us/sample
Epoch 15/140
95501/95501 - 56s - loss: 0.0033 - val_loss: 0.0292 - 56s/epoch - 581us/sample
Epoch 16/140
95501/95501 - 55s - loss: 0.0032 - val_loss: 0.0300 - 55s/epoch - 580us/sample
Epoch 17/140
95501/95501 - 55s - loss: 0.0032 - val_loss: 0.0343 - 55s/epoch - 581us/sample
Epoch 18/140
95501/95501 - 55s - loss: 0.0032 - val_loss: 0.0328 - 55s/epoch - 581us/sample
Epoch 19/140
slurmstepd: error: *** JOB 36005605 ON mb-cas001 CANCELLED AT 2023-02-15T11:05:30 ***
