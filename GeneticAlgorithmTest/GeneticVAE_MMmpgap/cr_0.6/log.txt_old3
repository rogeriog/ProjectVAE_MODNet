start
Sat Feb 18 16:13:56 CET 2023
2023-02-18 16:13:57.880616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-18 16:14:49,050 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f97e710ffd0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
[1.4 145 0.0012000000000000001 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-18 16:14:57.090924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-18 16:14:57.606289: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-18 16:14:57.608220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22291 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.4_cr0.6_bs256_ep145_loss_mse_lr0.0012000000000000001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-18 16:15:02.783503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
  1/332 [..............................] - ETA: 17:57 26/332 [=>............................] - ETA: 0s    52/332 [===>..........................] - ETA: 0s 78/332 [======>.......................] - ETA: 0s104/332 [========>.....................] - ETA: 0s130/332 [==========>...................] - ETA: 0s157/332 [=============>................] - ETA: 0s184/332 [===============>..............] - ETA: 0s211/332 [==================>...........] - ETA: 0s238/332 [====================>.........] - ETA: 0s265/332 [======================>.......] - ETA: 0s292/332 [=========================>....] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 4s 2ms/step
correlation 0.08028067742771712
cosine 0.06298943649158809
MAE: 0.037517462
RMSE: 0.08126762
r2: 0.571555177554589
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.4custom_VAE', 'mse', 256, 145, 0.0012000000000000001, 0.6, 758, '--', '--', 0.08028067742771712, 0.06298943649158809, 0.03751746192574501, 0.08126761764287949, 0.571555177554589, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.0_cr0.6_bs128_ep85_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 37s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s190/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12720572344096615
cosine 0.09957187551734045
MAE: 0.047598958
RMSE: 0.1013092
r2: 0.33417857864812706
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.0custom_VAE', 'logcosh', 128, 85, 0.001, 0.6, 758, '--', '--', 0.12720572344096615, 0.09957187551734045, 0.047598958015441895, 0.10130920261144638, 0.33417857864812706, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 180 0.0005 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2107998     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2107998     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6214678     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,958,494
Trainable params: 13,945,858
Non-trainable params: 12,636
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.2_cr0.6_bs256_ep180_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 37s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s153/332 [============>.................] - ETA: 0s192/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08452188742304585
cosine 0.0663346121817142
MAE: 0.038724527
RMSE: 0.083297946
r2: 0.549880585334648
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.2custom_VAE', 'mse', 256, 180, 0.0005, 0.6, 758, '--', '--', 0.08452188742304585, 0.0663346121817142, 0.03872452676296234, 0.08329794555902481, 0.549880585334648, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 145 0.0012000000000000001 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.2999999999999998_cr0.6_bs256_ep145_loss_mse_lr0.0012000000000000001_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 39s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s115/332 [=========>....................] - ETA: 0s154/332 [============>.................] - ETA: 0s193/332 [================>.............] - ETA: 0s232/332 [===================>..........] - ETA: 0s271/332 [=======================>......] - ETA: 0s309/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08235826149240243
cosine 0.06462087305568094
MAE: 0.03765866
RMSE: 0.08231454
r2: 0.560445121383517
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.2999999999999998custom_VAE', 'mse', 256, 145, 0.0012000000000000001, 0.6, 758, '--', '--', 0.08235826149240243, 0.06462087305568094, 0.03765866160392761, 0.08231454342603683, 0.560445121383517, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 145 0.001 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.2999999999999998_cr0.6_bs128_ep145_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 37s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s116/332 [=========>....................] - ETA: 0s155/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s233/332 [====================>.........] - ETA: 0s272/332 [=======================>......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08109587509588076
cosine 0.06361371143399838
MAE: 0.037165698
RMSE: 0.081699364
r2: 0.5669904762673048
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.2999999999999998custom_VAE', 'mse', 128, 145, 0.001, 0.6, 758, '--', '--', 0.08109587509588076, 0.06361371143399838, 0.037165697664022446, 0.08169936388731003, 0.5669904762673048, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 128 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5959276     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,352,182
Trainable params: 13,340,050
Non-trainable params: 12,132
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.1_cr0.6_bs128_ep90_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 37s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s188/332 [===============>..............] - ETA: 0s227/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12612883016450244
cosine 0.09871297485528242
MAE: 0.047101427
RMSE: 0.100977406
r2: 0.33853259980253075
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.1custom_VAE', 'logcosh', 128, 90, 0.001, 0.6, 758, '--', '--', 0.12612883016450244, 0.09871297485528242, 0.047101426869630814, 0.10097740590572357, 0.33853259980253075, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0005 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.0_cr0.6_bs256_ep85_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 37s 36/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s112/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s188/332 [===============>..............] - ETA: 0s226/332 [===================>..........] - ETA: 0s264/332 [======================>.......] - ETA: 0s302/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.10376827196943683
cosine 0.08133096485779594
MAE: 0.043084253
RMSE: 0.091946915
r2: 0.45155357032864785
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.0custom_VAE', 'mse', 256, 85, 0.0005, 0.6, 758, '--', '--', 0.10376827196943683, 0.08133096485779594, 0.04308425262570381, 0.09194691479206085, 0.45155357032864785, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.001 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.6_cr0.6_bs256_ep145_loss_mse_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 37s 36/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s113/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s190/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0865506044916508
cosine 0.06788466810124172
MAE: 0.039385
RMSE: 0.08430672
r2: 0.5389116613588023
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.6custom_VAE', 'mse', 256, 145, 0.001, 0.6, 758, '--', '--', 0.0865506044916508, 0.06788466810124172, 0.03938499838113785, 0.08430671691894531, 0.5389116613588023, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 145 0.002 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/145
2023-02-18 16:16:13.278706: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f949004d4f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-18 16:16:13.278752: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2023-02-18 16:16:13.498328: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-18 16:16:15.595561: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1493/1493 - 16s - loss: 0.0250 - val_loss: 0.0176 - 16s/epoch - 10ms/step
Epoch 2/145
1493/1493 - 7s - loss: 0.0150 - val_loss: 0.0140 - 7s/epoch - 5ms/step
Epoch 3/145
1493/1493 - 7s - loss: 0.0135 - val_loss: 0.0129 - 7s/epoch - 5ms/step
Epoch 4/145
1493/1493 - 7s - loss: 0.0128 - val_loss: 0.0125 - 7s/epoch - 5ms/step
Epoch 5/145
1493/1493 - 7s - loss: 0.0126 - val_loss: 0.0123 - 7s/epoch - 5ms/step
Epoch 6/145
1493/1493 - 7s - loss: 0.0124 - val_loss: 0.0121 - 7s/epoch - 5ms/step
Epoch 7/145
1493/1493 - 8s - loss: 0.0123 - val_loss: 0.0121 - 8s/epoch - 5ms/step
Epoch 8/145
1493/1493 - 8s - loss: 0.0122 - val_loss: 0.0120 - 8s/epoch - 5ms/step
Epoch 9/145
1493/1493 - 7s - loss: 0.0122 - val_loss: 0.0120 - 7s/epoch - 5ms/step
Epoch 10/145
1493/1493 - 7s - loss: 0.0121 - val_loss: 0.0119 - 7s/epoch - 5ms/step
Epoch 11/145
1493/1493 - 8s - loss: 0.0121 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 12/145
1493/1493 - 7s - loss: 0.0120 - val_loss: 0.0118 - 7s/epoch - 5ms/step
Epoch 13/145
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0116 - 8s/epoch - 5ms/step
Epoch 14/145
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0115 - 8s/epoch - 5ms/step
Epoch 15/145
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0113 - 8s/epoch - 5ms/step
Epoch 16/145
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 17/145
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 18/145
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 19/145
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 20/145
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 21/145
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 22/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 23/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 24/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 25/145
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0109 - 7s/epoch - 5ms/step
Epoch 26/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 27/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 28/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 29/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 30/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 31/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 32/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 33/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 34/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 35/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 36/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 37/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 38/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 39/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 40/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 41/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 42/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 43/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 44/145
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 45/145
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 46/145
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 47/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 48/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 49/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 50/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 51/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 52/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 53/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 54/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 55/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 56/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 57/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 58/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 59/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 60/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 61/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 62/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 63/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 64/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 65/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 66/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 67/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 68/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 69/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 70/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 71/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 72/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 73/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 74/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 75/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 76/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 77/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 78/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 79/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 80/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 81/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 82/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 83/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 84/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 85/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 86/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 87/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 88/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 89/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 90/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 91/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 92/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 93/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 94/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 95/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 100/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 101/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 102/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 103/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 104/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 105/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 110/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 111/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 112/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 113/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 114/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 115/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 116/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 117/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 118/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 119/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 120/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 121/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 122/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 123/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 124/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 125/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 126/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 127/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 128/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 129/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 130/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 131/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 132/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 133/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 134/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 135/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 136/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 141/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 142/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 143/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 144/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 145/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009662165306508541
  1/332 [..............................] - ETA: 52s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s223/332 [===================>..........] - ETA: 0s260/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07991631593045495
cosine 0.06270079916821217
MAE: 0.03717615
RMSE: 0.0812674
r2: 0.5715572743091674
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'mse', 64, 145, 0.002, 0.6, 758, 0.00987617950886488, 0.009662165306508541, 0.07991631593045495, 0.06270079916821217, 0.03717615082859993, 0.08126740157604218, 0.5715572743091674, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 180 0.002 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 8s - loss: 0.0511 - val_loss: 0.0902 - 8s/epoch - 20ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0190 - val_loss: 0.0884 - 2s/epoch - 6ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0172 - val_loss: 0.0177 - 2s/epoch - 6ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0193 - val_loss: 0.0611 - 2s/epoch - 6ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0153 - val_loss: 0.0708 - 2s/epoch - 6ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0149 - val_loss: 0.0624 - 2s/epoch - 6ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0140 - val_loss: 0.0145 - 2s/epoch - 7ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0304 - 2s/epoch - 6ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0261 - 2s/epoch - 6ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0129 - val_loss: 0.1017 - 2s/epoch - 6ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0147 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0142 - 2s/epoch - 7ms/step
Epoch 15/180
374/374 - 3s - loss: 0.0140 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 16/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0122 - 3s/epoch - 8ms/step
Epoch 17/180
374/374 - 3s - loss: 0.0122 - val_loss: 0.0113 - 3s/epoch - 8ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0120 - 2s/epoch - 7ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0112 - 2s/epoch - 7ms/step
Epoch 21/180
374/374 - 3s - loss: 0.0113 - val_loss: 0.0112 - 3s/epoch - 8ms/step
Epoch 22/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 23/180
374/374 - 3s - loss: 0.0136 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 24/180
374/374 - 3s - loss: 0.0156 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 25/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 30/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0160 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0151 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 34/180
374/374 - 3s - loss: 0.0139 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 38/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 43/180
374/374 - 3s - loss: 0.0156 - val_loss: 0.0150 - 3s/epoch - 7ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0155 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 48/180
374/374 - 3s - loss: 0.0113 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0130 - 2s/epoch - 7ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0154 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0142 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0131 - 2s/epoch - 7ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 61/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 62/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 63/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 64/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 8ms/step
Epoch 65/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 8ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 67/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 68/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 69/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0108 - 3s/epoch - 8ms/step
Epoch 70/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0117 - 3s/epoch - 8ms/step
Epoch 71/180
374/374 - 3s - loss: 0.0122 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 72/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 73/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 74/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0118 - 3s/epoch - 8ms/step
Epoch 75/180
374/374 - 3s - loss: 0.0124 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 76/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 77/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 78/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 8ms/step
Epoch 79/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 80/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 81/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 82/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 8ms/step
Epoch 83/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 84/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 85/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 86/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 8ms/step
Epoch 87/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 88/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 89/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 90/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0112 - 3s/epoch - 8ms/step
Epoch 91/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 92/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 93/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 94/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 95/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 96/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 97/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 98/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 99/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 100/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 101/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 102/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 103/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 104/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 105/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 106/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 107/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 108/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 109/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 110/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 111/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0106 - 3s/epoch - 8ms/step
Epoch 112/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 113/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 114/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 8ms/step
Epoch 115/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 116/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 117/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 118/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 8ms/step
Epoch 119/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 120/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 121/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 122/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 123/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 124/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 125/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 126/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 127/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 128/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 129/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 8ms/step
Epoch 130/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 131/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 132/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 133/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 8ms/step
Epoch 134/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 135/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 136/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 137/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 8ms/step
Epoch 138/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 139/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 140/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 141/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 8ms/step
Epoch 142/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 143/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 144/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 145/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0098 - 3s/epoch - 8ms/step
Epoch 146/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 147/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 148/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 149/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 8ms/step
Epoch 150/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 151/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 152/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 153/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 154/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 155/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 156/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 157/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 158/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 159/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 160/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 8ms/step
Epoch 161/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 162/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 163/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 164/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 8ms/step
Epoch 165/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 166/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 167/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 168/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 8ms/step
Epoch 169/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 170/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 171/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 8ms/step
Epoch 172/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 173/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 174/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 175/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 8ms/step
Epoch 176/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 177/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 178/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 179/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 8ms/step
Epoch 180/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009474717080593109
  1/332 [..............................] - ETA: 51s 36/332 [==>...........................] - ETA: 0s  72/332 [=====>........................] - ETA: 0s106/332 [========>.....................] - ETA: 0s142/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s214/332 [==================>...........] - ETA: 0s250/332 [=====================>........] - ETA: 0s286/332 [========================>.....] - ETA: 0s322/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07685670654840214
cosine 0.06032495820158345
MAE: 0.036504842
RMSE: 0.07960594
r2: 0.5888970156449833
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 256, 180, 0.002, 0.6, 758, 0.009597770869731903, 0.009474717080593109, 0.07685670654840214, 0.06032495820158345, 0.0365048423409462, 0.07960593700408936, 0.5888970156449833, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 85 0.0007 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/85
374/374 - 8s - loss: 0.0385 - val_loss: 0.0226 - 8s/epoch - 23ms/step
Epoch 2/85
374/374 - 3s - loss: 0.0160 - val_loss: 0.0237 - 3s/epoch - 8ms/step
Epoch 3/85
374/374 - 3s - loss: 0.0154 - val_loss: 0.0176 - 3s/epoch - 7ms/step
Epoch 4/85
374/374 - 3s - loss: 0.0146 - val_loss: 0.0173 - 3s/epoch - 7ms/step
Epoch 5/85
374/374 - 3s - loss: 0.0143 - val_loss: 0.0280 - 3s/epoch - 7ms/step
Epoch 6/85
374/374 - 3s - loss: 0.0141 - val_loss: 0.0166 - 3s/epoch - 8ms/step
Epoch 7/85
374/374 - 3s - loss: 0.0137 - val_loss: 0.0198 - 3s/epoch - 7ms/step
Epoch 8/85
374/374 - 2s - loss: 0.0135 - val_loss: 0.0162 - 2s/epoch - 7ms/step
Epoch 9/85
374/374 - 3s - loss: 0.0133 - val_loss: 0.0192 - 3s/epoch - 7ms/step
Epoch 10/85
374/374 - 3s - loss: 0.0131 - val_loss: 0.0133 - 3s/epoch - 8ms/step
Epoch 11/85
374/374 - 3s - loss: 0.0129 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 12/85
374/374 - 2s - loss: 0.0127 - val_loss: 0.0130 - 2s/epoch - 7ms/step
Epoch 13/85
374/374 - 3s - loss: 0.0126 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 14/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0237 - 3s/epoch - 8ms/step
Epoch 15/85
374/374 - 2s - loss: 0.0138 - val_loss: 0.0129 - 2s/epoch - 7ms/step
Epoch 16/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 17/85
374/374 - 3s - loss: 0.0123 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 18/85
374/374 - 3s - loss: 0.0123 - val_loss: 0.0121 - 3s/epoch - 8ms/step
Epoch 19/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 20/85
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 7ms/step
Epoch 21/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 22/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 8ms/step
Epoch 23/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 24/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 25/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 26/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0119 - 3s/epoch - 8ms/step
Epoch 27/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 28/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 29/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 30/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 31/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 32/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 33/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 34/85
374/374 - 3s - loss: 0.0123 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 35/85
374/374 - 3s - loss: 0.0116 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 36/85
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 37/85
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 38/85
374/374 - 3s - loss: 0.0114 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 39/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 40/85
374/374 - 2s - loss: 0.0113 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 41/85
374/374 - 3s - loss: 0.0116 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 42/85
374/374 - 3s - loss: 0.0130 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 43/85
374/374 - 3s - loss: 0.0130 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 44/85
374/374 - 3s - loss: 0.0148 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 45/85
374/374 - 2s - loss: 0.0143 - val_loss: 0.0125 - 2s/epoch - 7ms/step
Epoch 46/85
374/374 - 3s - loss: 0.0124 - val_loss: 0.0116 - 3s/epoch - 8ms/step
Epoch 47/85
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 48/85
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 7ms/step
Epoch 49/85
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 7ms/step
Epoch 50/85
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 8ms/step
Epoch 51/85
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 52/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 53/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 54/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 8ms/step
Epoch 55/85
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 56/85
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 57/85
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 7ms/step
Epoch 58/85
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 8ms/step
Epoch 59/85
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 60/85
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 61/85
374/374 - 3s - loss: 0.0107 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 62/85
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 8ms/step
Epoch 63/85
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 64/85
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 65/85
374/374 - 3s - loss: 0.0106 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 66/85
374/374 - 3s - loss: 0.0108 - val_loss: 0.0105 - 3s/epoch - 8ms/step
Epoch 67/85
374/374 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 68/85
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 69/85
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 70/85
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 8ms/step
Epoch 71/85
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 72/85
374/374 - 3s - loss: 0.0104 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 73/85
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 74/85
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 8ms/step
Epoch 75/85
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 76/85
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 77/85
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 78/85
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 8ms/step
Epoch 79/85
374/374 - 3s - loss: 0.0102 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 80/85
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 81/85
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 82/85
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 83/85
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 84/85
374/374 - 3s - loss: 0.0102 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 85/85
374/374 - 3s - loss: 0.0104 - val_loss: 0.0100 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0100234504789114
  1/332 [..............................] - ETA: 45s 36/332 [==>...........................] - ETA: 0s  72/332 [=====>........................] - ETA: 0s109/332 [========>.....................] - ETA: 0s145/332 [============>.................] - ETA: 0s166/332 [==============>...............] - ETA: 0s202/332 [=================>............] - ETA: 0s239/332 [====================>.........] - ETA: 0s276/332 [=======================>......] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08866963001116239
cosine 0.0695534652408635
MAE: 0.039601963
RMSE: 0.08521921
r2: 0.5288768169178807
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 256, 85, 0.0007, 0.6, 758, 0.010422230698168278, 0.0100234504789114, 0.08866963001116239, 0.0695534652408635, 0.03960196301341057, 0.08521921187639236, 0.5288768169178807, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 80 0.00030000000000000003 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/80
747/747 - 10s - loss: 0.0338 - val_loss: 0.0210 - 10s/epoch - 14ms/step
Epoch 2/80
747/747 - 4s - loss: 0.0166 - val_loss: 0.1608 - 4s/epoch - 6ms/step
Epoch 3/80
747/747 - 5s - loss: 0.0166 - val_loss: 0.0256 - 5s/epoch - 6ms/step
Epoch 4/80
747/747 - 4s - loss: 0.0144 - val_loss: 0.0679 - 4s/epoch - 6ms/step
Epoch 5/80
747/747 - 4s - loss: 0.0139 - val_loss: 0.0144 - 4s/epoch - 6ms/step
Epoch 6/80
747/747 - 5s - loss: 0.0128 - val_loss: 0.0128 - 5s/epoch - 6ms/step
Epoch 7/80
747/747 - 4s - loss: 0.0122 - val_loss: 0.0119 - 4s/epoch - 6ms/step
Epoch 8/80
747/747 - 5s - loss: 0.0118 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 9/80
747/747 - 5s - loss: 0.0116 - val_loss: 0.0117 - 5s/epoch - 6ms/step
Epoch 10/80
747/747 - 4s - loss: 0.0117 - val_loss: 0.0124 - 4s/epoch - 6ms/step
Epoch 11/80
747/747 - 5s - loss: 0.0129 - val_loss: 0.0115 - 5s/epoch - 6ms/step
Epoch 12/80
747/747 - 4s - loss: 0.0115 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 13/80
747/747 - 5s - loss: 0.0114 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 14/80
747/747 - 5s - loss: 0.0115 - val_loss: 0.0120 - 5s/epoch - 6ms/step
Epoch 15/80
747/747 - 4s - loss: 0.0131 - val_loss: 0.0114 - 4s/epoch - 6ms/step
Epoch 16/80
747/747 - 5s - loss: 0.0117 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 17/80
747/747 - 5s - loss: 0.0113 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 18/80
747/747 - 5s - loss: 0.0112 - val_loss: 0.0114 - 5s/epoch - 6ms/step
Epoch 19/80
747/747 - 4s - loss: 0.0114 - val_loss: 0.0111 - 4s/epoch - 6ms/step
Epoch 20/80
747/747 - 5s - loss: 0.0112 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 21/80
747/747 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 22/80
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 23/80
747/747 - 4s - loss: 0.0112 - val_loss: 0.0119 - 4s/epoch - 6ms/step
Epoch 24/80
747/747 - 5s - loss: 0.0130 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 25/80
747/747 - 5s - loss: 0.0113 - val_loss: 0.0170 - 5s/epoch - 6ms/step
Epoch 26/80
747/747 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 27/80
747/747 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 28/80
747/747 - 4s - loss: 0.0112 - val_loss: 0.0125 - 4s/epoch - 6ms/step
Epoch 29/80
747/747 - 5s - loss: 0.0139 - val_loss: 0.0111 - 5s/epoch - 7ms/step
Epoch 30/80
747/747 - 4s - loss: 0.0113 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 31/80
747/747 - 5s - loss: 0.0114 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 32/80
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 33/80
747/747 - 5s - loss: 0.0110 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 34/80
747/747 - 5s - loss: 0.0114 - val_loss: 0.0119 - 5s/epoch - 6ms/step
Epoch 35/80
747/747 - 4s - loss: 0.0116 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 36/80
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 37/80
747/747 - 4s - loss: 0.0110 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 38/80
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 39/80
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 40/80
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 6ms/step
Epoch 41/80
747/747 - 5s - loss: 0.0109 - val_loss: 0.0115 - 5s/epoch - 6ms/step
Epoch 42/80
747/747 - 5s - loss: 0.0111 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 43/80
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 44/80
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 45/80
747/747 - 5s - loss: 0.0108 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 46/80
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 47/80
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 6ms/step
Epoch 48/80
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 49/80
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 50/80
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 51/80
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 52/80
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 53/80
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 54/80
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 55/80
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 56/80
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 57/80
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 58/80
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 59/80
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 60/80
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 61/80
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 62/80
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 63/80
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 64/80
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 65/80
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 66/80
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 67/80
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 68/80
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 7ms/step
Epoch 69/80
747/747 - 5s - loss: 0.0102 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 70/80
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 71/80
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 72/80
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 73/80
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 74/80
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 75/80
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 76/80
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 77/80
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 6ms/step
Epoch 78/80
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 79/80
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 80/80
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009956341236829758
  1/332 [..............................] - ETA: 49s 36/332 [==>...........................] - ETA: 0s  72/332 [=====>........................] - ETA: 0s108/332 [========>.....................] - ETA: 0s144/332 [============>.................] - ETA: 0s180/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s252/332 [=====================>........] - ETA: 0s288/332 [=========================>....] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08704714153539088
cosine 0.06829064757002555
MAE: 0.039212078
RMSE: 0.08456385
r2: 0.5360945851560256
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 128, 80, 0.00030000000000000003, 0.6, 758, 0.01011411938816309, 0.009956341236829758, 0.08704714153539088, 0.06829064757002555, 0.03921207785606384, 0.08456385135650635, 0.5360945851560256, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 140 0.001 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/140
1493/1493 - 14s - loss: 0.0264 - val_loss: 0.0194 - 14s/epoch - 9ms/step
Epoch 2/140
1493/1493 - 8s - loss: 0.0149 - val_loss: 0.0140 - 8s/epoch - 5ms/step
Epoch 3/140
1493/1493 - 8s - loss: 0.0134 - val_loss: 0.0128 - 8s/epoch - 6ms/step
Epoch 4/140
1493/1493 - 8s - loss: 0.0128 - val_loss: 0.0125 - 8s/epoch - 6ms/step
Epoch 5/140
1493/1493 - 8s - loss: 0.0125 - val_loss: 0.0123 - 8s/epoch - 5ms/step
Epoch 6/140
1493/1493 - 8s - loss: 0.0123 - val_loss: 0.0119 - 8s/epoch - 6ms/step
Epoch 7/140
1493/1493 - 8s - loss: 0.0119 - val_loss: 0.0115 - 8s/epoch - 6ms/step
Epoch 8/140
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0113 - 8s/epoch - 6ms/step
Epoch 9/140
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 10/140
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 11/140
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 12/140
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 13/140
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 6ms/step
Epoch 14/140
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 6ms/step
Epoch 15/140
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 16/140
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 17/140
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 18/140
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 19/140
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 20/140
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 21/140
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 22/140
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 23/140
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 24/140
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 25/140
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 26/140
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 27/140
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 28/140
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 29/140
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 30/140
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 31/140
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 32/140
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 33/140
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 34/140
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 35/140
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 36/140
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 37/140
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 38/140
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 39/140
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 40/140
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 41/140
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 42/140
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 43/140
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 44/140
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 45/140
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 46/140
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 47/140
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 48/140
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 49/140
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 50/140
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 51/140
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 52/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 53/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 54/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 55/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 56/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 57/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 58/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 59/140
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 60/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 61/140
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 62/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 63/140
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 64/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 65/140
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 66/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 67/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 68/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 69/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 70/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 71/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 72/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 73/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 74/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 75/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 76/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 77/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 78/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 79/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 80/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 81/140
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 82/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 83/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 84/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 85/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 86/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 87/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 88/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 89/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 90/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 91/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 92/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 93/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 94/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 95/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 96/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 97/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 98/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 99/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 100/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 101/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 102/140
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 103/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 104/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 105/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 106/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 107/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 108/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 109/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 110/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 111/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 112/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 113/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 114/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 115/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 116/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 117/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 118/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 119/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 120/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 121/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 122/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 123/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 124/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 125/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 126/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 127/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 128/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 129/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 130/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 131/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 132/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 133/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 134/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 135/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 136/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 137/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 138/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 139/140
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 140/140
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009736484847962856
  1/332 [..............................] - ETA: 55s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s110/332 [========>.....................] - ETA: 0s146/332 [============>.................] - ETA: 0s171/332 [==============>...............] - ETA: 0s207/332 [=================>............] - ETA: 0s244/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s317/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08191345239467455
cosine 0.06432412547904323
MAE: 0.037793137
RMSE: 0.082216755
r2: 0.5614889498473448
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 64, 140, 0.001, 0.6, 758, 0.009949288330972195, 0.009736484847962856, 0.08191345239467455, 0.06432412547904323, 0.03779313713312149, 0.08221675455570221, 0.5614889498473448, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 175 0.002 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/175
1493/1493 - 14s - loss: 0.0319 - val_loss: 0.0196 - 14s/epoch - 9ms/step
Epoch 2/175
1493/1493 - 8s - loss: 0.0161 - val_loss: 0.0157 - 8s/epoch - 5ms/step
Epoch 3/175
1493/1493 - 8s - loss: 0.0135 - val_loss: 0.0125 - 8s/epoch - 5ms/step
Epoch 4/175
1493/1493 - 8s - loss: 0.0134 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 5/175
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0117 - 8s/epoch - 5ms/step
Epoch 6/175
1493/1493 - 8s - loss: 0.0118 - val_loss: 0.0114 - 8s/epoch - 5ms/step
Epoch 7/175
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0113 - 8s/epoch - 5ms/step
Epoch 8/175
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0113 - 8s/epoch - 5ms/step
Epoch 9/175
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 10/175
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 11/175
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 12/175
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 13/175
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 14/175
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 15/175
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 16/175
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 17/175
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 18/175
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 19/175
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 20/175
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 21/175
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 22/175
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 23/175
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 24/175
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 25/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 26/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 27/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 28/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 29/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 30/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 31/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 32/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 33/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 34/175
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 35/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 36/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 37/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 38/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 39/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 40/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 41/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 42/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 43/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 44/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 45/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 46/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 47/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 48/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 49/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 50/175
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 51/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 52/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 53/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 54/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 55/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 56/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 57/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 58/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 59/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 60/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 61/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 62/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 63/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 64/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 65/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 66/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 67/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 68/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 69/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 70/175
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 71/175
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 72/175
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 73/175
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 74/175
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 75/175
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 76/175
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 77/175
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 78/175
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 79/175
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 80/175
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 81/175
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 82/175
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 83/175
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 84/175
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 85/175
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 86/175
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 87/175
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 88/175
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 89/175
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 90/175
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 91/175
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 92/175
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 93/175
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 94/175
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 95/175
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 96/175
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 97/175
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 98/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 99/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 100/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 101/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 102/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 103/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 104/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 105/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 106/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 107/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 108/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 109/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 110/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 111/175
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 112/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 113/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 114/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 115/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 116/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 117/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 118/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 119/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 120/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 121/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 122/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 123/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 124/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 125/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 126/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 127/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 128/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 129/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 130/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 131/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 132/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 133/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 134/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 135/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 136/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 137/175
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 138/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 139/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 140/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 141/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 142/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 143/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 144/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 145/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 146/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 147/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 148/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 149/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 150/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 151/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 152/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 153/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 154/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 155/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 156/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 157/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 158/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 159/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 160/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 161/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 162/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 163/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 164/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 165/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 166/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 167/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 168/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 169/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 170/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 171/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 172/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 173/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 174/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 175/175
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00964298564940691
  1/332 [..............................] - ETA: 57s 35/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s141/332 [===========>..................] - ETA: 0s162/332 [=============>................] - ETA: 0s198/332 [================>.............] - ETA: 0s235/332 [====================>.........] - ETA: 0s271/332 [=======================>......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08031048811901932
cosine 0.06307033133470828
MAE: 0.037477855
RMSE: 0.08161876
r2: 0.5678444418546097
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'mse', 64, 175, 0.002, 0.6, 758, 0.009880050085484982, 0.00964298564940691, 0.08031048811901932, 0.06307033133470828, 0.03747785463929176, 0.0816187635064125, 0.5678444418546097, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 85 0.0005 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5959276     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,352,182
Trainable params: 13,340,050
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/85
1493/1493 - 15s - loss: 0.0288 - val_loss: 0.0195 - 15s/epoch - 10ms/step
Epoch 2/85
1493/1493 - 8s - loss: 0.0154 - val_loss: 0.0144 - 8s/epoch - 5ms/step
Epoch 3/85
1493/1493 - 8s - loss: 0.0136 - val_loss: 0.0130 - 8s/epoch - 5ms/step
Epoch 4/85
1493/1493 - 8s - loss: 0.0128 - val_loss: 0.0124 - 8s/epoch - 5ms/step
Epoch 5/85
1493/1493 - 8s - loss: 0.0125 - val_loss: 0.0122 - 8s/epoch - 6ms/step
Epoch 6/85
1493/1493 - 8s - loss: 0.0123 - val_loss: 0.0118 - 8s/epoch - 5ms/step
Epoch 7/85
1493/1493 - 8s - loss: 0.0118 - val_loss: 0.0115 - 8s/epoch - 5ms/step
Epoch 8/85
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0113 - 8s/epoch - 6ms/step
Epoch 9/85
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0112 - 8s/epoch - 6ms/step
Epoch 10/85
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 11/85
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 12/85
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 13/85
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 14/85
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 15/85
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 6ms/step
Epoch 16/85
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 6ms/step
Epoch 17/85
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 18/85
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 19/85
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 20/85
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 21/85
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 22/85
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 23/85
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 24/85
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 25/85
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 26/85
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 27/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 28/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 29/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 30/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 31/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 32/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 33/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 34/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 35/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 36/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 37/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 38/85
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 39/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 40/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 41/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 42/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 43/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 44/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 45/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 46/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 47/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 6ms/step
Epoch 48/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 6ms/step
Epoch 49/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 50/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 51/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 6ms/step
Epoch 52/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 6ms/step
Epoch 53/85
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 54/85
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 55/85
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 56/85
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 57/85
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 58/85
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 59/85
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 60/85
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 61/85
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 62/85
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 63/85
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 64/85
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 65/85
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 66/85
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 67/85
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 68/85
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 69/85
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 70/85
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 71/85
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 72/85
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 73/85
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 74/85
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 75/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 76/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 77/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 78/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 79/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 80/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 81/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 82/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 83/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 84/85
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 85/85
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009975663386285305
  1/332 [..............................] - ETA: 53s 35/332 [==>...........................] - ETA: 0s  71/332 [=====>........................] - ETA: 0s107/332 [========>.....................] - ETA: 0s144/332 [============>.................] - ETA: 0s169/332 [==============>...............] - ETA: 0s206/332 [=================>............] - ETA: 0s243/332 [====================>.........] - ETA: 0s279/332 [========================>.....] - ETA: 0s315/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08651636730734914
cosine 0.06790261468795664
MAE: 0.039171822
RMSE: 0.08435039
r2: 0.5384343049473628
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'mse', 64, 85, 0.0005, 0.6, 758, 0.010148352012038231, 0.009975663386285305, 0.08651636730734914, 0.06790261468795664, 0.03917182236909866, 0.08435039222240448, 0.5384343049473628, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 145 0.001 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 12s - loss: 0.0251 - val_loss: 0.0170 - 12s/epoch - 8ms/step
Epoch 2/145
1493/1493 - 8s - loss: 0.0148 - val_loss: 0.0139 - 8s/epoch - 5ms/step
Epoch 3/145
1493/1493 - 8s - loss: 0.0134 - val_loss: 0.0129 - 8s/epoch - 5ms/step
Epoch 4/145
1493/1493 - 8s - loss: 0.0128 - val_loss: 0.0126 - 8s/epoch - 5ms/step
Epoch 5/145
1493/1493 - 8s - loss: 0.0126 - val_loss: 0.0123 - 8s/epoch - 6ms/step
Epoch 6/145
1493/1493 - 8s - loss: 0.0124 - val_loss: 0.0121 - 8s/epoch - 5ms/step
Epoch 7/145
1493/1493 - 8s - loss: 0.0123 - val_loss: 0.0121 - 8s/epoch - 5ms/step
Epoch 8/145
1493/1493 - 8s - loss: 0.0122 - val_loss: 0.0121 - 8s/epoch - 5ms/step
Epoch 9/145
1493/1493 - 8s - loss: 0.0122 - val_loss: 0.0120 - 8s/epoch - 5ms/step
Epoch 10/145
1493/1493 - 8s - loss: 0.0121 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 11/145
1493/1493 - 8s - loss: 0.0121 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 12/145
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0118 - 8s/epoch - 5ms/step
Epoch 13/145
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 14/145
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0118 - 8s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 8s - loss: 0.0119 - val_loss: 0.0117 - 8s/epoch - 5ms/step
Epoch 16/145
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0114 - 8s/epoch - 5ms/step
Epoch 17/145
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0113 - 8s/epoch - 5ms/step
Epoch 18/145
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 19/145
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 21/145
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 22/145
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 23/145
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 24/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 25/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0106 - 8s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 27/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 28/145
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 6ms/step
Epoch 29/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 30/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 31/145
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 33/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 34/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 35/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 36/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 37/145
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 38/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 39/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 40/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 43/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 44/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 45/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 46/145
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 47/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 49/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 50/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 51/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 52/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 53/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 55/145
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 56/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 57/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 58/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 59/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 60/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 61/145
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 62/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 63/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 64/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 65/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 66/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 69/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 70/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 71/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 72/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 73/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 74/145
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 75/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 76/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 86/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 87/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 90/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 92/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 93/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 102/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 103/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 104/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 110/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 111/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 112/145
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 115/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 117/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 118/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 119/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 121/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 122/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 125/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 127/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 128/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 131/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 132/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 133/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 134/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 135/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 137/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 140/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 7ms/step
Epoch 142/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 7ms/step
Epoch 145/145
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009656614623963833
  1/332 [..............................] - ETA: 58s 33/332 [=>............................] - ETA: 0s  67/332 [=====>........................] - ETA: 0s101/332 [========>.....................] - ETA: 0s135/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s227/332 [===================>..........] - ETA: 0s258/332 [======================>.......] - ETA: 0s293/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08127279600982976
cosine 0.06379742109905852
MAE: 0.037562683
RMSE: 0.08198701
r2: 0.5639360392383667
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'mse', 64, 145, 0.001, 0.6, 758, 0.009879197925329208, 0.009656614623963833, 0.08127279600982976, 0.06379742109905852, 0.03756268322467804, 0.08198700845241547, 0.5639360392383667, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[1.2999999999999998 145 0.0012000000000000001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 13s - loss: 0.0253 - val_loss: 0.0172 - 13s/epoch - 9ms/step
Epoch 2/145
1493/1493 - 9s - loss: 0.0151 - val_loss: 0.0146 - 9s/epoch - 6ms/step
Epoch 3/145
1493/1493 - 9s - loss: 0.0135 - val_loss: 0.0129 - 9s/epoch - 6ms/step
Epoch 4/145
1493/1493 - 9s - loss: 0.0128 - val_loss: 0.0125 - 9s/epoch - 6ms/step
Epoch 5/145
1493/1493 - 9s - loss: 0.0125 - val_loss: 0.0123 - 9s/epoch - 6ms/step
Epoch 6/145
1493/1493 - 9s - loss: 0.0124 - val_loss: 0.0122 - 9s/epoch - 6ms/step
Epoch 7/145
1493/1493 - 9s - loss: 0.0123 - val_loss: 0.0120 - 9s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0117 - 9s/epoch - 6ms/step
Epoch 9/145
1493/1493 - 9s - loss: 0.0117 - val_loss: 0.0114 - 9s/epoch - 6ms/step
Epoch 10/145
1493/1493 - 9s - loss: 0.0115 - val_loss: 0.0113 - 9s/epoch - 6ms/step
Epoch 11/145
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 12/145
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 13/145
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 16/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 17/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 18/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 19/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 21/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 22/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 23/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 24/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 25/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 27/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 28/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 29/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 30/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 31/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 33/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 34/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 35/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 36/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 37/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 38/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 39/145
1493/1493 - 10s - loss: 0.0108 - val_loss: 0.0107 - 10s/epoch - 6ms/step
Epoch 40/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 44/145
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 45/145
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 46/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 47/145
1493/1493 - 10s - loss: 0.0106 - val_loss: 0.0105 - 10s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 49/145
1493/1493 - 10s - loss: 0.0106 - val_loss: 0.0105 - 10s/epoch - 6ms/step
Epoch 50/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 51/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 52/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 53/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 56/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 57/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 58/145
1493/1493 - 10s - loss: 0.0105 - val_loss: 0.0104 - 10s/epoch - 6ms/step
Epoch 59/145
1493/1493 - 10s - loss: 0.0105 - val_loss: 0.0104 - 10s/epoch - 6ms/step
Epoch 60/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 61/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 62/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 63/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 64/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 65/145
1493/1493 - 10s - loss: 0.0105 - val_loss: 0.0103 - 10s/epoch - 6ms/step
Epoch 66/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 69/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 70/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 71/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 72/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 73/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 74/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 75/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 76/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 10s - loss: 0.0103 - val_loss: 0.0102 - 10s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 10s - loss: 0.0102 - val_loss: 0.0101 - 10s/epoch - 6ms/step
Epoch 86/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 87/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 90/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 92/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 93/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 10s - loss: 0.0101 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 102/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 103/145
1493/1493 - 10s - loss: 0.0101 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 104/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 10s - loss: 0.0101 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 110/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 111/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 112/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0098 - 10s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0098 - 10s/epoch - 6ms/step
Epoch 117/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 119/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 121/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0098 - 10s/epoch - 6ms/step
Epoch 125/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 127/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 128/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0098 - 10s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 131/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 132/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0098 - 10s/epoch - 6ms/step
Epoch 133/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0098 - 10s/epoch - 6ms/step
Epoch 134/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 135/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0097 - 10s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0099 - 10s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0098 - 10s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 145/145
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00972734484821558
  1/332 [..............................] - ETA: 58s 36/332 [==>...........................] - ETA: 0s  71/332 [=====>........................] - ETA: 0s104/332 [========>.....................] - ETA: 0s139/332 [===========>..................] - ETA: 0s172/332 [==============>...............] - ETA: 0s206/332 [=================>............] - ETA: 0s240/332 [====================>.........] - ETA: 0s276/332 [=======================>......] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08132307266249911
cosine 0.06383845361726849
MAE: 0.037588373
RMSE: 0.0823903
r2: 0.5596356904680105
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 64, 145, 0.0012000000000000001, 0.6, 758, 0.009949869476258755, 0.00972734484821558, 0.08132307266249911, 0.06383845361726849, 0.03758837282657623, 0.08239030092954636, 0.5596356904680105, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 85 0.0009 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/85
374/374 - 7s - loss: 0.0405 - val_loss: 0.0200 - 7s/epoch - 18ms/step
Epoch 2/85
374/374 - 3s - loss: 0.0165 - val_loss: 0.0185 - 3s/epoch - 7ms/step
Epoch 3/85
374/374 - 3s - loss: 0.0153 - val_loss: 0.0188 - 3s/epoch - 8ms/step
Epoch 4/85
374/374 - 3s - loss: 0.0148 - val_loss: 0.0178 - 3s/epoch - 8ms/step
Epoch 5/85
374/374 - 3s - loss: 0.0143 - val_loss: 0.0170 - 3s/epoch - 7ms/step
Epoch 6/85
374/374 - 3s - loss: 0.0140 - val_loss: 0.0171 - 3s/epoch - 7ms/step
Epoch 7/85
374/374 - 3s - loss: 0.0137 - val_loss: 0.0151 - 3s/epoch - 7ms/step
Epoch 8/85
374/374 - 3s - loss: 0.0135 - val_loss: 0.0201 - 3s/epoch - 8ms/step
Epoch 9/85
374/374 - 3s - loss: 0.0133 - val_loss: 0.0147 - 3s/epoch - 8ms/step
Epoch 10/85
374/374 - 3s - loss: 0.0130 - val_loss: 0.0137 - 3s/epoch - 7ms/step
Epoch 11/85
374/374 - 3s - loss: 0.0128 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 12/85
374/374 - 3s - loss: 0.0127 - val_loss: 0.0140 - 3s/epoch - 7ms/step
Epoch 13/85
374/374 - 3s - loss: 0.0126 - val_loss: 0.0135 - 3s/epoch - 8ms/step
Epoch 14/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0141 - 3s/epoch - 7ms/step
Epoch 15/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0151 - 3s/epoch - 7ms/step
Epoch 16/85
374/374 - 3s - loss: 0.0126 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 17/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 18/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0122 - 3s/epoch - 8ms/step
Epoch 19/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 8ms/step
Epoch 20/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 21/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 22/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 23/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 24/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 8ms/step
Epoch 25/85
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 8ms/step
Epoch 26/85
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 27/85
374/374 - 3s - loss: 0.0113 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 28/85
374/374 - 3s - loss: 0.0117 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 29/85
374/374 - 3s - loss: 0.0112 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 30/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0120 - 3s/epoch - 8ms/step
Epoch 31/85
374/374 - 3s - loss: 0.0123 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 32/85
374/374 - 3s - loss: 0.0154 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 33/85
374/374 - 3s - loss: 0.0172 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 34/85
374/374 - 3s - loss: 0.0157 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 35/85
374/374 - 3s - loss: 0.0124 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 36/85
374/374 - 3s - loss: 0.0130 - val_loss: 0.0128 - 3s/epoch - 8ms/step
Epoch 37/85
374/374 - 3s - loss: 0.0131 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 38/85
374/374 - 3s - loss: 0.0126 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 39/85
374/374 - 3s - loss: 0.0168 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 40/85
374/374 - 3s - loss: 0.0129 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 41/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 42/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0117 - 3s/epoch - 8ms/step
Epoch 43/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 44/85
374/374 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 45/85
374/374 - 3s - loss: 0.0117 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 46/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 47/85
374/374 - 3s - loss: 0.0130 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 48/85
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 8ms/step
Epoch 49/85
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 50/85
374/374 - 3s - loss: 0.0114 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 51/85
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 52/85
374/374 - 3s - loss: 0.0113 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 53/85
374/374 - 3s - loss: 0.0116 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 54/85
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 55/85
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 8ms/step
Epoch 56/85
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 8ms/step
Epoch 57/85
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 8ms/step
Epoch 58/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 59/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 60/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 61/85
374/374 - 3s - loss: 0.0113 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 62/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 63/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 8ms/step
Epoch 64/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 65/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0131 - 3s/epoch - 7ms/step
Epoch 66/85
374/374 - 3s - loss: 0.0141 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 67/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 68/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 69/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0151 - 3s/epoch - 7ms/step
Epoch 70/85
374/374 - 3s - loss: 0.0164 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 71/85
374/374 - 3s - loss: 0.0113 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 72/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 8ms/step
Epoch 73/85
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 74/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 75/85
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 76/85
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 77/85
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 78/85
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 79/85
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 80/85
374/374 - 3s - loss: 0.0108 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 81/85
374/374 - 3s - loss: 0.0112 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 82/85
374/374 - 3s - loss: 0.0108 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 83/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 84/85
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 85/85
374/374 - 3s - loss: 0.0108 - val_loss: 0.0135 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.01351634506136179
  1/332 [..............................] - ETA: 1:01 35/332 [==>...........................] - ETA: 0s   70/332 [=====>........................] - ETA: 0s102/332 [========>.....................] - ETA: 0s136/332 [===========>..................] - ETA: 0s172/332 [==============>...............] - ETA: 0s207/332 [=================>............] - ETA: 0s241/332 [====================>.........] - ETA: 0s276/332 [=======================>......] - ETA: 0s312/332 [===========================>..] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.10159488466048393
cosine 0.07954042365853849
MAE: 0.047633577
RMSE: 0.091278
r2: 0.4595281488651358
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 256, 85, 0.0009, 0.6, 758, 0.010819763876497746, 0.01351634506136179, 0.10159488466048393, 0.07954042365853849, 0.0476335771381855, 0.09127800166606903, 0.4595281488651358, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 85 0.0007 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/85
747/747 - 9s - loss: 0.0300 - val_loss: 0.0200 - 9s/epoch - 11ms/step
Epoch 2/85
747/747 - 5s - loss: 0.0158 - val_loss: 0.0192 - 5s/epoch - 6ms/step
Epoch 3/85
747/747 - 5s - loss: 0.0148 - val_loss: 0.0151 - 5s/epoch - 7ms/step
Epoch 4/85
747/747 - 5s - loss: 0.0141 - val_loss: 0.0190 - 5s/epoch - 6ms/step
Epoch 5/85
747/747 - 5s - loss: 0.0135 - val_loss: 0.0139 - 5s/epoch - 6ms/step
Epoch 6/85
747/747 - 5s - loss: 0.0130 - val_loss: 0.0129 - 5s/epoch - 6ms/step
Epoch 7/85
747/747 - 5s - loss: 0.0127 - val_loss: 0.0125 - 5s/epoch - 7ms/step
Epoch 8/85
747/747 - 5s - loss: 0.0125 - val_loss: 0.0124 - 5s/epoch - 7ms/step
Epoch 9/85
747/747 - 5s - loss: 0.0124 - val_loss: 0.0122 - 5s/epoch - 6ms/step
Epoch 10/85
747/747 - 5s - loss: 0.0123 - val_loss: 0.0125 - 5s/epoch - 6ms/step
Epoch 11/85
747/747 - 5s - loss: 0.0125 - val_loss: 0.0126 - 5s/epoch - 6ms/step
Epoch 12/85
747/747 - 5s - loss: 0.0124 - val_loss: 0.0121 - 5s/epoch - 7ms/step
Epoch 13/85
747/747 - 5s - loss: 0.0119 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 14/85
747/747 - 5s - loss: 0.0116 - val_loss: 0.0114 - 5s/epoch - 6ms/step
Epoch 15/85
747/747 - 5s - loss: 0.0114 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 16/85
747/747 - 5s - loss: 0.0114 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 17/85
747/747 - 5s - loss: 0.0112 - val_loss: 0.0111 - 5s/epoch - 7ms/step
Epoch 18/85
747/747 - 5s - loss: 0.0112 - val_loss: 0.0110 - 5s/epoch - 6ms/step
Epoch 19/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0110 - 5s/epoch - 6ms/step
Epoch 20/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 21/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 22/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 23/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 24/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 25/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 26/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 27/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 28/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 7ms/step
Epoch 29/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0108 - 5s/epoch - 7ms/step
Epoch 30/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 31/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 32/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 33/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 34/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 35/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 36/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 37/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 38/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 39/85
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 40/85
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 41/85
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 7ms/step
Epoch 42/85
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 43/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 7ms/step
Epoch 44/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 45/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 46/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 47/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 48/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 49/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 50/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 51/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 52/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 53/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 54/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 55/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 56/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 57/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 58/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 59/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 60/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 61/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 62/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 63/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 64/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 65/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 66/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 67/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 68/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 69/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 70/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 71/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 72/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 73/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 74/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 75/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 76/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 77/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 78/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 79/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 80/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 81/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 82/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 83/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 84/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 85/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009907940402626991
  1/332 [..............................] - ETA: 59s 35/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s140/332 [===========>..................] - ETA: 0s176/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s246/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08517140431467919
cosine 0.0668347416353639
MAE: 0.038638536
RMSE: 0.08365386
r2: 0.5460255963177074
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 128, 85, 0.0007, 0.6, 758, 0.010083300992846489, 0.009907940402626991, 0.08517140431467919, 0.0668347416353639, 0.038638535887002945, 0.08365385979413986, 0.5460255963177074, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 80 0.002 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/80
1493/1493 - 13s - loss: 0.0149 - val_loss: 0.0087 - 13s/epoch - 8ms/step
Epoch 2/80
1493/1493 - 8s - loss: 0.0080 - val_loss: 0.0076 - 8s/epoch - 6ms/step
Epoch 3/80
1493/1493 - 9s - loss: 0.0073 - val_loss: 0.0071 - 9s/epoch - 6ms/step
Epoch 4/80
1493/1493 - 9s - loss: 0.0069 - val_loss: 0.0068 - 9s/epoch - 6ms/step
Epoch 5/80
1493/1493 - 8s - loss: 0.0068 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 6/80
1493/1493 - 8s - loss: 0.0068 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 7/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 8/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 9/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 10/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 11/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 12/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 13/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 14/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 15/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 16/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 17/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 18/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 19/80
1493/1493 - 9s - loss: 0.0066 - val_loss: 0.0064 - 9s/epoch - 6ms/step
Epoch 20/80
1493/1493 - 9s - loss: 0.0064 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 21/80
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 22/80
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 23/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 24/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 25/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 26/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 27/80
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 28/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 29/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 30/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 31/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 32/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 33/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 34/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 35/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 36/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 37/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 38/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 39/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 40/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 41/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 42/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 43/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 44/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 45/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 46/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 47/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 48/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 49/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 50/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 51/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 52/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 53/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 54/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 55/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 56/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 57/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 58/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 59/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 60/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 61/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 62/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 63/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 64/80
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 65/80
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 66/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 67/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 68/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 69/80
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 70/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 71/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 72/80
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 73/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 74/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 75/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 76/80
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 77/80
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0060 - 8s/epoch - 6ms/step
Epoch 78/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
Epoch 79/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 80/80
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0060 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006022267509251833
  1/332 [..............................] - ETA: 1:05 34/332 [==>...........................] - ETA: 0s   68/332 [=====>........................] - ETA: 0s103/332 [========>.....................] - ETA: 0s139/332 [===========>..................] - ETA: 0s174/332 [==============>...............] - ETA: 0s210/332 [=================>............] - ETA: 0s245/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s317/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12328524783652457
cosine 0.09649331775018956
MAE: 0.046782915
RMSE: 0.10041631
r2: 0.3458633608176699
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'logcosh', 64, 80, 0.002, 0.6, 758, 0.0060850949957966805, 0.006022267509251833, 0.12328524783652457, 0.09649331775018956, 0.04678291454911232, 0.10041631013154984, 0.3458633608176699, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 145 0.0022 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 12s - loss: 0.0250 - val_loss: 0.0168 - 12s/epoch - 8ms/step
Epoch 2/145
1493/1493 - 9s - loss: 0.0150 - val_loss: 0.0145 - 9s/epoch - 6ms/step
Epoch 3/145
1493/1493 - 9s - loss: 0.0135 - val_loss: 0.0129 - 9s/epoch - 6ms/step
Epoch 4/145
1493/1493 - 9s - loss: 0.0128 - val_loss: 0.0125 - 9s/epoch - 6ms/step
Epoch 5/145
1493/1493 - 9s - loss: 0.0126 - val_loss: 0.0123 - 9s/epoch - 6ms/step
Epoch 6/145
1493/1493 - 8s - loss: 0.0124 - val_loss: 0.0122 - 8s/epoch - 6ms/step
Epoch 7/145
1493/1493 - 9s - loss: 0.0123 - val_loss: 0.0121 - 9s/epoch - 6ms/step
Epoch 8/145
1493/1493 - 9s - loss: 0.0122 - val_loss: 0.0120 - 9s/epoch - 6ms/step
Epoch 9/145
1493/1493 - 9s - loss: 0.0121 - val_loss: 0.0119 - 9s/epoch - 6ms/step
Epoch 10/145
1493/1493 - 9s - loss: 0.0120 - val_loss: 0.0117 - 9s/epoch - 6ms/step
Epoch 11/145
1493/1493 - 9s - loss: 0.0117 - val_loss: 0.0115 - 9s/epoch - 6ms/step
Epoch 12/145
1493/1493 - 9s - loss: 0.0115 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 13/145
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 14/145
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 15/145
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 16/145
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 17/145
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 18/145
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 19/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 20/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 21/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 22/145
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 23/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 24/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 25/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 26/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 27/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 28/145
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 29/145
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 30/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 31/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 32/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 33/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 34/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 35/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 36/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 37/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 38/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 39/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 40/145
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 41/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 42/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 43/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 44/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 45/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 46/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 47/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 48/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 49/145
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 50/145
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 51/145
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 52/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 53/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 54/145
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 55/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 56/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 57/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 58/145
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 59/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 60/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 61/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 62/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 63/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 64/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 65/145
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 66/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 67/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 68/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 69/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 70/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 71/145
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 72/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 73/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 74/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 75/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 76/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 77/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 78/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 79/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 80/145
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 81/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 82/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 83/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 84/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 85/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 86/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 87/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 88/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 89/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 90/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 91/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 92/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 93/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 94/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 95/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 96/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 97/145
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 98/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 99/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 100/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 101/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 102/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 103/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 104/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 105/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 106/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 107/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 108/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 109/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 110/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 111/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 112/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 113/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 114/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 115/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 116/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 117/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 118/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 119/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 120/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 121/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 122/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 123/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 124/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 125/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 126/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 127/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 128/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 129/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 130/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 131/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 132/145
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 133/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 134/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 135/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 136/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 137/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 138/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 139/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 140/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 141/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 142/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 143/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 144/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 145/145
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009731133468449116
  1/332 [..............................] - ETA: 1:03 34/332 [==>...........................] - ETA: 0s   66/332 [====>.........................] - ETA: 0s102/332 [========>.....................] - ETA: 0s136/332 [===========>..................] - ETA: 0s171/332 [==============>...............] - ETA: 0s204/332 [=================>............] - ETA: 0s239/332 [====================>.........] - ETA: 0s273/332 [=======================>......] - ETA: 0s308/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08061325305751976
cosine 0.06329274217429989
MAE: 0.037570674
RMSE: 0.08165088
r2: 0.5675042729737366
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'mse', 64, 145, 0.0022, 0.6, 758, 0.009929696097970009, 0.009731133468449116, 0.08061325305751976, 0.06329274217429989, 0.03757067397236824, 0.08165088295936584, 0.5675042729737366, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 80 0.0007 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/80
374/374 - 6s - loss: 0.0374 - val_loss: 0.0202 - 6s/epoch - 17ms/step
Epoch 2/80
374/374 - 3s - loss: 0.0164 - val_loss: 0.0173 - 3s/epoch - 7ms/step
Epoch 3/80
374/374 - 3s - loss: 0.0150 - val_loss: 0.0175 - 3s/epoch - 7ms/step
Epoch 4/80
374/374 - 3s - loss: 0.0149 - val_loss: 0.0165 - 3s/epoch - 7ms/step
Epoch 5/80
374/374 - 3s - loss: 0.0141 - val_loss: 0.0170 - 3s/epoch - 7ms/step
Epoch 6/80
374/374 - 3s - loss: 0.0139 - val_loss: 0.0172 - 3s/epoch - 7ms/step
Epoch 7/80
374/374 - 3s - loss: 0.0137 - val_loss: 0.0152 - 3s/epoch - 7ms/step
Epoch 8/80
374/374 - 3s - loss: 0.0134 - val_loss: 0.0150 - 3s/epoch - 7ms/step
Epoch 9/80
374/374 - 3s - loss: 0.0133 - val_loss: 0.0154 - 3s/epoch - 7ms/step
Epoch 10/80
374/374 - 3s - loss: 0.0131 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 11/80
374/374 - 3s - loss: 0.0129 - val_loss: 0.0152 - 3s/epoch - 7ms/step
Epoch 12/80
374/374 - 3s - loss: 0.0128 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 13/80
374/374 - 3s - loss: 0.0126 - val_loss: 0.0139 - 3s/epoch - 7ms/step
Epoch 14/80
374/374 - 3s - loss: 0.0124 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 15/80
374/374 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 16/80
374/374 - 3s - loss: 0.0119 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 17/80
374/374 - 3s - loss: 0.0117 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 18/80
374/374 - 3s - loss: 0.0116 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 19/80
374/374 - 3s - loss: 0.0117 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 20/80
374/374 - 3s - loss: 0.0114 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 21/80
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 22/80
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 23/80
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 24/80
374/374 - 3s - loss: 0.0111 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 25/80
374/374 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 26/80
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 27/80
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 28/80
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 29/80
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 30/80
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 31/80
374/374 - 3s - loss: 0.0108 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 32/80
374/374 - 3s - loss: 0.0111 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 33/80
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 34/80
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 35/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 36/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 37/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 38/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 39/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 40/80
374/374 - 3s - loss: 0.0113 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 41/80
374/374 - 3s - loss: 0.0117 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 42/80
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 43/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 44/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 45/80
374/374 - 3s - loss: 0.0107 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 46/80
374/374 - 3s - loss: 0.0121 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 47/80
374/374 - 3s - loss: 0.0113 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 48/80
374/374 - 3s - loss: 0.0119 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 49/80
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 50/80
374/374 - 3s - loss: 0.0110 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 51/80
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 52/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 53/80
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 54/80
374/374 - 3s - loss: 0.0105 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 55/80
374/374 - 3s - loss: 0.0108 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 56/80
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 57/80
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 58/80
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 59/80
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 60/80
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 61/80
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 62/80
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 63/80
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 64/80
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 65/80
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 66/80
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 67/80
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 68/80
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 69/80
374/374 - 3s - loss: 0.0102 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 70/80
374/374 - 3s - loss: 0.0110 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 71/80
374/374 - 3s - loss: 0.0105 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 72/80
374/374 - 3s - loss: 0.0103 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 73/80
374/374 - 3s - loss: 0.0109 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 74/80
374/374 - 3s - loss: 0.0102 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 75/80
374/374 - 3s - loss: 0.0104 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 76/80
374/374 - 3s - loss: 0.0107 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 77/80
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 78/80
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 79/80
374/374 - 3s - loss: 0.0101 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 80/80
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010007107630372047
  1/332 [..............................] - ETA: 50s 33/332 [=>............................] - ETA: 0s  69/332 [=====>........................] - ETA: 0s105/332 [========>.....................] - ETA: 0s139/332 [===========>..................] - ETA: 0s173/332 [==============>...............] - ETA: 0s207/332 [=================>............] - ETA: 0s243/332 [====================>.........] - ETA: 0s277/332 [========================>.....] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08696963311454911
cosine 0.06821903626266354
MAE: 0.03896649
RMSE: 0.08441248
r2: 0.537754005308471
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'mse', 256, 80, 0.0007, 0.6, 758, 0.010131875984370708, 0.010007107630372047, 0.08696963311454911, 0.06821903626266354, 0.03896649181842804, 0.08441247791051865, 0.537754005308471, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 85 0.0007 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.2999999999999998_cr0.6_bs128_ep85_loss_mse_lr0.0007_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 43s 33/332 [=>............................] - ETA: 0s  67/332 [=====>........................] - ETA: 0s102/332 [========>.....................] - ETA: 0s137/332 [===========>..................] - ETA: 0s172/332 [==============>...............] - ETA: 0s204/332 [=================>............] - ETA: 0s239/332 [====================>.........] - ETA: 0s275/332 [=======================>......] - ETA: 0s310/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08517140431467919
cosine 0.0668347416353639
MAE: 0.038638536
RMSE: 0.08365386
r2: 0.5460255963177074
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.2999999999999998custom_VAE', 'mse', 128, 85, 0.0007, 0.6, 758, '--', '--', 0.08517140431467919, 0.0668347416353639, 0.038638535887002945, 0.08365385979413986, 0.5460255963177074, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 140 0.002 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/140
1493/1493 - 12s - loss: 0.0145 - val_loss: 0.0086 - 12s/epoch - 8ms/step
Epoch 2/140
1493/1493 - 8s - loss: 0.0081 - val_loss: 0.0078 - 8s/epoch - 6ms/step
Epoch 3/140
1493/1493 - 8s - loss: 0.0074 - val_loss: 0.0071 - 8s/epoch - 6ms/step
Epoch 4/140
1493/1493 - 8s - loss: 0.0070 - val_loss: 0.0069 - 8s/epoch - 6ms/step
Epoch 5/140
1493/1493 - 9s - loss: 0.0068 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 6/140
1493/1493 - 9s - loss: 0.0068 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 7/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 8/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 9/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 10/140
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 11/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 12/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 13/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 14/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 15/140
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 16/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 17/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 18/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 19/140
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 20/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 21/140
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 22/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 23/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 24/140
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 25/140
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 26/140
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 27/140
1493/1493 - 8s - loss: 0.0066 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 28/140
1493/1493 - 8s - loss: 0.0066 - val_loss: 0.0065 - 8s/epoch - 6ms/step
Epoch 29/140
1493/1493 - 8s - loss: 0.0065 - val_loss: 0.0064 - 8s/epoch - 6ms/step
Epoch 30/140
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 31/140
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 32/140
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 33/140
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 34/140
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 35/140
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 36/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 37/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 38/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 39/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 40/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 41/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 42/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 43/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 44/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 45/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 46/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 47/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 48/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 49/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 50/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 51/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 52/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 53/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 54/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 55/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 56/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 57/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 58/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 59/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 60/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 61/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 62/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 63/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 64/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 65/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 66/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 67/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 68/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 69/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 70/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 71/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 72/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 73/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 74/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 75/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 76/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 77/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 78/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 79/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 80/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 81/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 82/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 83/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 84/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 85/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 86/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 87/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 88/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 89/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 90/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 91/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 92/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 93/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 94/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 95/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 96/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 97/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 98/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 99/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 100/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 101/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 102/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 103/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 104/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 105/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 106/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 107/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 108/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 109/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 110/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 111/140
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 112/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 113/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 114/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 115/140
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 116/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 117/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 118/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 119/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 120/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 121/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 122/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 123/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 124/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 125/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 126/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 127/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 128/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 129/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 130/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 131/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 132/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 133/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 134/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 135/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 136/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 137/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 138/140
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 139/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 140/140
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0061296275816857815
  1/332 [..............................] - ETA: 56s 32/332 [=>............................] - ETA: 0s  66/332 [====>.........................] - ETA: 0s101/332 [========>.....................] - ETA: 0s137/332 [===========>..................] - ETA: 0s171/332 [==============>...............] - ETA: 0s206/332 [=================>............] - ETA: 0s232/332 [===================>..........] - ETA: 0s265/332 [======================>.......] - ETA: 0s299/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12710029206667314
cosine 0.0995027116288161
MAE: 0.047850218
RMSE: 0.10212233
r2: 0.32344749277976526
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'logcosh', 64, 140, 0.002, 0.6, 758, 0.006138215307146311, 0.0061296275816857815, 0.12710029206667314, 0.0995027116288161, 0.047850217670202255, 0.10212232917547226, 0.32344749277976526, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[1.4 80 0.002 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/80
1493/1493 - 12s - loss: 0.0147 - val_loss: 0.0090 - 12s/epoch - 8ms/step
Epoch 2/80
1493/1493 - 8s - loss: 0.0081 - val_loss: 0.0083 - 8s/epoch - 6ms/step
Epoch 3/80
1493/1493 - 9s - loss: 0.0073 - val_loss: 0.0071 - 9s/epoch - 6ms/step
Epoch 4/80
1493/1493 - 9s - loss: 0.0069 - val_loss: 0.0068 - 9s/epoch - 6ms/step
Epoch 5/80
1493/1493 - 8s - loss: 0.0068 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 6/80
1493/1493 - 8s - loss: 0.0068 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 7/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 8/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 9/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 10/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 11/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 12/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 13/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 14/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 15/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 16/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 17/80
1493/1493 - 9s - loss: 0.0065 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 18/80
1493/1493 - 9s - loss: 0.0064 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 19/80
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 20/80
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 21/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 22/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 23/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 24/80
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 25/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 26/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 27/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 28/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 29/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 30/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 31/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 32/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 33/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 34/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 35/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 36/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 37/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 38/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 39/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 40/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 41/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 42/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 43/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 44/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 45/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 46/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 47/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 48/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 49/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 50/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 51/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 52/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 53/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 54/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 55/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 56/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 57/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 58/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 59/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 60/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 61/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 62/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 63/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 64/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 65/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 66/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 67/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 68/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 69/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 70/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 71/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 72/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 73/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 74/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 75/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 76/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 77/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 78/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 79/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 80/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0062288460321724415
  1/332 [..............................] - ETA: 52s 35/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s104/332 [========>.....................] - ETA: 0s139/332 [===========>..................] - ETA: 0s174/332 [==============>...............] - ETA: 0s209/332 [=================>............] - ETA: 0s235/332 [====================>.........] - ETA: 0s269/332 [=======================>......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12805109257647285
cosine 0.10020264403531563
MAE: 0.048076227
RMSE: 0.10320887
r2: 0.3089743256832265
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'logcosh', 64, 80, 0.002, 0.6, 758, 0.006156580056995153, 0.0062288460321724415, 0.12805109257647285, 0.10020264403531563, 0.04807622730731964, 0.10320886969566345, 0.3089743256832265, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 80 0.002 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/80
1493/1493 - 12s - loss: 0.0142 - val_loss: 0.0092 - 12s/epoch - 8ms/step
Epoch 2/80
1493/1493 - 8s - loss: 0.0080 - val_loss: 0.0075 - 8s/epoch - 6ms/step
Epoch 3/80
1493/1493 - 8s - loss: 0.0074 - val_loss: 0.0072 - 8s/epoch - 6ms/step
Epoch 4/80
1493/1493 - 8s - loss: 0.0069 - val_loss: 0.0068 - 8s/epoch - 6ms/step
Epoch 5/80
1493/1493 - 9s - loss: 0.0068 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 6/80
1493/1493 - 9s - loss: 0.0068 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 7/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 8/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 9/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 10/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 11/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 12/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 13/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 14/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 15/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 16/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 17/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 18/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 19/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 20/80
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 6ms/step
Epoch 21/80
1493/1493 - 9s - loss: 0.0067 - val_loss: 0.0067 - 9s/epoch - 6ms/step
Epoch 22/80
1493/1493 - 9s - loss: 0.0066 - val_loss: 0.0066 - 9s/epoch - 6ms/step
Epoch 23/80
1493/1493 - 9s - loss: 0.0065 - val_loss: 0.0064 - 9s/epoch - 6ms/step
Epoch 24/80
1493/1493 - 9s - loss: 0.0064 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 25/80
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 26/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 27/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 28/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 29/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 30/80
1493/1493 - 9s - loss: 0.0063 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 31/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 32/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 33/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 34/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 35/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 36/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 37/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 38/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 39/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 40/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 41/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 42/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 43/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 44/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 45/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 46/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 47/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 48/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 49/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 50/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 51/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 52/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 53/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 54/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 55/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 56/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 57/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 58/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 59/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
Epoch 60/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 61/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 62/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 63/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 64/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 65/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 66/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 67/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 68/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 69/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 70/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 71/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 72/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 73/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 74/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 75/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 76/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 77/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 78/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 79/80
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0063 - 9s/epoch - 6ms/step
Epoch 80/80
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0063 - 8s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006313768681138754
  1/332 [..............................] - ETA: 55s 34/332 [==>...........................] - ETA: 0s  67/332 [=====>........................] - ETA: 0s101/332 [========>.....................] - ETA: 0s137/332 [===========>..................] - ETA: 0s171/332 [==============>...............] - ETA: 0s205/332 [=================>............] - ETA: 0s230/332 [===================>..........] - ETA: 0s265/332 [======================>.......] - ETA: 0s299/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12866297139846913
cosine 0.1006810416141367
MAE: 0.04842988
RMSE: 0.10544709
r2: 0.2786775218662418
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'logcosh', 64, 80, 0.002, 0.6, 758, 0.006165724713355303, 0.006313768681138754, 0.12866297139846913, 0.1006810416141367, 0.048429880291223526, 0.10544709116220474, 0.2786775218662418, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 85 0.0007 128 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/85
747/747 - 8s - loss: 0.0177 - val_loss: 0.0091 - 8s/epoch - 11ms/step
Epoch 2/85
747/747 - 5s - loss: 0.0082 - val_loss: 0.0083 - 5s/epoch - 6ms/step
Epoch 3/85
747/747 - 5s - loss: 0.0078 - val_loss: 0.0081 - 5s/epoch - 6ms/step
Epoch 4/85
747/747 - 5s - loss: 0.0076 - val_loss: 0.0078 - 5s/epoch - 6ms/step
Epoch 5/85
747/747 - 5s - loss: 0.0073 - val_loss: 0.0073 - 5s/epoch - 6ms/step
Epoch 6/85
747/747 - 5s - loss: 0.0071 - val_loss: 0.0072 - 5s/epoch - 6ms/step
Epoch 7/85
747/747 - 5s - loss: 0.0069 - val_loss: 0.0069 - 5s/epoch - 6ms/step
Epoch 8/85
747/747 - 5s - loss: 0.0069 - val_loss: 0.0069 - 5s/epoch - 6ms/step
Epoch 9/85
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 10/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 11/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 12/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 13/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 14/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 15/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 16/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 17/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 18/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 19/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 20/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 21/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 22/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 23/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 24/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 25/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 26/85
747/747 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 27/85
747/747 - 5s - loss: 0.0066 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 28/85
747/747 - 5s - loss: 0.0066 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 29/85
747/747 - 5s - loss: 0.0066 - val_loss: 0.0065 - 5s/epoch - 6ms/step
Epoch 30/85
747/747 - 5s - loss: 0.0066 - val_loss: 0.0065 - 5s/epoch - 6ms/step
Epoch 31/85
747/747 - 5s - loss: 0.0064 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 32/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 33/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 34/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 7ms/step
Epoch 35/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 36/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 37/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 38/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 39/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 40/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 41/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 42/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 43/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 44/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 45/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 46/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 47/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 48/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 49/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 50/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 51/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 52/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 53/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 54/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 55/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 56/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 57/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 58/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 59/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 60/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 61/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 62/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 63/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 64/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 65/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 66/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 67/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 68/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 69/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 70/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 71/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 72/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 73/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 74/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 75/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 76/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 77/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 78/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 79/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 80/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 81/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 82/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 83/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 84/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 85/85
747/747 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.005970483645796776
  1/332 [..............................] - ETA: 52s 35/332 [==>...........................] - ETA: 0s  70/332 [=====>........................] - ETA: 0s103/332 [========>.....................] - ETA: 0s138/332 [===========>..................] - ETA: 0s159/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s225/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s295/332 [=========================>....] - ETA: 0s328/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12095211950666807
cosine 0.09464513481258673
MAE: 0.04625238
RMSE: 0.098919496
r2: 0.36521936662991916
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'logcosh', 128, 85, 0.0007, 0.6, 758, 0.006025885697454214, 0.005970483645796776, 0.12095211950666807, 0.09464513481258673, 0.046252381056547165, 0.09891949594020844, 0.36521936662991916, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 85 0.0009 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/85
747/747 - 8s - loss: 0.0304 - val_loss: 0.0191 - 8s/epoch - 11ms/step
Epoch 2/85
747/747 - 5s - loss: 0.0159 - val_loss: 0.0169 - 5s/epoch - 6ms/step
Epoch 3/85
747/747 - 5s - loss: 0.0149 - val_loss: 0.0156 - 5s/epoch - 6ms/step
Epoch 4/85
747/747 - 5s - loss: 0.0141 - val_loss: 0.0225 - 5s/epoch - 6ms/step
Epoch 5/85
747/747 - 5s - loss: 0.0135 - val_loss: 0.0150 - 5s/epoch - 6ms/step
Epoch 6/85
747/747 - 5s - loss: 0.0129 - val_loss: 0.0125 - 5s/epoch - 6ms/step
Epoch 7/85
747/747 - 5s - loss: 0.0122 - val_loss: 0.0120 - 5s/epoch - 6ms/step
Epoch 8/85
747/747 - 5s - loss: 0.0119 - val_loss: 0.0117 - 5s/epoch - 6ms/step
Epoch 9/85
747/747 - 5s - loss: 0.0117 - val_loss: 0.0128 - 5s/epoch - 6ms/step
Epoch 10/85
747/747 - 5s - loss: 0.0119 - val_loss: 0.0118 - 5s/epoch - 6ms/step
Epoch 11/85
747/747 - 5s - loss: 0.0122 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 12/85
747/747 - 5s - loss: 0.0114 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 13/85
747/747 - 5s - loss: 0.0113 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 14/85
747/747 - 5s - loss: 0.0112 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 15/85
747/747 - 5s - loss: 0.0112 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 16/85
747/747 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 17/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0110 - 5s/epoch - 6ms/step
Epoch 18/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 19/85
747/747 - 5s - loss: 0.0117 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 20/85
747/747 - 5s - loss: 0.0113 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 21/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 22/85
747/747 - 5s - loss: 0.0115 - val_loss: 0.0118 - 5s/epoch - 6ms/step
Epoch 23/85
747/747 - 5s - loss: 0.0123 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 24/85
747/747 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 25/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 26/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 27/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 28/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0118 - 5s/epoch - 6ms/step
Epoch 29/85
747/747 - 5s - loss: 0.0122 - val_loss: 0.0110 - 5s/epoch - 7ms/step
Epoch 30/85
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 31/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 32/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 33/85
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 34/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 35/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 36/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 37/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 38/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0109 - 5s/epoch - 7ms/step
Epoch 39/85
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 40/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 41/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 42/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 43/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 44/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 45/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 46/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 47/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 48/85
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 49/85
747/747 - 5s - loss: 0.0107 - val_loss: 0.0106 - 5s/epoch - 7ms/step
Epoch 50/85
747/747 - 5s - loss: 0.0107 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 51/85
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 52/85
747/747 - 5s - loss: 0.0107 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 53/85
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 54/85
747/747 - 5s - loss: 0.0106 - val_loss: 0.0105 - 5s/epoch - 7ms/step
Epoch 55/85
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 56/85
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 57/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 58/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 59/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 60/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 61/85
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 62/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 63/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 64/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 65/85
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 66/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 67/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 68/85
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 69/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 70/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 71/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 72/85
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 73/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 74/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 75/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 76/85
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 77/85
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 78/85
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 79/85
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 80/85
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 81/85
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 82/85
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 83/85
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 84/85
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 85/85
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009764961898326874
  1/332 [..............................] - ETA: 51s 33/332 [=>............................] - ETA: 0s  66/332 [====>.........................] - ETA: 0s100/332 [========>.....................] - ETA: 0s136/332 [===========>..................] - ETA: 0s157/332 [=============>................] - ETA: 0s191/332 [================>.............] - ETA: 0s227/332 [===================>..........] - ETA: 0s263/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08276505446460981
cosine 0.06495192625636745
MAE: 0.03824712
RMSE: 0.082469895
r2: 0.5587849607760184
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 128, 85, 0.0009, 0.6, 758, 0.009932070039212704, 0.009764961898326874, 0.08276505446460981, 0.06495192625636745, 0.03824711963534355, 0.0824698954820633, 0.5587849607760184, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 85 0.002 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
2023-02-18 20:21:59.208156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:21:59.208239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24033838102
MaxInUse:                  24033838102
NumAllocs:                   312449101
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:21:59.208330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:21:59.208338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 124
2023-02-18 20:21:59.208343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:21:59.208347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:21:59.208351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 546
2023-02-18 20:21:59.208355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 94
2023-02-18 20:21:59.208359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:21:59.208363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:21:59.208367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 120
2023-02-18 20:21:59.208371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:21:59.208376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:21:59.208380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:21:59.208384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:21:59.208396: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:21:59.208400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:21:59.208404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 94
2023-02-18 20:21:59.208408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:21:59.208412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:21:59.208416: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 48
2023-02-18 20:21:59.208420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:21:59.208424: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:21:59.208427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:21:59.208431: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:21:59.208435: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:21:59.208439: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 32
2023-02-18 20:21:59.208443: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:21:59.208447: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:21:59.208451: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:21:59.208455: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:21:59.208459: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:21:59.208463: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:21:59.208467: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:21:59.208471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:21:59.208475: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:21:59.208479: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 39
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1999999999999997custom_VAE', 'logcosh', 64, 85, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1999999999999997 85 0.002 64 2]) is not valid.
[1.1999999999999997 80 0.0007 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
2023-02-18 20:22:03.454041: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:03.454407: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23550985046
MaxInUse:                  24033838102
NumAllocs:                   312449157
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:03.454477: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:03.454483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 124
2023-02-18 20:22:03.454488: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:03.454492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:03.454496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 546
2023-02-18 20:22:03.454500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 94
2023-02-18 20:22:03.454504: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:03.454508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:03.454512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 120
2023-02-18 20:22:03.454516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:03.454520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:03.454534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:03.454539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:03.454543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:03.454547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:03.454551: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 94
2023-02-18 20:22:03.454555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:03.454559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:03.454563: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 48
2023-02-18 20:22:03.454567: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:03.454571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:03.454575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:03.454579: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:03.454583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:03.454587: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 32
2023-02-18 20:22:03.454591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:03.454595: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:03.454599: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:03.454603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:03.454607: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:03.454611: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:03.454615: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:03.454619: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:03.454623: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:03.454626: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1999999999999997custom_VAE', 'mse', 128, 80, 0.0007, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1999999999999997 80 0.0007 128 1]) is not valid.
[1.2999999999999998 80 0.0009 128 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
2023-02-18 20:22:07.256398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:07.256465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23584934046
MaxInUse:                  24033838102
NumAllocs:                   312449213
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:07.256536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:07.256543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 124
2023-02-18 20:22:07.256547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:07.256551: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:07.256556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 553
2023-02-18 20:22:07.256560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 95
2023-02-18 20:22:07.256564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:07.256568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 216
2023-02-18 20:22:07.256581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 120
2023-02-18 20:22:07.256585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:07.256589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:07.256593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:07.256597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:07.256601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:07.256605: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:07.256609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 95
2023-02-18 20:22:07.256613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:07.256617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 84
2023-02-18 20:22:07.256621: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 48
2023-02-18 20:22:07.256625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:07.256629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:07.256633: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:07.256636: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 56
2023-02-18 20:22:07.256640: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:07.256644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 32
2023-02-18 20:22:07.256648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:07.256652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:07.256656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:07.256660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:07.256664: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:07.256668: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:07.256672: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:07.256676: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:07.256680: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:07.256684: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.2999999999999998custom_VAE', 'mse', 128, 80, 0.0009, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.2999999999999998 80 0.0009 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_3.pkl
[1.4 85 0.0007 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
2023-02-18 20:22:13.766676: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:13.766750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23587359294
MaxInUse:                  24033838102
NumAllocs:                   312449269
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:13.766823: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:13.766830: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 124
2023-02-18 20:22:13.766835: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:13.766839: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:13.766843: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 553
2023-02-18 20:22:13.766847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 95
2023-02-18 20:22:13.766862: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:13.766867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:13.766871: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 130
2023-02-18 20:22:13.766875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:13.766879: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:13.766883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:13.766887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:13.766891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:13.766895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:13.766899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 95
2023-02-18 20:22:13.766903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:13.766907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:13.766911: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 51
2023-02-18 20:22:13.766914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:13.766918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:13.766922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:13.766926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:13.766930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:13.766934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 34
2023-02-18 20:22:13.766938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:13.766942: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:13.766946: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:13.766950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:13.766954: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:13.766958: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:13.766962: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:13.766966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:13.766970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:13.766974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.4custom_VAE', 'mse', 64, 85, 0.0007, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.4 85 0.0007 64 1]) is not valid.
[1.1999999999999997 85 0.002 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
2023-02-18 20:22:17.473768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:17.473840: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23618863798
MaxInUse:                  24033838102
NumAllocs:                   312449325
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:17.473907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:17.473914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 124
2023-02-18 20:22:17.473918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:17.473932: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:17.473937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 560
2023-02-18 20:22:17.473941: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 96
2023-02-18 20:22:17.473945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 152
2023-02-18 20:22:17.473949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:17.473953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 130
2023-02-18 20:22:17.473957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:17.473960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:17.473964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:17.473968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:17.473972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:17.473976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:17.473980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 96
2023-02-18 20:22:17.473984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 60
2023-02-18 20:22:17.473988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:17.473992: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 51
2023-02-18 20:22:17.473996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:17.474000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 73
2023-02-18 20:22:17.474003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:17.474007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:17.474011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:17.474015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 34
2023-02-18 20:22:17.474019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:17.474023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:17.474027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:17.474031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:17.474035: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:17.474039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:17.474043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:17.474047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:17.474050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:17.474054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.1999999999999997custom_VAE', 'mse', 256, 85, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.1999999999999997 85 0.002 256 1]) is not valid.
[2.7 180 0.002 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
2023-02-18 20:22:21.607253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:21.607343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23655358022
MaxInUse:                  24033838102
NumAllocs:                   312449385
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:21.607425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:21.607442: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-18 20:22:21.607448: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:21.607452: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:21.607456: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 560
2023-02-18 20:22:21.607460: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 96
2023-02-18 20:22:21.607464: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:21.607468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:21.607472: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 130
2023-02-18 20:22:21.607476: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:21.607480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:21.607484: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:21.607487: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:21.607491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:21.607495: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:21.607499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:21.607503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 96
2023-02-18 20:22:21.607507: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:21.607511: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:21.607515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 51
2023-02-18 20:22:21.607519: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:21.607523: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:21.607527: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:21.607531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:21.607535: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:21.607539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 34
2023-02-18 20:22:21.607543: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:21.607547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:21.607550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:21.607554: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:21.607558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:21.607562: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:21.607566: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:21.607570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:21.607574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:21.607578: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:21.607582: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:21.607589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.7custom_VAE', 'mse', 128, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.7 180 0.002 128 1]) is not valid.
[1.4 85 0.0007 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
2023-02-18 20:22:25.412854: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:25.412928: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23691732270
MaxInUse:                  24033838102
NumAllocs:                   312449441
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:25.413011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:25.413018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-18 20:22:25.413022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:25.413026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:25.413030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 567
2023-02-18 20:22:25.413034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 97
2023-02-18 20:22:25.413038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:25.413042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:25.413046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:25.413050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:25.413054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:25.413058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:25.413062: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:25.413066: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:25.413070: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:25.413074: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:25.413078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 97
2023-02-18 20:22:25.413082: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:25.413086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:25.413090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:25.413094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:25.413098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:25.413102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:25.413106: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:25.413110: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:25.413114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:25.413118: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:25.413121: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:25.413125: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:25.413129: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:25.413133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:25.413137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:25.413141: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:25.413148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:25.413152: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:25.413156: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:25.413160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:25.413164: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.4custom_VAE', 'mse', 128, 85, 0.0007, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.4 85 0.0007 128 1]) is not valid.
[1.4 85 0.002 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
2023-02-18 20:22:29.512195: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:29.512269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23691732270
MaxInUse:                  24033838102
NumAllocs:                   312449497
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:29.512344: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:29.512350: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-18 20:22:29.512355: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:29.512359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:29.512363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 567
2023-02-18 20:22:29.512367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 97
2023-02-18 20:22:29.512371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:29.512375: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:29.512379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:29.512383: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:29.512387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:29.512391: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:29.512395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:29.512399: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:29.512403: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:29.512407: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:29.512411: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 97
2023-02-18 20:22:29.512415: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:29.512419: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:29.512423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:29.512427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:29.512431: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:29.512435: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:29.512439: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:29.512443: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:29.512447: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:29.512451: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:29.512455: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:29.512468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:29.512472: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:29.512476: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:29.512480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:29.512484: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:29.512488: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:29.512492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:29.512496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:29.512500: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:29.512504: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.4custom_VAE', 'mse', 256, 85, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.4 85 0.002 256 1]) is not valid.
[2.6 180 0.002 128 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:22:33.282753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:33.282815: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23757305734
MaxInUse:                  24033838102
NumAllocs:                   312449553
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:33.282883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:33.282889: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 128
2023-02-18 20:22:33.282894: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:33.282898: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:33.282902: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 574
2023-02-18 20:22:33.282906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 98
2023-02-18 20:22:33.282910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:33.282914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:33.282918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:33.282922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:33.282926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:33.282930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:33.282934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:33.282937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:33.282941: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 32
2023-02-18 20:22:33.282945: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:33.282949: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 98
2023-02-18 20:22:33.282953: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:33.282957: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:33.282961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:33.282965: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:33.282969: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:33.282973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:33.282984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:33.282989: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:33.282993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:33.282997: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:33.283001: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 12
2023-02-18 20:22:33.283005: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:33.283008: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:33.283012: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:33.283016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:33.283020: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:33.283024: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:33.283028: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 8
2023-02-18 20:22:33.283032: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:33.283036: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:33.283040: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'logcosh', 128, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 180 0.002 128 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_4.pkl
[1.5 85 0.0009 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
2023-02-18 20:22:40.512811: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:40.512889: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23730551030
MaxInUse:                  24033838102
NumAllocs:                   312449613
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:40.512970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:40.512978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:22:40.512982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:40.512986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:40.512990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 574
2023-02-18 20:22:40.512994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 98
2023-02-18 20:22:40.512998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:40.513002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:40.513006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:40.513011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:22:40.513015: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:40.513019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:40.513022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:40.513026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:40.513030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:40.513034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:40.513038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:40.513052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 98
2023-02-18 20:22:40.513056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:40.513060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:40.513064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:40.513068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:22:40.513072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:40.513076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:40.513080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:40.513084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:40.513088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:40.513092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:40.513096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:40.513100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:22:40.513104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:40.513108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:40.513112: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:40.513116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:40.513120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:40.513124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:40.513128: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:40.513132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:40.513136: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:40.513139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:40.513143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.5custom_VAE', 'mse', 128, 85, 0.0009, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.5 85 0.0009 128 1]) is not valid.
[2.7 180 0.002 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
2023-02-18 20:22:44.144413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:44.144487: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23798549742
MaxInUse:                  24033838102
NumAllocs:                   312449669
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:44.144558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:44.144565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:22:44.144569: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:44.144573: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:44.144577: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 581
2023-02-18 20:22:44.144581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 99
2023-02-18 20:22:44.144585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:44.144589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:44.144593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:44.144607: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:22:44.144612: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:44.144616: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:44.144620: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:44.144624: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:44.144627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:44.144631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 22
2023-02-18 20:22:44.144635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 20
2023-02-18 20:22:44.144639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 99
2023-02-18 20:22:44.144643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:44.144647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:44.144651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:44.144655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:22:44.144659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:44.144663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:44.144667: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:44.144671: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:44.144675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:44.144679: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:44.144683: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:44.144686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:22:44.144690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 9
2023-02-18 20:22:44.144694: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:44.144698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 6
2023-02-18 20:22:44.144702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:44.144706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:44.144710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:44.144714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:44.144718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 6
2023-02-18 20:22:44.144722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 4
2023-02-18 20:22:44.144726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:44.144730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.7custom_VAE', 'mse', 256, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.7 180 0.002 256 1]) is not valid.
[2.6 180 0.0018 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:22:47.628704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:47.628780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23796124494
MaxInUse:                  24033838102
NumAllocs:                   312449725
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:47.628853: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:47.628870: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:22:47.628875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:47.628879: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:47.628884: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 581
2023-02-18 20:22:47.628888: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 99
2023-02-18 20:22:47.628892: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:47.628896: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:47.628900: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:47.628904: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:22:47.628908: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:47.628912: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:47.628916: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:47.628920: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:47.628924: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:47.628928: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 32
2023-02-18 20:22:47.628932: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:47.628936: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 99
2023-02-18 20:22:47.628940: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:47.628944: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:47.628948: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:47.628952: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:22:47.628956: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:47.628960: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:47.628964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:47.628968: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:47.628972: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:47.628976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:47.628980: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:47.628984: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:22:47.628988: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 12
2023-02-18 20:22:47.628992: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:47.628996: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:47.629000: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:47.629004: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:47.629008: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:47.629012: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:47.629019: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 8
2023-02-18 20:22:47.629023: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:47.629027: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:47.629031: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'mse', 256, 180, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 180 0.0018 256 1]) is not valid.
[2.6 80 0.0009 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:22:51.014239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:51.014305: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23861697958
MaxInUse:                  24033838102
NumAllocs:                   312449781
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:51.014378: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:51.014385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:22:51.014390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:51.014394: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:51.014398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 588
2023-02-18 20:22:51.014402: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 100
2023-02-18 20:22:51.014406: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:51.014410: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:51.014414: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:51.014418: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:22:51.014422: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:51.014426: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:51.014430: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:51.014434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:51.014438: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-18 20:22:51.014442: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:22:51.014446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:51.014450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 100
2023-02-18 20:22:51.014454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:51.014458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:51.014462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:51.014466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:22:51.014470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:51.014474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:51.014478: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:51.014484: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:51.014488: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:51.014492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:51.014496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 9
2023-02-18 20:22:51.014509: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:22:51.014513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:22:51.014517: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:51.014521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:51.014525: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:51.014529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:51.014533: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:51.014537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-18 20:22:51.014540: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:22:51.014544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:51.014548: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:51.014552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'mse', 128, 80, 0.0009, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 80 0.0009 128 1]) is not valid.
[2.5 180 0.002 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
2023-02-18 20:22:54.385530: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:54.385603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23859272710
MaxInUse:                  24033838102
NumAllocs:                   312449837
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:54.385679: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:54.385686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:22:54.385690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:54.385694: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:54.385698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 588
2023-02-18 20:22:54.385702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 100
2023-02-18 20:22:54.385706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:54.385710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:54.385715: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:54.385719: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:22:54.385723: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:54.385727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:54.385731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:54.385735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:54.385739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 32
2023-02-18 20:22:54.385743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 32
2023-02-18 20:22:54.385747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:54.385751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 100
2023-02-18 20:22:54.385755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:54.385759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:54.385763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:54.385776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:22:54.385781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:54.385785: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:54.385789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:54.385793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:54.385797: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:54.385800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:54.385804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 12
2023-02-18 20:22:54.385808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:22:54.385812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 12
2023-02-18 20:22:54.385816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:54.385820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:54.385824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:54.385828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:54.385832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:54.385836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 8
2023-02-18 20:22:54.385840: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 8
2023-02-18 20:22:54.385844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:54.385848: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:54.385851: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.5custom_VAE', 'mse', 256, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.5 180 0.002 256 1]) is not valid.
[2.5 180 0.0018 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
2023-02-18 20:22:57.762267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 310050816/25447170048
2023-02-18 20:22:57.762652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23922420926
MaxInUse:                  24033838102
NumAllocs:                   312449893
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:22:57.762732: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:22:57.762739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:22:57.762744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:22:57.762748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:22:57.762752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 595
2023-02-18 20:22:57.762756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 101
2023-02-18 20:22:57.762760: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:22:57.762764: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:22:57.762768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:22:57.762772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:22:57.762776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:22:57.762780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:22:57.762784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:22:57.762800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:22:57.762804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:22:57.762808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 32
2023-02-18 20:22:57.762812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:22:57.762816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 101
2023-02-18 20:22:57.762820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:22:57.762824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:22:57.762828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:22:57.762832: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:22:57.762836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:22:57.762840: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:22:57.762844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:22:57.762848: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:22:57.762852: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:22:57.762856: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:22:57.762860: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:22:57.762864: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:22:57.762867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 12
2023-02-18 20:22:57.762871: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:22:57.762875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:22:57.762879: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:22:57.762883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:22:57.762887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:22:57.762891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:22:57.762895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 8
2023-02-18 20:22:57.762899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:22:57.762903: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:22:57.762907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.5custom_VAE', 'mse', 256, 180, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.5 180 0.0018 256 1]) is not valid.
[2.5 85 0.0009 128 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
2023-02-18 20:23:01.261964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 276496384/25447170048
2023-02-18 20:23:01.262039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23922420926
MaxInUse:                  24033838102
NumAllocs:                   312449949
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:01.262113: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:01.262120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:01.262124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:01.262128: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:01.262132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 595
2023-02-18 20:23:01.262146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 101
2023-02-18 20:23:01.262151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:01.262155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:01.262159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:01.262163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:23:01.262167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:01.262171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:01.262175: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:01.262179: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:01.262183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:01.262187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 32
2023-02-18 20:23:01.262191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:01.262195: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 101
2023-02-18 20:23:01.262199: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:01.262203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:01.262207: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:01.262211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:23:01.262215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:01.262219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:01.262223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:01.262227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:01.262231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:01.262235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:01.262239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:01.262243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:23:01.262247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 12
2023-02-18 20:23:01.262251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:01.262254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:01.262258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:01.262262: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:01.262266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:01.262270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:01.262274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 8
2023-02-18 20:23:01.262278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:01.262282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:01.262286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.5custom_VAE', 'mse', 128, 85, 0.0009, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.5 85 0.0009 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_5.pkl
[2.6 180 0.0018 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:23:07.160886: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 242941952/25447170048
2023-02-18 20:23:07.160962: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23987994390
MaxInUse:                  24033838102
NumAllocs:                   312450005
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:07.161047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:07.161054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:07.161058: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:07.161063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:07.161067: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 602
2023-02-18 20:23:07.161071: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 102
2023-02-18 20:23:07.161075: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:07.161079: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:07.161083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:07.161087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 10
2023-02-18 20:23:07.161091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:07.161095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:07.161099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:07.161103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:07.161107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:07.161111: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:23:07.161115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:07.161119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 102
2023-02-18 20:23:07.161123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:07.161127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:07.161131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:07.161135: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 3
2023-02-18 20:23:07.161139: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:07.161143: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:07.161147: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:07.161151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:07.161155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:07.161159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:07.161163: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:07.161167: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 2
2023-02-18 20:23:07.161171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:23:07.161175: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:07.161179: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:07.161185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:07.161189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:07.161193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:07.161197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:07.161201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:23:07.161205: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:07.161209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:07.161213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'mse', 64, 180, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 180 0.0018 64 1]) is not valid.
[1.5 180 0.0007 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
2023-02-18 20:23:10.276168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 242941952/25447170048
2023-02-18 20:23:10.276225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     23961239670
MaxInUse:                  24033838102
NumAllocs:                   312450061
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:10.276304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:10.276310: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:10.276315: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:10.276319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:10.276323: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 602
2023-02-18 20:23:10.276327: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 102
2023-02-18 20:23:10.276331: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:10.276335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:10.276339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:10.276343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 20
2023-02-18 20:23:10.276347: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:10.276351: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:10.276354: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:10.276358: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:10.276362: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:10.276366: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 32
2023-02-18 20:23:10.276370: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:10.276374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 102
2023-02-18 20:23:10.276378: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:10.276382: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:10.276386: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:10.276390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-18 20:23:10.276394: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:10.276398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:10.276402: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:10.276414: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:10.276419: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:10.276423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:10.276427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:10.276431: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 4
2023-02-18 20:23:10.276435: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 12
2023-02-18 20:23:10.276439: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:10.276443: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:10.276447: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:10.276451: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:10.276454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:10.276458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:10.276462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 8
2023-02-18 20:23:10.276466: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:10.276470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:10.276474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.5custom_VAE', 'mse', 64, 180, 0.0007, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.5 180 0.0007 64 1]) is not valid.
[1.6 175 0.0005 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
2023-02-18 20:23:13.433637: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 242941952/25447170048
2023-02-18 20:23:13.433696: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24002483662
MaxInUse:                  24033838102
NumAllocs:                   312450117
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:13.433769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:13.433776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:13.433780: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:13.433784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:13.433788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 609
2023-02-18 20:23:13.433793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 103
2023-02-18 20:23:13.433797: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:13.433801: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:13.433805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:13.433809: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 20
2023-02-18 20:23:13.433814: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 42
2023-02-18 20:23:13.433819: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:13.433823: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:13.433827: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:13.433831: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:13.433835: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 32
2023-02-18 20:23:13.433838: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:13.433851: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 103
2023-02-18 20:23:13.433855: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:13.433859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:13.433863: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:13.433867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-18 20:23:13.433871: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 15
2023-02-18 20:23:13.433875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:13.433879: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:13.433883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:13.433887: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:13.433891: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:13.433895: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:13.433899: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 4
2023-02-18 20:23:13.433902: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 12
2023-02-18 20:23:13.433906: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 10
2023-02-18 20:23:13.433910: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:13.433914: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:13.433918: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:13.433922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:13.433926: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:13.433930: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 8
2023-02-18 20:23:13.433934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:13.433938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:13.433942: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.6custom_VAE', 'mse', 256, 175, 0.0005, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.6 175 0.0005 256 1]) is not valid.
[2.6 175 0.002 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:23:16.620808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 209387520/25447170048
2023-02-18 20:23:16.621084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24026813134
MaxInUse:                  24060036126
NumAllocs:                   312450173
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:16.621162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:16.621168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:16.621172: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:16.621176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:16.621180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 609
2023-02-18 20:23:16.621184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 103
2023-02-18 20:23:16.621188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:16.621192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:16.621196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:16.621210: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 20
2023-02-18 20:23:16.621214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:16.621218: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:16.621222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:16.621226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:16.621230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:16.621234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:23:16.621238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:16.621242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 103
2023-02-18 20:23:16.621246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:16.621250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:16.621254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:16.621258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-18 20:23:16.621261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:16.621265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:16.621269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:16.621273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:16.621277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:16.621281: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:16.621285: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:16.621289: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 4
2023-02-18 20:23:16.621293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:23:16.621305: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:16.621309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:16.621313: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:16.621317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:16.621321: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:16.621325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:16.621329: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:23:16.621332: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:16.621336: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:16.621340: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'mse', 256, 175, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 175 0.002 256 1]) is not valid.
[2.5 180 0.0022 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
2023-02-18 20:23:19.808025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 142278656/25447170048
2023-02-18 20:23:19.808093: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24089961350
MaxInUse:                  24121910230
NumAllocs:                   312450229
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:19.808168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:19.808183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:19.808188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:19.808192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:19.808196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 616
2023-02-18 20:23:19.808200: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 104
2023-02-18 20:23:19.808204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:19.808208: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:19.808212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:19.808216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 20
2023-02-18 20:23:19.808219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:19.808223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:19.808227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:19.808231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:19.808235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 52
2023-02-18 20:23:19.808239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:23:19.808243: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:19.808247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 104
2023-02-18 20:23:19.808251: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:19.808255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:19.808259: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:19.808263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 6
2023-02-18 20:23:19.808267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:19.808271: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:19.808274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:19.808278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:19.808282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:19.808286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:19.808290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 18
2023-02-18 20:23:19.808294: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 4
2023-02-18 20:23:19.808304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:23:19.808308: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:19.808312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:19.808316: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:19.808320: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:19.808324: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:19.808328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-18 20:23:19.808335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:23:19.808339: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:19.808343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:19.808346: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.5custom_VAE', 'mse', 256, 180, 0.0022, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.5 180 0.0022 256 1]) is not valid.
[1.5 175 0.0018 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
2023-02-18 20:23:25.766964: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 142278656/25447170048
2023-02-18 20:23:25.767039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24065631878
MaxInUse:                  24122583182
NumAllocs:                   312450285
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:25.767117: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:25.767124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:25.767128: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:25.767133: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:25.767137: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 616
2023-02-18 20:23:25.767141: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 104
2023-02-18 20:23:25.767145: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:25.767149: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:25.767153: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:25.767157: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 30
2023-02-18 20:23:25.767161: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:25.767165: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:25.767169: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:25.767173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:25.767177: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:25.767181: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:23:25.767185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:25.767189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 104
2023-02-18 20:23:25.767193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:25.767197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:25.767201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:25.767205: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 9
2023-02-18 20:23:25.767209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:25.767213: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:25.767217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:25.767223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:25.767227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:25.767231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:25.767235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:25.767249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-18 20:23:25.767253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:23:25.767257: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:25.767261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:25.767265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:25.767269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:25.767273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:25.767277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:25.767280: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:23:25.767284: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:25.767288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:25.767292: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.5custom_VAE', 'mse', 256, 175, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.5 175 0.0018 256 1]) is not valid.
[2.6 180 0.0022 256 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:23:29.004223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 108724224/25447170048
2023-02-18 20:23:29.004296: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24131205342
MaxInUse:                  24164428334
NumAllocs:                   312450341
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:29.004379: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:29.004385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:29.004390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:29.004394: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:29.004398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 623
2023-02-18 20:23:29.004402: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 105
2023-02-18 20:23:29.004406: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:29.004410: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:29.004414: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:29.004418: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 30
2023-02-18 20:23:29.004422: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:29.004426: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:29.004430: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:29.004434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:29.004437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:29.004441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 52
2023-02-18 20:23:29.004445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:29.004449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 105
2023-02-18 20:23:29.004453: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:29.004457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:29.004461: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:29.004474: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 9
2023-02-18 20:23:29.004479: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:29.004483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:29.004487: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:29.004491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:29.004495: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:29.004499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:29.004502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:29.004506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 6
2023-02-18 20:23:29.004510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 18
2023-02-18 20:23:29.004514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:29.004518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:29.004522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:29.004526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:29.004530: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:29.004534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:29.004538: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 12
2023-02-18 20:23:29.004542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:29.004546: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:29.004550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'logcosh', 256, 180, 0.0022, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 180 0.0022 256 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_6.pkl
[1.5 180 0.0018 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
2023-02-18 20:23:34.959516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 108724224/25447170048
2023-02-18 20:23:34.959892: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24104450622
MaxInUse:                  24164428334
NumAllocs:                   312450397
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:34.959974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:34.959982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:34.959986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:34.959990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:34.959994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 623
2023-02-18 20:23:34.959998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 105
2023-02-18 20:23:34.960002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:34.960006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:34.960010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:34.960014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 40
2023-02-18 20:23:34.960018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:34.960022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:34.960026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:34.960043: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:34.960048: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:34.960052: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:23:34.960056: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:34.960060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 105
2023-02-18 20:23:34.960064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:34.960068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:34.960072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:34.960076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 12
2023-02-18 20:23:34.960080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:34.960084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:34.960088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:34.960092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:34.960096: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:34.960100: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:34.960104: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:34.960108: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 8
2023-02-18 20:23:34.960112: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:23:34.960116: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:34.960120: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:34.960124: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:34.960128: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:34.960132: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:34.960136: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:34.960140: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:23:34.960144: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:34.960148: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:34.960153: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.5custom_VAE', 'mse', 256, 180, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.5 180 0.0018 256 1]) is not valid.
[2.6 180 0.002 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:23:38.156103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 75169792/25447170048
2023-02-18 20:23:38.156173: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24170024086
MaxInUse:                  24203247078
NumAllocs:                   312450453
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:38.156249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:38.156256: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:38.156260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:38.156264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:38.156277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 630
2023-02-18 20:23:38.156282: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 106
2023-02-18 20:23:38.156286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:38.156290: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:38.156294: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:38.156302: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 40
2023-02-18 20:23:38.156306: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:38.156310: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:38.156314: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:38.156318: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:38.156322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:38.156326: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 52
2023-02-18 20:23:38.156330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 10
2023-02-18 20:23:38.156334: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 106
2023-02-18 20:23:38.156338: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:38.156342: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:38.156346: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:38.156350: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 12
2023-02-18 20:23:38.156354: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:38.156358: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:38.156362: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:38.156365: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:38.156369: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:38.156373: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:38.156377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:38.156381: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 8
2023-02-18 20:23:38.156385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 18
2023-02-18 20:23:38.156389: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:38.156393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 3
2023-02-18 20:23:38.156397: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:38.156401: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:38.156405: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:38.156409: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:38.156413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 12
2023-02-18 20:23:38.156417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 2
2023-02-18 20:23:38.156421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:38.156425: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 180 0.002 256 2]) is not valid.
[2.7 175 0.002 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
2023-02-18 20:23:41.328913: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 75169792/25447170048
2023-02-18 20:23:41.328983: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24172449334
MaxInUse:                  24228727182
NumAllocs:                   312450509
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:41.329072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:41.329078: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:41.329083: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:41.329087: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:41.329091: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 630
2023-02-18 20:23:41.329095: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 106
2023-02-18 20:23:41.329099: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:41.329103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:41.329107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:41.329111: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 40
2023-02-18 20:23:41.329115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:41.329119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:41.329123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:41.329126: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:41.329130: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:41.329134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:23:41.329138: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 20
2023-02-18 20:23:41.329142: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 106
2023-02-18 20:23:41.329146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:41.329150: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:41.329154: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:41.329158: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 12
2023-02-18 20:23:41.329162: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:41.329166: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:41.329170: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:41.329174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:41.329178: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:41.329181: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:41.329185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:41.329189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 8
2023-02-18 20:23:41.329193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:23:41.329197: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:41.329201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 6
2023-02-18 20:23:41.329208: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:41.329212: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:41.329216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:41.329220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:41.329224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:23:41.329228: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 4
2023-02-18 20:23:41.329232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:41.329236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.7custom_VAE', 'mse', 256, 175, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.7 175 0.002 256 1]) is not valid.
[1.5 180 0.0005 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
2023-02-18 20:23:44.489253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 75169792/25447170048
2023-02-18 20:23:44.489329: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24211268078
MaxInUse:                  24230435390
NumAllocs:                   312450565
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:44.489420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:44.489428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:44.489434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:44.489439: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:44.489444: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 637
2023-02-18 20:23:44.489450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 107
2023-02-18 20:23:44.489455: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:44.489459: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:44.489463: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:44.489467: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 50
2023-02-18 20:23:44.489471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:44.489475: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:44.489479: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:44.489483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:44.489487: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:44.489491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 42
2023-02-18 20:23:44.489494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 20
2023-02-18 20:23:44.489498: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 107
2023-02-18 20:23:44.489502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:44.489506: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:44.489510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:44.489514: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 15
2023-02-18 20:23:44.489518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:44.489522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:44.489526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:44.489541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:44.489545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:44.489549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:44.489553: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:44.489557: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 10
2023-02-18 20:23:44.489561: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 15
2023-02-18 20:23:44.489565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:44.489569: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 6
2023-02-18 20:23:44.489573: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:44.489577: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:44.489581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:44.489585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:44.489589: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 10
2023-02-18 20:23:44.489593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 4
2023-02-18 20:23:44.489597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:44.489601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.5custom_VAE', 'mse', 128, 180, 0.0005, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.5 180 0.0005 128 1]) is not valid.
[2.6 180 0.002 256 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-18 20:23:47.795258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 8060928/25447170048
2023-02-18 20:23:47.795336: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24238022798
MaxInUse:                  24271245790
NumAllocs:                   312450621
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:47.795414: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:47.795420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 132
2023-02-18 20:23:47.795424: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 103
2023-02-18 20:23:47.795428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:47.795432: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 637
2023-02-18 20:23:47.795436: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 107
2023-02-18 20:23:47.795440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:47.795444: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:47.795448: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:47.795452: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 40
2023-02-18 20:23:47.795456: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:47.795460: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:47.795464: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:47.795468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:47.795472: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:47.795476: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 52
2023-02-18 20:23:47.795480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 20
2023-02-18 20:23:47.795492: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 107
2023-02-18 20:23:47.795496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:47.795501: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:47.795504: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:47.795508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 12
2023-02-18 20:23:47.795512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:47.795516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:47.795520: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:47.795524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:47.795528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:47.795532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:47.795536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:47.795540: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 8
2023-02-18 20:23:47.795544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 18
2023-02-18 20:23:47.795548: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:47.795552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 6
2023-02-18 20:23:47.795556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:47.795560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:47.795563: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:47.795567: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:47.795571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 12
2023-02-18 20:23:47.795575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 4
2023-02-18 20:23:47.795579: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:47.795583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 180 0.002 256 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_7.pkl
[2.6 180 0.0022 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-18 20:23:53.766744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 16614016 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 8060928/25447170048
2023-02-18 20:23:53.766821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23374528512
InUse:                     24286977206
MaxInUse:                  24306837790
NumAllocs:                   312450670
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-18 20:23:53.766922: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-18 20:23:53.766929: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 130
2023-02-18 20:23:53.766934: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 104
2023-02-18 20:23:53.766938: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-18 20:23:53.766942: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-18 20:23:53.766946: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 644
2023-02-18 20:23:53.766950: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 107
2023-02-18 20:23:53.766954: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 142
2023-02-18 20:23:53.766958: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 206
2023-02-18 20:23:53.766962: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 140
2023-02-18 20:23:53.766966: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 40
2023-02-18 20:23:53.766970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 32
2023-02-18 20:23:53.766974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 86
2023-02-18 20:23:53.766978: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 54
2023-02-18 20:23:53.766982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11120, 32
2023-02-18 20:23:53.766986: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-18 20:23:53.766990: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 62
2023-02-18 20:23:53.766994: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 20
2023-02-18 20:23:53.766998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 108
2023-02-18 20:23:53.767002: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 57
2023-02-18 20:23:53.767006: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 81
2023-02-18 20:23:53.767010: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 54
2023-02-18 20:23:53.767014: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 12
2023-02-18 20:23:53.767018: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 12
2023-02-18 20:23:53.767022: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 71
2023-02-18 20:23:53.767026: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 21
2023-02-18 20:23:53.767030: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 54
2023-02-18 20:23:53.767034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8428960, 12
2023-02-18 20:23:53.767038: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 36
2023-02-18 20:23:53.767042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-18 20:23:53.767046: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 8
2023-02-18 20:23:53.767050: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 21
2023-02-18 20:23:53.767054: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 8
2023-02-18 20:23:53.767060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 6
2023-02-18 20:23:53.767064: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 22
2023-02-18 20:23:53.767068: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 14
2023-02-18 20:23:53.767072: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14055680, 8
2023-02-18 20:23:53.767076: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-18 20:23:53.767080: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 13
2023-02-18 20:23:53.767084: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 4
2023-02-18 20:23:53.767088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 26
2023-02-18 20:23:53.767092: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 38
2023-02-18 20:23:53.767118: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at stateless_random_ops_v2.cc:67 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[3286,1264] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0
Traceback (most recent call last):
  File "genetic.py", line 44, in <module>
    ga=continue_run_ga('tmp/ga_instance_generation_2.pkl')
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 249, in continue_run_ga
    ga_instance.run()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1409, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 352, in train_autoencoder
    model = create_autoencoder(n_inputs=n_inputs, layers_structure=layers_structure, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 585, in create_autoencoder
    decoder_output = Dense(n_inputs, activation='linear', name='outputlayer')(d)
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[3286,1264] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0 [Op:StatelessRandomUniformV2]
Sat Feb 18 20:24:31 CET 2023
done
