start
Sun Feb 19 02:23:56 CET 2023
2023-02-19 02:23:59.854724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-19 02:23:59.966485: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-19 02:24:40,898 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f5c161a52b0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
[1.5 180 0.0018 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-19 02:24:48.180078: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-19 02:24:48.558394: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-19 02:24:48.558509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20633 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:0f:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/180
2023-02-19 02:24:51.318788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-02-19 02:24:51.377341: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x559242da5b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-19 02:24:51.377360: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6
2023-02-19 02:24:51.381727: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-19 02:24:51.495412: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
374/374 - 5s - loss: 0.0406 - val_loss: 0.0242 - 5s/epoch - 12ms/step
Epoch 2/180
374/374 - 1s - loss: 0.0167 - val_loss: 0.0202 - 1s/epoch - 4ms/step
Epoch 3/180
374/374 - 1s - loss: 0.0153 - val_loss: 0.0204 - 1s/epoch - 4ms/step
Epoch 4/180
374/374 - 1s - loss: 0.0150 - val_loss: 0.0265 - 1s/epoch - 4ms/step
Epoch 5/180
374/374 - 1s - loss: 0.0144 - val_loss: 0.0198 - 1s/epoch - 4ms/step
Epoch 6/180
374/374 - 1s - loss: 0.0141 - val_loss: 0.0499 - 1s/epoch - 4ms/step
Epoch 7/180
374/374 - 1s - loss: 0.0141 - val_loss: 0.0151 - 1s/epoch - 4ms/step
Epoch 8/180
374/374 - 1s - loss: 0.0135 - val_loss: 0.0150 - 1s/epoch - 4ms/step
Epoch 9/180
374/374 - 1s - loss: 0.0133 - val_loss: 0.0168 - 1s/epoch - 4ms/step
Epoch 10/180
374/374 - 1s - loss: 0.0131 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 11/180
374/374 - 1s - loss: 0.0129 - val_loss: 0.0135 - 1s/epoch - 4ms/step
Epoch 12/180
374/374 - 1s - loss: 0.0127 - val_loss: 0.0138 - 1s/epoch - 4ms/step
Epoch 13/180
374/374 - 1s - loss: 0.0126 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 14/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0141 - 1s/epoch - 4ms/step
Epoch 15/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0161 - 1s/epoch - 4ms/step
Epoch 16/180
374/374 - 1s - loss: 0.0127 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 17/180
374/374 - 1s - loss: 0.0123 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 18/180
374/374 - 1s - loss: 0.0122 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 19/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 20/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 21/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 22/180
374/374 - 1s - loss: 0.0117 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 23/180
374/374 - 1s - loss: 0.0117 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 24/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 25/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 26/180
374/374 - 1s - loss: 0.0126 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 27/180
374/374 - 1s - loss: 0.0134 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 28/180
374/374 - 1s - loss: 0.0129 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 29/180
374/374 - 1s - loss: 0.0150 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 30/180
374/374 - 1s - loss: 0.0117 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 31/180
374/374 - 1s - loss: 0.0115 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 32/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 33/180
374/374 - 1s - loss: 0.0113 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 34/180
374/374 - 1s - loss: 0.0113 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 35/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 36/180
374/374 - 1s - loss: 0.0135 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 37/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 38/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 39/180
374/374 - 1s - loss: 0.0113 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 40/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 41/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 42/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 43/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 44/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 45/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 46/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 47/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 48/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 49/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 50/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 51/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 52/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 53/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 54/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 55/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 56/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 57/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 58/180
374/374 - 1s - loss: 0.0113 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 59/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 60/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 61/180
374/374 - 1s - loss: 0.0138 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 62/180
374/374 - 1s - loss: 0.0116 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 63/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 64/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 65/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 66/180
374/374 - 1s - loss: 0.0129 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 67/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 68/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 69/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 70/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 71/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 72/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 73/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 74/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 75/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 76/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 77/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 78/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 79/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 80/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 81/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 82/180
374/374 - 1s - loss: 0.0157 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 83/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 84/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 85/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 86/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 87/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 88/180
374/374 - 1s - loss: 0.0144 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 89/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 90/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 91/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 92/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 93/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 94/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 95/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 96/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 97/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 98/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 99/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 100/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 101/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 102/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 103/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 104/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 105/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 106/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 107/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 108/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 109/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 110/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 111/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 112/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 113/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 114/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 115/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 116/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 117/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 118/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 119/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 120/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 121/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 122/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 123/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 124/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 125/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 126/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 127/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 128/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 129/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 130/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 131/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 132/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 133/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 134/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 135/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 136/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 137/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 138/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 139/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 140/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 141/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 142/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 143/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 144/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 145/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 146/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 147/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 148/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 149/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 150/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 151/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 152/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 153/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 154/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 155/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 156/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 157/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 158/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 159/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 160/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 161/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 162/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 163/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 164/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 165/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 166/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 167/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 168/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 169/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 170/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 171/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 172/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 173/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 174/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 175/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 176/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 177/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 178/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 179/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 180/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009694327600300312
  1/332 [..............................] - ETA: 45s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08261211295645506
cosine 0.06482042704736607
MAE: 0.03792697
RMSE: 0.082402214
r2: 0.5595081696242017
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 256, 180, 0.0018, 0.6, 758, 0.009822667576372623, 0.009694327600300312, 0.08261211295645506, 0.06482042704736607, 0.037926968187093735, 0.08240221440792084, 0.5595081696242017, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 180 0.002 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 4s - loss: 0.0320 - val_loss: 0.0680 - 4s/epoch - 11ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0172 - 2s/epoch - 5ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0088 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0083 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0075 - val_loss: 0.0089 - 2s/epoch - 5ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0072 - val_loss: 0.0084 - 2s/epoch - 5ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0070 - val_loss: 0.0077 - 2s/epoch - 5ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0068 - val_loss: 0.0257 - 2s/epoch - 5ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0071 - val_loss: 0.0069 - 2s/epoch - 5ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0068 - 2s/epoch - 5ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0078 - 2s/epoch - 5ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0085 - 2s/epoch - 5ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0067 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0068 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0073 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0077 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0067 - 2s/epoch - 5ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0074 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0071 - 2s/epoch - 5ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0076 - val_loss: 0.0073 - 2s/epoch - 5ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0068 - 2s/epoch - 5ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0068 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0067 - 2s/epoch - 5ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0070 - 2s/epoch - 5ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0070 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0088 - 2s/epoch - 5ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0095 - val_loss: 0.0068 - 2s/epoch - 5ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0068 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0073 - 2s/epoch - 5ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0085 - 2s/epoch - 5ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0079 - val_loss: 0.0067 - 2s/epoch - 5ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0071 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 81/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 82/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 84/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 85/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 87/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0087 - 2s/epoch - 5ms/step
Epoch 88/180
374/374 - 2s - loss: 0.0092 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 89/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 93/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0067 - 2s/epoch - 5ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0067 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 111/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 112/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 136/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 140/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 162/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 166/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 172/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 173/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 180/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0060224831104278564
  1/332 [..............................] - ETA: 38s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12601650496990421
cosine 0.09862155181661535
MAE: 0.047477145
RMSE: 0.10090202
r2: 0.33951984136566676
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, 0.006080062128603458, 0.0060224831104278564, 0.12601650496990421, 0.09862155181661535, 0.04747714474797249, 0.1009020209312439, 0.33951984136566676, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 175 0.002 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 4s - loss: 0.0537 - val_loss: 0.0385 - 4s/epoch - 11ms/step
Epoch 2/175
374/374 - 2s - loss: 0.0188 - val_loss: 0.0566 - 2s/epoch - 5ms/step
Epoch 3/175
374/374 - 2s - loss: 0.0173 - val_loss: 0.0280 - 2s/epoch - 5ms/step
Epoch 4/175
374/374 - 2s - loss: 0.0154 - val_loss: 0.0237 - 2s/epoch - 5ms/step
Epoch 5/175
374/374 - 2s - loss: 0.0146 - val_loss: 0.0211 - 2s/epoch - 5ms/step
Epoch 6/175
374/374 - 2s - loss: 0.0142 - val_loss: 0.0354 - 2s/epoch - 5ms/step
Epoch 7/175
374/374 - 2s - loss: 0.0142 - val_loss: 0.0549 - 2s/epoch - 5ms/step
Epoch 8/175
374/374 - 2s - loss: 0.0139 - val_loss: 0.0157 - 2s/epoch - 5ms/step
Epoch 9/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0176 - 2s/epoch - 5ms/step
Epoch 10/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.1134 - 2s/epoch - 5ms/step
Epoch 11/175
374/374 - 2s - loss: 0.0146 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 12/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0147 - 2s/epoch - 5ms/step
Epoch 13/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0177 - 2s/epoch - 5ms/step
Epoch 14/175
374/374 - 2s - loss: 0.0139 - val_loss: 0.0145 - 2s/epoch - 5ms/step
Epoch 15/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 16/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 17/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 18/175
374/374 - 2s - loss: 0.0157 - val_loss: 0.0148 - 2s/epoch - 5ms/step
Epoch 19/175
374/374 - 2s - loss: 0.0189 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 20/175
374/374 - 2s - loss: 0.0250 - val_loss: 0.0159 - 2s/epoch - 5ms/step
Epoch 21/175
374/374 - 2s - loss: 0.0315 - val_loss: 0.0183 - 2s/epoch - 5ms/step
Epoch 22/175
374/374 - 2s - loss: 0.0364 - val_loss: 0.0161 - 2s/epoch - 5ms/step
Epoch 23/175
374/374 - 2s - loss: 0.0166 - val_loss: 0.0149 - 2s/epoch - 5ms/step
Epoch 24/175
374/374 - 2s - loss: 0.0166 - val_loss: 0.0141 - 2s/epoch - 5ms/step
Epoch 25/175
374/374 - 2s - loss: 0.0147 - val_loss: 0.0161 - 2s/epoch - 5ms/step
Epoch 26/175
374/374 - 2s - loss: 0.0210 - val_loss: 0.0143 - 2s/epoch - 5ms/step
Epoch 27/175
374/374 - 2s - loss: 0.0145 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 28/175
374/374 - 2s - loss: 0.0137 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 29/175
374/374 - 2s - loss: 0.0133 - val_loss: 0.0128 - 2s/epoch - 5ms/step
Epoch 30/175
374/374 - 2s - loss: 0.0130 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 31/175
374/374 - 2s - loss: 0.0129 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 32/175
374/374 - 2s - loss: 0.0128 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 33/175
374/374 - 2s - loss: 0.0126 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 34/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 35/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 36/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 37/175
374/374 - 2s - loss: 0.0129 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 38/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0222 - 2s/epoch - 5ms/step
Epoch 39/175
374/374 - 2s - loss: 0.0257 - val_loss: 0.0159 - 2s/epoch - 5ms/step
Epoch 40/175
374/374 - 2s - loss: 0.0243 - val_loss: 0.0133 - 2s/epoch - 5ms/step
Epoch 41/175
374/374 - 2s - loss: 0.0132 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 42/175
374/374 - 2s - loss: 0.0170 - val_loss: 0.0155 - 2s/epoch - 5ms/step
Epoch 43/175
374/374 - 2s - loss: 0.0370 - val_loss: 0.0147 - 2s/epoch - 5ms/step
Epoch 44/175
374/374 - 2s - loss: 0.0153 - val_loss: 0.0138 - 2s/epoch - 5ms/step
Epoch 45/175
374/374 - 2s - loss: 0.0140 - val_loss: 0.0132 - 2s/epoch - 5ms/step
Epoch 46/175
374/374 - 2s - loss: 0.0132 - val_loss: 0.0133 - 2s/epoch - 5ms/step
Epoch 47/175
374/374 - 2s - loss: 0.0135 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 48/175
374/374 - 2s - loss: 0.0139 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 49/175
374/374 - 2s - loss: 0.0139 - val_loss: 0.0136 - 2s/epoch - 5ms/step
Epoch 50/175
374/374 - 2s - loss: 0.0129 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 51/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 52/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 53/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 54/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 55/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 56/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 57/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 58/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 59/175
374/374 - 2s - loss: 0.0233 - val_loss: 0.0180 - 2s/epoch - 5ms/step
Epoch 60/175
374/374 - 2s - loss: 0.0466 - val_loss: 0.0147 - 2s/epoch - 5ms/step
Epoch 61/175
374/374 - 2s - loss: 0.0144 - val_loss: 0.0156 - 2s/epoch - 5ms/step
Epoch 62/175
374/374 - 2s - loss: 0.0166 - val_loss: 0.0136 - 2s/epoch - 5ms/step
Epoch 63/175
374/374 - 2s - loss: 0.0142 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 64/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 65/175
374/374 - 2s - loss: 0.0129 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 66/175
374/374 - 2s - loss: 0.0127 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 67/175
374/374 - 2s - loss: 0.0128 - val_loss: 0.0139 - 2s/epoch - 5ms/step
Epoch 68/175
374/374 - 2s - loss: 0.0229 - val_loss: 0.0158 - 2s/epoch - 5ms/step
Epoch 69/175
374/374 - 2s - loss: 0.0231 - val_loss: 0.0142 - 2s/epoch - 5ms/step
Epoch 70/175
374/374 - 2s - loss: 0.0149 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 71/175
374/374 - 2s - loss: 0.0134 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 72/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 73/175
374/374 - 2s - loss: 0.0129 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 74/175
374/374 - 2s - loss: 0.0129 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 75/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 76/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 77/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 78/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 79/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0144 - 2s/epoch - 5ms/step
Epoch 80/175
374/374 - 2s - loss: 0.0158 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 81/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 82/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 83/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 84/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 85/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 86/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 87/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 88/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0167 - 2s/epoch - 5ms/step
Epoch 89/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 90/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 91/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 92/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 93/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 94/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0179 - 2s/epoch - 5ms/step
Epoch 95/175
374/374 - 2s - loss: 0.0130 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 96/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 97/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 98/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 99/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 100/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 101/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0502 - 2s/epoch - 5ms/step
Epoch 102/175
374/374 - 2s - loss: 0.0246 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 103/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 104/175
374/374 - 2s - loss: 0.0136 - val_loss: 0.0161 - 2s/epoch - 5ms/step
Epoch 105/175
374/374 - 2s - loss: 0.0163 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 106/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 107/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 108/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 109/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 110/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 111/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 112/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0189 - 2s/epoch - 5ms/step
Epoch 113/175
374/374 - 2s - loss: 0.0205 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 114/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 115/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 116/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 117/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0128 - 2s/epoch - 5ms/step
Epoch 118/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 119/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 120/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 121/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0167 - 2s/epoch - 5ms/step
Epoch 122/175
374/374 - 2s - loss: 0.0130 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 123/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 124/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 125/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0151 - 2s/epoch - 5ms/step
Epoch 126/175
374/374 - 2s - loss: 0.0139 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 127/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0158 - 2s/epoch - 5ms/step
Epoch 128/175
374/374 - 2s - loss: 0.0156 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 129/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 130/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 131/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 132/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 133/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 134/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 135/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 136/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 137/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 138/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 139/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 140/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 141/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 142/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0147 - 2s/epoch - 5ms/step
Epoch 143/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 144/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 145/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 146/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 147/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 148/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 149/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 150/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 151/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 152/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 153/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 154/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 155/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 156/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 157/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 158/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 159/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 160/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 161/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 162/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 163/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 164/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 165/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 166/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 167/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 168/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 169/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 170/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 171/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 172/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 173/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 174/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 175/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010015762411057949
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.09069129428097854
cosine 0.07111035395782937
MAE: 0.039826743
RMSE: 0.08612733
r2: 0.5187818732922624
RMSE zero-vector: 0.23411466903540806
['2.7custom_VAE', 'mse', 256, 175, 0.002, 0.6, 758, 0.010155257768929005, 0.010015762411057949, 0.09069129428097854, 0.07111035395782937, 0.03982674330472946, 0.08612733334302902, 0.5187818732922624, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 180 0.0005 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 5s - loss: 0.0310 - val_loss: 0.0244 - 5s/epoch - 7ms/step
Epoch 2/180
747/747 - 3s - loss: 0.0162 - val_loss: 0.0165 - 3s/epoch - 4ms/step
Epoch 3/180
747/747 - 3s - loss: 0.0149 - val_loss: 0.0156 - 3s/epoch - 4ms/step
Epoch 4/180
747/747 - 3s - loss: 0.0141 - val_loss: 0.0145 - 3s/epoch - 4ms/step
Epoch 5/180
747/747 - 3s - loss: 0.0134 - val_loss: 0.0133 - 3s/epoch - 4ms/step
Epoch 6/180
747/747 - 3s - loss: 0.0130 - val_loss: 0.0135 - 3s/epoch - 4ms/step
Epoch 7/180
747/747 - 3s - loss: 0.0128 - val_loss: 0.0125 - 3s/epoch - 4ms/step
Epoch 8/180
747/747 - 3s - loss: 0.0125 - val_loss: 0.0124 - 3s/epoch - 4ms/step
Epoch 9/180
747/747 - 3s - loss: 0.0124 - val_loss: 0.0122 - 3s/epoch - 4ms/step
Epoch 10/180
747/747 - 3s - loss: 0.0123 - val_loss: 0.0121 - 3s/epoch - 4ms/step
Epoch 11/180
747/747 - 3s - loss: 0.0122 - val_loss: 0.0120 - 3s/epoch - 4ms/step
Epoch 12/180
747/747 - 3s - loss: 0.0121 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 13/180
747/747 - 3s - loss: 0.0120 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 14/180
747/747 - 3s - loss: 0.0117 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 15/180
747/747 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 4ms/step
Epoch 16/180
747/747 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 17/180
747/747 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 18/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 19/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 20/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 21/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 22/180
747/747 - 3s - loss: 0.0113 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 23/180
747/747 - 3s - loss: 0.0116 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 24/180
747/747 - 3s - loss: 0.0114 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 25/180
747/747 - 3s - loss: 0.0116 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 26/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 27/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 28/180
747/747 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 29/180
747/747 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 30/180
747/747 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 31/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 32/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 33/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 34/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 35/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 36/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 37/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 38/180
747/747 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 39/180
747/747 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 40/180
747/747 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 41/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 42/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 43/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 44/180
747/747 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 45/180
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 46/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 47/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 48/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 49/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 50/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 51/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 52/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 53/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 54/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 55/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 56/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 57/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 58/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 59/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 60/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 61/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 62/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 63/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 64/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 65/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 66/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 67/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 68/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 69/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 70/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 71/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 72/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 73/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 74/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 75/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 76/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 77/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 78/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 79/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 80/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 81/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 82/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 83/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 84/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 85/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 86/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 87/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 88/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 89/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 90/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 91/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 92/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 93/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 94/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 95/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 96/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 97/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 98/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 99/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 100/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 101/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 102/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 103/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 104/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 105/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 106/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 107/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 108/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 109/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 110/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 111/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 112/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 113/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 114/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 115/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 116/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 117/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 118/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 119/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 120/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 121/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 122/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 123/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 124/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 125/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 126/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 127/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 128/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 129/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 130/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 131/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 132/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 133/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 134/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 135/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 136/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 137/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 138/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 139/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 140/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 141/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 142/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 143/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 144/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 145/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 146/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 147/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 148/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 149/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 150/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 151/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 152/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 153/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 154/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 155/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 156/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 157/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 158/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 159/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 160/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 161/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 162/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 163/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 164/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 165/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 166/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 167/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 168/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 169/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 170/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 171/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 172/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 173/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 174/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 175/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 176/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 177/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 178/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 179/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 180/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009579644538462162
  1/332 [..............................] - ETA: 39s 42/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s168/332 [==============>...............] - ETA: 0s210/332 [=================>............] - ETA: 0s252/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0804995616348204
cosine 0.06315419927121467
MAE: 0.03712324
RMSE: 0.08138188
r2: 0.5703493206173434
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 128, 180, 0.0005, 0.6, 758, 0.009754188358783722, 0.009579644538462162, 0.0804995616348204, 0.06315419927121467, 0.037123240530490875, 0.08138187974691391, 0.5703493206173434, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 180 0.002 256 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.6_cr0.6_bs256_ep180_loss_logcosh_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 38s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12601650496990421
cosine 0.09862155181661535
MAE: 0.047477145
RMSE: 0.10090202
r2: 0.33951984136566676
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.6custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, '--', '--', 0.12601650496990421, 0.09862155181661535, 0.04747714474797249, 0.1009020209312439, 0.33951984136566676, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 175 0.0018 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 4s - loss: 0.0497 - val_loss: 0.0258 - 4s/epoch - 11ms/step
Epoch 2/175
374/374 - 2s - loss: 0.0202 - val_loss: 0.0291 - 2s/epoch - 5ms/step
Epoch 3/175
374/374 - 2s - loss: 0.0171 - val_loss: 0.0200 - 2s/epoch - 5ms/step
Epoch 4/175
374/374 - 2s - loss: 0.0155 - val_loss: 0.6666 - 2s/epoch - 5ms/step
Epoch 5/175
374/374 - 2s - loss: 0.0198 - val_loss: 0.0214 - 2s/epoch - 5ms/step
Epoch 6/175
374/374 - 2s - loss: 0.0141 - val_loss: 0.0178 - 2s/epoch - 5ms/step
Epoch 7/175
374/374 - 2s - loss: 0.0137 - val_loss: 0.0151 - 2s/epoch - 5ms/step
Epoch 8/175
374/374 - 2s - loss: 0.0134 - val_loss: 0.0554 - 2s/epoch - 5ms/step
Epoch 9/175
374/374 - 2s - loss: 0.0141 - val_loss: 0.0168 - 2s/epoch - 5ms/step
Epoch 10/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0149 - 2s/epoch - 5ms/step
Epoch 11/175
374/374 - 2s - loss: 0.0128 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 12/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0278 - 2s/epoch - 5ms/step
Epoch 13/175
374/374 - 2s - loss: 0.0144 - val_loss: 0.0190 - 2s/epoch - 5ms/step
Epoch 14/175
374/374 - 2s - loss: 0.0136 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 15/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 16/175
374/374 - 2s - loss: 0.0128 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 17/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 18/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 19/175
374/374 - 2s - loss: 0.0146 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 20/175
374/374 - 2s - loss: 0.0183 - val_loss: 0.0139 - 2s/epoch - 5ms/step
Epoch 21/175
374/374 - 2s - loss: 0.0305 - val_loss: 0.0265 - 2s/epoch - 5ms/step
Epoch 22/175
374/374 - 2s - loss: 0.0443 - val_loss: 0.0181 - 2s/epoch - 5ms/step
Epoch 23/175
374/374 - 2s - loss: 0.0160 - val_loss: 0.0143 - 2s/epoch - 5ms/step
Epoch 24/175
374/374 - 2s - loss: 0.0148 - val_loss: 0.0146 - 2s/epoch - 5ms/step
Epoch 25/175
374/374 - 2s - loss: 0.0165 - val_loss: 0.0154 - 2s/epoch - 5ms/step
Epoch 26/175
374/374 - 2s - loss: 0.0141 - val_loss: 0.0132 - 2s/epoch - 5ms/step
Epoch 27/175
374/374 - 2s - loss: 0.0138 - val_loss: 0.0144 - 2s/epoch - 5ms/step
Epoch 28/175
374/374 - 2s - loss: 0.0202 - val_loss: 0.0135 - 2s/epoch - 5ms/step
Epoch 29/175
374/374 - 2s - loss: 0.0141 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 30/175
374/374 - 2s - loss: 0.0141 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 31/175
374/374 - 2s - loss: 0.0134 - val_loss: 0.0143 - 2s/epoch - 5ms/step
Epoch 32/175
374/374 - 2s - loss: 0.0209 - val_loss: 0.0172 - 2s/epoch - 5ms/step
Epoch 33/175
374/374 - 2s - loss: 0.0270 - val_loss: 0.0183 - 2s/epoch - 5ms/step
Epoch 34/175
374/374 - 2s - loss: 0.0484 - val_loss: 0.0168 - 2s/epoch - 5ms/step
Epoch 35/175
374/374 - 2s - loss: 0.0190 - val_loss: 0.0155 - 2s/epoch - 5ms/step
Epoch 36/175
374/374 - 2s - loss: 0.0153 - val_loss: 0.0145 - 2s/epoch - 5ms/step
Epoch 37/175
374/374 - 2s - loss: 0.0148 - val_loss: 0.0140 - 2s/epoch - 5ms/step
Epoch 38/175
374/374 - 2s - loss: 0.0140 - val_loss: 0.0135 - 2s/epoch - 5ms/step
Epoch 39/175
374/374 - 2s - loss: 0.0137 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 40/175
374/374 - 2s - loss: 0.0142 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 41/175
374/374 - 2s - loss: 0.0133 - val_loss: 0.0128 - 2s/epoch - 5ms/step
Epoch 42/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 43/175
374/374 - 2s - loss: 0.0129 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 44/175
374/374 - 2s - loss: 0.0136 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 45/175
374/374 - 2s - loss: 0.0127 - val_loss: 0.0135 - 2s/epoch - 5ms/step
Epoch 46/175
374/374 - 2s - loss: 0.0139 - val_loss: 0.0124 - 2s/epoch - 5ms/step
Epoch 47/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0154 - 2s/epoch - 5ms/step
Epoch 48/175
374/374 - 2s - loss: 0.0168 - val_loss: 0.0124 - 2s/epoch - 5ms/step
Epoch 49/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 50/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 51/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 52/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 53/175
374/374 - 2s - loss: 0.0128 - val_loss: 0.0153 - 2s/epoch - 5ms/step
Epoch 54/175
374/374 - 2s - loss: 0.0156 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 55/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0200 - 2s/epoch - 5ms/step
Epoch 56/175
374/374 - 2s - loss: 0.0380 - val_loss: 0.0141 - 2s/epoch - 5ms/step
Epoch 57/175
374/374 - 2s - loss: 0.0138 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 58/175
374/374 - 2s - loss: 0.0126 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 59/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 60/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 61/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 62/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0136 - 2s/epoch - 5ms/step
Epoch 63/175
374/374 - 2s - loss: 0.0138 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 64/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 65/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 66/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 67/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 68/175
374/374 - 2s - loss: 0.0126 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 69/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 70/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 71/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 72/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 73/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 74/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 75/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 76/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 77/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 78/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 79/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0152 - 2s/epoch - 5ms/step
Epoch 80/175
374/374 - 2s - loss: 0.0145 - val_loss: 0.0204 - 2s/epoch - 5ms/step
Epoch 81/175
374/374 - 2s - loss: 0.0184 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 82/175
374/374 - 2s - loss: 0.0143 - val_loss: 0.0194 - 2s/epoch - 5ms/step
Epoch 83/175
374/374 - 2s - loss: 0.0398 - val_loss: 0.0141 - 2s/epoch - 5ms/step
Epoch 84/175
374/374 - 2s - loss: 0.0154 - val_loss: 0.0191 - 2s/epoch - 5ms/step
Epoch 85/175
374/374 - 2s - loss: 0.0466 - val_loss: 0.0148 - 2s/epoch - 5ms/step
Epoch 86/175
374/374 - 2s - loss: 0.0143 - val_loss: 0.0174 - 2s/epoch - 5ms/step
Epoch 87/175
374/374 - 2s - loss: 0.0273 - val_loss: 0.0147 - 2s/epoch - 5ms/step
Epoch 88/175
374/374 - 2s - loss: 0.0151 - val_loss: 0.0133 - 2s/epoch - 5ms/step
Epoch 89/175
374/374 - 2s - loss: 0.0135 - val_loss: 0.0135 - 2s/epoch - 5ms/step
Epoch 90/175
374/374 - 2s - loss: 0.0138 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 91/175
374/374 - 2s - loss: 0.0130 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 92/175
374/374 - 2s - loss: 0.0128 - val_loss: 0.0124 - 2s/epoch - 5ms/step
Epoch 93/175
374/374 - 2s - loss: 0.0126 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 94/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 95/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 96/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 97/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 98/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 99/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 100/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 101/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 102/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 103/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 104/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 105/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0214 - 2s/epoch - 5ms/step
Epoch 106/175
374/374 - 2s - loss: 0.0191 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 107/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 108/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 109/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0168 - 2s/epoch - 5ms/step
Epoch 110/175
374/374 - 2s - loss: 0.0155 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 111/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0143 - 2s/epoch - 5ms/step
Epoch 112/175
374/374 - 2s - loss: 0.0149 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 113/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 114/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 115/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 116/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 117/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 118/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 119/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 120/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 121/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 122/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 123/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 124/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 125/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0191 - 2s/epoch - 5ms/step
Epoch 126/175
374/374 - 2s - loss: 0.0183 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 127/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 128/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 129/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 130/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0168 - 2s/epoch - 5ms/step
Epoch 131/175
374/374 - 2s - loss: 0.0159 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 132/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 133/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 134/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 135/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 136/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 137/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 138/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 139/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 140/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0216 - 2s/epoch - 5ms/step
Epoch 141/175
374/374 - 2s - loss: 0.0289 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 142/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 143/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 144/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 145/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 146/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 147/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 148/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 149/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 150/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 151/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 152/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 153/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 154/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 155/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 156/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 157/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 158/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0277 - 2s/epoch - 5ms/step
Epoch 159/175
374/374 - 2s - loss: 0.0132 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 160/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 161/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 162/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 163/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 164/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 165/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 166/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 167/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 168/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0160 - 2s/epoch - 5ms/step
Epoch 169/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 170/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 171/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 172/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 173/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 174/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 175/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010010229423642159
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08847455817979999
cosine 0.06942220957850763
MAE: 0.039479606
RMSE: 0.08514436
r2: 0.5297034920176898
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 256, 175, 0.0018, 0.6, 758, 0.010115254670381546, 0.010010229423642159, 0.08847455817979999, 0.06942220957850763, 0.03947960585355759, 0.08514436334371567, 0.5297034920176898, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 180 0.002 256 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 4s - loss: 0.0320 - val_loss: 0.0132 - 4s/epoch - 11ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0309 - 2s/epoch - 5ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0165 - val_loss: 0.0094 - 2s/epoch - 5ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0080 - val_loss: 0.0097 - 2s/epoch - 5ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0080 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0080 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0079 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0076 - val_loss: 0.0090 - 2s/epoch - 5ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0074 - val_loss: 0.0080 - 2s/epoch - 5ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0070 - val_loss: 0.0077 - 2s/epoch - 5ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0068 - val_loss: 0.0075 - 2s/epoch - 5ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0067 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0067 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0068 - 2s/epoch - 5ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0078 - 2s/epoch - 5ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0078 - 2s/epoch - 5ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0068 - val_loss: 0.0069 - 2s/epoch - 5ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0067 - 2s/epoch - 5ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0072 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0074 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0069 - 2s/epoch - 5ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0076 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0066 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0065 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0069 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 81/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 82/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 84/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 85/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 87/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 88/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 89/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 93/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0064 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 111/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 112/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 136/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 140/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 162/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 166/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 172/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 173/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 180/180
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006045245565474033
  1/332 [..............................] - ETA: 37s 42/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s168/332 [==============>...............] - ETA: 0s210/332 [=================>............] - ETA: 0s252/332 [=====================>........] - ETA: 0s294/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.1255887128634584
cosine 0.09827100761063451
MAE: 0.04775515
RMSE: 0.100760445
r2: 0.3413743470178167
RMSE zero-vector: 0.23411466903540806
['2.7custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, 0.006080524064600468, 0.006045245565474033, 0.1255887128634584, 0.09827100761063451, 0.04775514826178551, 0.10076044499874115, 0.3413743470178167, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 180 0.002 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 5s - loss: 0.0384 - val_loss: 0.0316 - 5s/epoch - 7ms/step
Epoch 2/180
747/747 - 3s - loss: 0.0169 - val_loss: 0.0535 - 3s/epoch - 4ms/step
Epoch 3/180
747/747 - 3s - loss: 0.0154 - val_loss: 0.0157 - 3s/epoch - 4ms/step
Epoch 4/180
747/747 - 3s - loss: 0.0143 - val_loss: 0.0211 - 3s/epoch - 4ms/step
Epoch 5/180
747/747 - 3s - loss: 0.0136 - val_loss: 0.0135 - 3s/epoch - 4ms/step
Epoch 6/180
747/747 - 3s - loss: 0.0130 - val_loss: 0.0126 - 3s/epoch - 4ms/step
Epoch 7/180
747/747 - 3s - loss: 0.0126 - val_loss: 0.0124 - 3s/epoch - 4ms/step
Epoch 8/180
747/747 - 3s - loss: 0.0124 - val_loss: 0.0125 - 3s/epoch - 4ms/step
Epoch 9/180
747/747 - 3s - loss: 0.0123 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 10/180
747/747 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 11/180
747/747 - 3s - loss: 0.0119 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 12/180
747/747 - 3s - loss: 0.0115 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 13/180
747/747 - 3s - loss: 0.0116 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 14/180
747/747 - 3s - loss: 0.0117 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 15/180
747/747 - 3s - loss: 0.0121 - val_loss: 0.0125 - 3s/epoch - 4ms/step
Epoch 16/180
747/747 - 3s - loss: 0.0151 - val_loss: 0.0144 - 3s/epoch - 4ms/step
Epoch 17/180
747/747 - 3s - loss: 0.0155 - val_loss: 0.0130 - 3s/epoch - 4ms/step
Epoch 18/180
747/747 - 3s - loss: 0.0122 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 19/180
747/747 - 3s - loss: 0.0118 - val_loss: 0.0121 - 3s/epoch - 4ms/step
Epoch 20/180
747/747 - 3s - loss: 0.0121 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 21/180
747/747 - 3s - loss: 0.0116 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 22/180
747/747 - 3s - loss: 0.0117 - val_loss: 0.0132 - 3s/epoch - 4ms/step
Epoch 23/180
747/747 - 3s - loss: 0.0139 - val_loss: 0.0144 - 3s/epoch - 4ms/step
Epoch 24/180
747/747 - 3s - loss: 0.0176 - val_loss: 0.0126 - 3s/epoch - 4ms/step
Epoch 25/180
747/747 - 3s - loss: 0.0131 - val_loss: 0.0216 - 3s/epoch - 4ms/step
Epoch 26/180
747/747 - 3s - loss: 0.0313 - val_loss: 0.0130 - 3s/epoch - 4ms/step
Epoch 27/180
747/747 - 3s - loss: 0.0127 - val_loss: 0.0124 - 3s/epoch - 4ms/step
Epoch 28/180
747/747 - 3s - loss: 0.0123 - val_loss: 0.0118 - 3s/epoch - 4ms/step
Epoch 29/180
747/747 - 3s - loss: 0.0120 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 30/180
747/747 - 3s - loss: 0.0119 - val_loss: 0.0169 - 3s/epoch - 4ms/step
Epoch 31/180
747/747 - 3s - loss: 0.0232 - val_loss: 0.0131 - 3s/epoch - 4ms/step
Epoch 32/180
747/747 - 3s - loss: 0.0136 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 33/180
747/747 - 3s - loss: 0.0120 - val_loss: 0.0122 - 3s/epoch - 4ms/step
Epoch 34/180
747/747 - 3s - loss: 0.0118 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 35/180
747/747 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 36/180
747/747 - 3s - loss: 0.0117 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 37/180
747/747 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 38/180
747/747 - 3s - loss: 0.0115 - val_loss: 0.0360 - 3s/epoch - 4ms/step
Epoch 39/180
747/747 - 3s - loss: 0.0150 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 40/180
747/747 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 4ms/step
Epoch 41/180
747/747 - 3s - loss: 0.0114 - val_loss: 0.0265 - 3s/epoch - 4ms/step
Epoch 42/180
747/747 - 3s - loss: 0.0200 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 43/180
747/747 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 4ms/step
Epoch 44/180
747/747 - 3s - loss: 0.0114 - val_loss: 0.0113 - 3s/epoch - 4ms/step
Epoch 45/180
747/747 - 3s - loss: 0.0113 - val_loss: 0.0151 - 3s/epoch - 4ms/step
Epoch 46/180
747/747 - 3s - loss: 0.0123 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 47/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 48/180
747/747 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 49/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0118 - 3s/epoch - 4ms/step
Epoch 50/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 51/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 52/180
747/747 - 3s - loss: 0.0110 - val_loss: 0.0141 - 3s/epoch - 4ms/step
Epoch 53/180
747/747 - 3s - loss: 0.0121 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 54/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 55/180
747/747 - 3s - loss: 0.0108 - val_loss: 0.0137 - 3s/epoch - 4ms/step
Epoch 56/180
747/747 - 3s - loss: 0.0119 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 57/180
747/747 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 58/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 59/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 60/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 61/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 62/180
747/747 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 63/180
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 64/180
747/747 - 3s - loss: 0.0106 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 65/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 66/180
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 67/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 68/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 69/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 70/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 71/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 72/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 73/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 74/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 75/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 76/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 77/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 78/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 79/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 80/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 81/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 82/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 83/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 84/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 85/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 86/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 87/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 88/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 89/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 90/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 91/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 92/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 93/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 94/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 95/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 96/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 97/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 98/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 99/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 100/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 101/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 102/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 103/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 104/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 105/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 106/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 107/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 108/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 109/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 110/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 111/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 112/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 113/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 114/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 115/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 116/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 117/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 118/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 119/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 120/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 121/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 122/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 123/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 124/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 125/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 126/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 127/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 128/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 129/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 130/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 131/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 132/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 133/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 134/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 135/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 136/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 137/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 138/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 139/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 140/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 141/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 142/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 143/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 144/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 145/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 146/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 147/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 148/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 149/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 150/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 151/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 152/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 153/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 154/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 155/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 156/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 157/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 158/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 159/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 160/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 161/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 162/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 163/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 164/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 165/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 166/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 167/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 168/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 169/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 170/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 171/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 172/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 173/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 174/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 175/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 176/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 177/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 178/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 179/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 180/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009598047472536564
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08095095760746808
cosine 0.06357082462832023
MAE: 0.03762551
RMSE: 0.08161483
r2: 0.5678861561382746
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 128, 180, 0.002, 0.6, 758, 0.009771831333637238, 0.009598047472536564, 0.08095095760746808, 0.06357082462832023, 0.0376255102455616, 0.08161482959985733, 0.5678861561382746, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 175 0.0018 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 5s - loss: 0.0387 - val_loss: 0.0304 - 5s/epoch - 7ms/step
Epoch 2/175
747/747 - 3s - loss: 0.0197 - val_loss: 0.0214 - 3s/epoch - 4ms/step
Epoch 3/175
747/747 - 3s - loss: 0.0151 - val_loss: 0.0163 - 3s/epoch - 4ms/step
Epoch 4/175
747/747 - 3s - loss: 0.0144 - val_loss: 0.0346 - 3s/epoch - 4ms/step
Epoch 5/175
747/747 - 3s - loss: 0.0138 - val_loss: 0.0134 - 3s/epoch - 4ms/step
Epoch 6/175
747/747 - 3s - loss: 0.0131 - val_loss: 0.0130 - 3s/epoch - 4ms/step
Epoch 7/175
747/747 - 3s - loss: 0.0127 - val_loss: 0.0122 - 3s/epoch - 4ms/step
Epoch 8/175
747/747 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 4ms/step
Epoch 9/175
747/747 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 10/175
747/747 - 3s - loss: 0.0119 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 11/175
747/747 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 12/175
747/747 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 13/175
747/747 - 3s - loss: 0.0114 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 14/175
747/747 - 3s - loss: 0.0122 - val_loss: 0.0113 - 3s/epoch - 4ms/step
Epoch 15/175
747/747 - 3s - loss: 0.0114 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 16/175
747/747 - 3s - loss: 0.0121 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 17/175
747/747 - 3s - loss: 0.0116 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 18/175
747/747 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 19/175
747/747 - 3s - loss: 0.0112 - val_loss: 0.0113 - 3s/epoch - 4ms/step
Epoch 20/175
747/747 - 3s - loss: 0.0114 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 21/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 22/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 23/175
747/747 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 24/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0113 - 3s/epoch - 4ms/step
Epoch 25/175
747/747 - 3s - loss: 0.0118 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 26/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 27/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 28/175
747/747 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 29/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 30/175
747/747 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 31/175
747/747 - 3s - loss: 0.0109 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 32/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 33/175
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 34/175
747/747 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 35/175
747/747 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 36/175
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 37/175
747/747 - 3s - loss: 0.0130 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 38/175
747/747 - 3s - loss: 0.0121 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 39/175
747/747 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 40/175
747/747 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 41/175
747/747 - 3s - loss: 0.0116 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 42/175
747/747 - 3s - loss: 0.0108 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 43/175
747/747 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 44/175
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 45/175
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 46/175
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 47/175
747/747 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 48/175
747/747 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 49/175
747/747 - 3s - loss: 0.0108 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 50/175
747/747 - 3s - loss: 0.0112 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 51/175
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 52/175
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 53/175
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 54/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 55/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 56/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 57/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 58/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 59/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 60/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 61/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 62/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 63/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 64/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 65/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 66/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 67/175
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 68/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 69/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 70/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 71/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 72/175
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 73/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 74/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 75/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 76/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 77/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 78/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 79/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 80/175
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 81/175
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 82/175
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 83/175
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 84/175
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 85/175
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 86/175
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 87/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 88/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 89/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 90/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 91/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 92/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 93/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 94/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 95/175
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 96/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 97/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 98/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 99/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 100/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 101/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 102/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 103/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 104/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 105/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 106/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 107/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 108/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 109/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 110/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 111/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 112/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 113/175
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 114/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 115/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 116/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 117/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 118/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 119/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 120/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 121/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 122/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 123/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 124/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 125/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 126/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 127/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 128/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 129/175
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 130/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 131/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 132/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 133/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 134/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 135/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 136/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 137/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 138/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 139/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 140/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 141/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 142/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 143/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 144/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 145/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 146/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 147/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 148/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 149/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 150/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 151/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 152/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 153/175
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 154/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 155/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 156/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 157/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 158/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 159/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 160/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 161/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 162/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 163/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 164/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 165/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 166/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 167/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 168/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 169/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 170/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 171/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 172/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 173/175
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 174/175
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 175/175
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0094764968380332
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0774937267971632
cosine 0.060827654918049846
MAE: 0.036564134
RMSE: 0.079962
r2: 0.5852109270168199
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 128, 175, 0.0018, 0.6, 758, 0.009643145836889744, 0.0094764968380332, 0.0774937267971632, 0.060827654918049846, 0.03656413406133652, 0.07996200025081635, 0.5852109270168199, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 180 0.002 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 4s - loss: 0.0405 - val_loss: 0.0288 - 4s/epoch - 10ms/step
Epoch 2/180
374/374 - 1s - loss: 0.0165 - val_loss: 0.2549 - 1s/epoch - 4ms/step
Epoch 3/180
374/374 - 1s - loss: 0.0170 - val_loss: 0.0255 - 1s/epoch - 4ms/step
Epoch 4/180
374/374 - 1s - loss: 0.0164 - val_loss: 0.0179 - 1s/epoch - 4ms/step
Epoch 5/180
374/374 - 1s - loss: 0.0143 - val_loss: 0.0157 - 1s/epoch - 4ms/step
Epoch 6/180
374/374 - 1s - loss: 0.0139 - val_loss: 0.0165 - 1s/epoch - 4ms/step
Epoch 7/180
374/374 - 1s - loss: 0.0138 - val_loss: 0.0152 - 1s/epoch - 4ms/step
Epoch 8/180
374/374 - 1s - loss: 0.0135 - val_loss: 0.0152 - 1s/epoch - 4ms/step
Epoch 9/180
374/374 - 1s - loss: 0.0133 - val_loss: 0.0185 - 1s/epoch - 4ms/step
Epoch 10/180
374/374 - 1s - loss: 0.0132 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 11/180
374/374 - 1s - loss: 0.0128 - val_loss: 0.0136 - 1s/epoch - 4ms/step
Epoch 12/180
374/374 - 1s - loss: 0.0127 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 13/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 14/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 15/180
374/374 - 1s - loss: 0.0124 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 16/180
374/374 - 1s - loss: 0.0123 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 17/180
374/374 - 1s - loss: 0.0123 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 18/180
374/374 - 1s - loss: 0.0124 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 19/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 20/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 21/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 22/180
374/374 - 1s - loss: 0.0122 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 23/180
374/374 - 1s - loss: 0.0122 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 24/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 25/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 26/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 27/180
374/374 - 1s - loss: 0.0131 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 28/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 29/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 30/180
374/374 - 1s - loss: 0.0123 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 31/180
374/374 - 1s - loss: 0.0127 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 32/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 33/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0126 - 1s/epoch - 4ms/step
Epoch 34/180
374/374 - 1s - loss: 0.0133 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 35/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 36/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 37/180
374/374 - 1s - loss: 0.0116 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 38/180
374/374 - 1s - loss: 0.0130 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 39/180
374/374 - 1s - loss: 0.0116 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 40/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 41/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 42/180
374/374 - 1s - loss: 0.0127 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 43/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 44/180
374/374 - 1s - loss: 0.0117 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 45/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 46/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 47/180
374/374 - 1s - loss: 0.0113 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 48/180
374/374 - 1s - loss: 0.0115 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 49/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 50/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 51/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 52/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 53/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 54/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 55/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 56/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 57/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 58/180
374/374 - 1s - loss: 0.0147 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 59/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 60/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 61/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 62/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 63/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 64/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 65/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 66/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 67/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 68/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 69/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 70/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 71/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 72/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 73/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 74/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 75/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 76/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 77/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 78/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 79/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 80/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 81/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 82/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 83/180
374/374 - 1s - loss: 0.0171 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 84/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 85/180
374/374 - 1s - loss: 0.0129 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 86/180
374/374 - 1s - loss: 0.0124 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 87/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 88/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 89/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 90/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 91/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 92/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 93/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 94/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 95/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 96/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 97/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 98/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 99/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 100/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 101/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 102/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 103/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 104/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 105/180
374/374 - 1s - loss: 0.0120 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 106/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 107/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 108/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 109/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 110/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 111/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 112/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 113/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 114/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 115/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 116/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 117/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 118/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 119/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 120/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 121/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 122/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 123/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 124/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 125/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 126/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 127/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 128/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 129/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 130/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 131/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 132/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 133/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 134/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 135/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 136/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 137/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 138/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 139/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 140/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 141/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 142/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 143/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 144/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 145/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 146/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 147/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 148/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 149/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 150/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 151/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 152/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 153/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 154/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 155/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 156/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 157/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 158/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 159/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 160/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 161/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 162/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 163/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 164/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 165/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 166/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 167/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 168/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 169/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 170/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 171/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 172/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 173/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 174/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 175/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 176/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 177/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 178/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 179/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 180/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009641852229833603
  1/332 [..............................] - ETA: 36s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08262952825751702
cosine 0.06481836162173339
MAE: 0.03785651
RMSE: 0.08241785
r2: 0.5593410316612316
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 256, 180, 0.002, 0.6, 758, 0.009767694398760796, 0.009641852229833603, 0.08262952825751702, 0.06481836162173339, 0.037856511771678925, 0.08241785317659378, 0.5593410316612316, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 180 0.002 256 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.7_cr0.6_bs256_ep180_loss_logcosh_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 35s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.1255887128634584
cosine 0.09827100761063451
MAE: 0.04775515
RMSE: 0.100760445
r2: 0.3413743470178167
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.7custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, '--', '--', 0.1255887128634584, 0.09827100761063451, 0.04775514826178551, 0.10076044499874115, 0.3413743470178167, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 180 0.002 256 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.5_cr0.6_bs256_ep180_loss_logcosh_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 35s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12634173039834184
cosine 0.09882654159800583
MAE: 0.047489487
RMSE: 0.10104788
r2: 0.3376091577403504
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.5custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, '--', '--', 0.12634173039834184, 0.09882654159800583, 0.04748948663473129, 0.10104788094758987, 0.3376091577403504, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[2.7 180 0.002 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 5s - loss: 0.0225 - val_loss: 0.0117 - 5s/epoch - 7ms/step
Epoch 2/180
747/747 - 3s - loss: 0.0120 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 3/180
747/747 - 3s - loss: 0.0081 - val_loss: 0.0089 - 3s/epoch - 4ms/step
Epoch 4/180
747/747 - 3s - loss: 0.0079 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 5/180
747/747 - 3s - loss: 0.0075 - val_loss: 0.0079 - 3s/epoch - 4ms/step
Epoch 6/180
747/747 - 3s - loss: 0.0072 - val_loss: 0.0077 - 3s/epoch - 4ms/step
Epoch 7/180
747/747 - 3s - loss: 0.0070 - val_loss: 0.0070 - 3s/epoch - 4ms/step
Epoch 8/180
747/747 - 3s - loss: 0.0069 - val_loss: 0.0068 - 3s/epoch - 4ms/step
Epoch 9/180
747/747 - 3s - loss: 0.0068 - val_loss: 0.0067 - 3s/epoch - 4ms/step
Epoch 10/180
747/747 - 3s - loss: 0.0066 - val_loss: 0.0065 - 3s/epoch - 4ms/step
Epoch 11/180
747/747 - 3s - loss: 0.0065 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 12/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0066 - 3s/epoch - 4ms/step
Epoch 13/180
747/747 - 3s - loss: 0.0082 - val_loss: 0.0067 - 3s/epoch - 4ms/step
Epoch 14/180
747/747 - 3s - loss: 0.0065 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 15/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 16/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 17/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 18/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 19/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 20/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 21/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0066 - 3s/epoch - 4ms/step
Epoch 22/180
747/747 - 3s - loss: 0.0073 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 23/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 24/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 25/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 26/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 27/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0069 - 3s/epoch - 4ms/step
Epoch 28/180
747/747 - 3s - loss: 0.0069 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 29/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 30/180
747/747 - 3s - loss: 0.0063 - val_loss: 0.0064 - 3s/epoch - 4ms/step
Epoch 31/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 32/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0067 - 3s/epoch - 4ms/step
Epoch 33/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0076 - 3s/epoch - 4ms/step
Epoch 34/180
747/747 - 3s - loss: 0.0069 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 35/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 36/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0067 - 3s/epoch - 4ms/step
Epoch 37/180
747/747 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 38/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 39/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 40/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 41/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 42/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 43/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 44/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 45/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 46/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 47/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 48/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 49/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 50/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 51/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 52/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0063 - 3s/epoch - 4ms/step
Epoch 53/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 54/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 55/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 56/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 57/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 58/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 59/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 60/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 61/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 62/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 63/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 64/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 65/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 66/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0062 - 3s/epoch - 4ms/step
Epoch 67/180
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 68/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 69/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 70/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 71/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 72/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 73/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 74/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 75/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 76/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 77/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 78/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 79/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 80/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 81/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 82/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 83/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 84/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 85/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 86/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 87/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 88/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 89/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 90/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 91/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 92/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 93/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 94/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 95/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 96/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 97/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 98/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 99/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 100/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 101/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 102/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 103/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 104/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 105/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 106/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 107/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 108/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 109/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 110/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 111/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 112/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 113/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 114/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 115/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 116/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 117/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 118/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 119/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 120/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 121/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 122/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 123/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 124/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 125/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 126/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 127/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 128/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 129/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 130/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 131/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 132/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 133/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 134/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 135/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 136/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 137/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 138/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 139/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 140/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 141/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 142/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 143/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 144/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 145/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 146/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 147/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 148/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 149/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 150/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 151/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 152/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 153/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 154/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 155/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 156/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 157/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 158/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 159/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 160/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 161/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 162/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 163/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 4ms/step
Epoch 164/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 165/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 166/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 167/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 168/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 169/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 170/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 171/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 172/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 173/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 174/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 175/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 176/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 177/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 178/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 179/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
Epoch 180/180
747/747 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006016082596033812
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s171/332 [==============>...............] - ETA: 0s214/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12374307195555252
cosine 0.0967974326282998
MAE: 0.046796642
RMSE: 0.10007872
r2: 0.35025405692644185
RMSE zero-vector: 0.23411466903540806
['2.7custom_VAE', 'logcosh', 128, 180, 0.002, 0.6, 758, 0.006079388316720724, 0.006016082596033812, 0.12374307195555252, 0.0967974326282998, 0.04679664224386215, 0.10007871687412262, 0.35025405692644185, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 175 0.002 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 4s - loss: 0.0396 - val_loss: 0.0214 - 4s/epoch - 10ms/step
Epoch 2/175
374/374 - 1s - loss: 0.0168 - val_loss: 0.0190 - 1s/epoch - 4ms/step
Epoch 3/175
374/374 - 1s - loss: 0.0167 - val_loss: 0.0162 - 1s/epoch - 4ms/step
Epoch 4/175
374/374 - 1s - loss: 0.0146 - val_loss: 0.0199 - 1s/epoch - 4ms/step
Epoch 5/175
374/374 - 1s - loss: 0.0143 - val_loss: 0.0178 - 1s/epoch - 4ms/step
Epoch 6/175
374/374 - 1s - loss: 0.0141 - val_loss: 0.0270 - 1s/epoch - 4ms/step
Epoch 7/175
374/374 - 1s - loss: 0.0139 - val_loss: 0.0157 - 1s/epoch - 4ms/step
Epoch 8/175
374/374 - 1s - loss: 0.0136 - val_loss: 0.0150 - 1s/epoch - 4ms/step
Epoch 9/175
374/374 - 1s - loss: 0.0133 - val_loss: 0.0158 - 1s/epoch - 4ms/step
Epoch 10/175
374/374 - 1s - loss: 0.0128 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 11/175
374/374 - 1s - loss: 0.0124 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 12/175
374/374 - 1s - loss: 0.0122 - val_loss: 0.0260 - 1s/epoch - 4ms/step
Epoch 13/175
374/374 - 1s - loss: 0.0124 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 14/175
374/374 - 1s - loss: 0.0118 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 15/175
374/374 - 1s - loss: 0.0117 - val_loss: 0.0151 - 1s/epoch - 4ms/step
Epoch 16/175
374/374 - 1s - loss: 0.0119 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 17/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 18/175
374/374 - 1s - loss: 0.0115 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 19/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 20/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 21/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 22/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 23/175
374/374 - 1s - loss: 0.0121 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 24/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 25/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 26/175
374/374 - 1s - loss: 0.0128 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 27/175
374/374 - 1s - loss: 0.0143 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 28/175
374/374 - 1s - loss: 0.0128 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 29/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 30/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 31/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 32/175
374/374 - 1s - loss: 0.0111 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 33/175
374/374 - 1s - loss: 0.0113 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 34/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 35/175
374/374 - 1s - loss: 0.0115 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 36/175
374/374 - 1s - loss: 0.0157 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 37/175
374/374 - 1s - loss: 0.0134 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 38/175
374/374 - 1s - loss: 0.0116 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 39/175
374/374 - 1s - loss: 0.0115 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 40/175
374/374 - 1s - loss: 0.0113 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 41/175
374/374 - 1s - loss: 0.0113 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 42/175
374/374 - 1s - loss: 0.0130 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 43/175
374/374 - 1s - loss: 0.0118 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 44/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 45/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 46/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 47/175
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 48/175
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 49/175
374/374 - 1s - loss: 0.0115 - val_loss: 0.0153 - 1s/epoch - 4ms/step
Epoch 50/175
374/374 - 1s - loss: 0.0272 - val_loss: 0.0209 - 1s/epoch - 4ms/step
Epoch 51/175
374/374 - 1s - loss: 0.0315 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 52/175
374/374 - 1s - loss: 0.0127 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 53/175
374/374 - 1s - loss: 0.0129 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 54/175
374/374 - 1s - loss: 0.0121 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 55/175
374/374 - 1s - loss: 0.0140 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 56/175
374/374 - 1s - loss: 0.0121 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 57/175
374/374 - 1s - loss: 0.0123 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 58/175
374/374 - 1s - loss: 0.0117 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 59/175
374/374 - 1s - loss: 0.0116 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 60/175
374/374 - 1s - loss: 0.0115 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 61/175
374/374 - 1s - loss: 0.0114 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 62/175
374/374 - 1s - loss: 0.0119 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 63/175
374/374 - 1s - loss: 0.0116 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 64/175
374/374 - 1s - loss: 0.0116 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 65/175
374/374 - 1s - loss: 0.0113 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 66/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 67/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 68/175
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 69/175
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 70/175
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 71/175
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 72/175
374/374 - 1s - loss: 0.0110 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 73/175
374/374 - 1s - loss: 0.0110 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 74/175
374/374 - 1s - loss: 0.0110 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 75/175
374/374 - 1s - loss: 0.0109 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 76/175
374/374 - 1s - loss: 0.0112 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 77/175
374/374 - 1s - loss: 0.0109 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 78/175
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 79/175
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 80/175
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 81/175
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 82/175
374/374 - 1s - loss: 0.0107 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 83/175
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 84/175
374/374 - 1s - loss: 0.0106 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 85/175
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 86/175
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 87/175
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 88/175
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 89/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 90/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 91/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 92/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 93/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 94/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 95/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 96/175
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 97/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 98/175
374/374 - 1s - loss: 0.0110 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 99/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 100/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 101/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 102/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 103/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 104/175
374/374 - 1s - loss: 0.0104 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 105/175
374/374 - 1s - loss: 0.0107 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 106/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 107/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 108/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 109/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 110/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 111/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 112/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 113/175
374/374 - 1s - loss: 0.0105 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 114/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 115/175
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 116/175
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 117/175
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 118/175
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 119/175
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 120/175
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 121/175
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 122/175
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 123/175
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 124/175
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 125/175
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 126/175
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 127/175
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 128/175
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 129/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 130/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 131/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 132/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 133/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 134/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 135/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 136/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 137/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 138/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 139/175
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 140/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 141/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 142/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 143/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 144/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 145/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 146/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 147/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 148/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 149/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 150/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 151/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 152/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 153/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 154/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 155/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 156/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 157/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 158/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 159/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 160/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 161/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 162/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 163/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 164/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 165/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 166/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 167/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 168/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 169/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 170/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 171/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 172/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 173/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 174/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 175/175
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009516261518001556
  1/332 [..............................] - ETA: 36s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07938635220089095
cosine 0.06229257479147151
MAE: 0.036877982
RMSE: 0.08086038
r2: 0.5758381578387943
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 256, 175, 0.002, 0.6, 758, 0.009665443561971188, 0.009516261518001556, 0.07938635220089095, 0.06229257479147151, 0.03687798231840134, 0.08086037635803223, 0.5758381578387943, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 180 0.002 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 5s - loss: 0.0304 - val_loss: 0.2377 - 5s/epoch - 7ms/step
Epoch 2/180
747/747 - 3s - loss: 0.0164 - val_loss: 0.0182 - 3s/epoch - 4ms/step
Epoch 3/180
747/747 - 3s - loss: 0.0150 - val_loss: 0.0157 - 3s/epoch - 4ms/step
Epoch 4/180
747/747 - 3s - loss: 0.0142 - val_loss: 0.0153 - 3s/epoch - 4ms/step
Epoch 5/180
747/747 - 3s - loss: 0.0136 - val_loss: 0.0136 - 3s/epoch - 4ms/step
Epoch 6/180
747/747 - 3s - loss: 0.0131 - val_loss: 0.0133 - 3s/epoch - 4ms/step
Epoch 7/180
747/747 - 3s - loss: 0.0128 - val_loss: 0.0126 - 3s/epoch - 4ms/step
Epoch 8/180
747/747 - 3s - loss: 0.0125 - val_loss: 0.0123 - 3s/epoch - 4ms/step
Epoch 9/180
747/747 - 3s - loss: 0.0124 - val_loss: 0.0122 - 3s/epoch - 4ms/step
Epoch 10/180
747/747 - 3s - loss: 0.0123 - val_loss: 0.0121 - 3s/epoch - 4ms/step
Epoch 11/180
747/747 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 4ms/step
Epoch 12/180
747/747 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 4ms/step
Epoch 13/180
747/747 - 3s - loss: 0.0121 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 14/180
747/747 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 15/180
747/747 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 4ms/step
Epoch 16/180
747/747 - 3s - loss: 0.0120 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 17/180
747/747 - 3s - loss: 0.0119 - val_loss: 0.0118 - 3s/epoch - 4ms/step
Epoch 18/180
747/747 - 3s - loss: 0.0119 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 19/180
747/747 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 20/180
747/747 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 4ms/step
Epoch 21/180
747/747 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 4ms/step
Epoch 22/180
747/747 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 23/180
747/747 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 4ms/step
Epoch 24/180
747/747 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 25/180
747/747 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 4ms/step
Epoch 26/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 27/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 28/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 29/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 30/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 31/180
747/747 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 32/180
747/747 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 33/180
747/747 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 34/180
747/747 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 35/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 4ms/step
Epoch 36/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 37/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 38/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 39/180
747/747 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 40/180
747/747 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 41/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 42/180
747/747 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 43/180
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 44/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 45/180
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 4ms/step
Epoch 46/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 47/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 48/180
747/747 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 49/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 50/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 51/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 52/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 53/180
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 54/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 55/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 56/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 57/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 58/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 59/180
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 60/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 61/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 62/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 63/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 64/180
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 65/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 66/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 67/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 68/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 69/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 70/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 71/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 72/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 73/180
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 74/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 75/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 76/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 77/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 78/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 79/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 80/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 81/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 82/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 83/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 84/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 85/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 86/180
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 87/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 88/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 89/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 90/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 91/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 92/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 93/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 94/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 95/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 96/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 97/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 98/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 99/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 100/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 101/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 102/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 103/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 104/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 105/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 106/180
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 107/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 108/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 109/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 110/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 111/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 112/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 113/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 114/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 115/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 116/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 117/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 118/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 119/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 120/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 121/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 122/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 123/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 124/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 125/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 126/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 127/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 128/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 129/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 130/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 131/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 132/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 133/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 134/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 135/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 136/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 137/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 138/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 139/180
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 140/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 141/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 142/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 143/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 144/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 145/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 146/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 147/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 148/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 149/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 150/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 151/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 152/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 153/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 154/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 155/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 156/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 157/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 158/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 159/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 160/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 161/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 162/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 163/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 164/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 165/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 166/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 167/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 168/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 169/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 170/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 171/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 172/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 173/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 174/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 175/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 176/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 177/180
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 178/180
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 179/180
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 180/180
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009350872598588467
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s297/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07500768154354039
cosine 0.05886914074151326
MAE: 0.035859387
RMSE: 0.078723066
r2: 0.5979647949844688
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 128, 180, 0.002, 0.6, 758, 0.009543688967823982, 0.009350872598588467, 0.07500768154354039, 0.05886914074151326, 0.03585938736796379, 0.07872306555509567, 0.5979647949844688, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 180 0.0022 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 4s - loss: 0.0507 - val_loss: 0.0270 - 4s/epoch - 11ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0218 - val_loss: 0.0235 - 2s/epoch - 5ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0175 - val_loss: 0.0183 - 2s/epoch - 5ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0155 - val_loss: 0.0384 - 2s/epoch - 5ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0153 - val_loss: 0.0189 - 2s/epoch - 5ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0146 - val_loss: 0.0192 - 2s/epoch - 5ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0142 - val_loss: 0.0181 - 2s/epoch - 5ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0135 - val_loss: 0.0290 - 2s/epoch - 5ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0850 - 2s/epoch - 5ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0164 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0138 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0218 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0189 - val_loss: 0.0133 - 2s/epoch - 5ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0153 - val_loss: 0.0138 - 2s/epoch - 5ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0202 - val_loss: 0.0163 - 2s/epoch - 5ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0274 - val_loss: 0.0138 - 2s/epoch - 5ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0148 - 2s/epoch - 5ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0201 - val_loss: 0.0214 - 2s/epoch - 5ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0340 - val_loss: 0.0177 - 2s/epoch - 5ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0182 - val_loss: 0.0182 - 2s/epoch - 5ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0290 - val_loss: 0.0144 - 2s/epoch - 5ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0143 - val_loss: 0.0150 - 2s/epoch - 5ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0182 - val_loss: 0.0159 - 2s/epoch - 5ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0189 - val_loss: 0.0138 - 2s/epoch - 5ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0136 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0192 - 2s/epoch - 5ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0203 - val_loss: 0.0209 - 2s/epoch - 5ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0208 - val_loss: 0.0136 - 2s/epoch - 5ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0124 - 2s/epoch - 5ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0154 - 2s/epoch - 5ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0151 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0184 - 2s/epoch - 5ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0212 - 2s/epoch - 5ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0139 - val_loss: 0.0173 - 2s/epoch - 5ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0163 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0158 - 2s/epoch - 5ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0199 - val_loss: 0.0290 - 2s/epoch - 5ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0462 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0612 - val_loss: 0.0256 - 2s/epoch - 5ms/step
Epoch 81/180
374/374 - 2s - loss: 0.0962 - val_loss: 0.0172 - 2s/epoch - 5ms/step
Epoch 82/180
374/374 - 2s - loss: 0.0151 - val_loss: 0.0135 - 2s/epoch - 5ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 84/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 85/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 87/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 88/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 89/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 93/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0223 - 2s/epoch - 5ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0332 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 111/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 112/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0229 - 2s/epoch - 5ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 136/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0144 - 2s/epoch - 5ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 140/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0142 - 2s/epoch - 5ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0135 - 2s/epoch - 5ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 162/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 166/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 172/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 173/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 180/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0098 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009788758121430874
  1/332 [..............................] - ETA: 40s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s297/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08304599517253584
cosine 0.06516758392115966
MAE: 0.038209725
RMSE: 0.08261235
r2: 0.5572587814459771
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 256, 180, 0.0022, 0.6, 758, 0.010393111035227776, 0.009788758121430874, 0.08304599517253584, 0.06516758392115966, 0.03820972517132759, 0.08261235058307648, 0.5572587814459771, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 180 0.002 256 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 4s - loss: 0.0249 - val_loss: 0.0110 - 4s/epoch - 10ms/step
Epoch 2/180
374/374 - 1s - loss: 0.0085 - val_loss: 0.0163 - 1s/epoch - 4ms/step
Epoch 3/180
374/374 - 1s - loss: 0.0082 - val_loss: 0.0090 - 1s/epoch - 4ms/step
Epoch 4/180
374/374 - 1s - loss: 0.0079 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 5/180
374/374 - 1s - loss: 0.0078 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 6/180
374/374 - 1s - loss: 0.0076 - val_loss: 0.0088 - 1s/epoch - 4ms/step
Epoch 7/180
374/374 - 1s - loss: 0.0075 - val_loss: 0.0084 - 1s/epoch - 4ms/step
Epoch 8/180
374/374 - 1s - loss: 0.0074 - val_loss: 0.0141 - 1s/epoch - 4ms/step
Epoch 9/180
374/374 - 1s - loss: 0.0073 - val_loss: 0.0090 - 1s/epoch - 4ms/step
Epoch 10/180
374/374 - 1s - loss: 0.0071 - val_loss: 0.0075 - 1s/epoch - 4ms/step
Epoch 11/180
374/374 - 1s - loss: 0.0070 - val_loss: 0.0072 - 1s/epoch - 4ms/step
Epoch 12/180
374/374 - 1s - loss: 0.0070 - val_loss: 0.0074 - 1s/epoch - 4ms/step
Epoch 13/180
374/374 - 1s - loss: 0.0069 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 14/180
374/374 - 1s - loss: 0.0069 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 15/180
374/374 - 1s - loss: 0.0068 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 16/180
374/374 - 1s - loss: 0.0069 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 17/180
374/374 - 1s - loss: 0.0067 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 18/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0072 - 1s/epoch - 4ms/step
Epoch 19/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 20/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 21/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 22/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 23/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 24/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 25/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 26/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 27/180
374/374 - 1s - loss: 0.0073 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 28/180
374/374 - 1s - loss: 0.0074 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 29/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 30/180
374/374 - 1s - loss: 0.0079 - val_loss: 0.0073 - 1s/epoch - 4ms/step
Epoch 31/180
374/374 - 1s - loss: 0.0086 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 32/180
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 33/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 34/180
374/374 - 1s - loss: 0.0068 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 35/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 36/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 37/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 38/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 39/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 40/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 41/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 42/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 43/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 44/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 45/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 46/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 47/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 48/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 49/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 50/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 51/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 52/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 53/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 54/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 55/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 56/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 57/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0081 - 1s/epoch - 4ms/step
Epoch 58/180
374/374 - 1s - loss: 0.0073 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 59/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 60/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 61/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 62/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 63/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 64/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0071 - 1s/epoch - 4ms/step
Epoch 65/180
374/374 - 1s - loss: 0.0072 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 66/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 67/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 68/180
374/374 - 1s - loss: 0.0068 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 69/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 70/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 71/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 72/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 73/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 74/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 75/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 76/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 77/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 78/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 79/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 80/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 81/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 82/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 83/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 84/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 85/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 86/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 87/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 88/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 89/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 90/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 91/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 92/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 93/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 94/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 95/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 96/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 97/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 98/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 99/180
374/374 - 1s - loss: 0.0067 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 100/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 101/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 102/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 103/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 104/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 105/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 106/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 107/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 108/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 109/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 110/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 111/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 112/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 113/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 114/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 115/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 116/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 117/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 118/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 119/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 120/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 121/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 122/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 123/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 124/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 125/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 126/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 127/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 128/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 129/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 130/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 131/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 132/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 133/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 134/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 135/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 136/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 137/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 138/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 139/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 140/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 141/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 142/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 143/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 144/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 145/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 146/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 147/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 148/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 149/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 150/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 151/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 152/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 153/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 154/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 155/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 156/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 157/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 158/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 159/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 160/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 161/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 162/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 163/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 164/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 165/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 166/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 167/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 168/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 169/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 170/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 171/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 172/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 173/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 174/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 175/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 176/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 177/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 178/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 179/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 180/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00600518798455596
  1/332 [..............................] - ETA: 36s 42/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s168/332 [==============>...............] - ETA: 0s210/332 [=================>............] - ETA: 0s251/332 [=====================>........] - ETA: 0s293/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.1250351418527261
cosine 0.09784229590090275
MAE: 0.046961237
RMSE: 0.10053231
r2: 0.3443510199561156
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, 0.006069660186767578, 0.00600518798455596, 0.1250351418527261, 0.09784229590090275, 0.046961236745119095, 0.10053230822086334, 0.3443510199561156, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 180 0.002 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 4s - loss: 0.0407 - val_loss: 0.0266 - 4s/epoch - 10ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0171 - val_loss: 0.0215 - 2s/epoch - 4ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0162 - val_loss: 0.0190 - 2s/epoch - 4ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0151 - val_loss: 0.0603 - 2s/epoch - 4ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0147 - val_loss: 0.0168 - 2s/epoch - 4ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0141 - val_loss: 0.0249 - 2s/epoch - 4ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0139 - val_loss: 0.0181 - 2s/epoch - 4ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0888 - 2s/epoch - 4ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0140 - val_loss: 0.0156 - 2s/epoch - 4ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0139 - 2s/epoch - 4ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0129 - val_loss: 0.0133 - 2s/epoch - 4ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0134 - 2s/epoch - 4ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0128 - 2s/epoch - 4ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0139 - 2s/epoch - 4ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0177 - 2s/epoch - 4ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0135 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 4ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0131 - 2s/epoch - 4ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0157 - val_loss: 0.0182 - 2s/epoch - 4ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0189 - val_loss: 0.0145 - 2s/epoch - 4ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0187 - val_loss: 0.0150 - 2s/epoch - 4ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0172 - val_loss: 0.0130 - 2s/epoch - 4ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0122 - 2s/epoch - 4ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0136 - val_loss: 0.0163 - 2s/epoch - 4ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0262 - val_loss: 0.0138 - 2s/epoch - 4ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0131 - 2s/epoch - 4ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0135 - val_loss: 0.0135 - 2s/epoch - 4ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0203 - val_loss: 0.0175 - 2s/epoch - 4ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0264 - val_loss: 0.0136 - 2s/epoch - 4ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0128 - 2s/epoch - 4ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0129 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0146 - 2s/epoch - 4ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0156 - val_loss: 0.0125 - 2s/epoch - 4ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0165 - 2s/epoch - 4ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0232 - val_loss: 0.0136 - 2s/epoch - 4ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0128 - 2s/epoch - 4ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0122 - 2s/epoch - 4ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0136 - 2s/epoch - 4ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0135 - val_loss: 0.0120 - 2s/epoch - 4ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0125 - 2s/epoch - 4ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0118 - 2s/epoch - 4ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 4ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0124 - 2s/epoch - 4ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0114 - 2s/epoch - 4ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 4ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 4ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0146 - 2s/epoch - 4ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 4ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0118 - 2s/epoch - 4ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 4ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0131 - 2s/epoch - 4ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0111 - 2s/epoch - 4ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0123 - 2s/epoch - 4ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0134 - 2s/epoch - 4ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0182 - val_loss: 0.0117 - 2s/epoch - 4ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0109 - 2s/epoch - 4ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 4ms/step
Epoch 81/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 4ms/step
Epoch 82/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 84/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 85/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 87/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 88/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 89/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 93/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0106 - 2s/epoch - 4ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0119 - 2s/epoch - 4ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0123 - 2s/epoch - 4ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 111/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 112/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0118 - 2s/epoch - 4ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0103 - 2s/epoch - 4ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0104 - 2s/epoch - 4ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 136/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 140/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0100 - 2s/epoch - 4ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 4ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 4ms/step
Epoch 162/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 166/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 172/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 173/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 180/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009655887261033058
  1/332 [..............................] - ETA: 36s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s255/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08155294753473478
cosine 0.0639848076270474
MAE: 0.037738904
RMSE: 0.08193262
r2: 0.5645145637238864
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 256, 180, 0.002, 0.6, 758, 0.009778892621397972, 0.009655887261033058, 0.08155294753473478, 0.0639848076270474, 0.03773890435695648, 0.08193261921405792, 0.5645145637238864, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[2.7 180 0.0024000000000000002 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 4s - loss: 0.0510 - val_loss: 0.0278 - 4s/epoch - 11ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0217 - val_loss: 0.2011 - 2s/epoch - 5ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0196 - val_loss: 0.0175 - 2s/epoch - 5ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0155 - val_loss: 0.0300 - 2s/epoch - 5ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0158 - val_loss: 0.0300 - 2s/epoch - 5ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0149 - val_loss: 0.0304 - 2s/epoch - 5ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0144 - val_loss: 0.0235 - 2s/epoch - 5ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0141 - val_loss: 0.0214 - 2s/epoch - 5ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0351 - 2s/epoch - 5ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0146 - val_loss: 0.0756 - 2s/epoch - 5ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0162 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0149 - 2s/epoch - 5ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0437 - val_loss: 0.0141 - 2s/epoch - 5ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0243 - val_loss: 0.0162 - 2s/epoch - 5ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0169 - val_loss: 0.0164 - 2s/epoch - 5ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0171 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0154 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0173 - 2s/epoch - 5ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0237 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0213 - 2s/epoch - 5ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0420 - val_loss: 0.0142 - 2s/epoch - 5ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0378 - val_loss: 0.0229 - 2s/epoch - 5ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0613 - val_loss: 0.0198 - 2s/epoch - 5ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0153 - val_loss: 0.0147 - 2s/epoch - 5ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0141 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0142 - 2s/epoch - 5ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0142 - val_loss: 0.0131 - 2s/epoch - 5ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0141 - val_loss: 0.0125 - 2s/epoch - 5ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0308 - 2s/epoch - 5ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0442 - val_loss: 0.0145 - 2s/epoch - 5ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0144 - val_loss: 0.0228 - 2s/epoch - 5ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0577 - val_loss: 0.0370 - 2s/epoch - 5ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0424 - val_loss: 0.0161 - 2s/epoch - 5ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0157 - val_loss: 0.0147 - 2s/epoch - 5ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0149 - val_loss: 0.0139 - 2s/epoch - 5ms/step
Epoch 81/180
374/374 - 2s - loss: 0.0141 - val_loss: 0.0156 - 2s/epoch - 5ms/step
Epoch 82/180
374/374 - 2s - loss: 0.0164 - val_loss: 0.0133 - 2s/epoch - 5ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0135 - val_loss: 0.0129 - 2s/epoch - 5ms/step
Epoch 84/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0128 - 2s/epoch - 5ms/step
Epoch 85/180
374/374 - 2s - loss: 0.0130 - val_loss: 0.0166 - 2s/epoch - 5ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0181 - val_loss: 0.0132 - 2s/epoch - 5ms/step
Epoch 87/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0127 - 2s/epoch - 5ms/step
Epoch 88/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0123 - 2s/epoch - 5ms/step
Epoch 89/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 93/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0141 - 2s/epoch - 5ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0117 - 2s/epoch - 5ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0132 - 2s/epoch - 5ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 111/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 112/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0249 - 2s/epoch - 5ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0237 - val_loss: 0.0149 - 2s/epoch - 5ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0243 - 2s/epoch - 5ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0201 - val_loss: 0.0174 - 2s/epoch - 5ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0181 - val_loss: 0.0180 - 2s/epoch - 5ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0139 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0178 - 2s/epoch - 5ms/step
Epoch 136/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0155 - 2s/epoch - 5ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0104 - 2s/epoch - 5ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 140/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0158 - 2s/epoch - 5ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0126 - 2s/epoch - 5ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0171 - 2s/epoch - 5ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0142 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0144 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 5ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 162/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0156 - 2s/epoch - 5ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0142 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 166/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 172/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 173/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0162 - 2s/epoch - 5ms/step
Epoch 180/180
374/374 - 2s - loss: 0.0147 - val_loss: 0.0102 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010240267962217331
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s214/332 [==================>...........] - ETA: 0s257/332 [======================>.......] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08404451335123517
cosine 0.0659472859505658
MAE: 0.0384959
RMSE: 0.08307677
r2: 0.5522672579910404
RMSE zero-vector: 0.23411466903540806
['2.7custom_VAE', 'mse', 256, 180, 0.0024000000000000002, 0.6, 758, 0.014674298465251923, 0.010240267962217331, 0.08404451335123517, 0.0659472859505658, 0.03849589824676514, 0.08307676762342453, 0.5522672579910404, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 175 0.002 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/175
1493/1493 - 7s - loss: 0.0270 - val_loss: 0.0176 - 7s/epoch - 5ms/step
Epoch 2/175
1493/1493 - 5s - loss: 0.0156 - val_loss: 0.0146 - 5s/epoch - 3ms/step
Epoch 3/175
1493/1493 - 5s - loss: 0.0137 - val_loss: 0.0132 - 5s/epoch - 3ms/step
Epoch 4/175
1493/1493 - 5s - loss: 0.0129 - val_loss: 0.0125 - 5s/epoch - 3ms/step
Epoch 5/175
1493/1493 - 5s - loss: 0.0126 - val_loss: 0.0123 - 5s/epoch - 3ms/step
Epoch 6/175
1493/1493 - 5s - loss: 0.0124 - val_loss: 0.0122 - 5s/epoch - 3ms/step
Epoch 7/175
1493/1493 - 5s - loss: 0.0123 - val_loss: 0.0121 - 5s/epoch - 3ms/step
Epoch 8/175
1493/1493 - 5s - loss: 0.0122 - val_loss: 0.0120 - 5s/epoch - 3ms/step
Epoch 9/175
1493/1493 - 5s - loss: 0.0122 - val_loss: 0.0119 - 5s/epoch - 3ms/step
Epoch 10/175
1493/1493 - 5s - loss: 0.0121 - val_loss: 0.0118 - 5s/epoch - 3ms/step
Epoch 11/175
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0115 - 5s/epoch - 3ms/step
Epoch 12/175
1493/1493 - 5s - loss: 0.0116 - val_loss: 0.0113 - 5s/epoch - 3ms/step
Epoch 13/175
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0111 - 5s/epoch - 3ms/step
Epoch 14/175
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 15/175
1493/1493 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 16/175
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 4ms/step
Epoch 17/175
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 18/175
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 19/175
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 20/175
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 21/175
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 22/175
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 23/175
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 24/175
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 25/175
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 26/175
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 27/175
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 28/175
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 29/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 4ms/step
Epoch 30/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 31/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 32/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 33/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 34/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 35/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 36/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 37/175
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 38/175
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 39/175
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 4ms/step
Epoch 40/175
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 41/175
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 42/175
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 4ms/step
Epoch 43/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 44/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 45/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 46/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 47/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 48/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 49/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 50/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 4ms/step
Epoch 51/175
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 52/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 53/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 4ms/step
Epoch 54/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 55/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 56/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 57/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 58/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 59/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 4ms/step
Epoch 60/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 61/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 62/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 63/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 64/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 65/175
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 66/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 67/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 68/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 69/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 70/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 71/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 72/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 73/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 74/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 75/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 76/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 77/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 78/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 79/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 80/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 81/175
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 82/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 83/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 84/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 85/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 86/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 87/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 88/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 89/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 90/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 91/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 92/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 93/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 94/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 95/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 96/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 97/175
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 98/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 99/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 100/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 101/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 102/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 103/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 104/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 105/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 106/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 107/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 108/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 109/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 110/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 111/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 112/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 113/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 114/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 115/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 116/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 117/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 118/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 119/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 120/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 121/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 122/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 123/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 124/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 125/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 126/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 127/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 128/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 129/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 130/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 131/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 132/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 133/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 134/175
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 135/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 136/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 137/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 138/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 139/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 140/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 141/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 142/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 143/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 144/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 145/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 146/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 147/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 148/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 149/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 150/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 151/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 152/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 153/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 154/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 155/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 156/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 157/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 158/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 159/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 160/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 161/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 162/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 163/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 164/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 165/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 166/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 167/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 168/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 169/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 170/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 171/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 172/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 173/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 174/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 175/175
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009676224552094936
  1/332 [..............................] - ETA: 40s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08019335830252079
cosine 0.06295277310280453
MAE: 0.037296157
RMSE: 0.08136734
r2: 0.570502876839465
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 64, 175, 0.002, 0.6, 758, 0.009879414923489094, 0.009676224552094936, 0.08019335830252079, 0.06295277310280453, 0.03729615733027458, 0.08136734366416931, 0.570502876839465, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 175 0.0018 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 4s - loss: 0.0257 - val_loss: 0.0102 - 4s/epoch - 10ms/step
Epoch 2/175
374/374 - 1s - loss: 0.0086 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 3/175
374/374 - 1s - loss: 0.0081 - val_loss: 0.0089 - 1s/epoch - 4ms/step
Epoch 4/175
374/374 - 1s - loss: 0.0079 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 5/175
374/374 - 1s - loss: 0.0077 - val_loss: 0.0149 - 1s/epoch - 4ms/step
Epoch 6/175
374/374 - 1s - loss: 0.0076 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 7/175
374/374 - 1s - loss: 0.0075 - val_loss: 0.0077 - 1s/epoch - 4ms/step
Epoch 8/175
374/374 - 1s - loss: 0.0073 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 9/175
374/374 - 1s - loss: 0.0072 - val_loss: 0.0080 - 1s/epoch - 4ms/step
Epoch 10/175
374/374 - 1s - loss: 0.0071 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 11/175
374/374 - 1s - loss: 0.0071 - val_loss: 0.0073 - 1s/epoch - 4ms/step
Epoch 12/175
374/374 - 1s - loss: 0.0069 - val_loss: 0.0078 - 1s/epoch - 4ms/step
Epoch 13/175
374/374 - 1s - loss: 0.0069 - val_loss: 0.0075 - 1s/epoch - 4ms/step
Epoch 14/175
374/374 - 1s - loss: 0.0069 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 15/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0074 - 1s/epoch - 4ms/step
Epoch 16/175
374/374 - 1s - loss: 0.0066 - val_loss: 0.0079 - 1s/epoch - 4ms/step
Epoch 17/175
374/374 - 1s - loss: 0.0065 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 18/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 19/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 20/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0151 - 1s/epoch - 4ms/step
Epoch 21/175
374/374 - 1s - loss: 0.0079 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 22/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 23/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 24/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 25/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 26/175
374/374 - 1s - loss: 0.0079 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 27/175
374/374 - 1s - loss: 0.0096 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 28/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0091 - 1s/epoch - 4ms/step
Epoch 29/175
374/374 - 1s - loss: 0.0098 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 30/175
374/374 - 1s - loss: 0.0066 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 31/175
374/374 - 1s - loss: 0.0067 - val_loss: 0.0079 - 1s/epoch - 4ms/step
Epoch 32/175
374/374 - 1s - loss: 0.0116 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 33/175
374/374 - 1s - loss: 0.0069 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 34/175
374/374 - 1s - loss: 0.0067 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 35/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 36/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 37/175
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 38/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 39/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 40/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 41/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 42/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 43/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 44/175
374/374 - 1s - loss: 0.0065 - val_loss: 0.0078 - 1s/epoch - 4ms/step
Epoch 45/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 46/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 47/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 48/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 49/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 50/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 51/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 52/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 53/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 54/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 55/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 56/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 57/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 58/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 59/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 60/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0075 - 1s/epoch - 4ms/step
Epoch 61/175
374/374 - 1s - loss: 0.0077 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 62/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 63/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 64/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 65/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 66/175
374/374 - 1s - loss: 0.0065 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 67/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 68/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 69/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 70/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 71/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 72/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 73/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 74/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0072 - 1s/epoch - 4ms/step
Epoch 75/175
374/374 - 1s - loss: 0.0067 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 76/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 77/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 78/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 79/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 80/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 81/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 82/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 83/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 84/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 85/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 86/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 87/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 88/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 89/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 90/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 91/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 92/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 93/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 94/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 95/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 96/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 97/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 98/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 99/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 100/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 101/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 102/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 103/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 104/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 105/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 106/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 107/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 108/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 109/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 110/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 111/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 112/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 113/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 114/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 115/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 116/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 117/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 118/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 119/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 120/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 121/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 122/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 123/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 124/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 125/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 126/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 127/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 128/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 129/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 130/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 131/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 132/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 133/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 134/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 135/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 136/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 137/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 138/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 139/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 140/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 141/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 142/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 143/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 144/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 145/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 146/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 147/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 148/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 149/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 150/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 151/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 152/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 153/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 154/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 155/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 156/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 157/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 158/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 159/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 160/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 161/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 162/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 163/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 164/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 165/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 166/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 167/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 168/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 169/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 170/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 171/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 172/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 173/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 174/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 175/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006027880124747753
  1/332 [..............................] - ETA: 38s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.1255425666913508
cosine 0.09823183990001139
MAE: 0.046903368
RMSE: 0.10072849
r2: 0.34178965752217416
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 256, 175, 0.0018, 0.6, 758, 0.006069695111364126, 0.006027880124747753, 0.1255425666913508, 0.09823183990001139, 0.04690336808562279, 0.10072848945856094, 0.34178965752217416, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 180 0.0022 256 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
2023-02-19 04:27:12.960614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 313655296/23676715008
2023-02-19 04:27:12.960659: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22263485084
MaxInUse:                  22302798997
NumAllocs:                   221575924
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:12.960721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:12.960727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:12.960730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:12.960733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:12.960736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 385
2023-02-19 04:27:12.960746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 67
2023-02-19 04:27:12.960750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 44
2023-02-19 04:27:12.960753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:12.960755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 44
2023-02-19 04:27:12.960758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 32
2023-02-19 04:27:12.960761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 142
2023-02-19 04:27:12.960764: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:12.960767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 67
2023-02-19 04:27:12.960770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 18
2023-02-19 04:27:12.960773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:12.960775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 18
2023-02-19 04:27:12.960778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 12
2023-02-19 04:27:12.960781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 12
2023-02-19 04:27:12.960784: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:12.960787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 57
2023-02-19 04:27:12.960789: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 12
2023-02-19 04:27:12.960792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:12.960795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 8
2023-02-19 04:27:12.960798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 38
2023-02-19 04:27:12.960801: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:12.960804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:12.960806: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.7custom_VAE', 'logcosh', 256, 180, 0.0022, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.7 180 0.0022 256 2]) is not valid.
[1.6 170 0.002 256 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
2023-02-19 04:27:14.705063: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 313655296/23676715008
2023-02-19 04:27:14.705102: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22304729076
MaxInUse:                  22325170500
NumAllocs:                   221575980
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:14.705160: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:14.705165: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:14.705168: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:14.705171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:14.705174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 392
2023-02-19 04:27:14.705176: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 68
2023-02-19 04:27:14.705179: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 44
2023-02-19 04:27:14.705182: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:14.705184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:14.705187: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 32
2023-02-19 04:27:14.705189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 142
2023-02-19 04:27:14.705201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:14.705204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 68
2023-02-19 04:27:14.705206: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 18
2023-02-19 04:27:14.705209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:14.705211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:14.705214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 12
2023-02-19 04:27:14.705216: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 12
2023-02-19 04:27:14.705219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:14.705222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 57
2023-02-19 04:27:14.705224: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:14.705227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:14.705229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 8
2023-02-19 04:27:14.705232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 38
2023-02-19 04:27:14.705235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:14.705237: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:14.705240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.6custom_VAE', 'logcosh', 256, 170, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.6 170 0.002 256 2]) is not valid.
[1.4 180 0.002 256 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
2023-02-19 04:27:16.455605: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 313655296/23676715008
2023-02-19 04:27:16.455647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22299859332
MaxInUse:                  22358986412
NumAllocs:                   221576036
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:16.455710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:16.455718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:16.455722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:16.455727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:16.455731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 392
2023-02-19 04:27:16.455735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 68
2023-02-19 04:27:16.455740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:16.455744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:16.455748: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 44
2023-02-19 04:27:16.455752: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 32
2023-02-19 04:27:16.455757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 142
2023-02-19 04:27:16.455762: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:16.455765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 68
2023-02-19 04:27:16.455768: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:16.455771: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:16.455774: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 18
2023-02-19 04:27:16.455776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:16.455788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 12
2023-02-19 04:27:16.455791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:16.455793: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 57
2023-02-19 04:27:16.455796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 12
2023-02-19 04:27:16.455799: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:16.455802: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 8
2023-02-19 04:27:16.455805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 38
2023-02-19 04:27:16.455808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:16.455810: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:16.455813: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.4custom_VAE', 'logcosh', 256, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.4 180 0.002 256 2]) is not valid.
[1.5 180 0.002 128 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
2023-02-19 04:27:18.196114: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 313655296/23676715008
2023-02-19 04:27:18.196157: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22338678076
MaxInUse:                  22358986412
NumAllocs:                   221576092
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:18.196215: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:18.196220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:18.196223: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:18.196226: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:18.196229: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 399
2023-02-19 04:27:18.196232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 69
2023-02-19 04:27:18.196235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:18.196238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 120
2023-02-19 04:27:18.196241: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 44
2023-02-19 04:27:18.196244: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 32
2023-02-19 04:27:18.196247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 142
2023-02-19 04:27:18.196249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:18.196252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 69
2023-02-19 04:27:18.196255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:18.196258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 48
2023-02-19 04:27:18.196261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 18
2023-02-19 04:27:18.196264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:18.196266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 12
2023-02-19 04:27:18.196269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 32
2023-02-19 04:27:18.196272: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 57
2023-02-19 04:27:18.196275: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 12
2023-02-19 04:27:18.196278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:18.196280: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 8
2023-02-19 04:27:18.196291: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 38
2023-02-19 04:27:18.196294: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:18.196297: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:18.196300: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.5custom_VAE', 'mse', 128, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.5 180 0.002 128 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_3.pkl
[1.6 175 0.002 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
2023-02-19 04:27:28.898395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 280100864/23676715008
2023-02-19 04:27:28.898437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22341103324
MaxInUse:                  22381914764
NumAllocs:                   221576148
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:28.898497: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:28.898502: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:28.898505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:28.898508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:28.898510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 399
2023-02-19 04:27:28.898513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 69
2023-02-19 04:27:28.898516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:28.898518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:28.898521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:28.898523: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 32
2023-02-19 04:27:28.898526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 142
2023-02-19 04:27:28.898529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:28.898531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 69
2023-02-19 04:27:28.898534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:28.898536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:28.898539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:28.898541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:28.898544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 12
2023-02-19 04:27:28.898547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:28.898549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 57
2023-02-19 04:27:28.898552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:28.898554: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:28.898557: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 8
2023-02-19 04:27:28.898559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 38
2023-02-19 04:27:28.898562: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:28.898564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:28.898567: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.6custom_VAE', 'mse', 256, 175, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.6 175 0.002 256 1]) is not valid.
[1.5 175 0.0018 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
2023-02-19 04:27:33.179263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 246546432/23676715008
2023-02-19 04:27:33.179308: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22379922068
MaxInUse:                  22399089380
NumAllocs:                   221576204
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:33.179374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:33.179380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:33.179384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:33.179387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:33.179390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 406
2023-02-19 04:27:33.179393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 70
2023-02-19 04:27:33.179395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:33.179398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 120
2023-02-19 04:27:33.179401: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:33.179404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 32
2023-02-19 04:27:33.179407: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 142
2023-02-19 04:27:33.179410: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:33.179412: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 70
2023-02-19 04:27:33.179415: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:33.179418: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 48
2023-02-19 04:27:33.179421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:33.179424: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:33.179427: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 12
2023-02-19 04:27:33.179429: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 32
2023-02-19 04:27:33.179432: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 57
2023-02-19 04:27:33.179435: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:33.179438: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:33.179441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 8
2023-02-19 04:27:33.179443: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 38
2023-02-19 04:27:33.179446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:33.179449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:33.179452: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.5custom_VAE', 'mse', 128, 175, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.5 175 0.0018 128 1]) is not valid.
[2.5 180 0.002 128 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
2023-02-19 04:27:34.927555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 212992000/23676715008
2023-02-19 04:27:34.927596: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22404251540
MaxInUse:                  22446187324
NumAllocs:                   221576260
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:34.927653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:34.927657: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:34.927660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:34.927663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:34.927675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 406
2023-02-19 04:27:34.927684: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 70
2023-02-19 04:27:34.927688: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:34.927690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:34.927693: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:34.927695: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-19 04:27:34.927698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 142
2023-02-19 04:27:34.927701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:34.927703: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 70
2023-02-19 04:27:34.927706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:34.927708: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:34.927711: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:34.927713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:34.927716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-19 04:27:34.927718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:34.927721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 57
2023-02-19 04:27:34.927724: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:34.927726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:34.927729: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-19 04:27:34.927731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 38
2023-02-19 04:27:34.927734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:34.927736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:34.927739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.5custom_VAE', 'logcosh', 128, 180, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.5 180 0.002 128 2]) is not valid.
[2.6 175 0.002 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-19 04:27:36.672407: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 145883136/23676715008
2023-02-19 04:27:36.672450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22469825004
MaxInUse:                  22503047996
NumAllocs:                   221576316
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:36.672508: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:36.672513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:36.672516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:36.672519: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:36.672521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 413
2023-02-19 04:27:36.672524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 71
2023-02-19 04:27:36.672527: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:36.672529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:36.672532: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:36.672534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-19 04:27:36.672545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 152
2023-02-19 04:27:36.672548: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:36.672550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 71
2023-02-19 04:27:36.672553: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:36.672556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:36.672558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:36.672561: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:36.672563: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-19 04:27:36.672566: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:36.672568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 60
2023-02-19 04:27:36.672571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:36.672574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:36.672576: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-19 04:27:36.672579: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 40
2023-02-19 04:27:36.672581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:36.672584: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:36.672586: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'mse', 128, 175, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 175 0.002 128 1]) is not valid.
[2.6 175 0.002 128 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-19 04:27:38.415609: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 112328704/23676715008
2023-02-19 04:27:38.415651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22469825004
MaxInUse:                  22538639996
NumAllocs:                   221576372
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:38.415717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:38.415722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:38.415725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:38.415728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:38.415731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 413
2023-02-19 04:27:38.415733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 71
2023-02-19 04:27:38.415736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:38.415738: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:38.415741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:38.415743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-19 04:27:38.415746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 152
2023-02-19 04:27:38.415749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:38.415751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 71
2023-02-19 04:27:38.415754: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:38.415756: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:38.415759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:38.415770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:38.415773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-19 04:27:38.415776: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:38.415778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 60
2023-02-19 04:27:38.415781: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:38.415783: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:38.415786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-19 04:27:38.415788: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 40
2023-02-19 04:27:38.415791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:38.415794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:38.415796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'logcosh', 128, 175, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 175 0.002 128 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_4.pkl
[2.6 180 0.0018 256 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
2023-02-19 04:27:46.924194: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 45219840/23676715008
2023-02-19 04:27:46.924237: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22535398468
MaxInUse:                  22568621460
NumAllocs:                   221576428
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:46.924296: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:46.924301: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:46.924304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:46.924306: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:46.924309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 420
2023-02-19 04:27:46.924312: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 72
2023-02-19 04:27:46.924314: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:46.924317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:46.924319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:46.924322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-19 04:27:46.924325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 162
2023-02-19 04:27:46.924327: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 130
2023-02-19 04:27:46.924330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 72
2023-02-19 04:27:46.924332: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:46.924335: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:46.924337: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:46.924340: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:46.924343: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-19 04:27:46.924345: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:46.924348: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 63
2023-02-19 04:27:46.924350: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:46.924353: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 51
2023-02-19 04:27:46.924364: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-19 04:27:46.924367: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 42
2023-02-19 04:27:46.924369: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 34
2023-02-19 04:27:46.924372: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:46.924374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.6custom_VAE', 'logcosh', 256, 180, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.6 180 0.0018 256 2]) is not valid.
[2.7 180 0.0018 256 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
2023-02-19 04:27:48.671265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 11665408/23676715008
2023-02-19 04:27:48.671307: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22537823716
MaxInUse:                  22594101564
NumAllocs:                   221576484
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:48.671366: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:48.671371: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 99
2023-02-19 04:27:48.671374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 83
2023-02-19 04:27:48.671377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:48.671380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 420
2023-02-19 04:27:48.671382: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 72
2023-02-19 04:27:48.671385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:48.671387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:48.671390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:48.671393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-19 04:27:48.671395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 152
2023-02-19 04:27:48.671398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 140
2023-02-19 04:27:48.671400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 72
2023-02-19 04:27:48.671403: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:48.671405: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:48.671408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:48.671411: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:48.671413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-19 04:27:48.671416: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:48.671418: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 60
2023-02-19 04:27:48.671421: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:48.671423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 54
2023-02-19 04:27:48.671426: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-19 04:27:48.671428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 40
2023-02-19 04:27:48.671431: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 36
2023-02-19 04:27:48.671434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:48.671436: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.7custom_VAE', 'logcosh', 256, 180, 0.0018, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.7 180 0.0018 256 2]) is not valid.
[2.7 180 0.0018 256 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-19 04:27:50.099317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 17251072 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 11665408/23676715008
2023-02-19 04:27:50.099349: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21635399680
InUse:                     22605817392
MaxInUse:                  22609188444
NumAllocs:                   221576535
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-19 04:27:50.099413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-19 04:27:50.099417: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 98
2023-02-19 04:27:50.099420: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 84
2023-02-19 04:27:50.099423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-19 04:27:50.099426: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-19 04:27:50.099428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 427
2023-02-19 04:27:50.099432: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 72
2023-02-19 04:27:50.099434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 54
2023-02-19 04:27:50.099437: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 110
2023-02-19 04:27:50.099439: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 54
2023-02-19 04:27:50.099442: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 42
2023-02-19 04:27:50.099445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 152
2023-02-19 04:27:50.099447: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 150
2023-02-19 04:27:50.099450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 73
2023-02-19 04:27:50.099452: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 21
2023-02-19 04:27:50.099455: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 45
2023-02-19 04:27:50.099457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 21
2023-02-19 04:27:50.099460: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 14
2023-02-19 04:27:50.099463: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 15
2023-02-19 04:27:50.099465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 30
2023-02-19 04:27:50.099468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 60
2023-02-19 04:27:50.099470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 14
2023-02-19 04:27:50.099473: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 57
2023-02-19 04:27:50.099483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 10
2023-02-19 04:27:50.099486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 40
2023-02-19 04:27:50.099489: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 38
2023-02-19 04:27:50.099491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-19 04:27:50.099494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 36
2023-02-19 04:27:50.099515: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory
Traceback (most recent call last):
  File "genetic.py", line 44, in <module>
    ga=continue_run_ga('tmp/ga_instance_generation_7.pkl')
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 249, in continue_run_ga
    ga_instance.run()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1409, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 352, in train_autoencoder
    model = create_autoencoder(n_inputs=n_inputs, layers_structure=layers_structure, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 585, in create_autoencoder
    decoder_output = Dense(n_inputs, activation='linear', name='outputlayer')(d)
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]
Sun Feb 19 04:28:05 CET 2023
done
