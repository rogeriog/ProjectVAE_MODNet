start
Fri Feb 17 18:58:49 CET 2023
2023-02-17 18:58:52.820869: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-17 18:58:52.978590: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-17 18:59:37,767 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fc3da8a56a0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 90 0.001 128 1]
 [1.5 210 0.002 128 2]
 [1.5 60 0.002 64 2]
 [2.5 180 0.002 64 1]
 [2.0 150 0.002 64 1]
 [1.5 30 0.0005 256 1]
 [2.5 90 0.002 256 2]
 [1.5 180 0.0005 256 1]
 [1.5 150 0.001 256 2]
 [1.5 150 0.001 128 2]]
[2.0 90 0.001 128 1] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-17 18:59:42.224072: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-17 18:59:42.738188: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-17 18:59:42.754505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:37:00.0, compute capability: 7.0
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/90
2023-02-17 18:59:48.936342: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fc04fc2f790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-17 18:59:48.936384: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2023-02-17 18:59:48.941030: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-17 18:59:50.020657: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
747/747 - 13s - loss: 0.0342 - val_loss: 0.0232 - 13s/epoch - 17ms/step
Epoch 2/90
747/747 - 4s - loss: 0.0168 - val_loss: 0.0226 - 4s/epoch - 6ms/step
Epoch 3/90
747/747 - 4s - loss: 0.0154 - val_loss: 0.0159 - 4s/epoch - 6ms/step
Epoch 4/90
747/747 - 4s - loss: 0.0145 - val_loss: 0.0291 - 4s/epoch - 6ms/step
Epoch 5/90
747/747 - 4s - loss: 0.0136 - val_loss: 0.0153 - 4s/epoch - 6ms/step
Epoch 6/90
747/747 - 4s - loss: 0.0129 - val_loss: 0.0145 - 4s/epoch - 6ms/step
Epoch 7/90
747/747 - 4s - loss: 0.0123 - val_loss: 0.0119 - 4s/epoch - 6ms/step
Epoch 8/90
747/747 - 4s - loss: 0.0118 - val_loss: 0.0116 - 4s/epoch - 6ms/step
Epoch 9/90
747/747 - 4s - loss: 0.0117 - val_loss: 0.0117 - 4s/epoch - 6ms/step
Epoch 10/90
747/747 - 4s - loss: 0.0118 - val_loss: 0.0116 - 4s/epoch - 6ms/step
Epoch 11/90
747/747 - 4s - loss: 0.0116 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 12/90
747/747 - 4s - loss: 0.0114 - val_loss: 0.0118 - 4s/epoch - 6ms/step
Epoch 13/90
747/747 - 4s - loss: 0.0118 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 14/90
747/747 - 4s - loss: 0.0114 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 15/90
747/747 - 4s - loss: 0.0117 - val_loss: 0.0119 - 4s/epoch - 6ms/step
Epoch 16/90
747/747 - 4s - loss: 0.0122 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 17/90
747/747 - 4s - loss: 0.0119 - val_loss: 0.0123 - 4s/epoch - 6ms/step
Epoch 18/90
747/747 - 4s - loss: 0.0127 - val_loss: 0.0120 - 4s/epoch - 6ms/step
Epoch 19/90
747/747 - 4s - loss: 0.0118 - val_loss: 0.0112 - 4s/epoch - 6ms/step
Epoch 20/90
747/747 - 4s - loss: 0.0114 - val_loss: 0.0111 - 4s/epoch - 6ms/step
Epoch 21/90
747/747 - 4s - loss: 0.0113 - val_loss: 0.0114 - 4s/epoch - 6ms/step
Epoch 22/90
747/747 - 4s - loss: 0.0115 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 23/90
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 24/90
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 6ms/step
Epoch 25/90
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 26/90
747/747 - 4s - loss: 0.0111 - val_loss: 0.0113 - 4s/epoch - 6ms/step
Epoch 27/90
747/747 - 4s - loss: 0.0115 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 28/90
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 29/90
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 6ms/step
Epoch 30/90
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 6ms/step
Epoch 31/90
747/747 - 4s - loss: 0.0109 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 32/90
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 33/90
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 34/90
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 35/90
747/747 - 4s - loss: 0.0107 - val_loss: 0.0111 - 4s/epoch - 6ms/step
Epoch 36/90
747/747 - 4s - loss: 0.0111 - val_loss: 0.0106 - 4s/epoch - 6ms/step
Epoch 37/90
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 38/90
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 39/90
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 40/90
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 6ms/step
Epoch 41/90
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 42/90
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 43/90
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 44/90
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 45/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 46/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 47/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 48/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 6ms/step
Epoch 49/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 50/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 51/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 52/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 53/90
747/747 - 4s - loss: 0.0105 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 54/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 55/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 56/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 6ms/step
Epoch 57/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 58/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 59/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 60/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 61/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 62/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 63/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 64/90
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 65/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 66/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 67/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 68/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 69/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 70/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 71/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 72/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 6ms/step
Epoch 73/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 74/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 75/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 76/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 77/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 78/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 79/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 80/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 81/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 82/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 83/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 84/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 85/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 86/90
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 87/90
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 88/90
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 89/90
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 6ms/step
Epoch 90/90
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.01007557287812233
  1/332 [..............................] - ETA: 55s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 97/332 [=======>......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s162/332 [=============>................] - ETA: 0s195/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s261/332 [======================>.......] - ETA: 0s294/332 [=========================>....] - ETA: 0s327/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08951131819757568
cosine 0.07017972260500142
MAE: 0.039187886
RMSE: 0.08566711
r2: 0.5239114775761629
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 128, 90, 0.001, 0.6, 758, 0.010247400030493736, 0.01007557287812233, 0.08951131819757568, 0.07017972260500142, 0.03918788582086563, 0.08566711097955704, 0.5239114775761629, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 210 0.002 128 2] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/210
747/747 - 8s - loss: 0.0180 - val_loss: 0.0096 - 8s/epoch - 11ms/step
Epoch 2/210
747/747 - 4s - loss: 0.0085 - val_loss: 0.0088 - 4s/epoch - 6ms/step
Epoch 3/210
747/747 - 4s - loss: 0.0079 - val_loss: 0.0095 - 4s/epoch - 6ms/step
Epoch 4/210
747/747 - 4s - loss: 0.0077 - val_loss: 0.0080 - 4s/epoch - 6ms/step
Epoch 5/210
747/747 - 4s - loss: 0.0074 - val_loss: 0.0075 - 4s/epoch - 6ms/step
Epoch 6/210
747/747 - 4s - loss: 0.0072 - val_loss: 0.0076 - 4s/epoch - 6ms/step
Epoch 7/210
747/747 - 4s - loss: 0.0070 - val_loss: 0.0070 - 4s/epoch - 5ms/step
Epoch 8/210
747/747 - 4s - loss: 0.0069 - val_loss: 0.0068 - 4s/epoch - 6ms/step
Epoch 9/210
747/747 - 4s - loss: 0.0068 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 10/210
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 11/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 12/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 13/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 14/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 15/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 16/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 17/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 18/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 19/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 20/210
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 21/210
747/747 - 4s - loss: 0.0066 - val_loss: 0.0064 - 4s/epoch - 6ms/step
Epoch 22/210
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 23/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 24/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 25/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 26/210
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 27/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 28/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 29/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 30/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 31/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 32/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 33/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 35/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 36/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 37/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 38/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 39/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 43/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 44/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 45/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 46/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 47/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 48/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 49/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 50/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 51/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 52/210
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 54/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 55/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 56/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 58/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 59/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 60/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 61/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 62/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 64/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 65/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 67/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 68/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 69/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 70/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 71/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 72/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 73/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 74/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 75/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 76/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 77/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 78/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 79/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 80/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 81/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 82/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 83/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 84/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 85/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 86/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 87/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 88/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 89/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 90/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 91/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 92/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 93/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 94/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 95/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 96/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 97/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 98/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 99/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 101/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 102/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 103/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 104/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 105/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 106/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 107/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 108/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 109/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 111/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 112/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 113/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 114/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 115/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 116/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 118/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 120/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 121/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 122/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 123/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 124/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 125/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 126/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 127/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 128/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 129/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 130/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 131/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 132/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 133/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 134/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 135/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 136/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 137/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 138/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 139/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 140/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 141/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 142/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 143/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 144/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 145/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 146/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 147/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 148/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 149/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 150/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 151/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 152/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 153/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 154/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 155/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 156/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 157/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 158/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 159/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 160/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 161/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 162/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 163/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 164/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 165/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 166/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 167/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 168/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 169/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 170/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 171/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 172/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 173/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 174/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 175/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 176/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 177/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 178/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 179/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 180/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 181/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 182/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 183/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 184/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 185/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 186/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 187/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 188/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 189/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 190/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 191/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 192/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 193/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 194/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 195/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 196/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 197/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 198/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 199/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 200/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 201/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 202/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 203/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 204/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 205/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 206/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 207/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 208/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 209/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 210/210
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006024451926350594
  1/332 [..............................] - ETA: 48s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 95/332 [=======>......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s158/332 [=============>................] - ETA: 0s191/332 [================>.............] - ETA: 0s223/332 [===================>..........] - ETA: 0s255/332 [======================>.......] - ETA: 0s287/332 [========================>.....] - ETA: 0s320/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12260026284772586
cosine 0.09587813760485688
MAE: 0.046495415
RMSE: 0.09963912
r2: 0.3559495782995997
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 128, 210, 0.002, 0.6, 758, 0.0060624596662819386, 0.006024451926350594, 0.12260026284772586, 0.09587813760485688, 0.04649541527032852, 0.09963911771774292, 0.3559495782995997, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 60 0.002 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/60
1493/1493 - 11s - loss: 0.0157 - val_loss: 0.0088 - 11s/epoch - 7ms/step
Epoch 2/60
1493/1493 - 8s - loss: 0.0081 - val_loss: 0.0081 - 8s/epoch - 5ms/step
Epoch 3/60
1493/1493 - 8s - loss: 0.0073 - val_loss: 0.0072 - 8s/epoch - 5ms/step
Epoch 4/60
1493/1493 - 8s - loss: 0.0069 - val_loss: 0.0068 - 8s/epoch - 5ms/step
Epoch 5/60
1493/1493 - 8s - loss: 0.0068 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 6/60
1493/1493 - 8s - loss: 0.0068 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 7/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 8/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 9/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 5ms/step
Epoch 10/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 11/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 6ms/step
Epoch 12/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 5ms/step
Epoch 13/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 14/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 5ms/step
Epoch 15/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 5ms/step
Epoch 16/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 5ms/step
Epoch 17/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 5ms/step
Epoch 18/60
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 19/60
1493/1493 - 8s - loss: 0.0066 - val_loss: 0.0065 - 8s/epoch - 5ms/step
Epoch 20/60
1493/1493 - 8s - loss: 0.0065 - val_loss: 0.0064 - 8s/epoch - 5ms/step
Epoch 21/60
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0063 - 8s/epoch - 5ms/step
Epoch 22/60
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0063 - 8s/epoch - 5ms/step
Epoch 23/60
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 24/60
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 25/60
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 26/60
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 27/60
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 28/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 29/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 30/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 31/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 32/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 33/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 34/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 35/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 36/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 37/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 38/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 39/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 40/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 41/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 42/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 43/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 44/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 45/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 46/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 47/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 48/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 49/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 50/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 51/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 52/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 53/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 54/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 55/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 56/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 57/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 58/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 59/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 60/60
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006168372463434935
  1/332 [..............................] - ETA: 46s 32/332 [=>............................] - ETA: 0s  63/332 [====>.........................] - ETA: 0s 94/332 [=======>......................] - ETA: 0s125/332 [==========>...................] - ETA: 0s156/332 [=============>................] - ETA: 0s188/332 [===============>..............] - ETA: 0s220/332 [==================>...........] - ETA: 0s250/332 [=====================>........] - ETA: 0s282/332 [========================>.....] - ETA: 0s314/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.1299982068479578
cosine 0.1017389926389618
MAE: 0.048300974
RMSE: 0.10361899
r2: 0.3034716892597582
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 64, 60, 0.002, 0.6, 758, 0.006168858148157597, 0.006168372463434935, 0.1299982068479578, 0.1017389926389618, 0.04830097407102585, 0.10361898690462112, 0.3034716892597582, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 180 0.002 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/180
1493/1493 - 13s - loss: 0.0311 - val_loss: 0.0910 - 13s/epoch - 9ms/step
Epoch 2/180
1493/1493 - 8s - loss: 0.0164 - val_loss: 0.0149 - 8s/epoch - 5ms/step
Epoch 3/180
1493/1493 - 8s - loss: 0.0138 - val_loss: 0.0129 - 8s/epoch - 5ms/step
Epoch 4/180
1493/1493 - 8s - loss: 0.0128 - val_loss: 0.0123 - 8s/epoch - 5ms/step
Epoch 5/180
1493/1493 - 8s - loss: 0.0122 - val_loss: 0.0117 - 8s/epoch - 6ms/step
Epoch 6/180
1493/1493 - 8s - loss: 0.0118 - val_loss: 0.0115 - 8s/epoch - 5ms/step
Epoch 7/180
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0113 - 8s/epoch - 5ms/step
Epoch 8/180
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0113 - 8s/epoch - 5ms/step
Epoch 9/180
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 10/180
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 11/180
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 12/180
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 13/180
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 14/180
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 15/180
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 16/180
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 17/180
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 18/180
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 19/180
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 20/180
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 6ms/step
Epoch 21/180
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 22/180
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 23/180
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 24/180
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 25/180
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 26/180
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 27/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 28/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 29/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 30/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 31/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 32/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 33/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 34/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 35/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 36/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 37/180
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 38/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 39/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 40/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 41/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 42/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 43/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 44/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 45/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 46/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 47/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 48/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 49/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 50/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 51/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 52/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 53/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 54/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 55/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 56/180
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 57/180
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 58/180
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0106 - 8s/epoch - 6ms/step
Epoch 59/180
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 60/180
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 61/180
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 62/180
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 63/180
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 64/180
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 65/180
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 66/180
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 67/180
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 68/180
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 69/180
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 70/180
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 71/180
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 72/180
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 73/180
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 74/180
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 75/180
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 76/180
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 77/180
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 78/180
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 79/180
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 80/180
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 81/180
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 82/180
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 83/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 84/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 85/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 86/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 87/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 88/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 89/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 90/180
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 91/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 92/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 93/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 94/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 95/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 96/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 97/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 98/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 99/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 100/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 101/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 102/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 103/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 104/180
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 105/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 106/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 107/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 108/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 109/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 110/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 111/180
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 112/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 113/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 114/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 115/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 116/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 117/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 118/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 119/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 120/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 121/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 122/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 123/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 124/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 125/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 126/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 127/180
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 128/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 129/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 130/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 131/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 132/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 133/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 134/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 135/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 136/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 137/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 138/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 139/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 140/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 141/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 142/180
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 143/180
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 144/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 145/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 146/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 147/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 148/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 149/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 150/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 151/180
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 152/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 153/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 154/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 155/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 156/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 157/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 158/180
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 159/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 160/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 161/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 162/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 163/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 164/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 165/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 166/180
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 167/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 168/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 169/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 170/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 171/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 172/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 173/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 174/180
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 175/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 176/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 177/180
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 178/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 179/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 180/180
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009581278078258038
  1/332 [..............................] - ETA: 53s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s160/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s224/332 [===================>..........] - ETA: 0s255/332 [======================>.......] - ETA: 0s287/332 [========================>.....] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07928805412704254
cosine 0.06223195195310394
MAE: 0.0371715
RMSE: 0.08096974
r2: 0.5746900264262509
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'mse', 64, 180, 0.002, 0.6, 758, 0.009846475906670094, 0.009581278078258038, 0.07928805412704254, 0.06223195195310394, 0.03717150166630745, 0.08096974343061447, 0.5746900264262509, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 150 0.002 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 12s - loss: 0.0290 - val_loss: 0.0212 - 12s/epoch - 8ms/step
Epoch 2/150
1493/1493 - 8s - loss: 0.0159 - val_loss: 0.0173 - 8s/epoch - 5ms/step
Epoch 3/150
1493/1493 - 8s - loss: 0.0136 - val_loss: 0.0126 - 8s/epoch - 5ms/step
Epoch 4/150
1493/1493 - 8s - loss: 0.0124 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 5/150
1493/1493 - 8s - loss: 0.0119 - val_loss: 0.0116 - 8s/epoch - 5ms/step
Epoch 6/150
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0114 - 8s/epoch - 5ms/step
Epoch 7/150
1493/1493 - 8s - loss: 0.0115 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 8/150
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 9/150
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 10/150
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 11/150
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 12/150
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 13/150
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 14/150
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 15/150
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 6ms/step
Epoch 16/150
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 17/150
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 18/150
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 19/150
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 20/150
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 21/150
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 22/150
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 23/150
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 24/150
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 25/150
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 26/150
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 27/150
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 28/150
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 29/150
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 30/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 31/150
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 32/150
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 33/150
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 34/150
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 35/150
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 36/150
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 37/150
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 38/150
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 39/150
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 40/150
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 41/150
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 42/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 43/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 44/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 45/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 46/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 47/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 48/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 49/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 50/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 51/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 52/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 53/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 54/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 55/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 56/150
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 57/150
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 58/150
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 59/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 60/150
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 61/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 62/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 63/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 64/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 65/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 66/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 67/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 6ms/step
Epoch 68/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 69/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 70/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 71/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 72/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 73/150
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 74/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 75/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 76/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 77/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 78/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 79/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 80/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 81/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 82/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 6ms/step
Epoch 83/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 84/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 85/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 86/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 87/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 88/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 89/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 90/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 91/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 92/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 93/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 94/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 95/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 96/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 97/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 98/150
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 99/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 100/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 101/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 102/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 103/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 104/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 105/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 106/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 107/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 108/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 109/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 110/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 6ms/step
Epoch 111/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 112/150
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 113/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 114/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 115/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 116/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 117/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 118/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 119/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 120/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 121/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 122/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 123/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 124/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 125/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 126/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 127/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 128/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 129/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 130/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 131/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 132/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 133/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 134/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 135/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 136/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 137/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 138/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 139/150
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 6ms/step
Epoch 140/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 141/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 142/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 143/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 144/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 145/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 146/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 147/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 6ms/step
Epoch 148/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 149/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 150/150
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009795027785003185
  1/332 [..............................] - ETA: 51s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 95/332 [=======>......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s157/332 [=============>................] - ETA: 0s189/332 [================>.............] - ETA: 0s221/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s285/332 [========================>.....] - ETA: 0s317/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0814413660052494
cosine 0.06395543610717143
MAE: 0.037484214
RMSE: 0.082169674
r2: 0.561990906508261
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 64, 150, 0.002, 0.6, 758, 0.009926564991474152, 0.009795027785003185, 0.0814413660052494, 0.06395543610717143, 0.03748421370983124, 0.08216967433691025, 0.561990906508261, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/30
374/374 - 6s - loss: 0.0406 - val_loss: 0.0192 - 6s/epoch - 16ms/step
Epoch 2/30
374/374 - 2s - loss: 0.0163 - val_loss: 0.0195 - 2s/epoch - 6ms/step
Epoch 3/30
374/374 - 2s - loss: 0.0158 - val_loss: 0.0171 - 2s/epoch - 6ms/step
Epoch 4/30
374/374 - 2s - loss: 0.0147 - val_loss: 0.0240 - 2s/epoch - 6ms/step
Epoch 5/30
374/374 - 2s - loss: 0.0145 - val_loss: 0.0180 - 2s/epoch - 6ms/step
Epoch 6/30
374/374 - 2s - loss: 0.0140 - val_loss: 0.0173 - 2s/epoch - 6ms/step
Epoch 7/30
374/374 - 2s - loss: 0.0139 - val_loss: 0.0179 - 2s/epoch - 6ms/step
Epoch 8/30
374/374 - 2s - loss: 0.0136 - val_loss: 0.0157 - 2s/epoch - 6ms/step
Epoch 9/30
374/374 - 2s - loss: 0.0133 - val_loss: 0.0158 - 2s/epoch - 6ms/step
Epoch 10/30
374/374 - 2s - loss: 0.0131 - val_loss: 0.0138 - 2s/epoch - 7ms/step
Epoch 11/30
374/374 - 2s - loss: 0.0129 - val_loss: 0.0133 - 2s/epoch - 7ms/step
Epoch 12/30
374/374 - 2s - loss: 0.0127 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 13/30
374/374 - 2s - loss: 0.0123 - val_loss: 0.0123 - 2s/epoch - 7ms/step
Epoch 14/30
374/374 - 2s - loss: 0.0120 - val_loss: 0.0120 - 2s/epoch - 7ms/step
Epoch 15/30
374/374 - 2s - loss: 0.0118 - val_loss: 0.0201 - 2s/epoch - 7ms/step
Epoch 16/30
374/374 - 2s - loss: 0.0126 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 17/30
374/374 - 2s - loss: 0.0117 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 18/30
374/374 - 2s - loss: 0.0130 - val_loss: 0.0115 - 2s/epoch - 7ms/step
Epoch 19/30
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 20/30
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 21/30
374/374 - 2s - loss: 0.0113 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 22/30
374/374 - 2s - loss: 0.0124 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 23/30
374/374 - 2s - loss: 0.0113 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 24/30
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 25/30
374/374 - 2s - loss: 0.0116 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 26/30
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 27/30
374/374 - 2s - loss: 0.0111 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 28/30
374/374 - 2s - loss: 0.0117 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 29/30
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 30/30
374/374 - 3s - loss: 0.0111 - val_loss: 0.0113 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.011306478641927242
  1/332 [..............................] - ETA: 43s 32/332 [=>............................] - ETA: 0s  64/332 [====>.........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s160/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s224/332 [===================>..........] - ETA: 0s256/332 [======================>.......] - ETA: 0s288/332 [=========================>....] - ETA: 0s320/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.10678990054952842
cosine 0.0836953482693472
MAE: 0.04860187
RMSE: 0.09323863
r2: 0.43609483104687535
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 256, 30, 0.0005, 0.6, 758, 0.011148258112370968, 0.011306478641927242, 0.10678990054952842, 0.0836953482693472, 0.048601869493722916, 0.09323862940073013, 0.43609483104687535, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 90 0.002 256 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/90
374/374 - 6s - loss: 0.0311 - val_loss: 0.0639 - 6s/epoch - 16ms/step
Epoch 2/90
374/374 - 2s - loss: 0.0093 - val_loss: 0.1638 - 2s/epoch - 6ms/step
Epoch 3/90
374/374 - 2s - loss: 0.0095 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 4/90
374/374 - 2s - loss: 0.0082 - val_loss: 0.0287 - 2s/epoch - 6ms/step
Epoch 5/90
374/374 - 2s - loss: 0.0086 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 6/90
374/374 - 2s - loss: 0.0079 - val_loss: 0.0333 - 2s/epoch - 6ms/step
Epoch 7/90
374/374 - 2s - loss: 0.0088 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 8/90
374/374 - 2s - loss: 0.0074 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 9/90
374/374 - 2s - loss: 0.0072 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 10/90
374/374 - 2s - loss: 0.0071 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 11/90
374/374 - 2s - loss: 0.0068 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 12/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 13/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 14/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0078 - 2s/epoch - 6ms/step
Epoch 15/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 16/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 17/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 18/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0189 - 2s/epoch - 7ms/step
Epoch 19/90
374/374 - 3s - loss: 0.0090 - val_loss: 0.0079 - 3s/epoch - 7ms/step
Epoch 20/90
374/374 - 3s - loss: 0.0074 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 21/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 22/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 23/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 7ms/step
Epoch 24/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0068 - 2s/epoch - 7ms/step
Epoch 25/90
374/374 - 3s - loss: 0.0075 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 26/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 27/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0068 - 2s/epoch - 7ms/step
Epoch 28/90
374/374 - 2s - loss: 0.0072 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 29/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 30/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 31/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 32/90
374/374 - 2s - loss: 0.0073 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 33/90
374/374 - 2s - loss: 0.0093 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 34/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0080 - 2s/epoch - 7ms/step
Epoch 35/90
374/374 - 2s - loss: 0.0099 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 36/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 37/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 38/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 7ms/step
Epoch 39/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 40/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 41/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0088 - 2s/epoch - 7ms/step
Epoch 42/90
374/374 - 2s - loss: 0.0129 - val_loss: 0.0068 - 2s/epoch - 7ms/step
Epoch 43/90
374/374 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 7ms/step
Epoch 44/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 45/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0071 - 2s/epoch - 7ms/step
Epoch 46/90
374/374 - 2s - loss: 0.0078 - val_loss: 0.0089 - 2s/epoch - 7ms/step
Epoch 47/90
374/374 - 2s - loss: 0.0119 - val_loss: 0.0075 - 2s/epoch - 7ms/step
Epoch 48/90
374/374 - 2s - loss: 0.0070 - val_loss: 0.0081 - 2s/epoch - 7ms/step
Epoch 49/90
374/374 - 2s - loss: 0.0094 - val_loss: 0.0068 - 2s/epoch - 7ms/step
Epoch 50/90
374/374 - 2s - loss: 0.0069 - val_loss: 0.0067 - 2s/epoch - 7ms/step
Epoch 51/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 7ms/step
Epoch 52/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 53/90
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 54/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 55/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 56/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 7ms/step
Epoch 57/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 58/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 59/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 60/90
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 61/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 62/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 63/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 7ms/step
Epoch 64/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 65/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 66/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0072 - 2s/epoch - 7ms/step
Epoch 67/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 68/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 69/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 70/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 71/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 72/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 73/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 74/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 75/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 76/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0065 - 2s/epoch - 7ms/step
Epoch 77/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 78/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 79/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 80/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 81/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 82/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 83/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 84/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 85/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 86/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 87/90
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 88/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 7ms/step
Epoch 89/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 90/90
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006103166379034519
  1/332 [..............................] - ETA: 43s 30/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s217/332 [==================>...........] - ETA: 0s249/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s313/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.13069584741879534
cosine 0.10228151753629058
MAE: 0.048130624
RMSE: 0.10266406
r2: 0.31625098103807253
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 256, 90, 0.002, 0.6, 758, 0.0061555290594697, 0.006103166379034519, 0.13069584741879534, 0.10228151753629058, 0.04813062399625778, 0.10266406089067459, 0.31625098103807253, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 180 0.0005 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 6s - loss: 0.0401 - val_loss: 0.0633 - 6s/epoch - 15ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0171 - val_loss: 0.0187 - 2s/epoch - 6ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0159 - val_loss: 0.0175 - 2s/epoch - 6ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0147 - val_loss: 0.0191 - 2s/epoch - 6ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0144 - val_loss: 0.0165 - 2s/epoch - 6ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0142 - val_loss: 0.0193 - 2s/epoch - 6ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0139 - val_loss: 0.0169 - 2s/epoch - 6ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0214 - 2s/epoch - 6ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0141 - 2s/epoch - 6ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0129 - val_loss: 0.0140 - 2s/epoch - 6ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0130 - 2s/epoch - 7ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0141 - 2s/epoch - 7ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0139 - val_loss: 0.0141 - 2s/epoch - 6ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0135 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0118 - 2s/epoch - 7ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0201 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 7ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 7ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 7ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0106 - 2s/epoch - 7ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 7ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0110 - 2s/epoch - 7ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 80/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 81/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 82/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 7ms/step
Epoch 84/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 85/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 87/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 88/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 89/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 93/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 111/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 112/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 7ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 136/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 140/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 162/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 166/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 7ms/step
Epoch 172/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 173/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 174/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 7ms/step
Epoch 180/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 8ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009538226760923862
  1/332 [..............................] - ETA: 48s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 94/332 [=======>......................] - ETA: 0s125/332 [==========>...................] - ETA: 0s157/332 [=============>................] - ETA: 0s188/332 [===============>..............] - ETA: 0s219/332 [==================>...........] - ETA: 0s250/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07935477151225478
cosine 0.06227845178767505
MAE: 0.03708499
RMSE: 0.08084068
r2: 0.5760449883800689
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 256, 180, 0.0005, 0.6, 758, 0.009653765708208084, 0.009538226760923862, 0.07935477151225478, 0.06227845178767505, 0.03708498924970627, 0.08084067702293396, 0.5760449883800689, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.001 256 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 6s - loss: 0.0249 - val_loss: 0.0105 - 6s/epoch - 15ms/step
Epoch 2/150
374/374 - 3s - loss: 0.0086 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 3/150
374/374 - 3s - loss: 0.0081 - val_loss: 0.0087 - 3s/epoch - 7ms/step
Epoch 4/150
374/374 - 3s - loss: 0.0079 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 5/150
374/374 - 3s - loss: 0.0078 - val_loss: 0.0091 - 3s/epoch - 7ms/step
Epoch 6/150
374/374 - 2s - loss: 0.0076 - val_loss: 0.0086 - 2s/epoch - 7ms/step
Epoch 7/150
374/374 - 2s - loss: 0.0075 - val_loss: 0.0084 - 2s/epoch - 6ms/step
Epoch 8/150
374/374 - 2s - loss: 0.0074 - val_loss: 0.0088 - 2s/epoch - 6ms/step
Epoch 9/150
374/374 - 2s - loss: 0.0073 - val_loss: 0.0169 - 2s/epoch - 6ms/step
Epoch 10/150
374/374 - 2s - loss: 0.0073 - val_loss: 0.0080 - 2s/epoch - 6ms/step
Epoch 11/150
374/374 - 2s - loss: 0.0071 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 12/150
374/374 - 2s - loss: 0.0070 - val_loss: 0.0075 - 2s/epoch - 6ms/step
Epoch 13/150
374/374 - 2s - loss: 0.0069 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 14/150
374/374 - 3s - loss: 0.0069 - val_loss: 0.0071 - 3s/epoch - 7ms/step
Epoch 15/150
374/374 - 3s - loss: 0.0068 - val_loss: 0.0068 - 3s/epoch - 7ms/step
Epoch 16/150
374/374 - 3s - loss: 0.0066 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 17/150
374/374 - 3s - loss: 0.0065 - val_loss: 0.0196 - 3s/epoch - 7ms/step
Epoch 18/150
374/374 - 3s - loss: 0.0070 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 19/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 20/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 21/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 22/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 23/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 24/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 25/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 26/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 27/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 28/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 29/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 30/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 7ms/step
Epoch 31/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 32/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 33/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 34/150
374/374 - 3s - loss: 0.0068 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 35/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 7ms/step
Epoch 36/150
374/374 - 2s - loss: 0.0072 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 37/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 38/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 39/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 40/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 41/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 42/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 43/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 44/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 45/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 46/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 47/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 48/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 49/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 50/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 51/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 52/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 53/150
374/374 - 2s - loss: 0.0065 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 54/150
374/374 - 2s - loss: 0.0066 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 55/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 56/150
374/374 - 2s - loss: 0.0064 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 57/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 58/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 59/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 60/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 61/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 62/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 63/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 64/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 65/150
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 66/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 67/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 68/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 69/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 70/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 71/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 72/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 73/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 74/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 75/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 76/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 77/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 78/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 79/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 80/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 81/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 82/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 83/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 84/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 85/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 86/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 87/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 88/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 89/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 90/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 91/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 92/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 93/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 94/150
374/374 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 95/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 96/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 97/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 98/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 99/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 100/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 101/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 102/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 103/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 104/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 105/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 106/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 107/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 108/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 109/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 110/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 111/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 112/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 113/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 114/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 115/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 116/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 117/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 118/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 119/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 120/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 121/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 122/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 123/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 124/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 125/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 126/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 127/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 128/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 129/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 130/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 131/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 132/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 133/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 134/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 135/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 136/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 137/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 138/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 139/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 140/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 141/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 142/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 143/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 144/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 7ms/step
Epoch 145/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 146/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 147/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 148/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 149/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 150/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0060157570987939835
  1/332 [..............................] - ETA: 48s 31/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 91/332 [=======>......................] - ETA: 0s122/332 [==========>...................] - ETA: 0s153/332 [============>.................] - ETA: 0s183/332 [===============>..............] - ETA: 0s213/332 [==================>...........] - ETA: 0s243/332 [====================>.........] - ETA: 0s273/332 [=======================>......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12506062316694577
cosine 0.09789954154526473
MAE: 0.046934646
RMSE: 0.100540034
r2: 0.34425032659336635
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 256, 150, 0.001, 0.6, 758, 0.006064592860639095, 0.0060157570987939835, 0.12506062316694577, 0.09789954154526473, 0.046934645622968674, 0.10054003447294235, 0.34425032659336635, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.001 128 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/150
747/747 - 7s - loss: 0.0181 - val_loss: 0.0094 - 7s/epoch - 10ms/step
Epoch 2/150
747/747 - 4s - loss: 0.0085 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 3/150
747/747 - 4s - loss: 0.0080 - val_loss: 0.0087 - 4s/epoch - 6ms/step
Epoch 4/150
747/747 - 4s - loss: 0.0076 - val_loss: 0.0166 - 4s/epoch - 5ms/step
Epoch 5/150
747/747 - 4s - loss: 0.0074 - val_loss: 0.0074 - 4s/epoch - 6ms/step
Epoch 6/150
747/747 - 4s - loss: 0.0071 - val_loss: 0.0072 - 4s/epoch - 6ms/step
Epoch 7/150
747/747 - 4s - loss: 0.0070 - val_loss: 0.0069 - 4s/epoch - 6ms/step
Epoch 8/150
747/747 - 4s - loss: 0.0069 - val_loss: 0.0068 - 4s/epoch - 6ms/step
Epoch 9/150
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 10/150
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 11/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 12/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 13/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 14/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 15/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 16/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 17/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 18/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 19/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 20/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 21/150
747/747 - 4s - loss: 0.0066 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 22/150
747/747 - 4s - loss: 0.0066 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 23/150
747/747 - 4s - loss: 0.0066 - val_loss: 0.0065 - 4s/epoch - 6ms/step
Epoch 24/150
747/747 - 4s - loss: 0.0065 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 25/150
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 6ms/step
Epoch 26/150
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 27/150
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 28/150
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 29/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 30/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 31/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 32/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 33/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 34/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 35/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 36/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 37/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 38/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 39/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 40/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 41/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 42/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 43/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 44/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 45/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 46/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 47/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 48/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 49/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 50/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 51/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 52/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 53/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 54/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 55/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 56/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 57/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 58/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 59/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 60/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 61/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 62/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 63/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 64/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 65/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 66/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 67/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 68/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 69/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 70/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 71/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 72/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 73/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 74/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 75/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 76/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 77/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 78/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 79/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 80/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 81/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 82/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 83/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 84/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 85/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 86/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 87/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 88/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 89/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 90/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 91/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 92/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 93/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 94/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 95/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 96/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 97/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 98/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 99/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 100/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 101/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 102/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 103/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 104/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 105/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 106/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 107/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 108/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 109/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 110/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 111/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 112/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 113/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 114/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 115/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 116/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 117/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 118/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 119/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 120/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 121/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 122/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 123/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 124/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 125/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 126/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 127/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 128/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 129/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 130/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 131/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 132/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 133/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 134/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 135/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 136/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 137/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 138/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 139/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 140/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 141/150
747/747 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 6ms/step
Epoch 142/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 143/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 144/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 145/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 146/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 147/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 148/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 149/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
Epoch 150/150
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.005913039669394493
  1/332 [..............................] - ETA: 51s 31/332 [=>............................] - ETA: 0s  60/332 [====>.........................] - ETA: 0s 88/332 [======>.......................] - ETA: 0s118/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s180/332 [===============>..............] - ETA: 0s211/332 [==================>...........] - ETA: 0s241/332 [====================>.........] - ETA: 0s272/332 [=======================>......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11819080745010074
cosine 0.09243226167596694
MAE: 0.045215614
RMSE: 0.097857505
r2: 0.3787757783950636
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 128, 150, 0.001, 0.6, 758, 0.005969959311187267, 0.005913039669394493, 0.11819080745010074, 0.09243226167596694, 0.04521561414003372, 0.0978575050830841, 0.3787757783950636, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 150 0.002 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5959276     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,352,182
Trainable params: 13,340,050
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 14s - loss: 0.0168 - val_loss: 0.0102 - 14s/epoch - 9ms/step
Epoch 2/150
1493/1493 - 8s - loss: 0.0085 - val_loss: 0.0078 - 8s/epoch - 6ms/step
Epoch 3/150
1493/1493 - 8s - loss: 0.0073 - val_loss: 0.0071 - 8s/epoch - 5ms/step
Epoch 4/150
1493/1493 - 8s - loss: 0.0069 - val_loss: 0.0068 - 8s/epoch - 5ms/step
Epoch 5/150
1493/1493 - 8s - loss: 0.0068 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 6/150
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0067 - 8s/epoch - 5ms/step
Epoch 7/150
1493/1493 - 8s - loss: 0.0067 - val_loss: 0.0066 - 8s/epoch - 5ms/step
Epoch 8/150
1493/1493 - 8s - loss: 0.0065 - val_loss: 0.0064 - 8s/epoch - 5ms/step
Epoch 9/150
1493/1493 - 8s - loss: 0.0064 - val_loss: 0.0063 - 8s/epoch - 5ms/step
Epoch 10/150
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0063 - 8s/epoch - 5ms/step
Epoch 11/150
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 12/150
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 13/150
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 14/150
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 15/150
1493/1493 - 8s - loss: 0.0063 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 16/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 17/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 18/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 19/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 20/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 21/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 22/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 23/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 24/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 25/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 26/150
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 27/150
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 28/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 29/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 30/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 31/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 32/150
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 33/150
1493/1493 - 10s - loss: 0.0062 - val_loss: 0.0061 - 10s/epoch - 7ms/step
Epoch 34/150
1493/1493 - 9s - loss: 0.0062 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 35/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 36/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 37/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 38/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 39/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 40/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 41/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 42/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 43/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 44/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 45/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 46/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 47/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 48/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 49/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 50/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 51/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 52/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 53/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 54/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 55/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 56/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 57/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 58/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 59/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 6ms/step
Epoch 60/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 61/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 62/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 63/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 64/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 65/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 66/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 67/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 68/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 69/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 70/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 71/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 72/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 73/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 74/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 75/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 76/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 77/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 78/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 79/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 80/150
1493/1493 - 8s - loss: 0.0062 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 81/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 82/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 83/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 84/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 85/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 86/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 87/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 88/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 89/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 90/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 91/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 92/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 93/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 94/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 95/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 96/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 97/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 98/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 99/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 100/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 101/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 102/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0062 - 8s/epoch - 5ms/step
Epoch 103/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 104/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 105/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 106/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 107/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 5ms/step
Epoch 108/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 109/150
1493/1493 - 10s - loss: 0.0061 - val_loss: 0.0061 - 10s/epoch - 6ms/step
Epoch 110/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 111/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 112/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 113/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 114/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 115/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 116/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 117/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 118/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 119/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 120/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 121/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 122/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 123/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 124/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 125/150
1493/1493 - 8s - loss: 0.0061 - val_loss: 0.0061 - 8s/epoch - 6ms/step
Epoch 126/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 127/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 128/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 129/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 130/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 131/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 132/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 133/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 134/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 135/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 136/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 137/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 138/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 139/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 140/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 141/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 142/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 143/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 144/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 145/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 146/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 147/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 148/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0062 - 9s/epoch - 6ms/step
Epoch 149/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
Epoch 150/150
1493/1493 - 9s - loss: 0.0061 - val_loss: 0.0061 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00614096550270915
  1/332 [..............................] - ETA: 1:02 30/332 [=>............................] - ETA: 0s   58/332 [====>.........................] - ETA: 0s 86/332 [======>.......................] - ETA: 0s114/332 [=========>....................] - ETA: 0s144/332 [============>.................] - ETA: 0s173/332 [==============>...............] - ETA: 0s203/332 [=================>............] - ETA: 0s234/332 [====================>.........] - ETA: 0s265/332 [======================>.......] - ETA: 0s295/332 [=========================>....] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12809539395056263
cosine 0.10024135469648912
MAE: 0.04768104
RMSE: 0.102477424
r2: 0.3187342078656513
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'logcosh', 64, 150, 0.002, 0.6, 758, 0.0061329021118581295, 0.00614096550270915, 0.12809539395056263, 0.10024135469648912, 0.047681041061878204, 0.10247742384672165, 0.3187342078656513, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 145 0.001 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/145
374/374 - 9s - loss: 0.0381 - val_loss: 0.0247 - 9s/epoch - 24ms/step
Epoch 2/145
374/374 - 3s - loss: 0.0175 - val_loss: 0.0186 - 3s/epoch - 7ms/step
Epoch 3/145
374/374 - 3s - loss: 0.0154 - val_loss: 0.0202 - 3s/epoch - 7ms/step
Epoch 4/145
374/374 - 3s - loss: 0.0147 - val_loss: 0.0188 - 3s/epoch - 7ms/step
Epoch 5/145
374/374 - 3s - loss: 0.0145 - val_loss: 0.0210 - 3s/epoch - 7ms/step
Epoch 6/145
374/374 - 3s - loss: 0.0141 - val_loss: 0.0162 - 3s/epoch - 7ms/step
Epoch 7/145
374/374 - 3s - loss: 0.0138 - val_loss: 0.0153 - 3s/epoch - 7ms/step
Epoch 8/145
374/374 - 3s - loss: 0.0135 - val_loss: 0.0167 - 3s/epoch - 7ms/step
Epoch 9/145
374/374 - 3s - loss: 0.0134 - val_loss: 0.0168 - 3s/epoch - 7ms/step
Epoch 10/145
374/374 - 3s - loss: 0.0131 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 11/145
374/374 - 3s - loss: 0.0129 - val_loss: 0.0133 - 3s/epoch - 7ms/step
Epoch 12/145
374/374 - 3s - loss: 0.0127 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 13/145
374/374 - 3s - loss: 0.0126 - val_loss: 0.0208 - 3s/epoch - 7ms/step
Epoch 14/145
374/374 - 3s - loss: 0.0132 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 15/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0148 - 3s/epoch - 7ms/step
Epoch 16/145
374/374 - 3s - loss: 0.0130 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 17/145
374/374 - 3s - loss: 0.0123 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 18/145
374/374 - 3s - loss: 0.0123 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 19/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 20/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 21/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0118 - 3s/epoch - 8ms/step
Epoch 22/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 23/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 24/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 25/145
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 7ms/step
Epoch 26/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 8ms/step
Epoch 27/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 28/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 29/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 30/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 31/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 32/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 33/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 34/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 35/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 36/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 37/145
374/374 - 3s - loss: 0.0128 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 38/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 39/145
374/374 - 3s - loss: 0.0133 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 40/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 41/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 42/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 43/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 44/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 45/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 46/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 47/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 48/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 49/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 50/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 51/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 52/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 53/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 54/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 55/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 56/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 57/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 58/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 59/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 60/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 61/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 62/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 63/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 64/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 65/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 66/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 67/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 68/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 69/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 70/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 71/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 72/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 73/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 74/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 75/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 76/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 77/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 78/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 79/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 80/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 81/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 82/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 83/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 84/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 85/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 86/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 87/145
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 88/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 89/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 90/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 91/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 92/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 93/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 94/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 95/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 96/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 97/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 98/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 99/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 100/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 101/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 102/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 103/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 104/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 105/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 106/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 107/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 108/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 109/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 110/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 111/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 112/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 113/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 114/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 115/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 116/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 117/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 118/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 119/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 120/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 121/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 122/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 123/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 124/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 125/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 126/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 127/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 128/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 129/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 130/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 131/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 132/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 133/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 134/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 135/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 136/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 137/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 138/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 139/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 140/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 141/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 142/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 143/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 144/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 145/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009708504192531109
  1/332 [..............................] - ETA: 47s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s124/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s183/332 [===============>..............] - ETA: 0s212/332 [==================>...........] - ETA: 0s241/332 [====================>.........] - ETA: 0s270/332 [=======================>......] - ETA: 0s299/332 [==========================>...] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08215626104233896
cosine 0.06445394226662671
MAE: 0.038039967
RMSE: 0.08224338
r2: 0.5612048028331441
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 256, 145, 0.001, 0.6, 758, 0.009808828122913837, 0.009708504192531109, 0.08215626104233896, 0.06445394226662671, 0.03803996741771698, 0.08224338293075562, 0.5612048028331441, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5959276     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,352,182
Trainable params: 13,340,050
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/90
374/374 - 6s - loss: 0.0277 - val_loss: 0.0130 - 6s/epoch - 16ms/step
Epoch 2/90
374/374 - 2s - loss: 0.0091 - val_loss: 0.0169 - 2s/epoch - 7ms/step
Epoch 3/90
374/374 - 3s - loss: 0.0085 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 4/90
374/374 - 3s - loss: 0.0082 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 5/90
374/374 - 3s - loss: 0.0081 - val_loss: 0.0152 - 3s/epoch - 7ms/step
Epoch 6/90
374/374 - 3s - loss: 0.0090 - val_loss: 0.0092 - 3s/epoch - 7ms/step
Epoch 7/90
374/374 - 3s - loss: 0.0074 - val_loss: 0.0079 - 3s/epoch - 7ms/step
Epoch 8/90
374/374 - 3s - loss: 0.0074 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 9/90
374/374 - 3s - loss: 0.0074 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 10/90
374/374 - 3s - loss: 0.0070 - val_loss: 0.0081 - 3s/epoch - 7ms/step
Epoch 11/90
374/374 - 3s - loss: 0.0069 - val_loss: 0.0070 - 3s/epoch - 7ms/step
Epoch 12/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0084 - 3s/epoch - 7ms/step
Epoch 13/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0165 - 3s/epoch - 7ms/step
Epoch 14/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 15/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0188 - 3s/epoch - 7ms/step
Epoch 16/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 17/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 18/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 19/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 20/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 21/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 22/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 23/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 24/90
374/374 - 3s - loss: 0.0110 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 25/90
374/374 - 3s - loss: 0.0082 - val_loss: 0.0069 - 3s/epoch - 7ms/step
Epoch 26/90
374/374 - 3s - loss: 0.0073 - val_loss: 0.0077 - 3s/epoch - 7ms/step
Epoch 27/90
374/374 - 3s - loss: 0.0074 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 28/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 29/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 30/90
374/374 - 3s - loss: 0.0075 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 31/90
374/374 - 2s - loss: 0.0065 - val_loss: 0.0066 - 2s/epoch - 7ms/step
Epoch 32/90
374/374 - 2s - loss: 0.0067 - val_loss: 0.0080 - 2s/epoch - 7ms/step
Epoch 33/90
374/374 - 2s - loss: 0.0091 - val_loss: 0.0085 - 2s/epoch - 7ms/step
Epoch 34/90
374/374 - 3s - loss: 0.0102 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 35/90
374/374 - 3s - loss: 0.0151 - val_loss: 0.0073 - 3s/epoch - 8ms/step
Epoch 36/90
374/374 - 3s - loss: 0.0069 - val_loss: 0.0068 - 3s/epoch - 7ms/step
Epoch 37/90
374/374 - 3s - loss: 0.0068 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 38/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 39/90
374/374 - 2s - loss: 0.0068 - val_loss: 0.0081 - 2s/epoch - 7ms/step
Epoch 40/90
374/374 - 3s - loss: 0.0119 - val_loss: 0.0074 - 3s/epoch - 7ms/step
Epoch 41/90
374/374 - 3s - loss: 0.0075 - val_loss: 0.0068 - 3s/epoch - 7ms/step
Epoch 42/90
374/374 - 3s - loss: 0.0068 - val_loss: 0.0080 - 3s/epoch - 7ms/step
Epoch 43/90
374/374 - 3s - loss: 0.0080 - val_loss: 0.0069 - 3s/epoch - 7ms/step
Epoch 44/90
374/374 - 3s - loss: 0.0068 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 45/90
374/374 - 3s - loss: 0.0066 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 46/90
374/374 - 3s - loss: 0.0066 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 47/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 48/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 49/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 50/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 51/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 52/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 53/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 54/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 55/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 56/90
374/374 - 3s - loss: 0.0083 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 57/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0068 - 3s/epoch - 7ms/step
Epoch 58/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 59/90
374/374 - 3s - loss: 0.0066 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 60/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 61/90
374/374 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 62/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0080 - 3s/epoch - 7ms/step
Epoch 63/90
374/374 - 3s - loss: 0.0069 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 64/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 65/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 66/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 67/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0068 - 3s/epoch - 7ms/step
Epoch 68/90
374/374 - 3s - loss: 0.0065 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 69/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 70/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 71/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 72/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 73/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 74/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 75/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 76/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 77/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 78/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 79/90
374/374 - 3s - loss: 0.0063 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 80/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 81/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 82/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 83/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 84/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 85/90
374/374 - 3s - loss: 0.0061 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 86/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 87/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 88/90
374/374 - 3s - loss: 0.0067 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 89/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 90/90
374/374 - 3s - loss: 0.0062 - val_loss: 0.0064 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0064020403660833836
  1/332 [..............................] - ETA: 44s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s124/332 [==========>...................] - ETA: 0s155/332 [=============>................] - ETA: 0s185/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s246/332 [=====================>........] - ETA: 0s277/332 [========================>.....] - ETA: 0s309/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.13039138938710684
cosine 0.10205631658485419
MAE: 0.049239635
RMSE: 0.102495015
r2: 0.31850371232231506
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'logcosh', 256, 90, 0.001, 0.6, 758, 0.0061700791120529175, 0.0064020403660833836, 0.13039138938710684, 0.10205631658485419, 0.0492396354675293, 0.10249501466751099, 0.31850371232231506, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 150 0.001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 15s - loss: 0.0270 - val_loss: 0.0179 - 15s/epoch - 10ms/step
Epoch 2/150
1493/1493 - 9s - loss: 0.0149 - val_loss: 0.0139 - 9s/epoch - 6ms/step
Epoch 3/150
1493/1493 - 9s - loss: 0.0134 - val_loss: 0.0129 - 9s/epoch - 6ms/step
Epoch 4/150
1493/1493 - 9s - loss: 0.0128 - val_loss: 0.0124 - 9s/epoch - 6ms/step
Epoch 5/150
1493/1493 - 9s - loss: 0.0125 - val_loss: 0.0122 - 9s/epoch - 6ms/step
Epoch 6/150
1493/1493 - 9s - loss: 0.0124 - val_loss: 0.0122 - 9s/epoch - 6ms/step
Epoch 7/150
1493/1493 - 9s - loss: 0.0121 - val_loss: 0.0117 - 9s/epoch - 6ms/step
Epoch 8/150
1493/1493 - 9s - loss: 0.0117 - val_loss: 0.0114 - 9s/epoch - 6ms/step
Epoch 9/150
1493/1493 - 9s - loss: 0.0115 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 10/150
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 11/150
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 12/150
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 13/150
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 14/150
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 15/150
1493/1493 - 10s - loss: 0.0112 - val_loss: 0.0110 - 10s/epoch - 6ms/step
Epoch 16/150
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 17/150
1493/1493 - 10s - loss: 0.0112 - val_loss: 0.0109 - 10s/epoch - 6ms/step
Epoch 18/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 19/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 20/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 21/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 22/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 23/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 24/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 25/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 26/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 27/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 28/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 29/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 30/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 31/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 32/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 33/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 34/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 35/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 36/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 37/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 38/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 39/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 40/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 41/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 42/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 43/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 44/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 45/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 46/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 47/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 48/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 49/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 50/150
1493/1493 - 10s - loss: 0.0108 - val_loss: 0.0106 - 10s/epoch - 6ms/step
Epoch 51/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 52/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 53/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 54/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 55/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 56/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 57/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 58/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 59/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 60/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 61/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 62/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 63/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 64/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 65/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 66/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 67/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 68/150
1493/1493 - 10s - loss: 0.0106 - val_loss: 0.0105 - 10s/epoch - 7ms/step
Epoch 69/150
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 70/150
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 71/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 72/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 73/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 74/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 75/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 76/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 77/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 78/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 79/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 80/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 81/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 82/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 83/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 84/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 85/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 86/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 87/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 88/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 89/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 90/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 91/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 92/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 93/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 94/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 95/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 96/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 97/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 98/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 99/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 100/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 101/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 102/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 103/150
1493/1493 - 10s - loss: 0.0101 - val_loss: 0.0099 - 10s/epoch - 7ms/step
Epoch 104/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 105/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 106/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 107/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 108/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 109/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 110/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 111/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 112/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 113/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 114/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 115/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 116/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 117/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 118/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 119/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 120/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 121/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 122/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 123/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 124/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 125/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 126/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 127/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 128/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 129/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 130/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 131/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 132/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 133/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 134/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 135/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 136/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 137/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 138/150
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 7ms/step
Epoch 139/150
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 6ms/step
Epoch 140/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 141/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 142/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 143/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 144/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 145/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 146/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 147/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 148/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 149/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0096 - 9s/epoch - 6ms/step
Epoch 150/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009668680839240551
  1/332 [..............................] - ETA: 52s 29/332 [=>............................] - ETA: 0s  56/332 [====>.........................] - ETA: 0s 85/332 [======>.......................] - ETA: 0s115/332 [=========>....................] - ETA: 0s147/332 [============>.................] - ETA: 0s179/332 [===============>..............] - ETA: 0s210/332 [=================>............] - ETA: 0s240/332 [====================>.........] - ETA: 0s270/332 [=======================>......] - ETA: 0s300/332 [==========================>...] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08065202665405367
cosine 0.06332529307287951
MAE: 0.0374048
RMSE: 0.08157029
r2: 0.5683577284724568
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 64, 150, 0.001, 0.6, 758, 0.009889333508908749, 0.009668680839240551, 0.08065202665405367, 0.06332529307287951, 0.037404801696538925, 0.08157029002904892, 0.5683577284724568, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 150 0.001 256 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1820716     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1820716     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5446445     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,134,746
Trainable params: 12,123,626
Non-trainable params: 11,120
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 9s - loss: 0.0263 - val_loss: 0.0112 - 9s/epoch - 24ms/step
Epoch 2/150
374/374 - 2s - loss: 0.0089 - val_loss: 0.0177 - 2s/epoch - 7ms/step
Epoch 3/150
374/374 - 2s - loss: 0.0084 - val_loss: 0.0097 - 2s/epoch - 7ms/step
Epoch 4/150
374/374 - 3s - loss: 0.0080 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 5/150
374/374 - 3s - loss: 0.0080 - val_loss: 0.0136 - 3s/epoch - 8ms/step
Epoch 6/150
374/374 - 3s - loss: 0.0077 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 7/150
374/374 - 3s - loss: 0.0076 - val_loss: 0.0088 - 3s/epoch - 7ms/step
Epoch 8/150
374/374 - 3s - loss: 0.0074 - val_loss: 0.0094 - 3s/epoch - 7ms/step
Epoch 9/150
374/374 - 3s - loss: 0.0073 - val_loss: 0.0083 - 3s/epoch - 7ms/step
Epoch 10/150
374/374 - 3s - loss: 0.0071 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 11/150
374/374 - 3s - loss: 0.0070 - val_loss: 0.0075 - 3s/epoch - 7ms/step
Epoch 12/150
374/374 - 3s - loss: 0.0067 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 13/150
374/374 - 3s - loss: 0.0075 - val_loss: 0.0185 - 3s/epoch - 7ms/step
Epoch 14/150
374/374 - 3s - loss: 0.0079 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 15/150
374/374 - 3s - loss: 0.0065 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 16/150
374/374 - 3s - loss: 0.0065 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 17/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0068 - 3s/epoch - 7ms/step
Epoch 18/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 19/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 20/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 21/150
374/374 - 3s - loss: 0.0067 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 22/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 23/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0072 - 3s/epoch - 7ms/step
Epoch 24/150
374/374 - 3s - loss: 0.0068 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 25/150
374/374 - 3s - loss: 0.0069 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 26/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0070 - 3s/epoch - 7ms/step
Epoch 27/150
374/374 - 3s - loss: 0.0077 - val_loss: 0.0071 - 3s/epoch - 7ms/step
Epoch 28/150
374/374 - 3s - loss: 0.0084 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 29/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 30/150
374/374 - 3s - loss: 0.0065 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 31/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 32/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 33/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 34/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 35/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 36/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0066 - 3s/epoch - 7ms/step
Epoch 37/150
374/374 - 3s - loss: 0.0066 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 38/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0085 - 3s/epoch - 7ms/step
Epoch 39/150
374/374 - 3s - loss: 0.0080 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 40/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 41/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 42/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 43/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 44/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0067 - 3s/epoch - 7ms/step
Epoch 45/150
374/374 - 3s - loss: 0.0070 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 46/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 47/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0064 - 3s/epoch - 7ms/step
Epoch 48/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 49/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0065 - 3s/epoch - 7ms/step
Epoch 50/150
374/374 - 3s - loss: 0.0068 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 51/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 52/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 53/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 54/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 55/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 56/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 57/150
374/374 - 3s - loss: 0.0065 - val_loss: 0.0063 - 3s/epoch - 7ms/step
Epoch 58/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 59/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 60/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 61/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 62/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 63/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 64/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 65/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 66/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0064 - 3s/epoch - 8ms/step
Epoch 67/150
374/374 - 3s - loss: 0.0065 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 68/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 69/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 70/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 71/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 72/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 8ms/step
Epoch 73/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 74/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 75/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 76/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 77/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 78/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 79/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 80/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 81/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 82/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 83/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 84/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 85/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 86/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 87/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 88/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 89/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 90/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 91/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 92/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 93/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 94/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 95/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 96/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 97/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 98/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 99/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 100/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 101/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 102/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 103/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 104/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 105/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 106/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 107/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 108/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 109/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 110/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 111/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 112/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 113/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 114/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 115/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 116/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 117/150
374/374 - 3s - loss: 0.0064 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 118/150
374/374 - 3s - loss: 0.0063 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 119/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 120/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 121/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 122/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 123/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 124/150
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 125/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 126/150
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 127/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 128/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 129/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 8ms/step
Epoch 130/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 8ms/step
Epoch 131/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 132/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 133/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 134/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 8ms/step
Epoch 135/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 136/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 137/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 138/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 139/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 140/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 141/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 142/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 143/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 144/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 145/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 146/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 147/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 148/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 149/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 150/150
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0060481540858745575
  1/332 [..............................] - ETA: 48s 31/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s120/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s180/332 [===============>..............] - ETA: 0s211/332 [==================>...........] - ETA: 0s242/332 [====================>.........] - ETA: 0s273/332 [=======================>......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12658732684937865
cosine 0.0990456890467873
MAE: 0.04732998
RMSE: 0.10115484
r2: 0.3362059931794118
RMSE zero-vector: 0.23411466903540806
['1.9custom_VAE', 'logcosh', 256, 150, 0.001, 0.6, 758, 0.006086753681302071, 0.0060481540858745575, 0.12658732684937865, 0.0990456890467873, 0.04732998088002205, 0.10115484148263931, 0.3362059931794118, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 150 0.002 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 13s - loss: 0.0270 - val_loss: 0.0173 - 13s/epoch - 8ms/step
Epoch 2/150
1493/1493 - 9s - loss: 0.0155 - val_loss: 0.0143 - 9s/epoch - 6ms/step
Epoch 3/150
1493/1493 - 9s - loss: 0.0136 - val_loss: 0.0131 - 9s/epoch - 6ms/step
Epoch 4/150
1493/1493 - 9s - loss: 0.0129 - val_loss: 0.0124 - 9s/epoch - 6ms/step
Epoch 5/150
1493/1493 - 10s - loss: 0.0126 - val_loss: 0.0123 - 10s/epoch - 6ms/step
Epoch 6/150
1493/1493 - 9s - loss: 0.0124 - val_loss: 0.0121 - 9s/epoch - 6ms/step
Epoch 7/150
1493/1493 - 9s - loss: 0.0123 - val_loss: 0.0121 - 9s/epoch - 6ms/step
Epoch 8/150
1493/1493 - 9s - loss: 0.0122 - val_loss: 0.0120 - 9s/epoch - 6ms/step
Epoch 9/150
1493/1493 - 9s - loss: 0.0121 - val_loss: 0.0119 - 9s/epoch - 6ms/step
Epoch 10/150
1493/1493 - 9s - loss: 0.0119 - val_loss: 0.0115 - 9s/epoch - 6ms/step
Epoch 11/150
1493/1493 - 9s - loss: 0.0116 - val_loss: 0.0114 - 9s/epoch - 6ms/step
Epoch 12/150
1493/1493 - 9s - loss: 0.0115 - val_loss: 0.0112 - 9s/epoch - 6ms/step
Epoch 13/150
1493/1493 - 9s - loss: 0.0114 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 14/150
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0111 - 9s/epoch - 6ms/step
Epoch 15/150
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 16/150
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 17/150
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 18/150
1493/1493 - 9s - loss: 0.0112 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 19/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 20/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 21/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 22/150
1493/1493 - 9s - loss: 0.0111 - val_loss: 0.0109 - 9s/epoch - 6ms/step
Epoch 23/150
1493/1493 - 10s - loss: 0.0110 - val_loss: 0.0108 - 10s/epoch - 7ms/step
Epoch 24/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 25/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 26/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 27/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 28/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 29/150
1493/1493 - 9s - loss: 0.0110 - val_loss: 0.0108 - 9s/epoch - 6ms/step
Epoch 30/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 31/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 32/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 33/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 34/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 35/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 36/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 37/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 38/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 39/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 40/150
1493/1493 - 9s - loss: 0.0109 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 41/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0107 - 9s/epoch - 6ms/step
Epoch 42/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 43/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 44/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 45/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 46/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 47/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 48/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 49/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 50/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 51/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 52/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 53/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 54/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 55/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 56/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 57/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 58/150
1493/1493 - 10s - loss: 0.0108 - val_loss: 0.0106 - 10s/epoch - 7ms/step
Epoch 59/150
1493/1493 - 10s - loss: 0.0107 - val_loss: 0.0106 - 10s/epoch - 6ms/step
Epoch 60/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 61/150
1493/1493 - 9s - loss: 0.0108 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 62/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 63/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 64/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 65/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 66/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 67/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 68/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 69/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 70/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 71/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 72/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 73/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 74/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 75/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 76/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 77/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 78/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 79/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 80/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 81/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 82/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 83/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0106 - 9s/epoch - 6ms/step
Epoch 84/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 85/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 86/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 87/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 88/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 89/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 90/150
1493/1493 - 9s - loss: 0.0107 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 91/150
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 92/150
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0105 - 9s/epoch - 6ms/step
Epoch 93/150
1493/1493 - 10s - loss: 0.0106 - val_loss: 0.0104 - 10s/epoch - 6ms/step
Epoch 94/150
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 95/150
1493/1493 - 9s - loss: 0.0106 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 96/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 97/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 98/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0104 - 9s/epoch - 6ms/step
Epoch 99/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 100/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 101/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 102/150
1493/1493 - 9s - loss: 0.0105 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 103/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 104/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0103 - 9s/epoch - 6ms/step
Epoch 105/150
1493/1493 - 9s - loss: 0.0104 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 106/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0102 - 9s/epoch - 6ms/step
Epoch 107/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 108/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 109/150
1493/1493 - 9s - loss: 0.0103 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 110/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 111/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0101 - 9s/epoch - 6ms/step
Epoch 112/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0100 - 9s/epoch - 6ms/step
Epoch 113/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 114/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 115/150
1493/1493 - 9s - loss: 0.0102 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 116/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 117/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 118/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 119/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 120/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 121/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 122/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 123/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 124/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 125/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 126/150
1493/1493 - 9s - loss: 0.0101 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 127/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0099 - 9s/epoch - 6ms/step
Epoch 128/150
1493/1493 - 10s - loss: 0.0100 - val_loss: 0.0098 - 10s/epoch - 7ms/step
Epoch 129/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 130/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 131/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 132/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 133/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 134/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 135/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 136/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 137/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 138/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 139/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 140/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 141/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 142/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 143/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 144/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 145/150
1493/1493 - 9s - loss: 0.0100 - val_loss: 0.0098 - 9s/epoch - 6ms/step
Epoch 146/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 147/150
1493/1493 - 10s - loss: 0.0099 - val_loss: 0.0097 - 10s/epoch - 7ms/step
Epoch 148/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 149/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
Epoch 150/150
1493/1493 - 9s - loss: 0.0099 - val_loss: 0.0097 - 9s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009719467721879482
  1/332 [..............................] - ETA: 50s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 91/332 [=======>......................] - ETA: 0s121/332 [=========>....................] - ETA: 0s151/332 [============>.................] - ETA: 0s182/332 [===============>..............] - ETA: 0s213/332 [==================>...........] - ETA: 0s244/332 [=====================>........] - ETA: 0s275/332 [=======================>......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08127027931997023
cosine 0.06379944886457592
MAE: 0.03741507
RMSE: 0.082009755
r2: 0.5636942531886635
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 64, 150, 0.002, 0.6, 758, 0.009930829517543316, 0.009719467721879482, 0.08127027931997023, 0.06379944886457592, 0.037415068596601486, 0.08200975507497787, 0.5636942531886635, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[1.4 145 0.0012000000000000001 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/145
374/374 - 6s - loss: 0.0382 - val_loss: 0.0288 - 6s/epoch - 16ms/step
Epoch 2/145
374/374 - 3s - loss: 0.0166 - val_loss: 0.0308 - 3s/epoch - 7ms/step
Epoch 3/145
374/374 - 3s - loss: 0.0156 - val_loss: 0.0250 - 3s/epoch - 7ms/step
Epoch 4/145
374/374 - 3s - loss: 0.0152 - val_loss: 0.0399 - 3s/epoch - 7ms/step
Epoch 5/145
374/374 - 3s - loss: 0.0146 - val_loss: 0.0157 - 3s/epoch - 7ms/step
Epoch 6/145
374/374 - 3s - loss: 0.0141 - val_loss: 0.0174 - 3s/epoch - 7ms/step
Epoch 7/145
374/374 - 3s - loss: 0.0139 - val_loss: 0.0163 - 3s/epoch - 7ms/step
Epoch 8/145
374/374 - 3s - loss: 0.0137 - val_loss: 0.0160 - 3s/epoch - 7ms/step
Epoch 9/145
374/374 - 3s - loss: 0.0134 - val_loss: 0.0151 - 3s/epoch - 7ms/step
Epoch 10/145
374/374 - 3s - loss: 0.0132 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 11/145
374/374 - 3s - loss: 0.0130 - val_loss: 0.0138 - 3s/epoch - 7ms/step
Epoch 12/145
374/374 - 3s - loss: 0.0128 - val_loss: 0.0133 - 3s/epoch - 7ms/step
Epoch 13/145
374/374 - 3s - loss: 0.0126 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 14/145
374/374 - 3s - loss: 0.0126 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 15/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 16/145
374/374 - 3s - loss: 0.0123 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 17/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 18/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 19/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 20/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 21/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 22/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 23/145
374/374 - 3s - loss: 0.0132 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 24/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 25/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 26/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 27/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 28/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 29/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 30/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 9ms/step
Epoch 31/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 32/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 33/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 34/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 35/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 36/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 37/145
374/374 - 3s - loss: 0.0127 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 38/145
374/374 - 3s - loss: 0.0153 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 39/145
374/374 - 3s - loss: 0.0147 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 40/145
374/374 - 3s - loss: 0.0123 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 41/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 42/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 43/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 44/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 45/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 46/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 47/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 48/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 49/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 50/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 51/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 52/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 53/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 54/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 55/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 56/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 57/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 58/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 59/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 60/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 61/145
374/374 - 3s - loss: 0.0130 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 62/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 63/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 64/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 65/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 66/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 67/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 68/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 69/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 70/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 71/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 72/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 73/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 74/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 75/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 76/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 77/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 78/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 79/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 80/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 81/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 82/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 83/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 84/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 85/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 86/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 87/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 88/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 89/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 90/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 91/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 92/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 8ms/step
Epoch 93/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 94/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 95/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 96/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 97/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 8ms/step
Epoch 98/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 99/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 100/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 101/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 102/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 103/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 104/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 105/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 106/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 107/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 108/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 109/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 110/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 111/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 112/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 113/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 114/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 115/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 116/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 117/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 118/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 119/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 120/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 121/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 122/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 123/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 124/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 125/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 126/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 127/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 128/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 129/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 130/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 131/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 132/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 133/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 134/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 135/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 136/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 137/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 138/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 139/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 140/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 141/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 142/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 143/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 144/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 145/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009674199856817722
  1/332 [..............................] - ETA: 52s 28/332 [=>............................] - ETA: 0s  57/332 [====>.........................] - ETA: 0s 87/332 [======>.......................] - ETA: 0s117/332 [=========>....................] - ETA: 0s146/332 [============>.................] - ETA: 0s177/332 [==============>...............] - ETA: 0s207/332 [=================>............] - ETA: 0s238/332 [====================>.........] - ETA: 0s269/332 [=======================>......] - ETA: 0s299/332 [==========================>...] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08028037769701002
cosine 0.06298917610039587
MAE: 0.03751695
RMSE: 0.081267476
r2: 0.5715566288161135
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 256, 145, 0.0012000000000000001, 0.6, 758, 0.009755097329616547, 0.009674199856817722, 0.08028037769701002, 0.06298917610039587, 0.03751695156097412, 0.08126747608184814, 0.5715566288161135, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.001 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/85
747/747 - 8s - loss: 0.0202 - val_loss: 0.0106 - 8s/epoch - 10ms/step
Epoch 2/85
747/747 - 5s - loss: 0.0088 - val_loss: 0.0238 - 5s/epoch - 7ms/step
Epoch 3/85
747/747 - 5s - loss: 0.0083 - val_loss: 0.0085 - 5s/epoch - 6ms/step
Epoch 4/85
747/747 - 5s - loss: 0.0078 - val_loss: 0.0084 - 5s/epoch - 6ms/step
Epoch 5/85
747/747 - 5s - loss: 0.0074 - val_loss: 0.0087 - 5s/epoch - 6ms/step
Epoch 6/85
747/747 - 4s - loss: 0.0071 - val_loss: 0.0088 - 4s/epoch - 6ms/step
Epoch 7/85
747/747 - 5s - loss: 0.0069 - val_loss: 0.0069 - 5s/epoch - 6ms/step
Epoch 8/85
747/747 - 5s - loss: 0.0066 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 9/85
747/747 - 5s - loss: 0.0065 - val_loss: 0.0064 - 5s/epoch - 6ms/step
Epoch 10/85
747/747 - 5s - loss: 0.0064 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 11/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 12/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0064 - 5s/epoch - 6ms/step
Epoch 13/85
747/747 - 5s - loss: 0.0064 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 14/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 15/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 16/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 17/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0064 - 5s/epoch - 6ms/step
Epoch 18/85
747/747 - 5s - loss: 0.0065 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 19/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 20/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 21/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 22/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 23/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 24/85
747/747 - 5s - loss: 0.0065 - val_loss: 0.0065 - 5s/epoch - 6ms/step
Epoch 25/85
747/747 - 5s - loss: 0.0069 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 26/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 27/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 28/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 29/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 30/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 31/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 32/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 33/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 34/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 35/85
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 36/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0066 - 5s/epoch - 6ms/step
Epoch 37/85
747/747 - 5s - loss: 0.0063 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 38/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 39/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 40/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 41/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 42/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 43/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 44/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 45/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 46/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 47/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 48/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 49/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 50/85
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 51/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 52/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 53/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 54/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 55/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 56/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 57/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 58/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 59/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 60/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 61/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 62/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 63/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 64/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 65/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 66/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 67/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 68/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 69/85
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 70/85
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 71/85
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 72/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 73/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 74/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 75/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 76/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 77/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 78/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 79/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 80/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 81/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 82/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 83/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 84/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 85/85
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006064963527023792
  1/332 [..............................] - ETA: 51s 29/332 [=>............................] - ETA: 0s  58/332 [====>.........................] - ETA: 0s 88/332 [======>.......................] - ETA: 0s118/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s179/332 [===============>..............] - ETA: 0s209/332 [=================>............] - ETA: 0s240/332 [====================>.........] - ETA: 0s270/332 [=======================>......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.1272046714908799
cosine 0.0995710259096334
MAE: 0.047598712
RMSE: 0.10130882
r2: 0.33418352282374236
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'logcosh', 128, 85, 0.001, 0.6, 758, 0.006102964282035828, 0.006064963527023792, 0.1272046714908799, 0.0995710259096334, 0.047598712146282196, 0.10130882263183594, 0.33418352282374236, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 180 0.0005 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2107998     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2107998     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6214678     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,958,494
Trainable params: 13,945,858
Non-trainable params: 12,636
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 10s - loss: 0.0471 - val_loss: 0.0236 - 10s/epoch - 26ms/step
Epoch 2/180
374/374 - 3s - loss: 0.0174 - val_loss: 0.0207 - 3s/epoch - 7ms/step
Epoch 3/180
374/374 - 3s - loss: 0.0164 - val_loss: 0.0286 - 3s/epoch - 7ms/step
Epoch 4/180
374/374 - 3s - loss: 0.0158 - val_loss: 0.0341 - 3s/epoch - 7ms/step
Epoch 5/180
374/374 - 3s - loss: 0.0152 - val_loss: 0.0158 - 3s/epoch - 7ms/step
Epoch 6/180
374/374 - 3s - loss: 0.0139 - val_loss: 0.0192 - 3s/epoch - 7ms/step
Epoch 7/180
374/374 - 3s - loss: 0.0137 - val_loss: 0.0167 - 3s/epoch - 7ms/step
Epoch 8/180
374/374 - 3s - loss: 0.0132 - val_loss: 0.0146 - 3s/epoch - 7ms/step
Epoch 9/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0154 - 3s/epoch - 7ms/step
Epoch 10/180
374/374 - 3s - loss: 0.0124 - val_loss: 0.0238 - 3s/epoch - 7ms/step
Epoch 11/180
374/374 - 3s - loss: 0.0125 - val_loss: 0.0347 - 3s/epoch - 7ms/step
Epoch 12/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 13/180
374/374 - 3s - loss: 0.0119 - val_loss: 0.0415 - 3s/epoch - 7ms/step
Epoch 14/180
374/374 - 3s - loss: 0.0155 - val_loss: 0.0119 - 3s/epoch - 8ms/step
Epoch 15/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 16/180
374/374 - 3s - loss: 0.0126 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 17/180
374/374 - 3s - loss: 0.0130 - val_loss: 0.0165 - 3s/epoch - 7ms/step
Epoch 18/180
374/374 - 3s - loss: 0.0500 - val_loss: 0.0167 - 3s/epoch - 7ms/step
Epoch 19/180
374/374 - 3s - loss: 0.0295 - val_loss: 0.0156 - 3s/epoch - 7ms/step
Epoch 20/180
374/374 - 3s - loss: 0.0164 - val_loss: 0.0168 - 3s/epoch - 8ms/step
Epoch 21/180
374/374 - 3s - loss: 0.0198 - val_loss: 0.0144 - 3s/epoch - 7ms/step
Epoch 22/180
374/374 - 3s - loss: 0.0148 - val_loss: 0.0142 - 3s/epoch - 7ms/step
Epoch 23/180
374/374 - 3s - loss: 0.0190 - val_loss: 0.0150 - 3s/epoch - 7ms/step
Epoch 24/180
374/374 - 3s - loss: 0.0175 - val_loss: 0.0139 - 3s/epoch - 7ms/step
Epoch 25/180
374/374 - 3s - loss: 0.0134 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 26/180
374/374 - 3s - loss: 0.0129 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 27/180
374/374 - 3s - loss: 0.0126 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 28/180
374/374 - 3s - loss: 0.0142 - val_loss: 0.0156 - 3s/epoch - 7ms/step
Epoch 29/180
374/374 - 3s - loss: 0.0459 - val_loss: 0.0174 - 3s/epoch - 7ms/step
Epoch 30/180
374/374 - 3s - loss: 0.0420 - val_loss: 0.0169 - 3s/epoch - 7ms/step
Epoch 31/180
374/374 - 3s - loss: 0.0153 - val_loss: 0.0144 - 3s/epoch - 7ms/step
Epoch 32/180
374/374 - 3s - loss: 0.0144 - val_loss: 0.0142 - 3s/epoch - 7ms/step
Epoch 33/180
374/374 - 3s - loss: 0.0140 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 34/180
374/374 - 3s - loss: 0.0138 - val_loss: 0.0145 - 3s/epoch - 7ms/step
Epoch 35/180
374/374 - 3s - loss: 0.0167 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 36/180
374/374 - 3s - loss: 0.0133 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 37/180
374/374 - 3s - loss: 0.0168 - val_loss: 0.0138 - 3s/epoch - 7ms/step
Epoch 38/180
374/374 - 3s - loss: 0.0146 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 39/180
374/374 - 3s - loss: 0.0136 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 40/180
374/374 - 3s - loss: 0.0133 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 41/180
374/374 - 3s - loss: 0.0126 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 42/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 43/180
374/374 - 3s - loss: 0.0125 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 44/180
374/374 - 3s - loss: 0.0123 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 45/180
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 46/180
374/374 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 47/180
374/374 - 3s - loss: 0.0119 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 48/180
374/374 - 3s - loss: 0.0119 - val_loss: 0.0167 - 3s/epoch - 7ms/step
Epoch 49/180
374/374 - 3s - loss: 0.0196 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 50/180
374/374 - 3s - loss: 0.0124 - val_loss: 0.0140 - 3s/epoch - 7ms/step
Epoch 51/180
374/374 - 3s - loss: 0.0155 - val_loss: 0.0138 - 3s/epoch - 7ms/step
Epoch 52/180
374/374 - 3s - loss: 0.0147 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 53/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 54/180
374/374 - 3s - loss: 0.0120 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 55/180
374/374 - 3s - loss: 0.0122 - val_loss: 0.0168 - 3s/epoch - 7ms/step
Epoch 56/180
374/374 - 3s - loss: 0.0201 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 57/180
374/374 - 3s - loss: 0.0121 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 58/180
374/374 - 3s - loss: 0.0119 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 59/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 60/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 61/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 62/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 63/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0154 - 3s/epoch - 7ms/step
Epoch 64/180
374/374 - 3s - loss: 0.0154 - val_loss: 0.0173 - 3s/epoch - 7ms/step
Epoch 65/180
374/374 - 3s - loss: 0.0306 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 66/180
374/374 - 3s - loss: 0.0125 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 67/180
374/374 - 3s - loss: 0.0133 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 68/180
374/374 - 3s - loss: 0.0120 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 69/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 70/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 71/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 72/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 73/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 74/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0190 - 3s/epoch - 7ms/step
Epoch 75/180
374/374 - 3s - loss: 0.0225 - val_loss: 0.0126 - 3s/epoch - 8ms/step
Epoch 76/180
374/374 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 8ms/step
Epoch 77/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 78/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 79/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0116 - 2s/epoch - 7ms/step
Epoch 81/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 82/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 83/180
374/374 - 3s - loss: 0.0124 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 84/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 85/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0203 - 3s/epoch - 7ms/step
Epoch 86/180
374/374 - 3s - loss: 0.0135 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 87/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 88/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0236 - 3s/epoch - 7ms/step
Epoch 89/180
374/374 - 3s - loss: 0.0208 - val_loss: 0.0223 - 3s/epoch - 7ms/step
Epoch 90/180
374/374 - 3s - loss: 0.0267 - val_loss: 0.0146 - 3s/epoch - 7ms/step
Epoch 91/180
374/374 - 3s - loss: 0.0143 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 92/180
374/374 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 93/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 94/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 95/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 96/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 97/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 98/180
374/374 - 3s - loss: 0.0122 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 99/180
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 100/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 101/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 102/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0228 - 3s/epoch - 7ms/step
Epoch 103/180
374/374 - 3s - loss: 0.0155 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 104/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 105/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 106/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 107/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 108/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 109/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 110/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 111/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 112/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 113/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0133 - 3s/epoch - 7ms/step
Epoch 114/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 115/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0190 - 3s/epoch - 7ms/step
Epoch 116/180
374/374 - 3s - loss: 0.0244 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 117/180
374/374 - 3s - loss: 0.0112 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 118/180
374/374 - 3s - loss: 0.0134 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 119/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 120/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 121/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 122/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 123/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 124/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 125/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 126/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 127/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 128/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 129/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 130/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0149 - 3s/epoch - 7ms/step
Epoch 131/180
374/374 - 3s - loss: 0.0151 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 132/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 133/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 134/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 135/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 136/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 137/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 138/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 139/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 140/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 141/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0140 - 3s/epoch - 7ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 143/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 8ms/step
Epoch 144/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 145/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 146/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 147/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 148/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 149/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 150/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 151/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 152/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 153/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 154/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 155/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 156/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 157/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 158/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 159/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 160/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 161/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 162/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 163/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 164/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 165/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 166/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 167/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 168/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 169/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 170/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 171/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 172/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 173/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 174/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 175/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 176/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 177/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 178/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 179/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 180/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0099 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009930228814482689
  1/332 [..............................] - ETA: 48s 31/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 87/332 [======>.......................] - ETA: 0s115/332 [=========>....................] - ETA: 0s143/332 [===========>..................] - ETA: 0s172/332 [==============>...............] - ETA: 0s200/332 [=================>............] - ETA: 0s231/332 [===================>..........] - ETA: 0s262/332 [======================>.......] - ETA: 0s293/332 [=========================>....] - ETA: 0s323/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08452150202049115
cosine 0.06633429917307831
MAE: 0.03872455
RMSE: 0.08329776
r2: 0.5498826074863742
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'mse', 256, 180, 0.0005, 0.6, 758, 0.009874345734715462, 0.009930228814482689, 0.08452150202049115, 0.06633429917307831, 0.03872454911470413, 0.08329775929450989, 0.5498826074863742, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 145 0.0012000000000000001 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/145
374/374 - 9s - loss: 0.0384 - val_loss: 0.0196 - 9s/epoch - 25ms/step
Epoch 2/145
374/374 - 3s - loss: 0.0165 - val_loss: 0.0216 - 3s/epoch - 7ms/step
Epoch 3/145
374/374 - 3s - loss: 0.0154 - val_loss: 0.0173 - 3s/epoch - 7ms/step
Epoch 4/145
374/374 - 3s - loss: 0.0147 - val_loss: 0.0172 - 3s/epoch - 8ms/step
Epoch 5/145
374/374 - 3s - loss: 0.0142 - val_loss: 0.0165 - 3s/epoch - 7ms/step
Epoch 6/145
374/374 - 3s - loss: 0.0140 - val_loss: 0.0253 - 3s/epoch - 7ms/step
Epoch 7/145
374/374 - 3s - loss: 0.0138 - val_loss: 0.0148 - 3s/epoch - 7ms/step
Epoch 8/145
374/374 - 3s - loss: 0.0135 - val_loss: 0.0209 - 3s/epoch - 7ms/step
Epoch 9/145
374/374 - 3s - loss: 0.0134 - val_loss: 0.0145 - 3s/epoch - 7ms/step
Epoch 10/145
374/374 - 3s - loss: 0.0131 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 11/145
374/374 - 3s - loss: 0.0129 - val_loss: 0.0137 - 3s/epoch - 7ms/step
Epoch 12/145
374/374 - 3s - loss: 0.0127 - val_loss: 0.0131 - 3s/epoch - 7ms/step
Epoch 13/145
374/374 - 3s - loss: 0.0126 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 14/145
374/374 - 3s - loss: 0.0125 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 15/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 16/145
374/374 - 3s - loss: 0.0123 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 17/145
374/374 - 3s - loss: 0.0123 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 18/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 19/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 20/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 21/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 22/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 23/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 24/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 25/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 26/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 27/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 28/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 29/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 30/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 31/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 32/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 33/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 34/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 35/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 36/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 37/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 38/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 39/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 40/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 41/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 42/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 43/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 44/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 45/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 46/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 47/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 48/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 49/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 50/145
374/374 - 3s - loss: 0.0130 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 51/145
374/374 - 3s - loss: 0.0132 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 52/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 53/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 54/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 55/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 56/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 57/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 58/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 59/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 60/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 61/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 62/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 63/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 64/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 65/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 8ms/step
Epoch 66/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 67/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 68/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 69/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 70/145
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 71/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 72/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 73/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 74/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 75/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 76/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 77/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 78/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 79/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 80/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 81/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 82/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 83/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 84/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 85/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 86/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 87/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 88/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 89/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 90/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 91/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 92/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 93/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 94/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 95/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 96/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 97/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 98/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 99/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 100/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 101/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 102/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 103/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 104/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 105/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 106/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 107/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 108/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 109/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 110/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 111/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 112/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 113/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 114/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 115/145
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 116/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 117/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 118/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 119/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 120/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 121/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 122/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 123/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 124/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 125/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 126/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 127/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 128/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 129/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 130/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 131/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 132/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 133/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 8ms/step
Epoch 134/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 135/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 136/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 137/145
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 138/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 139/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 140/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 141/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 142/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 143/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 144/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 145/145
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009713147766888142
  1/332 [..............................] - ETA: 48s 31/332 [=>............................] - ETA: 0s  61/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s248/332 [=====================>........] - ETA: 0s280/332 [========================>.....] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08235925044534434
cosine 0.064621601690448
MAE: 0.03765889
RMSE: 0.08231504
r2: 0.560439724831722
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 256, 145, 0.0012000000000000001, 0.6, 758, 0.009823501110076904, 0.009713147766888142, 0.08235925044534434, 0.064621601690448, 0.03765888884663582, 0.08231504261493683, 0.560439724831722, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 145 0.001 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/145
747/747 - 8s - loss: 0.0300 - val_loss: 0.0179 - 8s/epoch - 11ms/step
Epoch 2/145
747/747 - 5s - loss: 0.0158 - val_loss: 0.0180 - 5s/epoch - 6ms/step
Epoch 3/145
747/747 - 5s - loss: 0.0149 - val_loss: 0.0178 - 5s/epoch - 6ms/step
Epoch 4/145
747/747 - 5s - loss: 0.0142 - val_loss: 0.0169 - 5s/epoch - 6ms/step
Epoch 5/145
747/747 - 5s - loss: 0.0137 - val_loss: 0.0139 - 5s/epoch - 6ms/step
Epoch 6/145
747/747 - 5s - loss: 0.0131 - val_loss: 0.0131 - 5s/epoch - 6ms/step
Epoch 7/145
747/747 - 5s - loss: 0.0128 - val_loss: 0.0129 - 5s/epoch - 6ms/step
Epoch 8/145
747/747 - 5s - loss: 0.0126 - val_loss: 0.0125 - 5s/epoch - 6ms/step
Epoch 9/145
747/747 - 5s - loss: 0.0124 - val_loss: 0.0122 - 5s/epoch - 6ms/step
Epoch 10/145
747/747 - 5s - loss: 0.0123 - val_loss: 0.0120 - 5s/epoch - 7ms/step
Epoch 11/145
747/747 - 5s - loss: 0.0122 - val_loss: 0.0120 - 5s/epoch - 7ms/step
Epoch 12/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0119 - 5s/epoch - 6ms/step
Epoch 13/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0119 - 5s/epoch - 7ms/step
Epoch 14/145
747/747 - 5s - loss: 0.0120 - val_loss: 0.0118 - 5s/epoch - 6ms/step
Epoch 15/145
747/747 - 5s - loss: 0.0120 - val_loss: 0.0119 - 5s/epoch - 6ms/step
Epoch 16/145
747/747 - 5s - loss: 0.0120 - val_loss: 0.0118 - 5s/epoch - 6ms/step
Epoch 17/145
747/747 - 5s - loss: 0.0119 - val_loss: 0.0118 - 5s/epoch - 6ms/step
Epoch 18/145
747/747 - 5s - loss: 0.0119 - val_loss: 0.0117 - 5s/epoch - 6ms/step
Epoch 19/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0117 - 5s/epoch - 6ms/step
Epoch 20/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 21/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0117 - 5s/epoch - 6ms/step
Epoch 22/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0117 - 5s/epoch - 6ms/step
Epoch 23/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 24/145
747/747 - 5s - loss: 0.0117 - val_loss: 0.0115 - 5s/epoch - 6ms/step
Epoch 25/145
747/747 - 5s - loss: 0.0116 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 26/145
747/747 - 5s - loss: 0.0114 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 27/145
747/747 - 5s - loss: 0.0113 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 28/145
747/747 - 5s - loss: 0.0112 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 29/145
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 30/145
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 31/145
747/747 - 5s - loss: 0.0110 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 32/145
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 33/145
747/747 - 5s - loss: 0.0109 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 34/145
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 35/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 36/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 37/145
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 38/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 39/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 7ms/step
Epoch 40/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 41/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 42/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 43/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 44/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 7ms/step
Epoch 45/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 46/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 47/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 48/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 49/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 50/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 51/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 52/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 53/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 54/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 55/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 56/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 57/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 58/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 59/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 60/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 61/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 62/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 63/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 64/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 65/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 66/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 67/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 68/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 69/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 70/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 71/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 72/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 73/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 74/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 75/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 76/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 77/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 78/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 7ms/step
Epoch 79/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 80/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 7ms/step
Epoch 81/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 82/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 83/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 84/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 85/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 86/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 87/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 88/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 89/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 90/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 91/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 92/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 93/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 94/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 95/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 96/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 97/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 98/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 99/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 100/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 101/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 102/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 103/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 104/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 105/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 106/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 107/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 108/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 109/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 110/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 111/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 112/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 113/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 114/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 115/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 116/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 117/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 118/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 119/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 120/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 121/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 122/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 123/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 124/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 125/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 126/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 127/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 128/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 129/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 130/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 131/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 132/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 133/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 134/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 135/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 136/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 137/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 7ms/step
Epoch 138/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 139/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 140/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 141/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 142/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0097 - 5s/epoch - 6ms/step
Epoch 143/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 144/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 6ms/step
Epoch 145/145
747/747 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009632447734475136
  1/332 [..............................] - ETA: 50s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s216/332 [==================>...........] - ETA: 0s247/332 [=====================>........] - ETA: 0s278/332 [========================>.....] - ETA: 0s308/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08109572143924043
cosine 0.06361354143831625
MAE: 0.037165593
RMSE: 0.0816993
r2: 0.5669912009295811
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 128, 145, 0.001, 0.6, 758, 0.009825066663324833, 0.009632447734475136, 0.08109572143924043, 0.06361354143831625, 0.03716559335589409, 0.08169929683208466, 0.5669912009295811, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 90 0.001 128 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5959276     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,352,182
Trainable params: 13,340,050
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/90
747/747 - 8s - loss: 0.0204 - val_loss: 0.0119 - 8s/epoch - 11ms/step
Epoch 2/90
747/747 - 5s - loss: 0.0091 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 3/90
747/747 - 5s - loss: 0.0083 - val_loss: 0.0086 - 5s/epoch - 6ms/step
Epoch 4/90
747/747 - 5s - loss: 0.0077 - val_loss: 0.0088 - 5s/epoch - 6ms/step
Epoch 5/90
747/747 - 5s - loss: 0.0074 - val_loss: 0.0077 - 5s/epoch - 6ms/step
Epoch 6/90
747/747 - 5s - loss: 0.0071 - val_loss: 0.0079 - 5s/epoch - 6ms/step
Epoch 7/90
747/747 - 5s - loss: 0.0070 - val_loss: 0.0069 - 5s/epoch - 6ms/step
Epoch 8/90
747/747 - 5s - loss: 0.0068 - val_loss: 0.0073 - 5s/epoch - 6ms/step
Epoch 9/90
747/747 - 5s - loss: 0.0066 - val_loss: 0.0065 - 5s/epoch - 6ms/step
Epoch 10/90
747/747 - 5s - loss: 0.0064 - val_loss: 0.0065 - 5s/epoch - 6ms/step
Epoch 11/90
747/747 - 5s - loss: 0.0068 - val_loss: 0.0064 - 5s/epoch - 6ms/step
Epoch 12/90
747/747 - 5s - loss: 0.0064 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 13/90
747/747 - 5s - loss: 0.0063 - val_loss: 0.0065 - 5s/epoch - 6ms/step
Epoch 14/90
747/747 - 5s - loss: 0.0066 - val_loss: 0.0063 - 5s/epoch - 6ms/step
Epoch 15/90
747/747 - 5s - loss: 0.0065 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 16/90
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 17/90
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 18/90
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 19/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 20/90
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 21/90
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 6ms/step
Epoch 22/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 23/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 24/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 25/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 26/90
747/747 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 27/90
747/747 - 5s - loss: 0.0063 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 28/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 29/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 30/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 31/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 32/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 33/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 6ms/step
Epoch 34/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 35/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 36/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 37/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 38/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 39/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 40/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 41/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 42/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 43/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 44/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 45/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 46/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 47/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 48/90
747/747 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 49/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 50/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 51/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 52/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 53/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 54/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 55/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 56/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 57/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 58/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 59/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 60/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 61/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 62/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 63/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 64/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 65/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 66/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 67/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 68/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 69/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 70/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 71/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 72/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 73/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 74/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 75/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 76/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 77/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 78/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 79/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 80/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 81/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 82/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 83/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 84/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 85/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 86/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 87/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 88/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 89/90
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 90/90
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006054394878447056
  1/332 [..............................] - ETA: 52s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s153/332 [============>.................] - ETA: 0s183/332 [===============>..............] - ETA: 0s214/332 [==================>...........] - ETA: 0s244/332 [=====================>........] - ETA: 0s275/332 [=======================>......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12612790748887004
cosine 0.098712244878119
MAE: 0.047101256
RMSE: 0.10097707
r2: 0.33853706021174707
RMSE zero-vector: 0.23411466903540806
['2.1custom_VAE', 'logcosh', 128, 90, 0.001, 0.6, 758, 0.006102765444666147, 0.006054394878447056, 0.12612790748887004, 0.098712244878119, 0.047101255506277084, 0.10097707062959671, 0.33853706021174707, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 85 0.0005 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/85
374/374 - 6s - loss: 0.0455 - val_loss: 0.0249 - 6s/epoch - 17ms/step
Epoch 2/85
374/374 - 3s - loss: 0.0179 - val_loss: 0.0186 - 3s/epoch - 7ms/step
Epoch 3/85
374/374 - 3s - loss: 0.0160 - val_loss: 0.0216 - 3s/epoch - 7ms/step
Epoch 4/85
374/374 - 3s - loss: 0.0147 - val_loss: 0.0214 - 3s/epoch - 7ms/step
Epoch 5/85
374/374 - 3s - loss: 0.0148 - val_loss: 0.0156 - 3s/epoch - 7ms/step
Epoch 6/85
374/374 - 3s - loss: 0.0140 - val_loss: 0.0183 - 3s/epoch - 7ms/step
Epoch 7/85
374/374 - 3s - loss: 0.0137 - val_loss: 0.0157 - 3s/epoch - 7ms/step
Epoch 8/85
374/374 - 3s - loss: 0.0134 - val_loss: 0.0157 - 3s/epoch - 7ms/step
Epoch 9/85
374/374 - 3s - loss: 0.0132 - val_loss: 0.0162 - 3s/epoch - 7ms/step
Epoch 10/85
374/374 - 3s - loss: 0.0129 - val_loss: 0.0143 - 3s/epoch - 7ms/step
Epoch 11/85
374/374 - 3s - loss: 0.0128 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 12/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 13/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0141 - 3s/epoch - 7ms/step
Epoch 14/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 15/85
374/374 - 3s - loss: 0.0125 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 16/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 17/85
374/374 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 18/85
374/374 - 3s - loss: 0.0115 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 19/85
374/374 - 3s - loss: 0.0115 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 20/85
374/374 - 3s - loss: 0.0136 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 21/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 22/85
374/374 - 3s - loss: 0.0130 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 23/85
374/374 - 3s - loss: 0.0148 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 24/85
374/374 - 3s - loss: 0.0217 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 25/85
374/374 - 3s - loss: 0.0126 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 26/85
374/374 - 3s - loss: 0.0127 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 27/85
374/374 - 3s - loss: 0.0135 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 28/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0132 - 3s/epoch - 7ms/step
Epoch 29/85
374/374 - 3s - loss: 0.0224 - val_loss: 0.0155 - 3s/epoch - 7ms/step
Epoch 30/85
374/374 - 3s - loss: 0.0242 - val_loss: 0.0166 - 3s/epoch - 7ms/step
Epoch 31/85
374/374 - 3s - loss: 0.0247 - val_loss: 0.0156 - 3s/epoch - 7ms/step
Epoch 32/85
374/374 - 3s - loss: 0.0140 - val_loss: 0.0138 - 3s/epoch - 7ms/step
Epoch 33/85
374/374 - 3s - loss: 0.0212 - val_loss: 0.0247 - 3s/epoch - 7ms/step
Epoch 34/85
374/374 - 3s - loss: 0.0486 - val_loss: 0.0152 - 3s/epoch - 7ms/step
Epoch 35/85
374/374 - 3s - loss: 0.0151 - val_loss: 0.0138 - 3s/epoch - 7ms/step
Epoch 36/85
374/374 - 3s - loss: 0.0137 - val_loss: 0.0133 - 3s/epoch - 7ms/step
Epoch 37/85
374/374 - 3s - loss: 0.0134 - val_loss: 0.0131 - 3s/epoch - 7ms/step
Epoch 38/85
374/374 - 3s - loss: 0.0131 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 39/85
374/374 - 3s - loss: 0.0130 - val_loss: 0.0152 - 3s/epoch - 7ms/step
Epoch 40/85
374/374 - 3s - loss: 0.0208 - val_loss: 0.0146 - 3s/epoch - 7ms/step
Epoch 41/85
374/374 - 3s - loss: 0.0153 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 42/85
374/374 - 3s - loss: 0.0131 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 43/85
374/374 - 3s - loss: 0.0128 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 44/85
374/374 - 3s - loss: 0.0127 - val_loss: 0.0158 - 3s/epoch - 7ms/step
Epoch 45/85
374/374 - 3s - loss: 0.0167 - val_loss: 0.0184 - 3s/epoch - 7ms/step
Epoch 46/85
374/374 - 3s - loss: 0.0187 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 47/85
374/374 - 3s - loss: 0.0128 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 48/85
374/374 - 3s - loss: 0.0126 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 49/85
374/374 - 3s - loss: 0.0124 - val_loss: 0.0143 - 3s/epoch - 7ms/step
Epoch 50/85
374/374 - 3s - loss: 0.0149 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 51/85
374/374 - 3s - loss: 0.0124 - val_loss: 0.0121 - 3s/epoch - 8ms/step
Epoch 52/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0120 - 3s/epoch - 8ms/step
Epoch 53/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 54/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 55/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 56/85
374/374 - 2s - loss: 0.0118 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 57/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0289 - 3s/epoch - 8ms/step
Epoch 58/85
374/374 - 3s - loss: 0.0398 - val_loss: 0.0160 - 3s/epoch - 7ms/step
Epoch 59/85
374/374 - 3s - loss: 0.0149 - val_loss: 0.0278 - 3s/epoch - 7ms/step
Epoch 60/85
374/374 - 3s - loss: 0.0593 - val_loss: 0.0168 - 3s/epoch - 7ms/step
Epoch 61/85
374/374 - 3s - loss: 0.0164 - val_loss: 0.0141 - 3s/epoch - 7ms/step
Epoch 62/85
374/374 - 3s - loss: 0.0141 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 63/85
374/374 - 3s - loss: 0.0135 - val_loss: 0.0131 - 3s/epoch - 7ms/step
Epoch 64/85
374/374 - 3s - loss: 0.0131 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 65/85
374/374 - 3s - loss: 0.0129 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 66/85
374/374 - 3s - loss: 0.0127 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 67/85
374/374 - 3s - loss: 0.0132 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 68/85
374/374 - 3s - loss: 0.0124 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 69/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 70/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 71/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 72/85
374/374 - 3s - loss: 0.0120 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 73/85
374/374 - 3s - loss: 0.0121 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 74/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 75/85
374/374 - 3s - loss: 0.0118 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 76/85
374/374 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 77/85
374/374 - 3s - loss: 0.0122 - val_loss: 0.0177 - 3s/epoch - 7ms/step
Epoch 78/85
374/374 - 3s - loss: 0.0179 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 79/85
374/374 - 3s - loss: 0.0119 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 80/85
374/374 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 81/85
374/374 - 3s - loss: 0.0116 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 82/85
374/374 - 3s - loss: 0.0115 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 83/85
374/374 - 3s - loss: 0.0114 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 84/85
374/374 - 3s - loss: 0.0114 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 85/85
374/374 - 3s - loss: 0.0114 - val_loss: 0.0118 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.011838855221867561
  1/332 [..............................] - ETA: 44s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s121/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s183/332 [===============>..............] - ETA: 0s214/332 [==================>...........] - ETA: 0s245/332 [=====================>........] - ETA: 0s275/332 [=======================>......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.10376594335548481
cosine 0.08132921974815335
MAE: 0.04308365
RMSE: 0.091945834
r2: 0.4515665207572313
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 256, 85, 0.0005, 0.6, 758, 0.011380000971257687, 0.011838855221867561, 0.10376594335548481, 0.08132921974815335, 0.04308364912867546, 0.0919458344578743, 0.4515665207572313, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.001 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/145
374/374 - 6s - loss: 0.0411 - val_loss: 0.0201 - 6s/epoch - 16ms/step
Epoch 2/145
374/374 - 3s - loss: 0.0173 - val_loss: 0.0193 - 3s/epoch - 7ms/step
Epoch 3/145
374/374 - 3s - loss: 0.0156 - val_loss: 0.0178 - 3s/epoch - 7ms/step
Epoch 4/145
374/374 - 3s - loss: 0.0152 - val_loss: 0.0191 - 3s/epoch - 7ms/step
Epoch 5/145
374/374 - 3s - loss: 0.0146 - val_loss: 0.0187 - 3s/epoch - 7ms/step
Epoch 6/145
374/374 - 3s - loss: 0.0142 - val_loss: 0.0185 - 3s/epoch - 7ms/step
Epoch 7/145
374/374 - 3s - loss: 0.0139 - val_loss: 0.0155 - 3s/epoch - 7ms/step
Epoch 8/145
374/374 - 3s - loss: 0.0136 - val_loss: 0.0155 - 3s/epoch - 7ms/step
Epoch 9/145
374/374 - 3s - loss: 0.0133 - val_loss: 0.0150 - 3s/epoch - 7ms/step
Epoch 10/145
374/374 - 3s - loss: 0.0131 - val_loss: 0.0138 - 3s/epoch - 8ms/step
Epoch 11/145
374/374 - 3s - loss: 0.0129 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 12/145
374/374 - 3s - loss: 0.0127 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 13/145
374/374 - 3s - loss: 0.0126 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 14/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 15/145
374/374 - 3s - loss: 0.0123 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 16/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0118 - 3s/epoch - 8ms/step
Epoch 17/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 18/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 19/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 20/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 21/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 22/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 23/145
374/374 - 3s - loss: 0.0150 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 24/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 25/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 26/145
374/374 - 3s - loss: 0.0133 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 27/145
374/374 - 3s - loss: 0.0197 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 28/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 29/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 30/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 31/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 32/145
374/374 - 3s - loss: 0.0132 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 33/145
374/374 - 3s - loss: 0.0183 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 34/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 35/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 36/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 37/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 38/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 39/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0146 - 3s/epoch - 7ms/step
Epoch 40/145
374/374 - 3s - loss: 0.0230 - val_loss: 0.0131 - 3s/epoch - 7ms/step
Epoch 41/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 42/145
374/374 - 3s - loss: 0.0132 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 43/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 44/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 45/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 46/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 47/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0144 - 3s/epoch - 7ms/step
Epoch 48/145
374/374 - 3s - loss: 0.0189 - val_loss: 0.0134 - 3s/epoch - 7ms/step
Epoch 49/145
374/374 - 3s - loss: 0.0153 - val_loss: 0.0149 - 3s/epoch - 7ms/step
Epoch 50/145
374/374 - 3s - loss: 0.0208 - val_loss: 0.0126 - 3s/epoch - 7ms/step
Epoch 51/145
374/374 - 3s - loss: 0.0125 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 52/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 53/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 54/145
374/374 - 3s - loss: 0.0165 - val_loss: 0.0240 - 3s/epoch - 7ms/step
Epoch 55/145
374/374 - 3s - loss: 0.0629 - val_loss: 0.0143 - 3s/epoch - 7ms/step
Epoch 56/145
374/374 - 3s - loss: 0.0135 - val_loss: 0.0132 - 3s/epoch - 7ms/step
Epoch 57/145
374/374 - 3s - loss: 0.0130 - val_loss: 0.0148 - 3s/epoch - 7ms/step
Epoch 58/145
374/374 - 3s - loss: 0.0168 - val_loss: 0.0130 - 3s/epoch - 7ms/step
Epoch 59/145
374/374 - 3s - loss: 0.0128 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 60/145
374/374 - 3s - loss: 0.0126 - val_loss: 0.0122 - 3s/epoch - 7ms/step
Epoch 61/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0132 - 3s/epoch - 7ms/step
Epoch 62/145
374/374 - 3s - loss: 0.0150 - val_loss: 0.0123 - 3s/epoch - 7ms/step
Epoch 63/145
374/374 - 3s - loss: 0.0124 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 64/145
374/374 - 3s - loss: 0.0121 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 65/145
374/374 - 3s - loss: 0.0120 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 66/145
374/374 - 3s - loss: 0.0119 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 67/145
374/374 - 3s - loss: 0.0118 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 68/145
374/374 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 69/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 70/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 71/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0115 - 3s/epoch - 8ms/step
Epoch 72/145
374/374 - 3s - loss: 0.0116 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 73/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 74/145
374/374 - 3s - loss: 0.0114 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 75/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 76/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 77/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 78/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 79/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 80/145
374/374 - 3s - loss: 0.0112 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 81/145
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 82/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 83/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 84/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 85/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 86/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0128 - 3s/epoch - 7ms/step
Epoch 87/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 88/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 89/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 90/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 91/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 92/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 93/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 94/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 95/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 96/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0343 - 3s/epoch - 7ms/step
Epoch 97/145
374/374 - 3s - loss: 0.0180 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 98/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 99/145
374/374 - 3s - loss: 0.0122 - val_loss: 0.0147 - 3s/epoch - 7ms/step
Epoch 100/145
374/374 - 3s - loss: 0.0284 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 101/145
374/374 - 3s - loss: 0.0115 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 102/145
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 103/145
374/374 - 3s - loss: 0.0110 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 104/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 105/145
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 106/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 107/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 108/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 109/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 110/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 111/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 112/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 113/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 114/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 115/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 116/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 117/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 118/145
374/374 - 3s - loss: 0.0136 - val_loss: 0.0155 - 3s/epoch - 7ms/step
Epoch 119/145
374/374 - 3s - loss: 0.0146 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 120/145
374/374 - 3s - loss: 0.0108 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 121/145
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 122/145
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 123/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 124/145
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 125/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 126/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 127/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 128/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 129/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 130/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 131/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 132/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 8ms/step
Epoch 133/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 8ms/step
Epoch 134/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 135/145
374/374 - 3s - loss: 0.0103 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 136/145
374/374 - 3s - loss: 0.0104 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 137/145
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 7ms/step
Epoch 138/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0102 - 3s/epoch - 8ms/step
Epoch 139/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 140/145
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 141/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 142/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 143/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 144/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 145/145
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010003396309912205
  1/332 [..............................] - ETA: 47s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 86/332 [======>.......................] - ETA: 0s117/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s179/332 [===============>..............] - ETA: 0s210/332 [=================>............] - ETA: 0s240/332 [====================>.........] - ETA: 0s267/332 [=======================>......] - ETA: 0s295/332 [=========================>....] - ETA: 0s324/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0865509596818625
cosine 0.06788497998653337
MAE: 0.03938486
RMSE: 0.0843068
r2: 0.5389107962671172
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 256, 145, 0.001, 0.6, 758, 0.010108538903295994, 0.010003396309912205, 0.0865509596818625, 0.06788497998653337, 0.039384860545396805, 0.08430679887533188, 0.5389107962671172, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[1.1999999999999997 180 0.00030000000000000003 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 9s - loss: 0.0377 - val_loss: 0.0310 - 9s/epoch - 23ms/step
Epoch 2/180
374/374 - 3s - loss: 0.0163 - val_loss: 0.0197 - 3s/epoch - 7ms/step
Epoch 3/180
374/374 - 3s - loss: 0.0151 - val_loss: 0.0162 - 3s/epoch - 7ms/step
Epoch 4/180
374/374 - 3s - loss: 0.0146 - val_loss: 0.0206 - 3s/epoch - 7ms/step
Epoch 5/180
374/374 - 3s - loss: 0.0143 - val_loss: 0.0184 - 3s/epoch - 7ms/step
Epoch 6/180
374/374 - 3s - loss: 0.0140 - val_loss: 0.0282 - 3s/epoch - 7ms/step
Epoch 7/180
374/374 - 3s - loss: 0.0138 - val_loss: 0.0178 - 3s/epoch - 7ms/step
Epoch 8/180
374/374 - 3s - loss: 0.0135 - val_loss: 0.0151 - 3s/epoch - 7ms/step
Epoch 9/180
374/374 - 3s - loss: 0.0133 - val_loss: 0.0287 - 3s/epoch - 7ms/step
Epoch 10/180
374/374 - 3s - loss: 0.0133 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 11/180
374/374 - 3s - loss: 0.0129 - val_loss: 0.0139 - 3s/epoch - 7ms/step
Epoch 12/180
374/374 - 3s - loss: 0.0128 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 13/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 14/180
374/374 - 3s - loss: 0.0126 - val_loss: 0.0133 - 3s/epoch - 7ms/step
Epoch 15/180
374/374 - 3s - loss: 0.0125 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 16/180
374/374 - 3s - loss: 0.0123 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 17/180
374/374 - 3s - loss: 0.0123 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 18/180
374/374 - 3s - loss: 0.0122 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 19/180
374/374 - 3s - loss: 0.0121 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 20/180
374/374 - 3s - loss: 0.0119 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 21/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0117 - 3s/epoch - 7ms/step
Epoch 22/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 23/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0124 - 3s/epoch - 7ms/step
Epoch 24/180
374/374 - 3s - loss: 0.0127 - val_loss: 0.0125 - 3s/epoch - 7ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 26/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 27/180
374/374 - 3s - loss: 0.0126 - val_loss: 0.0119 - 3s/epoch - 7ms/step
Epoch 28/180
374/374 - 3s - loss: 0.0123 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 29/180
374/374 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 30/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0120 - 3s/epoch - 7ms/step
Epoch 31/180
374/374 - 3s - loss: 0.0131 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 32/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0115 - 3s/epoch - 7ms/step
Epoch 33/180
374/374 - 3s - loss: 0.0115 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 34/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0113 - 3s/epoch - 7ms/step
Epoch 35/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 36/180
374/374 - 3s - loss: 0.0112 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 37/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0129 - 3s/epoch - 7ms/step
Epoch 38/180
374/374 - 3s - loss: 0.0146 - val_loss: 0.0135 - 3s/epoch - 7ms/step
Epoch 39/180
374/374 - 3s - loss: 0.0126 - val_loss: 0.0121 - 3s/epoch - 7ms/step
Epoch 40/180
374/374 - 3s - loss: 0.0142 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 41/180
374/374 - 3s - loss: 0.0116 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 42/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0114 - 3s/epoch - 7ms/step
Epoch 43/180
374/374 - 3s - loss: 0.0114 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 44/180
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 45/180
374/374 - 3s - loss: 0.0118 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 46/180
374/374 - 3s - loss: 0.0112 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 47/180
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 48/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 49/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 50/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 51/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 52/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 53/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 54/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0112 - 3s/epoch - 7ms/step
Epoch 55/180
374/374 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 56/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 57/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 58/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 59/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 60/180
374/374 - 3s - loss: 0.0126 - val_loss: 0.0127 - 3s/epoch - 7ms/step
Epoch 61/180
374/374 - 3s - loss: 0.0148 - val_loss: 0.0116 - 3s/epoch - 7ms/step
Epoch 62/180
374/374 - 3s - loss: 0.0113 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 63/180
374/374 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 64/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 65/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 66/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 67/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 68/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 69/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 70/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 71/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 72/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 73/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 74/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 75/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0136 - 3s/epoch - 7ms/step
Epoch 76/180
374/374 - 3s - loss: 0.0150 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 77/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 78/180
374/374 - 3s - loss: 0.0111 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 79/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 80/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 81/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 82/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 8ms/step
Epoch 83/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0111 - 3s/epoch - 7ms/step
Epoch 84/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 85/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 86/180
374/374 - 3s - loss: 0.0108 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 87/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0110 - 3s/epoch - 8ms/step
Epoch 88/180
374/374 - 3s - loss: 0.0109 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 89/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0107 - 3s/epoch - 7ms/step
Epoch 90/180
374/374 - 3s - loss: 0.0107 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 91/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 92/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 93/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 94/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 95/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 96/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 97/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 98/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0105 - 3s/epoch - 7ms/step
Epoch 99/180
374/374 - 3s - loss: 0.0105 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 100/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0109 - 3s/epoch - 7ms/step
Epoch 101/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 7ms/step
Epoch 102/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 103/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0110 - 3s/epoch - 7ms/step
Epoch 104/180
374/374 - 3s - loss: 0.0110 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 105/180
374/374 - 3s - loss: 0.0104 - val_loss: 0.0106 - 3s/epoch - 7ms/step
Epoch 106/180
374/374 - 3s - loss: 0.0106 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 107/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 108/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 109/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 110/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 111/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 112/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 113/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 114/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 115/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 116/180
374/374 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 117/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 118/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 119/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 120/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 121/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 122/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0103 - 3s/epoch - 7ms/step
Epoch 123/180
374/374 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 124/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 125/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 126/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0102 - 3s/epoch - 7ms/step
Epoch 127/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 128/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 129/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 130/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0101 - 3s/epoch - 7ms/step
Epoch 131/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 132/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 133/180
374/374 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 134/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0100 - 3s/epoch - 7ms/step
Epoch 135/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 136/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 137/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 138/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 139/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 140/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 141/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 7ms/step
Epoch 142/180
374/374 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 143/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 8ms/step
Epoch 144/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 8ms/step
Epoch 145/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 146/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 147/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 148/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 149/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 150/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 7ms/step
Epoch 151/180
374/374 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 152/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 153/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 154/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 155/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 156/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 157/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 158/180
374/374 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 159/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0097 - 3s/epoch - 7ms/step
Epoch 160/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 161/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 162/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 163/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 164/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 165/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 166/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 167/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 168/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 169/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 170/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0096 - 3s/epoch - 7ms/step
Epoch 171/180
374/374 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 172/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 173/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 174/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 175/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 176/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 177/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 178/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 179/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
Epoch 180/180
374/374 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009457916021347046
  1/332 [..............................] - ETA: 52s 29/332 [=>............................] - ETA: 0s  60/332 [====>.........................] - ETA: 0s 91/332 [=======>......................] - ETA: 0s121/332 [=========>....................] - ETA: 0s151/332 [============>.................] - ETA: 0s181/332 [===============>..............] - ETA: 0s211/332 [==================>...........] - ETA: 0s239/332 [====================>.........] - ETA: 0s269/332 [=======================>......] - ETA: 0s300/332 [==========================>...] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07774611213411244
cosine 0.06103447677277638
MAE: 0.03682687
RMSE: 0.080025665
r2: 0.5845501747501054
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'mse', 256, 180, 0.00030000000000000003, 0.6, 758, 0.009589066728949547, 0.009457916021347046, 0.07774611213411244, 0.06103447677277638, 0.03682687133550644, 0.08002566546201706, 0.5845501747501054, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.2 145 0.0012000000000000001 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2780)         3516700     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2780)        11120       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2780)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2107998     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2107998     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6214678     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,958,494
Trainable params: 13,945,858
Non-trainable params: 12,636
__________________________________________________________________________________________________
Epoch 1/145
747/747 - 8s - loss: 0.0354 - val_loss: 0.0210 - 8s/epoch - 11ms/step
Epoch 2/145
747/747 - 5s - loss: 0.0166 - val_loss: 0.0417 - 5s/epoch - 6ms/step
Epoch 3/145
747/747 - 5s - loss: 0.0152 - val_loss: 0.0150 - 5s/epoch - 6ms/step
Epoch 4/145
747/747 - 5s - loss: 0.0141 - val_loss: 0.0195 - 5s/epoch - 7ms/step
Epoch 5/145
747/747 - 5s - loss: 0.0135 - val_loss: 0.0133 - 5s/epoch - 6ms/step
Epoch 6/145
747/747 - 5s - loss: 0.0130 - val_loss: 0.0128 - 5s/epoch - 6ms/step
Epoch 7/145
747/747 - 5s - loss: 0.0126 - val_loss: 0.0124 - 5s/epoch - 6ms/step
Epoch 8/145
747/747 - 5s - loss: 0.0122 - val_loss: 0.0118 - 5s/epoch - 6ms/step
Epoch 9/145
747/747 - 4s - loss: 0.0118 - val_loss: 0.0116 - 4s/epoch - 6ms/step
Epoch 10/145
747/747 - 5s - loss: 0.0116 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 11/145
747/747 - 5s - loss: 0.0117 - val_loss: 0.0113 - 5s/epoch - 7ms/step
Epoch 12/145
747/747 - 5s - loss: 0.0114 - val_loss: 0.0114 - 5s/epoch - 7ms/step
Epoch 13/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0124 - 5s/epoch - 7ms/step
Epoch 14/145
747/747 - 5s - loss: 0.0124 - val_loss: 0.0114 - 5s/epoch - 7ms/step
Epoch 15/145
747/747 - 5s - loss: 0.0119 - val_loss: 0.0153 - 5s/epoch - 7ms/step
Epoch 16/145
747/747 - 5s - loss: 0.0148 - val_loss: 0.0140 - 5s/epoch - 6ms/step
Epoch 17/145
747/747 - 5s - loss: 0.0166 - val_loss: 0.0133 - 5s/epoch - 6ms/step
Epoch 18/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0116 - 5s/epoch - 7ms/step
Epoch 19/145
747/747 - 5s - loss: 0.0120 - val_loss: 0.0152 - 5s/epoch - 7ms/step
Epoch 20/145
747/747 - 5s - loss: 0.0201 - val_loss: 0.0141 - 5s/epoch - 6ms/step
Epoch 21/145
747/747 - 5s - loss: 0.0140 - val_loss: 0.0119 - 5s/epoch - 7ms/step
Epoch 22/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 23/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0136 - 5s/epoch - 7ms/step
Epoch 24/145
747/747 - 5s - loss: 0.0131 - val_loss: 0.0115 - 5s/epoch - 7ms/step
Epoch 25/145
747/747 - 5s - loss: 0.0117 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 26/145
747/747 - 5s - loss: 0.0117 - val_loss: 0.0114 - 5s/epoch - 6ms/step
Epoch 27/145
747/747 - 5s - loss: 0.0114 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 28/145
747/747 - 5s - loss: 0.0113 - val_loss: 0.0113 - 5s/epoch - 6ms/step
Epoch 29/145
747/747 - 5s - loss: 0.0113 - val_loss: 0.0111 - 5s/epoch - 7ms/step
Epoch 30/145
747/747 - 5s - loss: 0.0119 - val_loss: 0.0114 - 5s/epoch - 6ms/step
Epoch 31/145
747/747 - 5s - loss: 0.0115 - val_loss: 0.0161 - 5s/epoch - 6ms/step
Epoch 32/145
747/747 - 5s - loss: 0.0165 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 33/145
747/747 - 5s - loss: 0.0115 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 34/145
747/747 - 5s - loss: 0.0113 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 35/145
747/747 - 5s - loss: 0.0151 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 36/145
747/747 - 5s - loss: 0.0117 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 37/145
747/747 - 5s - loss: 0.0118 - val_loss: 0.0137 - 5s/epoch - 6ms/step
Epoch 38/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0111 - 5s/epoch - 7ms/step
Epoch 39/145
747/747 - 5s - loss: 0.0112 - val_loss: 0.0110 - 5s/epoch - 6ms/step
Epoch 40/145
747/747 - 5s - loss: 0.0161 - val_loss: 0.0167 - 5s/epoch - 6ms/step
Epoch 41/145
747/747 - 5s - loss: 0.0226 - val_loss: 0.0121 - 5s/epoch - 6ms/step
Epoch 42/145
747/747 - 5s - loss: 0.0120 - val_loss: 0.0116 - 5s/epoch - 6ms/step
Epoch 43/145
747/747 - 5s - loss: 0.0116 - val_loss: 0.0119 - 5s/epoch - 6ms/step
Epoch 44/145
747/747 - 5s - loss: 0.0117 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 45/145
747/747 - 5s - loss: 0.0114 - val_loss: 0.0111 - 5s/epoch - 7ms/step
Epoch 46/145
747/747 - 5s - loss: 0.0113 - val_loss: 0.0111 - 5s/epoch - 7ms/step
Epoch 47/145
747/747 - 5s - loss: 0.0116 - val_loss: 0.0197 - 5s/epoch - 6ms/step
Epoch 48/145
747/747 - 5s - loss: 0.0119 - val_loss: 0.0110 - 5s/epoch - 7ms/step
Epoch 49/145
747/747 - 5s - loss: 0.0112 - val_loss: 0.0112 - 5s/epoch - 6ms/step
Epoch 50/145
747/747 - 5s - loss: 0.0112 - val_loss: 0.0110 - 5s/epoch - 6ms/step
Epoch 51/145
747/747 - 5s - loss: 0.0111 - val_loss: 0.0110 - 5s/epoch - 6ms/step
Epoch 52/145
747/747 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 6ms/step
Epoch 53/145
747/747 - 5s - loss: 0.0111 - val_loss: 0.0135 - 5s/epoch - 7ms/step
Epoch 54/145
747/747 - 5s - loss: 0.0122 - val_loss: 0.0109 - 5s/epoch - 7ms/step
Epoch 55/145
747/747 - 5s - loss: 0.0114 - val_loss: 0.0109 - 5s/epoch - 7ms/step
Epoch 56/145
747/747 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 6ms/step
Epoch 57/145
747/747 - 5s - loss: 0.0109 - val_loss: 0.0107 - 5s/epoch - 7ms/step
Epoch 58/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0107 - 5s/epoch - 6ms/step
Epoch 59/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0115 - 5s/epoch - 6ms/step
Epoch 60/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 61/145
747/747 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 62/145
747/747 - 5s - loss: 0.0107 - val_loss: 0.0148 - 5s/epoch - 6ms/step
Epoch 63/145
747/747 - 5s - loss: 0.0121 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 64/145
747/747 - 5s - loss: 0.0107 - val_loss: 0.0110 - 5s/epoch - 6ms/step
Epoch 65/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 66/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 67/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0106 - 5s/epoch - 6ms/step
Epoch 68/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 69/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 70/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0121 - 5s/epoch - 6ms/step
Epoch 71/145
747/747 - 5s - loss: 0.0112 - val_loss: 0.0111 - 5s/epoch - 6ms/step
Epoch 72/145
747/747 - 5s - loss: 0.0108 - val_loss: 0.0104 - 5s/epoch - 7ms/step
Epoch 73/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0104 - 5s/epoch - 6ms/step
Epoch 74/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 75/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0109 - 5s/epoch - 7ms/step
Epoch 76/145
747/747 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 77/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 78/145
747/747 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 79/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 80/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 81/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 82/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0105 - 5s/epoch - 6ms/step
Epoch 83/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 84/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 7ms/step
Epoch 85/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 86/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0103 - 5s/epoch - 6ms/step
Epoch 87/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 88/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 7ms/step
Epoch 89/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 7ms/step
Epoch 90/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 91/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 92/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 93/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 94/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 95/145
747/747 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 7ms/step
Epoch 96/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 97/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 7ms/step
Epoch 98/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 99/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 100/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 7ms/step
Epoch 101/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 7ms/step
Epoch 102/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 6ms/step
Epoch 103/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 104/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 105/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 106/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0102 - 5s/epoch - 7ms/step
Epoch 107/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 7ms/step
Epoch 108/145
747/747 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 7ms/step
Epoch 109/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 110/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0101 - 5s/epoch - 6ms/step
Epoch 111/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 112/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 113/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 114/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 115/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 116/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 117/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 118/145
747/747 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 119/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 120/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 121/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 6ms/step
Epoch 122/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0100 - 5s/epoch - 7ms/step
Epoch 123/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 7ms/step
Epoch 124/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 7ms/step
Epoch 125/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 126/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 127/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 7ms/step
Epoch 128/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 129/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 7ms/step
Epoch 130/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 131/145
747/747 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 7ms/step
Epoch 132/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 133/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 134/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 135/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0099 - 5s/epoch - 6ms/step
Epoch 136/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 137/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 138/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 139/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 140/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 141/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 142/145
747/747 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 7ms/step
Epoch 143/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 144/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
Epoch 145/145
747/747 - 5s - loss: 0.0099 - val_loss: 0.0098 - 5s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009763592854142189
  1/332 [..............................] - ETA: 53s 31/332 [=>............................] - ETA: 0s  62/332 [====>.........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s113/332 [=========>....................] - ETA: 0s134/332 [===========>..................] - ETA: 0s155/332 [=============>................] - ETA: 0s178/332 [===============>..............] - ETA: 0s200/332 [=================>............] - ETA: 0s223/332 [===================>..........] - ETA: 0s249/332 [=====================>........] - ETA: 0s280/332 [========================>.....] - ETA: 0s308/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08370754094650464
cosine 0.06568682809348741
MAE: 0.038157452
RMSE: 0.08291547
r2: 0.554003856490803
RMSE zero-vector: 0.23411466903540806
['2.2custom_VAE', 'mse', 128, 145, 0.0012000000000000001, 0.6, 758, 0.009930758737027645, 0.009763592854142189, 0.08370754094650464, 0.06568682809348741, 0.03815745189785957, 0.08291547000408173, 0.554003856490803, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 145 0.001 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
/var/spool/slurmd/job36093295/slurm_script: line 23: 15303 Killed                  python3 -u genetic.py >> log.txt
Fri Feb 17 23:42:20 CET 2023
done
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=36093295.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
