start
Wed Feb 22 14:24:21 CET 2023
2023-02-22 14:24:27.438369: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 14:24:27.544117: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-22 14:24:27.570904: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-22 14:25:07,956 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fc7103ac070> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
2023-02-22 14:25:09.345574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 14:25:09.704846: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-22 14:25:09.704963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20637 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:a2:00.0, compute capability: 8.6
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[2.0 180 0.002 256 1]
 [0.5 60 0.001 128 1]
 [2.5 120 0.0005 64 1]
 [0.5 90 0.002 128 1]
 [0.5 90 0.0005 256 2]
 [0.5 120 0.0005 64 1]
 [2.5 120 0.002 64 1]
 [2.0 120 0.0005 64 1]
 [1.5 30 0.002 256 1]
 [1.5 210 0.002 128 2]]
[2.0 180 0.002 256 1] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/180
2023-02-22 14:25:13.694850: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
374/374 - 3s - loss: 0.0398 - val_loss: 0.0254 - 3s/epoch - 8ms/step
Epoch 2/180
374/374 - 1s - loss: 0.0164 - val_loss: 0.0231 - 1s/epoch - 4ms/step
Epoch 3/180
374/374 - 1s - loss: 0.0155 - val_loss: 0.0185 - 1s/epoch - 4ms/step
Epoch 4/180
374/374 - 1s - loss: 0.0153 - val_loss: 0.0196 - 1s/epoch - 4ms/step
Epoch 5/180
374/374 - 1s - loss: 0.0144 - val_loss: 0.0174 - 1s/epoch - 4ms/step
Epoch 6/180
374/374 - 1s - loss: 0.0139 - val_loss: 0.0486 - 1s/epoch - 4ms/step
Epoch 7/180
374/374 - 1s - loss: 0.0135 - val_loss: 0.0157 - 1s/epoch - 4ms/step
Epoch 8/180
374/374 - 1s - loss: 0.0127 - val_loss: 0.0139 - 1s/epoch - 4ms/step
Epoch 9/180
374/374 - 1s - loss: 0.0122 - val_loss: 0.0135 - 1s/epoch - 4ms/step
Epoch 10/180
374/374 - 1s - loss: 0.0119 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 11/180
374/374 - 1s - loss: 0.0117 - val_loss: 0.0137 - 1s/epoch - 4ms/step
Epoch 12/180
374/374 - 1s - loss: 0.0116 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 13/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0166 - 1s/epoch - 4ms/step
Epoch 14/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0330 - 1s/epoch - 4ms/step
Epoch 15/180
374/374 - 1s - loss: 0.0138 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 16/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0125 - 1s/epoch - 4ms/step
Epoch 17/180
374/374 - 1s - loss: 0.0117 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 18/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 19/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 20/180
374/374 - 1s - loss: 0.0116 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 21/180
374/374 - 1s - loss: 0.0117 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 22/180
374/374 - 1s - loss: 0.0141 - val_loss: 0.0127 - 1s/epoch - 4ms/step
Epoch 23/180
374/374 - 1s - loss: 0.0164 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 24/180
374/374 - 1s - loss: 0.0171 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 25/180
374/374 - 1s - loss: 0.0132 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 26/180
374/374 - 1s - loss: 0.0115 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 27/180
374/374 - 1s - loss: 0.0113 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 28/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 29/180
374/374 - 1s - loss: 0.0130 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 30/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 31/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 32/180
374/374 - 1s - loss: 0.0145 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 33/180
374/374 - 1s - loss: 0.0115 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 34/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 35/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 36/180
374/374 - 1s - loss: 0.0198 - val_loss: 0.0138 - 1s/epoch - 4ms/step
Epoch 37/180
374/374 - 1s - loss: 0.0205 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 38/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 4ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 4ms/step
Epoch 41/180
374/374 - 1s - loss: 0.0112 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 42/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 43/180
374/374 - 1s - loss: 0.0113 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 44/180
374/374 - 1s - loss: 0.0111 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 45/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 46/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 47/180
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 48/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 49/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 50/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 51/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 52/180
374/374 - 1s - loss: 0.0114 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 53/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 54/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 55/180
374/374 - 1s - loss: 0.0139 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 56/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 57/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 58/180
374/374 - 1s - loss: 0.0109 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 59/180
374/374 - 1s - loss: 0.0125 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 60/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 61/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 62/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 63/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 64/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0152 - val_loss: 0.0105 - 2s/epoch - 4ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 4ms/step
Epoch 67/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 68/180
374/374 - 1s - loss: 0.0174 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 69/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 70/180
374/374 - 1s - loss: 0.0107 - val_loss: 0.0126 - 1s/epoch - 4ms/step
Epoch 71/180
374/374 - 1s - loss: 0.0141 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 72/180
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 73/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 74/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 75/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 76/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 77/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 78/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 79/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 80/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 81/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 82/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 83/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 84/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 85/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 86/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 87/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 88/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0134 - 1s/epoch - 4ms/step
Epoch 89/180
374/374 - 1s - loss: 0.0116 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 90/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 91/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 92/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 93/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 94/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 95/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 96/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 97/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 98/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 99/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 100/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 101/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 102/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 103/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 104/180
374/374 - 1s - loss: 0.0161 - val_loss: 0.0131 - 1s/epoch - 4ms/step
Epoch 105/180
374/374 - 1s - loss: 0.0151 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 106/180
374/374 - 1s - loss: 0.0105 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 107/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 108/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 109/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 110/180
374/374 - 1s - loss: 0.0115 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 111/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 112/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 113/180
374/374 - 1s - loss: 0.0127 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 114/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 115/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0138 - 1s/epoch - 4ms/step
Epoch 116/180
374/374 - 1s - loss: 0.0214 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 117/180
374/374 - 1s - loss: 0.0103 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 118/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 119/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 120/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 121/180
374/374 - 1s - loss: 0.0102 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 122/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 123/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 124/180
374/374 - 1s - loss: 0.0165 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 4ms/step
Epoch 126/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 127/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 128/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 129/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 130/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 131/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 132/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 4ms/step
Epoch 134/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 135/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0160 - 1s/epoch - 4ms/step
Epoch 136/180
374/374 - 1s - loss: 0.0121 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 137/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 138/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 139/180
374/374 - 1s - loss: 0.0135 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 140/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 141/180
374/374 - 1s - loss: 0.0101 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 142/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 143/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 144/180
374/374 - 1s - loss: 0.0122 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 145/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 146/180
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 147/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 148/180
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 149/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 150/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 4ms/step
Epoch 152/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 153/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 154/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 155/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 156/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 157/180
374/374 - 1s - loss: 0.0100 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 158/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 159/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0140 - 1s/epoch - 4ms/step
Epoch 160/180
374/374 - 1s - loss: 0.0110 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 161/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 162/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 163/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 164/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 165/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 166/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 167/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 168/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 169/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 170/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 171/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 172/180
374/374 - 1s - loss: 0.0104 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 173/180
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 4ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 4ms/step
Epoch 176/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 177/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 178/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 179/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 180/180
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009458208456635475
  1/332 [..............................] - ETA: 44s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s302/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07169930776177466
cosine 0.05491397729514906
MAE: 0.036470737
RMSE: 0.0791435
r2: 0.5940224050351995
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 256, 180, 0.002, 0.2, 252, 0.009550358168780804, 0.009458208456635475, 0.07169930776177466, 0.05491397729514906, 0.03647073730826378, 0.07914350181818008, 0.5940224050351995, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 60 0.001 128 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/60
747/747 - 3s - loss: 0.0228 - val_loss: 0.0148 - 3s/epoch - 4ms/step
Epoch 2/60
747/747 - 2s - loss: 0.0142 - val_loss: 0.0148 - 2s/epoch - 3ms/step
Epoch 3/60
747/747 - 2s - loss: 0.0136 - val_loss: 0.0141 - 2s/epoch - 3ms/step
Epoch 4/60
747/747 - 2s - loss: 0.0133 - val_loss: 0.0139 - 2s/epoch - 3ms/step
Epoch 5/60
747/747 - 2s - loss: 0.0130 - val_loss: 0.0129 - 2s/epoch - 3ms/step
Epoch 6/60
747/747 - 2s - loss: 0.0125 - val_loss: 0.0123 - 2s/epoch - 3ms/step
Epoch 7/60
747/747 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 3ms/step
Epoch 8/60
747/747 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 3ms/step
Epoch 9/60
747/747 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 10/60
747/747 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 3ms/step
Epoch 11/60
747/747 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 3ms/step
Epoch 12/60
747/747 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 3ms/step
Epoch 13/60
747/747 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 3ms/step
Epoch 14/60
747/747 - 2s - loss: 0.0105 - val_loss: 0.0102 - 2s/epoch - 3ms/step
Epoch 15/60
747/747 - 2s - loss: 0.0104 - val_loss: 0.0101 - 2s/epoch - 3ms/step
Epoch 16/60
747/747 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 3ms/step
Epoch 17/60
747/747 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 3ms/step
Epoch 18/60
747/747 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 3ms/step
Epoch 19/60
747/747 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 3ms/step
Epoch 20/60
747/747 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 3ms/step
Epoch 21/60
747/747 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 3ms/step
Epoch 22/60
747/747 - 2s - loss: 0.0100 - val_loss: 0.0097 - 2s/epoch - 3ms/step
Epoch 23/60
747/747 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 3ms/step
Epoch 24/60
747/747 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 3ms/step
Epoch 25/60
747/747 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 3ms/step
Epoch 26/60
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 27/60
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 28/60
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 29/60
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 30/60
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 31/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 32/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 33/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 34/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 35/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 36/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 37/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 38/60
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 39/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 40/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 41/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 42/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 43/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 44/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 45/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 46/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 47/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 48/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 49/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 50/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 51/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 52/60
747/747 - 2s - loss: 0.0096 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 53/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 54/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 55/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 56/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 57/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 58/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 59/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 60/60
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009320694021880627
  1/332 [..............................] - ETA: 36s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s173/332 [==============>...............] - ETA: 0s216/332 [==================>...........] - ETA: 0s259/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.06861120789872445
cosine 0.05259094852076537
MAE: 0.035800222
RMSE: 0.0774251
r2: 0.6114608670901338
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 128, 60, 0.001, 0.2, 252, 0.009515832178294659, 0.009320694021880627, 0.06861120789872445, 0.05259094852076537, 0.03580022230744362, 0.07742509990930557, 0.6114608670901338, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 120 0.0005 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0274 - val_loss: 0.0177 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0151 - val_loss: 0.0213 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0134 - val_loss: 0.0128 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0122 - val_loss: 0.0117 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0119 - val_loss: 0.0113 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0115 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 4ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0111 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0103 - 5s/epoch - 4ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009360157884657383
  1/332 [..............................] - ETA: 40s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s173/332 [==============>...............] - ETA: 0s216/332 [==================>...........] - ETA: 0s260/332 [======================>.......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06962338597694523
cosine 0.05337425162106516
MAE: 0.03591359
RMSE: 0.07804429
r2: 0.6052219516329586
RMSE zero-vector: 0.2430644284356365
['2.5custom_VAE', 'mse', 64, 120, 0.0005, 0.2, 252, 0.009616184048354626, 0.009360157884657383, 0.06962338597694523, 0.05337425162106516, 0.03591359034180641, 0.07804428786039352, 0.6052219516329586, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 90 0.002 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/90
747/747 - 3s - loss: 0.0230 - val_loss: 0.0154 - 3s/epoch - 4ms/step
Epoch 2/90
747/747 - 2s - loss: 0.0141 - val_loss: 0.0139 - 2s/epoch - 3ms/step
Epoch 3/90
747/747 - 3s - loss: 0.0136 - val_loss: 0.0140 - 3s/epoch - 3ms/step
Epoch 4/90
747/747 - 2s - loss: 0.0133 - val_loss: 0.0133 - 2s/epoch - 3ms/step
Epoch 5/90
747/747 - 2s - loss: 0.0130 - val_loss: 0.0129 - 2s/epoch - 3ms/step
Epoch 6/90
747/747 - 2s - loss: 0.0128 - val_loss: 0.0128 - 2s/epoch - 3ms/step
Epoch 7/90
747/747 - 2s - loss: 0.0125 - val_loss: 0.0124 - 2s/epoch - 3ms/step
Epoch 8/90
747/747 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 3ms/step
Epoch 9/90
747/747 - 2s - loss: 0.0118 - val_loss: 0.0115 - 2s/epoch - 3ms/step
Epoch 10/90
747/747 - 2s - loss: 0.0112 - val_loss: 0.0109 - 2s/epoch - 3ms/step
Epoch 11/90
747/747 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 3ms/step
Epoch 12/90
747/747 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 3ms/step
Epoch 13/90
747/747 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 3ms/step
Epoch 14/90
747/747 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 3ms/step
Epoch 15/90
747/747 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 3ms/step
Epoch 16/90
747/747 - 2s - loss: 0.0102 - val_loss: 0.0099 - 2s/epoch - 3ms/step
Epoch 17/90
747/747 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 3ms/step
Epoch 18/90
747/747 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 3ms/step
Epoch 19/90
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 20/90
747/747 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 3ms/step
Epoch 21/90
747/747 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 3ms/step
Epoch 22/90
747/747 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 3ms/step
Epoch 23/90
747/747 - 2s - loss: 0.0099 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 24/90
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 25/90
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 26/90
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 27/90
747/747 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 28/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 3ms/step
Epoch 29/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 30/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 31/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 32/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 33/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 34/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 35/90
747/747 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 36/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 37/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 38/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 3ms/step
Epoch 39/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 40/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 41/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 42/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 43/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 44/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 45/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 46/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 47/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 48/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 49/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 50/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 51/90
747/747 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 52/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 53/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 3ms/step
Epoch 54/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 55/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 56/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 57/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 58/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 59/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 60/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 61/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 62/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 63/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 64/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 65/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 66/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 67/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 68/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 69/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 70/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 71/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 72/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 73/90
747/747 - 2s - loss: 0.0095 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 74/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 75/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 3ms/step
Epoch 76/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 77/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 78/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 3ms/step
Epoch 79/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 80/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 81/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 82/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 83/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 3ms/step
Epoch 84/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 3ms/step
Epoch 85/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 86/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 3ms/step
Epoch 87/90
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 88/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0093 - 2s/epoch - 3ms/step
Epoch 89/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 3ms/step
Epoch 90/90
747/747 - 2s - loss: 0.0094 - val_loss: 0.0092 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009238907136023045
  1/332 [..............................] - ETA: 38s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s259/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06580692413412217
cosine 0.05042960865179458
MAE: 0.03464462
RMSE: 0.07589579
r2: 0.6266585689018352
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 128, 90, 0.002, 0.2, 252, 0.009394287131726742, 0.009238907136023045, 0.06580692413412217, 0.05042960865179458, 0.03464461863040924, 0.07589578628540039, 0.6266585689018352, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 90 0.0005 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/90
374/374 - 2s - loss: 0.0161 - val_loss: 0.0084 - 2s/epoch - 6ms/step
Epoch 2/90
374/374 - 1s - loss: 0.0075 - val_loss: 0.0078 - 1s/epoch - 3ms/step
Epoch 3/90
374/374 - 1s - loss: 0.0073 - val_loss: 0.0082 - 1s/epoch - 3ms/step
Epoch 4/90
374/374 - 1s - loss: 0.0072 - val_loss: 0.0090 - 1s/epoch - 3ms/step
Epoch 5/90
374/374 - 1s - loss: 0.0072 - val_loss: 0.0072 - 1s/epoch - 3ms/step
Epoch 6/90
374/374 - 1s - loss: 0.0071 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 7/90
374/374 - 1s - loss: 0.0071 - val_loss: 0.0076 - 1s/epoch - 3ms/step
Epoch 8/90
374/374 - 1s - loss: 0.0070 - val_loss: 0.0073 - 1s/epoch - 3ms/step
Epoch 9/90
374/374 - 1s - loss: 0.0069 - val_loss: 0.0070 - 1s/epoch - 3ms/step
Epoch 10/90
374/374 - 1s - loss: 0.0069 - val_loss: 0.0087 - 1s/epoch - 3ms/step
Epoch 11/90
374/374 - 1s - loss: 0.0069 - val_loss: 0.0071 - 1s/epoch - 3ms/step
Epoch 12/90
374/374 - 1s - loss: 0.0068 - val_loss: 0.0070 - 1s/epoch - 3ms/step
Epoch 13/90
374/374 - 1s - loss: 0.0068 - val_loss: 0.0069 - 1s/epoch - 3ms/step
Epoch 14/90
374/374 - 1s - loss: 0.0068 - val_loss: 0.0068 - 1s/epoch - 3ms/step
Epoch 15/90
374/374 - 1s - loss: 0.0068 - val_loss: 0.0069 - 1s/epoch - 3ms/step
Epoch 16/90
374/374 - 1s - loss: 0.0067 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 17/90
374/374 - 1s - loss: 0.0065 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 18/90
374/374 - 1s - loss: 0.0064 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 19/90
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 20/90
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 21/90
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 22/90
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 23/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 24/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 25/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 26/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 27/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 28/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 29/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 30/90
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 31/90
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 32/90
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 33/90
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 34/90
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 35/90
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 36/90
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 37/90
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 38/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 39/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 40/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 41/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 42/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 43/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 44/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 45/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 46/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 47/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 48/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 49/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 50/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 51/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 52/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 53/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 54/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 55/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 56/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 57/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 58/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 59/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 60/90
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 61/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 62/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 63/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 64/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 65/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 66/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 67/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 68/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 69/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 70/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 71/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 72/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 73/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 74/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 75/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 76/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 77/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 78/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 79/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 80/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 81/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 82/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 83/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 84/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 85/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 86/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 87/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 88/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 89/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 90/90
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005845435429364443
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s216/332 [==================>...........] - ETA: 0s260/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.10636316906538631
cosine 0.08121840154792155
MAE: 0.044047523
RMSE: 0.09561792
r2: 0.40741567384743116
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 256, 90, 0.0005, 0.2, 252, 0.005894503556191921, 0.005845435429364443, 0.10636316906538631, 0.08121840154792155, 0.0440475232899189, 0.09561792016029358, 0.40741567384743116, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 120 0.0005 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0201 - val_loss: 0.0149 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0137 - val_loss: 0.0138 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0130 - val_loss: 0.0127 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0127 - val_loss: 0.0124 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0124 - val_loss: 0.0120 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0119 - val_loss: 0.0114 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0110 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0111 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009350933134555817
  1/332 [..............................] - ETA: 41s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s171/332 [==============>...............] - ETA: 0s214/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s302/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06935915250374693
cosine 0.05316394275420125
MAE: 0.035804477
RMSE: 0.07795328
r2: 0.6061418872893412
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 64, 120, 0.0005, 0.2, 252, 0.009568849578499794, 0.009350933134555817, 0.06935915250374693, 0.05316394275420125, 0.03580447658896446, 0.0779532790184021, 0.6061418872893412, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 120 0.002 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0278 - val_loss: 0.0195 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0161 - val_loss: 0.0163 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0134 - val_loss: 0.0126 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0123 - val_loss: 0.0118 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0112 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 4ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 4ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009287477470934391
  1/332 [..............................] - ETA: 40s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s213/332 [==================>...........] - ETA: 0s256/332 [======================>.......] - ETA: 0s299/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06773220036351542
cosine 0.05190148099251527
MAE: 0.03524896
RMSE: 0.07701689
r2: 0.6155472626817285
RMSE zero-vector: 0.2430644284356365
['2.5custom_VAE', 'mse', 64, 120, 0.002, 0.2, 252, 0.00952284038066864, 0.009287477470934391, 0.06773220036351542, 0.05190148099251527, 0.03524896129965782, 0.07701689004898071, 0.6155472626817285, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 120 0.0005 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0252 - val_loss: 0.0169 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0147 - val_loss: 0.0141 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0130 - val_loss: 0.0122 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0121 - val_loss: 0.0117 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0119 - val_loss: 0.0114 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0115 - val_loss: 0.0110 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0112 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009350450709462166
  1/332 [..............................] - ETA: 41s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06949996984189202
cosine 0.053304307496956596
MAE: 0.03583735
RMSE: 0.0780117
r2: 0.6055517270914621
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 64, 120, 0.0005, 0.2, 252, 0.00958519522100687, 0.009350450709462166, 0.06949996984189202, 0.053304307496956596, 0.03583734855055809, 0.07801169902086258, 0.6055517270914621, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.002 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/30
374/374 - 2s - loss: 0.0360 - val_loss: 0.0311 - 2s/epoch - 6ms/step
Epoch 2/30
374/374 - 1s - loss: 0.0156 - val_loss: 0.0177 - 1s/epoch - 4ms/step
Epoch 3/30
374/374 - 1s - loss: 0.0149 - val_loss: 0.0179 - 1s/epoch - 4ms/step
Epoch 4/30
374/374 - 1s - loss: 0.0142 - val_loss: 0.0198 - 1s/epoch - 4ms/step
Epoch 5/30
374/374 - 1s - loss: 0.0138 - val_loss: 0.0152 - 1s/epoch - 4ms/step
Epoch 6/30
374/374 - 1s - loss: 0.0136 - val_loss: 0.0157 - 1s/epoch - 4ms/step
Epoch 7/30
374/374 - 1s - loss: 0.0133 - val_loss: 0.0162 - 1s/epoch - 4ms/step
Epoch 8/30
374/374 - 1s - loss: 0.0131 - val_loss: 0.0146 - 1s/epoch - 4ms/step
Epoch 9/30
374/374 - 1s - loss: 0.0128 - val_loss: 0.0141 - 1s/epoch - 4ms/step
Epoch 10/30
374/374 - 1s - loss: 0.0126 - val_loss: 0.0144 - 1s/epoch - 4ms/step
Epoch 11/30
374/374 - 1s - loss: 0.0122 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 12/30
374/374 - 1s - loss: 0.0117 - val_loss: 0.0132 - 1s/epoch - 4ms/step
Epoch 13/30
374/374 - 1s - loss: 0.0115 - val_loss: 0.0161 - 1s/epoch - 4ms/step
Epoch 14/30
374/374 - 1s - loss: 0.0114 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 15/30
374/374 - 1s - loss: 0.0110 - val_loss: 0.0161 - 1s/epoch - 4ms/step
Epoch 16/30
374/374 - 1s - loss: 0.0119 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 17/30
374/374 - 1s - loss: 0.0108 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 18/30
374/374 - 1s - loss: 0.0112 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 19/30
374/374 - 1s - loss: 0.0113 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 20/30
374/374 - 1s - loss: 0.0110 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 21/30
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 22/30
374/374 - 1s - loss: 0.0107 - val_loss: 0.0114 - 1s/epoch - 4ms/step
Epoch 23/30
374/374 - 1s - loss: 0.0116 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 24/30
374/374 - 1s - loss: 0.0117 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 25/30
374/374 - 1s - loss: 0.0106 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 26/30
374/374 - 1s - loss: 0.0135 - val_loss: 0.0120 - 1s/epoch - 4ms/step
Epoch 27/30
374/374 - 1s - loss: 0.0159 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 28/30
374/374 - 1s - loss: 0.0113 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 29/30
374/374 - 1s - loss: 0.0138 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 30/30
374/374 - 1s - loss: 0.0122 - val_loss: 0.0108 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.010806521400809288
  1/332 [..............................] - ETA: 34s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.09046851459130942
cosine 0.06929359239337582
MAE: 0.04205342
RMSE: 0.08838104
r2: 0.49372185974457833
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'mse', 256, 30, 0.002, 0.2, 252, 0.012240471318364143, 0.010806521400809288, 0.09046851459130942, 0.06929359239337582, 0.04205342009663582, 0.08838103711605072, 0.49372185974457833, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 210 0.002 128 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/210
747/747 - 3s - loss: 0.0146 - val_loss: 0.0135 - 3s/epoch - 4ms/step
Epoch 2/210
747/747 - 2s - loss: 0.0081 - val_loss: 0.0088 - 2s/epoch - 3ms/step
Epoch 3/210
747/747 - 2s - loss: 0.0077 - val_loss: 0.0083 - 2s/epoch - 3ms/step
Epoch 4/210
747/747 - 2s - loss: 0.0075 - val_loss: 0.0081 - 2s/epoch - 3ms/step
Epoch 5/210
747/747 - 2s - loss: 0.0072 - val_loss: 0.0112 - 2s/epoch - 3ms/step
Epoch 6/210
747/747 - 2s - loss: 0.0069 - val_loss: 0.0189 - 2s/epoch - 3ms/step
Epoch 7/210
747/747 - 2s - loss: 0.0068 - val_loss: 0.0065 - 2s/epoch - 3ms/step
Epoch 8/210
747/747 - 2s - loss: 0.0065 - val_loss: 0.0075 - 2s/epoch - 3ms/step
Epoch 9/210
747/747 - 3s - loss: 0.0064 - val_loss: 0.0064 - 3s/epoch - 3ms/step
Epoch 10/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0066 - 2s/epoch - 3ms/step
Epoch 11/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 12/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 13/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0064 - 2s/epoch - 3ms/step
Epoch 14/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 15/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 3ms/step
Epoch 16/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 17/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 18/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 19/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 20/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 21/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 3ms/step
Epoch 22/210
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 23/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 24/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 25/210
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 3ms/step
Epoch 26/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 27/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 28/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 29/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 30/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 31/210
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 32/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 33/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 34/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 35/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 36/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 37/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 38/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 39/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 40/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 41/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 42/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 43/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 44/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 45/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 46/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 47/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 48/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 49/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 50/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 51/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 52/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 53/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 54/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 55/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 56/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 57/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 58/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 59/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 60/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 61/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 62/210
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 63/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 64/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 65/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 66/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 67/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 68/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 69/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 70/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 71/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 72/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 73/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 74/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 75/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 76/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 77/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 78/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 79/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 80/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 81/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 82/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 83/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 84/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 85/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 86/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 87/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 88/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 89/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 90/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 91/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 92/210
747/747 - 3s - loss: 0.0060 - val_loss: 0.0059 - 3s/epoch - 3ms/step
Epoch 93/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 94/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 95/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 96/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 97/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 98/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 99/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 100/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 101/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 102/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 103/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 104/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 105/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 106/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 107/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 108/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 109/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 110/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 111/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 112/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 113/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 114/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 115/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 116/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 117/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 118/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 119/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 120/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 121/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 122/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 123/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 124/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 125/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 126/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 127/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 128/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 129/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 130/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 131/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 132/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 133/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 134/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 135/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 136/210
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 137/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 138/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 139/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 140/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 141/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 142/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 143/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 144/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 145/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 146/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 147/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 148/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 149/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 150/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 151/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 152/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 153/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 154/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 155/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 156/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 157/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 158/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 159/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 160/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 161/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 162/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 163/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 164/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 165/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 166/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 167/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 168/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 169/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 170/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 171/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 172/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 173/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 174/210
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 3ms/step
Epoch 175/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 176/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 177/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 178/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 179/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 180/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 181/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 182/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 183/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 184/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 185/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 186/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 187/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 188/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 189/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 190/210
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 3ms/step
Epoch 191/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 192/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 193/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 194/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 195/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 196/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 197/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 198/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 199/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 200/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 201/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 202/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 203/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 204/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 205/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 206/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 207/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 208/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 209/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 210/210
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00585293909534812
  1/332 [..............................] - ETA: 40s 42/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s171/332 [==============>...............] - ETA: 0s214/332 [==================>...........] - ETA: 0s257/332 [======================>.......] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.10663092143123122
cosine 0.08141288537598088
MAE: 0.04420947
RMSE: 0.09575805
r2: 0.4056773446541574
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'logcosh', 128, 210, 0.002, 0.2, 252, 0.005907718557864428, 0.00585293909534812, 0.10663092143123122, 0.08141288537598088, 0.044209469109773636, 0.09575805068016052, 0.4056773446541574, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 120 0.0005 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0251 - val_loss: 0.0164 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0147 - val_loss: 0.0201 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0130 - val_loss: 0.0122 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0120 - val_loss: 0.0150 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0119 - val_loss: 0.0114 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0115 - val_loss: 0.0111 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0111 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009355678223073483
  1/332 [..............................] - ETA: 42s 42/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s168/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06961113722244389
cosine 0.05335342521223535
MAE: 0.035747156
RMSE: 0.07804988
r2: 0.6051650429315008
RMSE zero-vector: 0.2430644284356365
['1.9custom_VAE', 'mse', 64, 120, 0.0005, 0.2, 252, 0.009564477019011974, 0.009355678223073483, 0.06961113722244389, 0.05335342521223535, 0.03574715554714203, 0.07804988324642181, 0.6051650429315008, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.4 120 0.0007 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0194 - val_loss: 0.0154 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0137 - val_loss: 0.0133 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0131 - val_loss: 0.0128 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0127 - val_loss: 0.0125 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0123 - val_loss: 0.0118 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0117 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009342985227704048
  1/332 [..............................] - ETA: 42s 42/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s212/332 [==================>...........] - ETA: 0s254/332 [=====================>........] - ETA: 0s297/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07038681047887475
cosine 0.053957980515100444
MAE: 0.036006443
RMSE: 0.07844998
r2: 0.6011066428977988
RMSE zero-vector: 0.2430644284356365
['0.4custom_VAE', 'mse', 64, 120, 0.0007, 0.2, 252, 0.009602805599570274, 0.009342985227704048, 0.07038681047887475, 0.053957980515100444, 0.036006443202495575, 0.07844997942447662, 0.6011066428977988, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.6 120 0.0005 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
Epoch 1/120
374/374 - 2s - loss: 0.0296 - val_loss: 0.0157 - 2s/epoch - 6ms/step
Epoch 2/120
374/374 - 1s - loss: 0.0145 - val_loss: 0.0169 - 1s/epoch - 4ms/step
Epoch 3/120
374/374 - 1s - loss: 0.0139 - val_loss: 0.0158 - 1s/epoch - 4ms/step
Epoch 4/120
374/374 - 1s - loss: 0.0136 - val_loss: 0.0168 - 1s/epoch - 3ms/step
Epoch 5/120
374/374 - 1s - loss: 0.0134 - val_loss: 0.0138 - 1s/epoch - 4ms/step
Epoch 6/120
374/374 - 1s - loss: 0.0132 - val_loss: 0.0199 - 1s/epoch - 3ms/step
Epoch 7/120
374/374 - 1s - loss: 0.0132 - val_loss: 0.0137 - 1s/epoch - 3ms/step
Epoch 8/120
374/374 - 1s - loss: 0.0128 - val_loss: 0.0135 - 1s/epoch - 3ms/step
Epoch 9/120
374/374 - 1s - loss: 0.0124 - val_loss: 0.0132 - 1s/epoch - 3ms/step
Epoch 10/120
374/374 - 1s - loss: 0.0121 - val_loss: 0.0149 - 1s/epoch - 3ms/step
Epoch 11/120
374/374 - 1s - loss: 0.0120 - val_loss: 0.0120 - 1s/epoch - 3ms/step
Epoch 12/120
374/374 - 1s - loss: 0.0116 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 13/120
374/374 - 1s - loss: 0.0114 - val_loss: 0.0223 - 1s/epoch - 3ms/step
Epoch 14/120
374/374 - 1s - loss: 0.0117 - val_loss: 0.0113 - 1s/epoch - 3ms/step
Epoch 15/120
374/374 - 1s - loss: 0.0110 - val_loss: 0.0110 - 1s/epoch - 3ms/step
Epoch 16/120
374/374 - 1s - loss: 0.0109 - val_loss: 0.0108 - 1s/epoch - 3ms/step
Epoch 17/120
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 3ms/step
Epoch 18/120
374/374 - 1s - loss: 0.0107 - val_loss: 0.0137 - 1s/epoch - 3ms/step
Epoch 19/120
374/374 - 1s - loss: 0.0110 - val_loss: 0.0106 - 1s/epoch - 3ms/step
Epoch 20/120
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 3ms/step
Epoch 21/120
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 3ms/step
Epoch 22/120
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 23/120
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 24/120
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 25/120
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 26/120
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 27/120
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 28/120
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 29/120
374/374 - 1s - loss: 0.0100 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 30/120
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 31/120
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 32/120
374/374 - 1s - loss: 0.0099 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 33/120
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 34/120
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 35/120
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 36/120
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 37/120
374/374 - 1s - loss: 0.0100 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 38/120
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 39/120
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 40/120
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 41/120
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 42/120
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 43/120
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 44/120
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 45/120
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 46/120
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 47/120
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 48/120
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 49/120
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 50/120
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 51/120
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 52/120
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 53/120
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 54/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 55/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 56/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 57/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 58/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 59/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 60/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 61/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 62/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 63/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 64/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 65/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 66/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 67/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 68/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 69/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 70/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 71/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 72/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 73/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 74/120
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 75/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 76/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 77/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 78/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 79/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 80/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 81/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 82/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 83/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 84/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 85/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 86/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 87/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 88/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 89/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 90/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 91/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 92/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 93/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 94/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 95/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 96/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 97/120
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 98/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 99/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 100/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 101/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 102/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 103/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 104/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 105/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 106/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 107/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 108/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 109/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 110/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 111/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 112/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 113/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 114/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 115/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 116/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 117/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 118/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 119/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 120/120
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009173582307994366
  1/332 [..............................] - ETA: 40s 42/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s169/332 [==============>...............] - ETA: 0s211/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s295/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06517834518255039
cosine 0.049957018667151694
MAE: 0.034608036
RMSE: 0.07556228
r2: 0.6299325841597427
RMSE zero-vector: 0.2430644284356365
['0.6custom_VAE', 'mse', 256, 120, 0.0005, 0.2, 252, 0.009267494082450867, 0.009173582307994366, 0.06517834518255039, 0.049957018667151694, 0.034608036279678345, 0.07556228339672089, 0.6299325841597427, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 120 0.0007 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0199 - val_loss: 0.0156 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0139 - val_loss: 0.0136 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0132 - val_loss: 0.0128 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0125 - val_loss: 0.0120 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0119 - val_loss: 0.0115 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0116 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009332033805549145
  1/332 [..............................] - ETA: 40s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06872877055660447
cosine 0.052674392417292305
MAE: 0.035457116
RMSE: 0.0775327
r2: 0.6103804467548344
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 64, 120, 0.0007, 0.2, 252, 0.00959395058453083, 0.009332033805549145, 0.06872877055660447, 0.052674392417292305, 0.03545711562037468, 0.07753270119428635, 0.6103804467548344, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 120 0.0007 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0257 - val_loss: 0.0190 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0146 - val_loss: 0.0134 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0128 - val_loss: 0.0151 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0125 - val_loss: 0.0115 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0117 - val_loss: 0.0113 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0115 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0111 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00935208611190319
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s173/332 [==============>...............] - ETA: 0s216/332 [==================>...........] - ETA: 0s260/332 [======================>.......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06992366917282511
cosine 0.053616485880358555
MAE: 0.035914868
RMSE: 0.07826225
r2: 0.6030136630245481
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 64, 120, 0.0007, 0.2, 252, 0.00959213636815548, 0.00935208611190319, 0.06992366917282511, 0.053616485880358555, 0.035914868116378784, 0.07826224714517593, 0.6030136630245481, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.4 120 0.002 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3033)         3836745     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3033)        12132       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3033)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          764568      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          764568      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4679221     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,057,234
Trainable params: 10,044,598
Non-trainable params: 12,636
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0275 - val_loss: 0.0307 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0152 - val_loss: 0.0148 - 5s/epoch - 4ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0130 - val_loss: 0.0120 - 5s/epoch - 4ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0120 - val_loss: 0.0118 - 5s/epoch - 4ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0112 - 5s/epoch - 4ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0110 - 5s/epoch - 4ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 4ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0108 - 5s/epoch - 4ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0107 - 5s/epoch - 4ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0106 - 5s/epoch - 4ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 4ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0104 - 5s/epoch - 4ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 4ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 4ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 4ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 4ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 4ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0102 - 5s/epoch - 4ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 4ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 4ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 4ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 4ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 4ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 4ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 4ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009395947679877281
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s215/332 [==================>...........] - ETA: 0s259/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0713974674535256
cosine 0.05475912889893263
MAE: 0.036356643
RMSE: 0.07901101
r2: 0.5953811031745818
RMSE zero-vector: 0.2430644284356365
['2.4custom_VAE', 'mse', 64, 120, 0.002, 0.2, 252, 0.009643626399338245, 0.009395947679877281, 0.0713974674535256, 0.05475912889893263, 0.036356642842292786, 0.07901100814342499, 0.5953811031745818, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.4 120 0.0005 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0194 - val_loss: 0.0144 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0137 - val_loss: 0.0134 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0131 - val_loss: 0.0128 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0125 - val_loss: 0.0119 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0111 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009346007369458675
  1/332 [..............................] - ETA: 38s  2/332 [..............................] - ETA: 5:34 45/332 [===>..........................] - ETA: 6s   88/332 [======>.......................] - ETA: 3s132/332 [==========>...................] - ETA: 1s175/332 [==============>...............] - ETA: 1s218/332 [==================>...........] - ETA: 0s262/332 [======================>.......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 2s 4ms/step
correlation 0.0697088293812655
cosine 0.053405080410475286
MAE: 0.035763316
RMSE: 0.07810635
r2: 0.6045936011651523
RMSE zero-vector: 0.2430644284356365
['0.4custom_VAE', 'mse', 64, 120, 0.0005, 0.2, 252, 0.009573119692504406, 0.009346007369458675, 0.0697088293812655, 0.053405080410475286, 0.03576331585645676, 0.0781063511967659, 0.6045936011651523, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 120 0.00030000000000000003 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0078 - 5s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0073 - val_loss: 0.0076 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0070 - val_loss: 0.0070 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0069 - val_loss: 0.0068 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0068 - val_loss: 0.0067 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0068 - val_loss: 0.0067 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0067 - val_loss: 0.0066 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0065 - val_loss: 0.0063 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005929796490818262
  1/332 [..............................] - ETA: 45s 41/332 [==>...........................] - ETA: 0s  82/332 [======>.......................] - ETA: 0s123/332 [==========>...................] - ETA: 0s164/332 [=============>................] - ETA: 0s204/332 [=================>............] - ETA: 0s246/332 [=====================>........] - ETA: 0s288/332 [=========================>....] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.10972237443748709
cosine 0.08382962551985962
MAE: 0.045038134
RMSE: 0.09709971
r2: 0.3889067901110721
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 64, 120, 0.00030000000000000003, 0.2, 252, 0.005974648054689169, 0.005929796490818262, 0.10972237443748709, 0.08382962551985962, 0.0450381338596344, 0.09709970653057098, 0.3889067901110721, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[1.7999999999999998 120 0.0005 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2275)         2877875     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2275)        9100        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2275)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3526303     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,560,382
Trainable params: 7,550,778
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/120
1493/1493 - 6s - loss: 0.0250 - val_loss: 0.0171 - 6s/epoch - 4ms/step
Epoch 2/120
1493/1493 - 5s - loss: 0.0146 - val_loss: 0.0164 - 5s/epoch - 3ms/step
Epoch 3/120
1493/1493 - 5s - loss: 0.0132 - val_loss: 0.0124 - 5s/epoch - 3ms/step
Epoch 4/120
1493/1493 - 5s - loss: 0.0122 - val_loss: 0.0120 - 5s/epoch - 3ms/step
Epoch 5/120
1493/1493 - 5s - loss: 0.0119 - val_loss: 0.0115 - 5s/epoch - 3ms/step
Epoch 6/120
1493/1493 - 5s - loss: 0.0115 - val_loss: 0.0111 - 5s/epoch - 3ms/step
Epoch 7/120
1493/1493 - 5s - loss: 0.0112 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 8/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 9/120
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 10/120
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 11/120
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 12/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 13/120
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 14/120
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 15/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 16/120
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 17/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 18/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/120
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 20/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 23/120
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 26/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/120
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 32/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 33/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 34/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 35/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/120
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 46/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 47/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/120
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 61/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 64/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 66/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 67/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 69/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 73/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/120
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 94/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 95/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 96/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 97/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 98/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 99/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 100/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 101/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 102/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 103/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 104/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 105/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 106/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 107/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 110/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 112/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 113/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 114/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 119/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/120
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009372260421514511
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s171/332 [==============>...............] - ETA: 0s214/332 [==================>...........] - ETA: 0s257/332 [======================>.......] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0688422136472119
cosine 0.05277102522880592
MAE: 0.03560644
RMSE: 0.07764626
r2: 0.6092381339020743
RMSE zero-vector: 0.2430644284356365
['1.7999999999999998custom_VAE', 'mse', 64, 120, 0.0005, 0.2, 252, 0.009591788984835148, 0.009372260421514511, 0.0688422136472119, 0.05277102522880592, 0.03560644015669823, 0.07764626294374466, 0.6092381339020743, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 120 0.00030000000000000003 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
Epoch 1/120
747/747 - 4s - loss: 0.0302 - val_loss: 0.0238 - 4s/epoch - 5ms/step
Epoch 2/120
747/747 - 3s - loss: 0.0159 - val_loss: 0.0209 - 3s/epoch - 4ms/step
Epoch 3/120
747/747 - 3s - loss: 0.0145 - val_loss: 0.0300 - 3s/epoch - 4ms/step
Epoch 4/120
747/747 - 3s - loss: 0.0138 - val_loss: 0.0146 - 3s/epoch - 4ms/step
Epoch 5/120
747/747 - 3s - loss: 0.0129 - val_loss: 0.0127 - 3s/epoch - 4ms/step
Epoch 6/120
747/747 - 3s - loss: 0.0123 - val_loss: 0.0123 - 3s/epoch - 4ms/step
Epoch 7/120
747/747 - 3s - loss: 0.0119 - val_loss: 0.0168 - 3s/epoch - 4ms/step
Epoch 8/120
747/747 - 3s - loss: 0.0124 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 9/120
747/747 - 3s - loss: 0.0112 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 10/120
747/747 - 3s - loss: 0.0110 - val_loss: 0.0109 - 3s/epoch - 4ms/step
Epoch 11/120
747/747 - 3s - loss: 0.0111 - val_loss: 0.0107 - 3s/epoch - 4ms/step
Epoch 12/120
747/747 - 3s - loss: 0.0108 - val_loss: 0.0112 - 3s/epoch - 4ms/step
Epoch 13/120
747/747 - 3s - loss: 0.0119 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 14/120
747/747 - 3s - loss: 0.0112 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 15/120
747/747 - 3s - loss: 0.0113 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 16/120
747/747 - 3s - loss: 0.0107 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 17/120
747/747 - 3s - loss: 0.0106 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 18/120
747/747 - 3s - loss: 0.0107 - val_loss: 0.0115 - 3s/epoch - 4ms/step
Epoch 19/120
747/747 - 3s - loss: 0.0116 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 20/120
747/747 - 3s - loss: 0.0106 - val_loss: 0.0104 - 3s/epoch - 4ms/step
Epoch 21/120
747/747 - 3s - loss: 0.0105 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 22/120
747/747 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 4ms/step
Epoch 23/120
747/747 - 3s - loss: 0.0103 - val_loss: 0.0106 - 3s/epoch - 4ms/step
Epoch 24/120
747/747 - 3s - loss: 0.0108 - val_loss: 0.0105 - 3s/epoch - 4ms/step
Epoch 25/120
747/747 - 3s - loss: 0.0108 - val_loss: 0.0110 - 3s/epoch - 4ms/step
Epoch 26/120
747/747 - 3s - loss: 0.0112 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 27/120
747/747 - 3s - loss: 0.0103 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 28/120
747/747 - 3s - loss: 0.0102 - val_loss: 0.0102 - 3s/epoch - 4ms/step
Epoch 29/120
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 30/120
747/747 - 3s - loss: 0.0102 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 31/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 32/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 33/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 4ms/step
Epoch 34/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 4ms/step
Epoch 35/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 36/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 37/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 4ms/step
Epoch 38/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 39/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 40/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 41/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 42/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 4ms/step
Epoch 43/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 44/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 45/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 46/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 47/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 4ms/step
Epoch 48/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 49/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 50/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 51/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 52/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 53/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 54/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 55/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 56/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 57/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 58/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 59/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 4ms/step
Epoch 60/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 61/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 62/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 63/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 64/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 65/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 66/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 67/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 68/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 69/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 70/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 71/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 72/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 73/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 74/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 75/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 76/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 77/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 78/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 79/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 80/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 4ms/step
Epoch 81/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 82/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 83/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 84/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 85/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 86/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 87/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 88/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 89/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 90/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 91/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 92/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 93/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 94/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 95/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 96/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 97/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 98/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 99/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 100/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 101/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 102/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 103/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 104/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 105/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 106/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 107/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 108/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 109/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 110/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 111/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 112/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 113/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 114/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 115/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 116/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 117/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
Epoch 118/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 119/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 4ms/step
Epoch 120/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009238596074283123
  1/332 [..............................] - ETA: 38s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s171/332 [==============>...............] - ETA: 0s214/332 [==================>...........] - ETA: 0s257/332 [======================>.......] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06808022634065515
cosine 0.05218320948489873
MAE: 0.03535678
RMSE: 0.07715575
r2: 0.6141604608856102
RMSE zero-vector: 0.2430644284356365
['1.9custom_VAE', 'mse', 128, 120, 0.00030000000000000003, 0.2, 252, 0.009413989260792732, 0.009238596074283123, 0.06808022634065515, 0.05218320948489873, 0.035356778651475906, 0.07715574651956558, 0.6141604608856102, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.6 90 0.0018 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 758)          958870      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 758)         3032        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 758)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          191268      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1218946     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,563,384
Trainable params: 2,559,848
Non-trainable params: 3,536
__________________________________________________________________________________________________
Epoch 1/90
1493/1493 - 6s - loss: 0.0207 - val_loss: 0.0149 - 6s/epoch - 4ms/step
Epoch 2/90
1493/1493 - 5s - loss: 0.0139 - val_loss: 0.0135 - 5s/epoch - 3ms/step
Epoch 3/90
1493/1493 - 5s - loss: 0.0131 - val_loss: 0.0126 - 5s/epoch - 3ms/step
Epoch 4/90
1493/1493 - 5s - loss: 0.0125 - val_loss: 0.0120 - 5s/epoch - 3ms/step
Epoch 5/90
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0115 - 5s/epoch - 3ms/step
Epoch 6/90
1493/1493 - 5s - loss: 0.0116 - val_loss: 0.0113 - 5s/epoch - 3ms/step
Epoch 7/90
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0110 - 5s/epoch - 3ms/step
Epoch 8/90
1493/1493 - 5s - loss: 0.0112 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 9/90
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 10/90
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 11/90
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 12/90
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 13/90
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/90
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/90
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 16/90
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/90
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/90
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/90
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 20/90
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/90
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/90
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/90
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 24/90
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 25/90
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/90
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/90
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 32/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/90
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 41/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 43/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 44/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 45/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/90
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 57/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 59/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 61/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 62/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 64/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 66/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 67/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/90
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 85/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/90
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00936503242701292
  1/332 [..............................] - ETA: 40s 43/332 [==>...........................] - ETA: 0s  85/332 [======>.......................] - ETA: 0s127/332 [==========>...................] - ETA: 0s170/332 [==============>...............] - ETA: 0s213/332 [==================>...........] - ETA: 0s256/332 [======================>.......] - ETA: 0s299/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06971457484889111
cosine 0.05341635455714052
MAE: 0.03575675
RMSE: 0.07808567
r2: 0.6048030713980279
RMSE zero-vector: 0.2430644284356365
['0.6custom_VAE', 'mse', 64, 90, 0.0018, 0.2, 252, 0.009605172090232372, 0.00936503242701292, 0.06971457484889111, 0.05341635455714052, 0.03575674816966057, 0.07808566838502884, 0.6048030713980279, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.9 120 0.0007 64 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2401)         3037265     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2401)        9604        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2401)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          605304      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3717949     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,975,426
Trainable params: 7,965,318
Non-trainable params: 10,108
__________________________________________________________________________________________________
2023-02-22 17:02:20.043463: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 85065728/23676715008
2023-02-22 17:02:20.043497: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22802915272
MaxInUse:                  22878326209
NumAllocs:                   328207204
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-22 17:02:20.043559: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-22 17:02:20.043565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 90
2023-02-22 17:02:20.043568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 93
2023-02-22 17:02:20.043571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 364
2023-02-22 17:02:20.043582: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-22 17:02:20.043585: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 44
2023-02-22 17:02:20.043587: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 132
2023-02-22 17:02:20.043590: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 44
2023-02-22 17:02:20.043592: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 64
2023-02-22 17:02:20.043595: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 44
2023-02-22 17:02:20.043598: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-22 17:02:20.043600: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-22 17:02:20.043603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-22 17:02:20.043605: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-22 17:02:20.043608: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 44
2023-02-22 17:02:20.043610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 64
2023-02-22 17:02:20.043613: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-22 17:02:20.043616: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-22 17:02:20.043618: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-22 17:02:20.043621: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 18
2023-02-22 17:02:20.043623: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-22 17:02:20.043626: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-22 17:02:20.043629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 27
2023-02-22 17:02:20.043631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-22 17:02:20.043634: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-22 17:02:20.043636: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 18
2023-02-22 17:02:20.043639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-22 17:02:20.043641: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-22 17:02:20.043644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 12
2023-02-22 17:02:20.043647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-22 17:02:20.043649: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-22 17:02:20.043652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-22 17:02:20.043654: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-22 17:02:20.043657: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-22 17:02:20.043660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-22 17:02:20.043662: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.9custom_VAE', 'logcosh', 64, 120, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.9 120 0.0007 64 2]) is not valid.
[2.1 120 0.0007 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          669060      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4102762     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,808,808
Trainable params: 8,797,688
Non-trainable params: 11,120
__________________________________________________________________________________________________
2023-02-22 17:02:21.872117: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-22 17:02:21.872153: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22838150520
MaxInUse:                  22878326209
NumAllocs:                   328207264
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-22 17:02:21.872220: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-22 17:02:21.872233: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 94
2023-02-22 17:02:21.872236: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 93
2023-02-22 17:02:21.872239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 371
2023-02-22 17:02:21.872241: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-22 17:02:21.872244: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 44
2023-02-22 17:02:21.872247: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 132
2023-02-22 17:02:21.872249: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 44
2023-02-22 17:02:21.872252: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 65
2023-02-22 17:02:21.872254: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 44
2023-02-22 17:02:21.872257: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 22
2023-02-22 17:02:21.872260: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-22 17:02:21.872262: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-22 17:02:21.872265: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 10
2023-02-22 17:02:21.872267: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-22 17:02:21.872270: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 44
2023-02-22 17:02:21.872273: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 65
2023-02-22 17:02:21.872275: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-22 17:02:21.872278: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-22 17:02:21.872280: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-22 17:02:21.872283: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 18
2023-02-22 17:02:21.872286: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 9
2023-02-22 17:02:21.872288: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-22 17:02:21.872291: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 27
2023-02-22 17:02:21.872293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-22 17:02:21.872296: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2675232, 3
2023-02-22 17:02:21.872299: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-22 17:02:21.872301: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 18
2023-02-22 17:02:21.872304: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-22 17:02:21.872306: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-22 17:02:21.872309: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 12
2023-02-22 17:02:21.872311: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 6
2023-02-22 17:02:21.872314: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-22 17:02:21.872317: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-22 17:02:21.872319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 2
2023-02-22 17:02:21.872322: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-22 17:02:21.872325: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-22 17:02:21.872330: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-22 17:02:21.872333: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['2.1custom_VAE', 'logcosh', 64, 120, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([2.1 120 0.0007 64 2]) is not valid.
[1.7999999999999998 115 0.0005 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2275)         2877875     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2275)        9100        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2275)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          573552      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3526303     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,560,382
Trainable params: 7,550,778
Non-trainable params: 9,604
__________________________________________________________________________________________________
2023-02-22 17:02:24.438399: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-22 17:02:24.438441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22833156816
MaxInUse:                  22878326209
NumAllocs:                   328207320
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-22 17:02:24.438504: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-22 17:02:24.438510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 94
2023-02-22 17:02:24.438513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 93
2023-02-22 17:02:24.438516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 371
2023-02-22 17:02:24.438518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-22 17:02:24.438521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 44
2023-02-22 17:02:24.438524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 132
2023-02-22 17:02:24.438526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 44
2023-02-22 17:02:24.438529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 65
2023-02-22 17:02:24.438531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 44
2023-02-22 17:02:24.438534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 32
2023-02-22 17:02:24.438536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-22 17:02:24.438539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-22 17:02:24.438542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-22 17:02:24.438544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 44
2023-02-22 17:02:24.438547: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 65
2023-02-22 17:02:24.438549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-22 17:02:24.438552: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 54
2023-02-22 17:02:24.438555: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-22 17:02:24.438557: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 18
2023-02-22 17:02:24.438560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 12
2023-02-22 17:02:24.438562: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-22 17:02:24.438565: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 27
2023-02-22 17:02:24.438568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-22 17:02:24.438570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-22 17:02:24.438573: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 18
2023-02-22 17:02:24.438575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 36
2023-02-22 17:02:24.438578: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-22 17:02:24.438581: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 12
2023-02-22 17:02:24.438583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 8
2023-02-22 17:02:24.438586: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-22 17:02:24.438590: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-22 17:02:24.438593: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-22 17:02:24.438596: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-22 17:02:24.438598: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-22 17:02:24.438601: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.7999999999999998custom_VAE', 'mse', 128, 115, 0.0005, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.7999999999999998 115 0.0005 128 1]) is not valid.
[0.5 90 0.0018 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-22 17:02:26.346069: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-22 17:02:26.346101: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22841750176
MaxInUse:                  22878326209
NumAllocs:                   328207376
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-22 17:02:26.346166: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-22 17:02:26.346171: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 94
2023-02-22 17:02:26.346174: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 93
2023-02-22 17:02:26.346177: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 378
2023-02-22 17:02:26.346180: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-22 17:02:26.346183: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 44
2023-02-22 17:02:26.346185: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 142
2023-02-22 17:02:26.346188: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 44
2023-02-22 17:02:26.346191: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 66
2023-02-22 17:02:26.346193: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 44
2023-02-22 17:02:26.346196: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 32
2023-02-22 17:02:26.346198: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-22 17:02:26.346201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-22 17:02:26.346204: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-22 17:02:26.346206: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 44
2023-02-22 17:02:26.346209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 66
2023-02-22 17:02:26.346211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-22 17:02:26.346214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-22 17:02:26.346217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-22 17:02:26.346219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 18
2023-02-22 17:02:26.346222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 12
2023-02-22 17:02:26.346225: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-22 17:02:26.346227: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 27
2023-02-22 17:02:26.346230: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-22 17:02:26.346232: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-22 17:02:26.346235: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 18
2023-02-22 17:02:26.346238: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-22 17:02:26.346240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-22 17:02:26.346250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 12
2023-02-22 17:02:26.346253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 8
2023-02-22 17:02:26.346255: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-22 17:02:26.346258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-22 17:02:26.346261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-22 17:02:26.346263: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-22 17:02:26.346266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-22 17:02:26.346268: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 90, 0.0018, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 90 0.0018 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[0.5 90 0.0018 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-22 17:02:37.129259: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-22 17:02:37.129293: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22841750176
MaxInUse:                  22878326209
NumAllocs:                   328207432
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-22 17:02:37.129352: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-22 17:02:37.129358: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 94
2023-02-22 17:02:37.129361: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 93
2023-02-22 17:02:37.129363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 378
2023-02-22 17:02:37.129366: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-22 17:02:37.129369: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 44
2023-02-22 17:02:37.129372: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 142
2023-02-22 17:02:37.129374: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 44
2023-02-22 17:02:37.129377: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 66
2023-02-22 17:02:37.129380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 44
2023-02-22 17:02:37.129382: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 32
2023-02-22 17:02:37.129385: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-22 17:02:37.129387: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-22 17:02:37.129390: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-22 17:02:37.129392: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 44
2023-02-22 17:02:37.129395: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 66
2023-02-22 17:02:37.129398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-22 17:02:37.129400: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-22 17:02:37.129403: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-22 17:02:37.129405: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 18
2023-02-22 17:02:37.129408: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 12
2023-02-22 17:02:37.129411: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-22 17:02:37.129413: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 27
2023-02-22 17:02:37.129416: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-22 17:02:37.129418: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-22 17:02:37.129428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 18
2023-02-22 17:02:37.129431: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-22 17:02:37.129434: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-22 17:02:37.129436: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 12
2023-02-22 17:02:37.129439: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 8
2023-02-22 17:02:37.129441: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-22 17:02:37.129444: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-22 17:02:37.129447: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-22 17:02:37.129449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-22 17:02:37.129452: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-22 17:02:37.129454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 128, 90, 0.0018, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 90 0.0018 128 1]) is not valid.
[0.5 90 0.002 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
2023-02-22 17:02:38.939342: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-22 17:02:38.939375: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22850343536
MaxInUse:                  22878326209
NumAllocs:                   328207488
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-22 17:02:38.939440: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-22 17:02:38.939445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 94
2023-02-22 17:02:38.939449: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 93
2023-02-22 17:02:38.939451: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 385
2023-02-22 17:02:38.939454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-22 17:02:38.939457: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 44
2023-02-22 17:02:38.939460: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 152
2023-02-22 17:02:38.939462: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 44
2023-02-22 17:02:38.939465: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 67
2023-02-22 17:02:38.939467: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 44
2023-02-22 17:02:38.939470: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 32
2023-02-22 17:02:38.939473: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-22 17:02:38.939475: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-22 17:02:38.939478: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-22 17:02:38.939480: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 44
2023-02-22 17:02:38.939483: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 67
2023-02-22 17:02:38.939486: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-22 17:02:38.939488: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 60
2023-02-22 17:02:38.939491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-22 17:02:38.939493: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 18
2023-02-22 17:02:38.939496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 12
2023-02-22 17:02:38.939499: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-22 17:02:38.939510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 27
2023-02-22 17:02:38.939513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-22 17:02:38.939516: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-22 17:02:38.939518: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 18
2023-02-22 17:02:38.939521: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 40
2023-02-22 17:02:38.939524: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-22 17:02:38.939526: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 12
2023-02-22 17:02:38.939529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 8
2023-02-22 17:02:38.939531: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-22 17:02:38.939534: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-22 17:02:38.939536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-22 17:02:38.939539: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-22 17:02:38.939542: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-22 17:02:38.939544: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'logcosh', 128, 90, 0.002, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 90 0.002 128 2]) is not valid.
[1.7999999999999998 120 0.0005 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-22 17:02:40.428654: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 11502400 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-22 17:02:40.428688: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22871986668
MaxInUse:                  22884850760
NumAllocs:                   328207539
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-22 17:02:40.428753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-22 17:02:40.428758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 93
2023-02-22 17:02:40.428761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 94
2023-02-22 17:02:40.428764: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-22 17:02:40.428767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 385
2023-02-22 17:02:40.428770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-22 17:02:40.428772: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 44
2023-02-22 17:02:40.428775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 142
2023-02-22 17:02:40.428787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 44
2023-02-22 17:02:40.428790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 66
2023-02-22 17:02:40.428792: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 44
2023-02-22 17:02:40.428795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9100, 42
2023-02-22 17:02:40.428797: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 54
2023-02-22 17:02:40.428800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-22 17:02:40.428803: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12132, 22
2023-02-22 17:02:40.428805: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 44
2023-02-22 17:02:40.428808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 67
2023-02-22 17:02:40.428810: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 18
2023-02-22 17:02:40.428813: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 57
2023-02-22 17:02:40.428816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 764064, 18
2023-02-22 17:02:40.428818: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 18
2023-02-22 17:02:40.428821: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2293200, 15
2023-02-22 17:02:40.428823: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2420208, 21
2023-02-22 17:02:40.428826: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 27
2023-02-22 17:02:40.428828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 12
2023-02-22 17:02:40.428831: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3057264, 9
2023-02-22 17:02:40.428834: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 18
2023-02-22 17:02:40.428836: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 38
2023-02-22 17:02:40.428839: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 12
2023-02-22 17:02:40.428841: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 12
2023-02-22 17:02:40.428844: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 11502400, 10
2023-02-22 17:02:40.428847: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 14
2023-02-22 17:02:40.428849: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-22 17:02:40.428852: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15334848, 6
2023-02-22 17:02:40.428854: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 12
2023-02-22 17:02:40.428857: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 21
2023-02-22 17:02:40.428859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
2023-02-22 17:02:40.428883: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: failed to allocate memory
Traceback (most recent call last):
  File "genetic.py", line 51, in <module>
    random_state = 1,
  File "genetic.py", line 25, in main
    os.environ["TF_CPP_VMODULE"]="gpu_process_state=10,gpu_cudamallocasync_allocator=10"
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 209, in genetic_hypertune_autoencoder
    # get scores and genes of ga_instance
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1413, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 357, in train_autoencoder
    model = create_autoencoder(n_inputs=n_inputs, layers_structure=layers_structure, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 590, in create_autoencoder
    decoder_output = Dense(n_inputs, activation='linear', name='outputlayer')(d)
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]
Wed Feb 22 17:02:49 CET 2023
done
